<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Naive Bayes Classifier Tutorial | Naive Bayes Classifier Example | Naive Bayes in R | Edureka | Coder Coacher - Coaching Coders</title><meta content="Naive Bayes Classifier Tutorial | Naive Bayes Classifier Example | Naive Bayes in R | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Naive Bayes Classifier Tutorial | Naive Bayes Classifier Example | Naive Bayes in R | Edureka</b></h2><h5 class="post__date">2017-05-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/psHrcSacU9Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone so in this demonstration
we'll look at one of the most powerful
machine learning algorithms called nee
base right and it's a probabilistic
method based on something that we know
named the base theorem right and that's
how the kind of name came in here so
we're going to go through some basics of
machine learning the various types of
machine learning algorithms and with
that context we will slowly get into n
demonstration now where we will see the
working of from a base alright let's
start so this is the broad kind of an
objective which I will going to cover I
will talk something about a bit very
crisp details on machine learning I will
introduce to the kind of classification
problems which we can solve using
machine learning techniques some of the
algorithms which are used for
classification problem solving and then
we'll with that context head we'll
discuss in a base with some use cases
and finally a demonstration in R ok so
this machine learning so as like in many
of the use cases across industries or in
different context and in different
business setting people have been
finding scenarios and problem statements
where human beings are kind of slowly
being replaced by decisions taken by
some intelligent system so in all of
these use cases the common thing that
comes out very clearly is it is very
difficult to design a software system
which can be coded with our rules right
or maybe plenty of rules so in
traditional system of designing a
software and like coming out with a
completely automated processes the
algorithms that goes behind the
designing such software's had a very
clear logic or algorithm which is like
following a sequence of steps and in
these sequence of steps all these
algorithms have a predefined nature that
means you code a particular logic and
that logic is executed by the algorithm
and hence the software works on that but
with the problem statements
which was like hard to do this way for
instance if you want to design a system
for speech recognition right speech as a
specific data point is quite different
from one person to the other
right so it's very difficult to set a
rule-based or a logic based algorithm
which can recognize a particular speech
irrespective of who is speaking that
word or speaking that sentence so it's
very difficult to design such an
algorithm by just mean of a logic or a
procedure so there comes the aspect
where we need to learn certain aspects
or certain features of the voice that is
given as an input to this speech
recognition system and these are the
sort of typical examples where the
machine learning algorithms really
started to play a big role similar is
the case with let's say face recognition
everybody is different in terms of the
appearances so it's very difficult to do
a hard-coding or define a logic that
this is how a face looks like so you
need to design an intelligent system
which can like I iterate through
hundreds of thousands of images of the
face and then start to form a pattern
out of it and that patterns is what goes
behind any learning system right so I am
going to discuss certain aspects of this
and how to design such a learning system
and if you are able to successfully do
that a lot of problems which had
something very similar in this kind of
scope can then slowly be made a bit more
intelligent rather than new hot coding
things on it right and that is where
this paradigm shift is happening in
industry as well you are moving from an
elm or procedural or stepwise
algorithmic way of creating software's
to machine learning by itself and then
taking these sort of calls and building
a program based on that so that's how
your software's and products are moving
in the India next gen to come right so
having said that that machines are
trying to learn these phases and the
kind of voice data it is also possible
that there is a certain amount of error
in doing so right because we are not
giving any hard-coded rules which we
know that it's definitely going to work
like for instance if I want to give you
an example here what Google does Google
does a page ranking algorithm
right and that algorithm is about a
predefined steps you give a set of data
points to this algorithm it comes out
with a score for our page and based on
that score if a web page has a very good
PageRank it comes at the top of your
search result so which is a very
procedural way of doing it because
Google has cracked that algorithm right
but it is not so possible with all the
different sort of problems which is a
bit more complex than a page ranks idea
right and that's where machine learnings
are like slowly taking over the use
cases but if let's say Google would have
gone the other way of building a machine
learning algorithm rather than building
a procedural or heuristic based page
ranking algorithm it would also have
faced lot of errors and in that process
it might be possible that you might
start to get a lot of irrelevant
webpages for the search query you are
doing and that's the only reason Google
at till this point I have not
extensively changed page ranking
algorithm to work like a machine
learning procedural way right so keeping
both of these things in mind that there
are problems which are very hard for a
heuristic algorithm to solve and at the
same time there needs to be given an
effort where such a heuristic algorithm
is possible for the problem statement so
if you are able to evaluate on these two
aspects then you know that for a given
problem whether you want to go the
machine learning way or you want to go
through the heuristic way and giants
like Amazon and Google and Facebook of
the world have like mastered the skill
of identifying the problem and which
way'd like we should be moving right
having said that it's not so easy it
comes with a lot of experience and
working on many problems right so with
that context picking of the details here
is be supervised and unsupervised
learning is the two major kinds of
learning algorithms right which in its
own nuances has a lot of details
internally so I'm going to talk talk
about a supervised form of learning
today with taking the Navy base as an
example right so I'm going to elaborate
anyway on this in detail okay
one of the problem types that supervised
learning solves for us is classification
right given a record can I lie classify
it into different categories so for
example if it is fraud detection in a
banking system a fraud transaction
detection in a banking system whether a
given transaction is fraud or the
genuine right or if I am like trying to
understand if it is going to rain
tomorrow or not again a binary
classification yes or no right or if I
want to understand given a set of files
if that file is a malware and if yes
that's a binary problem to identify
whether it's a malware or not a binary
classification problem method and if I
want to go for and step further and say
which type of malware is it at Rosslyn
it's a virus or it's a warm and so on
right so then this becomes a multi-class
problem so in classification we are all
like interested in knowing which class
the data belongs to or which category
the beta belongs to obviously there is
going to be a learning involved here
from the past data that we have caught
there are many classification algorithms
the one which we are going to focus
today is neighbors all right all right
so starting directly on what neighbors
is all about let's take a very small fun
example here and then bases that we can
able to like understand this algorithm
much better so let's assume here that
like you are really great football
enthusiast and most of the time when you
like go to play or play with your
friends so it suddenly starts to rain
and maybe there are like other weather
condition it's like very strong wind or
things like it's too hot outside as in
the weather and there are many other
factors also which would be possible and
because of these reasons so you
sometimes go to the field but like end
up disappointed like because of these
weather conditions you could not play so
is it like some some way possible that
by feeding an algorithm or some set of
information about your weather condition
as such you can maybe come out with a
prediction whether we should play today
or not right like very cool algorithm
which you like can show to your friends
and be like tiki key person and
footballers right so these are the
possible weather combinations which we
can normally get we can have maybe in
summer we can have like in summer too
hot extreme weather versus a warm kind
of cool weather in monsoon it might be a
light result versus strong rain similar
in winters also there might be
possibilities of Vince and so on so
combination of weather conditions in the
summer monsoon and winters we can have
conditions like too hot like normal
versus too windy versus no wind right so
let's see how we can model this approach
and try to come out with certain
predictions so good thing is so the
fellow football enthusiast as well as we
understand machine learning and all the
ideas behind it has created a good
dataset from his past experiences right
and that data set has a lot of
information in it
are we going to get to the data set
discussion in a while but let's see what
kind of cases we can normally get in the
data set which is being collected we see
that there can be like different
scenarios based on that and these are
the typical sort of those scenarios so
let's say we are in the summer and the
temperature is like sunny outside right
which means the temperature has certain
high value so let's assume for now that
it's a more than 40 degree Celsius
outside right if that's the case it says
these the circles represents the sort of
probabilities associated with it so in
how many instances like in the past did
we had a really hot weather and we
played football as well so your friend
has collected this data and he says that
there were very few instances like that
so that's why the probability is marked
by the small circle and which is like
quite low less than 0.5 right but
whereas in summers when it was not too
hot like the temperature was less than
very less than 40 degree Celsius the
probability was quite high that's why
you see a light like AB larger circle
similar is the case with monsoon as well
right I think even in monsoon if there
is a like high temperature I don't think
there will be any high temperature but
here in this case if we somehow
categorize a temperature to be sunny in
this monsoon season a people did play
there is like this a bit of coolness on
top of it sunny weather similar is the
case with the winter as well so there is
like like hardly any case where it will
be too sunny
so it was like variable so when I say
sunny it is like the Outlook not in
terms of the temperature as such you can
see outside and see there is Sun out so
that's what we call sunny here right so
in a winter if there is Sun it's like
we're really Pleasant same is the case
with monsoon but not the case with
someone so this is the sort of typical
case one of the cases and let's look at
this case as well something very similar
I think this is like highlighted here
and in the other case where it is windy
right so there is like the strong flow
of air flowing from some direction so
your football is like difficult to
control and you also find difficult to
run in the fields so in the case when it
is summer and there is a strong wind you
can definitely see there is this high
probability of the match being played or
not whether you want to play the
football or not but until the previous
case the probabilities here was like
when it was sunny the probabilities were
quite low in the case here on the way
when it is like windy in the summers I
think it is not a problem people were
able to play that's why the probability
of playing is quite high in this circle
is quite big at the same time when it is
monsoon and even if there is no wind you
can see the probability is quite high
but the probability goes like quite low
when there is a strong wind so we can
get to know that in in some monsoon the
wind is like quite strong compared to
the summers right so that's why this
contrasting difference in the
probability of play similar is the case
with the winters but I think the winter
also doesn't have much strong winds so
we were still able to like play that's
why you see the probability lie quite
high ok so what if I want to combine
these two things together sunny as well
as windy the combinations which comes
out from sunny and windy so even for
that we are given some data set so the
data that we could right now have got
contains in one column something that we
call Outlook right and that outlook has
like various values to it and we have
some places where it is sunny and windy
there are some places when it is sunny
but not so windy and so on different
combinations with those combinations we
have recorded the weather the match was
played or not and from that we
calculated these probabilities like
these right so we want to put all these
ideas in a more formal way right and
what is really happening here is when
you have one attribute with you let's
say sunny here right
and we augment that attributes
information with one more attribute
let's say call windy here right so which
means by knowing a particular value of
an attribute there is a certain
difference in the probabilities so for
instance here if I say a particular
weather condition where it was not sunny
and there was wind the probability is
quite high right but if I like contrast
this to the other case then in the same
summers I don't know they know any
information about wind but all I know is
it was sunny so the probability of
playing that particular game of football
was quite low but the moment there is
one more information added like there is
no Sun and there is wind the
probabilities kind of slightly starts to
change it drifts in some different way
like you can see here the Sun is like
there is no Sun and the wind is like
present the probabilities are like quite
high that the match will be played so
nay Bayes tries to understand such
interaction so given a data set of
weather am a match was played along with
many attributes of the weather condition
a prior knowledge of a particular
attribute does influence your
probabilities right the probability can
either go down or go up so formally
putting this entire idea into a very
elegant formulation which is called
Bayes theorem which puts in all of these
information in one neat form so in order
to explain this in better terms if I
want to understand a probability of
whether a match will be played or not
that's represented by this symbol called
C so this is something that we call
class right and the class is either the
match is going to be played or not and
thats kind of also relates to the thing
which we discussed around the
classification problems right and if I
want to calculate this probability which
we call throw posterior probability
which is whether the match going to be
played or not given I have all these
attributes with me the attributes like
whether it is sunny or not whether it is
windy or not and so on
on the right hand side this base theorem
puts things be pretty elegantly on the
numerator you see something called
likelihood right so if I say what is the
probability of the match like to be
played today given I have these sort of
weather conditions
so let's first particularly take given
it is summer right in the likelihood I
might have the other information with me
what is the probability of the
particular weather to be or the weather
condition to be summer given I am kind
of able to play here so in order to show
you some calculation on this let's
discuss about this frequency table which
will then translate to the formulation
of that name based theorem so here this
frequency table tried to capture certain
information in in our data so the first
information is when it is sunny so there
are like two possible cases here yes or
no it is sunny or it is not and on the
column we have whether the match was
played or not so in total if you count
all these three plus four plus one plus
six there are 14 observations from the
historical data we have got and out of
these 14 observations there were three
times when the weather was sunny and the
match was played right but in contrast
to this there were four instances when
it was sunny and the match was not
played so obviously the other side is as
a high probability than the yes once and
similarly when the match was not played
sorry did move with the weather was not
sunny there were six instances when the
match was actually played so which is
like a strong indication versus this one
here which means when the outlook is not
too hot not too sunny the match was like
most of the time played right so I can
do the same for windy as well and the
other can either variable that we have
is the season so when it was summer I
could say the chances are like equally
likely to for the match to be played or
not played but man it was monsoon
interestingly there was no match scab
got canceled so every
match was played an inventor also the
chances are like equally likely whether
the match was played or not so with this
sort of frequency table it's easy for us
to know visualize what is these sort of
probabilities associated with each of
these cases of having a win or if it is
going to be sunny or not or even having
the information about season of 40 play
right so this frequency table be going
to likewise extend and we're going to
come out with something called a
likelihood table so these are counts
right frequency is what we call so with
these frequencies we can translate this
to a likelihood table which consists of
different probability measures right I'm
going to explain how this is calculated
so for instance let's take the variable
season right we can do this for the
other two variables as well so the glp-1
which we are going to take here is four
seasons and what we have got here is
like this you have a match either played
or not so if I like count all the
matches which were played which is this
column irrespective of which season it
is so that's three plus four plus two
here right there were like total nine
matches which was played out of the 14
observations given to us and then there
are these two plus four so two plus
three which is five matches or plays
where like it was not played
irrespective of which season it was so
if I want to translate this to a
likelihood table all I need to do is as
you know to translate such a frequency
into a probability measure in the
denominator we have the total possible
outcomes and in the numerator the
outcomes which favors our event and our
event here is whether the match was
played or not so there were like three
instances when the match was played
they divided by the total nine instances
of the match was played in overall so
these three instances belongs to D
season summer right so there were three
times when the match was
out of all the matches being played the
the observations which was given to us
so 3 by 9 is in particularly this core
for saying the probability of the season
being summer given the match was played
right so this symbol that you see here
the pipe kind of symbol it is read as
given probability of the season being
some are given the match was played so
this can be calculated
pretty easily like this 0.33 and this is
what our likelihood star so if I like go
back a couple of slides here and tell
you this particular Bayes theorem so
this portion or in the numerator is what
we called likelihood and and this
information is known to us and what we
are really interested in calculating is
this posterior probability which is
going to come up after we collect all
the data points ok so the next one the
portion which is like the probability of
a particular season to be summer or the
probability of our class which is
nothing but whether the match is going
to be played or not is what we call the
prior probabilities we know that right
from the data so in this case there were
5 observations when the the kind of
attribute value is the attribute
season's value is summer so we are able
to get the probability by like by this 5
by 14 and remember these are class
probabilities so the 14 in the
denominator is because of the fact that
we have 14 observations collected from
all these matches so which is 0.36 here
and similarly here in this probability
of the class which is whether the match
will be played or not it is an overall
level so 9 by 14 is that probability so
this is the probability of your
attribute which is the season and this
is the probability of your class which
is whether the match will be played or
not and both of these probabilities are
something that we know by the name of
prior probabilities these are known in
Prior even before we do the sort of
calculation from the data that obviously
throw all of these are coming from the
data we have so if we put all of these
into our base theorem
this is what we get the likelihood
multiplied by the probability of your
class and in the denominator you have
the prior probability of your attribute
which is nothing but yet it will season
here and in particular the value which
you get is for the summer season so this
becomes in very short form the
likelihood the probability of the match
being played given the season here is
summer so similarly if you do this for
all the other attributes as well so the
X that we have chosen and this point is
only for the season right but I can do
it for the other attributes or as well
which we have got the other attributes
like the sunny which is yes or no or
divinity which is again yes or no so if
I put all of these together like in this
form the probability of winter given yes
the probability of summer is equal to no
given essence and for the rest of the
attributes as well so remember all of
these are attributes and it is like
taking a specific value so in this case
I am trying to calculate the probability
of the match being played and there are
in total three attributes so that's why
you see here the value of season being
winter the value of the attribute
signing being know and the value of the
attribute windy being yes and this is
the prior probability of the class so in
overall this is a probability of whether
the match will be played or not given
all these attributes in the denominator
likewise we get the prior probabilities
of our attributes which is the season
sunny and the windy these are the three
attributes right if you put all these
values inside the this probability this
is what we get the probability of the
match being played given the season is
winter it is not sunny and it is windy
right these are the attributes we have
got and the probability is 0.62 three
six two two three similarly if I want to
get the other side of for this story
I would simply because there are only
two classes I can simply do 1 minus of
0.62 2/3 which will give me
probability of the match not being
played which is obviously less than this
so in very simple terms like you can see
that we have only come out with
probability here right so where is this
classification that we were initially
talking about and in very general terms
people normally fix a threshold of 0.5
so if I see the probability of the match
being played greater than 0.5 I can say
my decision here would be to play right
know otherwise so this threshold can be
changed also in typical all the cases
these probabilities are fixed at a level
of 0.5 anything which is greater than
that we accept that anything below the
the probability is like not accepted all
right yes so I'm happy you can play the
match right so this was a very simple
dummy example so let's build a really
strong use case on this with respect to
a real-world problem all right and these
are some typical examples that you can
see face recognition detecting email
spams and even categorizing the nooses
all the places where you can have some
sort of classification algorithm you can
do that you can use name base and like a
typical picture from the Oscars and you
can see there are like all happy
sentiment so this is like on another
step also that you can do which is again
a classification type of a problem
whether the expressions in the faces are
positive negative or neutral right okay
some more examples medical diagnosis so
for example if you are given a medical
history of per patient you can also
classify whether the patient is going to
be soon diagnosed with that disease or
not so for example diabetes if I have
all the carrier readings or like all the
medical tests for diabetes of a
particular patient I can classify the
patient he or she go is going to be
diagnosed with diabetes in one month
sign right these are again the
classification problem once again B
digit recognition so if you have a
handwriting
data set which is of the numbers 0 to 1
and somebody has returned this so these
are like different ways of writing 0 1 2
and so on
so given this as an input and I learn of
this data and classify given another new
data set to me can I classify the digits
either into 0 1 2 or 9 like these are
the numbers that I want to classify my
data into then I like to do that yes so
this is again a typical classification
problem examples so I'm not saying that
for all these problems named Bayes will
be the best algorithm to do but
neighbors can also be one of the
algorithms which can be used for
classifying right but obviously machine
learning is also about experimenting
with all these sort of algorithms we
have got and all these algorithms that
comes under classified solves the
classification problem has its own
benefits so you should be better written
from the in terms of doing any sort of
different experiments with your data and
then figure out which one does the best
ok so we're going to take this one
real-world example of predicting
employees salaries right so a typical
use case could be if you are a marketing
agency right or let's say some form who
is analyzing salaries of the employees
right from various forms and from
various backgrounds and so on so there
are hundreds of things which decides the
salary of a person right so if you have
your hands on certain information from a
market survey or from employees survey
across companies you might be able to
get some picture out of it and let's say
you decide as a marketing agency you're
such kind of reports are being produced
and many HR analytics companies does
this as well where they analyze the
current market trends and they search
try to come out with predictive models
around that and and the predictive
models could be given some skill sets
that you have what is the expected
salary that you might draw from job that
this skill set will give you so there
are many such predictive analytics kind
of use cases which is quite common in
the HR analytics so let's see if we have
some data of the employee
can we predict exactly what is going to
be the salary so there are like two ways
this problem can go in one way where we
can think of this as a regression
problem which means very precisely
predicting the value of the salary or
this can also be categorized like if I
am saying anything greater than $50,000
$50,000 is a high salary and anything
below $50,000 is a low salary so the two
classes in which I want to predict or
like put my employees into will be
either low or high keep in mind that
this prediction algorithm I am NOT
building for my own set of employees if
I am running a company I'm not building
it for my employees I know their
salaries right this might be the case
for some HR analytics come who is in
general wants to understand what's the
send trend like of so if I show you this
data this is what typically it contains
it has information like age of the
employee what is the type of the
employee some information on the
education like education definitely
plays a big role in deciding your
salaries and then general informations
like the marital status or whether the
employee has any sort of relationship or
a family right and even the race of the
employee so whether all of these are
important or not is a matter of
continuous experimentation but these are
the sort of information we have of any
employee in our database right so if
this HR analytics company wants to do
something with this they might be able
to use all of these things they are like
some even some of the variables like
capital gain sort of very rare to find
for a particular employee but yep this
data set has it submit which is
typically the income from the various
investment sources the person has the
employee has right other than obviously
this allottee okay so if I have this
data set I think there are certain
variables which are very clearly telling
me are useful and there are certain
variables which I believe can be removed
so let's maybe get rid of some of the
information like the marital status or
the employees relationship status or the
race of the employee
your gender capital gain and capital
loss so there are like certain biases as
well in this particular removal of the
field there are certain strong
assumptions but I am NOT any way of
getting into those sort of debates and
discussions which quite happen so in the
industry is regularly particularly with
the attributes like race and gender
right but I am NOT going to go into
those discussions but for but a very
safe assumption let's assume that a
Fairplay kind of an environment in
industry would normally don't want to
use any of these variables right so we
have a question here which says however
does one decide which variables to use
by pretty common question here so the
first judgment that anybody do on which
variables are important is based on
domain right so you know it for me to
remove these fields from the data point
there is a particular emphasis given on
the meaning of these variables so as an
HR person might be able to see these
variables and straightaway reject that
we should not be basing the employees
salary based on these variables like
doesn't matter if the employer is
married unmarried or not in a
relationship or if employees of one race
or a particular gender and so on he
should not be taken into consideration
keeping in ethics in mind keeping in the
industry standards in in mind and
keeping a and giving a fair play for
everybody to get get an equal
opportunity so if I like consider in
those lines or talk in those lines
domain knowledge else's to see which
variables are important and not so
that's first and if you have a like
enormous amount of variables with you
maybe let's say in millions term so if I
am doing an image classification an
image classification the pixel values
are black many right and I can create a
large matrix of data points numeric data
points so there are traditional methods
like principal component analysis and
many dimensionality reduction approaches
which helps you to represent a large
data set right which contains many
columns in a lower dimension so when I
say lower dimension if I you if you have
a data
thousand columns can I represent this
thousand columns in ten columns right so
this is called dimensionality reduction
and the way we do this normally is these
ten columns are a representative of all
the thousand columns we had in the
beginning right some linear combination
of the thousand columns and there are
like standard techniques if you would
like to go and read more about it called
one of the very popular one is called
principle component analysis which helps
us in doing this dimensionality
reduction so in short there are two ways
to do that one is domain knowledge the
second is some statistical methods or
mathematical methods like PCA all right
okay so now let's prepare this data for
training our model so the typical
machine learning process flow of coming
out with a model the first thing that we
do is to divide the data set into two
parts right the one which is used for
training and we will like reserve a
portion of the data set for validating
or what we call testing so imagine like
this you build your model on if let's
say if you have 10,000 observations with
you 10,000 rows of employees you use
let's assume 70% of that data may be
something like 7,000 data points so out
for this and which are like obviously
randomly selected so out of these 10,000
7,000 employees you choose and train
your model and when you are done with
your model you need to also see how good
is this model in terms of deploying it
in real world so I'm going to reserve
these three thousand data points for the
later point of time where I will
evaluate the goodness of this model
right so I'm going to describe that a
bit more when we are going to see the
demonstration this is like a standard
flow that I am walking you through first
right so once we divide this the next
immediate step that we might have is
obviously to build the model right so
here we are going to use a particular
function coming from our library I am
going to describe that during the
demonstration and build our model right
and then comes the other step after we
are done with the model building
we are going to see the output and see
if the output is like good or bad or
like in whichever way it is and then
we'll also see or what we call
devaluation once we are done with the
model building exercise we should be
able to find out if my standard metrics
on which I evaluate the models does that
tell me everything is good and I can
like proceed with this model being
deployed in a real world or like
something I need to do about improving
the accuracies right I am going to talk
about that as well so there are certain
standard ways we can improve like
including Laplace correction or doing
some normalization so I might not focus
on that part but I will be focusing on
building this model first and trying to
understand the various accuracy measures
right and then comes the model
validation which is again based on our
test data set right which we reserved in
the beginning okay so we're going to
discuss about the confusion matrix what
do you mean by sensitivity specificity
and all that different measures that it
gives me and obviously if everything
goes well we say that the model has some
very good accuracy and we can trust on
this model maybe at least 80% of the
time or maybe 90% of the time depending
on what is the accuracy and the most
important of the most sought-out
sort of the tap in the center and model
building process is the prediction where
we are like actually looking at a new
data set and saying yes this is what the
predictions are right so these are the
typical steps involved starting from
data acquisition to predicting the
values from the model that we have built
so now I'm going to get to a
demonstration in our so I have here some
data points
right so this is a dataset on the
employees data we just now looked at I'm
going to show you how this data is like
let's get the column names so these are
the columns that we saw in the
presentation right so age of the
employee the employee type education and
so on right and as I told you in the
beginning when we are building this
model there are two ways we can build
this model as a regression problem or as
a classification problem so we decide to
build this as a classification problem
because we are using a base and in order
to do that I am going to convert this
employee salary into a categorical
variable by saying a condition wherever
in the employee salary variable I have a
value of like greater than 50k
I am going to mark that as high and the
rest of it are going to be low like only
two class problem so that's why I'm
using a if-else here so let's create
this new variable which is called sa-l
and attach this to our main data frame
so now if I show you the data frame or
the data set this is how it looks like
the age of the employee and all that
details followed by the last two columns
where we have converted or like returned
this particular value which is so here
we don't have a crisp value we have we
have a value which says the salary is
less than 50k and this is like a good
ethical practice as well when you get a
salary of some person always anonymize
it or always like transform it in a way
so that the exact values are not known
so if it is less than 50000 K I don't
know whether it is one thousand two
thousand four 40,000 or like 50 thousand
it's just a range and in the last column
we have the variable that we just
created by using these values okay so
let me like just get rid of there are 16
variables here so I'm going to get rid
of the one
column that we just transformed into
something else so the two column which
was saying less than 50 K and all that
it is removed and I will keep only one
column here which is the low and high
right so this is the sort of data set on
which I want to train my body and
remember in the classification problem
the training happens with respect to the
class so when I read this one row it
tells me if you are working for a state
government and you have a bachelor's
degree with you so it's not too high you
don't have a master's you don't have a
PhD like let me ignore these variables
like marital status and all and there
are some capital gain values also the
number of workers in week that you are
like working for so if you have like
these values it seems like the salaries
are not so high for that but maybe if I
look at some cases where the salary is
high what is the sort of difference here
I'm just simply exploring the data to
generate some intuition for myself again
this is typically in happens in every
other data set also that you analyze you
need to create a intuition first before
you start to build a model so in this
record where I can see a person is
working for a private maybe a private
job maybe it could be an industry
company he or she has a master's and
like some more information and it seems
like the salary is high so I think some
indication here saying if you have a
really good degree with you which is at
least at a master's level there are like
good chances you will be paid high right
at least one inference which I can get
but anyway we are going to use many of
these variables to build our model here
so this is like just one variable that I
will explode and understood something
about it okay so now let's let's do this
sort of splitting the data into two
parts so in the slide we talked about
this process right we are given a data
set I want to preserve a set of
observations for the final validation
part right so to build my data set here
to build my training the algorithm here
I am using a function called sample
which simply
it takes in all the observations I have
got here the number of data points that
I have is thousand observation so there
are thousand employees data that I've
got so I'm going to randomly select some
70% of these employees to be part of my
training data right how I am going to do
that is to use by using a function
called sample which is randomly select
so the records based on a probability
right and the probability says here like
there are two values to this probability
which is 0.7 and 0.3 so what it means is
there is a 70% of the times you want to
select one and 30% of the time you
should select - or any other value so
what is this means here in the top first
let me separately run this for you and
things will be clear so if I take this
example here and if I run this
particularly you can see that it will
generate lots of ones and twos right
what it has done it has done by taking
this input which says in the range of
one to two generate me many such
integers like either one or two I could
also have like return one to ten then it
will generate ten random ten random
numbers so for instance if I do sample
of one to ten without specifying any
other parameter it will give me ten
random numbers between one to ten right
so here I am saying generate only
between one to two and do it like as
many n number of times as you have in my
rows so what I am doing here is in my
data set we have thousand rows right so
n rows give me thousand as a number
there and I am going to generate between
one to two thousand times these values
like one or two and the probability to
do that is being assigned as 0.7 and 0.3
which means 70% of the time generate me
one and thirty percent of the times give
me two so all I have to do then finally
is based on in which India
so I have one or two I am going to go to
my data set and say wherever this
sampling function has generated one
assign that to EMP a-train as my data
frame name so let me run this and then
this and then this right if I look at
now the number of rows in my EMP train
data all right you see here 688 like
pretty good number here in terms of what
we like were initially trying to do so
very close to 700 right because these
are probabilities you will not find
exactly 70% but very close to that so if
I do the same with test
you see here 300 types so that's how the
sampling works so you can change these
probabilities and based on that it will
maybe generate some other proportion as
well okay so I am done here with
splitting this file or putting the
observations into two parts so all I
have to do is simply use the library or
package called II one zero seven one
quite useful package it has many machine
learning models implemented in it and
one of these machine learning models is
may bass and also the carrot function
which I might be using for drawing the
final evaluation matrix which I am going
to discuss in a while so here what I am
saying is there is like a formula type
of interface to this function which
means this is my data right call EMP
train we know that we have split that
one in the earlier step and what it says
is use all the attributes given in my
data set to generate this model and
train it on the variable cell which is
nothing but high or low right so putting
this dot here means use all the
independent variables and alternative to
this could also be like I am selecting a
particular variable so for instance so I
am going to select a particular I can
run this model and this will be like
perfectly fine but because we decided to
run this model for only certain
variables so what I am going to do is
I'm going to print out all the column
names of the employee
training data so I'm going to select all
these and only keep the one which I am
interested in so let's say age is an
important one
okay so we'll keep age employee type
also seems to be important we'll keep
that as well the serial number might not
be required education like very
important I am going to keep that let's
say education category also will keep we
are not interested in marital status but
we'll keep the occupation right here and
the relationship of the employees are
also not important trace is also not
important and let's also take out the
capital gain and all these values and
keep the working are in weeks and the
country of residence right so what I am
doing here is I am like simply adding
the displace symbol
don't worry this is not literal addition
in this interface in this particular
model function like name base the place
simply means the combination of all
these variables like these are the
variables that I am going to use to
build the in a base model and it is
going to learn bases the label which is
high or low it seems good so let's run
this model okay that's quite fast
because of the small number of
observations we have let's put the
summary of this entire model okay so
like that's quite a big summary okay let
me read some of these for you so that
you can understand what it is doing
these are actually the likelihood tables
you remember we in this slide we were
like finding out what was the frequency
table and the likelihood table so in
this case all of these values that is
being printed out in these after this
particular header which is conditional
probabilities all these are likely holds
and the one which is this one is the UH
priori probabilities or like simply
saying prior probabilities of the class
you remember the class has only two
values low and high and this is that
P 6% of your employees are low having a
low salary so that's our prior
probability right in the given data set
of our 76% of our employees has low
salary and the rest is 0.23 and this the
one which is like starting from
conditional probability is all the
likelihood tables for each of the
attributes right so let me tell you how
to read this in the first case we have
this attribute called age of the
employee right it says the first column
here there is a small matrix here right
so this means because age of the
employee is a numerical value so for
numerical values like calculating the
probabilities is not like possible so
what we instead do is we calculate the
mean and a standard deviation so what it
means is if I read this value on an
average employees who have a age of 43
or like 44 years has the class in high
salary which is like making sense also
for me right because as you grow in age
you become like more experienced in your
domain and you start to draw high salary
whereas the average age for people who
are earning low salary is 36 years right
so these are averages keep in mind and
because I am putting here the averages I
should also be talking about the
standard deviation so that's what is
given in the second column and standard
deviation as you might know gives us the
understanding of how much variability is
present in the data
and here the variability is like in
terms of the age itself which in terms
means if I have the average of high
earning employees the average age of
those employees are 43.9 I can say plus
or minus 9.8 oh that's the sort of range
right so there are many cases where you
will see the variabilities are quite
high even young age employees are
running very high because of their
background in terms of academics or
something else right maybe they're clear
familywize pretty good and so many other
reasons so there is always a variability
in the data so it's important when you
talk about averages you also include the
standard deviation there similarly the
case with the low category as well look
at low salary category employees okay so
this same conditional probability table
kind of changes in its tone when we have
a categorical variable like the employee
status type like employees state type so
here we have these states like the
employee works for federal government or
local government or has never worked
works for private self-employed and so
on like there are many our like without
pay so these are the probabilities
associated with these conditional
probabilities and if you remember the
way to read this is the probability of
the employee working for federal
government and is like earning high
salary right these are conditional
probability so remember that so if I
like give you some contrasting examples
maybe this will be easy to understand
then most of it are like very close but
if I maybe take this one where I can see
the probability of employee working for
federal government the salary of salary
being high as like a higher probability
then is already being low but whereas
the same thing in private sector like
are the probabilities are very close so
maybe both this type of people are their
high salaries and the low salary people
so this gives me an indication rather
like what decides a high salary maybe I
can think of this in better terms if I
see the education so if this is my
education like 10th grade 11th grade
till masters or doctors as well you can
see the stock difference in the
probability measures people who have a
Doctorate degree the probability the
conditional probability there of having
a high salary is like quite high
compared to a doctorate candidate like
getting a very low salary there so this
difference is quite distinct the moment
I see this I can like definitely say
having a higher degree like give us this
indication that these allergies are also
going to be high but there would be
definitely exceptions
right so this is how you read this model
and then finally there are like many
more other columns as well so this is
the sort of summary of the model but the
one which I am interested in is what is
the prediction is this a really good one
or a bad one right so that's what we
want to understand so remember here the
prediction I am NOT going to do on the
training data set though we also look at
the training data set predictions as
well but in particular here I am looking
at the testing data set prediction so
let me run this okay so I am
using a function called predict the
model name is EMP underscore NV which we
created here and I am giving now a new
data set to this predict function right
which is the testing data which we
initially reserved for the accuracy
computation and all that so let me run
this so I am going to now evaluate how
good my model is by drawing something
called confusion matrix let me give you
what it is okay all right not bad at all
I think the accuracy is quite good
eighty-three point zero one percent so
that simply means if I take this matrix
of high and low like look at this
particular table a small table right so
the way to read that is in your original
data set the values for high and the
prediction that you have done exactly
matches so which means B 48 here are the
number of employees 50 48 are the number
of records or employees who has a D high
salary again the actual data set and
your model also predicts the same right
similarly these 211 observations your
actual data contained the value low and
your model also predicts the same
whereas these are the wrong predictions
the 30 and the 23 here so what it means
your predictions are here may be high
but whereas in the actual data the
predictions are low and the vice-versa
for this one as well so the off diagonal
elements are something which is not good
for me that's the sort of error so how
this 83 point 0 1 is calculated if I
look at this out of how many out of let
me like some this first what is the
total number of observations 48 plus 30
plus 23 plus 2 11 3 have 12 like this is
the number we saw for the testing data
set that we created at the beginning so
the accuracy here is based on the
diagonal elements so that means 48 plus
211 are the correct predictions and the
total observations that I have is 3 1 2
and here you go so this is the accuracy
measure that we have got which is like
83% here right and there are some more
information as well in this sort of
output like confidence interval so when
I am putting accuracy so it is also
possible for me to calculate like if
this accuracy that I am talking about is
going to be having certain marginal
errors right so when I say confidence
interval my confidence interval of 95%
which means I am 95% confident that the
accuracies are going to be somewhere
between zero point seven eight to
somewhere between zero point eight seven
for this instance of data I am able to
only get 83 but in general if you do
draw many samples from this given data
set of mind I can calculate something
called confidence interval and that's
good this confidence interval can have
different percentages right so there is
only 5% chances that my accuracy will
lie beyond the boundary which is
mentioned here right I cannot like get
there are only five percent chances that
I can get an accuracy of more than 87%
and the same for the boundary which is
less than that which is less than 78% so
at least so 78% of the accuracy I will
I'm I'm going to anyway get 95 percent
of the times right that's what this
gives you and then comes such a certain
things like p-value quite popular
measure if this p-value is less than
0.05 things are good
that means the sort of your accuracy is
very well coming out for the data and if
it is greater than 0.05 well it is not
that good right and there is some more
stats here some more evaluation metrics
I should call like the copper statistic
right
so this data fix is quite like
interesting if I want to talk about it
for a while there is always a process
that you can follow so this is your
accuracy which you have got from the
predicted model right but what if I
create a very simple model which says by
doing a random selection for example I
have only two classes possible here
either high or low
so what if I do a simple cord tossing of
a coin and if there comes a head I'll
say the employees sell a predicted
salary is high if if there comes a tail
the other wise right so if you do this
tossing of coin you know that 50% of the
time you are going to get head and 50%
of the time you are going to get tail so
if that means my accuracy of the model
even if I do this route force or like
simple knave approach I am going to get
a 50% accuracy or even if I like call a
chimp and ask the chimp to select either
a high or low the chances are that the
model that is created by the chimp is
going to be 50% accurate right so how
good is my accuracy with respect to that
accuracy of a random sort of a process
or a model that is created by a chimp
right that's what is like given by this
couple statistics and if you like want
to understand how that works so you can
like see here this is your total
accuracy and this is the sort of
accuracy that you get from a random Marv
stuff right so if this difference in the
numerator is not going to be low if it
is going to be low that means the couple
stats is not like very good and that can
only happen only when your random
accuracy and the total accuracy are very
close to each other but more this
difference higher will be the value of
your copper statistic and the better the
model right this is how it it works
and then there are these variables like
sensitivity and specificity I'll talk
about this in for a while so how does
these two important measures are like
consider normally is in any model that
you build keeping a real world context
in mind you have to always make sure
that both your wrong classification like
when there is a employee who has high
salary in the actual data but you are
predicting to be low and the other case
both of these wrong prediction cases
needs to be balanced I'll tell you why
it is important for instance if you are
building a model for a health care data
set wherein the task that is given to
you is to predict whether the patient is
going to be diagnosed with diabetes or
not right so now imagine what wrong can
happen if you predict a person who is
completely healthy but your model
predicts that this person is going to be
getting diagnosed with diabetes or maybe
some even more dangerous diseases like
cancer
so in that case kind of agony you are
going to put to that patient because
your model has done that it's going to
be like really really risky business for
you the other case is also not good
right whether you when your patient is
actually going to be diagnosed with a
particular disease but your models is it
that's not the case so if you want to
give or attach this to a monetary value
let's say you do this for a banking
system the same model can also be built
for a bank right where where a given
transaction needs to be identified as a
fraudulent transaction or a genuine
transaction if a fraudulent transaction
is tagged as a genuine transaction you
are like going to lose a lot of money
there the other way round is also not
good so sensitivity and specificity
particularly handles these two scenarios
well right if you are able to achieve a
balance between sensitivity and
specificity at the same time keeping the
accuracy also intact then you can be
certain that your model is completely
fine or completely good right but if
there is not a good balance between
sensitivity and specificity then that
means the invoice in either one
direction your model is like highly
skewed we
is like a danger point to be in right so
keep a balance of the sensitivity
specificity and here I can see that it's
not so different in terms of its values
at the same time I can see the
accuracies are not bad at all so 83% is
like quite good number right so I think
the one thing which we have not seen is
what is like the final prediction looks
like so this is how it looks like right
so high is or low but instead if you
want to let's say because nee base is
about probabilities I can also get you
the probability score instead of gain
getting the class name
so if I use the argument called type
equal to raw you will see that the
output of this prediction is actually
probability measures so I'll to read
this instead of class this is like the
probability of this employee having a
high salary is like zero point seven one
zero six versus the probability of the
employee getting a low salary is 0.28 so
whichever is like higher you can take
your classification in that direction
alright so that's it from me for on this
nape is or algorithm thanks for
listening I hope you enjoyed listening
to this video please be kind enough to
like it and you can comment any of your
doubts and queries and we will reply to
them at the earliest to look out for
more videos in our playlist and
subscribe to our red rig a channel to
learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>