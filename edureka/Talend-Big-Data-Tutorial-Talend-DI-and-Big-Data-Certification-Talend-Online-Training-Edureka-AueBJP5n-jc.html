<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talend Big Data Tutorial | Talend DI and Big Data Certification | Talend Online Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Talend Big Data Tutorial | Talend DI and Big Data Certification | Talend Online Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talend Big Data Tutorial | Talend DI and Big Data Certification | Talend Online Training | Edureka</b></h2><h5 class="post__date">2018-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AueBJP5n-jc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is shitted on behalf
of Eddie Rica thanks for watching our
previous videos on talent data
integration thanks for your responses as
well so today we are going to see Talent
Big Data tutorial a very interesting
topic Talon with Big Data a welcome one
and all for this webinar so today's
talent with Big Data agenda is like this
we'll first understand on a very big
picture what is Big Data and how does
talent go along with big data and tos
that is Talent open studio for a Big
Data how do we install it whether it is
similar to di or any differences there
and we will tell you how to install it
and also what are the big data component
the family of Big Data components in
Tallin how does they help us and we'll
also show you one talent demo job which
will be working on Big Data now let's
understand what is big data big data
refers to the voluminous and complex
datasets generated on the daily basis so
we all use Facebook Instagram you know
how many millions of users are there and
all of us post some data we have
hashtags all the stuff and if you look
at the medical domain you know banking
transactions all of them are very huge
data to handle so these data sets can be
structured semi-structured or
unstructured in nature so we need to
have a tool which would help us in
managing these kind of data these data
generally is useless and hold no meaning
unless mind or analyze properly so if I
give you a web blog of Flipkart or
Amazon
users you should tell me what is his
interest what is going to buy all those
stuff right so we have to analyze it in
a proper way until or unless that data
is useless storing and processing this
data is really difficult as we all know
a DBMS is not so good to handle big data
especially for the traditional data
processing software so this is the
overall big picture of big data so why
do we call any data as big data or how
do we categorize it so if you tell me an
example and if you ask me tell me
whether this kind of data is big data or
not so all of them will come to this
point where we see Phi
we saw big data so what are these five
e's first one is volume variety velocity
value and veracity so volume is like
it's increasing okay day by day so today
we have so much of users tomorrow we
have new users and they are also going
to post something on the social website
so our data is increasing and what is
the variety all of us will not post it
in some way or all customers will not
give you or no source data in one of the
way it will be a music file it will be a
JSON file can be a video it can be an
email anything and what is velocity bank
transactions so today I being one
customer was one bank I can do n number
of transaction so similarly other users
they also can access the system anytime
they want so the data generated velocity
is very high the frequency of new data
coming in so we all want real-time data
right so that is what we are pointing to
so data is changing every second and
what is the value of the data so that
value of the data is depending on your
business decisions that is what you have
to analyze from big data so it all is it
all constitute to a very good value for
analyzing the data so what is that data
important for how much valuable is it
and then what is veracity so you take
data and you integrate the data from
different job for the location so some
of the columns will be null which will
be mandatory for you to take some
reports few of them will be inconsistent
or wrong data so when it comes to clean
up the data with huge amount of data
which you are dealing with its a almost
impossible thing so that is also
constitute to your velocity so if your
data is falling under all of these
categories we categorize them as big
data right so hope you have got a good
understanding on how a data can be
categorized as big data
so our 5yz will help you to understand
so any of this is available in your data
the data is very high you realize
variety your data is changing very fast
all the stuff then you can categorize
them and big data right now coming to
Big Data technologies that is Apache
Hadoop so what is Hadoop so how do place
a framework that allows you to store and
process large data sets in
parallel and distribution fashion so we
all have heard about divide and rule
strategy right so we have a piece of
work so we all can share that work and
do it in parallel and get a solution
quickly so that is the you know big
picture of how do frame work so
previously why this idea was not
implemented at that point of time
hardware was very costly so even though
or you know you had this idea in mind
that is divided rule there was not a
framework present to do it because it
demanded the hardware which was costly
and now in today's would we get
commodity hardware which is very cheap
so time is money now we need the data
and the report faster so they are able
to invest on hardware so that is how
Hadoop came into picture and that is all
we are having more number of hard ways
we can parallely process the data and
get the results faster right that is a
general understanding of it so when we
come to HDFS what is a staff is Hadoop
distributed file system it is the file
management system of the Hadoop platform
which is used to store data across
multiple servers in a cluster so group
of commodity hard ways but together is a
cluster right so now I have different
systems I can divide a work and give a
piece of work to each of them and then
collect back the result so who is doing
this dividing and collecting back work
then comes the topic MapReduce what is
MapReduce it is a data processing
framework of Hadoop and mainly has two
functions mappers and reducers so
whichever I was telling you we can
assign piece of work to different
service that is mappers and once server
has completed that work we can get back
the results that is reducers which run
as tasks on various nodes in a cluster
and thus your parallel processing
happens and you get the result in a
quicker time right so this is hurroo
that Stephan's and MapReduce
understanding so before we go into the
architecture like so what are as I was
telling you there are different servers
and somebody called map will be dividing
the work and somebody call reduce will
be taking back the results all the stuff
so you would have a little bit of
confusion how would
happening internally right so that is
why we are giving you a glimpse of the
has TFS architecture as you can see here
I was telling you about different
servers right so how they are
representing a ice-like
they can be Dara nodes okay so each data
node will have locks which can hold the
data right and at any point of time will
have a replication of data why we are
introducing commodity Hardware they they
may be you know pol to fail so we should
have our application of the data so that
our data is available at any point of
time okay and this is fault tolerant
method at any point of time one
replication will be available that is
how this framework is designed right so
will have data nodes in one of the racks
like how I said we'll have different
servers we can have series of servers in
one rack another series of servers in
another right right so all these things
are called data nodes so who will divide
the work and how will take back the work
so we should we should have somebody who
is managing all the stuff right so who
manages all this is the name node so
name node is master here which will
speak to all your data nodes and it will
manage the tasks and it this kind of an
interface between HDFS and your program
okay so that is what you have to
understand what is name node and what is
data node and there is a fault-tolerant
method which will make my data
availability at any point of time
okay so name what is the master data
nodes are the one which actually execute
the tasks and each data node will have
blocks which will store the data right
and why we are telling about name node
and data nodes is
so in our ETL job which we'll be doing
today will have to configure these
values so you should know whenever you
are configuring those values you should
know what actually you're configuring so
that is why we have introduced the name
node and data node concepts here so now
let's try to understand what is the
resource manager so there'll be
something called yarn which is our
resource manager which will be managing
all the node managers okay what is this
node managers as we saw in our previous
architecture slide we had one name node
and five data nodes okay
so this is one example of it so we can
have different name nodes and different
data nodes I in the cluster right so in
that case we'll be having more than one
name node so node managers are the one
which will manage all the name loads
okay and when we have different name
nodes so we need different node managers
so that is what is represented over here
so when we have different node managers
obviously we need someone who will
manage all these node managers which is
called a node resource manager
okay so resource manager is like it
takes all the requests and it will
maintain first job will go to this name
nor the second job will go to this name
node so that is how it managers so
research manager is the one which will
manage all your node managers and node
managers intern manage your name nodes
and data roots right now let's
understand what are the Big Data
technologies available
so first we'll speak about Apache hi so
Apache hi is a data warehouse system
built upon the Hadoop system okay we
have Hadoop distribution file system
upon that we have an interface where we
can speak to so that it goes back and
fetches the data from history FS right
so it is used for analyzing structured
and semi-structured data so what what is
the you know a a selling point of hive
is way behind when the data came into
picture this need for somebody who
writes these mappers and reducers jobs
okay that was in Java but at that point
of time there were so many people who
had our database experience but not the
big data experience they only knew how
to write SQL queries okay but when
industries wanted to migrate to Big Data
they wanted an interface wherein if you
write an SQL query as you write for a
DBMS it would convert that into map and
reduce programs and it would fetch back
the details for you as you require right
so that is where I came into picture hi
lets you perform query similar to SQL
through high query language that is hql
so Apache hive supports DDL DML and
user-defined function just like a ver
store procedures
so what high made was it enabled people
who
databases who knew SQL queries to just
continue their job as they were doing
with our DBMS but the same query would
work on big data so how is is this so
that is why I've become famous so this
is all about a big pitcher of hike
coming to the next Big Data technologies
that is Apache picking so what is a
Pacific so Apache big is an abstraction
layer in history office and was
introduced by Yahoo so Yahoo had a white
paper on big and they developed this so
what how is big different from high or
what it does so it is a platform for
analyzing large data stack stored in
Hadoop distribution processing clusters
so hive and pig are used to one or the
other way to interact with your hats DFS
but how better they are and in which
cases we have to use hive and which case
you have to use P you can now understand
so high if I have a resource who knows
SQL he can directly jump in and write
SQL queries to analyze the big data set
right and what is the facility of big it
helps in performing mass kill parallel
computation on big data with lesser
lines of code so one line in big may be
equal to some n lines in high Q or in
your Java so it is very easy and really
powerful so two main components are pig
latin pig language and big compiler so
using pig whatever aggregation reports I
want your sales report monthly report
early report so I can do aggregations
with big data very faster so that's how
it is optimized way of using them for
reporting tricks okay so that is why
much me pig is famous for so now let's
understand how talent has you know
coupled these Big Data technologies what
are the components available and why it
has become so famous with big data so
talent with big data eyes first of all
ace automated so you would all be
surprised like what is this automated so
as I mentioned earlier so big data needs
mapper and reducer chops to do any of
the big data processing right so we
don't know how to write that map or
reduce programs neither people who are
using hi knew it they would only write
the SQL query which would automatically
be converted this
we the components which we are using in
Thailand we have to just configure them
for required it is to go and connect or
to do some specific actions but here
also we are not writing any mapper and
reducer progress by ourselves so it is
automatically generated so that is how
it is automated
it is very easy as you've already seen
the previous video of Thailand da so it
is just drag-and-drop configure and
plug-and-play so it is affordable it is
available in your open-source Enterprise
version even Enterprise version is
affordable and fast ultimately the
backend code is Java and the compilation
and execution speed are just like Java
so it is faster so with all these
selling points talented big data is a
very big hit so Tallinn open studio for
big data so what are the features of it
so talent open studio for big data is
built upon the talents integration
studio ok so it is a superset so if you
download TOS for big data it is like T
wise for data integration plus big data
components ok so why we have this
difference so if you are not at all
working on big data components why will
you simply download the related
components which are there in big data
version right so only if you are working
with big data you can go for this
product otherwise you can do everything
which is available in big data in DI as
well so di is a subset and TOS Big Data
is a superset which is containing da
okay so it is again an open source
software and provides an easy-to-use
graphical development environment to the
users so the learning curve is very less
so you can learn it very quickly and it
is psychologically designed for whatever
option you just go and search in the
palette you will get a component you
just have to configure them use it so
tio is for big data at the back end will
automatically generate the underlying
code in Java native code is in Java so
it generates your jar files it is an
powerful tool which leverages the Apache
Hadoop big data platform and helps users
to access transform move and synchronize
it so it is tightly coupled with Apache
Hadoop Big Data Platform
so after you see the components you will
understand it is
very easy like just how you develop a
normal di job you'll be developing the
big data job as well and you will be
surprised to see that so coming to the
advantages of talent open studio for big
data so as I already mentioned fast
designing it's just drag-and-drop
configured them and tell specific
programs to work on aggregations report
generation whatever it is so better
collaboration we have all the big data
components for customised components and
we have no special components for pick
which will work on pig related file
types all the stuff so early cleansing
as I mentioned one of the feature of
veracity wherein before going to Big
Data we can do all the cleansing
operations in di and then put them into
Big Data so efficient management as you
know it's all optimized code which isn't
getting generated your mappers and
reducers will also be an optimized code
which will be an efficient management
and easy scalability as you already know
you can scale it up scale out whatever
you want with big data along with Talent
so together with all these advantages we
see how talent big data works so we have
to first download the tos for Big Data
so we'll have to just log on to
challenge calm under download section so
we have the download section in the
download section we have the big data so
data integration plus Big Data
components will be your Big Data thing
so if you download only Big Data it is
as good as downloading data integration
plus Big Data components right so once
you click on this link you will be
starting your download which is again a
zip file so if your download doesn't
start you'll have a link to restart your
download over here you can just click on
it so once your download is complete so
you will have your zip file downloaded
here so once you unzip it you will have
executables for all the environments
like if you're using Windows if you're
using Linux so we'll have the
executables related to them so we can
launch the write executable as per your
system requirement so once you have
launched the executable you will be
seeing a window like this which opens up
openstudio for big data I'll be showing
you all this stepping demo as well so
we'll have to create a project to start
our development so we will have to click
on create project and then give a valid
name for the project and just still
finish so once you do that you'll have
the Welcome page open so you can close
the Welcome page and we can start a new
job so this is how your TOS big data
opening page looks like so we did a
components in talent is what we are
going to see now so once we see and get
to know about what are the components
for Big Data then we'll start the devil
so Big Data family in talent so if you
can see in the palette I'll be showing
you in this demo as well so in the
palette and a big data we have all the
possible big data's which are tightly
coupled with talent already present here
like Cassandra and as we already spoke
about big hive they are also present and
we have has TFS components which deals
with file management's and all other
things present over here right so Big
Data of family for talent so these
leverages the power of Big Data
technologies so talent provides a wide
range of built-in components for Big
Data they're all already available you
just have to drag and drop and use them
using these components you can connect
to the modules of the Hadoop system
right history office they create
connections to various third-party tools
used for transferring storing or
analyzing the Big Data so it's just like
how you use the talent data integration
take the components to read write or
filter out do aggregations whatever you
want so for all of them
they data components are already
available so Big Data family for HDFS so
we have T HDFS connection we have T
history F is put that we have the file
in local system will have to first
transfer that to history office so after
you transfer that using T HDFS input
component we can read the data from HDFS
and once we have done the analysis or
the business rule applications and then
we'll have to put in a different format
back to head Steffes we'll have to use
the T HDFS output
so tht office correction which helps us
in connecting so here is what I need all
the configurations what I mentioned
right on the name node what is the URL
for the name node what is the URL for
the resource manager all the stuff will
be provided in HDFS connection using
those inputs will be able to connect to
the right hash DFS system okay so this
component helps in connecting to the
given history office so that other
Hadoop components can reuse the
connections like head step is put or
input so this Creator has Steffes
connection and as I already told staff
is put it just helps copying files from
user defined directory and it would
transfer them to HDFS is also capable of
renaming them so once I transfer them in
Steffes I can even rename them and T has
TFS input as a component helps in
extracting the data in HDFS file so that
other component can cross it I can read
the input and I can give it to a filter
or an aggregator and I can do whatever
my requirement is null timidly T HDFS
output is for transferring the data
flows into given HDFS file system so we
have connection and put an output
components and for file transfers also
we have standard components so this is
all our history F is majorly used
components and we have other list of
components like here like we can compare
the Stephan's files we can copy delete
we can check whether a file is existing
or not before processing it we can have
you know raw output you can check the
properties of an HDFS file ok we can
rename or get the count so all these
things are already available these other
widely used components so this is the
complete family for HDFS so let's see
for how you and pick so big data family
for high so high has a very limited
components and we can achieve all our
requirements using these components only
we have T high connection as the name
suggests so it helps in establishing the
connection so resource manager is
required in T half connection which we
already saw in the architecture and T
hive input so this component helps in
executing the select queries so hive is
all about your SQL right so if I am
taking this input component I'll be able
to write the SQL queries as I know only
them so
input component code will convert my
query into mappers and reducers programs
right so it sends the data to the
component that follows so T hive load as
a component which helps in writing the
data are two different formats so I'll
have a file which I have to load it to
HDFS so why are high I'll be able to
load it as a table right so different
formats into a given hive table or in
exporting data from a hive table to a
particular director so it also does
multiple function it loads and also
inserts the data into a table right and
yes before we could insert a data into
the table we should have a component
which will help me in creating a table
right so we have T high create table
which helps in connecting to the
hydrator base being used and creates a
high table which will be dedicated for
the data in specific manner right so
though we have a file like HDFS
everything is file right HDFS is file
system so though they are underlaying
stored as files via hi I'll be
retrieving that data in file as a table
so which I know and how to analyze the
data I'll be aware in SQL language so
that is how these components help me in
converting those SQL queries and work on
Big Data so let's see what other big
data family for big so again big data
family for big has few more components
because big way of storing the data is
different so we need to have different
components for analyzing the data right
so that's why we have more components
here we have TPG aggregate we have TP
distinct we have filters you know
dedicated components for big itself so T
big load this components help in loading
the original input data to an output
stream with a single transaction so
after the data is validated so I have
complete data set every validation is
completed so if I give it to a big load
it does it in one transaction so it will
always be one row I'll be showing that
so big load is to load the data to your
hats Travis so what is stick big map so
this component helps in transforming the
data from single or multiple sources
again
this Tepig map can work on your
big components only so they are all
dedicated components so you can take
multiple sources you know you can join
you can do some transformations and you
can give a different filter result as
well so TP aggregate is a component as
the name itself suggests it's used for
aggregating the data based on one or
more columns and we also have TP joint
so which helps in executing the inner
joins and outer joins of two files only
two files the pig map can work on more
than two T peak join can work on only
two right so it joins and in order to
create the data used by the pig sources
so what is the difference between P pick
join and keeping mappers team a pig map
can do many things like joining for it
string writing in two different flows so
it's a heavy component it has lot of
code since it is doing multiple
functions but TP join is very dedicated
to only joining and we cannot do any
different transformations over there so
it's a light component so if your
intention is only to join we should
actually go 40 pig joint which is more
optimized if we have a requirement for
transforming filtering and then joining
we can go for TP map so that is how the
differences so this is a complete Big
Data family for P so we will first see
now how to develop a big data job and
talent it will take a very simple use
keys are say for example we are using
country names maybe which countries
being in social media a lot like today I
have five countries which are going
trending on hashtag and then in the
first row I will be getting six country
names separated by space ok I'll be
showing you that file likewise I will
get a data for a month or a year so I
will have two now it's a big data when
it comes to n here every day what is the
trending hash time or something like
that then I have to analyze which is the
most trending one where I need to give
an orderly way like which is first which
is last so I'll have to unlace on a very
big deal a set so given this use case so
how do we do a job in talent big data is
what we are going to see now as we have
showed you in the slide how to download
the Tiwi acerbic data so once you have
downloaded the talent big data so it
will be the zip file
already mentioned so once you unzip this
you'll be having your folder like this
for Tallinn big data and which would
include all the binaries for your
windows and linux
so since I'm using the Linux VM I'll be
opening the Linux Exe
that's shell script and this is the only
step you'll have to follow to launch the
talent open studio for Big Data the
prerequisite will be Java should be
installed and Java home should be
configured that's it no other steps we
will get a welcome page like how we are
seen where we will have to create a
project to proceed so now we'll create a
project by clicking on create a new
project option and then provide a valid
name let's call it it Rekha until create
so it creates a new project in Thailand
Open Studio for Big Data once it is
created
we'll have to select the project and see
finish and that's opens upper talent
open studio for big data for your
development so I'll just click on finish
it opens up a welcome page for you once
we finish the project we did a welcome
page from Thailand Open Studio for Big
Data we'll just close it
and then the other windows get activated
which has seen as you have explained all
I got windows in our previous video so
in repository where I create the jobs
and these are the configuration windows
and this is the palette window where I
will have all the component listed once
I create the job all these windows will
become active so how do I create a job I
come to repository I click on job
designs I right-click
and I see create job clover a meaningful
name like J stands for joke
big data demo
when I say finish
so once we have created the new job it
opens up our designer window for us for
the development and this is how my
designer window looks like so I will
have to first create the hdfs connection
to start my use keys so I'll come to
palette and type in th the office and I
get all the related components I'll drag
and drop tht office connection component
into my designer and I'll start
configuring it so for the first
configuration the property type will be
built in because we are providing the
values manually otherwise we'll have to
go for a repository which is metadata
and for the distribution since we have
our own setup not like we are not going
into any of the sandbox and we choose
custom unsupported whenever we say
distribution is custom unsupported we
have to provide the configuration just
for talent so I'll be providing them by
clicking on this button so I'll choose
import from zip option I have the I know
the set of jars which is required for
HDFS correction which is provided by the
setup team so we'll need all this
information to connect to head Steffes
so by just giving the zip file with all
the jars all our property settings will
be taken care by challenge
so it has imported all the jazz required
for HDFS connection so I simply say okay
and I connect start with other options
so authentication will be username and
here comes the name node URI option so
that is why we had included in our
architecture diagram and give you an
insight of what is named Lord so the
name node URI will be history office and
since I have my own HDFS setup in
localhost my server name will be
localhost and the port for name node
will be 9000 so we don't have to give
any other mandatory options so username
will be a breaker with this my
configuration for T has surface
connection is complete
so once T history of his connection
configuration is done my next step for
the use cases I'll have to move my input
file which is out dot CSV which is there
in my local host I'll have to move this
file to HDFS system so let's see how we
can do that so as we have already seen
in the PPT which is the component which
will help me to load my file which is in
local to HDFS is tht office put
component so as such was the HDFS put
and I find this component over here and
the other method is I can directly type
in over here th DF is put which puts
files onto a status I take this
component so on successful connection
how will I control my flow of components
is right click on this connection see on
component ok
that means successful connection is done
then you can move a file from my local
to HDFS so once HDFS connection is
successful we are moving to PS DFS put
so there in th step is put I am using
use existing connection and I am saying
I am connecting to the previously
created connection that is a staff is
connection 1 and the local directory is
documents wherein I have a file name out
dot CSV so from this local directory the
name is out dot CSV and when I move it
to HDFS I am renaming that us us city
dot CSV and this file will be moved to
user folder in HDFS right so by doing
these configurations I am moving the
file from my local host to head Steffes
so my second step of my use case is
darling so after I have moved that file
I'll have to read the file from HDFS and
I love to implement the solution to do
the word count on the city names right
so which is the component used to read
the data from Big Data is tht office
input component so I would take the
input component and as you already know
I will directly click this option use
existing connection and then I see HDFS
connection but before to that I will
have to provide the schema name so I'll
have only one column which will have
city name which will be a string right
so after this the file name which I am
going to read is after it has moved it
will be from user and when that file is
mood it would be renamed as yes city dot
see a sweet so I will take this value
and I'll tell to read that file which
will be created and it is a text file
and it has space as separator or does it
have any field separator because will be
you know base will be the field
separator which we'll be using to
denormalize later to do the word call
but initially we don't have any we have
only one field and with this options we
are able to complete the th TFS input
configuration once both these components
are finished their work successfully on
sub job ok
I'm creating this so the flow will be
first th step is connection on component
ok takes up residence and moves the file
once it has successfully moved on sub
job ok will be triggered and then it
will read the input right
after reading the input we'll have to
now normalize the data so I'll take
normalize component and I will take this
input I will give it to normalize
corporate and normalize component will
have the same schema and also if it is
not having a Content sing columns and
I'll tend to normalize it based on space
because if you observe my input I have
space in it right and then by this way I
am able to convert each of my single
room as a space as delimiter I can
create each of them as single values
right
so after this since each city name will
come out of this T normalized component
I can give it to an aggregate ill
component so I'll take this value
normally 2t aggregate and in this T
aggregate row
reconfiguring them as
so I'll take the city name but the
output I want is the count of it right
so I'll create count and make it integer
and I'll have to group by the city name
so city name will be my group right and
what operation I have to do is I'll have
to put value into the count column and
the function I use is also count and I
count the city names and I ignore no
values null values if any but space is
still a handler for us so we will get
one row in aggregation which is for
space since we are normalizing for space
delimiter you'll see in the output so
once this aggregation is completed the
data coming out of this aggregation row
I can just give it to th staffers out
and this can be again I'm using an
existing connection
columns can be synced and I'm using the
staffs connection and the file name
which I want to create is so and the
user itself can just tell create a new
file by saying US city cops something
like this and then it is a text file it
can be you know action is create and row
separator is slashing and fill-in
separator may be pipe and that's it so
in this way I'll be able to read the
input from Big Data do my normalization
do an aggregation on that so that I get
the count of each of the city and then
the result I'll be storing it to th the
office output right by this actually we
have completed the use case wherein by
running this job we can see the internet
output so but I'll also try to introduce
to one more big data technology which we
already saw in the Peabody that is hi so
let me take this file which is created
as an output in this D HDFS output
component and let me load that data in
to high so that we can query that data
using SQL query so what I will do after
the successful creation of this file
again on sub job okay I'll create it
I'll connection using T hive connection
component
so on subshock okay of this that means
all this component will finish their
execution successfully and then come and
create a high connection so again if any
jars are needed we can install them
directly
once the required jars are completed
download is complete then we'll start
configuration of the dehy connection
component so again I also is a custom
setup in our VM so I'll choose the
custom unsupported and I'll provide the
required just for the configuration in
the similar way we have it for I was
with so high connection so I'll just see
ok and it will import all the high
connection related jars so all the jars
has been imported for high connection so
once all the jars are imported then I'll
tell you see on which is my resource
manager related options the connection
mode will be standalone since it is on
my local host and hive server will be
iced over to by default my host is
localhost port is 10000 database which
you have already created for demo
purposes at Eureka
if you are doing it for first and we can
use default database which is a default
database which is existing in I and for
this I don't need to provide any
username and password since it is
already there in my local I'll have to
just provide resource manager which is
in again in my local host and port is 8
0 3 2 and will have to provide the name
no D array which is HDFS localhost 9000
which we've already used for that step
is connection as well and I have to
provide my Hadoop user which is Eddie
Rica so providing all this information
my hive connection should be successful
and then which I have already created
this file over here I will have to load
it to a table so before I'll have to
load that data from file to a table I
will have to create a table so we have a
component to create table so T hi
create table this is what I will take
once connection is successful on
component ok I'll send to create table
with that again I'll turn use existing
connection I take the hive connection
and schema and waste my report is having
city which is a string and count which
is an integer
so with this structure will create a
table for us city count table in format
of text file so we have create able
option so the best option we can choose
this create table if does not exist so
it will only create if it is not
existing and then the field separator
will be semicolon
so with this operations or settings we
will create the table in hi so once this
table is created using T hive load we
can load the data from the file which we
had created in HDFS to this table which
we are creating now so after this table
is successfully created I'll just go to
T hive load so T have load also I'll
tell you existing connection and I
choose the hive connection so action
will be load and the file path which
I'll have to load is which I have
created in th stair force output so for
hive will have to give the path from s
to F is only so that is why H Steffes
path is provided here and the table name
which we are going to load is the table
which we are creating in di create table
which is us City count copying that
value and I am telling load this file
data to the table which is here right so
it will use action is append so let's
leave it with this options and then
since my file is of pipe delimited that
may also give pipe delimited over here
so these all these settings let's now
run the job and check if there are any
problems so now we have completed the
development of the Kavli job we'll have
to now execute and see whether it works
as per our requirement but before we go
on to the complete execution
let's first finish the execution till
the final output which we are expecting
in history office it is created or not
that is because when we do an high load
it actually clears out the data in HDFS
before after loading it to the hive
so let's break the connection
and let me be activate this part of hive
and only run till this part and check
whether my requirement is right I mean
it's creating the file with the required
data okay
so the now job is executed till the
final output is reaching the HDFS output
so now let's first see the first step
whether it has moved the file from an
Eureka Documents folder that is out dot
CSV to user folder in HDFS by the name
of yes city dot CSV so how do I browse
it I go to localhost by double zero
seven zero iodized see browse this staff
this folder and we can see and refresh
this so we have yes dot C T dot CSV
created over here and let me show you
the content of it
so it is the same file which we have
moved from local which was outward CSV
and now it is available in HT office
with the name yes city right so with
this we have moved the file from local
to HDFS so after we have murdered we are
using tht office input to read that file
and we are you know splitting the city
names and we are aggregating them and we
are creating a output file with the name
u.s. city counts in the same folder so
now let's check whether we have that via
city counts file yes it has been created
now let's see the content of this file
so now we can see we have the aggregated
report with city name and count how many
times it has been used and as I
mentioned already since we are using
this space as delimiter
we'll get the values for space as well
which we have to ignore and consider
this rows for our business decision
purpose right but our intention is to
load this data into hype so let's go on
and activate the section which we have
deactivated for reason that it clears
out the file which is being loaded into
hi so now we'll activate that section
and we'll run this job and see whether
there'll be a table created and then do
we have the same data which was there in
history office in the table and we can
query they're using the same SQL query
right so now it should create a table
with table name yes City count and it
should load the data from my HDFS file
which I have created to this table so
now let's run this job and check whether
we have the required output
you
job has been successfully completed now
so now as the last check for our output
we'll have to log into hi and check
whether this table has been created and
this table has the required data right
so we'll check that
let me login to him now I look in the
terminal and I see five
now I'll use the command show databases
so it has default and rhetorical so I
tend to use Oracle database since I've
used that data base to create my table
- once we are using this database let me
check whether my table is created or not
by using show tables command so US city
count has been created now let me check
whether it has the data which we had
created in history of this output and we
have loaded that into hive so I see just
a simple SQL query select star from US
city underscore count so I have the
output which was already there in my at
steer thrust and same thing is me allow
allow you know it has been loaded into
my hi and then by using SQL query like
imagine me as the guy who only knows SQL
query I was just able to write in a
scale query to read the data which was
there in hi but actually the data in hi
is underlying in big data HDFS right so
with this we complete the simple use
case which we have taken and what
business decision can be taken as the
Los Angeles is the highest used city
name and the social media is what the
decision which I can take and based on
that decision the investments or the
future plans can be taken so this is all
about the demo which we were planning to
give you on Big Data demo hope you have
enjoyed it thank you one and all for
attending this demo please follow us for
further updates and videos on talent
thank you I hope you have enjoyed
listening to this video please be kind
enough to like it and you can comment
any of your doubts and queries and we
will reply them at the earliest do look
out for more videos in our playlist and
subscribe to any Rekha channel to learn
more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>