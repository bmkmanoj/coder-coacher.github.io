<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Real Time Use Case : ETL + Big Data | Reducing MapReduce Time with Talend Open Studio | Edureka | Coder Coacher - Coaching Coders</title><meta content="Real Time Use Case : ETL + Big Data | Reducing MapReduce Time with Talend Open Studio | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Real Time Use Case : ETL + Big Data | Reducing MapReduce Time with Talend Open Studio | Edureka</b></h2><h5 class="post__date">2015-05-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fN1cljvldks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I welcome each and every one of you on
behalf of ADA Rekha to this exciting
course of talent for Big Data we'll
start with the primarily with the
objective and I had also prepared one of
the use cases using some some of the
mass data in the in the banking industry
as well as I prepared some of the social
media web log data where a lot of
unstructured data has been used using
talent for big data to do data analysis
where companies are primarily you know
looking out for generating award cloud
and predominantly looking to generate
some of the business out of them all
right and we see also how this
Challenger has been implemented using
Hadoop so alright now it's time for to
see the big picture and a few of the use
cases that I have exclusively built for
you with few of my prior client
experience I have almost travel half of
the globe we have done a lot of
solutions for many of the clients you
know you in ETL in business intelligence
and big data and data analysis now we
will all see quickly what Allen can do
in minutes reducing the man-hours in
doing the MapReduce programming in
Hadoop all right all right and going
back to the industry use case the use
case which I had prepared ok for that
use case we have used the setup of we
are using hot invoke sandbox 1.3 we are
using talent open studio for Big Data
505 then using Windows 7 as you can see
and I'm using a normal normal laptop I'm
having a 4 GB RAM and it's i3 processor
so I'm not using any high configuration
hardware here I'm just using normal
windows 7 for zebra mine 3 processor to
make my Big Data sandbox my big data box
and my talent up and running and kicking
of the MapReduce program ok
now we have collected a unstructured web
block file where people came in to
Facebook and they have spoken lot of big
data threat of Hadoop and all that and
people can write as you understand
people can write and Facebook anything
they would like to do right and that
could possibly generate a huge weblog
data okay so we have generated that we
have extracted this one of the weblog
file from Facebook and what we are going
to do is we are we have extracted that
into our local file system obviously so
using the job everything is automated
Ellen is going to transfer this file
from the local to HDFS alright in
seconds and then talent is going to read
this file from the within the HDFS this
is going to analyze the data it is going
to process this data via the big script
and achieve the required result now what
does the oh I will tell you the business
case and what is the expected out of it
in minutes all right
and before we just jump into the
hands-on because we are not going to
come back to the slides anymore so
quickly this is the salary trend
okay for the Talon ETL developer for few
of the people who asked how is it
performing starting from 2012 to 2014 if
you see it's clearly showing upward
trend in terms of the job market okay it
has been taken from indeed let me
quickly show you that what all
specifications we have used okay we have
used Allen for big data this is how
talent for talent the open studio looks
like okay I'll explain more in depth in
detail of the architecture during our
class because today because of the time
constant of the webinar and doesn't
allow me to get it deep into NN but
primarily this is a graphical user
interface as you see and you see all the
components
here and the great idea is suppose most
supports most of the thing so use cases
one second use cases
see the file which we have used for our
analysis this is the file I hope
everyone can see my screen this is the
file that we have taken for the analysis
all right
it's an unstructured weblog file here
people have just normally use the text
data okay everyone just talk about big
data then most high up internet talk is
big data in 2014 Hadoop is so much in
demand for data storage and all that
anyone who came they have used any
punctuation mark they have spoken some
more other thing about big data now the
one of my clients expectation is they
want to make some business serious
business out of it they want to know
which are the word or which are the term
with which to you can connect with our
audience or you can connect to the pitch
of the market and you can do certain
business on top of it what is that that
people love to talk more and they want
to listen more which are the most hype
the world in this log file so this is
the kind of random analysis data
analysis and what about sentiment
analysis that my client wants to do on
top of the web log file and my client
has just given me this with love file as
simple as that
and they asked me that they are just
like how do you do the data analysis and
they want to know the top trend the top
5 most spoken about words and they want
to prepare on what cloud-based on top of
it you might have seen word cloud the
banners everywhere across the puddings
and all that that is primarily called a
word cloud okay so using talent for Big
Data what we have done is we have
connected to Big Data okay connecting to
the HDFS we have connected to HDFS step
1 these are all these things have been
done just within minutes just using
trying and drop to save time I had
prepared this job just before we started
the class
I have connected to the HDFS okay and
then I have transferred the files from
my local system the file which I am
showing you is in my local system in my
C Drive talent input web extract or dot
txt within fraction of second it is
going to Alan is going to transfer this
file from the local local file system
into the HDFS directory you to use a
root test data okay
and the file name then it is going to
transfer his name is going to and just
go in to name the file is what flow X
that's the file which is going to get
generated not okay and the same file I
am going to read okay what club X I am
going to read the same file okay and I
will just show you quickly how do you
access the HDFS data I can also access
the HDFS they say this is my hot and
walk sandbox one point three okay this
is my command prompt this is the Hadoop
command prompt way and I have just I'm
just using Oracle Virtual Machine if you
see I am using Oracle Virtual Machine
Manager to make the bottom of sandbox up
and running this is the command prompt
way of accessing the files same thing we
can also access through spending little
slow because yeah all right same thing I
can access through the URL will be URL
wait to see I can go to the same path
from the URL that I will be accessing
from here when it is placing the file
user root test data for cloud X okay use
a root test data user root s data
there is no such file as of now or word
clove X okay as soon as I will run the
talent job it is going to first transfer
the file from the local which is C Drive
talent input web extract 1 dot txt it is
going to transfer this file into the
HDFS step 1 ok first it will connect to
his difference second it will transfer
the files third once the file is
available within the HDFS it is going to
run a bunch of talent components where I
do not need to do any hand coding and it
is going to do a data analysis in HDFS
which is in Hadoop system which is a big
data area and what it will do it will
analyze this wave extract data in HDFS
and it will produce the most frequently
used spoken word again within the SDF s
so what it will do it will analyze the
data it will separate segregate all the
data it will sum and it will count of
frequency it will calculate the
frequency of the most spoken word and I
am going to short this data based on the
maximum count and I am going to display
the result here and I am also going to
create the file called word cloud output
3 into the target system ok so I will
quickly run this job and as of now as I
see there was no file there was no file
here called word cloud X but you will
see that in minutes not finish in
seconds because there are many things up
and running in my machine and it's just
a 4gb ram machine so if you have a 8 gig
or 60 Ram machine your machine will run
even faster than my machine ok and
that's a promise
and I let this job run I will do a quick
bit of explanation on the job how things
are working and then I will I promise
you a jump to all the questions
and I'll answer most of the question
that has been asked on the question
answer window so please stay tuned over
there this technical time to start up as
I said there are many things running on
my machine at the moment so it gets
heavy normally a cross industry and
during the real time project
implementation we use one of the strong
mother node in terms of hardware which
actually does the job so if you see now
it has started building the job it's
generating the job and quickly you will
see in the run console it will show the
result as well as it will store this
result in the HDFS it is interacting
with many interfaces plus my machine
also had one idea so yeah it has started
now as you see it is connecting to HDFS
perfect as soon as it connected to HDFS
it just finished the job in seconds can
you see that can you see that can I get
a quick guess from each and every one of
you can you see the result and I get to
cook yes
all right thank you very much now as you
see this job has started at 23:05 on 7th
of May and it finished in 23:05 within
7th me so hardly it would have taken
some 1 or 2 second actually to process
this job only thing is because I am not
using a very - configuration hardware so
I hit at and plus my machine has one
little bit overloaded so it took some
time to start up if you see the job
statistics log it is showing the word
and the word count all right and if you
see here the job it has read 11 rows
initially because they are worth 11 rows
here okay but how it has done the
analysis it has taken each an individual
war
it has taken each an individual word it
has split all the words it has formatted
it it has taken the aggregation in it it
has taken the count so after it has
taken the count altogether there are 130
words in the entire web log file imagine
if it would have been a huge web block
file also it would have done this come
and then it has summed up it has also
summed up the same words so anything
which is with upper case lower case
spelling is spelling everything it has
standardized also so it is also doing
data quality data cleansing let me
remind you
most of the MapReduce programming they
don't take care of data quality here the
advantage that you get you not only do
data analysis you not only do it on big
data but also you do with data quality
in cleansing so it makes a lot of sense
out of the business data and then it
produces the result here as well as it
has stored this result where so if you
see the first few records here first few
results so I come to know out of this
analysis that
words like in Big Data do is data they
are the most spoken and hyped words
during the discussion on the web about a
topic
so using this words I can get connected
to the sentiment of the web audience
this was the motive motive behind this
analysis so let's go quickly and check
this file when it got created user who
tested our cloud output three user test
data load test data war cloud output
three and do you see when it got created
they got created today at Mesa mint on
India times of 11:05 p.m. dantana you
see war with the word count all right
and with the same result that you were
supposed to achieve and have P written
any MapReduce programming or any Java
programming can I get a quick response
have we done any programming no not at
all
so this is a very small example and
within minutes I am going to showcase
you are tremendous a very quickly a
jazzy dashboard that we would be
generating from talent and the second
use case that okay
so Ravi is asking very good questions of
you know what where is the logic written
very good
that is where exactly we are going to
learn during the course curriculum but
very quickly I will explain you what is
it that I have done within the job so if
you see this web block file what we need
to do is we are very interested to know
each individual word frequency but here
there is no delimiter
there is no row separator right so what
we have to do we have to bring we have
to treat each an individual word as row
for that what I had done I had used I
had used a normalized component in
Tallin what it does you have to tell
that what I have done is reading I have
done using just one column I have not
segregated my data what I have told
normalizer is first you just separate
each an individual world with space so
if you see here all sentence have been
written with a separator of space
obviously when we write we write using
space bar right so cleverly I had used
the space as my separator and I have
given in the input and what I had asked
is you separate each and everything in
two so what it has done it has splitted
the column in two rows so 11 rows have
become 130 rows after that what I had
done using a team app component which is
equivalent in in formatting if you see
expression transformation in other
component if you see transformation what
I had done I have standardized the data
and I have transformed all the word into
the upper case so that when I will
compare if you see here in the
unstructured web block file somewhere it
is written with the big data with be
caps and be small somewhere it is
written big data with D caps and
as well but they are all the same so
when I will take a frequency and should
not be counting big data separate and
this big data separate they are all same
so in order to make this data quality in
one standard that is what I was telling
about data quality hadn't made
everything into uppercase after I did
the upper case I had aggregated my
result what I had done I had calculated
the count of each of the world and I
have represent the count I have given it
to a shorter shorter is sorting it based
on the descending order in terms of the
calculation and till hobro I am using to
display the data and displaying the data
as well as I am redirecting my input
into HDFS file so the process is
happening I am reading the file in HDFS
itself and I am doing all that else's
within HDFS and I am storing the result
back into HDFS I hope that makes sense
yeah I think I hope that answers most of
people's question sure Agumon is saying
as it's Kane is mentioned for what sound
so how it's designed for what county fun
so I had just answered that question I
believe can you check how many times the
word Amon is count it can we get hourly
mapping output in Tallin yes we can can
we handle all accomplished non
structured data with graphical talent
yes without using programming gas
pressure that is the beauty of talent
that is the reason talent is the biggest
competitor to author ETL tool because
talent is based on Eclipse Java platform
even though talent is not asking you to
do programming but talent has given the
capabilities and flexibility of
programming in component you can drag
and drop and you can literally do
program that will happen on back-end but
you really do not need to do the
programming can we integrate Python
other scripting level to get a real-time
for telling yes Prasanna yes because it
is based on a
a platform it also complements Python
the courses and certification that comes
with learning map reduce waste market is
exactly it is not going to waste
different area in both Atmos is asking
this question different area have
different purpose okay
talent is specifically talent for big
data is answering one particular area
which is data integration along with big
data
karate is asking is there any way to
filter out relevant data
yes there is there are components like
to filter Rouen odd where we can filter
out the data now we do not have too hard
for theory the run in order to remove
the - and all
ok so quickly let me show you the second
best example what we have done is that
is a containing analysis ok a banking
industry use case the second use case
that I am going to show you is the
banking industry use yes ok where what
bank this this particular Bank all
fantastic man ok what they have done is
they had done the campaigning in
different cities across India for a new
product launch ok
the the dampening analysis that they had
done in Bangalore the data that they
collected from the Bangalore zone is
this ok this is as simple as the data
the city name and the first law that
says and the first second field that it
says is the total application and that
the total enrollment and no response
it's like how many application they
distributed in Bangalore how many
actually conversion or how many
customers they've got which have done
the enrollment and how many no response
they have got ok as simple as this like
this then we keep on getting the data
from difference if the election may like
tell me like Hyderabad like by a length
and all this data comes in different
files and just with small bit of
information now their curiosity is they
want to make some business sense out of
this data they want to decide which are
the city where they are getting more
figures in terms of getting the customer
so obviously they need to focus more on
this cities and we charges it is they
are getting a very good conversion rate
and they want to know what is it that
their employees are doing in this city
is that they are getting more success
rate okay so in order to do these kind
of analysis
Ellen is quite capable of generating
some interesting graphs first it will
collect the data I'll explain you how it
does first it will collect all the data
from this directory okay
it will prepare one file by clubbing all
the file to unite all the finds it will
prepare one file then it will aggregate
the data it is going to do that using
the big scripting it will aggregate the
data it will produce one result and that
one result produced you can see is this
file one fight ok which will show only
Mumbai data one row only Hyderabad data
one know only Chennai data one row only
tell you data one row only manual data
one room once this data got generated in
Tallin if you see we can run a very
simple job which will read this
particular file ok this particular file
which got generated and clubbed from
different cities it is going to prepare
the intermediate data result for the
graph to get generated okay and once it
prepare this intermediate data that we
have it is going to prepare this graph
this particular graph that we have
generating is for Bank analysis so this
exercise I am doing to showcase you then
talent is not only a data integration in
big data tool but it is also a data
analysis and a very small scale
graphical dashboarding tool where rather
than looking at numbers if we see the
graph that speaks more than the numbers
cannot fire okay let me run this file
and we go to C Drive talent output into
the curve C Drive talent output data
Bank analysis so there is no such file
existing as of now I will just quickly
run this job
hang on there see the magic
this job just got finished with 15 rows
and if you see this file just got
created Bank analysis just now it's an
image file if I open this file there you
go
it speaks more than just mere data
it's my campaign analysis for Mumbai it
is saying that the total application
received was 10 or 11 and total
enrollment was less and no response was
Bor so the performance is not that good
so obviously I'm only looking at the
color I can interpret just looking at
this graph within fraction of second
that anywhere where the blue is more and
almost close to the red there the
success rate is good
so tell me has performed the best among
most of the city and who has performed
worst Chennai Mumbai in this kind of
city they have more no response and less
conversion so you see how quickly it
just generated the data the analysis
data not only the analysis data but it
has presented the data very nicely okay
let me go back to the question answer
window again okay so that more or less
just for your information that more or
less brings us to the use case we had
shown K we had showcased the two useless
one using a word cloud analysis
unstructured web block data
second we had collected the banking data
okay and from different set of files you
have clumped those files we have
prepared one intermediate file and then
we had used those files that
intermediate file to generate a graph so
that talent overcomes also the
limitation of showcasing the jazzy
dashboard graphs okay
alright sinner is asking can you please
show the source code generated by talent
the MapReduce program sinner
unfortunately the talent openstudio do
not give the access to the MapReduce
program on the back end but the
Enterprise Edition of the talent for Big
Data gives the excess on the back end to
the MapReduce program it is exactly same
as the MapReduce programming that you
will end up writing ok I hope that
answers your question
giantesses I took the big data Hadoop
course from him to raka but I think it's
giant ax everything has their own you
know flavor someone who is having a very
strong Java background for that person
MapReduce programming would not make too
much of a difference right so that
person will easily gel into MapReduce
programming but any person who doesn't
have a Java background or MapReduce
programming or any kind of program they
come then obviously talent for big data
is the definite solution yes serum charm
is saying when we have huge data then
how about analyzing the logic serum
right now also I can show you that 1
million of theta in Thailand just gets
generated within fraction of seconds so
in terms of velocity talent is very good
okay then we have some few questions
please stay tuned we have lot of
question a lot of interesting question
from lot of participants ok how do we
integrate the other databases with
talent ok this is a question from Sri is
Suresh I'll quickly show if you see
if you see this this particular window
scroll databases as I said you asked
you name the database talent has a
connector for this okay to talk about
as400 access DB logic if we do Firebird
trim clomp even some of the database we
might not have got high even for mix ma
sequel server mysql netezza
post GRE sequel redshift SAS Sybase
teradata Vertica alright everything they
have component all kind of in Oracle if
you see how to connect Oracle how to do
commit how to write logic how to run
queries or to get the data out
everything is there so I believe that
answers your question
Shiva Kumar is saying so for every now
so we need to design logic no shirokuma
the primary concept in talent is how do
we read the data once we read the data
then what do you want to do on top of
the data
what business logic you want to write on
top of the data based on that we select
the components okay how do we integrate
them okay that is answered
how can one build that will like to
interact with this dearest and and in
Kindle you sell things a lot of India
okay
srini is asking how do we process and
get the result in Excel Sweeney if you
see there is also one component called
input and output okay let me show you an
output that is something called T file
output Excel okay if you want to
generate excel file this is the output
that we need to use this we are going to
cover extensively during our course
curriculum that if we want to generate a
limited file or a database file or a PDF
file or an excel file or a fixed-width
file which is a mainframe pipe or a JSON
file any kind of file or even a multi
schema file on structured file we can
use talent for that right ma note says
perfect case study thank you thank you
very much Manos
and then we have Keisha one sure Priya
Priya says can you show one big use
kisses stay tuned dear Priya let me
answer few other question I will show
that in minutes yeah Ruchi is saying
okay person I still can be pushed out
put to the table or in a database yes
present if you see we can push the
output to databases I can write I can
push the data if you want to push the
data in HDFS itself we have to use the T
high output but if we want to bring the
data to local and then want to push the
data let's say in Oracle then I will use
T Oracle output I'll show you the
component Theora column okay inserts or
updates lines in the oracle table okay
then
wine is showing england is as a word it
increases the complex is saying bossy
this is the simple analysis of any web
log file we cannot decide what word we
want to see right we are just measuring
the frequency of the world so if there
is any specific port we want to skip we
can put a filter component which will
skip those specific word right third
embedment Allen a good combination for
big data professional Bilal is asking
his Hadoop admin plus talent is a good
combination of big data
definitely Billa I strongly recommended
sign Kieran is saying are there any
limitation in this form of MapReduce
like a scenario which can be done only
achieved with Java and Python I don't
think so I don't think so cycle there is
any limitation okay because it supports
if you see the big data if you see the
big data area okay it supports Cassandra
it's supposed house TV if suppose goggle
bigquery Google storage HBase s catalog
HDFS
I've MongoDB okay
it supports big it supports react
it supports whoo it supports high if
everything you name it it supports all
the platform so there is no difference
in fact it is more it makes more sense
to do it through Delhi Abdul is asking
can be updated I nice tables you can
tell him up till I believe there is a
concept in hdfs there is not there is
only over a 10 or there is any fresh
creation there is no update Logitech
that's a logic that has been given by
HDFS so there is no update much even
asking is talent also used a reporting
tool not exclusively Matthew Warren
motive Annan it's not a exclusive
reporting tool it's a predominantly a
Teta analysis data integration and big
data tool but yes they also have some
supporting layer supporting mechanism
for reporting can you example for a
small task of out of this and the logic
Shiva come on that is clear everything
will be killed okay I will show one
small file how to read and do a little
bit of transformation on that file when
I will show the big big way of doing it
does it have machine learning
capabilities giant ax asking as of now I
am not really sure this can be done
using our new students yeah okay can you
summarize the report workflow summarize
the report oh I didn't get that question
rape like what understand what kind of
analysis can be done any kind of manager
data level or anything that needs to be
addressed in terms of the type degree T
or big data can we email the graph after
the job execution yes uncle there is a
component called mail I'll show you the
component you see II send mail
since email with or without attachments
so once your graph is generated it will
just send out one email with that HP
okay
everything can be automated in talent
then we defined custom chart yes
Gilligan we can then Prashant is asking
if there is not much technical work
involved how will it help me to work on
it obviously you need to learn Talent
that's the beauty of it once you learn
Talent
you can do it in equality you can do
data integration you can do big data
there are so many options gets opened up
yes
a manager also can work on it yes that's
right Russian where is the talents of 12
so Brazil is asking where is talent
software has to install in the cluster
name not written on our port it has to
be installed yes on both here
what about scalability of Ln is it
strictly bound with the on-premise setup
and we connect to cloud it also can be
connected to cloud if you see I'll show
you who asked this question Krishna
Chaitanya is asking this question I'll
show you you see there is exclusively
something called cloud you have Amazon
Cloud you have Google storage you have
Salesforce you even have Dropbox as well
I believe it's five point five okay and
let me show you
yeah there's Dropbox as well so you name
it as I said you name anything in Big
Data
they have connector for it all right
this is very interesting isn't it
okay Boojum will asking can we modify
the backend code for based on Department
you can you have to be an expert on Java
yeah can you say
is indicated with talent I believe
talent the native native mataman
supports your jaspersoft report and some
of the other report which are
complementary to eclipse based but as I
said talent is predominantly a big data
and tec-9 division to Johar asking I am
a six plus years experience but you to
Big Data technology and want to switch
to area how should I go about it should
I begin with training on tonight
directly though on the Java
well Johar if you good in Java it is
also going to help you a lot Italian
because talent is based on Java platform
so I definitely recommend learning big
data through talent will make a lot of
sense for you to switch into the Big
Data idea Abdul is asking is talent a
licensed 100% open source they have two
flavor of it the one which I am using
right now is the open source and there
is no expiring
so in terms of learning it practicing it
and being comfortable with this tool one
of the best tool in the market they also
have a licensed one but you start using
it across industry or any project you
have to procure the license even people
are also using the open source edition
or their project work but obviously when
it comes to some flying sidewalk or
someone wants to use it exclusively with
proper license they do have option for
that as well
official asking will we have hands-on
access to the tool for our practice in
this course yes so I am asking if we
need to generate report every day
how will we trigger it automatically
instead of running it manually all right
so you are asking how do we run the
talent jobs externally right LM job is
quite supported even simply from a
Windows command prompt okay
and in fact third party till do like
control M orthosis or when you buy
license
they also give one third-party tool
which can be shedule in fact a small
shell script UNIX shell script also can
call a talent job yes so that is doable
Prasanna says a great webinar thanks a
lot cheers present no thank you
VI features of Allen rustic is asking so
I think to answer you the clarity VI
features of talent they have few of the
business intelligence component as you
see here right they have charts these
are again family subfamily the charts
they have tea bar Shakti line chat one
of the use keys I have shown using team
are check then they have slowly changing
temperature TB then you have jaspersoft
then you have OLAP cube and with an OLAP
cube you have Mondrian and Paulo and
then you have SPSS right so this is
primarily the business intelligence
family Rajesh is asking very critical
question what is the best way to
stepping into big data domain doing big
data a talk with talent or only talent
will be sufficient I would say doing
talent with big data with the knowledge
of víctimas DFS and Map Reduce will
make sense but yes if you're not doing
it also know how big did a Hadoop does
all kind of analysis whereas Talon looks
only for ETL no no no no I'll then do
all the analysis that with data is
capable of only different is differences
you don't write the program you do it in
a to know drag-and-drop which is more
easy why Italian only slipped for her to
know Ravi you can select Allen for
learning data integration if you want to
learn ETL also this course also makes
you a competitive ETL resource this
course also makes you a big dinner is
also there are multiple output of this
course curriculum
when Subramanyam says will talent bring
data from social media like every
Twitter yes if you see it also supports
file like JSON fine right I have done
one project on Facebook when the files
comes in JSON format if you see T file
input Jason it supports JSON format or
else if you know how to extract the data
from Facebook on the backend it
generates delimited files and obviously
under file if you see it supports
delimited file as well okay I hope that
answers your question Drummond saying we
need knowledge of Hadoop ecosystem to
work with talent a little bit of
knowledge is no harm but not mandatory
what are the big companies using talent
dual Venkateshwara is asking Citibank
are recently HP and then you have virgin
okay you go to the website I don't
remember all the name but they have
quite quite a big base of clients
Prasanna today time will not allow the
MDM features but definitely during the
class I will show the data quality in
India don't we need partitioning like in
an initiative detested yes Abhishek it
does support partitioning as well data
level how Thailand differs from
analytics to it like our and SAS Talon
Raman answer the question
Allen is freed of identity if you want
to be the backend guy who wants to excel
on Big Data and more on data integration
see there are a lot of tool in the
market to do analytics better analytics
to learn useless without data in a
structured and proper format and Ireland
is the area who does that ok
oh my answers your question
we're asking how output is going to be
decided whether it is Excel or word it's
completely up to you if you use an Excel
output it will be Excel it will use word
output it will be worked if it is a text
or producing text okay
Vijay is asking what is MDM we save MDM
stands for master data management
Krishna Chaitanya skin does talent
provide any reports on the MapReduce job
statistics yes
there is a family called logs and errors
you will see during course curriculum
that once you design any Java talent or
in Big Data how do you track the logs
and how do you make a reconciliation or
statistical process out of that job so
all kind of information we get under the
logs and errors so on T section are
saying I have the good knowledge about
MapReduce and think so do I need to
learn Talent
again you are going to do the
programming way right so let's say if
you spend 30 minutes to get a job done
it by a program Alan for Bing did a
person can just finish up that in one
minute so that's I hope that answers
your question
all right waylynn is saying there is no
coding involved in talent at all no
coding knowledge required is it
something like I've FD tested on a
larger scale well I would not compare it
with IBM datastage though I have worked
on the testers as well it is a ETL tool
and now talent for big data is a product
of for my talent which is X clearly
based on dictate idea and it uses the
core knowledge of the data integration
so there is no coding required
okay Abhishek saying what are the
invitations of talent come back to that
that means you're in jail dude I don't
see any limitations at all Abhishek I
have moved in and out in Tallin and I
have also worked on informatica
datastage and Benicio I don't see any
any limitations in talent whatever
acquisitive knowledge is needed to learn
talent no play because it is not fun you
can just come walk into the class and am
pretty sure when you walk out to the
walk out of the class of course
curriculum with the certification you
will be more than happy and you should
realize that you have taken the best
decision to learn the beginner this way
there any chance to change my course Big
Data and Hadoop to talent for big data
mine kundan is saying is that any chance
to change I have not been able to answer
then you have to contact our technical
team but I think ideally you should be
able to yes
Kiran is saying finally its Map Reduce
only so how come it's fast
chi'lan I never said it's fast I said
rather than you want to be hard worker
or you want to be smart workers that's
the difference between doing in
MapReduce way or doing it in talent way
yeah
these packages are available here like
big Genesius packages are available here
like big yes generally I will show you
quickly if you see here in Big Data
you have big component I'll show one big
job okay test X if I show one job I
think it will be more clear in terms of
big okay this is one of the big job that
I had build and once I will run this job
you would realize that it is actually
those people who have already done some
of the big data
courses you would have realized that
MapReduce program generates a file
called reducer ray so out of this once
the talent job is finished you would
realize that it generates a reducer file
so stay tuned there give me five minutes
let me answer all the questions
I love answering all the question let me
answer all the question I will show you
running this pick a lame job which will
do exactly the same thing which I can
bet you will take much more time
comparison to tell him if you do it in
man through this programming way okay
what are the advantages Ravi is asking
of Ellen or other 80-82
again for medical industries have any
show it is more flexible in terms of
entering any data one liner answer even
though there are much more advantages
like it it can connect to anything
everything I don't think other ETL most
of the ETL tool they connect to anything
I believe talent is more flexible that
way difference between Sukumar same
difference between and after adduced is
big is the system partition will fit
this talent see you become our talent is
just has they built a platform where it
is taking the advantage of existing
technologies in big data it is up to you
but then you want to run a thousand line
of pic script or you just want to do the
same thing using talent in five minutes
okay
the Sun is asking a Duke will be
installed on different nodes or will
tell it integrate with them determine
all roads that is something fashion that
you will see during the class how do we
connect from Ellen to handle just now I
have ran one job you would have seen in
HDFS only so we have to connect from
talent to the HDFS so sometimes it can
be executed in jump abrogate a
particular time
yes Prashant it also supports
scheduler when we will see during our
class how do we run the talent job
externally we do not have to come to the
graphical user interface we can just
kick off the job from anywhere and it
will do the same thing yes we can kick
off one particular job not one
particular job you can pick up thousands
of job at the same time
after brother of maybe Morelli Kiran is
asking how many days training a terrific
is providing on Thailand what about the
price
I believe Kiran you can check the price
online but the classroom session that
normally we take in Eureka X at 13 hours
class and that do we have a
certification project and hands on is 25
hours okay so it's a it's a 10 M module
program it's a 10 class each class
consists of three hours on weekends say
they saying where can we download this
tool you can download this yeah it's
available on the website it is being
answered we learn asking apart from
Thailand
do I need any other Hadoop component to
start learning
he says Davis part of Thailand
installation yes you don't need to learn
anything
see the last week when will talent would
start this Saturday saved we have
already finished the first batch we had
a immense response for our batch to at
max 3 the batch - I don't know if there
are still seats available you can
contact a director directly this we are
starting this off on 9th 9th of May 2nd
match and 3rd match is starting on June
Abdul is asking is it good only for big
data or it can replace data warehouse in
Toulouse like this assassin formatting
yes it can replace also if we a lot of
companies they are migrating from data
stage to talent informatica to talent
sss to talent it can also replace other
Intel tools big data is an extension of
it but data integration is the core
business villain same thanks a lot for
the nice and informative webinar it was
seriously change my perception thank you
very much belong what's over us is
thanks for the talent in for lunch mrs.
thanks good presentation got fair idea
see it is asking when when will the poor
start in near future
this Saturday say that is nine lines of
me
we're saying not a magnet when we get up
there are you saying can we connect
social media network like Twitter
Facebook through Tallinn yes we can
download the file and then Tannen can
get connected
what prerequisite is knowledge when it
will not tell em no prerequisite is from
one as I said three asking talent has
named not I'm job tractor like in Hadoop
yes Priya yes boss is asking talent is
enough to land a job in Big Data yes
alright very good webinar sir is saying
Ravi Thank You then any same thank you
very much for an immediate response very
different within five weeks and 15 days
give you them it's only on Saturday
Sunday so that will span for a month
each Saturdays and every three hours
I am the same can you share recording of
this seminar it is going to be uploaded
in the web site tonight so you can come
back and check and we already have the
previous sessions webinar on the IDA
website and also other YouTube under
exist thank you very much like thank you
or rock for their participation all
right let me show case those people who
are interested to see how does a sorry a
big scripting works okay if you see what
I am doing in this particular job
scenario I am just showing that talent
capability is not normally limited to
data analysis same thing what you can do
in MapReduce programming also it can go
okay okay so if you see this particular
big job okay I am reading one file
okay I am reading one file and then I am
filtering the data and I am generating
the I am storing the result okay
I am filtering the data and I'm storing
the result in the same file I'm doing it
in 1707 May one and then one more fine
aggregating and I am generating the file
f07 name - okay I'll run this job and by
the time the job is finished let me
answer some few more questions
if anyone's question this is a request
if anyone's question is still not
addressed and I it may be possible
because we have huge questions from all
of the participants and believe it or
not we had more than 300 registration
for this webinar today so anyone's
question I have still not answered
please paste it again now because I am
checking up the recent questions now and
there is huge pile up question for me
going back to the old question is not
possible now so if you can just leave
paste your question I would really
appreciate that and I'll I'll answer
that
Lakshmi gun saying do to learn talent do
we need to learn Hadoop concept - no not
really Lakshmana that's the beauty of
this course what talent do you have
videos updated in YouTube yes yes show
commodious villain says the villain says
STM is part of a line installation yes
both says do metadata provided ok Java
sister I think both you need to contact
it to make up for that and you know the
- what about Ireland + Hadoop I mean the
best option yes Rajesh that's the best
combination if you want to learn the
core Hadoop as well as how to handle
Hadoop smartly and Big Data best
combination
Keenen asking what is the overall
average time to roll out Thailand in
organization at approximate what first
theorem in fact I do provide corporate
trainings on these kind of areas as well
now average time that I see to train our
set of people in Tallinn is a month's
time okay to make them up and running
and in in terms of installation I think
if it's instantaneous but looking at the
ecosystem of any particular organization
I hope the rather say training and
installing everything to take up and
running amongst time and you've only
seen 10 okay I will power the
questionnaire II will I will come back
to it Ellen vs. Toyota was a sink sort
your take some came asking talent versus
a sort of assessing sort might take his
talent definitely a visitor
sooner definitely go for Alan because it
is more flexible in terms of what it can
do somebody said to pen down I have used
but now as well it's also a good tool
but in terms of flexibility and handling
different kind of data
Alan is fantastic the Lansing I am
Debbie a tenuous experience I want to
get into big data is starting with
Hadoop and institutional course a right
approach towards big data completely up
to you belong
if you want to exclusively be on the
high side then obviously what you are
saying is right but if you want to learn
overall big data along with bit of high
if you want to concentrate then talent
for big data is the best course gautham
asking talent can process PDF data file
as well yes my friend it can when data
stays 8.7 shaker asking data are still a
concern can support winte the how does
talent is different from the industries
of any other in theory tune it might be
repetitive question is elaborate your
answer no problem shaker and show you
one sample data shaker this is for
everyone okay
yeah this is the data data okay shaker
so if data stage eight point seven is
capable of processing the data and it
can support Big Data right now the file
that you see on your screen this is for
everyone does this finally look like any
structured file to anyone does it look
like a typical row column format file to
anyone can I get a quick answer Alan
will just process this file into
structured data in minutes
I begged tried in data stage that's my
take on it all right
so as I say Alan is very flexible in
terms of processing the data I have seen
other ETL tools also my friend trusts me
the beauty with which Dalian handles the
data is awesome its flexibility is to do
comparison to other detail do how can we
find in talent from sector all failures
comes in the in the run console in here
itself if you see okay let me see I just
ran one hang on there hang on there
I just ran one a pic result right big
job some people wanted to see a big job
how does it behave and all I just ran
one picture user
all right for all of you who have a
little bit of knowledge and big are
those who do not have knowledge and big
girl so I just ran one big job okay well
what I was doing is I am just reading a
very small file in pink I am doing
filtration in the same pipeline I am
filtering that data I am preparing one
file then s60 71 and I am aggregating
that data in the same pipeline and I'm
generating one more file all this zeros
have been made to 0-7 me too if you see
it created one reducer side right and it
has summed up it has three I'll also
show you which file it has taken up for
reading hang on there it is reading this
file user root input okay let us check
quickly user root input input is a file
and just show you what file I have taken
this is the file which I have taken in
my big scripting so the first value
which is ID if one link it has some de
the Sun here one plus two three is
coming in the reducer and in one it is
filtering the data
I am only taking the ID with well do one
okay if I go back again to use a root
servant made - if you see and one more
is big seven three one since I have only
filtered it has created one Napper okay
and the other one has created one
reducer are you happy for those who were
curious to know how does Talent supports
think can I get a quit smiling from
those who were curious to know how does
this works or does big works all right
thank you
okay okay let me take up some few more
questions before we wind up the session
in talent pools are that magician stuff
going to be covered
depends knowing what administrations are
talking about with the talent
administration or so Supriya says I have
two years of experience and application
support energon talent to work on talent
support something yes why not see the
primary motive of this course curriculum
is to make you make everyone a winner of
both the world even though let's say you
are still skeptical about big data still
you are about to take your hot scale on
Ln where you can do et8 you can do data
analysis you can do data integration
anything lot of projects they are
migrating from informatica to Tulane SSS
to Tulane Avenue sure to talent you can
become a continuous over there Abdul is
saying is it no scheme or don't know it
does generate other schema we will see
during the course how do we create
different schema how do we even create
dynamic schema you know Ln is also
beautiful tool in terms of accessing
venues a dynamic variable on the fly
during the runtime that's again one more
beauty which other ETL tool don't
support so treat is saying can you
please give me link or by direct Ln
present we're already taste regarding
isoprene the I think in YouTube also
they have pasted
most of the videos a few of the sessions
and in fact if you go to talent for Big
Data
course module in I do reckon you will
found you can find few of my videos over
there out alanine is better than Hadoop
Ln is using prairie Donna - yeah okay
Talon so say we're as I said in order to
find the failure if there is anywhere
here if you see here my my run console
and this is typically I had run one
MapReduce program okay if you see mine
unconsumed it is showing you most of the
MapReduce program in log all right so
all these logs are accessible and we can
read and we can redirect this loss right
let me see quickly some few more
question do we have any crash courses
like a 15 day course learning talent
yeah present now the host which we had
is not even 15 days it's just hardly
10-day course in divorce and each day is
three hours
Rosanna is asking 15 day crash course if
you are asking a continuous loop today
then definitely I would recommend them
to start off one yeah thank you for the
solution sedation soon as you saying can
you tell how secure are these tools for
banking project so this I just showcased
you one of the banking use case and
which is from one of my real-time
project experience yeah so don't worry
in terms of that because Hadoop has
already addressed through yon and other
security challenges in Hadoop to both
you can go to many of the area and they
can provide hill time I said a lot of
people are thanking for the webinar it
was great having you guys on board and
thank you very much for being here
anuraag saying I am a skill me a
developer typically working on exercises
and tableau or reporting we're learning
tile and help me oh well why not I can
always tell you I have also worked on
exercise SSRS SSS and other work I can
strongly tell you
talent is definitely much way ahead than
exercise yeah
does it process machine image video data
are not that I'm aware of ananka job
market scope is great and a rug
villain is seeing a very interesting
question can we read the invoice is
generated and hih is the data on it
yes and recently I just showed one data
villain you see this kind of data this
is what exactly I was talking about the
flexibility talent offers in order to
read this type of data is magnificent
this is actually of a block file all
right and talent is quite complicated
reading this kind of data
thank you very much it was great and
this is Assad my your webinar host and
your trainer signing off along with
iterator team thank you very much for
any enrollment queries or anything
please contact Erica
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>