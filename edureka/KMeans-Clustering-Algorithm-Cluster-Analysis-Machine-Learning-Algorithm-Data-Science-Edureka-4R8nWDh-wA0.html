<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>K-Means Clustering Algorithm - Cluster Analysis | Machine Learning Algorithm | Data Science |Edureka | Coder Coacher - Coaching Coders</title><meta content="K-Means Clustering Algorithm - Cluster Analysis | Machine Learning Algorithm | Data Science |Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>K-Means Clustering Algorithm - Cluster Analysis | Machine Learning Algorithm | Data Science |Edureka</b></h2><h5 class="post__date">2017-02-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4R8nWDh-wA0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone I'm rajma I'm
going to take you through the k-means
clustering so just a brief about me
before we get into this I've been in
this industry in the software industry
doing various kinds of STLCC activities
and currently leading that analytics
team and one of the MNCs
and I have been working on both big data
as well as data science and the
currently I'm dealing with both Big Data
and data science I have been on this
field of big data and data science for
past six years so this is a brief about
me let's get into the session today what
do we learn today so we will learn
introduction to machine learning we
learn cluster analysis types of
clustering introduction to k-means
clustering how K means clustering works
and a demo on are using in Netflix use
case so that could be lots of doubts are
waiting for information on the mission
learning so what is machine learning how
it is different from Big Data and how it
is different from the rest of the other
programming things so so machine
learning is something where we are
trying to learn from the existing data
which is not something like a
summarization sum of something count of
something count of some of the
attributes or summary of some of the
attributes is that primarily so it
basically starts with things like when
we do linear regression using Excel
that's one of the tools which can do
that so we are trying to take entire
data science and then trying to figure
out what will be a common equation
either being a linear or a quadratic or
financial or whatever it is so we try to
understand how the entire dataset
behaves and how close they are to the
model we are coming up and then we have
an option if we provide with new
attributes new sets of data it will be
able to predict how where it will
exactly fit so this is what machine
learning at a very very high level is so
for example one of the best examples is
that so we have these Gmail and Yahoo so
one of the biggest challenges is that
they have to
to isolate the spans from the normal
non-spam males right the males which are
actually targeted against you from the
actual people instead of the spammers
has to land your inbox and the rest of
the things has to get out of the inbox
and then gain a spam folder so what
happens is what happens initially they
categorize the sperms
and the hams whatever is non spam is
called hams
so they categorize that and then they
feed into these machine learning
algorithms and then they try to figure
out whether the model fits and so they
go ahead on regression on this to see
how close we can at you and then they
implement the system it becomes a kind
of filter before the Mail's comes to you
so what happens in this case as and when
the new mails are the news farms come in
they do an analysis on top of it and
then the system automatically adjusts
and then tries to filter it more
efficiently so this is one of the
classic examples we are going to see
much more examples in this session
suitors with the ability to learn
without being explicitly programmed in a
programming world you exactly say what
will be the input and what should be the
output that's where it stays but in a
machine learning as and when the data
comes the system adjusts itself to the
reality and then it behaves accordingly
so that is how the dynamic nature of
machine learning are the artificial
intelligences so in order to get into
this so what do we do this is the
highest level of how the system works so
we get the training data so when we say
we get the training data so in an
initial stage every data is like a
training data for us so what do we do
say if you have thousand samples so you
traditionally we split it into 60/40
70/30 or 8020 so 60% of the data or 70%
of the data using a sampling method has
been picked up because we cannot take
the initial 60 percent because that will
skew the entire model so we take the
sample of 60 persons randomly sample
them take the data and then be keep it
for training the model and then we keep
rest of the 40% as test which means so
for the text data we already have a
target attribute right we know the
outcome
so once the model once we create the
model with the training data so that's
your learner algorithm side we feed this
data into the learning algorithm and
then we get the output we get the model
so in the model what do we do that's the
third stage we're just building your
model then what you do you take the
training data remove the target
attribute and then pass it into the
model so this will give you an output so
now you compare the output which is a
predicted output with the actual output
now you get the inference you provide
the feedback into the learning algorithm
so that which means you tweak the
algorithm based on your inference and
then you again build the model so that
you try to achieve a better frequency
and k-means is one of the classic
examples where this iterative processes
explicitly visible so moving on so ml
use case which is a machine learning use
case for Google self-driving car what
happens with a Google self-driving car
Google self-driving car is math and it's
driverless currently you need to have a
person sitting there only for emergency
conditions but the rest of the things is
being done by the Google driving car so
what happens it has the cameras you can
see that it has the cameras at the top
and then so using the cameras it picks
up the environment so that's what it
does it picks the environment in its
totality through the sensors it has
other sensors too it takes the decision
like when to speed up when to speed on
when to overtake and when to turn so
it's like not only the camera you have
the other sensors also you will have
their Google Maps attached because it
needs to have starting place in the
final destination so it has something on
its artificial intelligence where it
knows from back to where it has to
travel it also gets feedback from the
other mechanism on which of the roads
are crowded and which of the clothes are
less crowded so it exactly knows the
direction and the roads which it has to
travel so what information it needs
dynamically the dynamic information is
how is the road ahead the census checks
for the road hog head computes those
information to decide whether how much
of space or how much of distance is free
right now so that whether it has to
increase it speeds or if it is
completely crowded and it has to reduce
it speeds and or if it has to take a
braking decision or it has to take a
turning decision so coming to the types
of machine learning there are two types
of machine learning
it's like supervised learning and
unsupervised learning so what happens in
a supervised learning you feed the
classifier with the training set and the
predefined tables that's what we have
been discussing right now so you know
from the data set which is being fed to
the model you know what are the
attributes and what is the target
variable so you know what is the outcome
so you feed that separately right these
are my attributes and this is what I
expect out of it so it will learn
categorize the particular data and the
specific model it does this algorithm to
figure out how the particular target
variable has been achieved based on the
input variables right so in this example
right you can see when and where should
I buy a house right so the features are
so buying a house am I going to buy a
house here am I going to buy a house in
North America West Europe Eastern Europe
those becomes buying a house in these
locations becomes your decision point
right buying are not buying is your
decision and what are the things which
actually makes your decision or which
helps your decision the house features
the area crime rate the bedrooms
distance to headquarters areas in square
feet and the locality okay
so you can see that this consists of
variables so those are the things which
comes into the picture there are that
can be numerical variables and there can
be categorical variables so things like
distance of headquarters areas in square
feet are all numerical so the area crime
rate
the house features the locality can be
categorical or if we can unless
otherwise you are able to convert them
into numerical so which means we already
deal with both categorical as well as
numerical variables right so based on
these when you feed the entire data with
these attributes and someone who has
already bought houses in that location
so that system starts learning if
something is bought in this location
these are the features so that is how
the system builds itself the model gets
created so if you go for the
unsupervised learning this is the case
where you don't know what is happening
there so there is no outcome on this for
example people getting into the mall and
then buying certain items right so we
don't have an attribute to say that this
person will buy these attributes at the
first shot so what happens we try to
understand who are the customers who
bought what right then we try to
classify them then we try to cluster
them and then finally we try to use this
as a predictive methodology for the rest
of the users so for example here an
image of roots is first fed into the
system right so the system identifies
different roots using features like
color size and it categorizes them right
so when a new float is shown it analyzes
its features and puts it into a category
having a similar feature right so this
is one of the kinds of classic initially
you start with something where you don't
have a target variable right and then
you cluster them you figure out a model
and then when a new fruit comes our when
your new attributes comes into the
system you classify into one of those
things which is already there so this is
the basic difference between the
supervised learning and an unsupervised
learning so if there are any questions
kindly message me on this chat and then
I'll try to respond as and when I see
that this is pretty much about the brief
about what machine learning is and what
types of machine learning algorithms are
what types of machine learning are there
priming the supervised and unsupervised
learning now let's move ahead into
cluster analysis so what is clustering
so primarily clustering means blooming
of objects based on the information
found in the data describing the objects
or their relationships so we have a set
of data and then we try to group them we
try to classify them say for example we
have people residing and there is a
cable connection going on if you want to
increase your chance of people
subscribing for this cable connection
you better understand the demography
there so that you can provide the
channels require channels to those
people target those people which means
right so if there is a particular
community which has particular language
preference right if you think that this
set of community has more of a Spanish
people or spanish-speaking people and
this community has more of a
french-speaking people and then more of
an english-speaking people it's better
you try to understand these classifiers
in order to target them better so
because you can have your set of
channels completely tied to them so that
those set of people can pick those can
subscribe for you else your competitor
gains a foothold there so this is one of
the reason we go for something like a
clustering so the goal is that the
objects and one group should be similar
to each other we will talk about this
more as we get into the algorithm so
understand this this will become
converted into mathematics here so the
goal is that the subjects and one group
should be similar to each other but it
has to be different from objects another
group right we will convert this exactly
into complete stat 6 as we get in so
that is the beauty of this machine
learning algorithm so you have to have a
tight community right and you call them
a group and there should be subsequent
difference or that should be good amount
of difference between the clusters which
means different groups if it is not
there how it is we will also see those
things ok so it deals with finding a
structure in a collection of unlabeled
data right so you see all the data to be
same in the initial phase and then you
try to find the structure of collections
within the
because at the first shot it's
completely unlabeled so some of the
examples of clustering methods are one
is k-means clustering which is what we
are going to deal today in detail then
it's Phi C R C means clustering you have
hierarchical clustering these are one of
the samples because there is so many
clustering methodologies and
particularly if you are going to use a
software like R so which has 4,000
packages right you are bound to see lots
and lots of clustering so there is a
question from Priyanka is clustering
when Leaford and supervised learning
know here we are going to deal with the
supervised learning only because we have
the target variables also so clustering
can be done on both supervised as well
as on supervised okay okay different
types of glistering the different types
of clustering here is like some of the
samples is that k-means clustering
leather is fuzzy means clustering c
means clustering or hierarchical
clustering apart from this there are
other sets of clustering also so these
are one of their samples so what we are
going to deal with this game means
clustering in order to explain for
nachiket I am just going back so what is
critical in this K means clustering or
any type of clustering is that the goal
of the objects in one group should be
similar to each other the objects within
the group should look similar and the
object in one group should be
significantly different from other
groups that's the crux of this
clustering so that's the main main thing
about the clustering moving ahead saying
we are coming to the clustering use
cases and these are some of the examples
it's being used in marketing widely okay
we will also see this as examples detail
examples so here discovering distinct
groups and customer databases such as
customers who make lot of long-distance
calls who are mostly on short distance
calls who are mostly on texting so
whatsapp has slightly changed these
things right this is some of the things
which we deal before the impact of
what's up what's up has slightly altered
these things
it's differently but still we do lots of
particularly for long distance calls and
for the other things like I is
the N and the broadband and all the
other things so discovering a distant
group of customer bases so that you can
target with you are features for those
customers so that they have a very high
hit rate of subscribing to your services
is one of the clear use cases so in
insurance industry so identifying groups
of crop insurance policyholders with
high average claim rate right so so it's
like things like you identify when it
becomes profitable when they make losses
and then is there a specific group which
is different on the insurance claims
which means right so what are the other
side effects of this you try to increase
premiums for those who has high average
claims and then you try to offer soups
for those who has less average claims we
know about the insurance in the auto
sector right these are some of the
things well they have to target for the
premiums and also it's one of the front
loaded stuff which means not like your
fixed deposits there are other things
where here the premium comes up front
and one leave and there is a loss or
something you go back and provide the
claims and the percentage of claims
should also remain less so that the
insurance company is not affected so
it's highly critical that we go the
insurance industry understands the type
of consumers to whom they're pitching
for the particular insurance policy next
its land use so identification of areas
a similar and land use in a ga AZ
database so this can be done for
multiple purposes right so the land use
primarily if you are going if you are
trying to create land banks and then you
are trying to provide a particular set
of industries and they're things like
cases where there are two thousand of
acres identified widely for bringing in
industries and then specific type of
industry so it becomes a cluster there
so for all those things and so it also
depends upon what kind of industry is
that what do they do
is it simply they use the land are they
pumping something into the land or they
extract something out of the land and
based on that the worst water source
should be nearby should the water shows
be far away or if there is anything
which is toxic which has to be taken
for all those things identifying the
land areas becomes critical so now even
India we have cartographic satellites
right which actually tries to find the
minerals below the surface at a certain
distance so once they identify they
decide what kind of operations can be
performed on the land use so all these
things the machine learning is playing a
vital role in identifying all those
things similarly these mix studies right
so identifying probably areas of oil and
gas exploration is based on the sea spec
data right similarly it's not only for
oil and gas exploration so they use it
for identifying shale oils right and
primarily try to see the crust below
which where the shale oil is available
and it's also helpful and identifying
the Seesmic zones so that proper
guidelines have given for building of
structures in those zones so machine
learning is being used in all these
areas okay getting no types of
clustering that is exclusive clustering
there is overlapping clustering and
hierarchical clustering so I think the
examples here the pictures here is the
kind of self-explanatory but getting
into the details right so we go for
exclusive class things are we are able
to identify exclusive clusters when the
items belongs exclusively to one cluster
and not several right Kame is thus the
sort of exclusive clustering very nicely
so what happens is out of the entire set
of data we are exactly able to say okay
these are the separate clusters right so
but sometimes you may not be able to do
because there will be data points there
are data points which lie on the border
and then it overlaps between two
clusters so item can belong to multiple
clusters its degree of association with
each cluster is known right so from the
center
how much is associated to the center is
completely known so far CRC means does
this sort of exclusive clustering so
this is where fuzzy clustering comes
into the picture the C means clustering
comes into the picture this is where we
have to use if the clusters are
overlapping hierarchicalclustering when
two clusters have a parent-child
relationship or a tree-like relationship
right so that's when you go for a
hierarchical clustering which means that
clusters in itself have a parent right
so that's when that is a relation
between the clusters that's when you go
for hierarchical clustering okay coming
to k-means clustering so k-means is one
of the simplest algorithms which uses
unsupervised learning method to solve
known clustering issues okay what
happens is in this game means clustering
one of the key things is that we don't
know how many clusters are there inside
right so what do we do is we try to
start with a number and then we try to
see them with multiple cluster points so
that the model goes ahead and splits the
entire population into slow many
clusters see to be very clear right so I
first start with cluster of two for the
entire population it tries to split the
entire population into two clusters and
then it comes up so then we try to see
it with three clusters right then we try
to see it with four plus so we increase
the number of clusters which is the K
value here so that is how this
clustering is done we do iterative
models in order to find which is the
appropriate cluster or where we have the
appropriate cluster right so that is
k-means clustering claim means
clustering in terms of inputs it needs a
number of clusters under the training
set so that is what it requires
so moving ahead right the one of the
example is there the Google News okay so
we can see a single topic containing
multiple links from different URLs are
different webpages this is one of the
classic examples of how the clustering
is done so various URLs related to trump
and Modi are grouped under one section
the k-means clustering automatically
clusters new stories about the same
topic into pre different clusters right
so that's what it does can we do the
clustering directly on this so that may
not be possible in the
particular case it might take us more to
do perform a k-means clustering because
news at traditionally unstructured data
so earlier BCI in types of structure in
types of machine learning we strong
supervised and unsupervised now we are
getting into another thing which is like
the data which is incoming it can be
either structured primarily if you are
retrieving it from a database or if you
are having something like a CSV file
right so where you know the exact number
of columns and then you have a list of
items which is like a record per sample
and then you have n number of rows in it
so columns becomes your attributes and
the rows becomes the data points for you
you can also have semi structured data
so semi structured data is in something
where you get the outputs from the web
service because to an extent the initial
portions which webserver it is coming
from which web server is viewing that
output what time it was and what men
other things like if at all there are
process ID or if there are anything
associated with if it's a slave or a
master or the IP address the initial
portion is usually kind of specified the
positions are specified or sometimes it
can be like key value pass it says time
is equal to so-and-so webserver is equal
to so-and-so app server is equal to
so-and-so so even if the positions are
changed you will be able to extract that
right so key well because I won K once
again can be spanning in two different
times it can be document based or it can
be straight key will be phase based but
after that you will have a free flowing
text so what do you do there in order to
convert that you take all the things
which are specified and then you take
the free flowing text but we definitely
know what form are it is going to be and
you have the other set of data which is
completely right it's like unstructured
data some of the locks can be
unstructured and similarly the news
items are the blocks are completely
unstructured because you don't have any
specific format and you cannot split
them into records so what do we do in
that in that case we first apply the
natural language processing or the text
mining
we bring those data in tourism in
dimensional space and using each and
every vector in the n-dimensional space
you go and map for the similarity that
is when you start applying things like
cosine similarity Euclidean distance
Manhattan distance jacquard is
coefficient so you have so many methods
by which you can find once you are
converted the text into the vectors I in
n-dimensional space you can do the
distance matrix between two different
types of data and then based on that you
can perform clustering in this case it's
not a single step process it's
multi-step process before we do a clear
means questioning but ultimately we do
the clustering so that we can group
items of similarity in this particular
case like the news items in particular
and we give the ones which has a maximum
hits at the topmost and then we also
list our google list all those things
which are similar and from different
URLs things like you can see here like
entity view of the independent of the
Guardian okay so the examples right so
if you want to one of the target here in
this particular slide is that you want
to place a school and then you want to
find a specific location to build
schools in this area so that the student
doesn't have to travel much so that is
the target so what we do we try to get
the plot of location of the students
from where they are traveling from the
existing their students data set way and
then we plot them and then we try to
find in which locations if a school is
placed that kids will be able to travel
faster or they have less time to travel
so here we have take know certain facts
like there will be parents who want to
send their kids to a particular score
irrespective of the distance but the one
of the main things which are trying to
attack is they're assuming that the
students are traveling to a farther
distance because they don't have a very
good school nearby you can see the
expansion of the same schools who have
been traditionally having one or two
schools in the entire city but now they
are spreading their wings across the
city this is before they decide where to
place the schools there are other
constraints to position the schools but
it should be near the very big roads and
other things part of all those
attributes
they also look for which other students
are what kind of students can come to
their school with a lesser distance so
that's one of our target attributes so
anyone who goes for these kinds of
branding or the business they will be
able to look into or they will be able
to take help from the clustering from
this adds itself because it has been
visually plotted we can kind of
understand where the school should be
and what kind of centroids are there
right you can see if you do boom
mathematically we'll be able to find
where the school should be positioned
and how many schools are needed for the
entire mass of this children so you do
multiple iterations and finally you come
to your conclusion that yeah this looks
good so now getting into the nitty
gritties of how it was done right which
means we have to get into how K means
works so how K means work it's like you
choose the number of clusters and
definitely we are going to iterate on it
right
so the wfs is find as the sum of the
squares distance between each member of
the cluster and its centroid this is our
good old with distance matrix which we
have studied in your variant classes
right it's a summation of it's primarily
the you calculate the variance right P
IJ minus Q I J and then you square it
that's your variance so particular you
do it for J and you do it for I it's a
square the sum of the squared distance
between each member that's what it does
how do we do it we start with one
cluster one cluster doesn't make
anything right so we start with two
clusters and then we see what is the
width in sum of squares for that you get
a sum within sum of squares and then you
go ahead and do the prediction two three
four five clusters and then you find a
position after which you don't want to
do for the clusters yes the clustering
with four works but the incremental
benefit we get out of it is very less so
this becomes your elbow point which is
three the number of clusters which is
three you can see that P is the data
point right and Q is the closest
centroid to the data point so those are
those two variables the idea of the
album with third is to choose the K
after which the hair WSS decrease is
almost constant that means the
incremental decrease in
within sum of squares if it is almost
negligible right that is when you stop
increasing the number of clusters
because the cost of increasing the
clusters and taking a decision in this
case the skooled right should you go for
if you put this in the question of
creating the number of schools should
you do two schools for the cluster three
schools for the cluster four schools for
the cluster five schools for the cluster
right so you see that there is a
significant benefit significant
optimization in the cost when the K is 3
that is three schools but if you will
start placing four five six schools read
the cost increases exponentially and the
benefit decreases drastically you don't
have that much benefit when you go for
higher clusters so that is how important
the K is so K becomes your decision
point so how do we initialize it we
randomized the initial K points call the
cluster centroids because that's what we
are seeing right here we start with K is
equal to 2 and the value of K can be
determined by the elbow curve as we
explained with the previous line right
so with 2 you try to start so what you
do we have to remember that the
centroids are a random points right now
you compute the distance matrix for
every data point that's why P I is the
data point so you compute it for every
data point and you computed for the
closest point in the center the closest
centroid for the data point right as you
do those rights you start with the
centroid and then you keep computing it
which means the model actually does that
and it achieves at a final centroid
right so you can see that this is how
the cluster assignment happens compute
the distance between the data point and
the cluster centroid depending upon the
minimum distance the data points are
divided into two groups right now
because you start with this cluster and
if you compute this this is how it will
look like right so you can see that so
all the data points which are closer to
the RH cluster are spotted here and all
the data points with a closer to the
blue-blue centroid are spotted here
right so now what happens is as you
compute for the centroid moves
the center moves closer and closer which
means it actually moves towards the
actual centroid right compute the mains
for blue dots and the reposition the
blue centered the cluster centroid to
this mean okay similarly do it for RN
start so the migration happens
iteratively within the model as you move
forward this is what has happened as you
move that repeat previous two steps
iterative later the cluster centroids
start changing their position at some
point the centroid doesn't change for
further computations that is when the
model converges so the model has
converged for these two clusters and
these are your boundaries okay so this
is one of the things there are other
models things like support vector
machines which actually tries to
increase the distance between these two
things right so it finds the support
witches where there is maximum distance
between the clusters so there are models
to do that so let me take a couple of
questions here so nachiket provider is
asking how should I initialize the
centroids so propagate this is this is
what I said you start with the random
points so you don't worry about where I
have to position my clusters as and when
it does it relatively as I was showing
here right so you can see that it starts
at any random location so particularly
in this example right they have taken
diagonally opposite positions where we
are not biased towards the cluster right
you can see that though these two things
are the cluster visually for us the
computer doesn't know about it so they
have picked up the points which is
completely tangential do it right and
the cluster automatically with the
iterations while computing the new
centroids it will automatically move to
the appropriate position so to answer
your question is a simple random
selection okay Priyanka has a question
how are further new centroids found and
further optimization what's the formula
behind the rocker and the prabhakar has
a question can I choose the centroids at
different positions so particularly in
this K means model right you can see
that we are not sure
using the initial random positions so
let me go back okay so the k-means if
you see here right the k-means
clustering go has only these two things
you have to tell it what is the number
of the clusters right and you had to
give the training set item these are the
only things only to see that if you are
going to choose the points right then
you are going to bias the decisions so
the model should not be biased by any
decisions if it has to perform in any
situation right that's the key part of
machine learning if you are going to
bias it so it's like a garbage in
garbage out so if you are providing a
garbage in what you get is a garbage out
right machine learning is not for taking
decision machine learning is primarily
for understanding so if you are chosen
to give and the other thing strength the
model should have is if you are biasing
it it should be able to come out of it
and then provide a same kind of output
which is unbiased right so that's the
case as you can see k-means doesn't have
any means to take the initial random
points it picks its own random points
that's the cracks you only tell how many
clusters you want and you provide the
training set so you don't have an option
to do that I hope I answered both
Priyanka's educates the questions should
be a message if you think that I haven't
answered you so this is primarily random
points it can be picked up from anywhere
but the model has to be robust enough to
find its centroid because on multiple
iterations irrespective of you if at all
there is a provision for you to give an
input point right it should converge at
a place where it has supposed to
converge so the random points become
important when your model is not capable
of finding global maximum or global
minimis Priyanka has after initial
random centroids
how are the new centroids form okay
that's within the model the centroids
are found by the model and iterates and
then it moves the centroid so you don't
deal with the centroid at all because
you don't give the initial centroid the
initial centroid is picked by it
randomly and then it computes the
distances and once it computes the
answers because you have given ke which
is you are saying that it has to be do
clusters so from the two clusters right
it forms two clusters in this case you
can see that so it forms two clusters so
for these clusters initially it was like
this so there is no particular thing so
use the pickoff so you calculate each
and every distance matrix here but once
you have calculated you know that these
are the points which are closer to this
and these are the points you closer to
it so you find the centroid for this
data set right and then you move your
current centroid which ever we are using
to as the centroid for this data set so
the centroid for this data set varies
from this position right
similarly the centroid for this data set
varies from this position and that is
when your centroids move here so now you
do the computations again so what
happens in certain cases right so these
points will come within the centroid
earlier it was orange now on the second
or third iteration these points can
become blue similarly these points can
become RH right so that's how the entire
cluster changes over the iterations so
once you find the next set of things
which are closer then you compute the
centroid again so it moves again closer
to the actual right to what it has to be
similarly this moves again closer to
what it has to be so that is when these
points are and the third or fourth or
fifth iteration what happens these
things start becoming orange because
your centroid has moved in the
northeastern direction or the eastern
direction right this moves in the
western direction so as and when this
moves in the west cent direction you get
more points which are closer so you
recompute the centroid so that's the
crux the threshold will be there
internally within the model so if the
movement of the centroid doesn't exceed
it
that's when you see it does converge
okay thank you Priyanka okay so this is
your optimization and the final step is
the convergence right so finally the
k-means clustering algorithm converges
right which means it divides the data
points into two clusters clearly visible
in our engine right so this is how the
convergence happens
then the model stops so nachiket is
asking does the clusters formed depend
on where the centroids I don't realize
know right it should not be that is that
should be the strength of the model
because nothing should be the starting
point should not be a biasing case all
right but there are technicalities here
right because if you go deeper into it
retching are we talking about the global
minimums are the local minimums right so
those are the questions which cannot be
covered just within the k-means yes so
we have to jump out if you have stuck in
a local minima so which means if you
have to find the local mean and there
are models which can do it either our
models which cannot do it but for your
question the initial starting points
will not decide the clusters so problem
statement so this is when we are going
for the example so challenges Netflix
wanted to increase its business by
showing more popular movies on its
website so once again right because the
problem statement is is there it becomes
very easy for us to proceed but one of
the biggest challenges in this industry
which is the data science industry is
defining the problem itself is a big
problem right so our aiming at what do I
want and what kind of models I have to
go after is in itself assigns here
okay thanks educate so solution so next
fix decided to group the movies based on
the budget
gross and face books which means from
the whole set of data available you have
already narrowed down to the attributes
which I have to look into so that I can
increase my target audience right so out
of you can see once I bring up the
example right out of 23-yard attributes
we have narrowed down to two so but how
do we do this that in itself is another
part of data science here so you do
something like one of the things which I
was saying is that SVM there are things
like principal component analysis where
you analyze which attribute
significantly increases my or increases
or decreases my target variable right
there are other data size model so we
are not going there but somehow we have
figured out from the entire list of
attributes we have chosen but just
and cross so these are the two
attributes which are picking and so that
we can go for a grouping so approaches
for this Netflix to a guy NDB dataset
right which is available it has taken a
five thousand values it applied k-means
clustering to the group so the target is
i want to find the groups and from the
groups i want to find which is the best
possible group right that will be my
target so this is how the script looks
like i will just run this right we take
the movie data set right we find the
dimensions of it we convert that into a
matrix right we omit the things where
there are values there are no values we
get just 500 samples out of it and then
we pick the columns this is your cross
and what is the other one yeah budget
and gross so you are 9th column and your
twenty third column accounts for the
budget in the cross so from that we
create a data matrix and then this is
where we provide inputs the k-means is
iterated from the k value you can see
the center's right which is it is
iterated from 2 to 15 which means I am
finding within sum of squares all the
way from k equal to 2 to k equal to 15 I
am finding those things and then I am
plotting I am finding the elbow and then
I am deciding which should be the target
proof okay let's go to the are coding
okay this is the same thing Josh shown
in the presentation right these are the
nitty-gritties of our I am whatever I
have I am removing all the values here
so whichever is an environment variable
right and then so I am setting the
working directory this is my working
directory this is how simple it is to
read the movie data so you have a movie
movie metadata dot CSV right so I don't
have to open it in a CSV because you can
see it right now it's as simple as to
read a particular data set into the
memory right and it's already read and
you can see that just by clicking you
can see the data and if you see here
right it tells you what this data is
right so it has five thousand forty
three rows and it has 28 attributes 28
variables in C attributes
the color the director name the number
of critic reviews the duration all those
things and your 90 field is nothing but
your gross and you can see that your
23rd field will be your budget so this
will be the budget so these are the two
attributes we are just I'm saying ahead
so this gives lots of information about
the data set you have taken and when you
are just clicking this you can see that
it launches a comment which is movie of
which is view of a movie made data
getting further right the dimensions
right so I am trying to see the
dimensions so it it gives these
variables here whatever is here it says
that it concerns are for the five
thousand forty three rows and twenty
eight columns so I am converting this
into a data matrix so that I can perform
certain further operations way so I am
converting that into a data matrix and
then the n8 automate does that it
removes so when we saw this we found
that there were few NS here right so
this is an NA which means for the number
of critics for this particular movie it
is not available so holes like this will
create skew and are sometimes the error
will be thrown by the models the holes
cannot be there in certain models right
certain models are sensitive to what
seven models are okay with very sparse
matrix so in this particular came ins we
cannot have that so we are moving this
row who if at all we have a row or with
an empty column then that that entire
row will be removed so that's what is
happening here so now if you see the
sample right so I have to run the sample
what did I do
out of the five thousand forty three
rows I have used the random sample so I
am picking only random 500 records right
so I have only 500 dataset that's what I
am doing right now
right and I'm storing it as an sample
now I am viewing the sample so which
means you can see right now it is
showing all the 500 entry is right now
so this is what we have right now and
this is the data set on which we are
going to operate out of those 500 I am
interested in only columns two columns
which is nine and thirty three which is
and budget what I am doing from the
sample I am picking only those two
columns and now if I do a view you can
see that you have 500 entries right and
you have just two columns which is cross
and budget and this is what I am going
to use for performing the model okay I
am converting that sample into a data
matrix because that's needed and now I
am going ahead and computing so you can
see that I am taking the sample variance
and sample matrix and then I am applying
the variance for every column so I'm
individually this is what the apply
apply - right and I'm computing the
width in sum of squares so now I am
computing within sum of squares right
and
applying a model is the most simplest
piece in the data science world the 70
to 80 percent of the work goes on
pre-processing and then it's primarily
on how to find the data how to pick the
data of your choice how to remove the
holes and there are other things like
how do i standardize because I cannot
have data of varying prices or it's like
bearing you can call it amplitudes or
for example if I can say right so one
data if I can do a summary it will show
summary of sample matrix right if I do
that it will tell you so right you can
see that the minimum is 1111 and the
maximum is 43 and odd whatever it is but
you can see for the my budget right it
starts at 30k right and it goes all the
way up to a four billion dollars
so sometimes k-means is agnostic to this
so it can handle this or sometimes
certain models we have to standardize
with means so we will convert this
either between 0 and 1 min and Max will
be minimum will be 0 and maximum will be
1 we can go that is like our range santa
session using range or we can do a z
range which main will be 0 and you will
have 1 Sigma 2 Sigma 3 Sigma as the
minus 1/2 Sigma 2 Sigma 3 Sigma as the
minimum and plus 1 Sigma 2
three sigma as the maximum so my
apologies this this stats are minimal
when you get into the data size well so
we have to understand what a standard
deviation is here so and that is what it
is but it's simple when anyone should be
able to learn if if you are able to
recollect your tenth max so so that's
what it so this line it does lots of
things it does the k-means clustering
from where k value is equal to two and
all the way up to k value is equal to 15
it retrieves there within sum of squares
value and then it positions within those
WS s so you can see that today again a
small sample you should happen it
happened so now we are plotting the
within sum of squares values because
that's of importance for us today
outside all the other things and then
now this is a simple line which takes
what should be their values and what are
the things which are going to plot it
talks about the type right and this is
what will come in your x-axis which is
the X lab and this is what will come in
your y-axis which will be your Y lab
right so when I plot it right this is
how you compute your sum of squares
within sum of squares you can see that
it starts at two what three the elbow
point actually reduces between three and
four the Delta is very less so that's
how you say that K is equal to three is
one of the good things okay so once we
do this right we got the elbow plot
right when you are saying how good these
clusters are right so we can see that in
on a distribution right we see that this
is how the clusters are arranged
particularly for between some of the
squares to total sum of the squares so
actually if I have to take between some
of the squares within some of the
squares and told some other squares that
itself is a session and that's forms the
basis for most of the influences right
so let's keep it simple we are achieving
something 2% here right the higher the
percentage value better as the model
right so k-means clustering is the best
so let's relate to the cluster
assignment to individual characteristics
like director Facebook Likes and other
things when we perform these things
right the cluster two has a maximum
movie likes as well as the director
likes we can see that right so when we
do for all the other attributes right
this is what we are seeing so plus r2
has the maximum and when we do a
aggregation there right if I want to
know the profit values of the movies
then we have certain parameters we say
which says if you look for the centers
it will tell what will be the predicted
values so and you see that cluster two
has the maximum there so that's how I
decide I will go with a cluster to its
making maximum profit and the maximum to
Facebook Likes that's all in this
session on the k-means clustering and if
you can find the positive comments from
individuals and then you have a it
record at assigns the course already you
can subscribe and you can learn thanks
everyone for joining this session was my
pleasure to explain your data science
and k-means clustering is hope you learn
further with error it or occur on data
science wish you all the best I hope you
enjoyed listening to this video please
be kind enough to like it and you can
comment any of your doubts and queries
and we will reply to them at the
earliest do look out for more videos now
playlists and subscribe to our Erica
channel to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>