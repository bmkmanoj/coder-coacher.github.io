<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning Tutorial | Deep Learning Tutorial for Beginners | Neural Networks | Edureka | Coder Coacher - Coaching Coders</title><meta content="Deep Learning Tutorial | Deep Learning Tutorial for Beginners | Neural Networks | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning Tutorial | Deep Learning Tutorial for Beginners | Neural Networks | Edureka</b></h2><h5 class="post__date">2017-06-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nl_4WFHQ4LU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone the Sasori from le Rekha
and welcome to today's session on deep
learning tutorial with tensorflow
so let us move forward and our look at
the agenda for today so this is the
agenda for today guys will understand
what exactly is deep learning and how it
works so we look at the motivation
behind deep learning and I would
actually vote then will focus on single
layer perceptron which are nothing but
the early deep learning models will also
look at various examples a singular
perception and guys when I talk about
perceptron these are nothing but the
artificial neurons and what are neurons
neurons or your brain cells then will
also understand the limitations of
singular perceptron that led to the
evolution of multi-layer perceptron well
understand how multi-layer perceptron
overcame those limitations and we'll
also look at various examples of
multi-layer perceptron in the beginning
I will be also talking about the co
structure how the course is structured
what our topics will be covered in what
all modules so I hope you all okay with
the agenda just give me a quick
confirmation by writing down in your
chat box or if you have any questions or
doubts you can ask me Akshay's he is
clear or so does Emma Emraan neha
pragati puja jack or at fine guys so
everyone is clear so let us move forward
and see how the course is structured
what are the various modules so this is
our course looks like I will start with
the introduction to deep learning that
is how deep learning evolved what are
the algorithms that were present before
deep learning then we'll understand the
fundamentals of a neural networks dural
networks are nothing but the networks
that try to mimic your brain then will
understand the fundamentals of deep
networks deep networks are nothing but
complex neural networks or you can even
say that these are subsets of neural
networks then we'll introduce you to
tensor flow which is nothing but a
library of Python that is used for deep
learning then we'll focus on various
types of neural networks versus
convolutional neural networks are is
used for image recognition then we'll
focus on recurrent neural networks and
then finally our BM and auto-encoders so
this is how our as course will be
structured fine guys let us move forward
and understand what all things we have
understood so far so guys these are the
things that we have studied in the
previous session we first understood why
artificial intelligence what is the need
of artificial intelligence then we saw
what a
exactly it is and we also focus on
various subsets of artificial
intelligence that is machine learning
and deep learning then we understood how
we came from machine learning to deep
learning and we saw what exactly is deep
learning along with various applications
of deep learning so guys we'll move
forward now and understand what exactly
is deep learning and how it works so
guys let us understand deep learning
with an analogy so how do you think our
brain is able to identify the difference
between a dog and a cat the reason is
since the day we are born we are
actually seeing different types of cats
and dogs in a day to day life and
because of that we are able to identify
the difference or spot the difference
between the two that is dog and a cat
even if we see different types of cats
and dogs we still know which is a cat
and which is a tall so this is because
we have seen a large cats and dogs in
our entire life but what if I want a
machine to do that task for me so how
will a machine identify whether the
given image is of a dog or a cat so one
way of doing that is we can train our
machine to a lot of images a lot of
images of different cats and dogs that
is different breeds of cats and dogs and
then what will happen once a training is
done we can provide it with an input
image then we'll manually extract
certain features features like nose
whiskers colors edges it can be anything
the important features which actually
helps us in classifying whether the
input image is a cat or a dog then we
make a machine learning model with that
and once it is done our machine learning
model is able to predict whether the
input image is of a cat or a dog but if
you notice here one very big
disadvantage with this is we have to
manually extract features features such
as nose whiskers all those features we
have to manually extract it and provide
it to our machine learning model and
trust me guys in every scenario it is
not possible if you have large number of
inputs you cannot do that you cannot
manually extract the features or
basically you can say columns which are
important for you in order to predict
what's the object is right and that led
to the evolution of a deep learning so
what happens in deep learning we skip
the manual step of feature extraction so
what we do we directly take the input
image and feed
it to a deep learning algorithm and
because of that what happens whatever
features are we already manually
provided apart from that there might be
many other features which are important
for example if our features don't
include the length of the neck and that
is one of the major feature in order to
identify or classify whether it is a cat
or a dog now what will happen our
algorithm will automatically determine
that feature and we'll take into
consideration and then it will classify
whether the input image is a cat or a
dog so what happens here we provide an
input image of a dog and then it will
automatically learn certain features
even if we don't provide it with those
features and after that it will give us
the probability and according to this
particular scenario
it says 95 percent chances of being a
dog 3 percent chances of some other
animal similarly 2 percent chances of
some other animal so since the highest
probability goes to the dog so the
prediction says that the object or the
input image is nothing but a talk so I
have a question popped on my screen it
is by Siddharth is asking deep learning
algorithms are different than machine
learning algorithms those of that not
always even in deep learning we have
supervised and unsupervised learning
algorithms but the major difference is
in deep learning we have more number of
hidden layers or you can say we have
more number of inputs and because of
those inputs there is a very important
concept that is called feature
extraction which allows the algorithm to
learn certain features even if we don't
explicitly provide it so this is what
the difference is the algorithms are
same supervised unsupervised even in
deep learning we use those kind of
algorithms I hope this answers your
question so that's a yes and we have one
more question
so is it a better version of machine
learning yes it is definitely a better
version of machine learning now always
there are two sides of a coin when I say
it is a better version of machine
learning is because if you have that
Percival's computational power then
definitely deep learning is a better
option and at the same time if you have
large number of inputs then also deep
learning is a better option than machine
learning but if you have small amount of
inputs and with less computational part
then definitely go with machine learning
alright so any other questions any other
thoughts guys you can ask me all right
so everyone's clear
I have one more question this is from
Neha she's asking what do you exactly
mean by feature extraction alright fine
now they I'll explain it to you now
believe this example and I will explain
it to you with a different example
suppose I have image of a human being
itself right
and I want my goddddd them to predict or
identify that particular image now what
happens in machine learning you have to
train your model with all possible
features of that particular humans face
for example edges sharpness colors all
those things you have to provide all
those features in order to make
prediction or in order to identify that
particular image in machine learning but
when I talk about deep learning the deep
learning algorithm itself generates
those features for example sharpness and
things that I've discussed and then it
will give you the output because many
times you are not able to provide all
the features right if I want a model
that can predict any sort of image that
can identify any image that is there in
the whole world so I cannot provide it
with all the images that are there in
the whole world I cannot train that
model with all the images that are
present everywhere right that is next to
impossible I cannot load in all the
features at once so that's when we use
deep learning to learn certain features
on its own with a little bit guidance as
well this is what the feature extraction
is I hope this answers your question all
right she's happy with answer fine guys
so we'll move forward and we'll
understand how exactly deep learning
works so the motivation behind deep
learning is nothing but the human brain
as we have seen in the previous analogy
as well what we are trying to do we are
trying to mimic the human brain we are
trying to mimic the way we think the way
we take decisions the way we solve
problems we are trying to make sure that
we have a system that can make our own
brain so obviously the motivation for
deep learning has to be our brain and
how our brain work with the help of our
brain cells which we call neuro now let
us understand how a neuron words and let
me tell you guys this is what we think
how a neuron works this is what our
studies tell us say yeah so these are
called dendrites so dendrites receive
signals from other neurons and then it
passes those signals to the cell body
now this cell body is where we perform
certain function it can be a Sigma
function that's what we believe we
believe it perform the Sigma function
that is nothing but sums all the inputs
then through axon what happens these
signals are fired to the next neuron and
the next neuron is present at some
distance and that distance is nothing
but synapse so it fires only when the
signals coming from the cell body
exceeds the particular limit then only
the cell or this neuron will fire the
signals to the next neuron and this is
how the neuron at the brain cell works
and we take the same process forward and
we try to create artificial neurons
so let us understand why we did
artificially wrongs with an example so I
have a data set of flowers say and that
data set includes sepal length sepal
width better length and petal width now
what I want to do I want to classify the
type of flower on the basis of this data
set now there are two options either I
can do it manually I can look at the
slider manually and determine by its
color or area means and I can identify
what sort of a flower it is I can train
a machine to do that now let me tell you
the problem with doing this process
manually first of all there might be
millions of inputs that will be given to
you and for a human brain to perform
that particular task is next to
impossible and at the same time we
always get tired at some point in time
right so we cannot just continue working
for a long period of time in single
stretch and the third point is human
error risk which is always there so
these are few limitations with the human
brain so what we can do we can train a
machine to do that task for us or we can
put our brain inside a machine so that
it can classify the flowers for us so
with this what will happen the machine
will never get tired and and will make
better predictions as well so this is
why we create artificial neurons so that
there is a system prison that can mimic
our brain and this is how this is what
exactly happens so this particular
artificial neuron can actually classify
the flowers or you can say can divide
the flowers on the basis of certain
features in our case it is sepal length
sepal width petal length and petal width
so on that basis it can classify the two
flowers so what we need here we need
some sort of a system that can actually
separate the two species and what is
that system is nothing but an artificial
neuron so we need artificial neuron and
one type of artificial neuron is a
perceptron now let me explain you
perception with the flow diagram that is
there in front of your screen now over
here what happens we have set of inputs
like x1 x2 - - - and except now these
inputs will be multiplied with their
corresponding weights which is w1 w2 w3
till WN in our case now these weights
actually define how important our input
is so the value of weight is high we
know that this particular input is very
very important for us after
multiplication all of these are some
together and
it is paid to an activation function now
the reason of using activation function
is to provide a threshold value so if
our signal is above that threshold value
a neuron or you can say our perceptron
will fire else it won't fire so that is
the reason why we use an activation
function there can be different types of
activation function there can be sigmoid
they can be step function sine function
depending on our use case we define the
activation function now the main idea
was to define an algorithm in order to
learn the values of the weights which
are w1 w2 w3 in our case to learn the
values of the weights that are then
multiplied with the input features in
order to make a decision whether a
neuron fires or not in context of
pattern classification which is in our
case large so we are trying to classify
the two species of flowers such an
algorithm could be useful to determine
if a sample belongs to one class or one
type of a species or another class or
another type of the flower we can even
call perceptron as a single layer binary
linear classifier to be more specific
because it is able to classify inputs
which are linearly separable and our
main task here is to predict to which of
the two possible categories a certain
data point belongs based on a set of
input variables now there is an
algorithm on which it works so let me
explain you that the first thing we do
is we initialize the ways of the
threshold now these weights can actually
be a small number or a random number and
it can even be 0 so it depends on the
use case that we have then what we do we
provide the input and then we calculate
the output now when we are training our
model or we are training our artificial
neuron we have the output values for a
particular set of inputs so we know the
output value but what we do we give the
input and we see what will it be the
output of our particular neuron and
accordingly we need to update the
weights in order to reduce a loss or you
can say in order to reduce the
difference between the actual output and
the desired output so what happens let
me tell you that so we need to update
the weights so how we are going to
update the width is we can say the new
weight is equal to the old weight plus
the learning rate learning rate we'll
discuss about it later in the session
but generally we choose learning rate
somewhere between 0.5 to 0.7
after that what happens we find the
difference between the design output and
the actual output and then we multiply
it with the input so on that basis we
get a new weight so this is how we
update the weight and then what happens
we repeat the steps two and three so we
are going to repeat the steps two and
three that is we are going to apply this
new weight again we are going to
calculate the output again we are going
to compare it with the desired output
and if it matches then it's fine
otherwise we are going to update it
again so this is how the whole
perceptron learning algorithm would and
don't worry guys I'm going to show it to
you practically how it actually works in
my pie chart any questions any doubt
still here you can ask me so we have a
question from Emma she's asking how the
artificial neuron is similar to the
neuron that we have in our brain all
right so that's a very interesting
question so let me go back to the
perceptron structure that we have so
yeah so over here what happens you can
consider these points as your dendrites
say similar to your neuron the brain
cells what we have we have multiple
dendrites through which we receive
multiple inputs so you can consider
these as dendrites and through dendrites
it reaches to a cell body so this
transfer function can be considered as a
cell body so in this cell body would
happen sum function is performed in our
case it is a Sigma function then what
happens it travels through the exon if
it reaches to a particular value or it
exceeds that particular value or the
threshold value then only our artificial
neuron will fire or you can say a
perceptron will fire so this is how we
can actually relate it to the Derron or
the pain cell that we have and obviously
we don't use any of these terms so yeah
this is how we are trying related with
the actual neuron so I hope this answers
your question Emma all right she is
happy with the answer sine geyser will
move forward and we will see the various
types of activation functions
so these are the various types of
activation functions that we use
although there can be many more
activation functions again I'm telling a
depends on your use case so we have a
step function so if our output is
actually above this particular value
then only our neuron will fire or you
can say the output will be +1 or if it's
less than this particular value then
we'll have no output or we'll say zero
output
similarly for the sigmoid function as
well and same goes for sine function so
our very interesting question from
Amelia she's asking why we choose only
these sort of activation functions or it
does a very good question now let me
tell you what happens we are actually
since we are trying to mimic our brain
and our studies tell us that the same
activation function is actually used in
our brain as well especially the sigmoid
function so our studies or the doctors
who have done the studies they tell that
the sigmoid function is what is there
present in our neuron in our brain so we
try to mimic that same thing and at the
same time the reason for choosing
sigmoid function is that it is easily
differentiable and if you differentiate
it twice you will get back the same
sigmoid function I hope this answers
your question Emilia alright she's happy
with the answers fine any other
questions any other doubts guys you can
write it down in the chat box and I'll
be happy to help you any questions or I
find there are no questions so let us
move forward and we'll focus on various
applications of this perceptron now as
I've told you earlier as well it can be
used to classify any linearly separable
set of inputs now let me explain you
with the diagram that is there in front
of your screen so we have different
types of dogs and we have horses and we
want a line that can separate these two
so our first iteration will produce this
sort of a line but here we can notice
that we have error here as we have
classified horse one of the horses as a
dog and a dog as a horse so error is two
here similarly we now what happens we
have updated the weight
now after updating the weight what
happens our error has reduced so what
happens now we have actually classified
all the horses correctly but one dog we
have classified wrong and we have
considered it as a horse so our error
becomes one once again what will happen
if you can remember the step two and
three of our perceptron learning
algorithm will be done and then after
that our weights will be updated and our
desired output will become equal to our
actual output and we get a line
something like this
so what we have now we have properly
classified dogs and horses
so this line separates both of them this
is how we can actually use a single
layer perceptron in order to classify
any linearly separable set of inputs now
we can even use it in order to implement
logic gates that is all Anton now let me
tell you how you can do that first we'll
look at or gate now in order gate what
happens here is the truth table truth
table according to that we have two
input X 1 and X 2 so if both are 0 we
get a 0 if any one of the input is high
or any one of the input is 1 we get the
output as 1 so what we need to do is we
need to make sure that our weights are
present in such a way that we get the
same output so how we have done that
when the value of beta is equal to 1 and
then we provide the input X 1 and X 2
after passing through this activation
function we get this sort of a graph
which is the graph for R or a gate now
if you want I could use a pen and I can
explain it to you now we have X 1 and X
2 as inputs so we provide first input as
0 X 1 and this also has 0 so 0 into 0 is
again 0 0 into 0 is 0 and then we passes
through this activation function now we
need to make sure that whatever value
that comes here should be greater than
0.5 then only our a neuron or the
perceptron will fire but since this
value is actually less than 0.5
so the neuron won't fire and our output
will be zero as you can see it over here
now let's take the other set of inputs
now our x1 is 0 and our x2 is 1 so 1
into 1 will what will be obviously 1 now
in this case our output is bigger than
0.5
then what will happen our neuron will
fire and we'll get 1 the output 1 here
as you can see it in the graph similarly
when my X 1 is 1 and x2 is 0 in that
case also our output is greater than 0.5
a neuron will fire and we'll get this
sort of a graph when I talk about 1 1
then our output will be 2 which is
greater than 0.5 so we get an output
that is 1 and this is how we get the
graph so I hope you all are clear all
right so this is how we get this graph
and now if you notice with the help of
the single a perceptron we are able to
classify the ones and zeros so this line
anything above this line is actually 1
and anything below this line we have
zeros so this is how we are able to
classify or able to implement or gate
similarly when I talk about an gate as
well so there's a difference in the
truth table of an gate in an gate what
happens we need to make sure that both
of our inputs are high in order to get a
high out if any of the input is low we
get a low output and as the reason we
choose an activation function that is
1.5 which means that if our value of the
output is about 1.5 then only our neuron
will fire and we'll get one here and
there is only one case that is when both
the inputs are high so when both inputs
are I let me just use my pen again and
I'll explain to you so x1 is 1 x2 is 1
we get something which is 2 which is
obviously greater than 0.5 pardon me for
my writing but yeah this is how it is
yeah so 1 plus 1/2 which is obviously
greater than 0.5 so what we get we are
neuron files and we get 1 here but for
the rest of the inputs all will be less
than 1.5 so that is why our neuron
doesn't file and we get a 0 output
don't worry guys I'll actually tell you
how to implement it I'll open by PyCharm
and I'll be showing it to you
practically how to implement at these
gates so for that what I'm going to do
is I'm going to first import a Python
library called tensorflow
the installation guide of this
particular library is present
NMS for both windows as well as for
linux so you can just go through it and
it's very easy to install that and I'll
actually execute this practically later
in the session you don't need to worry
about it so this is how we can implement
and and/or gate
I have a question popping my screen by
Imran he's asking why aren't we
implementing xor gate with the help of
single layer perceptron all right Imran
I am very impressed with your question
because we cannot implement an XOR gate
with a single neuron will actually
discuss that later in the session I will
tell you why we cannot do it because it
is not linearly separable and why it is
not linearly separable we'll also see
that that's a very good question Imran
but just be patient and it'll be covered
in us today session any other questions
any of those guys so let's move forward
guys and we'll actually understand I
usually use case that is of M this data
set the reason for using M this data set
is because it's already clean and will
be a perfect example for this
now what this M this data set contains
it basically contains handwritten digits
from 0 to 9 and in that data set we have
50,000 training images along with 10,000
testing images so we will train our
model with those 55,000 training images
and then we are going to test the
accuracy of a model with the help of
those are 10,000 testing images and for
all of this we need to understand first
what exactly is 10 to flow so let us
move forward guys I will understand
tensor flow and then we are going to
implement all these things that we have
discussed that is an gate or gate and at
the MS data set that I am talking about
so let us move forward guys and
understand what exactly is telling the
flow now what is tensor flow as I told
you earlier as well we use this tensor
so library in order to implement a deep
learning modes and the way the data is
represented in the deep learning model
is called
attention now what are tensors now
tensors are just the multi-dimensional
arrays or you can say an extension of
two dimensional tables matrices to data
with high dimension now let me explain
you with the examples that are there in
front of your screen so this particular
data is nothing but a tensor of
dimension 6 because we have 6 rows and
we have only one thing we'll call them
now over here we have 4 columns as well
as 6 rows so this becomes the tensor
dimension six comma four similarly over
here as well so we have another
dimension that is the third dimension in
which we have two values so we consider
this as a tensor of dimension six four
and two so this is nothing but a way of
representing data-intensive loop now if
you consider the tensor flow library at
its core it is nothing but a library
that performs matrix manipulation that
is what tends to flow is now let us move
forward and understand tends to flow in
a bit detail so as a name tells that it
consists of two words tensor as well as
flow now we understand what exactly
tensor is we saw it in the previous
slide as well now when I talk about flow
it is nothing but a data flow graph so
let me just give you an example that is
there in front of your screen so we
talked about wage and inputs so we
provide these weights and inputs and we
perform a matrix multiplication so as
beta is 1 tensor X input is 1 tensor
then we perform matrix multiplication
after that we add a bias which will
actually see it later in the session you
don't need to worry about this term then
what we do we add all of these so what
is this this is nothing but the Sigma
function in this perceptron that we have
seen then we pass it through an
activation function and the name of that
activation function is re Lu or Ray Lu
where you can say it and then I am your
on this file so this is nothing but a
slow or you can say a data flow graph
now let us understand few code basics of
tensor flow and before that if you have
any questions any doubts you can write
it on your chat box and I'll be happy to
help you we have a question from
Jackie's asking or tensor flow is the
library for Python only well Jack tends
the flow is available in R is well but
we are going to use Python in our course
alright so he's happy with answer fine
guys so there are no more questions
we'll move forward and understand the
code basics of tensor flow now the
tensor flow programs actually consists
of two parts one is building a
computational graph and another is
running a computational graph so we'll
first understand how to build a
computational graph now you can think of
a computational graph as a network of
nodes and with each node known as an
operation and running some function that
can be as simple as addition or
subtraction or it can be as complex as
say some multivariate equation now let
me explain it to you with the code that
in front of your screen the first thing
you do you import the tenth of slow
library then what do you do you define
to known and these nodes are constants
so we'll call that function we'll call
it as T F dot constant and we'll provide
a value that is three and it is nothing
but a float number of 32 bits similarly
we define one more node which is a
constant and it contains value 4 so
these are nothing but your constant
notes so this is basically what
computational graphics
so basically we have built a
computational graph and in this graph
each node takes zero or more tensors as
inputs and produces a tensor as an
output and one type of node is a
constant that I've told you earlier as
well and these tensor flow constants it
takes no inputs and it outputs of value
which is stored internally now what I'll
do I'll actually execute this in my
PyCharm so for that I'll open it so this
is my Python guys and I've already
installed tensor fill and I'm telling it
to you again that the installation guide
for your tensor flow in Windows as well
as Linux is present in your elements
just go through it and it's pretty easy
if you have still any doubts you can
contact our 24/7 support team so the
first thing that I need to do is impose
the tensor flow library for that I'm
going to type in here import tensor flow
as PF so if you are familiar with the
Python basics you know what it means
actually so I am importing tensor flow
library and in order to call that I'm
going to use the word Kiev so after that
I'm going to define my node 1 and node 2
which are constant nodes so for that I'm
going to type in here node 1 equal to TF
dot constant
and then I'm going to define a constant
value in this so it'll be three and
it'll be a float value of TF dot float
or 32-bit alright and now I'm going to
find my second constant node so I'll
type will hear no two TF dot constant
I'll put four in here and four that's it
and so the D F dot float32 will be
present implicitly I don't need to do
that again and again so I have created a
computational graph here so what if I
print it now so let us see what will be
the output node 1 comma node 2 let's go
ahead and run this and see what happens
if you notice here that printing the
nodes does not output the values 3 and 4
you might be expecting that right that
it should print 3 and 4 but instead they
are nodes that when evaluated would
produce 3 and 4 respectively so to
actually evaluate the nodes what we need
to do is we need to run this computation
graph so let me show you that for that
I'm again going to open the slides so
since I've told you earlier is when we
need to actually run this computational
graph within a session and what is this
session its session actually
encapsulates the control and state of
the tensorflow runtime
now the code that is there in front of
your screen what it will do it will
actually create a session object and
then it invokes its run method to run
enough of the computational graph in
order to evaluate node 1 and node 2 and
how it does that by running the
computational graph in the session so
now let me show it to you practically
how it happens so again I'm going to
open my pycharm this is my Python guys
so let me first comment this print
statement and now I'm going to run a
session so for that I'm going to type in
here cess equal to TF dot session and
now I'm going to print it so far I'm
going to have in print sets dot run and
what I want to run I wanna run node 1
and node 2 so I'm going to type in here
node 1 comma node 2 that's all
and now when I run this it will actually
give me the value three and four and yep
it does it gives me the value three and
four so what we did we first build a
computational graph we first saw how to
build a basically a computational graph
and then we understood that these all
are nodes so in order to get their value
we need to evaluate those nodes and how
we can do that by running the
computational graph inside a session and
then finally when we run that session we
get the output three and four which is
nothing but the values of those nodes so
any questions any doubts till here guys
how to build a computational graph and
how to actually run the session any
questions any doubts you can ask me
straightaway any questions guys finds
I've got confirmation from everyone and
no one has any doubt still here finds
all again open my slides and the one
thing that you must have noticed here is
that it will always produce a constant
result right so how to actually avoid
that so we saw how to run a
computational graph is with now let me
explain it to you with one more example
so this is one example in which we take
three constant nodes a B and C and we
perform certain operations like
multiplication addition and subtraction
and then we run the session and then we
finally close it and this is the diagram
how it looks like so we have three
constant nodes C B a a contains value
five B contains two similarly C contains
three first we add C and B then we add B
and a then we get two other nodes that
is e and D and then from that D and D
what we are going to do is we are going
to subtract both of them and then we get
the final output now let me go ahead and
execute this practically in my Python
so this is my fight on my game guys
first thing I need to do is import
tensorflow as TF then we are going to
find the 3 constant nodes so first is a
equals 2 TF thought constant and the
value that will be there in a will be 5
then I'm going to find one more constant
no that will be be TF dot constant and
the value that will be there a net will
be 2
and then I'm going to find the last
constant node and TF dot constant the Y
that will be there is three all right so
we have three constant nodes and now we
are going to perform certain operations
on them for that I'm going to find one
node let BD which will be equal to TF a
dot multiply
so the two notes that we want to
multiply that will be a comma B and then
there will be one more note in order to
perform some operation that is addition
to TF dot add add C and B and then we're
going to define one more note let be F
and inside that node we're going to
perform the subtraction operation TF a
dot subtract
D and E so we have build a computation
graph now we need to run it and you know
the process of doing that says is equals
to t f dot session
then we are going to define our variable
let it be or UTS outs whatever name that
you want to give in and just type in SS
a dot run F let's see if that happens or
not and then we are going to print it so
that I'm going to type in here print out
let's go ahead and run this and see what
happens
so we have got the value 5 which is
correct because if you notice our
presentation as well let me open it for
you over here we also we get the value 5
similar to our implementation and
PyCharm as well so this is how you can
actually build a computational graph and
run a computational graph I've given you
an example now guys let us move forward
because these are all the constant notes
what if I want to change the value that
is there in the node so for that we
don't use the constant notes for that we
use placeholders and variables let me
explain it to you first I'll open my
slides so since these are all constant
notes we cannot perform any operation
once we have provided a value it will
remain constant so basically a graph can
be parameterized except external inputs
as well and what are these these are
nothing but your placeholders and these
placeholders is basically a promise to
provide values later so there is an
example that is there in front of your
screen over here these three lines are
bit like a function or a lambda in which
we define a two input parameters a and B
and then an operation possible in them
so we are actually performing addition
so we can evaluate this graph with
multiple inputs by using seed underscore
dict parameter as you can see we are
doing it here so we are actually passing
all these values to our placeholders
here so these all values will be passed
and accordingly we will get the output
so let me show you practically how it
happens so I'm going to open my PI charm
once more I will remove all of this and
yeah so the first placeholder I am going
to name it as a TF a dot place holder
and what sort of a placeholder it will
be so I will consider it as float number
of three divots similarly I'm going to
define one more variable as well I'm
going to name it as B
then I'm going to define an operation
that I'm going to perform in them so I'm
just going to type in here other
underscore node equals to a plus B and
now add placeholders are currently empty
so that totally earlier is when
placeholders are nothing but a problem
is in order to feed values later so this
is how we have built a computational
graph now our next step is to start a
session
Cepeda i'm gonna type in its s equals to
TF a dot session correct
since these placeholders are currently
empty and we know that these
placeholders are nothing but a promise
in order to provide them with certain
values later so let's go ahead and
provide the values or you can say a list
of values so I'm going to type in a
friend says dot run
either underscore node
and then the value that I'm going to
feed in so basically I'm going to feed
in a dictionary a colon a three a colon
a a list of integer values between one
two three and one more will be B colon a
list of integer values between two and
four alright so this is done now let us
go ahead and execute this and see what
happens so we have got the output at
three and seven so basically if you add
one into you get three similarly you are
three and four you get seven it's pretty
easy mathematics but my main focus was
to make sure that you understand what
are placeholders so I hope you have got
the concept if you have any questions
any doubts you can write it on in the
chat box any questions guys fine guys so
we have no questions here so I'm again
going to open my slides and we'll see
what's up next so we understood what
exactly our placeholder is now's the
time to understand what our variables so
basically in deep learning we typically
want a model that can take arbitrary
inputs now in order to make the model
trainable we do able to modify the graph
to get new outputs with the same input
and what helps us to do that variables
basically allows us to add a trainable
parameters to our graph so in order to
declare variables what we do is you can
refer the code that is there in front of
your screen so we have taken two
variables in one place holder and the
first variable has a value 0.3 the
another variable has - point 3 and the
place holder obviously it will remain
empty and then later on we filled in
some values to it then we create a
linear model or you can say some
operation in which we are going to
multiply this W with X and then we are
going to add a bias or a B value - error
then what we need to do is we need to
initialize all the variables in the
tensorflow program so for that you must
explicitly call a special operation
which is a TF dot global variable
initializer and then just directly run
the session now let us go ahead and
execute this practically guys let me
remove all of this yeah
our first variable will be a W and we
are going to call in TF dot variable
and in that variable we are going to
store our value which is 0.3
and it is of a flow type 32-bit so just
type in here TF a dot load 32 yeah then
we are going to define one more variable
let it be B so I'm going to type in our
TF a dot variable and the value of this
variable will be minus of 0.3 again it
is float type of 32-bit so just type in
the TF dot load 32 now I'm going to
define a place holder that is X X is
equals to TF a dot place holder and it
is again of flow type 32-bit
and then some operation that we are
going to perform so I'm going to type in
here linear underscore model equal to W
multiplied by our x value that is a
placeholder and then we want to add a
bias to it or B value to it all right
now as we have seen earlier is where in
order to initialize constants we call TF
constant and their value will never
change but by contrast variables are not
initialized when you call TF not
variable so to initialize all the
variables in the tender flow program
what you need to do is you need to
explicitly call a special operation and
how you want to do it is type in here
I knit equal to TF dot Global underscore
variables underscore initializer
that's all and then and then we're going
to learn a session so you know the
process says is equals to TF dot session
I net now let's print it and before that
what we need to do is need to provide
the X placeholder with some values we'll
actually going to do that at the print
statement itself so I'm going to type
it's s dot run
run what run a linear model
and the values are be able to pass in X
will be a dictionary again and let me
show you how you can do that so X : and
it'll be a list of values from 1 comma 2
comma 3 comma 4 yeah that's all and we
are going to run it now so this is how
we get the output simple mathematics
what we have the first value of W will
be going 3 and the first value of a B
will be a minus point 3 and the first
value of x will be 1 so it'll be 3 minus
3 which is again 0 similarly for other
values as well you can calculate it it's
absolutely correct so what we have done
we have created a model but we don't
know how good it is yet so let's see how
we can actually evaluate that for that
I'm again going to open my slides so now
in order to evaluate the model on the
training data we need a placeholder Y as
you can see in front of your screen to
provide the desired values and we need
to write a loss function so this
placeholder Y will actually be provided
with the desired values for each set of
inputs and then we're going to calculate
the loss function and how are we going
to do that we are going to minus the
actual output with the desired output
and then we are going to do the square
of it after that we're going to sum all
of these square deltas and then we're
going to define one single scalar as
loss so this is how we are actually
going to calculate the loss and then
after that we need to provide the values
to X&amp;amp;Y placeholders so what I'm going to
do I am going to open my Python and then
I'm going to show you how correct our
model is on the basis of the values that
we provide to Y placeholder so guys this
is my PI time again and since here hour
I'm going to do is I'm going to define a
placeholder first Y is equals to TF dot
placeholder TF dot 432 so I'm going to
type in our TF dot float 32
square Delta I have explained you all
these things but yeah I'm going to tell
it to you again as well don't worry
about it
TF dot square the actual output minus
the desired output and then I'm going to
sum all those losses or you can say
square delta TF a dot reduce underscore
some
some type in here square does then
finally print it so I'm going to have
incest dot run
los
X : one going to go and recover for
zero
1 comma minus 2 comma minus 3 and that's
all and we are good to go let's run it
and see what will be the loss so the
loss is twenty three point six six which
is very very bad now our next step is to
realize how to actually reduce this loss
so let's go ahead with that I'm going to
open my slides once more so in order to
minimize the error not all to reduce
this loss to in circular provides
optimizers that slowly change each
variable in order to minimize the loss
function and the simplest optimizer is
gradient descent in order to do that we
have to call a function called TS dot
gradients so as you can see it in the
code itself so we have a TF dot train
dot gradient descent optimizer and this
is nothing but the learning rate point
zero one then train equals two optimizer
dot minimize the loss so we're going to
call this optimizer which is nothing but
the gradient descent optimizer in order
to minimize the loss and then we are
going to run it so let us see if that
happens or not so for that again I'm
going to open my PI charm and over here
let me first comment this current
statement and now I'm going to type in
here optimizer equals 2t F dot train
gradient descent optimizer
and the learning rate will be point zero
one and guys let me tell you this is
just an introductory session to
tensorflow and deep learning so all the
modules that we have discussed the
beginning will be covered in detail so
whatever topics are there those modules
will be covered in detail so you don't
need to worry about it so I'm just
giving you a general introduction and
overview of how things work in center
flow all these things all these gradient
descent optimizers all these things will
be discussed in detail in the upcoming
sessions now I'm going to type in here
train equal to optimizer dot minimize
loss
and then now let's go ahead and run this
says dot run
I in it
says store run and we get a feeling
values to our X&amp;amp;Y placeholders train ,
Excel have al use one two three four
we'll have values zero comma minus 1
comma minus 2 comma minus 3
friend says dot run w comma P now let me
first go ahead and comment these lines
so I've just made a mistake here this is
a in uppercase W and yeah so now we are
good to go and let's run this and see
what happens
so these are our final model parameters
so the value of a W will be around
0.9999 six nine and the value of RB will
be around a point nine nine nine nine
zero eight two so this is all we
actually build a model and then we
evaluate how good it is and then we try
to optimize it in the best way possible
so I've just given you a general
overview of how things works in
tensorflow
and in the later modules or in the
upcoming sessions we are actually going
to discuss tensorflow in much more
detail so now is the time to actually
implement the and an or gate that I was
talking about in the beginning of the
session so but the I'll let me first
remove all of this yeah now in order to
implement an gate our training data
should consist of the truth table for an
gate and we know the truth table and we
know that if any of the input is low
then output will be low and if both the
inputs are high and output will be high
one thing to note here guys is that the
bias is implemented by adding an extra
value of one to all the training
examples
so yeah enough for the explanation let's
go ahead and code it so I'm going to
type in here T comma F equal to one
point comma minus one point
and bias will always be one so I'm going
to type in our bias is e equals to one
point zero and now I'm going to provide
the training data so for that I'm going
to type in Train underscore in equal to
if one input is true and the another
input is also true then we have bias
then again if one input is true the
other input is false then also we have
bias similarly one input is false
another input is true then we have bias
and then finally when both the inputs
are false or zero
and we have bias right this is our
training data and the train out will
type in here as the output basically
train underscore out will be equal to
say up if both the inputs are true or
both inputs are 1 and output is true
oops I forgot comma everywhere let me
just go ahead and do that yeah so if
both the inputs are true the output will
be true if any of the input is fall the
output will be false so I'm just going
to type in here falls everywhere because
there's only one condition in which we
have the true output
all right so this is done now now as we
know that tensorflow works by building a
model out of empty tensors then plugging
in knowing values and evaluating the
model like we have done in the previous
example since the training data that we
have provided will remain constant
the only special tensorflow object we
have to worry about in this case is our
three cross one tensors of a vectors and
now what we are going to do we are going
to define a variable and we are going to
put in some random values in it so I'm
just going to type in TF a dot variable
and generate the random function key F
dot random underscore normal
three comma one
now what it is it is basically a
variable so its value may be changed on
each evaluation of the model as we train
with all values initialized to normally
distributed random numbers now that we
have our training data on weight tensor
we have everything needed to build a
model using tensorflow
so what we need to do we need to define
an activation function and we are going
to define a step activation function or
step function so for that what I'm going
to write here is a function step we're
going to define an own step function
although you can use the predefined
function as well that totally depends on
you I'm going to dive in here is greater
equal to TF dot greater
X comma 0
then we're gonna define one more
variable as float TF a dot t-- o--
underscore float
is underscored crater
then we are going to define one more
variable are doubled equal to TF dot
multiply
as underscore float comma to
then return T f dot subtract doubled
one so now this is how we have defined
our step function so with the step
function defined the output error and
the mean squared error of our model can
be calculated in one short line each let
me show you how you can do that so just
type in here output equal to call that
function that a step TF dot matrix
multiplication math mul
train underscore n comma W now for error
type in here error equals to TF dot
subtract
train underscore out comma output
and for me squared and I'm going to type
in here MSC equal to TF dot reduce
underscore mean TF dot square
and then either
now the evaluation of certain tensor
functions can also update the values of
variables like a tensor of 8w so in our
case basically we're going to update the
weights W so first we calculate the
desired adjustment based on error then
we add it to W now let me show you how
we will do that so if you can recall we
have done that in the previous example
as well where we were updating the
weights in Vice
so now I'm going to type in here Delta
is equal to TF dot Matt --ml or you can
say matrix multiplication train
underscore n comma error comma transpose
underscore a
equals to two
train equal to TF dot assign
w comma T F dot add W comma Delta
the first matrix multiplication and then
we're going to add it all the model has
to be evaluated by tensorflow session
which we have seen earlier as well but
before that we also need to initialize
all the variables so first let me just
type in Harris as equals 2t F dot
session
and then I'm going to type in here says
dot run
TF dot initialize underscore all
variables
so now what our next task is in order to
perform various iterations so that we
get zero error
so what we're going to do we define a
variable er R and our target so
basically our er R is nothing but their
error which can be equal to 1 or 0
because we are using binary output so
our error can be 1 and our target is to
make it a zero right now next what we're
going to do we're going to define a pork
now you can consider right now a pork as
nothing but the number of cycles or you
can say the number of iterations that
will be required in order to reach the
desired output or you can say in order
to reduce the error to zero so I'm just
going to define that so I'm going to
type in here a pork or you can give
whatever variable name you want and I'm
going to type in here marks underscore
epoch equal to zero comma 10 which means
that our it will start from the zero a
pork and a maximum value of a POC will
be 10 now I'm going to define while eerr
should be is greater than target
and a fork is less than maximum Epoque
increase the value of a pock y1
and then type in errr
run
mean square
, train
and let's just find me print it and see
what happens print a pork
mean square error
eerr so basically what it will print it
will print the cycles or epoch and the
error with respect to that particular
cycle so let us go ahead and execute
this and see what happens
oops I've typed the spelling of square
wrong uh pardon me for that so I'm just
going to make it right now squ ar e
alright let's go ahead and execute it
once more
so yeah in three epochs we got the value
of mean squared error as zero that means
it took us three iterations in order to
reduce the output to zero so this is how
you can actually implement a logic gate
or you can say this is how you can
actually classify the high and low
output of a particular logic gate using
single layer perceptron similarly you
can do that for or gate as well consider
this as an assignment come up with a
code that can actually classify the high
and low outputs of an or gate so any
questions any doubts you can write it
down in a charged box and I'll be happy
to help you any questions guys so we
have no questions here guys so what I'm
going to do I'm going to create a new
Python file and I'm going to name it as
M NIST which is nothing but the data set
on which we are going to perform the
classification of handwritten digits so
we're going to execute a use case and we
have told you earlier as well in this M
this data set we have handwritten digits
between from zero to nine and it has
55,000 training sets as well as 10,000
our test set so let us go ahead with
that the first thing I'm going to do is
download the data set but before that
let me just import tensorflow library as
TF and yet now let us download the data
set so for that I'm going to type in
from tensorflow dot examples dot
tutorial dot test import input
underscore data and now I'm going to
type in here n nest is equal to input
underscore data on this
underscore data dot read datasets type
in here M nest
underscored data comma one
underscore hot equals to true
now here and this is nothing but a
lightweight class which tows the
training validation and testing set as
num py Addis and when I talk about this
one hot equals to true is nothing but
100 encoded now let me tell you what one
hot encoding is so for that let me just
comment few lines so 1 or encoding means
that if I'm classifying something as a
seven so if I classify that my digit is
7 so how am I going to represent that so
I'm going to type in here 0 1 2 3 4 5 6
7 the bit will be active so I'm going to
type in there 1 then 8 and then 9
similarly if I want to represent say
that my digit is 2 so I'm going to type
in 0 1 on the second digit I'm going to
type in here as 1 and the rest all 0s so
I hope you've got the concept of what
exactly one hot encoding is so it's like
only one output is active at a time
that's all and I'm telling it to you
again and again we are going to discuss
everything in detail in our coming
sessions so next step is to start the
session like we do every time so I'm
going to type in here sets equals 2t F
dot interactive session so I'm just
going to type in here interactive
session yeah
and now we are going to do we are going
to build the computation graph by
creating nodes for the input images and
target output losses so for that I'm
going to define some placeholders for
that I'm gonna type in X 3 goes to TF
dot placeholder
TF dot 0:32
thirty-two and the shape will be
none Kharma 784
now the input images eggs will consist
of 2d tensors of floating-point numbers
here we assign it a shape of say none
comma separator fold as you can see so
where 784 is the dimensionality of
single flat and 28 by 28 pixel M nest
image of handwritten digits and what non
indicates it indicates that the first
dimension corresponding to the batch
size can be of any size means the first
dimension can be of any size we are not
putting any restrictions on that now I'm
going to define variable wipe this will
nothing which will be nothing but our
real labels or you can say the desired
output placeholder and I'm going to type
in here
TF dot slowed 32 and I'm going to define
the shape which will be none comma 10
because we have 10 classes and similarly
Y is also a 2d array where each row is
one hot 10 dimensional vector indicating
which digit class the corresponding M
nest image belongs to and now next step
is to define weight and biases for our
model like we have done in the previous
example so we could imagine treating
these like additional inputs but
tensorflow has even a better way to
handle them and what it is it is nothing
but variables so let us go ahead and do
that I'm going to type in here w.t.f dot
variable
TF dot zeroes I'm going to initialize it
to zeros and the shape will be a 784
comma 10 so it's like 28 cross 28 pixels
and a 10 classes
similarly when I talk about bias so it
will be T f dot variables
TF dot zeroes initializer two zeros and
the shape will be ten
we pass an initial value for each
parameter in the call to D F dot
variable feed now over here as you can
see that we initialize both W and B as
tensors full of zeros and W is a 784
cross 10 matrix because we have 784
input features and ten outputs and when
I talk about bias B it is a 10
dimensional vector because we have ten
classes and we have learned that before
we can use variables in a session we
need to first initialize it so we're
going to type it here says a dot run and
after that I'm going to type in TF dot
global underscore variables underscore
initializer all right so we have
initialized all the variables so our
next task is to predict the class and
the loss function so we can now
implement our regression model it takes
only one line we multiply the vectorized
input image X by the weight matrix W and
add the bus so for that I'm just going
to type in here y equal to TF dot Matt
mal this is nothing but matrix
multiplication X comma W plus B all
right so now we can specify a loss
function very easily so the loss
indicates how bad the models prediction
was on single example we try to minimize
that while training across all the
examples so now we can specify a loss
function now loss indicates how bad the
models prediction was on a single
example and we try to minimize that
while training across all the examples
now here our loss function will be
crossing trophy or you can say the
difference between the actual output and
the predicted output so for that what
I'm going to do I'm going to make use of
softmax crossing trophy function so here
the loss will be the difference between
the target output and the actual output
so for that what I'm going to do is I'm
just going to type in here a variable
name I'm going to name it as cross
entropy and yeah entropy equals 2t F dot
reduce underscore mean and then just
type in here let me just enter the next
line t f dot n n dot
softmax underscore cross-entropy with
logit and now over here I'm going to
type in first our target output which
will be labels equals to Y comma our
actual output so it will be logit is
equals to y that's all so over here let
me just give you a brief for idea what
is happening so labels is equals to y
means that this is our target output and
this is the actual output so it will be
will name it as y underscore and so what
exactly is happening it will calculate
the difference between the target output
and for the actual output for all the
examples then it is going to sum all of
them and then find out the mean so this
is what basically are this cross
Entropia variables will do now that we
have defined our model and training loss
function it is straightforward to train
using 10 to flow now we need to train
our model so tensorflow has a wide
variety of built-in optimization
algorithms as i've told you earlier span
and for this example we'll use steepest
gradient descent with a length of about
0.5 to descend the cross intro P so
basically we are going to use a point 5
learning rate or you can say the step
length so further I'm going to type in
here a train underscore step equal to TF
tour train dot gradient descent
optimizer and the step will be 0.5 and
minimize a loss so minimize cross and
shoppi that's it so what this one line
basically will do it will minimize the
cross entropy which is nothing but the
loss function that we have defined now
in the next step what we are going to do
we are going to load 100 training
examples in each training iteration so
each time the training iteration happens
it will take hundred examples we then
run the train underscore step operation
which is nothing but to reduce the error
using speed underscore dict or which is
nothing but we are going to feed the
real values to our placeholder Y now
next what we are going to do we can load
100 training examples in each training
iterations which means that for each
iteration will take 100 training
examples and will run the train
underscore step operation which is
nothing but the optimizer here and after
that we are going to use a feed
underscore dict to replace the
placeholder tensors x and y with the
training examples
so basically X will contain the input
images and Y will contain the actual
outputs or you can see the desired
output for that I'm going to type in
here for underscore in range thousand
batch equals to MMS dot train dot next
underscore patch 100 train underscore
step dot run and I'm going to feed in
the values to X &amp;amp; Y variable seed
underscore dict equal to X colon batch 0
comma Y colon patch 1
that's it now we need to evaluate our
model we need to figure out how well our
model is doing after that I'm going to
make use of TF dot R max function now
this D F dot arc max function let me
tell you how it works first I'm going to
type in her correct underscore
prediction
equals two RTF a dot equal T F dot Arg
max Y underscore comma 1 comma T F dot R
max y comma 1
so basically this RTF dark marks of why
underscore comma one is a label our
model things is most likely for each
input that means it is a predicted value
a white PF dot arc Max or Y comma one is
the true label it is there in our data
set present already and we know that it
is true so what we are doing they're
using TF dot equal function to check if
our actual prediction matches the
desired prediction this is how it is
working so now what we are going to do
we're going to calculate the accuracy to
determine what fraction are correct we
cast the floating-point numbers and then
take the mean repeat so now I'm going to
define a variable for accuracy's so this
is going to type in accuracy equals 2t F
dot reduce underscore mean
cast correct underscore prediction cover
a TF dot float32
and what we can do finally we can
evaluate our accuracy on the test data
and this should give us accuracy of
about 90% so let us see if that happens
I'm going to type in here print accuracy
not evaluate eval feed underscore dict
equal to X colon M nest dot test dot
images comma Y :
M nest dot test dot labels that's it
guys and I've done
a steak here instead of why it will be
why underscore because this is our
predicted value not the actual value and
why we have considered as actual value
and this y underscore will be our
predicted values this is the mistake
that I made so yeah now I think the code
looks pretty fine to me and we can go
ahead and run this let us see what
happens when we run this
so guys it is complete now and this is
accurate here for model which is 91.4%
and this is pretty bad though when you
talk about a data set like endless but
yeah with a singular which is very very
good so we have got an accuracy of
around 92% on the amnestied asset which
means that whatever the test data sets
were there that is like 10000 our test
images so on those test images whatever
the prediction our model has made our
91.4% correct so guys let me just give
you a quick recap of all things we have
discussed so first we saw what exactly
is deep learning and how it is different
from the traditional machine learning
then we understood the motivation behind
deep learning which is nothing but your
brain cells which are called neuro then
we understood the importance of why we
need artificial neurons and then we took
the example of artificial neuron which
is called a perceptron and we are look
at a couple of examples in which we
implemented the an gate we performed
image classification on the M nest data
sets which is nothing but handwritten
images between 0 to 9 and we also have
limited a couple of examples in order to
understand what are placeholders
variables and constants so till now guys
any questions any doubts you can write
it down in your chat box any questions
so we have a question from Imran I'm
he's asking why is the desired output
and why underscore is the actual output
which is exactly correct Imran this
other case is let me just explain it to
you with the code as well yeah this is
our Y which is nothing but the classes
the ten classes which are already
defined or you can say the desired
output and this is our Y underscore
which is the predicted output so I hope
this answers your question alright he's
pretty fine with it any other questions
any other doubts guys you can write it
down in your chat box I'll be happy to
help you find no questions we'll move
forward with the slides so now there are
certain limitations of single layer
perception let us understand that so in
order to understand that we'll take an
example and yeah if I can recall
somebody asked that question how can we
implement X or gate using single layer
perception so let us see if we can do
that or not so we have an XOR gate here
and this is the truth table so according
to this truth table if any one of the
input is high then the output is high
and if
both the inputs are low output is low
added both the inputs are high output is
low so how can we classify the high and
the low outputs with a single line
definitely you can't if you see the
point so one point is here one is here
here and here and which these two points
are high outputs and these two are low
so how can you classify with a single
line definitely you cut so now what's
the answer to this what if we use
multiple neurons using multiple neurons
we can have two lines that can separate
it now we could solve this problem if
you have multiple neurons so if you use
two neurons we can have two lines and
that can actually separate the high
outputs as well as the low outputs so
this is where we use multi-layer
perceptron with backpropagation so what
are multi-layer perceptron multi-layer
perceptrons they have the same structure
like the singular perceptron the only
difference is they have more than one
hidden layer so let me explain it to you
with an example so this is how a typical
multi-layer perceptron looks like so we
have input layer we have two hidden
layers and we have one output layer as
well now typically each of these input
layers are connected to the next hidden
layer each neuron is connected to the
next neuron presently the edges in layer
but the neurons of the same layer are
not connected to each other now what
happens the set of inputs are passed to
these input layers and the output of
this input layer will be passed to the
first hidden layer then after activation
function of the first hidden layer the
output will be passes an exit in layer
as the input and similarly finally we
get the output now you must be thinking
how the model learns from here so the
basically the model learns by updating
the weights and then guerrilla it uses
is called back propagation so the back
propagation algorithm helps the model to
learn and update the weight in order to
increase the efficiency so basically at
this process of from input layer to the
output layer is called a feed-forward
process and then when we back propagated
in order to increase the efficiency or
accuracy so that we can update the
weight that is called as a back
propagation we understand back
propagation in more detail so any
questions guys apart from this fine so
we have no questions so let us move
forward understand what exactly is not
propagation so what is back propagation
now let us understand this with an
example
so
we'll take the inputs as the least
generated from various sources and my
aim is to classify the leads on the
basis of the priority so there might be
certain lead which won't make a dot much
difference to me where as compared to
the other leads so in that case I need
to make sure the leads which are
important gets the highest amount of
weight how am I going to do that first
we'll see the output then accordingly
we'll calculate the error and based on
that error we are going to update the
way and this process is nothing but your
back propagation in a nutshell I could
say right although the algorithm is
pretty complex but yeah this is
basically what happens so in order to
classify the leads in the basis of
priorities we need to provide the
maximum weight to the most importantly
and how they're going to do that we're
going to compare the actual output of
the desired output and according the
difference we can update the weights so
what is back propagation so back
propagation is nothing but a supervised
learning algorithm for multi-layer
perceptron now let us understand what
exactly is this algorithm let us
understand this with an example that is
there in front of your screen so these
two are our input neurons these two are
our hidden neurons and these two other
output neurons now our aim is to get
point 0 1 and 0.99 as an output and at
the same time we have inputs as point
zero five and point one zero initially
we take the weights as we can see it
here so 0.15 w1 0.2 0 W 2 so these are
the weights plus we have two biases as
well now what we need to do is we need
to make sure that we have weights in
such a way that we get output as point
zero one point nine nine but let us see
if they get that same output when we
provide these kind of weights the net
input for this particular h1 will be
what it will be w1 into I even that is
the first input plus w2 into I two plus
a B 1 into 1 which gives us the answer
as point three double seven five
similarly the output for h1 will be
nothing but the activation function the
output and after the activation function
and we are using here sigmoid function
because our brain also uses a sigmoid
function that's what we believe in and
at the same time it is easily
differentiable and if you differentiate
it twice you get the same number so the
output of H 1 will be 0.5 9 something so
let me go back
so the yeah the output of this
particular H 1 will be 0.5 9 something
then we're going to calculate the output
of h2 as well similarly and we get the
output as 0.596 double eight four three
seven eight next up we are going to
repeat the process for the output layer
neurons as well so for the output layer
the net input will be W five into out of
s 1 w 6 into out of s 2 plus a B 2 that
is a bias so we get the output somewhere
like this and then the net output for
out o 1 will be after the activation
function which will be 0.75 1 similarly
the output for o 2 will be 0.77 now if
you notice that this is not the desired
output our desired output was point 0 1
and 0.99 what we got instead we got 0.75
and point seven seven so what we need to
do we need to update the weight so for
that we are going to calculate the error
now error for output o 1 is nothing but
the Sigma of 1 by 2 target minus output
whole square target is nothing but your
desired output and output is your actual
output so your error of output of oh one
neuron will be point two seven eight
similarly 402 will be point zero two
three so the total error comes down to
0.29 something next up we need to update
the weight so as to decrease this error
now what we going to do we will first
calculate the change in total error with
respect to any of the random weight will
take W 5 for example just to show you so
we will calculate the change in the
total error when we change with respect
to the weight W 5 so we can apply a
chain rule here so using partial
derivative we can calculate e total by
Delta of auto 1 into Delta power 1 by
Delta Netto 1 and then Delta of Netto 1
by Delta of W 5 as you can see it from
this particular example is when he total
change in e total with respect to auto
one change in outer one with respect to
NATO 1 and change the net o 1 with
respect to W 5 we're going to multiply
that and we're going to get this
particular term and let us see how we do
that so how was the total error change
with respect to the output
he told by he out 1 we're going to
calculate it it came around 0.7 for 1
similarly we're going to calculate how
much this output oh one change with
respect to the net output and we get
this after that we're going to calculate
it for net output change with respect to
W 5 also we got this and then finally
what we did we put all these values
together and we found out
to change in the error with respect to
the change in weight w5 which came to up
around point zero eight two one six
seven zero four one now is the time to
update the way so how we can update the
weight first thing we need to do is we
need to follow this formula so w5 plus
is nothing but the updated weight which
is equal to W 5 minus the learning rate
into Delta of a total by Delta of W 5 so
which came to around 0.35
eight nine one six four eight similarly
we can calculate the other weights as
well so we're going to repeat the same
process and we're going to calculate the
other weights as well then again we're
going to see how much is the loss if
still the loss prevails that we're going
to repeat the same back propagation
learning algorithm again for all the
weights so this process will keep on
repeating so this is our back
propagation actually works so we will
actually have a separate module in which
we'll discuss back propagation in much
more detail so now what I'm going to do
I'm going to make use of the same M this
data set and I'm going to increase the
efficiency of that data set by 97 to 99
percent with the help of multi-layer
perception so guys let us go ahead with
that so guys as I've told you earlier is
where we are going to use the same Emnes
data set which we have used in the
single err perceptron so I am going to
perform a classification using
multi-layer convolutional networks now
what a convolutional networks basically
these networks are used in order to
classify images we have a separate
module on convolutional networks so you
don't need to worry about it right now
so basically what I'm doing is I'm just
trying to show you that how we can
increase the efficiency using the
convolutional neural networks you don't
have to go in much detail about it
because it will be learning about it in
the upcoming modules I'm just giving you
a general overview or you can say a
taste of how things work in
convolutional neural networks so we give
an input image to this convolutional
network then this input images process
in the first convolutional layer using
the filter weights now this result in 16
new images one for each filter in the
convolutional layer the images are also
down sampled so the image resolution is
decreased from 28 cross 28 to fourteen
cross protein now these 16 smaller
images and from the first convolutional
layer are then processed in the second
convolutional layer we need filter
weights for each of these 16 channels
and we need filter weights for each
output channel of this
now there are total 36 output channels
so there are total of 16 cross 36 equal
to around 576 filters in the second
convolutional layer now the resulting
images are down sampled again to 7 cross
7 pixels the output of the consecutive
Ellucian layer is 36 images of 7 cross 7
pixels each now these are then flattened
to a single vector of length 7 cross 7
cross 36 which is a 1764 which is used
as the input to a fully connected layer
with 128 neurons and this feeds into
another for the connected layer widths
10 neurons one for each of the classes
which is used to determine the class of
the image that is which number is
depicted in the image so whatever number
that we provided so basically the output
layer or you can say the last layer or
the fully connected layer with 10
neurons is used to determine the class
or which digit is the input image so now
the input image depicts the number 7 and
the 4 copies of the images are shown
here so basically what happens whatever
filter we have in each layer it will be
present on the image pixels so there
will be a dot product of the filter and
the image pixel behind that so we'll get
a dot product there we are going to
repeat that same process in each of
these layers and for each of these
layers and for each of these images and
we are calculating the dot product so I
hope you get the point and you don't
need to worry about it if you are not
able to understand this concept right
now because we haven't talked about
convolutional neural networks at all so
I'm going to discuss convolutional
networks in the upcoming modules as I am
telling it to you again and again so let
me just show you that we can increase
the efficiencies so I have already done
the code and it will take time if I do
it right now
so this is what the efficiency we ended
up with so it is my Nina 8.8% so on the
test sets we had around 10000 test
samples out of which we predicted 9000
876 correctly so which is pretty good
actually if you see so if it is in the
single layer perceptron example that we
took we were getting accuracy of around
92% but here we are getting around 99
percent which is actually very good
alright so this is all for today's
session guys and if you have any
questions or any doubts you can write it
down in your chat box I will be happy to
help you
pooja sees nice introduction Thank You
pooja cidades amazing session Thank You
cidade for those kind words or it finds
we have no question there so let me just
give you a brief summary of what all
things we have discussed till now so
these are the topics that we have
covered in today's session we started
with what exactly is deep learning
we took an analogy of classification of
cats and dogs we understood how humans
identify the difference between a cat
and a dog and similarly how we can train
a machine to do that so we first looked
at a machine learning model and then we
focused on certain limitations of that
machine learning model and then we
understood that what exactly is deep
learning then we saw how deep learning
works with the motivation behind deep
learning is your brain cells so we
understood how the art brain cells or
neurons work then we focus on single
layer perceptron which are nothing but
the early deep learning models or you
can say an artificial neuron a single
artificial neuron we saw various
examples of single layer perception as
well as for example how to implement an
gate and now to perform classification
on the emne status it then I introduced
you to tensor flow we understood various
code basics of tensor flow V and this is
what exactly are tensors as well then we
focus on various limitations of single
layer perceptron and we took an example
of XOR gate in which I told you that if
the data points not linearly separable
then we cannot apply single layer
perceptron then I introduced you to
multi-layer perceptron that overcame the
limitations of single layer perceptron
we understood the learning algorithm of
multi-layer perceptron that is a back
propagation algorithm finally we saw a
use case of the same emiliÂ´s data set
in which to increase the efficiency from
92 percent to 99 percent so the 92
percent was there in the single a
perceptron but the same thing when we
did was multi-layer Andalucian networks
we got the efficiency which was around
at 99 percent so these are the things
that we have discussed today thank you
for attending today's session this video
will be uploaded into your LMS so you
can go through it if you have any
questions about you can ask our 24/7
support team or you can break your
doubts in the next class as well thank
you and have a great day I hope you
enjoyed listening to this video please
be kind enough to like it and you can
comment any of your doubts and queries
and we will reply to them at the
earliest to look out for more videos in
our playlist and subscribe to our ready
Rica channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>