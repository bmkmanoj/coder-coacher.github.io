<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Hadoop Training | Twitter Analysis with Apache Hive | Hadoop Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Big Data Hadoop Training | Twitter Analysis with Apache Hive | Hadoop Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Hadoop Training | Twitter Analysis with Apache Hive | Hadoop Tutorial | Edureka</b></h2><h5 class="post__date">2017-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1Ae4t0rjK3o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome everyone this is
Rashmi from Eddie Rica and I'm very
excited today to take up this live
session on Hadoop so we're going to do a
Twitter analysis today using different
Hadoop ecosystem tools so without
wasting any more time let me tell you
what the session will cover because I
don't want to hold you for much longer
because it is Friday so here is the
agenda so at first we'll learn what is
the need to learn big data analytics why
am I talking about this particular
technology and in order to answer this
question I will be telling you the big
data trends the salary you can expect as
a big data professional and what the
market holds for this technology then
we'll move on to understand some
fundamentals of Hadoop and we'll put all
the fundamentals into use where we'll be
doing a Twitter analysis we'll be doing
a sentiment analysis to find out how
popular is Big Data and Hadoop and also
I'll be telling you about the Big Data
and Hadoop training that we have at Eddy
Rica where I'll be explaining you the
entire course curriculum so let us get
started so the first topic is why learn
big data analytics now there are
numerous reasons because of which
different organizations and people are
going in for big data analytics and I
have tried to list down the four most
obvious reasons which according to me is
because of which people are taking big
data analytics so the first is cost
reduction
now when you analyze your data you will
gain a lot of insights and you'll find
out different ways on where you could
save your money on so hence big data
analysis can help you to save your money
and also it can reduce the total cost of
ownership of your organization as a
whole
the second it helps in better decision
making how because when you analyze your
data you'd be somewhat able to predict
that what is going to be the outcome of
implementing something and hence it will
help in or it will help you to make a
better decision because if you already
know the result how hard can
decision-making be right it will also
help you to improve your business
because with the insights that you'll
get by analyzing data you're analyzing
your organization's data you'll be able
to figure out that how can you implement
your business strategies more
effectively by cutting out both cost and
time it would also help you to
understand your target audience better
and improve your business and now let us
talk about the future generation tech
product that is believed to be a part of
everyone's day-to-day lives and future
something like self-driving cars how do
you think a car drives on its own it has
to analyze all its surrounding data
which is a lot of data let me tell you
because it has to predict that what
might be the possible obstacles that
might come on its way and it has to halt
let's talk about smart devices Internet
of Things how do you think this devices
are getting smarter every day it is
because they're getting trained on a lot
of data which is nothing but big data so
this are the various reasons the four
most obvious reasons which I think why
big data analytics is getting popular
every day and now let us talk about the
actual reason for which we want to learn
big data because we want to excel in our
career and we want to grow and learn
more skills right and of course we want
to earn some money so let us see the
salary trend of Big Data so these are
some of the indicated figures that I
have collected from pay scale now this
is the Hadoop salary in India so this
will give you a very fair estimate of
what you might be earning as a Big Data
professional in India so the different
positions in India as a Big Data
professionals are data analysts data
scientists data scientists and IT
solutions architect senior software
engineer software engineer and chief
technology officer or CTO so as a data
analyst you might expect your salary to
range somewhat from 4.6 lakhs but as a
CTO you'll be starting with 39 lakhs now
your salary will also vary according to
your experience so if you have less than
one year of experience you'll be
starting with 4.3 lakhs
however if you have more industry
experience you'll be starting with 45
lakhs and here I'm talking about the
general industry experience all right so
your salary will vary according to
positions and experience and here are
the salary trend of Big Data Hadoop
professional in US again have
categorized it according to different
positions and experience so the
different positions of a big
data professional in u.s. our data
scientists analytics manager director of
analytics data engineers software
engineers senior data scientists and
data architect so the data scientist
will be starting with $97,000 and as
director of analytics you will get
around 135 thousand dollars again it
also dairies according to your
experience so if you have less than one
year of experience you'll be starting
with seventy seven thousand dollars and
if you compare it to the salary of that
with the Big Data professionals in India
of course us is paying them a lot more
so maybe you should all consider getting
a job in the US right all right so let
us move on and understand what is the
market trend of Big Data and Hadoop
because obviously they're paying a lot
to the big data professionals but what
will you do with all the salary
experience if there aren't any jobs but
let me tell you that there are jobs and
there are plenty so I've listed down the
top five states in India which has the
maximum number of job opportunities so
there is Karnataka in the darkest blue
and the darker the color of the state it
represents more job opportunities so
Karnataka is the darkest blue it is
because Karnataka has Bangalore which is
known to be the Silicon Valley of India
apart from that there's Maharashtra
Telangana Andhra Pradesh Tamil Nadu and
Haryana and I think a lot of people
might be surprised to see northeast in
dog blue let me tell you that of course
North East is not known for its tech but
whatever companies that are emerging in
North East India are basically based on
Big Data and Hadoop and these are the
major recruiters in India who are hiring
big data professionals I've also listed
down the salaries they are giving out to
the big data professionals however it
will also vary according to the
different positions and experience so
the top recruiters in India are Mu Sigma
American Express whipper technologies
Infosys Accenture cave Gemini Flipkart
cognizant ericsson and Deloitte
however these are just the top ten
companies the big known names but there
are many other companies who are hiring
big data professionals big and small in
India and here is Hadoop interest by
region in the
us so here I have listed down the top
ten states with the most number of job
opportunities similar to that of India
alright so the top ten states and us are
Washington California Minnesota New York
Arkansas's Virginia Massachusetts I hope
I pronounced that correctly I always
have a hard time saying that then there
is Connecticut New Jersey Delaware so
again there are jobs in other states as
well but I have just listed down the top
ten and I have collected this from
Google Trends and these are the major
recruiters in u.s. the big names that
you already know Amazon Microsoft IBM
Facebook Citibank PayPal uber
technologies Google Apple Computer and
Expedia and again I will tell you the
same thing that there are lots of other
companies who are hiring big data
professionals in US as well so I've
talked about the salary you can expect
as a big data professional and the
market trend too just to show you that
there are a lot of jobs available for
big data professionals and let me also
tell you a very interesting fact that I
have just read and Gartner recently they
have published that by the end of 2019
there is going to be a scarcity of big
data professionals and the scarcity
would go up to 190,000 big data
professional in just us so it is going
to open up a lot of opportunities in
India and the South Pacific area as well
ok so we have done with the salary trend
and the market trends so let us try to
understand what is Hadoop so we'll go
through some fundamentals of Hadoop now
so let us see what is Hadoop so Hadoop
is a framework that allows us to store
and process large data sets in parallel
and distributed fashion so what was the
main problem that was related to Big
Data so the first problem was to store
all the data and that is why Hadoop came
up with a distributed framework to store
data which is known as HDFS and it
allows to dump any kind of data across
the cluster let it be structured data
semi structured data or unstructured
data you can dump anything in HDFS and
the next problem related to Big Data was
to process it and
Hadoop came up with yarn yarn stands for
yet another resource negotiator and this
allows parallel processing of data that
is stored distributed in HDFS so this is
how Hadoop has solved the big data
problem so by storage HDFS and
processing with yarn and the best part
about Hadoop is that it can integrate a
lot of different tools together and it
makes up the entire Hadoop ecosystem so
let me tell you give me so just I'll
just give you a brief introduction to
all this different tools so there is
MapReduce so this is the core component
of processing in Hadoop ecosystem
because it provides the logic of
processing so it is basically a software
framework that helps in writing
applications in different programming
languages like you can use Java Scala
etc then I've already talked about yarn
and HDFS and don't worry I'll be telling
you more about HDFS on yarn in the
coming slides then there is hive which
is a very popular analytical tool so
this was developed by Facebook it is
because they have been accumulating a
lot of data and they wanted a sequel
like query language in order to analyze
that data and hence they came up with
hive query language Sun hence hive and
then they have donated hive to the
Apache foundation and it became a part
of the Hadoop ecosystem then there is
Apache spark I'm very sure that if you
have ever googled about processing big
data you have come across Apache spark
it is a very popular tool in order to
process real-time data then for
scripting you have got Pig which is
again created by Yahoo and then they
have made it open source and donated it
to Apache foundation then a no sequel
database which is HBase then you have
got different machine learning libraries
like mahute and spark em ellipse so
these are libraries that contains
different machine learning algorithms on
which you can train your model on then
for scheduling you have got Suzy for
searching and indexing you can use
Apache Solr and loosin then Kafka and
apache storm now party storm is highly
optimized to process real-time streaming
data and in order to streamline all the
processes together you can use Apache
Kafka and then you can use zookeeper
barri because you can use all of these
tools together or you can use you might
be using few of these tools together
right so if you want to orchestrate this
process of managing all the tools you
can use zookeeper and embark for
management and coordination and in order
to ingest the data into HDFS you can use
different tools like flume or school so
scoop is used to ingest only structure
data into HDFS and flume is used to
ingest unstructured and semi-structured
data so I'll be telling you more about
flume and hive because that is what
we're going to use in our Twitter
analysis all right so I hope that you
have got an overall idea of all this
different tools in the Hadoop ecosystem
all right so now let us move on to HDFS
which is the storage component of Hadoop
so these are the demons that run on HDFS
daemons are nothing but processes so
this is laid out in a master/slave
topology the master daemon is known as
the name node and the slave daemon is
known as the data node so data node is
actually the place where your actual
data is stored into different blocks and
the name node manages all the data nodes
and how does it manage all the data
nodes so basically the name node keeps a
metadata of what data is contained in
which data node so basically it manages
a sheet that okay this data node has
this data block data node to has a data
block number for something like that so
it manages the metadata to keep a track
of what data is contained in which data
node and if you're thinking that there
is one more box that you can see which
is called secondary name node so this is
also a demon but if you're wondering
that whether it is a backup name node no
so secondary name node is basically an
assistant to name node since I told you
that the name node manages a metadata of
the which data is contained in which
data node the secondary name node helps
the name node or assists the name node
in managing that metadata alright and
like how you report to your managers the
same with data node also reports the
name node in the form of signals which
are known as heartbeats so this
heartbeats are just to inform the name
node that this day
to notice live and is functioning
properly alright and since I have told
you that in HDFS your data is stored in
different blocks so the default size of
each of the block is 128 MB so let's say
that if you have a file of 380 MB it
will be splitted up into different
blocks of 120 MB each so with a file of
380 M be the first block will occupy 128
MB the second block will occupy 128 MB
and the third block is occupying just
the remaining memory which is 124 MB so
I have a question for you guys I would
love if you try it out so let's say that
you have a file of 500 MB so how many
blocks will be created come on it's a
very easy one I would like if you just
give me some answers all right you can
leave your answers on the chat box and
if someone have answered for
congratulations to everyone who have
answered for blocks you are absolutely
correct so the first three will occupy
128 MB and the last one will just occupy
116 MB all right so I hope that you have
understood how data is stored in
different blocks in HDFS so now let us
move on and understand about the
processing unit of Hadoop which is yarn
now since I've told you that Hadoop can
integrate a lot of different tools
together so yarn is solely responsible
for the integration of different tools
together with the Hadoop framework
alright and here are the demons that run
on yarn again this is a master slave
topology the master daemon is known as
the resource manager and the slave
daemons are known as the node managers
and node manager is actually the daemon
is actually the place where the actual
processing happens ok this is where all
the data blocks that are stored in HDFS
gets processed in the node manager and
the resource manager just manages the
node managers by providing all the
resources that is needed in order to run
a particular job in the node manager so
this is how HDFS and
works so here are some basic commands
that you can try on so if you want to
start all the HDFS and yarn daemons he
can use this command make sure that you
go to the hadoop directory and run this
command dot slash as bin slash start all
dot Sh so it will start all the HDFS and
yarn daemons like your name node your
secondary name node data node resource
manager and node manager and if you want
to stop all this daemons use the same
command only replace start with stop I
just want to check if all the demons are
running you can use this come on
JPS so I'll be showing you during the
hands-on part when i'll between the
twitter analysis all right so now just
let us move on and give you a brief
description of what we're going to do so
we're going to do a twitter sentiment
analysis using the Hadoop ecosystem
tools all right
so we're going to find out that how
popular Big Data and Hadoop is okay so
and what is the best way to find out
about the popularity of something of
course it is going to be Twitter if
people are tweeting about some
particular topic definitely it has there
is a fuss going on about it right so
we'll try to find out how much activity
is going on in Twitter of about Big Data
and Hadoop alright so let me just show
you how we're going to perform so I have
laid out a solution strategy of what
we're going to do and how we're going to
analyze all the tweets so we'll be using
two tools flume and hive and we're going
to dump all the tweets into HDFS so the
first thing we'll do is that we'll fetch
all the tweets and using flume will
ingest it into HDFS and once all the
tweets about Big Data and Hadoop are
stored in HDFS will analyze all the
tools using hive alright so this is what
we're going to do so let me just go to
my virtual machine where I have set up
on my Hadoop cluster alright so I hope
that you are able to see my screen I
want some confirmation that you're able
to hear me properly and also see my
screen
all right so I guess you're able to hear
me and see my screen right
you
okay so let me just show you what are
the demons running here so I'll just run
this pseudo jps set is asking for
password and here are the demons that
are running in my virtual machine this
is all the demons the HDFS demons data
node name node secondary name node and
the yarn demons node manager and
resource manager all right so now let me
show you how to do the Twitter analysis
so first thing we need to do is that we
need to run a flume configuration file
okay so let me just show you where that
file is so I'll be just moving to the
directory which contains my flume
configuration file that I need to run in
order to fetch all much weight so it as
a user live then there is flume ng and I
have to go to the bin file which
contains my configuration directory all
right so here is my flume conf
so let me just open up the configuration
file and show you how it looks like so
I'll be using G edit alright so here is
my flume configuration file so we're
using flume in order to ingest all the
data into HDFS and the data I'm talking
about our tweets on Big Data and Hadoop
so in flume you have got three things
the first one is called source now
source must be connected to the data
source and in this case my data source
is Twitter because I'll be fetching
tweets right and then the second thing
is called sync sync should be connected
to the destination where I'll be dumping
all my data or my tweets into and in
this case I'm dumping it all into my
HDFS I've mentioned sink is HDFS and
then the third thing is called channel
so channel is basically a intermediate
buffer so this is so a channel should be
used it is because you need an
intermediate buffer just because the
writing and reading speeds are different
right
you can also relate it to yourself
because obviously you can read faster
than you can write with a pen on a page
right so that's why I'm using a channel
in this case I'm using my memory channel
so I've mentioned mem channel okay and
then I want only the tweets which has
Big Data and Hadoop mentioned so I've
mentioned the keywords that are one Big
Data and Hadoop so it will get me all
the tweets that has hashtag big data
hashtag Hadoop or even the tweet has
mentioned of this particular word Big
Data and Hadoop say it will only fetch
me those tweets and then in order to
connect to Twitter you have to use
certain API keys consumer key consumer
secret key and so let me show you where
you can find this so you just have to go
to apps a dot Twitter so here you can
there will be a button that says create
new application because basically
whatever we're doing we're creating a
new application so just go to this
particular website apps dot Twitter okay
so you'll find a form like this so name
your application and then provide a
description and also the website so the
website here basically means that the
landing page of your application and
since we don't have any you can provide
it with any kind of placeholder but make
sure that you provide it in this format
HTTP dot is a colon slash class and then
the URL so basically I've just provided
google.com it is harmless and then just
take on this and create a Twitter
application now I have already created
it so after we create your application
you're going to see something like this
so just go to keys and access tokens and
this is where you'll be finding all your
access tokens and your consumer API keys
in order to connect to Twitter so this
is where I have copy pasted all the keys
from in my configuration file so let us
go back and I have also mentioned that
where would it get dumped in HDFS that
will get dumped in a directory called
Twitter so let us go and create that
directory okay so for that creating a
directory in HDFS you have to use this
command Hadoop FS make dir and
we're calling it Twitter all right so
let me just show you the web UI for
Hadoop so you can just go to your
localhost colon five zero zero seven
zero so here you can just click on
browse file system and you can see it
has created the directory in my HDFS
Twitter okay so let me just go to the
flume configuration file so this is my
configuration file I hope that you have
understood what are the different
components that I've mentioned here so
now all we need to do is run this
particular file in order to fetch all
the data from Twitter so for that you'll
be using this command bin flume and G
you'll be running the configuration file
in order to fetch all the tweets now I'm
just copy pasting it only because I
don't want any typos because we don't
have much time so for that you have to
go to the folder your bin folder where
your so this is my bin folder here I'll
be running the command because I'm using
my flume agent and it doesn't my bin
folder so we're just pasting it and
enter so this is going to fetch all the
tweets related to Big Data and Hadoop
so this establishing connection and the
connection is established so basically
now it is gathering all the tweets
related to Big Data and Hadoop so I'm
just going to leave it on for a few
seconds more because we don't have much
time but when if you try to perform this
at your own home you can just leave it
for like half an hour an hour so that
you can fetch enough tweets about Big
Data Hadoop to get a better analysis of
everything that we're going to do okay
so I'm just going to leave it for
another few seconds
all right so I guess five seconds more
so here you can see that in my HDFS in
my Twitter directory there is one more
file created this is where all my tweets
are getting stored flume data dot some
numbers rtmp so this is another file
that is created inside my Twitter
directory which is storing all the
tweets so I guess it is good enough to
stop all right so I have stopped
fetching tweets so now let us go to my
HDFS and check what data it has gathered
so it has gathered around sixty two
point three seven KB's of data so if you
click here alright so this are the
different tweets so this doesn't look
very understandable because this is in a
semi structured format and if you
remember I told you that flume is used
to ingest unstructured and
semi-structured data and in this case
we're dealing with a semi structured
data okay so now I have fetched all my
tweets
it is in my HDFS right so all I need to
do now is I need to analyze it and for
that I have to use hive and in order to
go to my hive shell you just have to
type hive alright it is going to take a
few seconds to get into the hive shell
alright so I'm in my hide shell so the
first thing that I'm going to do is that
I'm going to add a jar file so this jar
file is basically asserting so this is
the command in order to add the jar
files add jar and I've mentioned the
path in my file system where the jar
file is located okay so this jar file is
actually a surday and a surday is
nothing but an interface that allows you
to instruct hive as to how a record or
how a data should be processed so sorry
basically means a serialize ER and
deserialize err however will not get
much into what surday is
so get to know more about when we
discuss
so basically I have added my surday here
so you can see added resource and the
next thing that I'm going to do is that
I'm going to create a table so I'll be
defining a schema using this command
alright so they're pasting so I'm
creating a table called tweets and I've
mentioned the different fields it will
contain along with the different data
types and I've also mentioned the sir
day it has okay so we're using H catalog
data Jason sir day alright so I'm just
going to run it okay I guess there is
one more tweet table so just to avoid
all the confusions so I'll just delete
the earlier table so you can just use
drop table tweets okay
so we'll run this command again alright
so it has created a different table
called tweets so if you want to see how
this looks like how the table looks like
so you can just type this command
describe tweets alright so here is what
the table looks like and if you would
have noticed the hive query language or
the hive queries that I'm running are
very much similar to sequel say if
you're already familiar with the sequel
language it is not going to be hard for
you to learn hive query language okay so
this is what the table looks like and
now I have created my table all I need
to do is that I have to get all the data
from the Twitter directory in my HDFS
and put it in my table okay so for that
I'll be using this command so I have to
use this command load data in path and I
have to provide the path in my HDFS so I
will just go and copy the actual path
okay
in the command because that might be not
the correct path here
okay so this is the command that we're
going to use that is what this command
is going to do is that it will get all
the data all the tweets in my Twitter
directory and it is going to dump it in
my table okay so it has dump so let me
just show you how the table looks like
now so you just use select star from the
table name is tweets and ended with a
semicolon just like how you do in sequel
alright so this is what it looks like so
now I have got fetch my tweets and as
they're in my HDFS from HDFS I have
created a table using hive and then I
have got all my tweets into the table so
now I can make some more analysis on it
right so let me clear the screen and let
us do some analysis on it so for that
we'll be running some hive queries let
me tell you what these commands do so
here I'm just pasting it so basically
we're going to find out what is the
activity about Big Data and Hadoop in
Twitter so if people are tweeting about
it and if there are many retweets on a
particular tweet it means that people
are discussing about that there is a lot
of fuss going on about Big Data and
Hadoop right so this is what we're going
to find out we're going to find out how
many tweets how many retweets on that
particular tweet and then finally we're
going to find out the top 10 tweets on
Big Data and Hadoop and we're going to
sort it in descending order so the one
so the most popular tweet about Big Data
and Hadoop will come on top and the rest
9 will come in a descending order ok so
I'm going to run it so just launching
job 1 out of 2 and there are two jobs
because we have got a nested query here
so then the first thing it is going to
find out whether what is the username of
the person who has made that tweet
whether it has a retweet and the second
job it is going to count how many
retweets and all ok and also let me tell
you that it might happen that I'm not
going to get top 10 since I
leave my flume longer to fetch all the
data all right so it might happen that
we might not get 10 and you can see that
there is a MapReduce going on because it
is together behind every hive query
there is a MapReduce job running and
here we have our result so here is the
username with this is char growth so for
one office tweet you have got three
thousand four retweets so definitely you
can just make out or you can just tell
that there is like a lot of activity
going on about Big Data and Hadoop
because if one tweet has got three
thousand more than three thousand
retweets and I have just left it for
just a few seconds right so there might
be more tweets about it so here is since
I was telling you might not get ten so
I've got one two three four five and six
one is null so you've got six rows okay
so this is the analysis that we have
made I hope you have enjoyed it you can
also go try it out at your home in your
own system if you have hive and flume
installed okay so now let us go back to
our presentation so if you want to learn
I have shown you very interesting things
but if you are interested to learn more
about Big Data hadoop training and you
want to take up at as your career option
let me tell you that you need a
structured training for that because
there are a lot of available content
online and there is a very good chance
that you might get lost or you might end
up learning just a little and not the
entire thing and that is why we at at
Eureka have designed a structured
training program for you so let me tell
you what this curriculum is all about so
the Big Data and Hadoop training at
Eureka is a five week of online live
instructor led training class so in the
first class you'll be learning about the
introduction to hadoop how Hadoop has
solved the big data problem and then in
the second class will be starting with
HDFS how to set up a Hadoop cluster a
single node cluster multi node cluster
etc
and as you move on from the first class
to the second class you will be doing
some assignments and you'll be studying
some case studies some real-life case
studies and let me tell you that
practice is very important and this is
exactly what you will find at Edie Rekha
because you'll learn all the concepts
from anywhere but it is very important
to practice because that is exactly what
you're going to be doing out there in
the real world right and I know it is
very hard to find relevant projects
online so you have got case studies and
assignment designs on the concept that
you have just learned recently in the
previous classes so you'll get a very
good practice in the third class you'll
be starting with Map Reduce in the
fourth class he has learned some more
advanced MapReduce concepts ok in the
class 5 you'll be starting with Pig
which is the scripting tool as I told
you then in the class 6 you'll be
learning about height so here you'll get
a detail about what a Sur day is and how
it works and everything about height and
some advanced concepts of hive also in
the seventh class and you'll be starting
with an introduction to HBase in the
seventh class HBase is a no sequel
database that is highly used in
industries nowadays some more advanced
HBase concepts in the eighth class in
the ninth class Apache spark which is
used to process real time the data here
we'll be learning about all the
different components of Apache spark
Apache like the spark sequel and then a
spark ml lab and all other related
concept resilient distributed databases
etc and the tenth class finally you
learned scheduling with Uzis and I'll be
commencing with your Hadoop project ok
so after you have completed all the
classes and you've completed your
assignment and project you will be Big
Data and Hadoop certified by Ed Eureka
and let me tell you that this
certificate that you're going to get is
a verifiable certificate and you can put
it up on your LinkedIn profile which is
I think very much beneficial for you
because it will give you a very good
exposure to your future employers and
apart from that Ed Eureka certification
also holds a very good industrial
recognition we have a lot of clients
that you might have heard of like Visa
forth so they trust on us and so
definitely you know that if these
companies are trusting on our
definitely it has got a very good
industrial recognition and in case that
if you're wondering that what if you
ever get stuck in between your
assignments or projects so don't worry
for that we at Eddie Rica have a 24/7
team of expert who is going to help you
and guide you in case you come across
any problem maybe a 3m in the morning or
at any time of the day they will get
back to you instantly and solve your
problem and apart from that there are
more benefits to enroll with Eddie Rica
first you'll get a lifetime access to
your LMS so your LMS will contain all
the required study materials that you'll
be needing in order to get trained on
Big Data and Hadoop and you'll have a
lifetime access to you and meaning that
even if you have completed the course
and you still want to revisit after some
time to go through all the concepts you
can still do that even if Hadoop has
come up with a new version and we have
upgraded our course according to that
you can still go and access the newly
upgraded course even if you have studied
the older version and even if you're
going for an interview and you want to
go through all the concepts again after
a year or so you will have access to it
for a lifetime
apart from that your LMS will also
contain a class recording after the
completion of each of the class so you
can go through it again and even if you
want to retain another class with our
industry experts who are going to
instruct you during the sessions you can
still do that without any extra cost and
the industry experts essentially are the
early birds in this particular
technology and they are the subject
matter experts they have essentially 12
to 15 years of industry experience and
they will be sharing real-life case
studies with you which are actually
ongoing projects and different companies
so definitely you'll get a taste of what
it is like to be a big data professional
even before you become one and finally
there are multiple projects to choose
from and this is very important because
you need to practice you need to put all
the concepts that you've learned into
use so let me just tell you what are the
different projects that we have at Eddie
Rica so the first one is related to
media so this here will be designing a
system very close to IMDB so here you'll
be finding out what are the most
top-rated movies so that you can
recommend it to the users and also
you'll be finding out some more
correspond
details stood with that movie like the
release your duration etc the second one
is for our fellow youtubers here the
YouTube moguls who want to be the moguls
in YouTube suffer them you'll be
analyzing to find out and identify which
are the top five categories that they
can make video on in order to make their
channel and make their videos more
popular ok so you'll be finding the top
5 categories and what are the top videos
the most rated and the most viewed
videos in each of the categories the
next one is related to aviation so here
you'll be pretending that you're working
as an analyst for a newly like airline
companies so they have given you the
tasks that you need to I need to find
out that which are the most busiest
airport which country has the most
busiest airports so that they can
station their newly crafted airplanes in
those airports so you'll be analyzing
the airline's data to find the list of
the operating airports in the country
and which country has the highest number
of airports then the next one is related
to retail so this one is ready to
customer support which i think is very
important because you need to keep your
customers happy right so here will be
given an organization data to analyze
and here you'll be finding out that what
are the total numbers of complaints
filed and which are the complaints that
did not have any timely response so they
can put them up in the top priority list
and they would be taken care of at the
earliest because you don't want unhappy
customers then their next one is related
to tourism the travel earth so here
you'll be designing a system very close
to TripAdvisor I'm very sure that you've
heard about TripAdvisor
so here you'll be actually recommending
people some popular holiday destinations
so be given a travel data set to find
out which are the most popular most
traveled places so that it can recommend
it to the users as the popular
destination that they should travel to
you'll also be finding out some top 20
air revenue destinations and some more
analysis on it and then there is the
imaginary bank of Eddie Rica this is
related to finance so here you'll be
finding out on categorizing different
loans according to the reasons given so
definitely so here you'll be like
categorizing like how many car loans
home loans person
loans etc so these are just the six of
the projects that I'm showing you there
are many others to choose from you can
choose to complete any one some of them
or even if you want you can choose to
complete all of them if it looks
interesting to you okay and don't worry
you'll be guided by our support team and
our experts in order to complete your
project and if you also see we've got a
wide variety of projects related to
every domain so the six that we saw have
like different domains media social
media aviation there is travel finance
retail etc right so these are very
interesting projects that you can choose
from if you choose to learn it from a
Eureka and enroll the big data and
hadoop course and if you're thinking
that okay I've chosen my project how can
I just execute it where will execute all
my programs so don't worry so once you
enroll good at Eureka you will be given
a virtual machine that will have all the
software's pre-installed for you in
order to run all your programs and
projects on ok and even if your system
let's say does not comply with the
configurations that is required in order
to install the VM and run the VM don't
worry you can still use the remote
instance so you'll be able to so you'll
be given an access to the remote Hadoop
cluster so you can access that this will
also have all the software's
pre-installed alright so then let me
just show you the ad Eureka LMS what
they're your LMS is going to look like
how everything is sort there ok so let
me just open the
okay so I hope you're able to see my
screen you are able to see the LMS right
so I want some confirmation
you
you
all right I guess you're able to see my
screen so here is what your LMS is going
to look like it will have different
modules so if you click on here so you
can find all the presentation quizzes
assignments tutorial and all the
interview questions so it'll contain all
the documents that is required for you
to understand Big Data and Hadoop and
you'll have a lifetime access to it so
you can just have all the details about
all the different modules here okay and
then there is something for you all so I
will be sharing this link so this is
basically a link to a survey form this
is going to look like this so please
fill it up so that we can improve our
content better and for you this is also
beneficial let me tell you how because
it will help you to avail some exclusive
offers if you ever choose to purchase
any course form at Eureka so make sure
that you fill up the survey form over
here so make sure you provide your
correct email address correct phone
number and your name so that we can get
back to you and provide you with the
exclusive offers and of course it will
help us to give you a better learning
experience as well okay and here you can
find if you want to learn more about Big
Data and Hadoop we have got some
tutorial videos on our Eddy Rica channel
on YouTube so you can go through this
playlist so it has got different videos
on hive on Pig and all other Hadoop
ecosystem tools on basic Big Data and
Hadoop so you can go through all the
videos and make sure that you click that
like button if it is able to help you if
you learn something okay and also if you
want to know if you're coming up with
more videos make sure that it click on
the bell icon on the video so it will
give you notifications every time we
upload any video which is going to be
very much beneficial for you since you
get to learn a lot okay and you can also
go through our blog series on Hadoop so
there are many blogs on Hadoop that you
can go through if you are fond of
reading I'm very sure that you're going
to have a very good reading experience
this is written in plain English very
understandable okay so make sure you go
through our you
tube and our blog Channel as well and
also please I will tell you guys again
please fill up the survey form this will
be beneficial for both of us a symbiosis
relationship right so you'll get an
exclusive offer and we'll get to
understand how can we make a better
learning experience for you
you
all right so you can leave your
questions on the chat box and also
feedbacks so make sure that you fill up
that survey form also make sure to LIKE
this video if you have enjoyed learning
with that Eureka and thank you so much
for giving us your valuable time I
appreciate all of you for attending this
session thank you so much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>