<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Science with R | Data Science Tutorial for Beginners | Introduction to Data Science | Edureka | Coder Coacher - Coaching Coders</title><meta content="Data Science with R | Data Science Tutorial for Beginners | Introduction to Data Science | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Science with R | Data Science Tutorial for Beginners | Introduction to Data Science | Edureka</b></h2><h5 class="post__date">2018-01-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oVwUpuALP78" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey guys this is Barney from India Rekha
and in this session we'll be
implementing data science concepts with
our I'll start off by giving a brief
introduction to data science and then
we'll take a data set and perform data
manipulation tasks on it following which
we'll find out interesting patterns from
the data by visualizing it with the help
of graphs and plots and finally we'll
build some machine learning algorithms
and evaluate their efficiency so what is
data science will understand that
through a story now this is John and
John stumbles upon terms such as data
science in machine learning and starts
wondering what they exactly are now John
decides to go to uncle George who is a
data scientist and uncle George happily
helps or curious John uncle George tells
us that data science is all about making
the data talk to us that us R is an
umbrella term which encompasses multiple
domains such as data manipulation data
visualization machine learning data
reporting deep learning and statistical
analysis and for most of his data
science tasks uncle George uses R and
hence will follow uncle George's advice
and use R for our data science tasks as
well uncle George tells us some features
of our it else's that are the steering
complete letters whatever computational
process it is it can be done with the
help of our R is also a dynamically
typed programming language which means
that the focus is on the value of the
object and not on the data type of the
object are also provides more than
10,000 free packages that is whatever
you need us there is a package for that
if you want to do data visualization
there is a package for that if you want
to do data manipulation there is a
package for that too and are can easily
be integrated with other popular
software such as tableau SQL Server and
so on now it's time to install our on
our systems we can install our from cran
repository so let's go ahead and
Staal are in Google I'll just simply
type out download are I click on the
first link and since I use a Windows
system I'll be downloading the latest
version of our four windows the latest
version is our three point four point
three I click on this link and the
download starts now we would also
require an IDE for our so our studio is
the IDE let's go ahead and download our
studio again I'll just type our download
our studio I will click on the first
link I select download again since I'm
using the Windows version I'll download
the Windows version of our studio so we
have both our and our studio on our
systems and we have installed it
successfully now it's time to implement
the data science concepts on real-life
data sets well start off with data
manipulation so you have uncle George
here again uncle George tells us that
data manipulation deals with simple
operations such as summarizing
aggregation and filtering now our
provides a specific package for data
manipulation which goes by the name of
deployer and our has some special
functions for this purpose such as
select filter mutate and arrange and for
the sake of the key study we'll be using
the hedge flights data set so let's
start data manipulation we would have to
load the deployer package and do that
we'll be using the library function with
the help of the library function we can
load any package and prior to loading
any package we're supposed to install it
so I click on the install tab I type out
and apply over here I select the plyo
and when I click on install the package
will be installed and since I've already
done it I don't need to install the
package again since this is done you
will also have to load the hitch flights
data set I load it with the library
function now it's time to look at the
data set with the help of view function
view of hedge flights head flights data
set comprises of all of the domestic
flights which departed or arrived from
the
Airport will start with the data
manipulation task now the first function
is select with the help of the Select
function we can select some specific
columns from the data set that s if you
want to select only the first five
columns from the data set we can do that
if our purpose is to select the first
two columns and the last two columns we
can also do that and otherwise if you do
want to select only the alternative
columns from the data set we can also do
that so over here this is the Select
function first you would have to give
the name of the data set since you're
working with the hedge flights data set
we've given the name of the data set
followed by the names of the column
separated by comma that is from the
hedge flights data set we are selecting
flight number arrival time and departure
time columns and storing the result into
the flight one object let's have a look
at the result this was the original data
set and from the original data set we've
selected only three columns which are
flight number arrival time and departure
time let's look at the second select
command now instead of giving the names
of the columns we can directly give the
column numbers that is from the heads
flights data set we are selecting the
fifth sixth and the eighth column so
this is the fifth column this is the
sixth column and this is the 8th column
so selected all those columns and we've
stored those columns into the flight one
object so view of flight one again 5th
6th and 8th column from the head face
data set and these are the only three
columns selected
now from the hedge flights dataset we
are selecting all those columns which
have the column name time that is if we
look at this data set then all of those
column names which consists of the
string time in their column name will be
selected
so we see that four columns have been
selected departure time arrival time
actual elapsed time and a time and since
all of the column names contain the
string time in them only these columns
have been selected similarly in this
command we are selecting all of the
columns from year to arrival time that
us this is the year column and this is
the arrival time column and if I just
simply put a colon in between these two
columns I will get all of the columns
between these two view of flight one now
this gives me all of the columns from
here to arrival time again instead of
giving the column names we can directly
give the column numbers so from the
hedge flights data set we are selecting
the first six columns that is 1 to 6 so
the result will be the same view flight
1
so you've selected the four six columns
and stored it into the flight one
dataset now that is done let's go ahead
with the final select command now from
the hedge flights data set we are
selecting all those columns which start
with D and all of those columns which
end with time that us if we look at this
data set all of those column names which
start with the string D and all of those
column names which end with the string
time let's have a look at the result
flight one so these are the two columns
which start with D and these are the
four columns which end with time quite
interesting isn't it
so that was the Select function with
which we were able to select some
specific columns from the data set now
it's time to head on to the next
function which is mutate with the help
of the mutate function we can bring in
new columns by tweaking the old columns
that us over here from the hedge flights
data set I am subtracting the air time
from the actual elapsed time and storing
the result into the actual ground time
so we get a new column which is actual
ground time which is basically air time
subtracted from actual elapsed time
let's look at the flight one data set
this is the actual elapsed time and this
is air time we'll get a new column which
is actual ground time so if you look at
these two values actual elapsed time is
60 and a time is 40 if you say 60 minus
40 you will get 20 which is actually
their actual ground time which gives you
20 similarly we can perform other mutate
functions and bring in new columns in
the head flights data set we did not
have the average speed of the flight we
only had two columns which told us the
distance traveled and the air time now
since you would also want to see what
was the average speed of the flight you
can just obtain it by this formula which
is distance upon a time and store the
result into the new column which is
average speed so
Stowe's the result again into the flight
one object let's have a look at the
flight one object now this is the flight
one object now we see that and you
column has been added which is average
speed which contains of the average
speed of all of the flights this is done
now we would want to find out the total
time spent in taxi so the total time
spent in taxi is taxi in plus taxi out
your flight 1 we get a new column which
is total taxi
this was taxi in and taxi out it was 7
and 13 so that becomes 20 minutes and
hence the total time spent in taxi was
20 minutes for the first row this would
be the final mutate command in this we
are trying to get a new column which
would tell us the total delay for the
flight initially we just had delays for
the arrival and departure and we did not
have any column which told us the total
delay of the flight and hence if you
would just add the arrival delay and the
departure delay you would get a new
column which would tell us a total delay
and you stored the result into a new
column which has time loss let's have a
look at the flight one object flight 1
we have a new column which has time loss
which tells us the total delay for the
flight we're done with the mutate
command as well now it's time to head on
to the next function which is filled up
with the help of filter function we can
filter out some specific rules depending
on a condition like over here from the
hedge flights data set we are selecting
only those observations where the
distance is greater than 3000 miles so
let's have a look at the flight one
object let's look at the distance column
this is the distance column we see that
all of the values are greater than 3000
just to be sure we'll also find out the
range for this range of flight one
dollar distance now see that the minimum
value of distance is 3266 and the
maximum value of distance is 3904
and hence this all of the distance
values are about 3000 we'll head on with
the next fill to command now from the
hedge flights dataset we are selecting
only those unique carriers which have
the tags
double-o u.s. or double-a let's look at
the result view of flight 1 we'll look
at the unique carriers we can use a
table function to find out what are the
unique carriers present so table of
flight $1 unique carrier which tells us
that there are only three unique
carriers which are double a double O and
us and hence out of these 23,000 387
entries only three unique carriers are
present time for next filter command
this is quite interesting over here from
the hedge flights data set we are trying
to understand
are there any values where the total
time spent in taxi is greater than the
total time spent in a plane that is taxi
in + taxi out is greater than 8 time
view of flight 1
we have taxi in and taxi out over here
we have the a time over here and if we
look at the second row we see that taxi
in pressed ax e out is 50 and over here
the a time is 43 that s the person has
spent 43 minutes in the plane and 50
minutes in the taxi similarly if we look
at the third row 35 + 10 is 45 and again
the time spent in plane is 43 while the
time spent in taxi is 45 thus all of
these 1389 entries consist of those
observations where the person has spent
more time in a taxi than the plane you
can also give multiple conditions in the
filter command
so over here we are trying to see how
many of the flights have departed before
5 a.m. and how many of the flights have
arrived after 10 p.m. that is departure
time is less than 500 and arrival time
is greater than 2200 view of flight 1
this is the departure time and this is
the arrival time we see that all of
these entries the arrival time is
greater than 2200 that as the flights
have arrived after 10 p.m.
this would be the final filter command
and over here from the hedge flights
data set we are trying to see how many
of the flights were destined towards JFK
Airport and out of all of those flights
how many of them were canceled that is
since we use and over here both of these
conditions need to be satisfied
she needs to be capital of you let's
make it capital the cistern view of
flight one let's look at the destination
column we see that there are 18 entries
and for all of those 18 entries the
destination is JFK and again for
canceled we see that for all of the 18
entries the value for canceled is 1
now you can also see that there are lot
of any values over here and that is
because since the flight was canceled
there is no departure time or arrival
time for all of these flights that was
filter then comes the arranged function
from the deployer package with the help
of arranged function we can either order
the data set in ascending or descending
order with respect to one particular
column so we are ordering the hedge
flights data set with respect to the
departure delay column let's look at the
result flight one let's look at
departure delay we see that this column
has been arranged in ascending order so
the minimum value is minus 33 and
obviously the maximum value would be the
last row of this data set
this arranged function is quite
interesting so we are adding the
departure delay plus arrival delay that
us will be arranging the data set in
such a way where the total delay is
minimum will be coming at the top and
where the total delay is maximum will be
coming at the bottom view of flight 1
let's look at departure delay and
arrival Deeley so these are two values
we see that arrival delay the minimum
arrival delay is minus 70 and the
minimum departure delay is minus one
over here so when we add these two the
total arrival delay plus departure delay
becomes minus 71 so this since this is
the minimum value this comes at the top
we're done with arrange now it's time
for the next function which are
summarized now summarize is a very handy
function because it gives us quick
results now we're trying to find out
from the hedge flight data set what was
the minimum distance traveled and what
was the maximum distance traveled all we
need to do is select the data set and
give the functions which would give us
the summarized result so minimum of
distance and maximum of distance we see
that minimum distance travelled by any
flight is 79 miles and the maximum
distance traveled by any flight is 3904
miles we'll head on with the next
summarized function and this from the
hitch flight data set we are trying to
determine what was the minimum arrival
delay the average delay and the maximum
arrival delay and the standard deviation
in arrival delay so this is the earliest
average latest and as deep so this tells
us on an average there was an arrival
delay of seven minutes and the maximum
arrival delay was 978 minutes
interesting observations again
this is done now with the help of the
pipe operator we can combine the
different functions from the deployer
package the pipe operator comes from the
magneto package and it helps us to
connect objects so what we're doing here
is from the hedge flights dataset we are
selecting only those columns which
contain the string time and from those
columns we are filtering out proves
where air time is greater than 60 let's
look at the result view of flight one so
from the hedge flights data set we've
selected only those columns which
contain the string time and that is why
we have departure time arrival time
actual elapsed time and a time and for
all of these observations the a time is
greater than 60 again just to be sure
let's find out the range of this range
of flight $1.00 air time so we see that
the minimum air time was 61 and the
maximum a time is 41 and hence the
condition is satisfied of you
let's combine new functions and new
conditions over here from the hedge
flights data set we are filtering out
those observations were unique carrier
as W and from all those observations we
are summarizing the result and we are
trying to find out the minimum air time
for that result so what we see is out of
all the observations where unique
carrier is W n the minimum air time is
25 minutes that was data manipulation
with the help of deploy a package let's
head on to the PPT again so a downward
data manipulation now it's time for data
visualization and uncle George is back
uncle George tells us that data
visualization deals with understanding
the significance of data through graphs
and plots and for the sake of the key
study we'll be using the house data set
now let's visualize the house data set
this is the dataset guys a comprise of
columns such as price of house the large
size whether the house has a waterfront
annoyed was the land value whether the
house is newly constructed on art and
other columns so on
for the visualization purpose we'll be
using the ggplot2 package so let's load
it again we can load any package with
the help of the library function prior
to loading any package we're supposed to
install it so I click on the install tab
I type our ggplot2 over here and when I
click on install the package will be
automatically installed this is done now
it's time to do univariate analysis with
the help of a histogram now
with the help of ggplot2 we can add
multiple layers over a single layer that
is what we're doing over here is we
select the dataset first
that'ss we obtain the data layer and on
to the data layer we assign new
aesthetics so we've selected the house
dataset and from the house dataset we
map the price data column on to the XE
static so this is the house data set and
we've mapped the price data column on to
the X aesthetic so done with the data
layer we're done with the aesthetics
layer comes the geometry layer and with
the help of geometries we can select any
plotting technique that is it can be
either a scatterplot box plot histogram
a bar plot and so on since so here we
are plotting a histogram they're doing
univariate analysis that is we're trying
to understand the distribution of price
we see that the average price is around
two likes you see this values to e + 0 5
for e + 0 5 6 e + 0 5 they basically
mean 2 into 10 power 5 + 4 into 10 power
5 which would basically amount to lakhs
full ax + 6 likes that as the average
price of the house is somewhere around 2
lakhs and the maximum price of the house
is somewhere closer to 8 like
let's add color to this plot until here
the command is the same so we're adding
new attributes into the job histogram
function which are fill and call so I'm
assigning the value pale green for to
the fill attribute and I am assigning
the value green to the call attribute
you see this color this is pale green
for and you see the color for the
boundary that is green so this was
univariate analysis with the help of a
histogram now it's very important to
understand where to use histogram and
where to use a bar plot so whenever you
want to see the distribution of a
continuous variable you will go with a
histogram and wherever you would want to
see the distribution of a categorical
variable you'll go with a bar plot since
we've already seen the distribution of a
continuous variable which is priced now
it's time to see the distribution of a
categorical variable which is a
conditioning variable so I've selected
the house data set and from the house
data set I'm mapping the air
conditioning column on to the X
aesthetic and the geometry selected by
me
is bar plot and the color is orange so
assume this and see we see that most of
the houses do not have centralized air
conditioning colors this distribution is
larger than this distribution so the
simple inferences most of the houses do
not have air conditioning al very few
houses have centralized air conditioning
this was histogram and bar plot next
comes a scatter plot again we have
selected the house data set now a
scatter plot is used for multivariate
analysis that is to observe how does one
variable change with respect to another
variable and we've map the price data
column on to the y is thetac and we've
mapped the living area data column on to
the x aesthetic the color is determined
by the number of rooms and the geometry
used is John Point which will give us a
scatter plot
with my prize onto the y-axis living
area onto the x-axis and the color is
determined by the number of rooms let's
zoom this we see that there is a linear
relationship between the price of the
house
and the living area that us as the size
of the living area increases the price
of the house also increases now if you
look at this part that is the colors if
you look at these colors which is mostly
green you see that green color
determines that the house has either
four or five rooms and the houses are
the four or five rooms then we can
conclude that it's living area is low
and also the price of the house is also
low and you see this dots over here the
color is pink so we can say that if the
color is pink then the house would have
around twelve rooms and it houses twelve
rooms then obviously the size of the
living area would be large and also the
price of the house will also be high so
these are the sort of inferences which
you can make the scatterplot this is
done next we'll be making a box plot a
box plot is used to understand how does
a continuous variable change with
respect to a categorical variable and
again over here we selected the house
dataset and on the house data set we map
the price on to the Y is thetac and
we've mapped the number of rooms onto
the x aesthetic the color is determined
by the number of rooms and the geometry
is boxplot so we see that as the number
of rooms increases the price of the
house increases you see these lines
these are the median values again so the
same inference as the number of rooms
increases the median value of the price
of the house also increases so that was
boxplot so these were the commands with
which we did a bit of data visualization
with the help of the ggplot2 package
let's again head on to the PPT to done
with data manipulation be done with data
visualization now it's time for machine
learning uncle George's back again now
uncle George tells is that machine
learning is the field that deals with
how to make the computers lower these
are the different types of machine
learning algorithms which are generally
present so broadly speaking we have
supervised learning reinforcement
learning and unsupervised learning
in supervised learning it is a type of
machine learning algorithm that uses
unknown data set to make predictions
that is we already have a set of labels
and from those set of labels we build a
model and we predict these values on a
new data set so let's go ahead and do it
practically
we'll start with linear regression
linear regression comes under the
purview of supervised learning and since
it comes into the purview of supervised
learning it has labels and we try to
determine how does one value change with
respect to another variable or in
simpler terms we try to understand how
does the dependent variable change with
respect to the independent variables
since we'll be building the linear
regression algorithm on top of the empty
cause data set let's have a look at this
so this is the empty cast data set and
we try to determine how does the
miles-per-gallon column vary with
respect to other columns now for any
machine learning algorithm we're
supposed to split the data into two
parts which is training and testing and
to do that we'll be needing the CA tools
I cage which gives us the sample dot
split function with the help of the
sample dot spread function we can split
the observations into two categories one
category of the observations will have
labels mark - true another category of
observations will have labels marked as
false so over here since we are trying
to predict the values of the
miles-per-gallon we've taken this as the
first parameter and split ratio is 0.65
that is 65% of the observations will
have the true label and 35% of the
observations will have the false label
the spreading is done now it's time to
divide the data so all those
observations where the split label is
true we'll select all this observation
z' from the empty cast data set and
assign them to the training set again
similarly we'll select all of those
observations
where the split tag is false and store
them in the test set let's look at the
number of rows in train and test set and
row or frame
tells us that there are 20 rows in the
training set similarly and row of tests
tells us that there are 12 rows in the
test set since the division is done now
it's time to build the linear model on
top of the training set so you see the
tilde symbol over here
the value which is given on the left
side of the tilde symbol is the
dependent variable and the value which
is given on the right side of the tilde
symbol are all the independent variables
and since we have given a dot over here
that basically means that we are trying
to determine how does miles-per-gallon
vary with respect to all other columns
of the data set that is miles-per-gallon
is the dependent data set and all of the
columns are the independent variables to
build the model and we store the result
in mode 1 this is done now it's time to
predict the values so we select the mod
1 and we evaluate the results on top of
the test set and we store the result in
an object which goes by the name of
result let's look at this view of so
these are the predicted values let's
combine the actual values and the
predicted values and store the result in
a new data set so actual values which
are the miles-per-gallon values from a
test set and the predicted values which
are stored in the result set I'm
combining these two values and I'm
storing them in a new data set which is
named final view or final well gave us
this so we see that these are the actual
values and these are the predicted
values now it's time to find out the
error in prediction and before we do
that since it is a matrix we're supposed
to convert it into a data frame so a
store data frame with the help of this
function we can convert a matrix to a
data frame we have converted the final
which was initially matrix into a data
frame since that is done it's time to
find out the arrow in prediction finding
how the error in prediction is quite
simple we just need to subtract the
predicted values from the actual values
that as actual minus predicted gives the
arrow in prediction that is what you see
over here final dollar actual minus
final dollar predicted is the error
we are final so these are the actual
values these are the predicted values
and this is the error in prediction now
to evaluate the performance of any model
we have something known as root mean
square error so the lower the root mean
square error the better the model so
root mean square error is again simple
so we need to square the error find out
its mean and then followed by the square
root store the result in our MSE one
let's look at our a messy one
so our MSE one has fifty two thousand
six hundred and eight now to compare
this model with another model let's
build a new one now in this LM function
in the independent variable side I am
selecting all of the variables except
the number of gears and the number of
carburetors there is all of the columns
except these two columns and again the
dependent variable is miles per gallon
data as the training set and I'm storing
the result in more two now that this is
done it's time to predict the values on
the test set so this is the mod 2 which
is the built model and I'm trying to
predict the values on the test set and
I've stored the result in the result
object it's time to bind the actual
values and the predicted values so the
actual values which were in the test set
and the predicted values which are
stolen result and combining those two
and storing and the final object this is
done let's look at final so these are
the actual values and these are the
predicted values again we would need to
find out the error in prediction before
we do that let's convert this matrix
into a data frame and we'll be using the
a store data frame function to convert
the metrics into a data frame we'll use
the C bind function now to combine this
final data set with the new column which
is Error error
we'll find out by subtracting predicted
values from the actual values view of
final so these are the actual values
predicted values and this is the arrow
in prediction now it's time to find out
the root mean square error so we'll find
out the square then we'll get the mean
and finally we will perform the square
root and store it an RMS c2 let's see
what is the value of pharmacy to
see that the values of our MC 1 and our
MC 2 are same both of them give the
value of 50 2608 that us they cannot say
which model is better than the other
model and hence both of the models are
similar in terms of accuracy that was
linear regression which comes under
supervised learning so done with linear
regression and you've used the empty cos
data set to perform linear regression
now it's time to perform classification
again classification comes under the
purview of supervised learning and since
it comes under supervised learning
classification also has labels if you
look at this slide we see that we have
an observation and we're trying to
assign a label to this observation and
if we assign the label of boy to the sub
servation the classification will be
right and if we assign the label girl to
in sub survey ssin the classification
would be wrong the sake of the key study
we'll be working with the census data
set so let's go ahead and do that view
of census data this is the census data
set which comprises of columns such as
age of the person has working sector
raise his six the native country and a
salary and here we're trying to classify
the salary that is whether the person
earns greater than 50k or less than 50k
this is the classification that we are
trying to do over here let's go ahead
and do that we've already looked at the
data set now it's time to perform
classification and we'll be building a
decision tree to do this our provides is
a package called our part where it
stands for recursive partitioning and
recursive partitioning can be used to
perform a decision tree algorithm again
prior to building any machine learning
algorithm we're supposed to split the
data into training and testing that is
why again we're supposed to lose the CA
tools package once that is done we'll
use a sampled or split function and
since we would want to classify the
salary column
we'll give this as the first parameter
and the second parameter is the split
ratio that a 65% of the observations
will have the true label
and 35% of the observations will have
the false label when I'm storing the
result in my split object now out of all
of the sub survey shion's where the
label is true they've been selected and
stored in the training set similarly all
of the sub survey shion's
which have the label false have been
selected and stored in the test set
so the splitting is done now it's time
to build the decision tree algorithm on
top of the training set
we'll use the our part function to build
the decision tree algorithm salary will
be the dependent variable and all of the
columns will be the independent
variables and that is why we've given
salary on the left side of the tilde
symbol and dot has been given on the
right side of the teller symbol that
basically means all of the variables
except the salary column will be the
independent variables the data is
trained and method used is class because
this is a classification algorithm and
we're trying to determine whether the
class is less than 50k or greater than
50k will store the result and mod
classify once the model has been built
we would want to predict the values on
the test set so we've selected the model
and we are predicting the values on the
test set and again since this is a
classification algorithm the type given
as class
the result is stored in the result
object so view of result these are the
predicted values let's determine the
accuracy of this prediction
we'll be loading the carrot package and
E one zero seven one package for this
purpose the carrot package consists of
the confusion matrix function which
helps us to determine the accuracy of
the prediction so let's go ahead and
determine the accuracy of the prediction
so this is the confusion matrix and this
is a table which tells us that these
were the actual values and these who are
the predicted values now wherever in the
actual values if the value was less than
or 50 K and the prediction was also less
than or 50 K these are all those values
and over here
these are the values where the actual
was greater than 50 K but it was
predicted that these values are less
than or equal to 50 K that is this has
been pricked it correctly but this has
been predicted wrongly again over here
actual is less than or equal to 50 K and
predicted is a greater than 50 K so this
is wrongly classified and this we see
that it is greater than 50 K and greater
than 50 K so this is correct
classification so if you look at the
left diagonal
this stands for those values which have
been correctly classified now to find
out the accuracy will just add seven
double nine to plus of one four six nine
which consists of all those observations
which have been correctly classified
upon all of the observation status seven
double nine two plus of 660 plus of one
two seven five plus of one four six nine
so this is the accuracy of the model
again if you compare this with this it
is the same so this tells us that the
accuracy is 83% and again as calculated
by us the accuracy is 83% not as 83% of
the values have been correctly
classified while the other 17% of the
values have been wrongly classified that
was classification which was under
supervised learning now it's time to
learn unsupervised learning and
unsupervised learning the observations
do not have any predefined labels so if
we look at this slide these are a group
of observations and these are been
grouped into two clusters and that has
been done on the basis of similar
features that is there is high intra
cluster similarity for all of these
observations and again
there is high in trucks Astor similarity
for all of these observations but when
we talk about inter cluster similarity
that is if it off with the first cluster
and the second cluster there is very low
similarity this is the idea behind
clustering there is high intra caster
similarity and low inter cluster
similarity for the case study we'll be
working with the iris dataset let's go
ahead and cluster these observations
this is the iris dataset which comprises
of five columns sepal length sepal width
better length petal width and the
species column let's look at the
different species of the iris flower
table of iris dollar species we see that
there are three species of the iris
flower which are setosa rosy color and
virginica and all of these three species
are equally distributed now we'll be
building a key means algorithm for the
clustering purpose and the k-means
algorithm only works with continuous
variables and hence we are supposed to
remove the last column which is a
categorical variable so we'll remove the
last column and store it in a new data
set and we'll name it as iris 4 let's
have a look at the iris 4 dataset this
is the new data set which comprised of
only 4 columns now let's build the
k-means algorithm for the key means
function we give the data set and we
give the number of clusters and we store
the result in k1 let's bind the result
to the iris 4 data set again view of IRS
4 so these are the four columns and
these are the clusters let us use a
table function and let us see how has
the clustering been done table of iris
$4 K $1 cluster see that there are 62
observations in first cluster 38
observations in second cluster and 50
observations in third cluster now let's
also combine the species data column to
this and let's see the result view of RS
4 so we have the cable dollar cluster
and also the species column let's go
down and let's analyze this properly we
see that for the first 50 observations
the cluster number is 3 that is all of
the forests 50 observations belong to
the third cluster and all of these
observations belong to the species at
oosa so what we can conclude from this
is all of the species of setosa family
have been clustered into the cluster 3
again
if we look at this we have virginica and
wer see color now this seems to be quite
ambiguous we cannot conclude anything so
we see that out of virginica and
versicolor some of the observations have
been clustered into the cluster 3 and
some of the observations from vesicular
and virginica have been grouped into
cluster 2 so this was clustering which
comes under the purview of unsupervised
learning so this is the end of the
session summing up we've learned data
manipulation they work with the deployer
package which provided us functions such
as select filter mutate arrange and
summarize after which we visualize the
data with the help of the DG plot to
package then we've understood some
machine learning algorithms we wonder
stood supervised learning and
unsupervised learning
we've learned that supervised learning
consists of labels and done supervised
learning does not consist of labels and
then supervised learning we've
implemented linear regression followed
by classification and for unsupervised
learning we've built the k-means
clustering algorithm so thank you for
watching the video hope you enjoyed it I
hope you enjoyed listening to this video
please be kind enough to like it and you
can comment any of your doubts and
queries and we will reply to them at the
earliest do look out for more videos in
our playlist and subscribe to our
advocate channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>