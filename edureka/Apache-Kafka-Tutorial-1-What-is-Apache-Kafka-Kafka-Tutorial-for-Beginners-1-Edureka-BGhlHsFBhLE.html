<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Kafka Tutorial - 1 | What is Apache Kafka? | Kafka Tutorial for Beginners - 1 | Edureka | Coder Coacher - Coaching Coders</title><meta content="Apache Kafka Tutorial - 1 | What is Apache Kafka? | Kafka Tutorial for Beginners - 1 | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Kafka Tutorial - 1 | What is Apache Kafka? | Kafka Tutorial for Beginners - 1 | Edureka</b></h2><h5 class="post__date">2015-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BGhlHsFBhLE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone welcome everyone good
morning good evening
those who joining from West Coast and
I'm Madan Mohan and it just briefed
about me so recently I started working
on a platform which works on predictive
analytics and provide a lot of
recommendations based on the consumer
personas and activities and the platform
we call it as a pro curvy so basically
we started exploring for a reliable and
distributed connecting system or a
messaging platform eight months back and
after having a lot of discussions and
after having a lot of insightful
analysis we felt Apache Kafka was the
natural fit and we just tried to work on
a Kafka for last eight months and we are
super excited with the kind of features
and the kind of scale capabilities it
was offering and it is still going good
for us so what we're going to go today
is I'm going to briefly talk about
Apache Kafka and our learnings in the
last six to eight months and I should
thank Eddie record to providing the
medium to share our knowledge and we'll
all just briefly touch base about in the
next horror so okay so if if time
permits I I may just quickly run through
a demo in the last five minutes so
briefly what we're going to cover today
in the next VAR have one hour or so is
basically a get a basic understanding of
what Apache Kafka is all about what's
the buzz about this messaging platform
and briefly talk about the high level
architecture and the components of the
messaging platform and what are the
different ways you could use Kafka for
development or protection use and also
we may also try to contrast Kafka today
with the most top contenders like
ActiveMQ rabbitmq and also as I
mentioned we're going to just briefly
run through
small demo if time permits at the end of
this session okay great so as you guys
might have just watching for the last
few years the biggest thing which is
getting adapted by majority of the smart
world of electronics is all of the
Internet of Things the big data stuff
the no sequel stuff lot of applications
and lot of things happening and where
everything depends on the data so today
we have the capability of capturing
every human interaction with computers
or every human interactions with
electronics or the change of the state
in the electronics all in the computers
so we have the capability today to
monitor and capture and aggregate all
the activities which is being changed in
the computer systems or in the
electronic systems so what we can do
potentially with these kind of
capabilities is basically say for
example a user interaction with the
internet applications like the users log
in times the log out the pages we just
registered the clicks the component
clicks the duration of stay on a
particular page where the user has
entered and where they where they are
exiting and various other touch points
just imagine if there are say 10 million
users who are visiting on a particular
internet applications just for a
hypothec encase right so you would have
say for example roughly say 100 250
pages so just imagine the kind of the
exponential data what you can gather on
the activity of a particular user and
the patterns of a particular user so the
data activity and the data growth is
enormous in a day's time imagine if you
have say multiple systems and multiple
components and all the users interacting
with all those things the enormous data
and the activity history what you could
capture is it's limitless so this is all
one of the scales of the data today how
it is growing into the world where you
could
get lots of lots of data activity from a
user to a system or a computer or an
electronic device apart from that as I
mentioned you could say for example
today we have the capability to go and
capture every phone record or every call
records of a mobile user from messaging
to voice calls so all those things are
possible so most of the mobile network
operators today they do capture all
these information of the user activity
on a phone or on a messaging platform
apart from that you could also have
noticed so when you have DTH connection
or a set of box connection even today
every event every channel you watch
every channel you change from which
channel to which channel it is being
jumped it is captured on the back end
that is what we call it as user activity
history so these are all the kinds of
things what people are doing as of today
and also you could also think of say
social networks where people are trying
to track what you're doing on a certain
place say for example LinkedIn who is
recommending who and who is getting
connected with whom all these kind of
activity history is being fed as a news
or a Fed as a feeds all these things are
happening in today's world apart from
that there are other things where
systems monitor the infrastructure
applications or these systems itself or
say for example simple monitoring health
checks
alerting all proactive monitoring and
alerting these are all the kinds of
things which is happening in today's
world where the data aggregation and the
data collection for any activity of a
system or with a human interaction with
the system or an electronic device is
being growing in today's world so being
said that what is the biggest challenge
when the data activity or the data
aggregation is being grown so one of the
biggest challenges with the increasing
data activity is
the data growth is how do we connect all
this increasing data which is being
populated by various systems and
connected to the right channels or
connected to the right place where the
data can be processed and the data can
be used for all the useful needs of the
business so in this critical work if we
don't leverage this kind of a data the
business at the cut throat competition
you will be always losing the business
in very few years so these are all some
of the challenges where you have to
increase your revenues you have to
increase your quality or to increase
your security optimizations and all your
product offerings so this kind of data
the secondary data has become the
primary data in most of the production
systems as of today so this kind of data
has been very critical for long long
time but today since we have the
capability the challenge has come where
we need effective messaging systems or
platform which can capture the big data
generating sources and try to analyze
and present all the rightful information
to the rightful sources at the right
time so that's the primary challenge and
that's what has been people trying to do
with various systems with lot of success
and challenges people managed it so far
but there was always a need which can do
say for example the real-time messaging
or where we can do multi point
connections using a platform which is
very efficient to scale to the big data
needs where the real-time data is being
backed real-time data or stream or
people do call it as stream data or
people do call it call it as activity
data so all these aspects has been
challenged and people used to manage it
in one way or the other way okay so what
I'm gonna do is maybe one or two
questions I would take it in between but
most of the questions I'm going to park
it at the end Kafka is not just for
instrumentation in capturing the data it
is a messaging platform there it
instrument
and collects data from various sources
and acts as a message broker to giving
it to various other consumers so we will
discuss about all these things once you
guys go through all these things it
would be pretty much easy for us to
understand okay okay so let's let's also
briefly talk about few real-time
analytics a for example so let's also
briefly talk about some of the real-time
problems where we had say for example
most of you might have noticed the ads
on Google ads where the moment you show
some interest on a particular product or
a service your ads follow you even after
you leave a particular asset so all
these things are captured at real time
so this is again a very good example of
activity being captured at real time and
process and pick the right ad and serve
it to you this is the mass systems where
people target on all these things and
also if you could notice there are
online books of subscription sites where
they try to restrict based on your
pattern of your consumption say for
example if a user or a bot is trying to
consume lot of data within a very short
span of time they usually go ahead and
restrict and say maybe you are not
trying to use this content or material
in the right way and I am restricting
you these are all the kind of activity
which is being really captured at real
time and they are trying to put
restrictions based on your usage or
based on your pattern and also you have
noticed where there are few such
relevant sites where they do go ahead
and make autocomplete recommendations
say for example on Amazon or Flipkart
the more you type something based on
your typing they go ahead and credit
some of these things if this is the kind
of thing you are trying to type this is
most likely that what you are looking
for maybe there are some sites even they
do provide
recommendations on the sites based on
what you are trying to search these are
all the real-time activities people try
to capture and they're trying to manage
all these things okay so as we just
discussed to manage and provide these
kind of features we said connecting all
this incoming information and trying to
give it to a different systems it is
kind of a huge challenge in older days
we used to have most of the time a
point-to-point system where there was a
system which used to generate some event
and ideally that goes and sits into
queue and there used to be a consumer
where it consumes and processes that
particular message or an item in today's
world that is no more a case where you
only have a point-to-point system where
there is only one producer or a one
source who's and writing a message and
only one consumer who is going to
consume it today the world is of all
Microsystems and heterogeneous
distributed systems where there will be
multiple systems which is generating
items or events or messages and at the
same time there will be heterogeneous
distributed systems just trying to
consume the events or messages from
various sources and trying to analyze
that and trying to make the business
decision or business information so the
world has changed drastically from a
point-to-point messaging the world has a
ward into a multi point-to-point where
the data has to be routed or managed to
through different systems because of all
these things a biggest need to have a
proper scalable or a highly scalable
distributed messaging system was the
requirement ok so if that is the problem
so it has to be highly scalable it has
to be properly distributed for big data
means if without distribution you cannot
have a big amount of data coming in and
can be managed within the messaging
platform and also like failures and
various other aspects so this was the
solution which people were looking
for having a very good platform
messaging platform where they could use
it for multiple points to multiple
points connection so the answer at least
for this session seems to be pretty
natural it's going to be the Apache
Kafka so with so many so much of data
coming in with all these kind of
challenges so we're going to make a
recommendation Apache Kafka fits the
bill in this kind of scenarios so let's
see what exactly Apache Kafka is all
about and what's the buzz going around
this okay so I'm just guys I'm just
reading some of the question just in
case if I have to take any one of them
which seems to be interesting so let me
just see
Shri Harry has a question can you please
provide a real-time example which you
use so that we'll get a clearer picture
okay I'm going to share one of my
examples at the end for sure which I am
using it in my current project so we
will definitely I'm going to definitely
share that at the end for couple of
minutes okay so Apache Kafka is an open
source I'm just giving you the
documented definition of what exactly it
is and then I'm going to describe what
it means okay so it's been a document
and it's being said that it is a message
broker and times they do also call it as
a publish subscriber system so don't get
confused with a publish subscriber
system which we traditionally used to
call it where there is a there is a
publisher and there is a subscriber who
is always trying to consume the messages
it is the publisher subscriber system in
a different design sense okay and also
it is called as a message broker
platform okay so you can use it
interchanging lis okay
so currently it is under Apache Software
Foundation and it's written in scaler so
those who are interested
initially it was developed and the few
developers in LinkedIn and then it got
moved on to a Apache kafka and the
people who developed at the LinkedIn
they moved to form a different theme
called confident so that's a history of
how Apache kafka evolved so as we
discuss the challenges and the problem
statement and we said we need something
like this so the people who develop
Apache Kafka at LinkedIn really face lot
of challenges with the existing systems
and that series and why they went and
developed this kind of system on their
own so when they started the development
the project intention was to have a
unified distributed messaging platform
which has a very high throughput for say
millions of or billions of messages per
day and also had a very should had a
very low latency for all publishing or
consumption and it should really really
handle the real-time activity data
stream from different systems so this
was the objective on which Apache Kafka
was build okay so basically they picked
up a design approach which was heavily
relied or heavily dependent on the
concept of transaction logs sometimes we
call it as transaction logs or commit
logs or right ahead logs so this was a
concept which is very very efficient
because of all the disk
effectively used techniques so the
commit head logs or the transaction logs
are primarily used in most of the
transaction or OLTP based system to
record all the changes or even switch is
happening in the LDP databases so this
was the technique which most of the
databases or no sequence use it to make
it their data consistent so the same
approach or the same
he has been adapted by the Apache Kafka
designers to make it consistent and make
it persistent in the data which the
messaging or which the messaging
platform supports so by doing that they
also focused on the distributed aspect
of Apache kaskell Kafka which is which
should be a which not just run in one
single machine but potentially it should
run on multiple machines with a cluster
centric design where if you run on
distributed machines then eventually the
partitioning challenges comes into the
play so the partition is if you have a
huge data set in your cluster you cannot
keep that entire data set cluster in one
machine or in a one node so you have to
partition that based on some strategy or
on some key strategy so then they have
to build in that partitioning or they
have to build in the distributed aspect
of the Kafka so they started thinking of
how do we have distribution so one of
the ways is having the partitions on the
data sets or on the messaging topic or
messaging queue so that's how they can
do the elastic aspect of the need so
that when you have partitions and when
you have distributed nature of your
cluster for messaging you can
elastically grow the system and it will
be also highly available with lot of
various aspects then also they have
thought about this highly throughput so
once they have lot of machines with lot
of data partitions so a data set can be
published or can be consumed from
different machines so that way you can
get a high throughput rather than
everything running in a one queue or
everything in running in a one node kind
of system so also a Kafka provides a
persistent messaging when I say
persistent messaging the data is
persisted into the transaction log kind
of a file system but most of the time it
is being served from
cash or the OS cash or did this cash why
kafka becomes a little bit more
efficient in the reads or writes on the
messages okay so also as I mentioned
Kafka is a system which has been billed
and used for real-time needs and it do
provide lot of things which is really
fast scalable and since the data is
persisted on the disk so it is durable
that even it can sustain the system or
few node crashes okay fine so let's be
briefly also talk about the little bit
design or the architectural aspects of
Kafka so Kafka primary has five
components in the cluster so the first
one is zookeeper zookeeper primarily is
used as a configuration and the registry
index kind of the service I will just
briefly talk about zookeeper as well and
the second one is the broker and the
third one is the topic the fourth one is
a producer and the consumer so a topic
is nothing but like many other
publish/subscribe messaging system it
maintains the messages or the bunch of
messages in its repository okay so topic
I think you should guys are all very
familiar about it so Kafka also uses the
concept of topics but the topics can be
partitioned and distributed in multiple
missions that's the difference and the
producers the producers are the ones
which processes and publishes the
incoming message or the activity data to
the broker or to the cluster and
consumers are the processes which
subscribe to the topics and pull all the
messages or the items or the data from
the topics
so I am using the word pull so in some
systems the in some publish/subscribe
systems the data are the messages or
items are pushed to the consumers in
Kafka rather it is being pulled from the
consumer so eventually the brokers is
the cluster or this servers which holds
the topic or which holds the messages in
the cluster so these are all the five
basic components which Kafka
architecture or the cluster has and what
happens is basically let's start from
top-down approach so the producers send
the messages over the network to the
Kafka clusters or the brokers which has
been installed or deployed and from the
brokers or the topics the consumers pull
all the messages from the topics of
their interest okay so all the
communication in the Kafka cluster
happens over the if it is a production
system it happens or the TCP protocol
which is the standard or the de-facto of
today world today's interaction or the
networking okay so guys I see a lot of
questions but due to a lot of time
constraints so we may have to take a few
of the questions at the end or what we
going to do is we may have to record all
the questions and we have to take it
offline to answer some of your questions
so at the end I will at least manage to
have two to three minutes to take very
important questions okay so please feel
free to stream all your questions and we
will have two paths that for awhile okay
great so we're going to talk about these
components in detail little bit so what
we're going to do is once we get into
the details will be much more clear and
much more confident about these things
okay fine so let's briefly talk about
the zookeeper component what we saw in
the architecture so how is the zookeeper
related to Kafka so zookeeper is a
independent Apache project which Kafka
recommends and incorporates for its
internal use so again as I mentioned it
is an open-source project which is
highly available system primarily used
for coordination and lookup service as a
registry index in a distributed system
say for example if there are so many
systems and components in in a platform
or in a architecture it becomes very
hard if you hard code say for example an
IP address or hostname for 3 to 4
services what happens if that hostname
or IP IP address board goes out of
service so there are various different
techniques say for example people used
to use virtual IPs or active standby or
hot beat techniques in older days - in
different ways to manage all these
active standby but in modern application
architecture people started using this
zookeeper has a configuration and lookup
service say for example if there are 10
to 15 servers and at any given point of
time you need to move to which server
you want to talk to for a particular
service - consumption so zookeeper can
keep track of all the nodes as said
nodes and it will try to monitor and say
if a particular node goes down it marks
it as down and it can give any one of
the nodes which is available from that
pool so these kind of things is what
zookeeper offers ok yeah it's kind of a
node manager where you can also store
configuration say for example instead of
keeping the configurations of a system
or a distributed environment in your
application or in your property file it
may be efficient if you store it in a
distributed platform where that can be
dynamically changed or where that can be
programmatically changed so zookeeper
offers you this kind of flex
where you can read the configurations or
the service look up from the distributed
system where the zookeeper offers all
those things so zookeeper is also like a
very quick index or a quick quick access
cache access where you can get all these
things ok great so little bit more about
zookeeper it also provides centralized
configuration it also provides your
locks say for example if you have two
active standby it elects one of the
nodes as active and it marks one of the
nodes as passive so all those things say
the naming service or the registering
lookup
so what zookeeper plays here in Kafka is
basically it serves as a coordination
between the nodes in the entire cluster
so even a publisher can interact with
zookeeper the brokers or the cluster
nodes can also interact with the
zookeeper and at times even the consumer
can interact with the zookeeper so there
are various means why the interact and
I'm not going to the details of it I'm
going to briefly just touch base and
skip on over to the next things so
primarily producers use zookeeper to
identify the lead broker so most of the
time the producers interact with the
zookeeper to identify if there are
one-to-many brokers in the cluster we
said broker is nothing but a node in the
cluster which owns the topics ok so if
you wanted to know which is the lead
node because Kafka publishes or the
producers try to interact with the lead
broker and remaining and all becomes the
follower followers or the follower nodes
so what happens is most of the producers
try to interact with the zookeeper to
identify the lead broker in the cluster
so that is one of the reasons why we
need the zookeeper and when it comes to
the brokers
what brokers does is when there is a
lead broker identified for it topic okay
and as I mentioned Apache Kafka also
provides high availability by
duplicating the data or replicating the
data into multiple nodes or multiple
brokers so in that case what happens is
there is a lead broker and there are
following brokers so the lead broker
takes the responsibility to own the
content and there are following broker
say for example if a lead brokers goes
out of service or it dies immediately
there are following brokers which
becomes the lead so there is no concept
of master slave or master master kind of
a thing they're all peers at any given
point of time zookeeper interacts and
zookeeper takes the responsibility of
electing a lead broker so that's a
reason why would you require a brokers
interacting with zookeeper and also the
brokers persist the message states
message state into the zookeeper so that
everyone is in sync with the message
state so that's another way of
interacting with the zookeeper from the
brokers and also the consumers retrieve
at particular topics state from
zookeeper to retrieve to which brokers
they can reach and consume the messages
so these are all the interaction touch
points in the Apache Kafka with
zookeeper okay fine so let's briefly
talk about what are all the different
ways you could potentially potentially
set up the Kafka cluster and how you can
leverage the Clough Kafka for various
various use cases so primarily there are
three modes which you can use it based
on your use case and your requirement
so the first one is the single node
single broker cluster the second one is
the insane node you could have multiple
brokers and the last one is multiple
nodes multiple brokers cluster so just
by looking at it you might just wonder
what are all the different use cases of
having this three modes so we're going
to just brief you talk about that as
well okay so in most of these aspects we
not going to touch base on how the zoo
zookeeper is eventually being set up the
zookeeper will be eventually treated as
a different entity or it's up to you how
you wanted to manage the zookeeper
because it's that itself is a different
component and that itself is a different
aspect all together okay
so what we're going to do is let's see
the first mode of doing things a single
node single broker cluster so in this
basically is primary most of the time it
would be useful if you if you are trying
to build an application or if you are
trying to learn Kafka so this is the
ideal mode where you can have a simple
virtual machine and in that single
virtual machine you could have all the
components running in the same virtual
machine so those who are familiar in
hadoo even Hadoop does paralysis a
similar model where it has a standalone
or a pseudo distributed mode or a
production mode which runs in a fully
distributed mode so Apache Kafka also
adapts the same technique or the
principle where all the components are
all the processes run in the single node
okay so it Apache Kafka also comes with
a very simple and clean configuration
management where it does support all the
configurations or all the
properties required for us to run the
entire cluster in a single node single
broker cluster architecture so primarily
if you are planning to run this in a
single machine all of these components
what is that you have to do is primarily
in Kafka you have options to start all
these components individually and run
all of them as a different process so
the first one to run is your zookeeper
and then you could start your server or
broker and then you have to create a
topic and then you have to start a
producer and start a consumer so these
are all the steps what you have to do if
you have to run your entire single load
single broker cluster okay so once you
finish all these things you are ready to
publish and subscribe messages between
your producer and your consumer to
whatever the topic you have created okay
fine so let's also briefly touch base on
the different mode the single node
multiple brokers and this let's briefly
touch base on how does this happen
and also that should clarify some of
your questions which I briefly had a
glimpse that whether a broker will hold
all the topics or how the topics are
partitioned or how the producers would
have called in the earlier model there
was no option that all the topics would
have been created on a single broker and
even though you couldn't had a option of
replication factor so by default you
wouldn't have any failover nothing so
it's a very simple model so you didn't
have to bother about lot of things but
in this model at least we have a option
to configure multiple brokers so when I
say multiple Brook
you could have more than one virtual
node in this same machine so here we can
have more than one brokers let's assume
if we have three brokers so the process
or the steps for us to run the cluster
in this mode more or less remains the
same instead of after you start the
zookeeper instead of starting just one
broker we have to start three brokers
with three different set of
configurations and with three different
set of port numbers on the same machine
say for example once you have started
the three different brokers remaining
all of them all the steps remains the
same
okay now let's briefly touch base on
what exactly is going to happen fine so
let's assume we have created a topic and
that topic potentially is partitioned on
to broker 1 and 2 and based on on the
partition the partitions most of the
time work on a key so based on the key
the producer either it can interact with
broker 1 as a lead for that particular
replica set or broker 2 as a lead for a
different partition or a replica set so
this is how a producer will or can
interact with different brokers based on
which broker it has to interact say for
example for a topic broker 1 could be
the lead and for a different topic
broker 2 could be the lead or for the
same or for the same topic for a
different partition broker 1 could be
the lead and broker 2 also could be the
lead this is one of the scenarios where
you could have all of them interacting
to all of them potential possible case
but not necessary that one producer has
to interact with all the brokers
it all depends on your topic your
partition and your replica sets so all
these things are retained in the
zookeeper the producer queries the
zookeeper based on the key and the topic
and tries to get to which broker it has
to interact for that particular the
topic okay so there are lot of questions
guys I am sure these are all questions
will which will take care for sure and
when it comes to consumer in this mode
the consumers the first point of
interaction is with zookeeper and say
for example a consumer is interested in
a topic if there if there are two topics
in the cluster the consumer first has to
know - which broker brokers it can go
ahead and read the content from so the
first point of contact is zookeeper
because the brokers has shared the topic
information and the topic state in the
zookeeper the consumer interacts with
the zookeeper and tries to get the state
of that particular topic and thereafter
it tries to reach to the brokers to
fetch the messages it pulls the messages
from the brokers or the cluster so the
another one good difference from
traditional publish/subscribe model is
in Kafka the consumers avoid
continuously polling or continuously
pulling the messages just to avoid the
overhead of whether there is messages or
not what it does is it goes and gets the
message is based on the topic offsets so
every time the consumer is responsible
to retain the offset from where hit it
has left reading and it passes the next
set of the offset from where it has left
and it goes and picks the information so
this is how the single node multiple
broker
cluster works in a very high level so
there are a lot of internal nitty
gritties or details which goes inside
since this is just the introductory
session we're gonna touch base very at
the high level okay and also the
producers can send the messages in
batches most of the systems or most of
the messaging platform lack this
capability where they cannot send the
messages or the data in batches Kafka is
really good in bundling the message
batches when there is real-time data
coming in which helps in lot of network
latencies or lot of round ups okay so
that's how the single node multiple
cluster happens okay great so let's
briefly also touch base on the multiple
node multiple brokers model so primarily
this kind of a setup is recommended for
production use and this is the way how
you could have any number of partitions
any number of topics for your
scalability needs and with high
availability and failover with
replication and with say replication
factor of two or three of the identified
is the basic requirement identified
basic recommendation is like say three
and even if there are two nodes which
goes down still your data is safe and it
provides the high availability with this
kind of a model where you could read the
same topic from multiple brokers by the
same consumer it increases the to put of
your in incoming items or your
processing capability okay and also
since it also processes the data in a
transaction log format your entire
messages or durable even Buehring crash
or it is persisted for
a certain point of time so that you
could use it within the window so your
messages are never lost
primarily there are few things which are
not sure whether I can jump into the
things you can run Kafka in a different
configurable modes
apart from these so there are modes like
whether you can get a message or it will
it will be persisted so there are few
techniques or few configurations where
you fine-tune the entire model of Kafka
itself can be fine-tuned or it can be
augmented in a very small way so in this
model most of the time it is recommended
that you have not many machines and run
all these processes or all these
primarily the brokers on different
machines or on different nodes so in
this model let me just briefly also
touch base the producer publishes the
message to a particular topic partition
and that particular topic partition can
be placed on any machine it could be on
load one or it could be unload two now
here in this model the topic which we
say for example if we have created a
test topic the test topic data would
have been distributed on road 1 and node
2 potentially node 1 there would be the
test topic messages on broker 1 as well
as on broker 2 because the broker 1
could be having a message say message ID
1 and a replica of that would also be in
stored on broker to possibly or it could
have been also stored by broker 1 in
node all those options are possible
because all these things are required to
support the high availability and as I
mentioned the consumer follows the same
technique of interacting with the
zookeeper and identifying to which
broker it has to interact to get the
data from the broker or the
clusters so primarily in this model
replications supports the failover and
all the lead broker management
everything is handled by the zookeeper
okay great so let's briefly talk about
one of the interesting use cases so most
of you would have noticed that we we get
LinkedIn activity news feeds where
LinkedIn shows as user information about
who has viewed their profile and who has
been searching for them we have seen all
these kind of things right so for us to
get this kind of information
LinkedIn uses internally apache cough
card to stream all the activity history
and provide all these insights in near
real-time so linkedin trains their
machine learning models against all the
activity and the user interactions with
the system and it tries to predict the
connections of say for example if you
are doing this kind of activity maybe
you should try to say look at this
people right
maybe these are all the job matches
which matches your activity maybe they
optimize and display similar profiles or
similar jobs or similar companies who
are working based on your activities so
these are all the things what linkedin
uses to make recommendations or the
content but eventually in the backend it
relies heavily on kafka to connect all
these systems for processing and analyze
and present this information ok and also
there is another prime very popular
feature on LinkedIn where Kafka is
heavily used is on the activity driven
newsfeed based on the relevant
the occurrences in the social network so
there is a newsfeed which keeps on
coming who who is doing what in your
circle what happened with whom and all
that stuff who got connected with whom
this is another feature which heavily
requires the data activity stream
processing and being able to present
this kind of information okay
so LinkedIn has roughly I have just read
and captured some data matrix or data
points from LinkedIn they have more than
say 200k 300k messages per second which
evolves from various different sources
and they have to connect it for
processing or for connected and store it
for later processing so they claim to
say that the axis of more than forty
forty to fifty billion messages are
delivered on a real-time basis to the
consumers based on various different
aspects and they also store this entire
activity history in Hadoop for later
analysis or later processing required
for various other aspects or various
other needs great so fine so apart from
using it for their functional needs
linden also uses Kafka primarily to do
lot of system monitoring application
monitoring and providing all the
application and system matrixes to their
businesses ok so I think we spoke a lot
about linden so I'm just going to
briefly touch base
so apart from lending there are a bunch
of people I think including my own
company who started adapting Kafka there
are a bunch of companies which you guys
can go through so there are a few people
like data surfed primarily uses to
collect all the social behavior or the
social active
uses and they do provide a platform or
API access for businesses to analyze and
consume that kind of data like similarly
for uber uses it for all the games
hosted on Facebook how how things are
going and all that those kind of stuff
I'm not going to touch a lot of details
about it but I think you guys can always
go and check the link where you could
find loads of loads of use cases and one
important thing I would like to touch
base is the lovely I think I've used
blog Li while back it's a beautiful
platform where it provides a neat
framework for all your log management
and reporting needs so even log Li uses
the kafka for back in log collection and
law movement to the processing center so
these are all some of these things great
so I think we have got a few minutes I'm
I may just stretch for a few more
minutes beyond our scheduled time so so
we had few questions like how about
ActiveMQ and what about RabbitMQ so i'm
not going to do a bashing session on
ActiveMQ or RabbitMQ rather than what
I'm planning to do is I'm going to
briefly share the differences which I'm
aware of so that we could just try to do
a contrast of it so as we mentioned
ActiveMQ is a really great feature rich
platform according to me and it has been
built on the NQ protocol and it is a
very very rich platform with lot of
features where CAF baccarat complete
with the features of what a active mq
offers at this point of time so
comparatively Kafka is very new it is
not even version 1 it if you try to
compare the documentation aspect from
ActiveMQ to say Kafka I would say
ActiveMQ would win hands down so there
is no compare
but when it comes to the scale and when
it comes to the throughput kafka really
are out beats say ActiveMQ in some
aspects and also I would also state
ActiveMQ is a real contender or
competitor to rabbit MQ rather than I
wouldn't put a head on with Kafka at
this point of time so some of the other
some of the other things what I could
think of active MQ is basically a
activemq uses a similar technique of
brokers but it uses as a mesh
architecture of brokers where you
connect to one of the members and
publish or consume that message so one
of the beauty beauty of Kafka is it is a
non-blocking publish/subscribe compared
to some other techniques which is like a
blocking publish/subscribe methodology
okay so one more difference I could
think of between ActiveMQ and Kafka is
basically in active mq basically you
don't know what is the node and for a
particular queue or a topic what is the
node you have to locate and how you have
to route the messages so everything is
taken care by the ActiveMQ at times this
becomes this looks very simple why do I
take the overhead of going and figuring
it out with zookeeper who is the leader
all I am bothered is I have a mesh I
will contact the mesh let it do the work
for me but if you take out certain
responsibility of doing it for yourself
there is lot of architectural and the
complexity of computing becomes simpler
on the cluster or on this server there
are all the few things which I could
think of between active mq and Kafka
so I'm not gonna run through about
RabbitMQ but rabbitmq I think that's the
one closes
which comes to Kafka again it is
feature-rich you cannot compare the
features of RabbitMQ or a ActiveMQ to
Kafka I would say rabbitmq is still a
very popular and preferred solution if
you have a message pattern which has to
be routed based on some conditions so in
those cases yes rabbitmq plays a very
good tool so I wouldn't the prefer Kafka
if you have to choose that based on some
conditional routing so these are all
some of those things things which I
could think of but I didn't want to bash
one thing or the other that series and
I've been trying to be objective and
trying to contrast things ok fine so
let's also try to quickly check linden
done very good benchmark testing on the
Kafka versus other platforms so
basically they did a producer test so
the screen shot what you're looking at
is basically the producer test so the
first test basically if you notice the
Kafka without batching you might see
that it almost just stays little ahead
of RabbitMQ and ActiveMQ in the
throughput when you do the batching when
you Club the batch the items or the
messages in a batch of 50 that's where
you could see exponential growth on the
throughput from the produces so the what
it means is basically it is recommended
if you do the batching in the messaging
from the producer you would have the
throughput which can be exponentially
increased but being said that we cannot
just ignore the fact that the other
systems does not provide the batching so
if you compare one item or one message
versus ActiveMQ Kafka rabbit and queue
you would see Kafka still stays ahead
but
not that significant but when it comes
to the batching of the messages you
would see the exponential growth which
is more than what you could think of
okay
so apart from that there are some disk
overheads Kafka seems to be a little bit
more efficient on the storage format on
an average it saves around say 140 bytes
so that seems to be very little but when
it comes to the volume of say millions
of millions of messages per day or per
hard that could be a significant
overhead or the significant cost factor
okay
so let's quickly jump on to the consumer
test similarly I am just briefly going
to talk about the consumer test so
consumer test I think you should
definitely see the Kafka just out stands
everything because it reads the topic
from various different brokers so that
is one of the reasons where you see the
throughput or the consumption aspect
compared to any other system
Kafka outperforms and there is no any
apples to apples comparison here so I
would just straight away give Kafka the
natural fit when it comes to the
consumption test okay great so now we
have reached to the end of the things as
I promised I'm going to briefly show you
the hands-on demo of just how to run the
I'm just gonna minimize everything just
to complete the exercise of what we have
so let's have I have already opened five
terminals and what I'm gonna do is as I
mentioned the first thing is I'm going
to start the zookeeper okay so for that
I have to change to and start this
zookeeper and similarly I'm going to do
the same to start a broker okay so I'm
going to start a broker so yeah I am
starting I have started a broker so what
I am going to do is once I started the
zookeeper and the broker I'm just gonna
go ahead and create a topic for today's
test okay so I have created the topic
called head yoga seminar topic and it
got me a message saying back the topic
has been created so once I have created
the topic so let me just go ahead and
check what are all the topics just I am
just trying to or I'll just ignore it
for a while okay okay
this is just - okay make sure we have
everything fine just to make sure the
zookeeper and the consumer and the
broker they are all synched up this will
make us sure that everything is working
fine so this at least now the broker the
zookeeper and topic has been created now
we can start the clients so there are
two clients one you we have to start a
producer to publish the messages and the
other one for consuming the messages
okay great so now we have the producer
ready so let's go ahead and start the
consumer as well okay fine so the simple
check what we're going to do is we're
gonna go ahead and type something and
say welcome to Apache Kafka world so you
might have noticed you would see the
consumer already consumed that message
so every line reads as one message and
say hello dude
let's type that so you would immediately
see that message has been consumed by
the consumer so this is the very high
level quick demo of things how things
work so that we could just get
first-hand experience in a very short
span of time okay so let's go back to
our desk and we do have a few reference
material which we have used in this
presentation so feel free to just go
ahead and access this content and do
check a couple of things I would like to
share before I get into a big couple of
questions so I'm gonna do that and also
I'm going to share my use case for what
I am using cough cough ah okay so please
make sure you provide us the feedback I
know it's been very cramped and it's
very quick in a short at very high level
quick-paced so that's where the sales
pitch comes in guys so we do have a
course which starts on the coming week
hosted by Eddie Erica so it's it's a
five-week course which we will get into
the details of it
and we will have much more time to
leverage all these things okay so now
let's jump on to the questions so what
I'm going to do is I'm going to pick the
two questions which I could potentially
answer and what I'm going to request the
aid worker support staff is to make sure
they persist all these questions and get
back to you offline okay great
there is one question a dealmaker say is
two weeks is it five or two can you
clarify it is actually five sessions two
weeks or two and a half weekends in fact
so both are right okay and where has a
question so what is a prerequisite to
take this course so there is there is a
slight requirement that you should know
basics of java and anyways we're going
to cover some of the basics of Hadoop
and other things
if the course required so so the
prerequisite is Java and we do have a
complimentary course on Java once you
sign up for that you would be
recommended to go through Java so that
when we do the Java programming it would
be much more easier for you to follow
just in case okay
okay so as I promised the last question
what I am going to take is my own
organization use case so this is a
startup started by me a year back and
what we are doing is basically we
monitor and capture the every product
the users or the consumers has and we
are trying to capture the user activity
or the behavior with that product so you
could imagine the amount of data what we
could get potentially from that it is a
slightly concept which relates to IOT of
things but in a different way so since
it has got lot of IP
information so I'm not going to share
the entire details till we launched it
but the idea is we're going to capture
every user activity with a product or
with a service and then we're going to
analyze and provide all the
recommendations or all the insights to
the consumers that what is the next
logical thing you could potentially do
with your product or service so that is
the entire thing what we are trying to
do for us to do that we have multiple
systems so we do have more than 17 to 18
micro systems within that 17 to 18 micro
systems I could just briefly show you
guys I don't mind so here is the we have
wrapped the entire kafka over a micro
service and we do use the kafka as a
micro service and publish all the events
coming from all the products or services
from the mobile handheld devices and we
are publishing it to various processors
one of the processes I could think of is
say for example I have immediately went
and purchased something so I immediately
notify a controller or a consumer that I
have purchased a product and you have to
archive the invoice invoice of what I
have purchased and you have to analyze
the details and send the reminder for
the next renewal to the consumer when
they did arrive so these are all lot of
scenarios or lot of things what we have
done on the app basically so I am sure I
think we also used same Apache kafka
some 8.2 version yeah so we have used
8.2 dot 1 which is just ready it is
recent one ok being said that I hope you
guys enjoyed it and please feel free to
give faith
recommendations or whatever you guys
think is good bad ugly so we are all
open to all your questions or
recommendations okay great and I have to
rush I've got another meeting and good
night and good morning guys and we'll
see you soon back bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>