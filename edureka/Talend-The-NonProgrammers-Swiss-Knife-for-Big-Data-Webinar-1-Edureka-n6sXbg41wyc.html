<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talend : The Non-Programmer's Swiss Knife for Big Data - Webinar - 1 | Edureka | Coder Coacher - Coaching Coders</title><meta content="Talend : The Non-Programmer's Swiss Knife for Big Data - Webinar - 1 | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talend : The Non-Programmer's Swiss Knife for Big Data - Webinar - 1 | Edureka</b></h2><h5 class="post__date">2015-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n6sXbg41wyc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we are hosting this webinar for talent
for big data which is called the swiss
knife for the non program let's get
started without wasting any more time
and i thank you each and every one of
you on behalf of erica for attending now
in princeton ready now we are going to
have the objective how we are going to
understand how it here is complementing
the hadoop which is big data system
Hadoop ecosystem will also learn how to
adopting two big data etl victor
industry is going to be benefited we
also understand why talent is used with
big data alright we will also do a
hands-on big data use case by the end of
this course not able to hear you so
let's get going like guys this is most
of the agenda that we are going to cover
today now what is what exactly is ETL
with big data or talent with big data
it's a graphical abstraction layer which
is sandwiched on top of the Hadoop
application yeah now the surprising
births going on up in the market these
days that introduction of huddle big
data is the doom for the ETL which is
not true this is really a myth all right
now the surprising stuff about this
currents your buzzers up this is really
not practical and these are all out
lenders comments made by some native
statements here now the typical
assertion as I said Hadoop eliminates
the need of ETL seriously that's not
really true now what no one seems to
question in response of these sort of
comments is the nape assumption these
statements are based on is it realistic
for a moment let's think is it realistic
for the organization to you know
transform their whole enterprise data
from from the
a legacy platform to the Hadoop system
there are a lot of challenges so it's
really not a the evolution of her etl
but we will see soon that how you kill
and big data they come all right in
between guide adjust for your
understanding how we have planned the
webinar we are going to finish up the
session in in half an hour's time then
we have reserved a question answer you
know question answer round for all the
participants for at least 4 15 minutes
to 30 minutes all right so by the end of
this webinar we are going to take up you
can write down your question on the
question board and by the end of this
webinar me a Nike or your webinar
presenter your tutor or the course is
going to take all the question one by
one you can trust me on that all right
now moving up you know moving forward
now this modern data integration tool in
platform they ensure the timely trusted
relevant and secure now modern
integration technology use optimizers to
process the information in both scale up
and scale down architecture now push
processing into database man system and
not just the data into the hadoop sorry
they also integrate from n to n now they
broker and publish the data layer that
abstracts processing such that multiple
application can consume and benefit from
the secure and curated data set etl no
doubt needs to continue of course to
evolve another to develop your
preference and performer but it also
takes care of the latins in need of the
modern application see honestly speaking
hundred will just another engine open
which PL and its associated technology
like data quality data profiling can run
well remaining what is commonly referred
to as ETL or was ignorant dismissing the
data challenge
an enterprise-wide data needs is just
irresponsible well this is one of my
favorites lag which explains lot of
myths in yes or no is writing etl
scripts in MapReduce code is still in
jail that's right yes it is still in
here now is equal running faster in few
cases M slower and others on Hadoop is
eliminated yes no absolutely not in
introduction of Hadoop changing when
where and how it happens yes now the
most important part with et al big data
is the question is not really what are
we eliminating that are we eliminating
et al or not but where does et al take
place and how are we changing the
definition and how does really
complement the big data alright they go
hand in hand all right now what is this
ATM all right now etl represent
stability to consistently and reliably
extract data with the high performance
and minimal impact to the flow system
represents the ability to transfer one
or more data set in the bag or real time
into the consumable format now obviously
the load stands for loading the data
into the persistent or the virtual stool
now this is more or less anyone who is
not from the background of VPN can
understand this is more or less what the
eto does moving forward the most
important slide in the webinar why do
you need to use why do we need to
combine the ETL with Hadoop and most
importantly what do we need to combine
talent with akil talent with Hadoop
sorry now now how learning this etl
along with the big data is in the major
business challenge all right major
business problem it is addressing now if
you see this entire product ruth offered
by talent we have big data we have data
integration we have data quality with a
master data management we have recently
introduced enterprise service bus we
a business process management anything
that you talk about data is part of Ln
products food and when you combine it
with Hadoop which we are going to sleep
that demo on the hill in minutes that
writing how Talon for big data is
abolishing or almost making the mattress
programming so much easy for any
developer whether you are starting fresh
whether your program manager whether you
are an EPL he or whether you're
technically whether you senior staff it
really doesn't matter learning talent
for big data is just so easy now why
this is this is one-stop solution now
talent is of course above seco system
which complement most of the environment
whether you talk about the Hadoop
ecosystem in terms of eight in terms of
five in terms of school in terms of
HBase H catalog Cassandra open the no
sequel database like MongoDB you name it
talent has a platform to establish the
connection and make the job up and
running in that environment all right as
you see most of the top class brand the
market leaders in the Big Data industry
whose mapper cloudera they all have done
the partnership with talent knowing the
capabilities that it has to offer
through its graphical user interface and
it gives you so much of time rather than
doing the MapReduce hand coding which
gives you obviously a upper edge now to
improve the efficiency of the Big Data
job design with graphic interface
abstract and generates the code run
transform inside hedule native support
for sdfs scoop HBase mahout big five
MapReduce as I said a partial a stool
embedded in hot rocks data platform now
they are obviously as I said certified
by the three market leaders cloud around
a power bottom box all right now we are
closing on to the action
white Alan this is what the talent
corporation has to say because the more
connected the data the word becomes the
more quickly a business must adapt very
underline statement yeah because the
more connected the world becomes the
more quickly a business was done imagine
if I'm only working on big data
environment as as a dictator user or I
am a MapReduce programmer or I just know
hi or I just know babe but I really do
not understand my business state up it
really doesn't make sense now why does
learning dallin helps here because
Stalin learning with the big data gives
you two operands one it makes you winner
of the leader of how to integrate your
data to understand starting from the
source where the data is coming from
plus it also gives you a control on the
big data area all right now that makes
you the winner of both the world
obviously talent is only the graphical
the only graphical user interface tool
which is capable enough to translate an
etn job to the MapReduce job that
Nietzschean job gets executed as a
MapReduce job on Hadoop and get the big
data running up and running in minutes
and make you a big data developer in
minutes this is a key innovation which
helps to reduce the entry barriers in
Big Data technology and allows retail
job developers to carry out the data
warehouse offloading to greater extent
now it's strong athlete eclipse-based
graphical workspace talent open studio
for big data enables the developer and
data scientist to leverage heading and
processing technology like sdfs HBase hi
Pig without having to write a dupe
application code a dupe application
seemly
integrate within minutes using the Talon
now before we go to the next slide let
me just have a quick look on the
question board all right hi I'm uni-t I
have different background deviously I
took two all right all right we will
what time your processing moves to the
data in Hendrick Boone goes to the
computation all right gasps and now Z
just within minutes we are going to
finish up the slide then it's all demo
and it's all questioner all right now as
you see on this one I'd why talent we
still we still justifying why talent
anyone who comes on you who's the
pressure who comes from a different
background because there are a lot of
news floating on the market should I go
to the MapReduce programming where
should I learn it through tool + speak
as talent for big data is awesome tool
when we are going to see the demo in
minutes you will be just mesmerized that
what it is capable of doing in minutes
now by simply selecting the graphical
components from the palette and
arranging them and if you know the right
set of configuration you can create a
Hadoop Java minutes and to load the data
into sdfs which is Hadoop distributed
file system it will use the Hadoop pick
to transform the data and the load data
into the Hadoop iris data warehouse it
can perform et al or ii ii leverage the
scoop scoop is nothing but it will take
out the data from traditional adobe on
the state of the system and pushes it
into built a tournament Wow talon open
studio talent open solution plus the
power of a dude makes talent open studio
for big data right for hadoop
application to be truly accessible to
your organization they need to be smooth
integrated into your overall data flow
now talent opens for big data
is the real tool for integrating the
head of publication into your broader
data architecture now one of the primary
reason why talent is so powerful it has
more built-in connected component than
any other data integration solution
available visited in the market with
their 800 plus connector that make it
easy to read from or write to any major
file for my database package enterprise
edition you name it most of the area
talent covers anything they don't cover
they keep on upgrading in their newer
versions so for example talent open
studio for big data you can use
drag-and-drop configurable components to
create data integration flow that move
the data from delimited log files into
Hadoop I've perform the operation I
extract the data from high in remind
sequel database that is n number of
possibilities guys see of course more
and more enterprise wanted to scale up
in Hadoop Big Data technology with the
use of existing pool of talent this is
where our IT industry and most of the
area they are struggling that going on
that you need to have a Java background
then only a Kalani and how about my
existing pool of resource how am I going
to train them on a Big Data project now
these are the kind of critical question
that the business need to address and
one of the solution is talent public
data now high-rise job trend in the data
scientists data analysis talent also
comes with the CGI transformation which
complements this area and reduces your
dependency on the simple excel in export
PA to Gartner is dallin is the best
technology in the market for data
integration in big data now these three
major players in the building industry
Hortonworks Cloudera mapa have already
tied up with talent for big data
solution and now mostly any level of
person in the industry can quickly get
started on this without much political
taste and I have a saying the bed I
don't know java programming how this
course helped me learn an excellent big
data
the biggest advantage that you get with
talent will be is there is no previous
ities trust me to learn this concept
whether you know whether you come with
prior knowledge of a Duke or not it
really doesn't matter you can be a big
data experts and up and running with the
data integration environment in minutes
guys this course has some or other thing
best things to offer each and every one
well that's it big data in 10 minutes
that for the big that's what the talent
claims go from zero to probe in big data
just under ten minutes you can be on
hand coding the talent for big data
sandbox is ready to run the virtual
environment which we will see in minutes
that includes the talent platform for
big data this is at the popular
distribution as you see on the crown cup
of the crown they have Cloudera button
works they have map are they also have
apache hadoop as i said anyone who can
use talent for big data starting from
the fresher the fresh graduates in IT
industry consulting starting from
professional starting from developers
but the managers anyone can get started
on this obviously i am having experience
on 10 years i am a manager I how do i
start with big data it really doesn't
allow me to flexibility of now start
lining the programming of the MapReduce
programming now so what should I do cohf
Italian for big data by the time we will
do a hands-on we would realize that this
is everyone's cup of tea well about to
see the hands-on demo we are about to
see the big picture now rather than
wasting any more time it is all quickly
see what talented do in minutes reducing
the man whores in doing the MapReduce
programming in Hadoop shall we I'm going
to show use case all right now for that
use case very important you know the
environment that i am using
i'm using talent open studio for big
data 5.5 i'm using a pre-configured
sandbox environment which is my hadoop
environment 1.3 in order to make it run
up and running i'm using oracle virtual
machine sandbox obviously my machine my
laptop is running on windows 7 64 bit
and i'm just using a normal standard
hardware configuration of 4gb ram and I
3 processor all right now with that
setup setup let's quickly go and see a
fresh demo I'll just explain you what is
it that we are going to do in the demo
yeah I am just logging into mice and bot
give me one sec that's it alrighty I
just show you the file or the data that
are using for this demo I will show you
this this particular use case is based
on an unstructured web log data that we
had extracted and on this particular
unstructured weblog data people are
talking a lot about Big Data say for
example this has been extracted from our
Facebook page of her company website who
does a lot of you know analysis no test
data alright righty that's it okay i'm
using this is my sample file that i'm
using well I let me quickly see the
questioner window
okay all right all right let's just go
this and then we can get up and running
in minutes guys how's the audio how are
the ppt because we are about to start
the hands-on can I get a yes from each
and every one all right Thank You San
yeah thank you now see everything that
you see on this demo it's all open
source it's all downloadable you just
need to know how to configure them
nothing you need to pay nothing there is
like sense all right everything has
downloaded from the word all right again
thank so each and every one can see my
screen there is a fine now that you
would be able to see and this is a
normal log file and these are just
simple text right and everyone is
talking about everyone is giving many
comments on the big data now one of the
primary requirement of one of the you
know weblog industry is they want to
study this log and they want to find out
what is it that people are talking about
the most the most hyped works the buzz
words out of this unstructured log data
using talent for big data without doing
any MapReduce programming we are just
going to do that in minutes all right if
we are going to do this in typical
MapReduce programming we may end up
writing some 100 200 300 lines of java
programming whereas using talent for big
data we will see that this is just a
cakewalk all right there are some
questions now hang on guys all right can
you kill ya so the word count dot txt
file is present in sdfs that's right Sam
I'll just show you you can I can view
this
if some of you would be aware about view
of the commands that we normally
exercise in Hadoop all right in order to
view a fine I use Hadoop FS minus cat
and I am going to view the same data
from my command prompt word cloud or 16
bingo does that answer your questions
and yes it is there in the head sdfs
yeah this is not ordinary fine this is
fine just there on HDFS all right now
quickly you're welcome now quickly what
I'm going to show here is all right now
what have I done here all right i am
doing I am connecting to the Hadoop and
violent okay can you see my full screen
now okay i am just connecting to my
Hadoop environment up from talent
connecting to the HDFS then what I am
doing is I am transferring this file
this file is actually there in my local
system I am transferring this file for
my local to the HDFS if I show you in my
local directory
see this file is their web extract all
right is the same file alrighty yeah
it's the same file so what I'm doing in
this is I am going to copy and rename
this file so that I can show you and
demo that how does talent moves file
between local to HDFS as well in minutes
i am going to rename this file with
extract one alright with extract one now
guys there is no such file called WebEx
chat one in my
will extract one all right now let me
see no there's no subscribe there's
neither of extract 1 so this file does
not exist as of now okay so what I'm
going to do using Calland i am going to
transfer this file from my local system
to my sdfs all right and then I am going
to read that file which i will just
transferred to my sgf a system all right
and I'll leave it as workload one okay i
will move with extract one toward club
one and then i'm going to read over
cloud one that's good that's it alright
and rest of the transformation process
what I'm going to do is I'm going to
arrange this unstructured log fight in a
particular manner then i am going to map
it using typical talent components and
these are very easy-to-use graphical
drag-and-drop components guy and i am
going to do is we are going to analyze
the web extract data and produce the
most frequently used or the spoken word
and that we are going to dump the data
back into what cloud one or cloud
underscore output 1 into our sdfs all
right and I'll just go and quickly run
my job please pay attention while the
job is running this job is actually
running on a sdfs platform as a
MapReduce program on the back end but i
am not writing any MapReduce programming
here I just did
drag and drop tick tick tick job done
happy happy it will take some time guys
there is many job running on my machine
many stuff I keep on doing here and
there all right by the time the job is
running guys surprisingly I did not
introduce me to everyone all right this
is nine and i have almost a decade
experience close to ten years of
experience in a teal big data business
intelligence industry evolved in many
geographic location including Europe
London and Middle East and we have
helped so many clients in developing and
building the piglet application the etl
application any application which
provides a business friendly solution so
that more or less for me and this
analysis job which I am showing in the
real-time example from from one of my
web customer one who are very interested
to design their work loud but they need
to know which are the key words which
actually frames there what cloud I
believe all of you guys you do
understand what is the word cloud do you
do you guys understand the word cloud
yeah I will show San the business logic
in minutes I will I just show the output
first knows if ya that is the login that
i use into the sandbox yeah that's
that's the root user to the sandbox so
it let me make my my environment up and
running now understand one thing talent
is a separate entity this big data area
is a separate entity so if I want to
take advantage of a droid drop
technology where I do not have up doing
any hand coding then of course I need to
establish a connection between this
interface to the Hadoop interface right
now my job just ran it sir it's alright
i also have made some provision in my
jobs so that before i go and check my
file i can literally see the output here
that is going to come wonderful so these
are my top rated word and these are the
most high in my log big data has been
used six times in this file in this web
extract is has been used six times
hadoop that the news four times data has
been used for x so has been used for
time ok i have not made any provision to
remove blanks that is the reason
something you can see after so we can
plug and play one component to ignore
all the blanks that's not a big deal and
now in is the most frequently used work
in our web extract now the similar thing
in E has already gone into what was the
file name we had given word cloud under
school
warum output I am going to show you this
from the comp prompt as less from de all
right I'm spilling the file word cloud
user to test return dot txt that's it
bingo I can see my pie from here I can
go to my web interface test data or
cloud output 1 there you go and you see
the time stamp is the right now I have
logged in from Indian time zone where we
are on 916 p.m. we just ran it three
four minutes back and the sandbox has
created this data wonderful all right
that's it so how much time it takes to
build this job hardly two or three
minutes now the logic which are used to
analyze this complex unstructured logged
a technical this is not coming in a row
and column format is ok so I'm going to
ask you something while but if we want
to calculate Mac richer from specific
row or column how we let them know what
to exactly all right now see see
everything is possible what I'm showing
you and talent is a very small demo
is capable of doing if you see it can
have such a control on such unstructured
log file can you imagine the kind of
control it will be having on a
structured file ok now the coming back
what logic and had used is I have just
cricket all my unstructured log
separated by space is I have using a
normalized component I did not normalize
everything I normalize line tied Rekha I
arranged each of the world one by one
one after other to make them each an
individual record that as the reason if
you see my screen my 11 rows which is
then their own isos data they have
becomes 10 because I split everything
because I wanted an individual control
on each of the word then I'm not
removing any to kick it because that is
not what I mean in this job rather than
what i have i have brought everything to
one platform I did up case so that if
I'm looking at Big Data small big head
over the large big data bout everything
is big data alright so i have used all
the focus while mapping in talent so
that everything is on single platform
then i have used aggregator
transformation what i have done is i did
a group by on each individual work but
by that time everything is enough case
so though if i show you my love file if
you see my log file all right you see
here there is something called big data
and these gaps here and there is
something called big data be small here
anyone can come on web and he can miss
Perren she can miss pair chicken they
can break in any format obviously a
control of the control of the audience
emotions of analytics is not in our pen
you can do be can use the two
two is best to analyze this kind of
unstructured data bring them massage the
data and then I use aggregate but my aim
was to see the top works the top
frequent words so I arranged them in
order where I wanted to see the count
the bigger count at the beginning and
the lesser count at N and I am just
using a display to make sure that what
is what am I doing here i am doing
perfectly fine there again I'm pushing
my data back to I do sdfs so I am
bringing and reading from ntfs I'm doing
the transformation and everything in HD
of this platform and then again i am
just recording or storing the data sdfs
so that these results can be later used
for business analytics or big data
analytics am I making sense here all
right where does the complete happens
the computation exactly yes and now
right now I am running on a standalone
installation of a single node cluster
alright so the instruction will go to
the jobtracker then the mother lode will
decide to which note the typical it is
running on a Hadoop platform but the
advantage that it gives you is any kind
of complex requirement you have you do
not need to write those how the lines of
not pudding you talked about I'll just
show you in minutes the capabilities of
talent for big data if you all see my
screen here you see can you all see that
is a family called big data here what
does it what does it doesn't support it
almost supports everything he see it
supports SD reference and I got a lot of
components within that to do my job it
also supports five
which we have in our in our regular
curriculum session we do a lot of
transformation on high withdraws from
the data we create the table dynamically
on the fly everything we do within hi
then you can you see the big we do have
big scripting job we do not need to ride
the big screen so indirectly talent for
big data is called the swiss knife of
the non programmer if you want to focus
on better technology and you are running
short of time Allen for big data is the
right tool for you if you think
MapReduce programming is too much for
you and for big data is the right tool
for you if you think you come from a
background where you do not want to put
all your previous experience ik
experience or technical experience into
vain and start from scratch talent for
big data is the right route for you can
you see MongoDB can you see X catalog
can you see H place can you see if we
could be Google bigquery can you see
Cassandra can you see comes TV you name
it we have a component here all right
can you see scoop anything everything is
there and their coat off some 800 or
connector you can write any kind of sore
system complex XML we have seen just
unstructured normal text log file we
still have a control on this kind of
fire we have all the kind of database
you name them starting from Oracle my
sleek well adapt my sequel server
everything we have over here and the
best practices we can parameterize
everything we do not need to hard code
anything there is the context then we do
have the concept of we do have the
concept of routines and code where we
can write the usable piece off for
transformation which can be used again
and again so primarily what you see on
your screen now is standing for big data
is open studio version this is the
palette window this is where we design a
job this is where we see the logs or
xserve one man army show
everything happens here all right now
coming back to our slide this is what we
had used we have performed the demo and
guys this is what our goal course
curriculum exciting post column is about
we are going to cover 10 modules within
30 hours of classroom session in module
1 will talk about role of open source
etl technology in big data into we going
to talk about how talent is
revolutionising the Big Data industry
with the reducing the effort of the
developer on big hive MapReduce
programming we will also all learn and
see how do we you know get used to with
most of the source and target system
that you not only need to process the
data in the data but you also need to
push the data big data right so you need
to be a champ on your current legacy
system as well only knowing big data
will not help you out because then you
are still dependent on your current
legacy platform users now on module four
five six we are going to go from the
basic transformation with the typical
advanced level of transformation with
covering lots of hands-on of complex
scenarios with module seven eight nine
ten exclusively focused only on big data
on module seven we'll talk about all the
concept that we require to understand
big data and specific specific elite
Allen for big data in introduce module
eight we talk about introduction to
talent for big data in module nine we
are going to talk about lot of use cases
and so on how does hi works on talent or
big data with simple connectors and til
10 we are going to see a little lot of
our demo sessions on big on talent and
he will have a four hours of n 2 n
project a typical industry data we will
use and i am going to give a business
requirement and we all going to work on
it
it and how does this course works we
have live online classes we have class
recording in lms if you miss out any
class don't worry the learning
management system has all recorded
videos 24 to 27 class support and Neil
being there is your puter and I and
always be there for any technical
support you require and we are having a
24 by 7 8 meter excellent team of
technical support which will always be
good to help you up and I love cute
quizzes due to the session and I'm going
to give all the kind of assignment which
will make you up and running and be a
master on data integration as well as in
big data and will make you are demanding
the source in the market alright which
organization will look after and then
post completion of this curriculum you
will receive a certificate from Erica
well that brings us to my favorite part
where i love to answer questions from
our lovely audience all right and thank
you guys for your patience and for being
with me to the entire webinar now please
go ahead and shoot me with your question
yofi it just for reports are part of
talent yes repeat just is quite capable
of running jasperreports because talent
is based on eclipse java platform based
application and i have worked on a
project where we configure the Jasper
report to the trigger along with Java
along with the talent sorry all right
some various questions which people have
asked all right I am still curious sanaz
constant curious to see where do we
write the business logic like I want to
write some custom logic like word starts
with or ends with right sure I'll show
you Sandra give me two minutes that goes
where does it go in darlin all right
I'll show you is there any certification
from respond that questions and I'm
coming by
to you now he is asked is there any
certification from cloudera regarding
talent nosy that is crowded is giving a
separate certification from Cloudera
sign on big data what talent is has to
offer after you completes this course
with us you should be having the
sufficient knowledge with you to finish
up a certification which talent is
offering which is called talent for
quick did that is independent of any
platform see clouded up Hortonworks
mapper they are all platforms it really
doesn't matter with whom you are doing
in as long as you are good in talent for
big data what we are using right now for
this demo is hot and works platform you
can use whether a platform not a big day
I hope that answers your question how is
the adaptability of talent in companies
and ask my friend just Google talent for
big data in us dies job or in India
hungry calm i believe i do not need to
answer this question you will get your
answer back can't hear you Mormon fees
why I would request our technical team
to look into that other ques tion is
asking any streaming and memory analytic
support yes yes Twitter streaming is
very much supported with the latest
version of talent yes hi Niekro kiosk is
just particular of the I answer that
question already this is Laxmi con
Mishra from Dehradun yeah yeah dodgy
tell me I am working as IT manager or is
it cool okay okay the question is
whether i can go for this course
definitely definitely mr. Mishra you can
definitely go for this course I have
already explained that during one of our
slide irrespective of what experience
what industry you belong this particular
course doesn't require any prerequisite
is I in like any Java or knowledge like
any big data knowledge the only thing
that is a very positive for this class
come with open mind and you need very
good in logical reasoning you should be
able to think that's it
all right super schnoz my question is
whether yeah you should go for this
course it's Talan mr. Mohammad Hafeez
he's asking someone has asked however
this logic sabraton and gone San this
Parker question let me answer all the
small questions where I can answer
without showing any demo then i will
quickly come to the job and will show
you where do we write the business one
writes an and on there ok mohammad
hafeez asked its is talent is similar to
SDC cloudy rain function but the
different are para no programming
knowledge needed and in SD seem a
programming knowledge is SDC is like
something like STP like hot and move
data platform like your Cloudera
platform if that is the case LG CU RM
took out there yes you're right in
tallinn you still do the same thing but
you really do not need to do any hand
coding you really do not need any
programming knowledge that's right now
see past so is there any limits is there
any demerits pom pom is that it remains
a constraint instead of using MapReduce
programming in Java no not at all if you
make things mata way but you still do
the same thing that is never a demerit
not see the Mohammad Hafeez ask sorry
mr. knife I still can't hear you not
sure what caused the problem can you
leave the gotomeeting and rejoin again
moment I have two crystal if most of the
people are able to hear me then I
believe the audio problem is on your
side okay now yeah guys keep on pouring
the question in the meantime I'll just
show sand there is a request from one of
our users where do we write the actual
logic all right now San this is for you
if you see my screen
if using a screen I write most of the
logic here there are M number functions
supported in talent if I want to create
a sequence and use a numeric sequence if
I want to do any kind of string handling
up cares down cares how to replace a
frame everything is predefined here you
just need to know the right set of
functions to be used all right if I want
to use a date I will go for talented ad
from the string and any kind of more
predominantly a Java function that is
being supported here all right so i
write my logic here like a function its
get it gets linked from the input and it
gets interpreted here and then the
written gets the return statement comes
here and then it gets propagate next all
right and if i want to write my pin
logic that I write in my tea aggregate
on what I want to do the group back I
was grouping by each and individual
world all right do I know do I need to
know sequin no known as he know that
also talent as on its own end you just
need to know the right set of components
to use drag and drop that same that
makes you a euro of all the world but
yeah obviously if you know little bit of
basic of sequin better but if you don't
know no harm coming back to San if you
see I was counting all my words using
your function gone comment and what I
was counting I was counting each
individual world and I was just doing it
using account which is the same concept
of sequel but doing it in drag drop
method I was storing it in an integer
because its account and I was just
popping it up in output as simple as
that
and the main key logic for lying was in
the key normalized this is a very
special component in talent when I was
flitting all my individual world in the
ad text or in the entire law with a
separator of space because there is a
space between each word so that is a
smarter way of processing the data all
right mama the peas you are cdh I didn't
get your question okay and understand
okay and so does it support all type of
file format yes it does yes it does
complex binary file I can show you one
unstruck l which we have just recently
process in our project and dumped it in
the flat file format if you see this is
a real time credit card login
information all right I have just a you
know what you got to mock up the data
but the format is more of a visa forced
machine you know as well as you swipe
your credit card the kind of data that
gets generated from the post machine
this is that log so this is do you do
anything that this is a row and column
typical rows and column format kind of
data this looks like there is no
structure way to this data in our course
curriculum you are going to be amazed
when we will cover these kind of
requirement under our hands on and use
case how do we process this kind of data
using talent and imagine if you are
getting these kind of millions of log
file in your big data environment what
is the complexity going to be involved
in aiding a MapReduce program and now no
I think unfortunately the way I'd rate
of lambda
I think Sandra right no I can't type in
any question guys I can't open anything
hang on let me write down something now
you'll be saying can you speak I don't
think he can speak hi guys can you see
the chat window you just said hi to all
yes can u see the chat window alright
make sense now Rosie sorry all right
then okay what else ok in terms of file
what sign was asking you talk about any
kind of complex file the kind of pilot
which I just show you I don't think more
complex than that kind of file you would
have imagined to process I guess so so
talk about any kind of files complex to
complex sort of looping XML Ellen is
very compositing in reading it that is
the beauty that is the key that lies
which makes it even stronger indicator
seeing big data and is no magic if you
learn MapReduce programming you more or
less get a control on the big data but
the level of control and flexibility
that you get with talent for big data is
just awesome there is no limit with the
power of the full data integration along
with big data that is what makes it you
know that cutting edge technology all
right what else what are the questions
you have been
mr. Mishra roughy well San Rafi i am
from the least here friday and saturday
are weekends i am not able to attend
morning classes let me know when new bat
will start at evening time all right
Rafi i will let this no to my technical
team i am pretty sure we are going to
the kind of them that we have got for
our us bags we are not able to
accommodate most of the people over
there and i'm really can't with the last
webinar that we have conducted into
Davina that we are convicted we have
awesome response and now i'm pretty sure
we are going to start this batch very
soon I understand on Friday Saturday
being weekend on the Middle East I'm
pretty sure i do Rica is going to start
one match very soon now knows you've
asked my question is if talent makes our
work that is here why people go and do
complex our programming and I mean to
say that what's the supremacy of java
coding them all right let me answer it
little diplomatically okay what talent
does on the graphical user interface on
the back end it is actually MapReduce
java programming at the end of day
because that is what her daughter
understands but in case of Alan Alan
does the tough part or the top job for
you where is in plain vanilla or the
home of MapReduce programming it is you
who has to do the n do anything now I
would not say why people go and do
complex it is it is the way industry is
emerging now with a Stalin for big data
in order to you know come and adopt for
Alan for big data people need to be
aware of the etl etle stay a clean
technology all right and now etn the
moment
starts yelling well with big dinner
people have already started adopting it
and it is really going guns guy but
extra vendors like hot rocks Cloudera
mappa they are providing the mapreduce
platform and now you imagined one thing
is talent would not be demanding and if
Ellen would not have those many
capabilities then these kind of
operators in big data they would have
never done the tie-up with talent that
answers this very question yeah what
else ok those you said I don't have any
experience in atl really doesn't matter
the way the course curriculum has been
designed not seen the way the course
curriculum has been designed you really
do not need to have any prior knowledge
of any prerequisite is because the first
half is going to concentrate on Ellen
for data integration even if you will or
you will not succeed or get a job or
talent for big data definitely I can I
can tell it with a lot of optimism that
alpha data integration is such a hopeful
area or such a strong area you will
definitely excel in that and Ellen for
big data is just a hybrid bridge between
talent for data integration and big data
it really does not matter if you do not
have any private experience it's such an
easy tool to learn you as I said you
just bring on board with logical
reasoning and keep your common sense
open the moment you start working on
talent you will understand that it is
very easy to use graphical user
interface doing and interfacing with any
third party vendor starting from
Cassandra hi a sdfs is just a cake
all right any more questions guys with
the level of questions sbach I am pretty
sure we are going to have an awesome
match sure nothing's sure as of now we
are starting up this exciting batch from
28 28 of februari so I pardon me 28 4
mar 2015 this Saturday Sunday upcoming
saturday sunday i believe we are having
a morning batch if you go to the website
talent website I think you can see our
you can see our exciting course and you
can also see the blog that I have
written the blog that i have written on
this particular concept of introduction
of etl in big data and how it is make
how does making the you know life of the
any developer or any professional who
want to make a make-or-break carrier
talent for big data all right you can
see my screen this is the course that we
have lost this weekend and we are having
lots and lots of assignment and projects
if you go exploring licking you see we
are starting with 28 of March it's a 30
year old classroom session starting at
seven to ten in the morning all right
and if you go you can see the Glock that
I had written you can see a me over
there alright see the webinar also got
laden also gonna be loaded so the
webinar that we had posted on we had
hosted one on 17 and we had boasted
we are hosting one to take so both are
we got so much of immense response so
you can go and check out the video and
you can also rate the blog that I had
written I will just show you the blog
which is there go and read it I think
many concepts you know which we may or
we may not be able to answer during this
yeah that's the that's the blog that I
was talking about alright and this
answers most of the myths in the market
what you should do should not do and how
do you excel in big data carrier but
with the shortage crash course is
possible alright and that's me all right
okay I just I'm just send ur blog to the
entire audience okay all rights yes what
are the questions we have if you don't
have any more questions guys then I
think you'd wrap up the session now just
quickly throw any questions you have or
we will wrap up our exciting webinar in
another two or three minutes right so
all good
after nine forty-seven I don't see any
questions guys alrighty so let me take
the opportunity to thank each and every
one of you who have attended the webinar
it was really great and I am very
optimistic to see many of you turning up
the session which we are starting on 20
a of mar 2015 thank you very much more
month and now zip up the team we come
back to you with the with the second
batch timings okay sand is asking the
skew is to know how the other retail
dudes are doing in this area I believe
Eiland is just going guns because they
have started early and I mean pentaho
does have some functionality and most of
the other detail tool they have also
started working upon the great idea but
I can strongly recommend talent for big
data one of the primary reason is i have
personally use it and i personally also
use other ETL tool I know Ellen is very
flexible on many aspect and second thing
the way Panik of big data is dying up
tying up with the big industry
definitely people have already realized
its potential yes Ravi you are going to
receive a detailed step-by-step
instruction sheet from edeka much before
the class start and you will be having a
big data cluster up and running on your
personal laptop before we start the
class so do not worry so before Hadoop
came into picture when we do ET le the
data goes to computation online how now
we're computation goes
data exactly exactly now etl if they're
since ages sun and it will leave the
primary you know concept of the tool
which was integrating the most
enterprise heterogeneous data bringing
them into common platform pushing them
into the reporting system pushing them
into the processing system so if you go
to any enterprise data architecture you
would realize that the backend system is
completely supported and the data is
completely moved by etl as of now so
imagine if people are migrating their
platform to big data in terms of storage
and if etl still offers that much of
flexibility and capability both in
traditional legacy system as well as in
big data then why do I need to do hand
coding I will definitely use all the
features of again so now talent change
the way it works where competition moves
to SDF is exactly that's right talent is
not exploiting a Hadoop San tannin is
just utilizing the core of module to the
best of talent so that both the things
can complement each other that is the
reason my blog says etl and big data on
the sami big net i'll prob is a very
good concept but still it needs those
easy to use user friendly area where
people can really use it you know people
people at different level they can
really use it to make the best offer
okay if you don't have any more
questions guys can I get thumbs up from
each and every one of you to wrap up the
session if you don't have any questions
yeah I got a yes from San how about Rafi
shrah yes thank you
I'm pretty sure mr. Mishra Mohammad
Hafeez a ruffie and sad I hope to see
all of you on our class very soon all
right so thank you very much guys have a
great week ahead this is mike your tutor
your friends anymore bye-bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>