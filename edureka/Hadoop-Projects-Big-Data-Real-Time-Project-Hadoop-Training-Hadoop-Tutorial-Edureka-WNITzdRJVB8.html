<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Projects | Big Data Real Time Project | Hadoop Training | Hadoop Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Hadoop Projects | Big Data Real Time Project | Hadoop Training | Hadoop Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Projects | Big Data Real Time Project | Hadoop Training | Hadoop Tutorial | Edureka</b></h2><h5 class="post__date">2018-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WNITzdRJVB8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is freshman from
Eddie Rica and today's video is on
Hadoop project tutorial so in this video
I will be demonstrating you a very
interesting project so that you can
learn more about Hadoop and also
practice that alone I must also tell you
that this project is also a part of the
Big Data Hadoop training at Edie Rica so
let's get started so this project is
based on the e-commerce domain so let me
give you an introduction to this project
in context of one of the biggest names
of e-commerce platforms none other than
Amazon so if you have ever shopped from
Amazon before which I presume you must
have you must have seen something like
this when you click on a product so
you'll view the details of the product
something like this and as you know most
of the e-commerce organizations do not
have any inventory so they tie up with
different merchants similar is the case
with Amazon Amazon provides the
merchants or the sellers a platform to
get connected to the buyers so when you
click on the details of a product you
can see that Amazon gives you something
like this and you also find something
like this there are 28 offers from this
price and if you click on it you can see
the name of the different sellers who
are selling the same product at
different prices and the prices they're
offering are listed like this but you
can see over here that by default Amazon
has selected the aperio retail Private
Limited for this particular product so
how does Amazon do that so it is
actually based on a merchant rating
system and as a platform as an
e-commerce platform you have to ensure
that you always display the product from
the best merchants in order to ensure
quality because you don't want angry
customers right so it is very important
that your customers are satisfied with
their product so you have to choose from
different merchants
for the same product in order to decide
which Merchants product needs to get
displayed by default and hence Amazon
has a merchant rating system in order to
decide that and this is what exactly
we're going to build all right so we're
going to make a merchant rating system
similar to this so here is the problem
statement so there are multiple
merchants selling the same type of
products as you can see that merchant 1
and merchant 6 are selling the same
shirt merchant 3 and 4 are selling the
same shoes and similarly 5 and 1 are
selling the same fans and there are
multiple other merchants who are selling
the same kind of products and you have
to build a merchant rating system or the
company wants to build a merchant rating
system in order to determine which merge
them sells the best product so that
their product would be displayed by
default and as a big data expert let's
just assume that you were hired by the
company has a big data expert you are
assigned this task so this is now your
problem to solve so the first thing your
organization will give you before you
start to do your work
is the data set so this is the data set
that you're going to get so this is the
transaction data set then has certain
fields like transaction ID customer ID
Merchant ID timestamp when the purchase
was made the invoice number the amount
and the segment of product that was
bought you have another data set which
is the merchant data so those are the
details about the different sellers or
the merchants so you have got your
merchant ID the tax registration number
the merchant name their mobile number
start date email address state country
pin code description longitude latitude
the location basically so these are all
the details about the merchant that you
have in your data set so let me just
show you the data set so this is the
data set that is in your HTF fest right
now so here is the transaction data
so here is the transaction data which is
a 2gb of file and the merchant data set
which is 20 MB because as you know there
are many transactions but a limited
number of sellers or merchants so that
is why the size of the data set the
merchant data set is quite smaller as
compared to the transaction one and to
tell you we haven't actually used the
entire data present in the data set we
have just selected a subset or a sub
data set you can say because the
original data set was quite huge and
this was a demo project so just for you
understanding we have chosen a sub data
set we just took 2 GBS of data out of it
alright and this is how it exactly looks
like so this is the CSV file of the data
set that we have this is the
transactions data alright so this is the
approach to solve so the first thing
we'll do is that will segregate the
merchants based on the price of their
products and their sales so we will be
segregating them into four categories so
the categories are the merchants who are
selling products that are below 5,000 or
less than $5,000 there is one more
category for merchants who sells
products between 5,000 to 10,000 dollars
another category of merchants who sells
products between 10,000 to 20,000 and
another category greater than 20,000 or
more than 20,000 will be using a simple
logic to approach solving this problem
so let's say that if there is a merchant
who's selling their products at quite a
low price and you see if he's not making
a good number of sales it means that he
is not selling quality products because
if a merchant who's selling the product
at quite a less price and people aren't
still buying from him it clearly
indicates that his products are not up
to the mark so it's a low rating for a
merchant similarly on the other hand if
you see a merchant who is selling their
product at quite a high price and yet he
has a very good number of sales so it
clearly indicates that
his products must be very good because
despite of the higher price people are
still buying from him
so in that case obviously the rating of
the merchant would be also very good
right so this is a simple logic that
we're going to use in order to rate our
merchants or sellers and you have three
options to choose from so you have got
Apache high which is a great analytical
tool who you have got hadoop mapreduce
and apache pig and today we will be
choosing hadoop mapreduce so MapReduce
is the core component of hadoop that
process huge amount of data in parallel
by dividing the work into a set of
independent tasks MapReduce is the data
processing layer of hadoop it is a
software framework for easily writing
applications that processes the vast
amount of structured and unstructured
data that is stored in your HDFS HDFS is
Hadoop distributed file system in Hadoop
MapReduce works by breaking the data
processing into two phases math phase
and the reduce phase and that is how
exactly it gets his name Map Reduce so
the map is the first phase of processing
where we specify all the complex logic
business rules reduces the second phase
of processing where we specify
lightweight processing like aggregation
or summing up the outputs but the
question is why choose MapReduce well
I'll give you two reasons for it first
is the custom input format now input
format is something that defines how
your input files are going to be split
and read same MapReduce you can create
your own custom input format instead of
using the default input format this
actually makes handling of your data
quite easier because here we can create
our custom input format for transactions
and pass it as an argument then we have
the distributed cashy
so distributed cashy is nothing but it
is a facility that is provided by
MapReduce framework to kashchei files
like your text files archives jars etc
that is needed by your application let's
understand this with an analogy just
think about it that there are three
students sitting on a table solving
chemistry problems and they have one
periodic table so you keep the periodic
table in the middle of the table so the
students all the three students can read
from the periodic table to find to see
the atomic numbers of different elements
and solve their problems right so one
periodic table every one can refer to it
and solve their own problems so this is
what distributed Kasia is so with
distributed cachet you can put the data
that will be used by your different data
nodes to refer in order to run MapReduce
jobs so we'll be learning more about how
to create your custom input format on
how to use the distributor caching in
the demo part alright so before that let
us just understand how MapReduce exactly
works so this is a sample MapReduce job
execution with an example so this is
your input file this is a text input
file so it contains some words so first
what will happen is that the input will
get divided into three splits alright so
I'm taking one sentence at a time so I
have got three splits over here and this
will distribute the work among all the
different map nodes then we will
tokenize the words in each of the mapper
and give value one to each of the tokens
or words
Sudheer one bear one River one similarly
here car one car one river one and now a
list of key value pair will be created
where the key is nothing but the
individual word and the value is one so
this is the key and this is the value
and this will happen on all the three
nodes so the mapping process remains
same on all the nodes so after the
mapper phase a partition process takes
place where the sorting and shuffling
happens here all the tuples with the
same
ye are sent to the corresponding reducer
so all the bear are together cars are
together deer and river are together so
after the sorting and shuffling phase
each reducer will have a unique key and
a list of corresponding values to that
key for example there 1 1 car 1 1 and 1
and so on
now comes the reducing phase now each
reducer counts the values which are
present in the list of values so reducer
gets a list of values which is 1 1 for
the key bear and then it counts the
number of ones in the list and gives the
final output as bear 2 similarly for car
it's 3 deer to river and count to 1 so 2
and finally all the output the key value
pairs are then collected and written in
the output files so this is your output
file so it has just combined the result
from different reducers and here is your
final output so if understood MapReduce
with the classic example of the word
count program now this is the generic
execution flow of the MapReduce job so
you have your input file over here so
the data for MapReduce task is stored in
input files and input files typically
lives in the HDFS the format of this
files is arbitrary while line-based log
files and binary format can also be used
and you have an input format now input
format defines how this input files are
split and read it selects the files or
other objects that are used for input
and input format creates the input split
so it logically represents the data
which will be processed by an individual
mapper one map task is created for each
split and thus the number of map tasks
will be equal to the number of input
splits the splits are then divided into
records and each record will be
processed by the mapper now let's talk
about the mapper some APRA processes
each input record from the record reader
and generates a key value pair
and this key value pair is generated by
the mapper is completely different from
the input pair the output of the mapper
is also known as the intermediate output
which is written to the local disk the
output of the mapper is not stored on
HDFS as this is temporary data and
writing on HDFS will create unnecessary
copies so then the mappers output is
passed on to the combiner for further
process the combiner is also known as
the mini reducer so Hadoop MapReduce
combiner performs local aggregation on
mappers output which helps to minimize
the data transfer between mapper and the
reducer once the combiner functionality
is executed the output is then passed to
the partitioner for further work now
partitioner comes on the picture if
you're working on more than one reducer
and here we have two reducers in the
example so if you have one reducer you
don't actually need a partitioner so the
petitioner takes the output from the
combiners and performs partitioning
partitioning of output takes place on
the basis of the key and then sorted so
by hash function a key is used to derive
the partition according to the key value
in Map Reduce each combined your output
is partitioned and a record having the
same key value goes to the same
partition and that each partition is
sent to a reducer so by using
partitioner it allows to have an even
distribution of the map output over the
reducer so after that comes the
shuffling and salting part so now the
output is shuffled to the reduced node
the shuffling is the physical movement
of the data which is done over the
network
once all the mappers are finished and
their output is shuffles on the reducer
nodes then this intermediate output is
merged and sorted which is then provided
as an input to the reduce phase now
comes the reducers it takes the set of
intermediate key value pairs produced by
all the mappers as the input and then
runs a reducer function on each of them
to generate the output
the output of the reducer is the final
output which is stored in HDFS so if you
have multiple reducers the result from
different reducers will combine and that
is going to be your final output which
will be written into the HDFS so this
was the MapReduce job execution flow so
we'll also be using the distributed
cache so you'll have different data
nodes each data node will have their
local copy of their data and if each of
the data nodes needs to refer something
we will keep that in the distributed
cache in in this case we'll be keeping
the merchants file
alright so distributed kasia is nothing
but think of it as a shared drive right
so if you have multiple users who wants
to have access to one data set so you
can just put it up in the shared drive
and all of your users can share the data
I use the same data right so this is
what distributed cache a is so
applications specify the files via URLs
to cache a via the job comp so I'll be
telling about the job con later in the
demo section and the distributer kasia
assumes that the file specified via urls
are already present on the filesystem
add the path specified by the URL and
are accessible by every machine in the
cluster so the framework will copy
necessary files to the slave node before
any jobs are executed on that node and
distributed cache a tracks modification
timestamps of the cache files so clearly
the cache files should not be modified
by the applications or externally while
the job is executing so how will it
works in our case we will be storing the
data into HDFS and will be executing
MapReduce program over that file so
we'll store the merchant data in the
distributed cache a then we'll segregate
the transaction data into categories
such as less than five thousand five
thousand to ten thousand dollars ten
thousand twenty thousand and greater
than twenty thousand dollars with the
merchant ID then we'll use the merchant
file from the distributed cache a and
map the merchant ID with the merchant
and at last we'll received the output as
the merchants name with date indicating
the number of sales and different
categories now let us talk about the
Code sections so the execution will
start from the main method where we'll
use the tool runner so the tool runner
can be used to run classes implementing
tool interface so it paused the January
Cadoo command-line arguments and
modifies the configuration of the tool
then it will point to the run method
which will point to the run mr jobs so
here we are specifying the driver code
so I'll be telling you about the driver
code later on so next the execution will
move to the mapper class which is the
transaction mapper the framework first
calls the setup method followed by the
map method for each key value pair in
the input split so in setup method we
are loading the cache a file and calling
a method where we'll be resolving the
merchant name from the Merchant ID next
in the map method we are creating the
object of transaction which we will be
using to catch all the fields of
transaction first and then using the
object of aggregate data we will create
this segment of transactions as we
discussed before in the fourth segment
less than five thousand ten thousand
twenty thousand those segments at last
using the merchant ID name map we will
resolve or find out the merchant name
the output of the map method would be
the key which will be the combination of
merchant name and date of sale while the
value would be in the form of number of
sales of different categories next the
execution will go to the partitioner
code where we'll have the get partition
method which will send the records with
the same key to the same reducer and it
allows the reducer code will execute
which will aggregate the data with the
same can provide the output so I'll be
showing you and explaining you all the
codes involved over here in the demo
part alright now let us move ahead
so first let me take you through this
transactional class where we are
defining all the variables corresponding
to the transaction file
as you can see we have God the
transaction ID customer ID merchant I
the timestamp invoice number invoice
amount and segment here so these are the
fields in the transactions and we have
created the variable for the same so
next we are creating the getter and
setter methods for each of the variables
so as to read the value of the field and
we write the value of the field so as
you can see here we are defining the
method get so get segment we have got
get segment here where we are returning
the value of the field and the set
method here the set segment where we are
writing the value of this field so
similarly we are doing it for all the
variables as you can see here so we have
the get and set for customer ID so we
have the get customer ID method which
returns the value of the field and we
have got the set customer ID which
writes the value of the field so this is
similar for all the fields in our
transaction data so similarly you can
have the aggregate data class which we
have used to create the categories of
the product so here we have fields like
order below 5,000 order below 10,000
order below 20,000 order above 20,000
which are nothing but the different
categories which we have defined earlier
and similar to the transaction class
here we are defining the getter and
setter methods so we have got get total
order method which returns the value of
total order and we have set totally
order method which is writing the value
of this field total order so we have the
same for all the different variables
that we have defined in the aggregate
data class the getter and setter methods
then we've got the aggregate writable
class so first here we are creating an
object of JSON class so jisan is
basically used to convert Java objects
to JSON format next we are initializing
the aggregate data object now we have
two constructors first one is the basic
constructor which is not taking any
argument the second constructor is
taking aggregate data format as an
argument and trying to initialize the
we get data object next we have the
getter method for the aggregate data
which will return the aggregate data
object after that we have the write
method which will write the value of
corresponding fields using the getter
method of the field like for order get
order below 5000 get order below 10,000
order below 20,000 get order above
20,000 then you have read fields method
which will be calling the setter method
of each field to assign the values to
the corresponding field and at last we
are overriding two string method which
will convert the aggregate data object
to JSON and then return the JSON so I
hope you guys are clear with the custom
input format so now let us take a look
at the main Java file which is the
merchant analytics Java Java so the main
class is the merchant analytics job
class inside which all the jobs will
reside so the execution will start from
the main method so first let us go to
the main method
so here we are using tool runner so tool
runner can be used to run classes
implementing the tool interface it
passes the generic aduke command-line
arguments and modifies the configuration
of the tool so tool Runner dot run
method runs the given two laughter
parsing the given generic arguments it
uses the given configuration or builds
one if not it sets the tools
configuration with the possibly modified
version of the conf here we are passing
the configuration object merchants
Analytics job object which is the main
class and arguments which we will be
providing while executing the job so in
our case there are three arguments first
is the path of the transaction files
second is the path of the merchants file
and third is the output directory now we
will execute the run method where we are
returning the values of the run em our
jobs method
we're also parsing the arguments that is
all the three parts that is the
transaction merchant and output
directory to the run em our jobs method
now let us see the run em our jobs
method so here we have the driver code
so first we initialize the configuration
object and then we will initialize the
controlled Java object so control job
class encapsulate Sam MapReduce job and
its dependency it monitors the stain of
the defending jobs and updates the state
of this job and now we are creating the
object of job class and we will define
the properties of the job so first we
have the set output key class property
where we are defining the output format
class of the key which is text class
similarly we are defining the set output
value class for output format class of
the value that is the aggregate right of
the class next we have set jar by class
which tells the class where all the
mapper and reducer code resides which
the Merchant Analytics job in our case
now we are specifying the reducer class
which is merchant order reducer
next we are providing the input
directory so the set input the recursive
method will read all the files from the
directories recursively if we are
providing the directory path so first
we're adding the input part of the
transaction file which is present in the
ultimen 0 then here we are also
specifying the input format of the file
and the mapper class that is the
transaction mapper next we are talking
about all the merchant file from the
directory provided in argument 1 and
adding this file to the distributed
cache a using the job dot add cachet
archive method moving ahead we are
setting the output directory path which
is provided in argument 2 and we are
also adding the timestamp as the
subdirectory and at last we are setting
the partitioner class that is the
Marchant partitioner and then we are
returning 0 or 1 depending on whether
the job has been executed successfully
or not and next we will take a look at
the transaction mapper class which
implements the mapper interface so it
Maps input key value pairs to a set of
intermediate key value pairs so maps are
the individual tasks which transform
input records into an intermediate
record the transformed intermediate
records need not to be of the same type
as the input records the hadoop
mapreduce framework spawns one map task
for each input split generated by the
input format for the job and mapper
implementations can access the job con
for the job via the job configurable and
initialize themselves the framework
first calls the set of method followed
by map method for each key value pair in
the input split so in the static method
we are loading the merchant file from
the cache a using the get cache a
archives method then from each file we
are calling the load merchant ID name in
caching so we're calling this method and
we are passing the path of the cache of
files and the configuration objects now
in this load merchant ID name in cache a
method we are initializing the object of
filesystem using the con and next we are
opening the file and then we are reading
the data
line by line from the file now here
first we're removing the codes from the
line and then we're splitting the line
using the comma and at last we're
putting the Merchant ID and merchant
name in the Merchant ID name map
variable so here you can see in the
merchants file that we have merchant ID
at index zero and merchant name at index
2 so this merchant ID name map will help
us in resolving the merchant name from
merchant ID and next we are defining
exception to notify us if the cash a
file is not read and at last we're
closing the object of the bufferedreader
now let's talk about the map function so
now the map function will be called so
the input format of key is long writable
and the value is txt we're also creating
a context of the mapper framework where
we will be writing our intermediate
output now the output format of the key
is txt and the value is aggregate
writable again here we are removing the
codes from the line and then we are
splitting the line using comma so in the
split array we have all the fields of
the transaction data stored in the
consecutive indexes now we are creating
an object of transaction class and
setting the values of the field using
setter methods and next we are creating
the objects of aggregate data and
aggregate writable class then using
transactions get invoice amount field we
are deciding that in which aggregate
data field the transaction would lie we
will set the value of corresponding
field of the aggregate data object to
one and next we have the output key
which will contain the merchant name and
the date of sale we will set the value
of corresponding field of that aggregate
data object to one and next we have the
output key which will contain the
merchant name and the date of the sale
so merchant ID name map method will
return the merchant name from the
merchant ID as we just discussed so we
are passing the values as merchant ID
and the date at last we'll pass the
intermediate result in form of key and
value to the context next the result
will be sent to the partitioner class
that is the merchant partitioner so it's
so over here so in this class we are
overriding the default get partition
method and in this method we are
converting the key using hash function
and using ABS method to return the
absolute value of a number and at last
we are using the modular function to get
the remainder and now we're dividing it
with the number of partitions so which
is nothing but the number of reducers
and in our case we have specified five
producers so the modulo five would
return the value between zero and four
and one more thing is the same key would
always have the same hash generated and
hence the modular result could be also
the same and thus the records with the
same key will be sent to the same
reducer and based on this records are
sent to the reducer so next we have the
reducer code and it's over here
that's the merchant order reducer the
reducer class as defined in the driver
code resides in the merchant order
reducer clause so here we have the key
in Buddhist texts and value input as
aggregate writable which was written by
our mapper class and the output key
format is again text and the output
value format is aggregate writable so
here our execution will move to reduce
method where we are passing the input
key value and context as argument and
here we are again creating the objects
of aggregate data and aggregate writable
class and next we are taking the input
values now here we are calling this set
of function of each category getting the
earlier value of that category and then
adding the new value of the new
aggregate data object for that category
so it will add the value to the
corresponding fields if there is a
record with the same key and at last we
are writing the key and value in context
dot write method so I have explained you
the code so now let us just go ahead and
execute it
so first let us move to the project
directory
so we have the bomb dot XML file which
has all the dependencies that we require
in order to run our MapReduce job so
this is the command so it has my jar
file and the path of my jar file then I
have got my main class over here which
is Merchant Analytics job so this is
where my main function is and then I'm
passing the three parts the first is my
transactions dot CSV this is my data
that is the path of my data set then my
merchant data this is the path of my
merchant data and finally my output
directory which is the result this is
the path over here so let us just go
ahead and execute this command
you
so the code is run so here are the
different parameters on which this
MapReduce job was run so you can see the
details over here so you can see the
number of reduce tasks where five since
we had five reducers so you can see all
the details here let me just show you
the result now so it is in the results
directory so there are two directories
over here because this was the earlier
one that when I had previously executed
it so this is the one that we have got
right now so let me just show it to you
so we have got five part files because
there are five reducers so I'm just
clicking on one part and you can just
click on download
alright let me just open it
so this is what you get so you get the
merchant name and the timestamp over
here and also the category our the
segregation that we did based on the
cost of the orders right so it was order
above $20,000 at this time stem and the
total order was one so this is the
format of the result so we have got a
lot of rows so this is the result so we
have just used a few fields of
parameters from the merchants file we
have just used the merchant name and the
ID over here since this is a sample
project sample demo project but the
scope of this particular project is huge
you can use a lot of other parameters
that was mentioned there like the
location you can analyze it based on
locations based on the time period where
the order was placed so you can take an
account different fields and improve
this or make this analysis even better
by yourself so when you're doing this
project as a part of your course
curriculum so you will be exploring the
other fields as well I have just shown
you with just using two fields the
merchant name and the ID all right but
there are many other projects that you
can choose from an IDE Eureka not just
that one so there are different projects
related to different domains so there is
one project related to media domain to
analyze movie ratings we have more on
social media where you can analyze the
YouTube data to identify which are the
top five categories which gets most
number of videos and which are the top
rated videos on YouTube so there is
another one related to the aviation
industry the airline data analysis then
you have got the customer complaint
analysis related to the retails you've
got the tourism data related to the
tourism industry banking and finance
you'll be analyzing loans and
categorizing it so there are many other
projects related to different domains
you can choose to complete any one of
them and even if you want you can choose
multiple of them and the best part about
doing this projects and executing this
projects is that you will get free
access to Ed Eureka cloud
now this is going to be very much
beneficial for you because the cloud
labs has got everything configured all
the software's that he needs so you
don't have to worry about setting the
same configuration setting a VM up by
your own in your computer so you can
directly access the ID rigueur cloud
labs it has everything configured and it
is asked for the industry standards so
even if you get a job as a Big Data
expert as a big data professional you
will be using the same infrastructure or
you'll be working in the same
infrastructure or in the same
architecture so even before becoming one
while learning it you'll get an
experience of what it is like to work in
a cloud lab environment right so by
executing all this projects you'll get a
very good hands-on knowledge about Big
Data and Hadoop and definitely you'll
become a Big Data expert thank you for
watching this video happy learning I
hope you have enjoyed listening to this
video please be kind enough to like it
and you can comment any of your doubts
and queries and we will reply them at
the earliest do look out for more videos
in our playlist and subscribe to any
rekha channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>