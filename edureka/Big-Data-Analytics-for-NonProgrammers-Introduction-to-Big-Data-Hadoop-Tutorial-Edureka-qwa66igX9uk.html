<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Analytics for Non-Programmers | Introduction to Big Data | Hadoop Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Big Data Analytics for Non-Programmers | Introduction to Big Data | Hadoop Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Analytics for Non-Programmers | Introduction to Big Data | Hadoop Tutorial | Edureka</b></h2><h5 class="post__date">2015-12-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qwa66igX9uk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Ashok a very quick
introduction about myself 20-plus years
of information technology experience and
I have about three plus years of
experience last three years I've spent
on a big data and I do product
consulting I have worked on retail
projects ecommerce and I've also worked
on network analytics a business
analytics in all those areas and I also
been I've been a pretty passionate about
being an educator as well so I've been
doing this for two plus years and this
is me and so today we have a pretty
interesting topic you know we have a
topic called big data analytics for
non-programmers so just a quick question
because you are you know if I can see
how many of you have joined I can see 22
people right so how many of how many of
you are actually non-programmers I can
see 23 names
just a quick gut check I just thought
I'll do it
non-programmer okay okay all right so it
makes it like five or six so that's not
bad it's it's about 25% I would say of
the crowd right so or less than that
right that's not bad so may rest of you
have a bit of programming bent so I'm
glad about that
but anyway it's good to know right so
thanks friends so what I'm going to do
today we are here to discuss you know
there is a very strong notion in Big
Data circle especially in Hadoop and all
that right people think that they cannot
conquer it
you know they cannot conquer Big Data
Hadoop without the knowledge of a
programming language especially Java so
this is a very wrong notion and we are
here to prove it right so we are here to
discuss and you know also make sure that
you know how we can still learn how we
can still use big data with some of the
other eCos
some tools which are specifically you
know which have been built for non Java
folks right so let's see that friends so
here is the deck so I'll also show you
know the structure is going to be
friends um I'm going to spend a few
minutes in the deck maybe 2025 minutes I
have very few slides by the way you know
like twelve or thirteen slides right
let's spend some time on discussing
those things then we shall move on to
thee or you know I have a couple of
demos so I want you to be little patient
there you know because even if it
extends let's say you know beyond 60
minutes so be patient with me you know
I'll try to wrap it up maybe as soon as
you know this I close with the demos
right is that fine friends or y'all with
me on that yes awesome alright so let's
start with the deck all let me give the
full screen can how to be learnt without
knowing Java how big can be used in
place of MapReduce querying data with
hive QL right so this is what we are
going to do pretty much right so this is
this is exactly our I would say it's a
moot point for us right so a lot of
people have this notion that you know
these two are inseparable and you know
we need to know Java so which is quite
wrong right so how do can be learnt
right so we are here to say Hadoop can
be learned without knowing Java that is
the first big statement we want to make
right we want to clearly say say to the
world that you know Hadoop can be done
without Java point number one let's move
forward
how okay so sure I'm I'll come to that
how we can do because to sure that you
know it's just a couple of words and I
mean if I need to give a short answer
that's why you know we are saying here
pink hi
these are all alternatives right they're
all alternatives for hood MapReduce
right correct
so the I mean that's a short answer and
we will see how right we will exactly
see how
tools like pig and hive that are
actually built on top of Hadoop offer
high-level languages for working with
data right so if you look at Pig and
high right if you look at it pig is a
data flow language right and it makes it
supremely easy for us to uh you know
anybody to quickly learn the language
and work on it right it's pretty simple
even the non programmers do not have
much of a problem right so it says if
you do not want to write MapReduce
program then you can use pig pig latin
for which knowledge of Java is not
required right I mean it you can also do
MapReduce but you know you can you know
pig is an abstraction of MapReduce in
other words right pig is an abstraction
of MapReduce and it hides the complexity
of MapReduce right so pig was actually
born in Yahoo correct so they were doing
lot of you know they were they do lot of
you know they were removing the board's
clicks because they they do they do
collect lot of such data right so pig is
primarily used as a transformation tool
in those cases and it is also used as a
you know it has it supports lot of
functions like regular expression and
all that so you can do lot of cleanups
using Pig correct and it is a dataflow
language it is quite high level it is
like whatever you know we do it as a
simple pseudocode this is the way I look
at it friends right if you if I tell you
you do a simple curve you know let's say
if I if I tell you to you know write a
simple program with a you know just open
this and do some action then printed all
those things right so it is biggest
almost you know it is like a pseudocode
ish language which has got lot of
functionalities which the sequel
provides like you know group by filter
we are going to see that in the
subsequent slide but my point is it
makes it very very simple right
pig makes it very simple for even the
non-programmers right and if you want to
view
a data and HDFS in readable form you can
use hi which again does not query any
does not require any knowledge of Java
right hi miss Eustace our data
warehousing right
it is a tool for querying and it is a
tool for you know especially mining the
data and all those areas right so hive
is extremely useful that way an hi
typically uses structured data right
structured data it uses so let's let's
see let's move little forward
wiping big simplifies complex MapReduce
program by using big lighting right pig
latin is a that's a language right and
additionally if you want to write your
MapReduce code you can also do it in any
language a Perl Python Ruby C and all
that but the most attractive features of
pig or 10 lines of pig is 200 lines of
Java right plus join you know I told you
right
whatever the sequel ish kind of
functions they are pretty much here join
group filter sort and it also supports
some data types like tuples and all that
which is probably not found in your
typical MapReduce right and it the most
important feature is the slide mentions
10 lines of pig is true in the lines of
Java which is significant right so it
completely improves the developers
productivity right so the statistics has
proven that a person you know if you
train a newcomer and novice right if you
train a novice of for six days someone
who has got you know let's say come
information from the engineering
background right let's put it that way
someone with that background with the
training of for five days they can write
a production ready code in pig right so
that's the kind of learning curve you
have it is just a very very easy
learning curve there's no uh you know
you just need to be logical that's all
right whereas for Java
you know the complexity right Java takes
anywhere between you know I think the
proven statistics is like you know it's
anywhere between sixty sixty-five days
for someone to write a production-ready
code so that's that's a pretty steep
climb for someone to make so that that's
the kind of difference we are talking
about and again there is a notion within
the big community some people think that
you know big is generally used for
prototyping right if you're given some
quick thing to prototype or it is used
as an ad hoc way to analyze the data
analyze the big data and all that it is
also used for sampling right so there is
a notion there also that you know pig is
used for such small things the answer is
a huge no Pig can be used for analyzing
any kind of data set right it can handle
whatever MapReduce can do right pretty
much it can do that plus it is also you
know the there could be scenarios in
which like you know pig is not so
efficient in you know certain things
like you know if you have very complex
loops or very very unstructured data
like you know video audio and all that
stuff right if you have such
combinations Pig may not have the
solution Pig may not have the solution
same time if it can do all the other
things right
you can still stick to Pig you can still
stick to big and probably have those
areas whatever things you know which is
not supported maybe you can use the
MapReduce maybe you can hire a developer
and just get that part done and call it
as a user-defined function UDF incent P
right that's possible right so the
bottom line is Pig can be used as an
alternative for your Java MapReduce
combination right so it makes it pretty
attractive friend so a lot of people I
have seen they were they've actually
embraced Pig and pig is a growing
community and it is open source you know
it makes it all the more effective
because it is open source it's pretty
simple and you know it just focuses on
the what pot not the whole pot right if
you look at Java MapReduce right it
gives you a complete handle so it gives
you that freedom at the same time it
comes with lot of baggage right you need
to define the file format you need to
define what is a key value pair and all
that stuff
here you just tell what needs to be done
and the whole board is handled by the
pig right pig engine let's put it that
way okay Jawad not mandatory in this
case so pick and chew any kind of data
right structured semi-structured or
unstructured data it's easy to learn
similar to sequel reads like a series of
steps and it is extensible as I told you
right use a different function you can
use it Python JavaScript Ruby also it so
you can do that provides common data
operations filters joints ordering
nested data types such as tuples bags
maps which are certainly missing from
MapReduce so it picks supports that plus
an ad hoc way of creating executing
managing contracts aborted by a
community of developers C as I told you
right ad hoc ways like you know again
you know we shouldn't think it's just
used for this it can be used for a
regular you know a huge you know let's
say you are having a very large cluster
and you know you have a running in the
production and all that you can very
well use Pig over there as well right so
I mean I've seen friends one thing which
I want to quote which I keep telling in
the many classes
there is one of the authors right I
don't remember the name of the person
one of the you know big evangelists who
has authored two three Java books right
he has authored two three books on Java
right so that person is a big evangelist
apparently right any advocates people to
actually go for big because he sees the
tremendous simplicity in that because
see the big data is all about you are
trying to solve a big data problem and
not to get hung up on a some complexity
in a language and all that because you
saw here to solve the problem right or
you want to analyze the problem that's
the that's the objective so he says you
know pig doesn't miss a beat there so
you are straight away on the job and you
you straight away you you focus on the
problem then get hung up on all this
Java related class path the JDK and all
those things right so
so that is coming out very clearly and
he says try to use Pig as much as
possible right he glorifies that pig and
he also says you know even if you cannot
use at least go with the you know the
places where you cannot use just use
MapReduce and you know use it as AUD of
within the pig right so that's that's
the path he's preaching correct and why
hi okay now it's time to look for hive
hive defines it has got a sequel like
query language in fact hive it is very
you know it has got the dialect of my
sequel friends so you'll if you know my
sequel right it is pretty much the same
thing um so people with my sequel
expertise you know they they would find
it pretty simple to use hi allows
programmers to plug in custom mappers
and reducers that's possible jarowair
owes infrastructure how maintains his
own warehouse provides tools to enable
easy EDL right how has this right so um
if you look at it features of hi you can
use hive to read write files on hadoop
run your reports from a bi tool it is
used as a document indexing predictive
modeling modeling and hypothesis testing
so you can you know any anything to you
know any futuristic things you can do
predictive you can use that in
predictive modeling right and
customer-facing bi because you know you
can do that because this is
predominantly a mining tool right how
can be used as a mining tool and you
know it does extensive mining and just
remember hive supports are you know it
comes with high-throughput it's pretty
similar to what hadoop does it doesn't
support low latency queries and all that
but there are ways to actually come up
with faster queries there are ways to do
that long processing it is use in and
data mining it's predominantly a rollup
you know suitable for wall up processing
reference you can do that okay any
questions french
before I start off in the demo I think I
probably finished it sooner than what I
thought okay if I have okay that's it
so of course this will be a certainly
you know I will step into the demo right
away but any questions
Tushar is saying I am reading out to
sharks question right now I am still
wondering how the problem looks like as
long as term big data is concerned how
the problem looks like big data is
concerned so too sure if I understand
your problem where you know your
question better I'm just trying to
rephrase it how I mean would Pig be able
to handle the big data is what your
question is all about is that what your
question is can be big handle big data
certainly yes - sure that's what I am
saying right in fact if you have very
small data right data said I think
you're insulting Pig you're insulting
pig or hype because they are suitable I
mean they have been designed they
actually they have been designed to work
on large data sets it can also do small
but you know in order to leverage the
power of Pig and all that you have to
use a very massive data set right is it
possible to write MapReduce code in
Fortran um I know you can do it in
Python you can do it in Perl you can do
it in the Ruby of course Java is
de-facto I'm not sure about Fortran
Alexander I'm not quite sure about that
but uh these languages are you know they
are there I'm not sure about Fortran
though I wish I knew the answer for this
yeah and and similarly you know just to
give you a little bit of perspective
about hive I was a you know the way Pig
was born in Yahoo I was actually born in
Oh Facebook right so Facebook you know
one fine day I mean they they were doing
lots and lots of you know they have
multiple servers and you know the kind
of data what they have right people
download
you know 500 million photographs upload
rather upload that many photographs and
you know people do like all those stuff
right so it's a pretty active vibrant
site which has got a population of you
know it's almost the more than the you
know it's it's inching towards 2 billion
right if someone takes a census you know
someone defines a country based on the
size size of the population Facebook
would be the probably the it can become
the number one nation kind of thing
right so it's becoming very uh it's it's
a growing community and all it is taken
the you know number one position in that
front right I mean it has destroyed
Orkut and Google+ has also you know
facing the wrath right so they are also
almost done now correct I'm not sure
whether they closed it but you know that
it's it's just couldn't compete with
Facebook here so Facebook has got deeply
entrenched with all the people and and
they are like really moving there right
so my point is with the kind of data
they're processing right so they do
extensive analysis analysis and you know
they process close to 100,000 queries
more than that per day right imagine the
you know number of query I mean the
solemn of the queries and all that so
they and their data was like growing
like a monster so they were using all
this Oracle and all that so so they had
to come to a platform where they can
handle big data so for them to do the
MapReduce right it is very tough right
so most of the guys were like you know
from the sequel background so they had
to they didn't want to get into a
trade-off situation like you know they
have to let go and you know probably or
trade them that has a one heck of a big
you know big problem they had so they
came up with an abstraction which is
high right which actually you know took
the you know the it takes a sequel
dialect as I told you and it internally
process processes things in MapReduce
right so it hides some complexity and
you know all those guys you know those
several programmers who have sequel X
they were leveraged here right okay so
now uh what are the prerequisites okay
let I'm looking at couple of questions
here let me answer this and go to the
demo friends quickly right I am new to
big data analytics I am a Java developer
what is the average time required to
learn Big Data it is a very subjective
you know it depends on you know it's a
subjective question guru process so it's
not like it's not like you know someone
with a great aptitude and you know
someone you know put some effort and all
that but you know a person with Java
background and I would say you know it
it is a kind of natural progression so
you can actually you know possibly try
to understand the Hadoop architecture
and how MapReduce works and all that
right with a little bit of effort there
I think if you spend the next three to
six months learning these things and you
know trying to do some projects in this
front I think you stand a good chance
what are the prerequisites
I think guru Prasad I think I hope I
answered your question could you tell
about HDFS sorry - Gupta I think I don't
want to deviate away from my main focus
but you know I will just tell you a
nutshell answer HDFS is Hadoop file
system which is an integral component of
of Hadoop right so there are two
components in Hadoop which is you know
when you say how do there are two
components which are together the one is
the Hadoop file system which is HDFS on
top of that you also have a MapReduce
right which is the processing so these
two are like kind of couple then that
constitute aadu right is that fine
is Big Data going to overtake DB and
become the only storage I think that's a
very wrong statement to make
john so the thing is big data is a big
data is a problem right so DB is a
database right big data is not the
database as such Bettina is a problem
okay and we have Hadoop as a framework
and which provides the solution right so
there are so many databases you know
which handles Big Data right let's say
Tara data which is not part of the
and all that it can handle big data but
it's not an open source and you can look
at some of the database traditional
database vendors like Oracle and all
they are all trying to adapt to big data
challenges like they've come up with
extra data and all that so there are
even these vendors are moving in that
direction and you know of course you
have lot of you know open source like
you know you have those no sequel
databases and all that so they are also
trying to you know their they are
gradually you know gaining market share
here right of course the top leaders are
still Adi BMS but you know these guys
are gaining market share they are
they're gradually getting in there does
it answer your question John all right
thank you
so let's more I think if I don't have
anything here let's once inference I
guess this is it
so I'll go to the demo part so just to
give you a you know I'll just present a
couple of demos here one on big and one
on five so let's see what this is my
virtual machine which I am running it on
the VM player right so this is my sample
data set friends let me start off with
Pig okay let me start off with pig
so I'll tell you this is my data set I'm
just trying to explain this I have a
bunch of URLs websites that are being
visited okay this is the number of times
the page has been visited this is the
page size right this is sort of
irrelevant right so this is the time
this is the number of page visits let me
let me possibly put this I'll say the
language right I'm sorry language and
this is the URL and this is the visits
and this is and this is the-- let's say
size right these are the four things I
have right
and if I were to ask your friend so it
clearly shows that you know that is
English
ah this is a site name and this is a
number of times the page has been
visited and here is the bit size right
here's the page size this is all I have
right now my problem statement here let
me put it this way my problem statement
here is to find out um let's say the
pages number of pages number of visits I
would say visits for each URL or site
right number of visits for each site
okay of for only English right
do you understand the problem fits did I
make it clear or did I confuse you here
my English is not so good so I just want
to I don't know whether I put that
correctly is that clear I'm saying to
find out the number of visits for each
URL for only English is that clear okay
all right thanks glad you glad it's
clear all right
now I want to ask your friends you know
I want to keep it like you know I want
to get your pick your brains as well
here how do I approach this problem you
know it's a very super simple problem so
I just want to you know get your
thoughts on this get your thoughts
counting no no just give me the simple
pseudocode friends you don't need to
give me any syntax or anything you just
tell me from a simple lemon Pesa yeah
okay Mohit says filter yeah that's the
kind of step I want whatever he says
right filter by language right so that's
one of the things we need to do right
filter by language okay so I would say
okay filter by language so it is like
you know in other words if I filter it
by English then I am filtering out
Chinese right I have only two categories
right
languages then what is the next step
friends what is the next step this is my
first step what is the next step
sum up visits okay count count the
visits okay
for each URL right okay before that
would you do something else
is there anything else to be done should
we do a grouping group by do you think
that is needed friends and then do the
sum up because you know I have
repetitions right correct
so I need to group these things so that
you know it is aggregated Plus group by
URL exactly right group by URL what is
the third thing I have to do count let's
say visits right it is this you know do
you think this logic will work if I if I
try to program eyes this simple
pseudocode right do you think this logic
should work okay people say yes okay let
me show you the actual code which I am
going to show you right so let's say
this one you will be surprised the kind
of you know you'll be surprised
you know how simple Pig is right so
let's say I am saying friends you know
this same file right I'm loading it
because you know it doesn't have any
schema right all I have is this data
correct so let me copy this here so that
you know you can see it alongside data
so exactly you can relate to what I am
saying what you have said so far right
so I'm you know piggy always you know I
need to assign it to a relation correct
so I cannot say load straight you know I
need to assign it to a relation right
and it is a data flow language right you
will find sequence of steps here so I am
loading it from the location of HDFS
right the pig whatever input because Pig
runs in HDFS mode by default that means
it has to run in the cluster right if it
has to run in the cluster then you have
to load the file input file has to be in
HDFS or two file system and output file
also needs to be generated in HDFS
correct friends so here if you actually
see right I am saying load what is that
I'm saying I'm saying big input sample
okay this this file right I I also have
it in HDFS I'll show you here so the
same file if I see big you see input
right you have this sample correct this
is the same file I have right so I am
loading that correct I am loading that
as you know project you know the
language is character array everything
is you know the couple of first couple
of things are character array page name
is a character array pitch count and
page size or integers right these are
some of the primitive you know the basic
datatypes pic supports and you know it
has got some other types as well so
that's not getting there and as you
rightly said filtered records right this
is my next ah relation okay I am saying
what do I do
I am filtering records right I am
filtering the courts PI language is
equal to double equal to I am doing
because you know I am checking for a
condition right is equal to English
correct this is English right I am
simply checking that then I am saying
grouped records right I need to group it
filtered records PI page name this is
page name
okay and whenever I have a double - it's
just a it's a comment in pig right it's
a comment in pig programming and next
thing I need to once I am done with the
grouping I need to for each group right
I need to count the number of visits
right I will be looping it through
correct so I am doing a for each grouped
records
generate group group is a alias here
okay generate group is nothing but you
know whatever is the
no the group here we have grouped it on
page name right the group actually
represents page name here right group
and the sum of filtered records dot page
count right some of this right so for
example if I you know let's say let's
say if I filter out friends let me tell
you the trance you know the way it is
going to happen right let's manually do
this and see let me filter out let me
remove this let me remove this let me
remove this right do you all agree that
Twitter is out of the game no because it
only had Chinese right and some of the
other pages of yahoo and google also
vanished along with right friends is
that clear now i have only four pages
here so in other words i have google
calm and what should be the conference
what should be the count of google okay
excellent Amit I made this fast off the
blocks yeah I need one yeah everybody is
saying there no excellent what about
Yahoo Yahoo is sixty right so thank you
yeah thanks okay this is all this is my
going to be my final answer right this
is my expected answer okay and you know
if you look at big right I'm like you
know okay first thing I have grouped it
and I'm looping it so these are all
results right if I want to do it you
know I can also use a ordinal number and
all that instead of doing this generate
group and all that I can also say
generate all of 0 right because it's
zero based I can I can also do that in
Pig and this is these are all comments
so let's say if I ignore this right
ignore these comments sorry let me
ignore this okay and let's say I also
ignore this and let me ignore this as
well okay let it run let it run you know
I want to give like you know a
meaningful name number sorry December
first
output right this number first output
and I want to give something like let's
say I want to give a this symbol caret
symbol December first output it's a
special first day of this month right so
let's put it this way and and and you
know once I get the results right this
this is going to pretty much give me
this right it is expected to give me
this the first first step
it would have filtered second step it
would have grouped by third step it
would have you know when I do the 4-h it
would have done this
okay let me also remove this just to
remove the confusion here okay and let
me remove this also okay
now remove all the comments basically so
just to make it look you know compact
and tidy here then I am sorting the
results by group I'm doing it on the
group otherwise you know you know so
that Google comes first right because
it's alphanumeric this is group is
nothing but here it generates base name
right I mean page name is the one that
is that's the one which is represented
by the alias group right then I am
storing the output on to the HDFS in
this location and is that clear friends
so far awesome
okay now I'm going to execute this file
so let me just tell you you know how how
it is like you know I can put that in
the grand shell so that you know I'll
pig is got a beautiful okay someone you
know an urban is asking what is this
right so on it but this is what to say
this is the let's say I am giving this
as the delimiter right I can give
anything under but let's say I can also
give this or I can give space as the
delimiter right so when it finally
prints here right so when I put caret
symbol it is expected to print something
like this I'd say
is that clear anymore sorry I should
have mentioned about that so let me put
it let's say I put this one right tilde
okay so it would print something like
this correct
all right now if you look at this
program I want to specifically ask the
non-programmers do you find this complex
because if you find this complex that
means I have not done my job
do you find this complex or do you find
it easy do you find it very intuitive
the way you have structured the
pseudocode pretty straightforward right
friends
I mean don't tell do you know just to
you know make me happy but you know I
want to be really you know I want you to
be candid here it it cannot be easier
than this friends so I'm very very very
sure about the you know why bow are you
saying it is complex or it is calm it's
easy pipe I'm saying no complex okay so
can I take it as easy or you're saying
no no no it is complex okay easy okay
Prasanna says easy all right
so friends yes provided you know all the
commands and constructs here absolutely
through sure I mean if you go to Apache
8p right it is super simple you have all
those commands and all that you just see
this look at it right it just it is just
the way you know that you look at the
pseudocode right just open the file
filter it group it loop it order it if
you want then you store it correct
filtered records are there any pick
specific user-defined no these are all
you know for these these are all pick
commands filter load group for each
these are all big keywords for your
example like you know for example
generate some these are all these you
know some is a function here right so
order all these are big keywords which
you need to be aware of correct
so these are all the things as Tasha
says right these are all the things
which you need to learn correct so but
to me this learning is not at all you
know is
I can I can certainly say that you know
if you have learned Java you will know
how easy it is right but if you even
start without knowing any programming
language this is very much like a pseudo
code although it supports sequel things
right
it just right it is like a pseudo code
right maybe some code can be copied and
reused to save them exactly to char see
the point is I am going to show you the
grunt shell grunt shell is a interactive
shell that is used in the pig right but
same time I should tell you you know the
whole thing right this this let's say
the this is the you know I'm going to
show one by one just to make you you
know make you feel comfortable but I can
run this as a script as well I can run
this as a script like you know sample
dot Pig I'll also run it towards the end
from the shell right from the shell I
can run it in one shot I don't need to
go through the grunt grunt is an
interactive shell which is primarily
used for debugging right so let's run
this friends one by one right so let me
copy this and you know put it in the
grunt shell let's try it out let's say
records is this okay now arm just a
minute I am running one by one see it
doesn't kick in the MapReduce until it
encounters a store or a dump statement
right so it doesn't kick in the
MapReduce so we are good with that
filtered records is this third one
grouped records right and results done
now store
sorted results order results by group
okay final one is store the results on
to something so let me say I and there
is a fantastic diagnostic operator
illustrate right if I want to do this
right illustrate what it does is you
know it suppose you don't you don't know
how
the pig is going to work you can always
use the illustrate it will tell you how
the data is going to be you know how it
is going to be fetched right from this
right right from the first step right
right from this all the way till here
right he to tell you how the data flow
is going to be okay so fellas Phil is
asking right now what is grunt so grant
is an interactive shell you know Phil so
why it is called grunt this because pigs
do grunt right pigs grunt in the literal
sense so it is called the grunt chill so
that's how it is here thanks if you look
at it this is going to call the
MapReduce okay this is going to call the
MapReduce okay look at it right it's
pretty so it has picked up actually
illustrate takes a sample of the data
and it will tell you how it is going to
approach the problem so the sample
itself is pretty small right well the
data said whatever we are taking is
pretty small so look at this it has
taken this then it has filtered for only
English right see it is actually telling
the approach how the pig would approach
the problem so you can you know in case
you want to modify or anything right you
can always you know go back to the you
revisit the script in case you are not
comfortable with what illustrators done
but this is a you know it is up to a
developer to see whether the
illustration is fine or not right
whether he wants to take a corrective
action based on this right then
the grouped records look at this it is
kind of getting the you know Google has
got this is called a couple in big right
nested data type so Google has got three
Yahoo has got one then it is like you
know 60 and 150 this is how it is then
you are sorting it out based on Yahoo
comes first so sorting it based on the
you know the URL so Google comes first
150 and Yahoo 60 and this is what it
says is this is how big is going to
solve the problem right it is going to
kick in the MapReduce and you know
because it has to do the aggregation or
all because we are doing a group by and
all that right so let's say you know we
are doing this you know this is a grunt
shell right let's say you know I want to
I don't want to run it in the grunt
shell because you know this is like a
debugging one right now you're pretty

this is the solution for you right what
if you want to run this code on a
nightly job daily in the production
right production server you want to hit
and run this program over and over and
over right so you want to run it from
the shell you don't want to come back to
the Grunch Elland do all these things
right so what I will do I will go back
here so let's say this is a hive shell
right so let's say I want to go here and
I am sorry
hmm oh this is where I am okay let's say
I want to do big sample not big
this is where my let me show you LS you
know this is where my sample not big is
there right sample don't big is there so
and I'll say a big sample dot big right
I want to run it like this right pretty
simple so let it run and I want to store
the information the final output in the
location where I just showed you right
this is where it is going to be stored
the location is here it is number first
whatever okay I think you know it is
going to be putting us this folder let
me October 24th new something here let's
see whether it puts it there right big
output okay so let's say I want to put
in somewhere else friends just let's say
December 1st right December 1st
big right December 1st big let's say
because this folder already exists right
it's not going to like it so let's do it
again
sorry about that I am doing it here it's
too minute the example not be there you
go
okay it's still doing
all right so it's almost done friends
just a minute
which editor would you use to write the
code how would you compile - sure I I
use G edit you can also use VI right
it's taking time huge okay
I'm it is saying it's taking huge time
for small data set here that's a very
very valid I mean it's done now it's
almost done now that's that's true a
myth but I will tell you why
big is leveraged to use a large data set
right
it is like friends let me tell you a
fancy example suppose you know you are
taking a huge you know let's say you are
block the ship right which can take
let's say if I thousand people right
unfortunately due to some circumstances
you have only five people in the chip
but do you think the operational expense
would be cut down it's not right because
the chips still has to use most of these
resources for it to move right so
similarly here this is supposed to
process a huge data set but what we have
it is like you know it's a it's a pretty
it you know we we are using a small data
set because it's you know we are just
doing a quick demo here right but it is
it can very well it will take it won't
take too much time for processing a
large data set right it's not going to
significantly it take same time no I am
NOT saying same time why but you know it
won't be like you know if you think this
is taking you know let's say two minutes
when you increase the data set it's not
going to be like directly proportional
it's not going to be like that right
it's going to be faster trust me on that
okay so it is done success I can see
success let's go back here big output
December first pic here you go okay I
mean I gave
I didn't put the what he said tilde
there this is the one we did right so we
used asterisk right so it is it has
generated this way right I can use any
delimiter there feds are you clear with
pic so I just wanted to I wanted you to
feel the simplicity right to me you know
that's that's the that's the one I am
trying to emphasize friends it's the
simplicity and also there is absolutely
no need for you to learn Java and all
the complexities of Java because if you
saw big what is that done look at this
it when you looked into this right it is
actually used to you know Map Reduce and
everything right it had used Map Reduce
correct it triggered a MapReduce job but
you are like you know you just start you
we all we did was you know we where we
just told the pig okay this is what I
want but what it did you know the whole
part is beautifully handled right there
is no problem at all it called the you
know job tracker whatever whatnot but it
did all those things but we are not
worried because we just focused on the
what part the whole part was very much
handle by pig so let me close this guy
um let me come to this and let's say you
know let me close this friends I don't
want to be here oh that's aiming so now
this is the editor someone asked me
right did you take did you use this
editor this is the editor you know which
I just showed you right it's like a
notepad age kind of editor you can also
use UVI right so that you can it it's
left to your choice so let it be let it
stay open and I am just switching my
switching the gear and moving on to high
friends let me quit from here right and
this is it right I'll okay very user
friendly as well the chef says
absolutely - sure very very user
friendly and supremely easy trust me on
that so the thing is you know Java and
all that you know that's just a portion
and our people think that you know
that's the only way they can do map
absolutely not there's so many other
ways and you know you can tackle the big
data problem right so this is a high
prompt so if I see show database and I'm
switching gear here I'm coming too high
so high as I said you know it is very
let's say it's sequel ish or it uses the
my sequel dialect correct so let's
create a database friends so I just saw
the database right this is a so let's
say I create a new database create let's
say December 1 right test I want to I
don't want to sound so complicated
ok let's put it this way this number one
test correct so before that you know I I
just need to you know the syntax is like
this create database December 1 test and
I need to end it with a semicolon done
okay now I say if I say show databases
it's supposed to show this as well
December 1 test is right up there right
so it has been it is coming so let me
load few things as part of the code
whatever I have right as you know let me
take let me go to hi I'm going to show
you a simple demo you know how I do
partitioning and all that so let's say I
want to open this I will open this and
this is it just okay all right so I'm
going to use this I'm going to use this
setting so it at least tells you which
database is actually in use otherwise
you don't realize which databases it is
actually using default right say if I
want to use my current database right
what do i what am I supposed to say
December first test right this is it so
I am there correct okay so friends I
look at the question window of bad with
me I'm going to look at the question
window once I am done with the hype demo
right please bear with me on that I am
definitely going to take those questions
so December 1 test so I have the
database with me right now look at this
the one I am going to use it right now
what I'm going to use it like you know I
I see I have a transaction record right
so this is actually a transaction one
dot txt you know I have loaded this file
let me see I don't know whether I loaded
this okay let me open this transaction 1
dot txt right it's a pretty big data set
like fairly big data set if you look at
it if you look at it it has called a
transaction number and it has got the
transaction date and it has got some
what is the third one
great customer number right customer
number amount and it has got the
category product city state payment mode
right it sits and you've got around you
know I think you have like you know
several thousands of records right - so
this is what I have so I am going to
analyze see what I'm going to do I am
going to create this database right
where do I you know not creating the
database I am creating a table correct
so now that you know we have created the
database so I put the context to use
this database current database which is
December first
Tarara now I am going to use this create
the table right that's the step step
number next correct so I am going to go
there
hi prompt so let me copy this over so
this is my create table look at the
syntax transaction number so I do all
these things right row format delimiter
I just want to tell you friends form ID
limited fields terminated by this stored
as text file right so what this means is
okay it is very quickly graded if you
look at this right the delimiter the
delimited delimiter used is actually
comma which is the delimiter I am
talking about friends I am talking about
this delimiter right
it is comma everywhere right here is the
delimiter fields terminator right fields
terminated is actually that's the
delimiter and stored as a it's a you can
have it as a sequential file and all
that so here it is a text file format
correct the file is a text file format
so it has created the table right within
the December first test database where
do I go and look at it if I go and check
let's say from here if I go and look at
it you know this is the place where I
have configured in the hive ql right
there is a hive XML you know there is a
hive XML configuration file right where
I actually set the location location of
my Pharos right so this is where it is
pointing to this is actually
configurable but you know I am just
saying for now this is where it is
warehouse if you look at it it has
created the database nicely and I have
the transaction records and why is it
empty friends any reason why is it empty
any guesses here okay no data okay
so for simple right we didn't know no we
executed it but we didn't load any data
exactly so that's where that's what
mohith is trying to say exactly right so
I am going to load the data that's my
next step right so if you look at the
syntax again super simple load data
local in path because I am loading it
from the local local file system I am
not picking it from the HDFS I can also
pick it from HDFS but you know I'm just
stayin you know I'm saying local data
local in path if I don't say local in
path right if I just say load data in
path then it's it'll look for this path
from HDFS right since this file is going
to be loaded from local I'm going to say
local s the keyword one of the keywords
so let's say I copy this and put it over
there
sure what it has done it has copied this
file right
it has copied this file on to the
location if I go and check it out now
this file is coming right all these
things so you may be wondering what is
what is that I'm trying to do right what
is that I'm trying to do because I want
to dynamically partition okay I want to
partition the data are based on the
category right
I want to partition based on the
category right so you must have heard
about the partition friends it is like
you know it is like a slice of a pie
right if I want to let's say you have
other cut data if people tell you to
partition it based on the zones right
you may put it you may you may probably
create four five partitions like you
know not not zone central zone where
Sony's zone so on so forth right right
this is how you're going to do so this
is like I am go to partition because in
hive you can also do a static
partitioning you can also do dynamic
partitioning which is like it's going to
happen during the runtime right so I'm
going to do a dynamic partition here
right so um I have done this like I have
loaded the data now if I do a select
star select star from if I say just as a
transaction records right transaction
records right transaction records is
this a yeah it's it's going to load
everything correct it's going to load
everything so I'll just let it run I'm
not worried about that for now right so
these are all some of the other things
but you know I'm not overly bothered
about you know you can do describe
describe will go into give you the
schema so let's say you know right it
gives you the schema right transaction
number this amount double category
string product all those things right
this is the describe part if I do a
count star you know it's going to tell
me how many number of it's actually
going to do a MapReduce right it's going
to do a MapReduce and tell me the
number of records right in this table
correct let it run meanwhile so if I
wanted to get the top 10 customers I can
do all this limit 10 and all that if I
want to do total spending by category
like if you look at it right category
sum of amount and transaction records
group by category so I can do all these
things so let's see how many records
it's same fetched one row total okay
it's 50,000 records right this is a it's
fairly a big data set right 50,000
records I have correct now if I do a
partition table what am I supposed to do
let's say you know first of all before I
do the partition I need to tell hive
because hi by nature it is very
conservative right it runs in the you
know sift as a safety measure it runs in
the strict mode which actually prohibits
if you see this write queries from
partition tables without a ver Clause so
sometimes you know if you don't give a
strict write a huge MapReduce job can
actually trigger an error if that's
possible if you don't make it non strict
right so I'm going to say dynamic
partition let's say I'm going to call
this s non-strict it is just a setting
friends right it's just a sitting so
I'll also doesn't say that you know
please let the dynamic partition to be
true I mean if it is already set as true
in the configuration file that's a
different story so I'm just setting it
you know just in case
correct so now I am doing the great
table transaction by category as I said
if you look at the syntax right
transaction number transaction date
customer number amount all those things
is coming except the category because
that is going to be the partitioned
parameter right the category comes in
here the rest of the things follows the
same order if you see that right if you
see this order category product city
right here it is category or
it's like product and city right
category has been taken out and used
used as a partitioning column here so I
am going to put this here done so it has
done that now what am I supposed to do I
have created you know if I if I go back
and check it out here friends December
DB it has created the second table and
it is empty because we haven't done the
loading part so the loading on to a
dynamic partition right you cannot do a
load all you have to do is the first one
is a base table right the first one
which contains all the records right now
I have to move the records from here and
on to this location on the partition
table location right I have to move that
if I move that it is going to kick in a
MapReduce job it is going to kick in a
MapReduce job then it is going to put
the you know it's going to create
partitions at runtime right friends so I
am saying insert overwrite table you
know this is a syntax for that insert
overwrite are insert overwrite table
transaction records by state correct so
this is the second table correct oh
sorry here instead I'm sorry insert
overwrite table transaction records by
category right this is the table which
we have created right and partition I am
again stating that you know partition
that okay partition based on category
then I say select this from transaction
record faces the base table distributed
by category so it is going to distribute
it like you know it's going to create
the reducer and make it make sure that
you know it gets distributed right so I
am going to run this friends I am going
just going to run this so this is going
to kick in a MapReduce process and it's
got a create T it is expected to create
the partitions and put the data
accordingly right so if I you know it is
basically in
we have multiple categories right so I
am going to put the data based on
category friends are you clear so far
whatever I am doing say it has created
that many partitions right
what override does okay I'll come to
that mohith good question yeah so far so
good excellent
so let's say I go back here if you look
at it right it is this is where I just
put the partitions if you look at it
it is created how many categories like
approximately 15 or something right if I
go to puzzles I have you know the part
file and it has put all the data here
right I have all the data here correct
and if I go to something else games I
have all this thing which are pertaining
to games correct so it is a proc you
know it has partition the data and put
the thing it has processed MapReduce and
you know it has put it it has
distributed right when you say
distribute by right it distributes it
based on it distributes the data and to
fit in nicely at each partition right so
you are Kumar that's precisely what you
are asking and Mohit is asking what is
but what is overwrite do so friends you
know this is where I am coming to right
he is asking what is over right
so that's insert over I can also do
insert table nothing will happen
right suppose I do I I execute this one
more time right
you know Mohit if I execute the same
thing again it's going to create again
right so every time when I do overwrite
right it wipes out everything it wipes
out everything you know from that table
then redo the it just does it again
right it just overrides it so that you
know it is just one copy every time so
we are ensuring that you know it
properly works so let let me show you
one thing you know initially when I
create at the table right let me when I
loaded it look at
this load data I also did have over
right here
correct suppose I don't do not have
override if I again load it right it'll
create a copy of this file in the
warehouse right so I will show you what
it has done since you have asked it
right I will show you if I can see that
part where it is loaded and all that
right ok
I can't okay let me just simply do that
part and show you again you know just a
second let me load it again know how
right let me load it again because the
table is anyway there if you see it
carefully right it says copying data
from this location copying data right
loading data to this duplicated deleted
look at this it is deleted it also it is
deleted what was contained there earlier
now it is going to put this again right
if I do not put overwrite it would have
created a copy of this file so it love
two identical copies right Moorhead so
overwrite is generally used so that you
know we always use one copy is that fine
all right awesome so so I've created the
partition you know if I want to query it
right let's say I want to query it by
games right because hive is all about
friends or you know we need to
intuitively partition it and you know I
can do extensive query in although it
has got high throughput I can we can
still you know use our you know we can
still use if we are very clear and
correct about the partitioning and you
know some of the indexing capabilities
right we can exploit those things then
we can use you know we can do a faster
analysis right so let's say select star
from select star from transaction by
category I don't know transaction
records right records by category where
um categories is games right if I want
to query this one for example Oh what is
it saying okay sorry
category category right this is what I
have again I getting of error not
supported of this is my bad friends
sorry about that okay
so it comes less than you know it's a
super fast no the way it fetched so many
rows right so because it is partitioned
if it is not partition you know the time
it takes right and our typical MapReduce
right you just saw the time if it
doesn't here and only if it is partition
it's not even doing the MapReduce right
we are avoiding the MapReduce so if it
is not that then it takes let's say I if
I say something like you know this is
partition on category right if I say
we're something else if I do it's going
to kick in a MapReduce can be
partitioned by multiple fields yes per
mohith it's possible to write let's say
city is equal to let's look at the data
just a minute Orlando right sorry
yeah look at this because this is not a
partitioned column right city right so
it is kicking in a MapReduce sub
partition yeah under one you can have
partition within a partition you it's
supported here as well so it has taken
look at them although it is 450 rows
look at the time interests occur right
we were able to fetch 363 thousand some
rows in you know less than a second
right so we can very well avoid the
MapReduce and you know the idea is to do
the dynamic partition in a very
efficient way right so friends I hope
you enjoyed the demo I just thought you
know I will give you a dosage of you
know what big and high what is the power
of big and high oh right so I'll take up
some you know can you tell us you know I
think one question which I whether I
missed any question all right so friends
any questions here can I take the
questions from now hope you enjoyed the
demo was it useful to you so let me
close this MapReduce not required you
vacuum are actually oh thanks MapReduce
not required no you akuma is asking no
the point is Yoko mori once you have
done the partition right once the data
is already segregated distributed by
based on the partitions then it is like
you know the way it is done right it is
all you know how doesn't need on those
partition the data right partition
fields how doesn't require to execute
MapReduce to find out so it's simply
there's a kind of you know loads from
that location from the warehouse right
whereas if you do not have anything
right if you if I if I try to query it
on a field which is not partition right
then you need to then it require it it
triggers MapReduce which is time taking
right ok
sure is saying it was useful I want to
know the name of the reference book to
understand learn various commands to
shall definitely be my guest on this so
you have do you know one of the books
which I can advocate is like you know
definitive how do definite of guide you
know you can probably take a look so it
has got a lot of useful information
there are numerous books to share you
know I don't want to influence you here
but there are numerous books here right
so um friends are may audible to char
says I lost voice am i audible here
anybody facing audio issues all right to
shut probably at your end is what I'm
thinking okay how do i execute big or
hype Phil is asking big is nothing you
know of village it's quite simple right
I can run the commands in big using the
grunt shell or I can sake you know put
the series of big commands you know in a
dot big file and just run it as you know
from the file from the shell prompt
right that's what I did sample dot big I
ran it for you right so that I can do
and similarly hive you have the you know
the way we ran it from the interactive
shell here I mean hi shel I would say
right this is you know I can still do
all the things here I can also put all
the series of you know commands and put
it put it as dot SQL file and run it
from the command prompt right that's
also possible in high res well correct
friends so any other questions you have
what software I've installed for Piggin
hype I have to install so I need one the
point is you know you can go to you know
once you have installed Hadoop of course
Hadoop is required right because this is
our do Pico system tool right both of
them so how do place needed once you
install the Hadoop right then you can go
to the up you know go to the pig Apache
and you know look for one of the stable
releases over there install that of you
know it will come up with the you know
tar you know
Dhar ball and you know you just need to
under that right then it should be
installed and similarly you need to do
for the highway as well right just go to
the Apache site that's the best source
what is the main purpose of partitioning
okay Alexander is asking and I'll tell
you the main purpose of partitioning is
you know partitioning is nothing but you
know segregation of data Alexander right
it's just a data segregation for you do
you know main purpose it to you know if
I need to query French this is used in
the database as well right if you have
seen some of the databases which are
which are in use even the RDBMS from
Oliver we partition the data why do we
need to do a partition the idea is you
know for faster querying and all that
right because if my data set is you know
having like hundred thousand rows if I
have like let's say five different
partitions and I if I can slice and dice
on those partition the area right which
is like you know probably twenty
thousand I maybe rice I may be querying
on the twenty thousand rows which is
going to be definitely faster right so
that's one of the basis for us to do
partitioning please type your email
address - sure I would suggest because
you know I don't want you guys to get
disappointed here the reason is you know
I don't want to be the bottleneck in
case if I don't respond and all that you
know I can give the email address but my
point is you know please talk to the
iterator support it because they
definitely work with SLA they're
isolated driven teams so they definitely
you know they they are pretty good in
you know responding back and all that
which platform do I need to use Windows
or Linux so - sure I think it's
absolutely left to you but Linux you
know latex is a great combination to
have right sure I am not from technical
background will it help me to get a job
fiber okay um vibe oh it's all about you
know your see if you are not from
technical background you can definitely
look at it from analytics aspect as well
right it is like see people are lot of
BA
they have this knowledge as well but you
know they can if you are on to the
business analytics friend right that
that is one of the areas you can look
into that right but still even if you're
non-technical person right it's good to
have this knowledge so that you know
lets you know some of the bees have also
taken this course
and they're people who are in the
analytics people are in the testing
background because data quality is one
of the rising domains right so that
involves lot of qualitative you know
aspects in the testing right so be a
people exactly right so those people you
know they they also need to have
knowledge in this right because they
need to know the possibilities what big
data can do and all that right so even
though those I would say you know it's
going to be very useful that way will it
help for data scientist okay that's a
data scientist is a you know it's it's a
pretty of you are the pointers data
scientist is a you know it's a very how
to put in here oh it is a very heavy
highly responsible job right and they
are fairly well paid
you know top-notch data scientist are
very very well paid the reason being you
know they have a breadth of knowledge
right they not only know big data they
have a good grip on the architecture
they have good grip on the you know
let's say lot of algorithms and you know
they are very good in al gore's right
very good in algorithms and you know
they can optimize they're very good in
you know analyzing the algorithms and
even optimizing them so they they are
good in many of these things and they
also very good in technology they also
know few domains so they have a very
good spread of knowledge right which
they really put to use you know doing
the data you know data analysis and all
that so it's a very fancy you know it's
a very it's a job that comes with you
know definitely they that pays you well
but you need to really have good
knowledge you need to develop that
correct
how to start working on this big data
who will give us an opportunity to share
the you know
a lot of people have asked me this
question the point is like Padilla is
you know it is really picking up so the
supply versus demand if you see the
graphs right
the supply is like very less compared to
the demand demand is growing like
monster right and the data is gonna
going to grow for the next five to ten
years right it's going to grow and grow
and grow and you'll of people need to
analyze the data this lot of data
related work right it's just not big
data is a as I told you it has got
varieties of rules so you don't people
again they make a you know they think
that you know it's just a picky eater
developer and Java they get constrained
it's not like that big data has got
architect role big data has got
developer role and you have big
developer you have high analyst and you
have big data analyst you have big data
let's say testers and you have Hadoop
admins right there are multiple flavors
of jobs right but you need to definitely
have a good grasp of on the architecture
and all that right so yeah its absolute
you know the possibilities are you know
infinite I will say lots and lots of
possibilities and lots and lots of areas
are opening up right it's a very
attractive proposition you know it's a
very you know great skill set to you
know add on right because it's only
going to grow and the future is
prettiest looks very certain and very
bright on this friend right any other
questions friends all right friends you
can i think this session has been
recorded so you will definitely have the
recording in the probably in the next 24
hours or so do you do you need email ad
no Bible I think any okay I have few
other questions coming up let me answer
this do you need email ad know why bow I
think if you have registered for the
course right anyway you will be informed
about the recording and all that ah any
idea on big data program management of
course Kumar the the point is you know
well I didn't mention about that said of
the some of the project managers right
I've seen people working in product
company
and all that so they also have managers
you know some of the you know people
work in the practice right we did a
practice in some of the companies right
and they have ahead of you know had big
data big data practice people work on
and big data research people work on
some research projects people work on
prototypes right and people work on like
as you have seen as you asked program
management right even for you to do you
know let's say you are doing a agile
using the agile methodology here to
manage projects and all that if you are
doing program management program project
management whatever it is here if you
have a good grip on the big data so
that's going to definitely help you even
in the program management perspective
right so a lot of people have gained the
knowledge in this front also because I
have hand a lot of classes here and and
I have seen a very very diverse set of
people coming in for the class you know
because they want to gain this and they
want to they don't want to miss the boat
you know the point is you know they
really want to gain traction in this
because this is the in thing right now
big data may need ID okay next question
is big data admin need idea on big data
programmer you know an urban both are
you know kind of different roles to have
okay
we did a programmer is a very generic
one you know you can have Big Data
programmer it's like you know somebody
is like a pig programmer or Java program
with this you can have varieties of
roles within that itself right big data
admin again if are you talking about the
Hadoop administrator are you talking
about some HBase administrator right so
it is lots and lots of offshoots are
there within each of them right that
that really gives him multiple
possibilities right how to administrate
exactly how to choose specific stream so
Mohit if you are very new right you know
you can look at it from this angle right
if you are wanting to become a big data
Hadoop developer or you know or
architect something
that you can probably choose some of the
courses well I mean it's it I'm leaving
it to your choice but all I'm saying is
you know you can look into a course like
you know a big data rate which is you
know for example I can say from ADA
rekha because I work with them they have
courses tailor-made for you know big
data you know someone who can start from
the scratch right so they have courses
like that they also have courses on
analytics they also have courses for you
know so many other you can take a look
and you can lil your you're free to
choose from other providers too but you
know I'm saying just look at it and then
you can definitely you know because they
have a plethora of courses you can
probably you know look at it to get some
idea what I am talking about right so um
I know SAS and our will Hadoop can help
in analytics domain absolutely other
Thea Sasson are just analytics but you
know if you are analyzing the data that
is coming from her do why not right you
should be able to do that correct so
it's definitely going to help is a
clearance all right
so friends I think it was a pleasure or
talking to you guys and ender my request
is you know please provide the feedback
towards the end I really because some of
the people have dropped out now so from
from this so thanks very much in fact
you know I'm really glad that you know
we are I hope we had a good session so
please provide your thoughts and
feedback your suggestions because that
goes a long way for us that really helps
us right thank you so much friends one
more time and and you all have a
fantastic future ahead thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>