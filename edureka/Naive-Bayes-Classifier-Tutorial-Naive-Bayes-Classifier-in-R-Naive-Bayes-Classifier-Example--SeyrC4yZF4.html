<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Naive Bayes Classifier Tutorial | Naive Bayes Classifier in R | Naive Bayes Classifier Example | Coder Coacher - Coaching Coders</title><meta content="Naive Bayes Classifier Tutorial | Naive Bayes Classifier in R | Naive Bayes Classifier Example - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Naive Bayes Classifier Tutorial | Naive Bayes Classifier in R | Naive Bayes Classifier Example</b></h2><h5 class="post__date">2015-03-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-SeyrC4yZF4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so without any further ado let's go
into knave based classifier okay and so
today so again as a nail as the name
service this is a classifier built on
something called les Bayes theorem okay
the be ayes base it's actually a
person's name called base and he came up
with the rule called Bayes rule back in
I think seventeen hundred's
okay not even 1907 mm but seventeen
hundred's it's a it's a rule about
probabilities and specifically about
prior probability and the posterior
probability and Navy Bayes classifier is
essentially the formulation of a
classification problem in the in the
aspect of for this Bayes rule okay in
the k-means we we looked at the
formulation of clustering problem as an
optimization function right similarly
you can formulate the classification
problem as a probability rule and you
can use Bayes rule to solve that problem
and such a approach is called naive
Bayes classifier okay
so as I mentioned its relies on Bayes
rule which is essentially a probability
equation connecting rare probability and
posterior probability so before we
understand the classifier obviously we
have to understand these basic things
called prior probability and the
posterior probability okay and before
that we need to know what probability is
and I assume every one of us know what
probability is so let's call let's talk
about prior probability okay so as
described on this particular slide let's
say um
one of your friend is talking to a
person in a train journey right and he
is telling you about that conversation
and if somebody else asks you what is
the probe what do you think are whom do
you think your friend is talking to a
man or a woman
okay so you have to guess who that
person is and what would you guess it's
a 50-50 chance right so your prior
probability that means without knowing
any information about the conversation
or about the specifics of that
particular incident you have some prior
probability associated with the man and
prior probability associated with a
woman okay so there is a point five
probability that your friend is talking
to a woman and there is a point high
probability that your friend is talking
to a man right however if you have extra
knowledge that let's say like a railway
person tells you that okay 70 percent of
passengers on this particular train are
women and 30% of them are men okay you
got that information somehow then what
is your belief or probability that your
friend is talking to a man and Prinze
talking to a woman it suddenly changes
right now your probability that that
person is man will be 0.3 and that
person is a woman is 0.7 right because
of the knowledge that you've gained so
your prior probability will now be 0.3
and point 7 okay so just to recap prior
probability is something that you just
know about a certain thing without any
specific information okay now this is a
probability that you start with and then
during the conversation let's say
your friend tells you that that person
has long hair okay
now suddenly your probabilities changed
right because since the person has long
hair there is higher likelihood that
that person is a female than male right
and so that's called kind of a posterior
probability after effect okay how things
will change after you know certain
information and the Bayes rule is kind
of connecting these two prior
probability and the posterior
probability okay so this example is just
to make you understand what is the
notion of a prior probability okay I and
I hope that's kind of gotten into your
head now with that okay so let's look at
a classification problem and we'll see
how it can be formulated as a
probability in terms of probability okay
so this to make our discussion much more
easier let's take this example you have
a bunch of data points some are colored
in red and some are colored in green
okay and our problem is that given as
the new data points that come in and
into your system you have to classify
them to be a green or red okay depending
on whatever you want to do as the data
points are coming in you want to
classify them to be green or red okay
without telling you any information
about the new data point that means I
won't tell you whether it's falling in
this area or this area or this area or
this area without telling any
information can you come up with the
probability that the point is green and
the probability that it is red can you
think of some idea of coming up with
that probability all I gave you is this
data set show you where the green
points are the grid points are how many
of them are there and so on I gave you
this training data and with their
training data can I come up with some
probability which tells you what is the
likelihood that the new point is green
and the likelihood that it's red I look
at this data and I kind of make a
reservation that about two-thirds of my
data is green and one-third of my data
is red right so that means I have some
prior bias towards red towards the green
values right it's likely that it can be
the chance of the new data point being
green is more than the chance of that
being red right
that's the prior probability I simply
count how many greens are there how many
Reds are there and I assume the new data
also kind of follows the same
distribution and therefore I compute my
prior probably okay so that's the notion
of the prior probability without any
information about the data point that is
the probability that you expect the new
data point follow okay now let's say the
degree the video suggests that
description prior probability of green
is essentially the number of green
objects divided by the total number of
objects obviously and the prior
probability of red is similarly number
of red divided by the total number and
in this particular example it can be
let's say 2/3 40 by 60 and 20 by 60 okay
now
okay okay so now a new data point comes
into your system okay and let's say that
is our this white circle the small white
circle since its color is not decided
yet it's in white cover now you have to
decide given its location and other
information that you may want to have
from that point given that all that
information now you have to make a
decision whether it's going to be a rake
or a tree okay
and one possible way of doing that is I
draw a small neighborhood around that
particular point okay let's say some
circle with some radius now I can't okay
with this circle now can you tell me
whether it is likely to be agree in all
red okay I got a new data point in my
system I put a small neighborhood around
it thinking that that neighborhood
drives my decision process now what
should this color be okay if it's
obviously red and that is because there
are more number of red points in the
neighborhood than the Green Point and
therefore we expect that new point to be
red color okay how do we how do we
formalize this inclusion which is
essentially we are talking about the
likelihood what is the likelihood that
this becomes a grid point and what is
the likelihood that it becomes a green
point and in this case likelihood is
simply defined as the number of red
points divided by the total number of
points in that neighborhood and the
number of green points divided by the
total number of points in the
neighborhood okay that's
likelihood that we are computing given
the details of the point okay so the
prior probabilities are computed without
any knowledge about the point and the
likelihood is computed after knowing
what the data point is okay which is
exactly a computer here likely to doubt
the point is Green is this and
likelihood of that point being red is
this okay so here actually in this
denominator it took all the green cases
and all the red cases not just the
neighborhood but idea is the same okay
did you now if I want to compute the
overall probability that something is
green and overall probability that
something is red but now we have two
notions right one intuition is telling
us something about distribution of green
and red points without any knowledge of
the individual data point which is a
prior problem P and there is another
intuition that came into picture which
is in the form of likelihood which is
capturing that you separate two are
distinct features of the given data
point now we have to combine these two
types of conclusions in order to come up
with our final probability of the new
data point being green and the new data
point being red okay and one simplest
way you still simply multiply these two
probabilities okay and which is what you
get to the posterior probability okay
posterior probability that X is green is
equal to the prior probability that X is
green okay and likelihood that X is
green first thing we computed already as
a oops 40 by 60 okay which is our 4 by 6
and
the likelihood that it is green is we
computed here one over 40 and we
multiply them it's 1 over 60 and
similarly the posterior probability that
X is red is essentially the
multiplication of the prior probability
that X is red and the likelihood that X
is red given the data point okay and
that's sis which is 1 1 over 20 now you
look at these two probabilities
obviously this is greater and therefore
you classify that point to be red okay
so now if you think about it you are
taking this classification problem and
modeling gift as probabilities and
specifically by computing the prior
probability which is the initial beliefs
that you have in terms of class
distribution without any knowledge about
the data point and then there is a
notion of likelihood which is the
knowledge that you are gaining specific
to that data point okay
and then you are mixing these two
information and coming up with the
posterior probability which is what you
are finally computing and making your
decision based on de ok that's the idea
of naive Bayes classifier so let's look
at the mathematical formulation okay you
know before we look at the formulation
of the classification problem actually
let's go to the classification problem
and come back to Bayes rule now three so
before I start if you don't follow
completely about the math it's
absolutely fine as long as you
understood the concept of prior
probability posterior probability and
how we came up with the final result
using those two concepts okay now let's
say we have a class label so let's say
we have the label attribute and it has K
distinct values okay
in the examples that we have seen so far
we have only two classes yes no spam no
spam fraudulent normal and so on right
so that means K is equal to 2 but in a
general case k may be more than 2 as
well okay so let's say my label has K
values which are c1 2 CK okay that means
given a new test record I want to
predict one of these values for that
particular record which is a class label
that we are predicting okay so so our
classification problem in terms of
probability is essentially formal
addresses given the values of all the
features of a given attribute given in
the card we want to predict what is the
probability that my class label is c1
and what is the probability that label
is say 2 and so on
okay in other words we want to predict
or we want to compute probability of my
class is equal to C I I can be 1 2 3
until K ok what is the probability that
what is the probability of C I given the
specific test record and specific
district has specific values for
individual attributes and those
attributes are this F 1 F 2 X n are the
features okay given the specific values
for individual features which are even
to VN given these values what is the
probability that my class is CI okay
that's way it's a conditional
probability given this what is the
probability of this okay so you are
essentially it's pretty straightforward
right give it a test regard you want to
predict what the class label is and the
way I am going to do that is I compute
the probability for every possible class
label and I choose the one with the
highest probability
right that means let's say given an
email I have I want to compute what is
the probability that it is a spam what
is the probability that it is a non spam
and they compute the probability and
accordingly I make the classification
okay that's straightforward but in terms
of probability it's called it's like
posterior probability right because you
are giving the value and then computing
the probability of the class okay that's
why this is a posterior probability now
we compute this posterior probability
using Bayes rule okay okay
so we understood what why posterior
probability is the actual classification
problem right now we want to compute
this left-hand side for every class and
the way to compute is something called
Bayes rule and let us look at what Bayes
rule is okay so let's look at this
equation probability that a given B can
be written as something like this okay
let's take a example probability that a
given B okay let's say a is raining okay
probability that it rains given a B is
cloudy okay so you want to compute what
is the probability that it rains
given that the bay is going to be cloudy
okay so that means tomorrow I will tell
you that okay tomorrow it is going to be
cloudy can you tell me what is the
probability that it will rain okay
that's exactly the problem of computing
probability of rain given cloudy
condition okay that's what we want to
compute and how can we compute how would
you compute let's say you have the
record of all
last one year data which has two values
whether it rained or not and whether
it's cloudy or not let's say I give you
the data okay and I ask you this
question tomorrow it is going to be
cloudy can you tell me whether it will
or what is the probability that it will
bring okay and how would I compute that
essentially I look at all the places I
mean all the days in which it is cloudy
okay let's say out of 365 200 or cloudy
days let's say I'm in Seattle and every
every day it is flowing and out of all
these 200 days I will see how many of
them are actually raining right maybe
hundred of them will be then I compute
my probability to be hundred divided by
200 right pretty straightforward
everybody with me so far
yes so in terms of mathematical
formulation what what did I do
I looked at all if you look at the
denominator I looked at all the days in
which it is windy cloudy okay and in the
numerator I took all the days in which
it is raining right that means raining
and cloudy right because we looked at
the rainy days out of those 200 that
means I am looking at rainy and cloudy
okay that's exactly the conclusion that
is captured here so probability of rain
given cloudy is equal to probability of
rain and cloudy divided by probability
of cloudy okay that's exactly what Bayes
rule is it's very simple it's just
captures the intuition that we have
already but in a nice mathematical
manner but this is done back in 1700 so
you should give him
some credibility killer okay actually
just interesting anecdotal story here is
that bass actually came up with this
equation trying to prove whether God
exists or not for whatever reason I
don't know what a and B in his in his
case but he was trying to prove or
disprove that God exists and he came up
with this Bayes rule in the context of
that problem so it's interesting to
another source so okay this is Bayes
rule now if I just turn around a and B
and say B of B given K which is again
very simple P of a and B divided by P of
a okay so these are the same intuition
but the condition is different so here
you are telling given that tomorrow will
reign what is the probability that it
will be cloudy
that's this question and this question
is tomorrow it is going to be cloudy
what is the probability that it is
raining okay sighs Eva I have no idea
what the conclusion was about his
observation of God but that was the
problem was using it now if I rearrange
these equations I can say probability of
a and B is equal to this basically
multiplied these two right that says and
if I rearranged further I can say
probability of a given B is equal to
this one right I'm simply replacing P of
a and B with this part of the equation
okay that's the final Bayes rule
now what Bayes rule says is if I want to
compute P of a given B that means P of
rain given cloudy I will compute this
way okay I put in all the equations and
I compute my this thing and this is
exactly called posterior
we are actually if we go here I simply
apply that equation this equation here
okay my a is essentially C equal to C I
and B is equal to the actual record okay
and here P of a which is probability of
C equal to C I right that's my a and B
this part is probability that B given a
okay so that means probability that the
record given class and then divided by
probability of B which is probability of
for the record okay now if you look at
individual components of this equation
property of C equal to C a what does
that mean it does not have any
indication of about this data point it
simply says what is the probability that
class is equal to C 1 what is the
probability that class equals C 2 right
this is exactly our notion of prior
probability right what is the
probability that it is green what is the
probability that it is infinite without
knowing anything about the data point
and that's our prior probability and we
know it already how to compute that
right simply count the number of
instances in your training data with
class 1 number of instances in your
training data which class 2 and so on
right now if you look at this component
what does it say it's essentially says
given so I tell you that its class equal
to C 1 now what is the probability that
I will observe this data in class equal
to C 1 right that's exactly what this
means probability that I observe these
values for the individual features given
that the class is equal to C 1 and how
would I get them it's essentially you
look at all your data and get all the
records with class equal to CI and then
you observe what is a problem
the individual values of this right it's
like saying I look at all my windy place
okay
and then I looked at which one are
raining in those Wiggly days okay this
is a likelihood that's exactly what we
computed in that circle right so now you
can see that posterior probability is
modeled using prior probability and the
likelihood simply by using the Bayes
rule and the denominator which is a
probability that this point occurs which
is kind of common for all your C equals
C 1 C equal to C 2 C equals C 3 if you
write all the equations the denominator
is common right so you don't really care
about it you just compute the numerator
and then you can compare all the values
based on the numerator so typically you
don't compute this one okay everybody
you follow so far so the likelihood here
how can we compute this like level okay
what is likelihood given that class is
equal to C I we want to compute what is
the probability that I observe this data
okay and how can i compute the
probability that I observe this data I
have basically able to look at all my
data in which C is equal to C I and then
I want I have to compute something like
a joint probability that value is V 1
value is V 2 and value is V n and so on
okay that's like a joint probability and
we make a simplistic assumption and we
transform this equation into something
like this okay we kind of divided that
entire probability into individual
probabilities so here we are saying
given C equal to C a what is the
probability that my first feature value
is V 1 times given my C equal to C I
what is the probability that second
value is V 2
okay so this is slightly different from
here here what I am saying is given C
equal to C I what is the probability
that my first value is V one and second
value is V two and third value is V 3
and so on whereas here I am saying I
don't care about all the values I just
care about first value being V 1 what is
the probability and here probability
that second value is we do I don't care
about all those and Here I am saying the
int value is V n and I don't care about
all the others okay so we make a
simplistic assumption that this is equal
to this and we do that by because it's
very easy to compute this from your data
right I just need to take all the data
points in which C is equal to C I and
then count how many times V one occurs
and then count how many times V 2 occurs
and so on ok and this is called
conditional independence given the
condition is equal to C equal to C I we
are assuming all of these values are
independent of each other and that's why
we multiply so you don't need to
understand conditional independence but
we make the simplistic assumptions and
and that's way it's called nave model
and that's why it's called nave Bayes
classifier okay and as you can see every
entry in the numerator can be computed
simply by accounting ok so let's go to
another example which is again a toy
example it's not only a real world
example but help you understand and we
can go through Q since we already
understood what this is so here the
problem is you have a thousand pieces of
fruits or thousand fruits and they
belong from the categories of banana
orange and some other fruit
okay and the three characteristics that
you want that you record for each fruit
are its length its taste and its color
okay so whether it's long or not long
whether it's sweet or not sweet
whether it's yellow or not yellow okay
and you have a bunch of data that you
observe from these thousand pieces and
you construct a table like this okay
from so here I have three classes right
banana orange and other fruit so my K is
equal to three in my mathematical
formulation okay so C 1 C 2 C 3 and for
each of the class we compute how many of
them are there and you can come do this
counting and construct our table okay
now first thing we want to do is prior
probabilities right how can i compute
prior probability simply count how many
bananas are there how many yellows
oranges are there and how many other
fruits are there right which are
essentially given by here 500 divided by
thousand three hundred divided by
thousand two hundred divided by thousand
those are my prior probabilities that
something is banana or in another fruit
okay which is exactly this okay and
let's compute the likelihood and what is
the likelihood in our case let's look at
the equation again okay this part is
likelihood right and what is this given
that it is a banana what is the
probability that my first feature is
equal to v1 and the first feature is a
length right long or not long so we want
to compute what is the probability that
the fruit is long given that it is
banana what is the probability that
front is orange sorry
what is the probability that fruit is
long given it is an orange and the long
given other fruit okay so these are the
three values that you compute for b1 and
similarly you will compute probability
that not long given banana probability
that not long given given orange and
give another fruit and so on
okay so you compute the probabilities
that individual feature values given the
class level okay that's what you compute
here list of all possible likelihood
functions okay now if we are given a
data point which tells you that okay it
is long it is sweet and it is yellow
okay now you want to predict the class
label for that particular fruit okay
in terms of probability you want to
compute probability that it is banana
given probability that it is banana
given F 1 is equal to 1 F 2 is equal to
sweet X 3 is equal to yellow okay and
similarly probability that C is equal to
orange or C 2 given F 1 is equal to long
f2 equals sweet and F 3 equal to yellow
and so on okay and by applying that
Bayes rule we exactly compute this part
probability the banana f 1 equal to long
F 2 equal to C naught sweet and F 3
equal to yellow as this right this is a
prior probability that it is a banana
and probability that long given banana
probability that sweet given banana
probability that yellow given banana
okay we do not need to compute
probability of evidence because it's
common for all the classes so we compute
this numerator it comes to that it comes
to this point to phi2 okay and similarly
you can compute probability of orange
given F 1 equal to long after equal to
sweet of 3 equal to yellow and similarly
are the true
and you get zero and point zero one
eight seven five okay and you can simply
see that this one is much larger than
all the other two and therefore you
declare this particular guy to be your
banana okay that's as simple as that
okay so let's look at a little hard okay
so let's go to our so for doing nay base
there is a package called e10 7-month
okay don't ask me why that name I don't
know why the authors gave that name
maybe there is a good reason if it's
used in many places I cannot give one
specific example where naive ace is used
but it's pretty commonly used wherever
the classification problem is there like
a decision tree or something you can
also apply a new base okay so if you are
convinced about that I use a random
decision tree then in the same example
you can put in a base now in this
function package e 1 0 7 1 I load it I
it's already installed so I simply load
it and there is a function called nave
base ok and it looks pretty similar to
what we have seen before you give a
formula you give a data and it builds a
Navy base models okay so NB is equal to
navies of my formula is again similar to
before class variable is a function of
all the other attributes and my data is
equal to B
or the training set if I split it into
training in testing and the tip I have
my naive Bayes classifier and I can look
at the structure of my Bayes classifier
as well and it tells you what are the
prior probabilities what are the list of
tables that it views and so on and the
final labels which are yes and no okay
and yeah these are basically individual
histograms within each of the attribute
but you don't really all need all this
you simply call that it use that model
in calling predict right
you can call predict and then your naive
based model and your test set since we
haven't divided into training tests I am
using same B as my test set and then
this we have seen only class is equal to
type is equal to plus okay so I have a
bunch of predictions using this new base
model does this work with both numeric
and factor variables well factor
variables are already there there are
categorical attributes but will this
work with the numeric attributes yes it
can work with numerical attribute but I
need to discretize my data so I need to
discretize my data in order to make it
categorical attribute just like we did
it for decision trees right so you do
the same thing here also and you
composite it okay so last time we talked
about this two things right this is too
long yeah so if I say class is equal
type is equal to class it is going to
tell me whether it's a yes or a No
or rather class 1 or class 2 whether
it's patient has a risk of diabetes or
not right and there's something called
prob or depending on the algorithm if
you do that then it will give you the
probability instead of simply predicting
yes or no it will tell you the
probability right so what did we say in
our classification model also we compute
this probability for every class right
or rather if you go here we compute the
probability that it is banana
probability that it is our it's
probability that it is as a fruit okay
and then we choose the maximum among
that so in this debit is case we had two
classes so we probably compute
probability that it is no probability
that it is yes and we simply choose the
maximum value and that to be yes here
and similarly for the second record we
do the same thing okay and this one is
more than this one and therefore you
chose it to be no and so on
okay now okay and depending on the
algorithm you other way to look at this
probability 2s no is essentially by
putting the threshold okay let's say I
say that okay if the probability is
greater than 0.5
I say class one if the probability is
less than point five okay I say class
two so that's another way of taking
probabilities and converting them into
individual decisions okay whether the
now base we simply take the maximum and
convert them into yes and no and that's
our classification okay so let's go to
ROC curve
okay so this is a confusion matrix that
we have seen already on one side you
have the predicted values or the test
outcome and on the other side you have
the real value which is a gold standard
okay from the testing data and we
already saw what is it propose to you
what is a true negative and so on and
using these values you can also compute
these different statistics accuracy
precision specificity sensitivity and so
on okay this we have already seen
now R was the color tells you
essentially what
where does your classifier stand when
compared to a random classifier
okay and the way you typically draw this
curve is by computing two things one is
false positive rate FPR and the other
one is true positive rate okay and the
true positive rate we have seen already
in the last class it's also called
recall which is the number of true
positives given by having divided by
total positives okay this is just the
equation that we computed last year last
week right that's a true positive and
that's on the y-axis and on the x-axis
you have false positive rate okay
it simply says how many false positives
are in out of all the negative records
okay so these numbers you can simply
compute from this table and you can get
a value for false positive rate and a
value for true positive rate and that
should give you a point in this entire
ROC space okay so here ROC space you
have values from 0 to 1 so false
positive rate and you have values 0 to 1
for true positive rate ok given this
entire space
where would you like your classifier to
be what is the best point that we want
to achieve okay can anybody kind of
think about that I have false positive
rate here I have true positive rate here
and what is the best point that I that I
have to aim for this point as it already
says here perfect classification that
means we want zero false positives that
means we don't want to make any
misprediction at the same time we want
to make full recall recall of one right
that means we want to predict all the
positive examples that are there in the
test set and that's my perfect
classification okay so this is the best
point a classifier can achieve now but
for a given classifier you will a false
positive rate and negative a true
positive rate might fall anywhere here
right a classifier can be here at Point
C at a it'd be our X C okay now out of
all this how do you choose which one is
the best okay the way to figure that out
is by simply looking at how does the
random guess works okay I won't do any
computation I simply tell you randomly
whether it's class 1 or class 2 okay
and even with that simple classifier I
can leave 60 percent accuracy assuming
the class labels are kind of equally
likely okay and that random guess is
shown as this red color red line okay if
I am doing below that line that means I
have a worse true positive rate and the
worst false positive rate than this
random guess then obviously it's a
hopeless class for it right so which is
this so C is hopeless e'en because it's
getting more false positive rate and
more true positive rate than a
random guess okay
or rather less true positive it so
anyway
above this diagonal will be a good good
point to aim for okay
now out of a and C I will choose C
better than a depending on whether true
positive and the misclassification error
and so on okay so this is the ROC space
now there's something called as ROC
curve and that looks like this okay
which is essentially this is again the
random exact same x-axis y-axis and I
have a random point now for every
classification model I compute this
entire curve okay
ROC curve and for model a I compute that
model B I computer and the way I compute
that so we have seen already in the
previous slide that so ie run decision
tree algorithm right yesterday we saw a
debate it's data set we ran a decision
tree algorithm and we got the confusion
matrix and we computed true positives
false positives false negatives true
negatives and so on and using those
values I can compute money true positive
rate and I can compute my false positive
rate okay that gives me a single point
in this entire ROC space right it gives
me a single point because single
classifier now imagine the naive Bayes
classifier with the probabilities that
were given them okay now with an eye
Bayes classifier or any other classifier
like logistic regression I get a
probability that it belongs to class 1
and the probability that it belongs to
class 2 right now there is a mechanism
in which I can convert this
probabilities
into Sno decisions right ultimately I
want to give the user S or no decisions
as opposed to probability right so in a
base
what we did is if probability that class
1 is greater than probability that it
class true then I choose class 1 so I
simply chose a maximum but in other
cases you just have a probability that
it's a probability that it belongs to
class 1 ok now you can kind of make it
into a binary problem which is yes or no
simply by putting the threshold ok
that means whenever the probability or
when or my probability is greater than
80% I will say its class 1 otherwise is
a class 2 ok
and with that threshold of 80% I get
some classification I get some
predictions and with those predictions I
get true positives false positives and
all that and I get a a data point in my
ROC space ok
so for every threshold value
now instead of 80% if I say a is 70%
okay if I say anything greater than 70%
is positive anything less than 70% is
negative ok that gives me different set
of predictions and therefore different
set of true positives false positives
and all the contingency table ok are
there confusion matrix and then from
that I get a different values for T P R
and s P R and therefore a different
point in my ROC space ok so by changing
the threshold that you are using in
order to go from probabilities to s or
no decisions we are able to get
different points in the ROC space ok if
I simply plot all those points I get an
ROC curve which looks like this ok and
the the more area under the curve under
the taro sequel the better it is so when
will I have highest area when the curve
is like this right so that means I am
covering the entire area that means I am
kind of moving towards this lift and the
point which we already thought to be a
best classification model okay so that's
the some high-level description of the
ROC curve okay even if you don't
understand just remember that there is
something called ROC curve which will be
used to evaluate how good the model is
and whenever there is a need you can
always look it up and read more about it
okay that's it and and if you want more
details look at this particular paper
it's an excellent introduction to ROC
analysis how to measure the efficiency
of an algorithm recall and accuracy so
let's go to this one match
so recall is not equal to accuracy
recall is something different from
accuracy okay recall is basically this
one the true positives divided by the
true positives plus false negatives okay
that means there are let's say 100
positive values in your data set okay
this is a ground truth that 100
positives are there so my denominator
will be hundred and then the numerator
will be how many of them I actually
computed to be positive and that cell
recall an accuracy is basically total
number of correct predictions which
includes positives as well as negatives
so it's essentially true positives plus
true negatives divided by all the values
that's your accuracy whereas a recall
simply focuses on the positives not on
the negatives okay and whereas a
precision is essentially comparing with
the how many positives that you
retrieved it okay so let's say I score
70 of my records to be positive and only
50 you are correctly considered as
positives so 50 is my true positives and
20 is false positives that means even
though they are not positive I am
mistakenly telling them as positive okay
so my precision is the the percentage or
the ratio of records that are required
to be correct so it's a 50 divided by 70
that's precision okay so the key
difference between precision and the
recall is the denominator
in one case you are looking at the total
number of positives that you predicted
and in the other case in the case of
recall you are looking at the total
number of positives in the ground truth
okay so that's the
difference and and if you just sit and
think about it it should be clear what
the difference is actually but the key
is to identify the difference which is a
denominator so yes the question is is
there a difference between lift curve
and ROC curve they are kind of similar
but they are different okay and lift
curves are essentially used mostly in
the context of like a classic example
for lift curve that people do is for
campaign advertising right let's say I
have a product and I want to send a
postal advertisement to to my customers
okay let's say I have 10,000 customers
and the thing I can do is I can send
them a mail to each of my 10,000
customers and the only fraction of
people will respond to my advertisement
right only let's say at 1 percent of
people or 10 percent of the people will
respond to your advertisement which is
100 people under a thousand people in
10,000 right but you need enough budget
to send mails to all these 10,000 people
right you need to print and do whatever
right you need to send by post so
typically you don't want to send to all
the 10,000 people for budgetary
constraints or for any other purpose
right now let's say your budget allows
you to send only 500 5000 makes instead
of 10,000 okay now how would you choose
which 5,000 to use okay how do I select
my 5,000 customers out of these 10,000
customers
to whom I send this particular campaign
message okay and one simple way is to do
do a random selection okay our I can do
some some other data mining method or a
more intelligent method right let's say
if the campaign is about some product
related to young people then I choose
based on their age are based on their
demographics based on their gender and
so on right so I can come up with some
extra put some extra logic in order to
figure out who those 500 people to whom
I send that particular campaigning
campaign message okay so the lift curve
is essentially captures how how much
improvement your specialized method is
going to give compared to a random
selection of 5,000 people okay that's
the thing that our oseco cap I mean the
lift curve captures compared to a random
selection of people how much does our
gain okay so in a way it's similar to
ROC because they are in ROC you are
comparing with the cards classification
that relies on random guess and here you
are say you are relying they you're
comparing based on the values that are
given by random campaigning process so
it's kind of similar but at the same
time both are different and they are
used in a different context typically so
this to conclude so we essentially
looked at different classification
models decision trees random forests and
male based classifiers and we looked at
multiple methods in which we can kind of
gauge the performance of these
classification models right using true
positives false positives precision
accuracy recall f-measure and so on
and then we learned about
cross-validation as a technique to again
evaluate the quality of a model with a
much more robustness okay by changing
the training and testing sets and we
just now looked at ROC curves in order
to kind of compare different things so
that should give a complete kind of a
closure on our classification models and
in the next class we will see how to so
whatever we did so far using our are
kind of limited by a single machine
right and in the next class we will see
how can we scale our using extra
packages like Hadoop related packages
okay so we will see how our and Hadoop
can coexist together using these extra
scalable packages okay that would be the
topic of for next class
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>