<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AWS Interview Questions And Answers | AWS Solution Architect Interview Questions | Edureka | Coder Coacher - Coaching Coders</title><meta content="AWS Interview Questions And Answers | AWS Solution Architect Interview Questions | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AWS Interview Questions And Answers | AWS Solution Architect Interview Questions | Edureka</b></h2><h5 class="post__date">2017-07-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/d9siPwxzEVI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey guys this is humans from Radio Rica
today's session is going to be on AWS
interview questions right so this
session might be useful for people who
are looking out for profiles like aw a
solution architect or people who are
appearing for AWS shops administrators
all right so without wasting any more
time let's go ahead and discuss today's
agenda so we will be covering all these
topics today we'll start with some
general cloud computing questions and
then we move on to ec2 then we'll be
talking about Amazon storage VPC
database AWS or scale and load balancer
then we'll see cloud trail and route 53
after that we move on to SaaS sqs and
elastic beanstalk questions and towards
the end we'll be discussing opsworks
and kms right so guys these are the
topics that we'll be discussing today
any doubt many of these topics because
we just want to discuss the no worries
to expert level questions related to
this topic so if you are not clear with
these topics or if you think you are a
beginner still you might go and refer to
the previous recordings in the LMS and
don't go ahead these questions with that
we are going to discuss today all right
so guys this is our agenda for today are
we clear with it
all right I'm getting confirmation so
Parvati is clear so it's Jason all right
the back is clear as well okay I'm
getting quite a few confirmations so I
can see most of you are giving me a go
all right guys so let's start with
today's session then with the first
question now our first question says I
have some private servers on my premises
also I have distributed some of our
workload on the public cloud what is
this architecture called so basically
our workload has been divided between
the public cloud and the private cloud
now they're asking me what is this
architecture called it's a pretty basic
question guys but if you look at the
options they're quite confusing the
first option is a virtual private
network then we have private cloud which
is obviously not there
then we have a virtual private cloud
could be the option and then we have
hybrid cloud all right guys so what do
you think what do you think is the right
answer for this phone guys let's be more
interactive in this session because if
it's a two-way thing then it's going to
be interesting for you and for me as
well so let's make it as interactive as
possible and let's get the most out of
the session span right so as she says
it's either virtual private cloud or
hybrid cloud so are you sure let's
actually only one out of all the four so
give one answer okay I can see some of
you are saying the right answer some are
confused it's okay I shall clear your
doubts all right guys so the answer is
hybrid cloud now why a hybrid cloud
because okay so let's actually discuss
the first three options which are
actually not the right answer
so it is not a virtual private network
because a virtual private network is
something that you use to connect your
private cloud and your public right so
to connect between your private cloud in
the public cloud you actually have to
make a connection and that connection is
done using a virtual private network all
right then we have private cloud so
private cloud is something wherein you
have is your own servers on our own
premise right but in our case we have
public cloud and also it is obviously
not private cloud virtual private cloud
is
not the option is there because a voice
will private cloud is basically a
logical isolation kind of thing wherein
you isolate your instances from the rest
of the instances on your AWS
infrastructure and this logical
isolation cloud is called a virtual
private cloud and then you have hybrid
cloud which I think fits aptly by its
name as well within its mixture of your
public cloud and your private cloud
infrastructure right so let's see the
answer so the answer is hybrid cloud and
explanation is like this because we are
using both the public cloud and your
pron from Isis servers which is a
private cloud we call it an hybrid
architecture right and it says here that
if you wouldn't be better if your
private and public cloud were all on the
same network ID so basically when you
connect your public cloud and private
cloud together using Virtual Private
Network you basically are accessing one
network and you feel that all your
resources which are there on the public
cloud and the private cloud are actually
there in one network right so it seems
it's a virtual private hydro virtually
you feel that you are on the same
network but it's they're actually two
different resources or two different
locations from where you are accessing
your resources all right guys so guys
any questions regarding to the first
question that you have discussed
anything that you're not cleared it was
a very basic question but then we are
getting a very a lot of concepts here we
have a virtual private network concept
then we have the virtual private cloud
concept right so it can be confusing and
this is how they asked you in interviews
as well right so you have to be very
clear in your answer you have to be very
clear in your thoughts that what shall
be the right answer
all right so I can see that people are
giving me a go there all clear okay guys
so let's move on to the next question
then so our next question starts with
our section one which is easy to
questions so from here we'll be talking
all about AWS so let's start with the
question first
so we have a video transcoding
application and the videos are processed
according to a queue the processing of
the video isn't wrapped in one instance
it is resumed in another instance okay
good enough then currently there is a
huge backlog of videos which needs to be
processed but this you need to add more
instances but you need these instances
only until your backlog is reduced
alright so once your backlog is reduced
you don't need those many servers
so which pricing option should be the
efficient should be the most cost
efficient for this ok guys so first of
all when you have questions like this a
lot of things are added into way to make
it confusing so first of all the thing
is the first line reads that it's a
video transcoding application so it is
not relevant to your question right it
is not relevant to what is being asked
so you can discuss it out and then it
says the videos are processed according
to a queue again it's there confused you
don't the first thing that you should
look out into a question with you're
trying too many or trying to figure out
an answer is the important part what is
important in the question you should be
able to answer that so according to me
the thing that is important is that
there is a huge backlog of video so
there is a lot of pending work and this
pending work has to be produced right
and once it is reduced we will not be
needing those many servers so basically
we are increasing our number of servers
to actually reduce the number of
backlogs that we have and once you have
reduced that we have an application
wherein we don't need those many servers
anymore so we should get rid of them
right so now it will ask me which
pricing option should be efficient for
this setup now you have three kind of
pricing options you have on divine
pricing then you have spot pricing and
then you have reserved Christ right so
your spot pricing is basically used when
you want servers at the minimum cost so
basically what happens is why spot
pricing has an introduced is because of
this that you AWS has centers right it
has servers owns where it has
service now not all the time that the
servers are actually being used some of
the times they are idle right so in
times like this when the servers that
idle what the inner bill does is it
gives you a discount that since no
server is being used I shall give you a
discount if you want to use my service
now in this case you use pot pricing so
if you are going for sport pricing you
see these reduced rates from AWS
whenever the servers are idle and you
should bid a rate right so say example
servers are being offered at some
particular price and you say ok I want
these many servers but I can only afford
$10 so as long as these servers and be
allotted to me for $10 I shall use them
right so set your price at $10 and then
you use the service but the moment the
demand increases in that particular
server location the prices go up again
all right and if the price crosses $10
your server shall be shut down right you
will not be able to access that server
anymore right so this is hot spot
pricing is you basically built for the
minimum price and whenever the price go
up your server is taken from you right
then second type of pricing is called
reserved pricing wherein you reserve
your source for a particular amount of
time say of one year a term or a
three-year term right so it the
application for this could be when say I
have a company right and my company has
a website so my website is hosted on AWS
now my website is going to be there till
my company is there right so it makes
sense for me to actually deserve the
instances for like maximum term possible
because I have no plan to sell my
company and hence take down my website
right now
the reason people offer resorb instances
is because as compared to the on-demand
pricing the result pricing is actually
cheap right so if you reserve your
instances for a longer term you get
discounts from AWS right and then we
have on demand pricing where and we can
get as many servers as we want at the
time what we want as per your
requirement at whatever time you require
and the pricing for them our standard
a lot say they are high but they are
standard but they are more than resort
pricing and your sport pricing now our
question see is that we have to reduce
the backlog and once the backlog has
been reduced we'd have to get rid of the
service so obviously we will not be
using reserved instances because we
cannot say when our backlog will be
ending right
we cannot be using spot prices because
we want that backlog to be reduced as
soon as possible so what we'll do is
we'll be using on-demand instances or
on-demand pricing and using that we will
reduce the workload or we use the
backlog of the videos and once it's been
reduced we will reduce server size for
our instance right so the answer for
this should be on-demand instances and
if you read the explanation you should
be using an on-demand instance for the
same because the workload has to be
processed now meaning it is urgent
secondly you don't need them once your
backlog is cleared there for reserved
instance out of the picture and since
the work is urgent you cannot stop the
work on your instances because the spot
price spike right so therefore spot
pricing can also not be used and hence
we'll be using on demand instance all
right guys so any doubt in this question
anything that you are not clear with why
are we using one demand pricing okay so
since you guys are clear let's go on to
the next question
so our next question says we have a
distributed application that
periodically processes large amount of
data now like I said first we have to
identify what in the question is an
important information and what is not
right because some of the things that
you are mentioned the questions are
there to confuse you right so we have to
identify what is important so it says we
have a disability application that
periodically processes large volumes of
data and it is designed to recover
gracefully from an Amazon ec2 instance
failure that means if one Amazon region
goes down your traffic will be
redirected to our healthy region wherein
the servers are ready to take the
traffic right so it's a failover
and you are required to accomplish this
task in the most cost efficient way so I
think the most important lines in this
question is first of all the agenda is
to get it done in the least cost
possible right the most cost-efficient
way and then it says that the data is
processed periodically as in the data is
not being processed continuously but
periodically it could be a one-week term
or a three or four day term right so it
is not being used continuously so first
of all if we are not using it
continuously
I think reserved instances are out of
the picture because your servers will be
idle most of the times right same is the
case with on-demand instances as well so
we are left with spot instances and
dedicated instances and there you get it
instances or something wherein you get
the whole server so basically what
happens is in your cloud infrastructure
on every single server you have a lot of
clients using it right you have a lot of
clients using that particular server but
right that server has been internally
divided into your operating systems
right so you will never know that some
other person is also working on your
machine but this is how it is it is a
multi-tenant architecture but when we
have dedicated instances basically you
get a server for your own use right
there is no one else on that particular
server that you are using so I don't
think it is related to something what we
are asking the question and obviously as
you must have guessed that dedicated
instances and are cheap they are quite
expensive because you get the whole
server right so the most cost efficient
way and since the job is done
periodically I think it should we should
go for spot instances on-demand
instances one more reason is that it is
not acceptable for this particular use
cases because you'll have to actually
pay a lot right so I think the most
closely going option is between spot
instances and on-demand instances
because on-demand instances could be
launching them and could be shut down
anytime we going and since it's periodic
it makes sense you use on-demand
instances as well but guys if you look
at spot instances and on-demand
instances the cheaper option is with
spot instances so we should go for spot
instances
first and then on the Mannings let's see
the answer yes our anti-sea is its spot
instances why because since the work we
are dressing here is not continuous a
reserved instance shall be I'll at times
good and same goes with on-demand
instances and it does not make sense we
launched an on-demand instance whenever
work comes up since it expensive alright
so spotting sensitive will be the right
fit because of the low rates and no
long-term commitment bang bang guys so I
will be using spot instances for this
particular use case any doubt scales
anything that we are not sure off in
this question okay so Rohit is saying
why not use on demand instances because
we can launch them anytime we want right
through it but if you look at the
question the question says we have to do
it in the most cost efficient way and
obviously since spot pricing is an
option like that that it is meant to
give you a server that a very cheap
price and again the manual work is going
to be same in on-demand instances as
well you have to launch the servers
manually at that instant when you want
to process your data and in spot
instances as well you have to manually
launch your servers when you want to
process your data and both of them can
be ended at whatever time you want but
since spot instances is more cheaper we
will choose that option
yeah but then if this question or if the
statement was not there that we are not
bounded by costs then obviously we would
have gone with on-demand instances all
right is that here
okay guys any more doubts all right so
let's move on to the next question then
so our next question is how is stopping
and terminating an instance different
from each other
nice question it's actually pretty
confusing for beginners but I think you
guys should be enjoying this so let's
see what the answer is so stopping and
starting an instance is basically when
you stop an instance it is actually not
deleted it is there on the system but
you are not charged for a stopped
instance right but when you terminate an
instance what
happened this its first if this instance
is in a running state it first comes to
a stop state and then it gets deleted
right so stopping an instant and
terminating an instance basically
reference it when you stop an instance
you are not charged although but your
instance is not deleted so you might be
incurring some storage charges but when
you talk about terminating an instance
you're stopping the instance but you're
also deleting the instance at that
instant as well right now you are not
charged with any storage options is that
right so this is the difference between
stopping an instance and terminating
instance oh guys are we clear between
the two states stopping and terminating
okay so invasion is asking me so we are
not charged when we use stop instance No
so once you have I think what you want
to say is once you stop an instance you
are not charged right is that what
you're asking animation yeah so like for
example if you have an easy-to instance
and you have put it to stop you will not
be charged for the computing anymore
right so you might be charged for the
storage that your EB s is using but you
will not be charged for the computing or
the ec2 pricing path right but once you
terminate and sense
nothing is charged there is nothing on
the desk everything is deleted and
that's how you should choose when you
aren't trying to stop an instance all
right so if you want to delete the
instance you should go for terminate and
if you want just want to stop and maybe
resume it as a like-new your time you
shall stop the particular instance okay
guys so any more doubts on this question
okay so since most if you were clear
let's go ahead and move on to the next
question
so the next question says when will you
incur costs with an elastic IP address
and the options are when an IP e IP is
allocated when it is located and
associated with the running instance
when it is allocated and associated with
the stopped instance and costs are
incurred regardless of whether the e IP
is associated with the running instance
or not okay so the answer for this guys
is I think it's a tough one because I
most of you would not have used in
elastic IP address until now
so it is you're charged when you are
allocated an elastic IP address but you
have not detached it with your stopped
instance in the sense that you are not
using your instance as well but you are
attached in elastic IP address with the
stopped instance so in that case you are
also not using it and also the other
users on the Internet you know also not
use that particular IP address and that
is the reason I guess that Amazon
charges you so basically you are charged
for three scenes you're charged with in
three cases for an elastic IP so the
thing cases are when you use more than
one elastic IP with your instance when
your elastic IP is our showstopper
instance which we just discussed and
when elastic IP is not attached to any
instance now a viably being charged is
because of this because guys IP address
is a real commodity right P it is a
limited commodity it is not something
which is there in the countless terms I
will show a particular set of IP
addresses are there now as you must have
recalled an elastic IP address is
something which is static right so the
difference between an elastic IP address
and a normal IP address is this that say
you launch an instance and in Sadu
instance right and you are located an IP
address now that address will be
attached to your instance as long as
that instance is running but whenever
you restart your instance or when you
stop that instance and run it again that
IP address will change right to avoid
that we have elastic IP addresses now
what happens in the elastic IP address
is this that is your instance or is
actually physically attached to an IP
address and that IP address never
changes even when you stop that instance
you when your running their instance
that IP address is going to be the same
all right
so and the third scene is that the IP
address is not attached to any instance
all right so you allocate yourself an
elastic IP but you don't attach it to
any of your running instances so in that
case what happens is that IP address is
blocked with you but you're not using it
or
your own and also the people outside you
might want to use a static IP address
cannot use it as well right so it
becomes a problem and that is the reason
amazon has regularized this and they
charge you if you have IP addresses
which are not being used by you right
and so yeah so it means and they are not
being used by you right so there is any
questions on the slide any questions or
any doubts related this particular
question pretty straightforward right
okay so yeah so I can see everyone
understood it okay guys let's move on to
a next question so our next question
says how is the support instance
different from an on demand instance or
a reserved inches we have discussed it
earlier as well so let's see the answer
so like I said a spot instance is
basically used for bidding and on demand
instance is used when you want the
server then and there right and a
reserved instance is used when you're
looking at long term commitments in
terms of your application right so let's
read the explanation so first of all
let's understand the spot instance on
demand instance and his audience are all
models for pricing good moving along
spot instances provide the ability for
customers to purchase compute capacity
with no upfront commitments so you don't
commit anything you are not saying that
I am going to use it for like an ear or
one-and-a-half years or two years or
three years you just get that instance
for as long as that bidding price is
there right and we get it at alley rates
usually lower than on demand rate in
each region spot instances are just like
bearings so like I said you bid your
price and spot price fluctuates based on
supply and demand for instances so
whenever there is a demand for instances
your spot price increases automatically
but the customer will never pay more
than the maximum price that they specify
so whatever price that you have
specified for your particular server you
will not be charged even a penny more
than that right so if the spot price
goes beyond it your server will be
terminated
so if this part price moves higher than
a customer's maximum price the customers
ec2 instance will be shut down
automatically like I said but the
reverse is not true so yeah so when your
pisspot instance is same but your server
got terminated right but if the spot
price again comes down below $10 it is
not like your server will start running
again right so you have to do it again
manual you have to again put your
bidding price again you have to choose a
server and we launched right so and then
in spot an on-demand instance there is
no commitment for the duration for the
user side even around emergencies but
the references that the on-demand
instance is basically more expensive
than your on-demand instances and there
is no commitment for the deviation from
the user side however in reserved
instances one has to stick to the time
period that has es shortened so if you
take so if you say you will reserve your
instances for 1l these instances will be
yours for one yeah before that time you
shall not be able to come out of the
plan and that is why you have some
payment some upfront payments that you
have to do in case you're opting for
your reserved instances right guys so
any doubts on this question anything
they didn't understand we have discussed
it quite a few times now so I think it
should be clear to you okay so Jason is
giving me and go guys are those any
doubts and on this question okay so I
can see people are pretty confident okay
guys so that sounds awesome to me
all right guys so let's move on to the
next question now so the next question
says what kind of network performance
parameters can you expect my large
instances in a cluster placement group
alright so this question is actually a
very good question so when you launch
instances in a cluster placement group
what basically happens is you get three
benefits out of it so first of all you
get 10 GB per second of single flow data
you get 20 GB PS and multi flow that is
the data will be coming to and going
from you as well so the bandwidth is 20
Gbps
the network traffic outside the
placement group will be limited to five
Gbps
so whatever instances you have included
and the cluster placement rope will get
these speeds there are 10 Gbps and 20
Gbps and the instances which would be
outside this placement group will
experience speeds that would be limited
to 5 Gbps now these speeds are actually
speeds which are related to each other
it's not the internet speed task so say
there are two servers in a cluster
placement group so these two servers can
interact with each other let's speed up
10 Gbps right so this is what this
question is all about so in whatever
services or whatever instances that you
have launched in class separation group
and if they are related to each other
they can actually transfer to and fro at
a very high speed and this is what class
replacement group is all about all right
any questions that you have on this ok
cool so I think everyone is clear let's
go ahead with some point in the next
question all right so our next question
is to deploy a 4 node cluster of Hadoop
in AWS which instance type can be used
so the type instance which is used is
called EMR that is elastic MapReduce now
before I go on to why we are using it
let's understand what happens in a
Hadoop cluster
so the Hadoop cluster follows a master
slave concept the master machine
processes all the data slay machine
store the data and act as data nodes
right so your machine is actually
storing the data or the hope cluster
storing the data on the data nodes right
and the master server will be doing all
the process and this is how it works so
then obviously since the master is doing
all the processing the master has been
given a higher ram and a much better CPU
then your slave service right so you can
configure the configuration of the
machine depending on your workload so if
your workload is more if it requires
more processing you cannot actually
choose and instance like that so in this
case we shall choosing a/c 4.8 x-large
instance for the master machine whereas
for the slave machine we can choose an I
to - large instance and it should be
is optimized right and if you don't want
to deal with configuring your instance
and installing the loop class or
manually now all of this is done when
you are doing it manually like so our
balances that I told you was EMS that is
elastic MapReduce but if you want to do
everything manually that you want to
step clusters and everything you can do
it using ec2 as well in that case you'll
be actually deploying quite a few we see
two servers binding to do server will be
your master machine and the other is
room servers will become your slaves and
this is what the configuration is for
but if you don't want to deal with all
these hassles what you can do is you can
straightaway launch and Amazon EMR which
is elastic MapReduce which automatically
configures the server Cu and you dump
your data whatever data that has to be
processed you dump it on s3 and what EMR
will do is it will pick up the data from
whatever bucket that you have specified
process it and dump it back to whatever
bucket you have certified either in the
same bucket or some other packet as well
right so this is how you shall approach
and loop cluster alright guys so any
doubts and whatever we have discussed
are you clear with what loop actually
looks like and adjust right so the
purpose is not to understand how a loop
cluster the working of a loop cluster
but to see basically how a whole loop
architecture is and depending on that
deciding what instance to be used so if
you are using easy to you should have a
master machine I am some slave machines
and if you don't want to deal with all
these hassles you can actually go for
EMR which is elastic MapReduce and
elastic MapReduce to each and every
configuration for you right
so guys I'll be clear with this question
any doubts in this question anything
that isn't understand ok so I see a lot
of people are giving me a go okay guys
so let's go to 1x question now so a next
question says how do you choose an
availability zone now this is a very
important question guys this actually
tests your architects skills because as
an architect you are required to do a
specific task that
the features but at the minimum cost
pose right so you have to choose an
availability zone with the cost in mind
as well so let's see how shall we choose
an availability zone so we have taken an
example here so basically if you look at
the figure the region that we get to
choose between is Mumbai and North
Virginia right the instance type is M
4.4 X large there are 16 virtual CPUs
and a 64 gigabyte RAM and then the
pricing for Mumbai it's 691 dollars
monthly right and for not Virginia for 1
year it's 480 dollars monthly and the
latency from USA to India is low and
from India to USA it's hard right so
these are the parameters that we have
these are the things that we have to
analyze right so if you go and see the
explanation it says there's a company
which has user base in India right so
our user base is in India right so if
it's in India we shall be choosing a
region which if it's in India you would
say why not choose the region which is
more closer to you but as an architect
you have to actually look at a lot of
things and let's see how we'll go about
it right so with the references of a
figure the reasons to choose between a
Mumbai and North Virginia and if you
compare the early prices now the early
price if you look at the Mumbai is at
point nine dollars oddly and not
Virginia is point six titles are d right
so obviously the cost is less if we host
a website or not Virginia region but
cause not only the fact of the
performance should also be high so let's
look at the performance statistics so if
you look at the latency guys so latency
basically means the response time right
so the response time is something like
this that from USA to India the latency
is low right so that means that if you
are accessing from USA your website the
latency is low but from India to USA
that means if your website is hosted in
India and a person is USA is trying to
access your website the latency is high
all right so obviously
a person is in USM he's trying to access
your website obviously if the server is
in USA your latency would be lower right
but like what we are analyzing is
between India and USS if it's from USA
to India the latency is low but what we
have observed is that if it's from India
to USA that if the website is hosted in
India and the US a guy is trying to
access it the latency is slight high
right now if you look at both of things
so North Virginia is less in price as
well and is less in latency as well so
all obvious choice should be not
Virginia right so you should take
performance you should take pricing and
some logics as well when you are
considering or your in your choosing and
availability zone right so this is
actually something which is very trivial
for an organization because this lays
down bases and how your website is going
to perform manage on the club right so
this is about that time so guys any
doubts in this question anything that
you not clear with anything that you
didn't understand I shall beat it again
for you
okay so a cache is asking me can you
repeat latency again okay so cache
latency is basically the response time
so what is once I mean this say I try to
access your website right and when I
type in your name everything gets
processed in the back right the name of
your website is first converted to an IP
address then IP address is tried to we
ping the IP address and then that server
responds us with a page that map it goes
on a browser and everything right so
this is all the process now when I try
to access your website when your website
is being hosted in say India and I'm in
u.s. say it takes on point seven seconds
for this request to be sold right but if
your website is hosted in the US and I
am trying to access it from the US the
response time is say around point zero
three seconds right so the latency
becomes low right it was 0.7 seconds 0.6
seconds and you are trying it from a
different scenario now it is less so the
lower the latency the better it is the
better that is so lower the response
same as in the faster the reverb side
reaches you the
more profit it is for you and that is
what latency is all about is it clear
cache now so did that answer your
question in cash
okay so a consciousness that guys any
more question anything that you want to
ask in any other question that we've
discussed so far I shall repeat it again
anything that is not clear to you okay
so let's move forward okay so our next
question says if one static IP address
is it enough for every instance that you
have running on your AWS infrastructure
so it depends every instance comes with
its own private and public address the
private address is associated
exclusively with the instance and it is
returned to Amazon easy to only when it
is top codominant it right so only when
it will be stopped or terminated your IP
address will change but if you don't
stop your instance your instance is
being executed from the time it has been
launched your IP address will not change
right so this about that so if you don't
plan on stopping our instance I think
opting for an elastic IP address should
not be in your checklist right however
this can be replaced by elastic IP which
stays in the instance as long as the
user doesn't matter you detach it right
so if your instance demands that it
should be stopped in your future and be
launched again at a particular time and
you don't want your IP address to be
changed you shall go ahead and opt for
an elastic IP and sometimes it happens
guys that some fault in that
availability might happen in your server
might crash or something like that in my
crease start in that case your IP
address will change so to be on the
safer side even if you are hosting a web
site which is there 24/7 it is better if
you opt for a static IP address right
now again if you have a lot of instances
and you are hosting multiple web sites
on your one into the server in that case
you might acquire more than one elastic
IP address because each website and like
I said should have a static IP address
associated with it so with that we have
answer this question guys so guys
anything that you're not clear there
okay so anything that you're not clear
with guys anything I shall repeat it
again
Quesada was asking me what if we have
one website on three or four instances
right sort of so if you have one website
deployed on four or five instances to
handle the traffic in that case one last
thing I address is enough for this
particular use case yeah okay guys so
yeah
any more doubts guys all right so if you
are clear let's move on to the next
question so the next question says what
are the best practices for security in
Amazon ec2 right now there are ways that
you practice that you have actually
learned from somewhere and the next way
is the best way to do it right now
Amazon has actually given out some best
practices that you should do when you
are actually using this services so if
you follow these best practices these
are actually very efficient and Amazon
suggests you that it should be followed
so let's look at the best practices guys
so one should use I am to control access
to your AWS resources so I am is
basically a tool where in you get grant
access to your users - for your AWS
resources in the sense that if you are a
company and you have one either press
account with you and some of your
employees want to actually view what
instances you have launched it is not
advisable to give your account
information to your employees what you
can do is you can actually assign a role
for that particular user as in say that
user has to be given only the view
rights so he can only view your AWS -
body cannot change anything or you
cannot launch a new instance or delete
one right so like it says you should use
I am to control access to your era
versus versus you should restrict access
by only allowing trusted host and
network to access ports on your instance
right and then you should review the
rules in your security groups regularly
you should review the inbound IP
addresses the out by nappy addresses so
that there is no malicious server that
is trying to interact with you right and
only open up permissions that you
require you should not play around with
the permissions or
particular server or instance because if
you don't know anything you might cause
a permanent damage to a server when I
say permanent damage I mean it might be
a damage for your business right and
it's a the disable password based login
for instances password can be found or
cracked right yes if you log in using
your passwords those passwords can be
cracked using brute force or may be
guessing or maybe someone might see you
typing that particular password and your
account might get compromised if so you
should not use password based logins you
should basically die using your public
key private key setup where in that
public key is uploaded match against
your private key and then you get the
authentication that's all right so when
to try using that instead of password
based logins all right guys so these are
the best practices for using easy - any
doubts in whatever we have discussed
anything that you didn't understand here
ok so let's move forward then let's move
on to the next question so we are done
with these two questions guys we covered
around 10 questions regarding easy to do
anything that you might want to add here
or might want to ask that they didn't
understand before and more asked now
regarding easy - okay so you guys are
giving me a go alright guys so let's
begin with the next section which talks
about Amazon storage alright so let's
look at our first question you need to
configure an Amazon s3 bucket to serve
static assets for your public-facing web
application which method will ensure
that all objects uploaded to the bucket
are set to public read alright guys so
let's look at the options here so the
first option is set permissions on the
object to public read during upload
alright so this is not something that I
will suggest you because every time you
wish a log load an object you shall set
its permission to publicly which is not
possible alright so it is not something
which is robust right the second option
is configure the bucket policy to set
all objects to public
this is something that we can do but
let's explore the other options as well
use I am roles to set the bucket to
publicly not something that I recommend
and then Amazon s3 objects default
public read so no action as me that this
is not something which is that alright
so here we will say that we should
consider the bucket policy to set all
objects to public read/write so rather
than making it change like I said it is
not possible to make change to every
object right so rather than making
changes to every object it is better to
set the policy of the whole bucket right
I am is used to give more granular
permissions since this is the website
all objects would be public by default
alright so like the question says it's a
static website right so the default
permission for the objects it will be
public by default and if you want it to
be only public read you have to manually
set it in the bucket policy once and
whenever more objects are added to the
bucket then all the objects shall have
that property of public read alright so
guys I'll be clear with this question
anything that you're not clear with I
shall repeat it for you anything that
you're not here ok so people are saying
clear ok so let's move ahead alright so
the questions is a customer wants to
leverage s3 and Amazon gracious part of
the backup and archive infrastructure
the customer class use third-party
software suppose this integration which
approach will limit the access of the
third-party software to only Amazon s3
bucket named company backup alright so
let's look at the options our custom
bucket policy limited to Amazon s3 API
in three Amazon glaciers archive company
backup no custom and bucket policy
limited to Amazon s3 API and company
backup okay a custom I am user policy
limited to the Amazon s3 API for the
Amazon glacier
archive company backup and the D option
has a custom I am user policy limited to
the Amazon s3 API and company backup now
as you can see here guys we are trying
to
a very granular permission for the use
of s3 bucket coil company back up for
our software right so what we can do is
we shall go with I think option D we're
in a custom I am user policy will be
limited to the Amazon SV API and the
company backup option right to see is
not the answers in a trip via will not
be accessing Amazon Keysha accompanied
by the policies bucket policies will not
be used what will be used is an iamb
policy since we are going ahead for a
third-party software which is trying to
access the company backup pocket right
so we shall be using an iamb policy for
this particular thing so like the
explanation says thank you for the
previous questions this use case in was
granular permissions and hence I am
would be used here all right make sense
next question a customer implemented AWS
Storage gate paired with the Gateway
cased volume at the main office right so
the Gateway has a volume at the main
office and Evan takes the link between
the main and branch office offline so
the link gets broken because of some
reason which methods will enable the
branch office to access their data all
right so let's look at the options there
guys so the option is is is to by
implementing a lifecycle policy on the
Amazon s3 bucket okay B option is make
an Amazon glacier restore API calls load
the files into another Amazon s3 bucket
within four to six hours now 46 hours is
a lot of time launched new a doubler
Storage Gateway instance ami and Amazon
ec2 and we stole from a gateway snapshot
this seems feasible create an amazon EBS
volume from a gateway snapshot and mount
it to an Amazon ec2 instance now
what you should take from this questions
forget about the options aid what you
should take from this question is that
it's we're talking about a business here
right and the link has gone down now
rather than troubleshooting the problem
or trying to understand what the problem
here is you should fear we go ahead and
launch new connection right so I think
we should go with our option see that is
launched new AWS Storage Gateway
instance am I in Amazon ec2 and restore
from on caifa snapshot and that is what
it is here because well like I said time
is of the essence in a business right
and I think when time is of the essence
you won't want your link to be up in the
next four or six hours you wanted them
in there right and that is the reason
you should actually go ahead and launch
new connection so it's a launch a new
way to bless or at Gateway instance MI
and I understand you so from the gateway
snapshot so you get it might have dumped
a snapshot its recent configuration so
you can take that snapshot and be
restored in a couple of minutes say like
maximum by a half an hour it is so one
option says 3 4 is excels that is
actually not viable alright moving that
guys let's move on to the next question
when you need to move data over long
distances using the internet for
instance across countries or continents
your Amazon s3 bucket which method or
service should you use all right so the
options are Amazon glaciers so know
obviously Amazon cliches and archiving
service would not be using it Amazon
CloudFront it's a content delivery
network we can actually use it then you
have Amazon transfer acceleration it
basically accelerates your transfer
using CDN which can be used and then we
have Amazon snowball which is basically
used when you are transferring data over
long distances right when you have huge
amount of data and you want it to be
moved you actually use Amazon snowball
but the thing with the Amazon snowball
is guys that is only limited to the US
right now and over here we talk about
transferring data across
countenance it's a snowball is out of
the picture Amazon glacier is out of the
picture and transfer acceleration is
actually using cloud front for
transferring data it's a bit transfer
acceleration what happens is your data
is case to a nearby location and from
that location you receive your data so
that data comes at a very high speed
rate than your normal traditional
transfer right so you shall go ahead and
do it will transfer acceleration let's
see well answer it so yes it is transfer
acceleration let's see what the
explanation says you will not use
snowball because some of the snowball
service is not support crosses and data
transfer and since you're running across
countries no one's going to be used true
then transfer accelerations I'd be right
choice here as it shuttles the data
transfer with the use of optimized
network paths which is CDN and Amazon
Canada with me exactly up to 300 person
compared to normal data transfer speed
so a data transfer speed is increased up
good 300% as compared to when you are
transferring it traditionally right so
transfer acceleration should be the
choice for you
all right so guys are we clear with this
question it's pretty straightforward the
options you could actually make out from
the options what the correct answer is
all right
so okay so let's go ahead then so the
next question is how can you speed up
data transfer in snowball okay so let's
see the real answer can be increased by
performing multiple copy operations at
one time that is if the location is
powerful enough you can initiate
multiple CP commands each from different
terminals all right so if your
workstation has a lot of processing
power you can actually exploit that
power by launching multiple terminals
air workstation and starting the CP
command for different chunks of your
files so then data will actually get
transferred Palelei which will actually
help copying things faster right
coughing from multiple workstation to
the same symbol so again you are copying
everything parallel and hence it
increases the threshold or the bandwidth
of the data coming into snowball
answer large files over creating a batch
of small files this will reduce the
encryption overhead right so you can
transfer large files or you can create a
batch of group of small files which is
again a LAN for large file so that the
encryption overhead produces because
each file is encrypted when it is
transferred to the snowball and
eliminating unnecessary hops that is
basically what it's saying is that you
should not have an intermediate system
in between it is better if you
transferred directly from your main
server to this snowball so with that the
transfer speed will increase and hence
your data transfer and it happened more
faster right so this is how you can
speed up data transfer and snow are
pretty straightforward I know it's it
might be clear you any doubts anyways
any doubts and what in what we have
discussed in this question okay let's
move ahead so we are done with the
Amazon storage section guys are we're
done with the questions let's go ahead
and discuss the virtual private cloud
now now this is guys a very important
section because most of the questions
will be asked from this section so let's
actually focus more on this section now
so let's see the first question so it
says launched an ec2 instance and assign
each instance a predetermined private IP
address what should you do so you should
launch instance from a private Amazon
EMI assign a group of sequential elastic
IP address for instance launch instance
in a V PC or launch instance in a place
to grow so I told you guys that
placement group is basically when your
monitor increased the transfer speed
between two particular instances or two
particular servers right so and what we
want to do is if we want to assign each
instance a predetermined private IP
address so placement group will not help
assign a group of sequential elastic
ITRs to the instances we can do this but
if you cost us because we are taking in
a lot of classic IP addresses launch the
instance from a private AMI
now elastic IP address guys it could be
done when we were dealing with public IP
addresses but
what we are dealing with is private IP
address that is the IP addresses which
are there within your organization so
obviously classic IP addresses again
cannot be used so D and B option is out
of the picture then launch instance from
a private EMI so this is irrelevant it
does not make sense so the C option will
say launch the instance and then Amazon
VP C is the correct option because the
best way of connecting to your cloud
resources from your own data center is
the VP C once you connect to data center
to the VP scene widget into the
president each and sign a private IP
address which can be accepted as a
sensor and we can actually specify which
instance will have what IP address
because it's a private IP address right
so you can have full control of it
having said that guys any doubt and this
question anything that you are not clear
with we shall discuss it okay so you
guys are giving me kill okay guys so
let's go ahead okay so is it possible to
change the private IP addresses of an
easy-to while is running or stopped and
a VP C so is it possible to change IP
address of an instance if it's running
or if it's stopped right so let's see
what is the answer for this so primary
private IP addresses this - then since
you are its lifetime and cannot be
changed right
but secondary private upgrade races can
we unassigned assign the move between
interfaces or instances at any point all
right so you cannot change the primary
private IP address for your particular
instance but you can go ahead and change
the secondary private address for your
particular instance if even if it is
running or stop or whatever right
obviously not if it's terminated so the
true state switch comes there is running
a stop it so if your instance is running
you can change the secondary IP address
but in none of the cases you can change
the private IP address because it is -
to the system as long as the system is
running all right pretty straightforward
let's move on to the next question so
the next question is why do you make
subnets
so let's look at the options because
there is
orders of networks to efficiently
utilize networks that have a large
number of hosts because there is
shortage of force and to efficiently
utilize networks that have small number
of forts alright so sub let's let's
understand well some metal sub bands are
basically sub networks that you create
in a network that you create in a
network to increase the accessibility or
managing number of hosts now the reason
you do it is because there are a lot of
hosts in a company and there are a lot
of computers now to make managing these
computers easier what you can do is you
can subdivide the network right and
which each subdivision the number of
hosts gets divided and hence it becomes
easier for you to handle the number of
force right and that is the reason it is
used so if you look at the explanation
says if there is a network which has a
large number of hosts managing all these
hosts can be a tedious job
therefore we divide this network into
subnets or sub networks so that managing
these hosts becomes simpler right so you
make subnets to actually to make your
job easier of managing a large number of
hosts all right moving ahead guys let's
move on to the next question which is
which of the following is true ok this
can be tricky you can attach multiple
route tables to a subnet you can attach
multiple subnets to a route table both a
and B and none of these ok guys so what
do you think okay so I've got mixed
responses from you guys so some of you
are saying you can - multiple out tables
to assemble it some of you are saying
you can attach multiple subnets to our
table none of you are saying both a and
B and none of these is well-known for
you're saying ok so I can see there are
5050 between a and B ok so let's look at
what is the right answer the right
answer is you can attach multiple
subnets to without it so basically why
we will not attach multiple out tables
were some method because a route table
is basically used to transfer to route
your packets to a particular destination
right so if you have multiple route
tables in your network it will lead to
confusion as to where the packet has to
go because each route table have will
have then a different location and if it
has the same location there is no point
of having multiple router tables right
so a packet can go only one place and
that is why you have only one route
table but you can have a lot of subnets
which are attached work route table to
ease out the routing process all right
so you can attach multiple subnets were
out Abel is the correct option in this
particular question all right moving
ahead guys so in cloud front what
happens when content is not present at
head location and a request is made to
it
so cloud friend is the content living
network that I told you guys about so
what happens when the content is not
present at net locations of an edge
location is basically a place which is
nearer to the user right so the question
says if the guy is trying to access a
website which has cloud front enabled
right but the education which is near to
that user is not having that particular
website what will happen in that case
and the options are an error 4:04 not
found this returned cloud front delivers
the content directly from the original
server and stores it in the case of the
education the request is kept on hold
till content is delivered to the edge
location and the request is routed to
the next closest edge location right so
let's see what's the correct answer all
right so in this case what happens is
you're the first request which happens
like this is actually served using the
origin server but meanwhile your request
is being served from Puritan server the
website is kissed onto the edge location
as well the new edge location which has
not been used as a fail otherwise the
web site would happen case here as by
now
right but could be a case wherein you
have newly applied cloud Frontier
website and it is not them at much of
the cottage locations so when a user
tries to access it from a new location
and if the edge location near to him
does not have those many servers what
happens is your website is so using the
origin server so
you might experience some more latency
right the latency could be more but
meanwhile we've I will be soft that
website that website will be cased in
the background to the ED login so
whenever you'll be trying to access that
website from the next time latency will
actually be less because now the website
will be there in your education alright
so this is what happens moving ahead if
my awl Direct Connect fails will I lose
my connectivity so Direct Connect is
basically a way that it can actually the
AWS infrastructure so it is basically a
leased line that you have between your
infrastructure and AWS infrastructure so
let's say what the explanations is so if
a backup aw direct direct connect has
been configured in the event of a
failure it will switch over to the
second one that is the failover right
and it is recommended to enable by
function forwarding detection with when
configuring your connection sort to
enjoy easier faster detection and
failover so if you enable by that
excellent forwarded direction what will
happen is that if there is a failure
which is going to happen it will be
detected more faster and in that case
you can take preventive measures right
on the other end if you have configured
a backup IPSec VPN connection all the
POC traffic will failover to the backup
we pin connection automatically right so
if you have taken the measures it will
go on to the feed over but if you do not
have a double direct connect failover
like installed in your in network or in
your infrastructure your VPC traffic
will drop in the event of a failure so
your EPC the traffic will actually be
dropped you will no longer be able to
access the infrastructure that you were
connected to in case it fails and you
have not planned anything over all right
so this is what happens ok so yeah next
question is so yeah so we're done with V
PC guys any questions that you feel but
not clear and V PC you can ask me now I
shall clear at you right now anything
that you're not clear with ok so since
most of you
you are giving me a go let's move on to
the next section which is Amazon
database all right so our first question
says if I launch a standby audience
instance will it be in the same
availability zone as my primary and the
options are only for oracle RDS types
yes only if it is configured at launch
on No
so if you launch a standby RDS instance
will it be in the same availability zone
so I think it's a no because what's the
point of having a same standby RDS
instance if it's the same a root alone
and it agrees with me so since the
purpose of having a standby instance is
to avoid an infrastructure failure if it
happens then the standby instance is
stored in a different availability zone
which is physically different
independent infrastructure right so it
makes sense because if you are standby
instance is in the same availability
zone as your RDS as a primary RDS
instance and that particular will
abilities on fails your standby as well
as your primary instance will go down
and you don't want that right and that
is the reason you have a standby
instance so you have a standby instance
whenever your primary device is down you
will fail over to the standby innocence
so if there is a natural calamity in
that particular region or the power
outage in that particular region your
primary instance should fail and to have
a standby instance in some other
availability zone so that whenever one
availability zone goes down your traffic
will be redirected to the other areas on
various standby RDS and sensors actually
being launched all right so this was
about that guy it's pretty
straightforward
let's move on to the next question so
when would I prefer provisioned IPS IOPS
over standard Aria storage so provision
IOPS basically offers you higher
input/output States right so when do we
prefer provision RPS so the Ops guys you
have a batch oriented workload if you
use production online transaction
processing will TP workloads if you have
workloads that are not sensitive to
consistent performance or all of the
above
all right so basically you would use
provision IOPS when you want higher
rates of input outputs whenever you are
storing something and I think when you
have a batch oriented workflow that is
workload that is continuous in nature
right so when something is continuous in
nature
you obviously want it to be Z faster and
written faster as well right so the
input output rate has to be higher
I think the option is the perfect use
case for this so I would prefer a
provisioned IOPS whenever I have a batch
reinterred workload let's see what
answer says okay an answer agrees with
me as well so provision IOPS deliver
high i/o rates but on the other hand is
expensive wealth engine the enable full
utilization of systems therefore
provision IOPS will be preferred for
patented workflows right so a batch
render workload has continuous workload
right there is no interruption
it could be utilized provisioned IOPS
that you will be paying for will be
utilized fully but if you look at other
workloads with length the online
transaction and the P option that add
with that is online transaction
processing workload it wouldn't have
been continuous because you don't know
when your goal will be back right so and
Praveen diappears like I said is
expensive so why not use a workload
which is continuous and at the same time
it does not intervene or does not have
any pauses or does not idle for some
time and hence it will be value for your
money right so I will choose the
provision IOPS whenever I have a pageant
workload then it gets the things done
faster all right moving ahead guys let's
look at the next question which is if
I'm running my DB instance as a multi
easy deployment can I use the standby DB
instance for either write operations
along with the primary DB instance so no
you cannot do that because the reason
for a standby instance is basically for
a failover right so no standby instance
cannot be used with try mitigation only
one DB instance can be used a time
understand by these de vincennes is
solely used for standby purposes all
right and the explanation says it cannot
be used unless the primary instance goes
down right so you cannot use your
standby instance unless and until your
primary instance is not working anymore
all right
so you cannot use them simultaneously
okay
having said that guys let's move on to
the next question so your company's
branch offices are all over the world
they use a software with a multi digital
deployment on AWS
so they're using multi regions for their
particular workload they have servers
and multi regions and they use mysql 5.6
for data persistence okay the task is to
run an oddly batch process and read data
from every region to compute cross
regional reports which will be
distributed to all the branches okay so
it is a nolle process wherein your data
will be read from every region if we
process and will be distributed to all
the branches again this should be done
in the shortest time possible so
basically time is of the essence
and how will it build this DB
architecture in order to meet the
requirement so the requirement is that
it should be done in the shortest time
possible and the kind of workload is a
batch process and one should get the
reports from all the regions it is
computed and the computed result is sent
back to the individual offices so let's
look at the options for each regional
deployment use RDS - here with the
master in the region and reduplicate in
the height biters for each regional
deployment use MySQL on ec2 with the
master in the region and send oddly EPA
snapshots to the HQ region for each
regional value either if I serve the
master in the region and send Ally RDS
Naturals to the HQ region for each
regional deployment use MySQL initiative
with the master in the region and use s3
to copy data or files Ally to HP region
as a first of all we should not be using
my still and easy to because it does not
good practice we should always go for
distributed workloads and hence will be
using RDS so B and D are out of the
picture so since we are using RDS MySQL
the mastering the region and reader
click on the excretion and the other
option says with a master in the region
and send oddly RDS snapshots to the HQ
region
all right so let's see what the answer
is for this so the answer is RDS MySQL
the master in the region and a read
replica and the HQ region now for this
level so let's look at the explanation
so we will take an aria instance as a
master because it will manage our
database for us so since we have to read
from every region reported each replica
of this instance and everything where
the data has to be read from option C is
not correct since putting a read replica
would be more efficient than putting a
snapshot obviously because re replica is
lesser in size then a snapshot a
snapshot is huge right if you take a
snapshot of your primary instance every
time it will actually be very huge for
you to send it to every office it it
will actually slow down the bandwidth it
will I eat up a lot of bandwidth of your
internet because you'll be sending
snapshots a Denali basis so if you see
this you'll be sending snapshot every R
in snapshots like I said are not small
in size at the pre huge right so it
shall not be an efficient way of doing
it so the better way of doing it is by
installing at each replica and the HQ
reasons wherein from the read Africa you
can read it because all you want to do
is read report right so you can do it
using the read replica
so the next question says can I run more
than one DB instance for Amazon RDS for
free right so let's look at the
explanation so yes you can do that and
you can do it actually using the free
tier right any you was exceeding 750
instance us will be billable for you
right and actually what happens in free
tier it says this is basically like this
that you can in totality you are given
sense 15 instance hours right so you can
either run any easy to or you can run in
RDS or you can run some other service
now the thing is that with IDs for
example in Amazon's singing for an
example like if you run to RDS instances
right for now the number of hours that
have been allotted to you is 750 hours
right so if you run two instances
together these instance odds will
actually be divided by two and then each
of your service will run for free for
375 hours right and similarly if you
increase the number of instances again
so it will be divided by the number of
instances that you have running right
now now this is not limited to audience
this is limited to each and every
service which is there in free tier
right so the more the number of services
lesser the number of Oz become or the
division takes place between the number
of hours right all right so any any
doubts in this particular question guys
all right let's move ahead then so next
question is which a deployed service
will be used to collect and process
ecommerce data for near real-time
analysis right so the options are Amazon
ElastiCache a Amazon DynamoDB Amazon
redshift and then you have Amazon
Elastic MapReduce
it's a which service will you use to
collect and process ecommerce data for
denier time analysis so dynamodb is a no
sequel database right elastic MapReduce
the hope cluster redshifts is basically
a database bare house and elastic cached
is basically a very fast memory that you
have so it is basically used when you
have
and so it's basically similar to your
arm which is accessible more faster than
your primitive storage devices like your
hard drives or SSDs right so same is the
case of elastic a shape so the answer
for this would be Amazon DynamoDB and
Amazon redshift now Amazon DynamoDB why
did we used because it's a no sequel
database right for every unstructured
data that will be coming from a
e-commerce website will be storing your
DynamoDB and redshift is actually a data
warehouse which is basically used for
analysis right so it will pick up the
data from DynamoDB analyze it and then
you can review it for whatever analysis
you want to do so this is what we'll be
using will be using dynamo DB and we'll
be using Amazon redshift alright guys
sounds simple ok so like basically you
can answer each and every question guys
that we have discussed so far it's not
something that it's rocket science that
you can understand it the basic thing is
the underlying thing is that you should
understand each and every service
properly right so if you don't know what
dynamodb is or if you don't know what
redshift is you cannot answer this
question but if you have the basic
understanding that dynamodb is a no
sequel database and a trip to the data
warehouse service you can easily
identify that for analysis as you I will
use redshift and since the e-commerce
data will be going to this is going to
be unstructured the data that I am going
to analyze it will be stored in dynamo
DB nice pretty common sense ok so guys
any doubt in this particular question so
as a pitcher should I explain something
else also that I missed out anything
that you feel is not understood by you I
can explain it again anything that I
feel I should repeat
okay so let's move on to the next
question then so our next question says
a company is deploying a new 2t of our
application in AWS the company has
limited staff and requires high
availability and the application
requires complex queries and table
joints which configuration provides the
solution for the company's requirement
so let's look at the options here the
option is is MySQL installed on to
Amazon ec2 instance in a single ability
zone so single level razÃ³n will not be
something that we should be looking out
for because we wanted to be highly
available our application Amazon RDS for
MySQL and multi JZ okay then we have you
know Amazon ElastiCache a it's not a
database then we have Amazon DynamoDB so
dynamodb is a no sequel database MySQL
is a relational database right and our
application requires complex queries and
table joins now the answer for this guys
it should be dynamo DB because dynami we
had the ability to scale more than RDS
or any other relational database service
and therefore dynamo DB would be an
option is right and we can deploy it in
multi ability zones as well and then it
sells so our purpose of being highly
available and also it can withstand
complex queries and table joins all
right seems good seems good enough let's
move on to the next question so our next
question is what happens to my backups
and my DB snapshots if I delete my DB
instance all right so when you delete a
DB instance guys you have an option of
creating a final DB snapshot now if you
do that you can restore your DB from
that DB snapshot RDS retains the user
gradiation along with all other manually
create a new snapshot for the instance
deleted but if you delete the snapshot
of your DB instance there is no way that
you can get it back but once you try
deleting your instance it will always
ask you whether you want a snapshot or
not alright even if you are in a hurry
and you click yes that yes is actually
for creating a stamp on it and people
tend to go for a person
right so in any case if you progressed
yes it will create a snapshot you have
to be careful if you don't want to start
prod to be created
alright so yeah so that is the scene so
this question release plate
straightforward answered our query let's
move forward so which are the following
use cases are suitable for Amazon
DynamoDB alright so that will be like I
told you guys it's a no sequel database
so is it managing web sessions showing
JSON document storing metadata for
Amazon s3 objects or running relational
joins and complex objects now the answer
is managing obsession showing JSON
documents as storing metadata for Amazon
s3 objects right so is a B and C now
relational database store entry data in
the form table rows right shown on
relational database represent data as a
form JSON documents running relational
joint itself answer the fact this stance
is more suitable for a relational
database that is running list joins and
everything right and I never dream
believe you mingle
no SQL database can be more related to
tasks a B and C right so if you look at
the options running relational joins
although you can do that on dynamodb as
well but why will do it when you have
lesser databases right so I think the
most app for the force should be a
relational database the first three are
perfectly app for our dynamodb and hence
we have put the answer like this all
right let's move on to the next question
so our next question is your application
has to retrieve data from your users
mobile every five minutes the data is
stored in dynamo dB
okay so your data is retrieved every
five minutes and restoring the anomaly
be later every day at a particular time
the data is extracted into SC on a per
user basis and then your application is
later used to visualize the data to the
user so what basically is happening is
your mobile is sending data every five
minutes to DynamoDB and every day on a
particular time the DynamoDB data is
dumped onto s3 and then that
data averages Dunbar SC is used for
analysis alright so this is the
particular task you are asked to
optimize the architecture of the second
system to lower cost
what would you did all right so these
are the options this elastic cash a to
cash a reads from the Amazon DynamoDB
table and reuse provision read
throughput alright so like I said last
ik cached is a very fast storage service
so whatever is there on ElastiCache a
can be read very fast right and it can
be dumped onto elastic I say at that
very much speed as well let's see what
our explanation says since of work
requires the data to be extracted and
analyzed compromise this process a
person would use provisioned i/o but
since it is expensive using an elastic
cache a memory instead to case a the
reserves in the memory can reduce the
provision read throughput and hence
reduce cost without affecting the
performance so absolutely right if you
want increased performance since we want
to analyze data we have to access real
order data and then be able to visualize
it right so for reading a lot of data
one way of doing it is that you have it
on your s3 main hard drive so s3
filesystem and you read it in a
particular fashion and in a regional
mount fashion the other way to do it is
that dump the data on elastic caching
and review from there now as you know
that the data which is dumped in elastic
a shape can be done at a very fast rate
and even the reading can be done at a
very fast rate having said that if you
had be used provision i/o in this case
since you are using it every day
provision i/o is very expensive in terms
it is expensive it is more expensive
than your elastic cache it right and if
you use elastic I said you will get the
same results all right so why not use
elastic asset that provision ioad then
both are offering high speeds right so
this is how you can improve the backend
architecture
moving on the next question says you are
running a website on ec2 instances
deployed across multiple availability
zones with multiple easy RDS - here xlrd
be instance the site performs the high
number of small readwrite per second and
relies on
consistency model after comprehensive
tests you discover that there is a read
contention on re s my scale which are
the best approaches to meet these
requirements right and we have to choose
to answer so what basically we are
trying to do is we are running a web
site on ec2 instances right which is
deployed across multiple Easy's with an
extra large DB instance so it is
deployed across multiple energies and
the website is actually deployed and
easy to write it performs the high
number of small reader writes per second
and realize and eventually as general
okay after comprehensive test you'll
score there is a read contention on
Gradius MySQL in the sense that the
instances that are for your website have
a read contention in the sense they are
competing to read on aureus - scale
which are the best approaches to meet
these requirements now how can we reduce
the contention so that the radiance you
can be reduced
let's see the answer for this so the
answer is deploy elastic cache a
in-memory cache is running in each and
beauty zone and see increase the RDS by
excellence in size and implement
provisioned IOPS okay so what basically
this will do is that first of all we are
increasing the vinson size so that we
can serve more users and at the same
time to serve the requests faster since
we are doing a lot of reading right
small little resonance a lot of reader
rights to do them faster we have
implemented provision value along with
that we can also deploy an elastic
caches which can dump data onto itself
so basically why we're using elastic
cases because sometimes there is the
reading of data which has already been
read right so something which is
accessed more frequently can be stored
on elastic cache and how will that help
is that the provision IO you are charged
for each and every input/output right
but if that frequently accessed data is
an elastic asset the IO for that can be
reduced and hence cores can be reused
and at the same time when you are
reading it from elastic cache a you are
reading it from the same exact speed
behind
with prone areas high speed and with the
logic caches well you have high speed
alright so when you are doing that you
are getting the data faster as well and
at the same time you are reducing costs
so that should be the viable solution
for this moving further guys let's start
with an X question so our next question
says a startup is running a pilot
deployment of around hundred sensors to
measure Street noise and air quality in
urban areas for three months
all right it was noted that every month
around 4gb of sensor data is generated
the company uses a load balance or a
scale layer of V zero instances and RDS
database I'm on TV since tourists the
pilot will success and now they want to
reply 300k sensors which need to be
supported by the backend okay so you
need to store the data for at least two
years and I leave it set over the
following would you prefer okay so
basically they are increasing the number
of sensors and they want to store the
data for at least two years and after
that they'll analyze it so they're
asking which option to choose options
are add an asterisk NS rescued ingest
layer to buffer writes two areas
instance ingest data into dynamodb table
and move all data on each cluster okay
here plays audiences with the six node
redshift were 796 TB of storage or keep
the current architecture but upgrade the
RDS to with 3 TB and 10k provision IDPs
and the answer is replace the ideas
instance with the six node racial class
of a 96 TB of storage and the
explanation is that you should be
preferred that ship class or group ever
because it is easy to scale also the
work would be done in parallel through
the nodes therefore is perfect for a
bigger workload like in our use case ok
so as well since each month 4 GB of data
is generated
therefore in two years it should be
around 96 DB and since the servers will
be increased 1 @k in number 96 e we will
approximate we become 90 60 B and hence
option C is the right answer
ok so it's a mathematical answer guys so
it says we with 96 GB for 2
years each month four GB of data is
generated all right and therefore in two
years it should be around a decision
it's basically two years has 24 months
24 into 4 is 9 to 6 GB and since the
servers will be increased 200k right so
now we had 100 signal sensors so we are
increasing it by a thousand factor right
so if 100 sensors were generating
negative ZB in two years hundred
thousand will increase ninety six
thousand GB right so ninety six thousand
GB is approximately 96 TB and hence this
answer right where you do using a red
shift red shift cluster because it is
easy to scale and we have chosen 96 DB
because that is how we came to it
mathematically alright so this is the
answer all right cool will it's a match
all right let's move on to the next
question
guys if you're not clear with the math I
can repeat it again any one of you wants
me to repeat the match again okay so
Shashank is asking me to repeat ok
Shashank so basically 4 GB of data is
generated every month by a hundred
sensors alright
this is the scene right now so if you
can read the question over here it says
4 GB of data is generated every month
right and right now there are hundred
sensors now if 4 GB of data is generated
every month and if we do it for two
years and stretch 2 years has 24 months
to 24 months in to 4 GB accounts to 96
GB so 100 sensors in 2 years will
generate 96 GV now the sensors have now
been increased 200,000 so if 100 centers
within reading landis is GB 100,000
sensors will generate 96,000 gb agreed
yes
so 96,000 GB if you do the maths it's
approximately 96 DB because thousand 24
GB is equal to 1 TB all right so we will
have a number which will be less than 96
TV right but it is always good to have
more storage right so will school with
96 DB
somewhere around our number and we'll be
using the redshift cluster because it is
easy to scale right so are you clear
sang now alright let's move on to a next
question guys next section actually so
we are done with the database questions
now let's move on to section five let's
talk about order scaling and load
balancer right so let's look at the
first question suppose you have an
application where you have to render
images and also do some general
computing from the following services
which service will be best fit your need
right so basically you have to render
images and you have to do general
computing so what should we you so you
should use a classic load balancer and
application load balancer both of them
or none of these a classic load balancer
basically distributes a traffic equally
between your instances but what
application load balancer does is it
identifies what is the kind of
application that you're trying to do and
then according to that it redirects you
to the particular server right now in
our particular use case we can either
render images or we can do general
computing right so we can have separate
servers for each one of these and I
would prefer application exactly so I
would prefer an application load
balancer in this case because like my
application that isn't either either
images or I'll do some general cloud
computing work right so that can be
easily done using application load
balancer
moving ahead let's move on to the next
question which is what are the
difference between scalability and
elasticity
okay so scalability is the ability of a
system to increase its hardware
resources
so basically when you're scaling your
particular computer you are basically
not increasing the number of source but
what you are doing is you are increasing
the specification of your computer right
so for example if you having 2 GB of RAM
before now you will increase it to 4 GB
I see we are having filled hundred GB of
hard drive you will increase it to 1tb
this is what scalability is called
elasticity is the ability of a system to
has increasing the workload by adding
additional hardware resources in a sense
if you were having two or three servers
before now you will have six or seven
servers right so you increase
number of servers rather than increasing
the hardware specification is over will
be the precursor in the sense they will
have the same Hardware Ã«social or the
same hardware specification at the first
server and hence elasticity is that that
increased the number of servers rather
than increasing the hardware
specification so a next question is how
will you change the instance cipher
instances which are running in your
application tier and I using auto
scaling and they will change it from the
following areas so how will you change
the instance type so basically in easy
too we have a lot of instance I price
you have t2 you have T to dot micro you
have I to dot I CRO yeah I to rock large
I know how to change your instance type
which are running in your application
there and I using order scaling so the
areas from where you can change are the
options are the auto scaling policy
configuration or scaling group auto
scaling tags configuration or scaling
launch configuration right so the answer
is auto scaling wants for configuration
so auto scaling tax and figuration is
used to attach metadata to your
instances to change instance type you
have to use the order scaling launch
configuration so it's a pretty
straightforward questions it's a thing
that you there is no concept but you
should know that whenever you want to
change instant type even your ec2 which
is - with auto scaling you have to go to
auto scaling launch configuration to
change alright moving on the next
question says you have a content
management system running on an Amazon
ec2 instance that is approaching hundred
percent CPU utilization right so which
option will reduce load on the Amazon
ec2 instance so the only way that your
CPU fly Edition will get reduced in your
ec2 instance is by reducing the traffic
right now what ways can you discover
here to reduce the traffic now the
traffic can be reduced by applying a
load balancer to your ec2 right so if
you simply auto scale there is no
possibility of the traffic going to the
next server until and unless a load
balancer is attached so if you see the
options the only load balancer that I
can see is an option a so I think option
a should be the right answer and yes
this so creating a loan and auto scaling
group will not solve the issue like I
said until your - the load balancer to
it once you adapt the load balancer to
an auto scaling group it will
efficiently distribute the load among
all the instances and hence this is the
answer guys the other thing is if you
attach a CloudFront distribution cloud
front is a content delivery network it
is basically used to reduce the agency
and hence that will not be used auto
scaling group from the instance using
the create or scaling group action like
I said so you have to attach node
balancer for the scaling to actually
work and create an odds configuration
from the instance using create launch
configuration action so this is not the
correct answer the correct answer is a
create load balancer and register Amazon
ec2 instance with it all right
having said that guys is there any doubt
regarding this question anything that I
didn't understand in this question ok so
let's move on to the question 39 which
is when should I use a classic load
balancer and when should I use an
application load balancer so like I told
you guys whenever there's a
differentiation in the traffic wherein
you want the traffic of a particular
type of application like for example in
the previous questions we saw that for
rendering images you wanted another
server and for general computing were on
it and as a so all right so what we did
was we have attached an application load
balancer and according to the
application according to what was
required so if rendering images is the
use case it will redirect the traffic to
that particular server if general
computing was required to to redirect it
to that particular server and in
comparison in within classic load
balance or the classic load balance it
is actually used like this that when you
want to just distribute the traffic your
traffic equally among the running
instances so that kind of load balancer
is called the classic load balancer so
let's have a look at the explanation a
classic load balancer is ideal for
simple load balancing of traffic across
multiple ec2 instances it so wide on a
projection load balancer is ideal for
micro services or container based
architectures where there is the need to
route traffic to multiple services or
load balance across multiple ports on
the same easier instance
right so whenever there's an application
wherein you have to load balance
according to the work in that case you
were using the application load balancer
all right pretty straightforward let's
move on to the next question so what
does connection training do right so
connection draining is a phenomenon in
AWS what does it do so a it terminates
instances which are not in use be read
out traffic from instances which are to
be updated of a health check reroute
traffic from instances which I have more
workload two instances with have less
workload drained all the connections
from an instance with one click so the
answer is its reroute traffic from
instances which have to be updated of
here's a check so whenever there's an
instance which fails a health check or
where is an instance which has to be
updated I mean maybe there is a software
which has to be patched or maybe the
instance is not performing well and you
want to see or you want to put it in a
maintenance so what do you do is you can
enable connection draining and what will
happen is it will pull out all the
connections for that particular instance
and distributed among the other
instances which are running at that time
and once the connection has been drained
from that particular instance we will
put it down for maintenance or restarted
as to whatever you want right so if it
fails a health check you might want to
delete it and start a new one and if you
want to put it down for maintenance you
can maintain it with your own time
because that will require for the
traffic to go out from that instance
first otherwise there will be a photo
for for the traffic if you put down your
instance abruptly right so the correct
way to do it would be to enable
connection draining on that particular
instance array now the connections on
that particular instance which has we
maintained or updated and then you can
go ahead and put it down right so again
very straightforward question I don't
think you guys would have any doubts
let's move ahead and look at the next
question which is when an instance is
unhealthy it is terminated and replaced
with a new one which are the following
so is that so is it sticky sessions is
it fault-tolerance connection draining
or monitoring so right so it is
terminated and replaced with the new one
so the answer is false
because if you look at the other
services so you have connection draining
with drains the connection it does not
terminate or replaces it right if you
have sticky sessions even if the
instance starts this session remains the
same so that is what sticky sessions is
all about if you talk about monitoring
monitoring will not do that and fault
tolerance is the answer so what Poydras
does is let's look at the explanation
the e will be detected and since is an
LD it start shouting incoming traffic to
other le instances in deviis right if
all the instances and the region becomes
an LD and if you have instances some
other value to zone your traffic is
redirected to them once instance become
held again the rialto back to the
original instances this phenomenon is
called fault tolerance right while
collar is basically what it does is
before terminating when it becomes
unhealthy it takes a traffic it puts it
down to other instances which are
working fine then terminates it and
replace it with the new one and then
drains back all the connections back to
this instance this is what fault or nuts
are all right moving out there is the
next question says a user has set up an
auto scaling group due to some issue the
group has faced a large single instance
for more than 24 hours what will happen
to order scaling in this condition at
the options are auto scaling will keep
trying to launch the instance for 72
hours or scaling will suspend the
scaling process or to scaling this chart
an instance in a separate region the
auto scaling will group will be
terminated automatically the answer is
all the scaling will suspend the scaling
process now why will I do so because it
has a feature where in you can resume
and pause the auto scaling process so
when sit encounter that there is a
problem with the configuration in the
auto scaling groups it suspends the
process temporarily and you can go ahead
and make the particular changes and like
presume the auto scaling process again
right look at the other options there is
no option as in there that it will keep
trying to launch systems for 72 hours
that is not the case if we start an
instance in a separate region that it
cannot do on its own you have to specify
it and auto scaling group will be
terminated automatically that also does
not happen it basically just pauses it
or suspended for the time being
and when you intervene you can correct
the configuration and then it's a resume
again this is how it functions all right
moving on guys we are done with this
section let's move on next section which
talks about cloud trail and route 53
right so let's talk about cloud trail so
you have an easy-to security group yeah
so you have an easy-to security group
with several running in zero instances
you change the security groups allow in
one traffic on a new port and protocol
and then launched several new instances
in the same new group right the new
instances rule apply immediately to all
the instances in the script is cada
group immediately to the new instances
only immediately to the new instead of
an old instances but you stopped and
restarted before the new rules apply or
to all the instances but it may take
several minutes for whole instances to
see the changes right now the correct
answer is it applies it immediately to
all instances in the security group
because this is imperative guys this is
common sense because if you had ruled
which you modified and you had several
instances before in the security group
if you change it now what is the point
of having a cloud provider to us who
cannot deal with such changes because
this is something which is very basic
right you might want to do some
maintenance or you might have updated
something it's so what it does is
immediately puts the changes to all the
older instances as well and hence move
sword so this is the thing so the
explanation says any rules specified in
an ec2 security group applies
immediately to all instances
irrespective of when they are launched
before or after adding a rule so this is
the feature guys which is awesome right
moving ahead guys let's move on to the
44th question which is to create a
mirror image of your environment another
region for disaster recovery which of
the following AWS resources do not need
to be recreated in the second breach so
you have route 50 record sets elastic IP
addresses ec2 keepers launch
configuration and security group so we
have to choose two answers so record
output p3 record set an elastic IP
addresses and the reason is rustic IP
addresses static guys so even if your
instance is going to another region the
IP address will remain the same because
that is the point of having a static
right the other thing is route repeat in
regards as now rule 53 is something
which basically translates your website
name to the IP address right so record
sets are irrespective of what reason you
don't say men they remain the same
all right so they will not change let's
look at the explanation elastic IP zone
or three hypothesis a common assets and
therefore there is no need to replicate
them since elastic IPS and all 53 are
valid across reasons I said this is the
explanation guys moving forward let's
look at section which talks about a
doubles sqs SNS SES and a double s
elastic beanstalk alright so which of
the following services you would not use
to deploy an app so the options are acid
beanstalk lambda of slugs and
CloudFormation so if you would not you
should deploy an app alright so if you
talk about classic Beanstalk it is
actually a platform as a service which
is used to deploy an application so the
question is which you would not use to
deploy an app so if you talk about
lambda guys so lambda is used for
executing background apps
it is tasks it is not used to deploy an
application and hence it will not be
used we talked about ops folks it is a
configuration management tools so
basically you have stack and you have
layers in it right so different layers
contribute to become a stack and it is
basically used to deploy an application
but with various resources in it right
so off socks can be used and same can is
the case with cloud formation in cloud
formation basically you specify a JSON
script and reading the JSON script it
launches the particular services
respectively so let's look at the
explanation lambda is used for running
service applications can we use reply
functions triggered by events when we
say several as you mean without worrying
about the computer users running in the
background is not designed for creating
applications which are publicly accessed
alright so like I said lambda is used by
clicking on tasks it is not used to
deploy an application and hence this is
the rygan so that this is the service
which will not be used to try an app
moving forward guys let's look at the 46
question powered elastic beanstalk apply
updates by having a duplicate ready with
updates before swapping or by updating
on instance while it is running or by
taking the instance down in the
maintenance window or update should be
installed manually so with the elastic
beam shock what happens is it creates a
duplicate and right and redirects your
traffic or the duplicate and then
updates itself this is what happens so
elastic means alt prepares you period
what if the sensors before updating
there is an instance it is also traffic
to duplicate instance so that in case
your basic application fails it will
switch back to the original instance and
there will be no downtime actions like
the users which is imperative because
downtime equals to loss of customers
right no customer wants to see 404 page
for your website right so this is
something that customers look for it
when they are investing in your sources
so you will never see Google at a
downtime even as he pays looking around
I even if they do they have it for like
seconds in a year right so no customer
wants to see a 404 page and hence if
they do it becomes bad for your business
and hence we have this feature
Andrassy Beanstalk moving ahead guys
let's move on to direct question now
so the 407 question is what happens if
my application stops responding to
requests in elastic Beanstalk
so in elastic we install applications
have a system in days or avoiding
freezes in the underlying infrastructure
if an Amazon ec2 instance fields for any
reason Beanstalk will use or scaling
from automatically launch a new instance
Beanstalk can also detect if your
application is not responding on the
custom link even though the
infrastructure appears early it will be
logged as environmental event example
bad version will deploy a you can take
an appropriate action so elastic
Beanstalk already has systems in place
which can handle these kind of
situations so even if the health checks
are showing the instance is already and
the link that last you been surprised
with is not responding or the website is
not functioning it will flag it as an
environment event and will flag it as in
a bad code has been uploaded or a bad
environment has been created something
that I because if the instance is
running fine there is always something
wrong with the code this has been
updated in the instance and that is why
the problem is occurring
so this is how it handles problems in
elastic Beanstalk which is a very cool
application it is a platform as a
service and like I said is used to
deploy an application moving forward
guys let's move on to section which
talks about AWS opsworks
in AWS KMS alright so let's start the
first question so I created a key in
Oregon region to encrypt my data in not
Virginia region for security purposes I
had a two users through the key and Sun
Ray WS account
I want to encrypt an object Nestle so
when I try the key I just created was
not listed what could be the reason so
what he did was he created the key in
Oregon region and he encrypted the data
in North Virginia region so there are
two different reasons online world right
and then he added two users to the key
so he assigned to users to the key and
an exterminator press account that he
created in the Oregon region now he
wanted to encrypt an object in se so
when he tried the key that he created
was not listed so what could be the
reason so basically the question is
pretty clear guys that the key has been
created in the separate region and the
data which is to be encrypted is in a
separate region right so the options are
external AWS accounts are not supported
a reverse s3 cannot be integrated with
kms the key should be in the same region
and new keys take some time to reflect
in the list so the answer is the key
should be in the same region the
expedition is like I said the key
created the data to be encrypted should
be in the same region hence the approach
taken here to secure the data is
incorrect
right so either the data has to be
launched in an Oregon region or the key
has to be created in the North Virginia
region any of the different things we'll
both find all right moving on guys let's
move on to the 49 question which is a
company needs to monitor the read and
write IOPS for the AWS - Claudia's
instance and send real-time alerts to
their operation treats bitch readable
service can and compresses so you have
SES Club watch simple queue service and
all 53 so route 53 will not be used
since it's a DNS service simple queue
service the queuing service so it
again not be using its buffer like
service then you have plowed watch and
SES so the answer is cloud watch because
cloud watch has alarms in them which can
be used to trigger an alarm available
particular metric is cost right so let's
see the explanation so Amazon Cloud
watch the cloud monitoring tool and
hence this is the right size for
mentioned use case the other options is
they are used for the purposes for
example alpha 3 is used for DNS service
therefore cloud word should be the app
choice
so like I said in cloud watch what
happens is you have a setting for lamps
now the alarms are triggered with
whatever metric you apply right so you
can set a metric that says readings and
whenever it crosses a certain limit
create an alarm right and in that case
you might use SES - may be alert the
operations team but before the ACS
because ACS cannot read the IOPS
messages right the messages can only be
read back loud watch because that is
what the cloud monitoring tool is all
about alright so I guess this answer is
clear guys any doubts in this answer
anything that you're not understood in
this particular answer ok
moving forward we have reached this is
the final question guys so the question
is what happens when one of the
resources and a stack cannot be created
successfully any debris SWAT sucks right
so what happens when a resource is not
created successfully negative less
objects so in the event like this a
total rollback occurs in the sense if
there are 5 resources to be deployed and
the fifth resource fails to deploy it's
not like that the full resources will be
there the whole process is rolled back
and the services before this particular
instance will also be deleted let's read
the explanation so when an event like
this occurs the automatic rollback on
arrow feature is enabled which causes
all the Yale abuse resources which will
create successfully to the point without
error occur to be deleted this is
helpful since it does not leave behind
any erroneous data exactly so it is
useful because what if your first
instance is linked to your fifth
instance with just failed to get
deployed right so in this case there can
be chances of
Dependencies remaining in your
environment without you knowing it so to
avoid that there is an automatic
rollback on error feature in your AWS
opsworks which takes care of this right
so whenever even if it's the third or
fourth instance if it faces the third
instance and there are five like
instances the first word applied
successfully and the third ends up feels
you roll back on this first Winston sit
as well and will also notify you but the
same that the third resource deployment
failed and that is the reason the first
two instances were also deleted all
right so guys with this will conclude
our session right so any questions any
questions that you have which I think
should be answered right now which you
feel it have not been answered in the
interview questions I'd be happy to
answer them anything any question that
you had a doubt in and you remember it
now I shall answer that also
I'm all yours guys anything that you're
not cleared with okay so Samia saying I
session you're welcome Sammy's okay
Roger the saying nice session Thank You
Rogers Jason says awesome session thank
you Jason sort of is saying well done
you too okay guys so since I think so as
if you all are clear I'm not getting
inquiries so shall we wrap this up okay
guys so thank you for attending today's
session guys I hope you found it useful
these questions are very important guys
I would suggest you to go to the
recording again go to the questions read
the explanations expressions are
self-explanatory right so you would have
noticed that I read just some of the
questions they get the explanations
actually were so good right so go
through the questions the explanations
once again they might help you in your
last-minute studies and I know that some
questions you might have doubt later on
so you can always contact the support
team for that if the support team is on
our able to one so you have your my
email please email me with a query and I
will reply back to it as soon as
possible
all right guys with that I would like to
end today's session thank you for
attending today's session guys have
good night I hope you enjoyed listening
to this video please be kind enough to
like it and you can comment any of your
doubts and queries and we will reply to
them at the earliest you look out for
more videos in our playlist and
subscribe to our retro Rica channel to
learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>