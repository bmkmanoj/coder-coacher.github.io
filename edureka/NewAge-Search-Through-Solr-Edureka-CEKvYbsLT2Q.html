<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New-Age Search Through Solr | Edureka | Coder Coacher - Coaching Coders</title><meta content="New-Age Search Through Solr | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New-Age Search Through Solr | Edureka</b></h2><h5 class="post__date">2015-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CEKvYbsLT2Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi guys and today we all are here for
the knowledge sharing of the topic
Apache Solr and I would be taking you on
a one I long journey of and I'm going to
assist you with exploring Apache Solr
today and I have been an IT for drum
about nine years so working on solar for
last term six and a half more than six
and a half years so I started off
working on Apache Solr 2.2 and since
then I have seen it growing and growing
I mean it's getting better and better
with each releases um they have made
this thing so fabulous so we would be
discussing what makes it so fabulous and
we would discuss that what makes it so
powerful and so sought after so we're
going to be starting off with the Apache
Solr and we would be exploring the new
age of search through the stool and this
is a very very powerful tool I tell you
guys so any of this session basically
you would be understanding that what is
the need of a search engine for an
enterprise-grade application the
objective and challenges of the search
engine how is indexing and searching
handled in lucene I mean I'll as of now
if you guys are wondering why are we
discussing leucine as well I mean the
session was actually designated to be
talking about apache solr so we would be
discussing why we are actually talking
about leucine as well because leucine is
actually the the magic back end enum the
hood of Apache Solr then we would be
exploring the solar and myths
architecture we would be talking about
the near real-time search with the solar
and solar clout then we would also be
talking about leveraging the
capabilities of solar along with Hadoop
and we would be discussing a little
brief about yarn as well I mean how can
we configure and leverage the
capabilities which are delivered by
through solar and again the foremost and
the main concern vai vir here is the job
opportunities for the developers who
work on solar so if we are good enough
we can begin this journey so let me just
first of all begin with a very basic
idea of what solar is giving to a web
application so any any idea guys why
would we really need a search engine I
mean we all know and I believe we all do
understand and acknowledge that apart
the Apache Solr is basically leveraging
the search capabilities in your room
maybe new application or existing
application so why do you think search
is so important why do you think we
would need the search capability on any
web application any any thoughts any
inputs ok Kunal shares that in order to
accommodate for search that is not exact
accounts for spelling mistakes when a
text etc ok now that's some quite a deep
dive canal so great i mean if if you do
understand all of that however from a
layman point of view what do you think
ok there is too much data to to be
searched for so you would need search
engine to organize your search ok right
so okay so two most and foremost I would
say reasons why anyone would like to
have a search engine or search
capability on their web application is
speed and relevancy I think everything
would be summarized with these two
points first of all we would like to
attain speed the the results should come
as soon as you type or rum at least in
in a blink off a few nanoseconds
actually and then the search string or
the search text which you are looking
for the results should be pretty
relevant to your content you're looking
for so we have we have few more inputs
here because executing like okay maybe
like as in seek
yeah that's pretty slow true pavan says
complex search queries can be slow to
run so we would need search engine
absolutely so actually I mean you most
of us do understand why we would need a
tool like solar or elastic search in our
application so this is basically to
boost search and why would we need
searches to attain the speed and
relevancy on any application so if
you'll look here we have put up a
snapshot maybe we do not have the web
application I mean as a site of this
particular you know ecommerce website
but we just taken a little example of
what search engines or what search gives
to your application so if you look here
this is a very fancy or snapshot which
we have taken from flickr com i'm sure
every one of you must have visited this
website once in your lifetime so looking
here we were looking for quad core
mobile and this is the search string
search text which we have entered in our
drum in our text text field which
provides us the search capabilities on
the application so if you look here we
have few suggestions on this particular
you know texts that are you looking for
a quad core mobile phones with kitkat OS
or the sony mobile phones with quad-core
drum processor or maybe the quad core
mobile phones with 4.5 inches of screen
so it's like autosuggestion thing which
is already been activated on your
website which is basically enabling the
user it is basically assisting the user
to make the search more precise so that
they can get more relevant results along
with that they have a very nice feature
accommodated there in which is also
talking about the popular products with
maybe similar search string along with
that if you notice the left hand side of
the snapshot you have a very nice
grouping done as well maybe I'm not very
sure or what kind of configuration of
mobile phones I'm looking for but I'm
sure that I need a phone
between say 10,000 to 18,000 of range I
would just quickly go ahead and select
this particular group on my left hand
side and I get all the products which
fall into that particular group of ten
thousand to 18,000 or maybe I am very
particular about a brand to own and I
say that I am actually looking forward
to own HTC as of now so I just quickly
go ahead and say that I'm looking for a
phone in the range of say a 10,000 or
18,000 to 25,000 of range considering
HTC mobile phones are little pricey so I
say that I select the brand of my choice
and I give a price range of the phones i
might be looking for and I get niche
results so all the results which I get
are also sorted in the manner I would
like to see them maybe I would like to
see the results sorted on the basis of
the prices they have along with that I
do also see that few of the features
have already been listed here I'm in a
very small and the kind of comprehensive
information about the user rating the
price range few quick features like what
kind of processor it has and what kind
of camera it has these kind of features
have already been listed for you guys so
this makes it a little easier for the
customer to you know approach the user
interface and select the product they're
looking for so this is the kind of
features this is the kind of
capabilities which a search engine gives
you along with that not to miss we have
the right hand side pain as well on the
snapshot which is basically telling that
these are also the kind of products
people are looking for so these are the
popular products as well which kind of
match with the search string you're
looking for or maybe the price range
you're looking for so this this gives a
user very not a very nice you know user
experience it gives to the user that
they get everything on one comprehensive
you know screen so they get these
suggestions they get the spell
correction they get the grouping of the
products on the basis of price on the
basis of brands or any internal feature
which is again very very configurable
then they also get to know that what
type of products are popular in the
market which they might also consider so
this kind of UI is delivered by the
search engines so just to give again a
little more depth into it we have the
text based search which is provided if
you see here we have highlighted the
field number one here which is there in
to ensure that whatever you might be
looking for you can just type in here
and the query is going to be um you know
interpreted in the manner which our
search engine would understand and it is
going to render all the relevant results
to you along with that you have filters
therein and along with that you have
everything pretty neatly organized in
form of documents which are which are
easily sorted as well so they're all
sortable results which you see here so
over here if you look there are few key
features which every search engine for
the matter of fact should have so what
are they first of all they should be
optimized for faster text searches so we
have spoken about speed so speed is the
primary and the foremost concern of the
web application and if it is a ecommerce
application then even more important it
should have the flexible schema so I
believe all of us do have some sort of
DB knowledge we all belong to IT so we
do understand that term when we talk
about schema of any traditional
databases we need to have the fixed
schema so if you're trying to insert a
maybe in inserted data in the database
table there in you need to comply by the
syntax and you need to comply by the
fields or the columns we essentially
call them you need to provide the
information as per the configuration of
that table if the column does not expect
any null values you need to provide the
value for that particular column so the
constraints are pretty rigid however a
search engine basically should
have the flexible schema or no schema at
all so it should also support these the
sorting of the document like I just said
I would want to see the lower price
products first following the higher
priced products later on it should be
web scalable which means that if you
have more number of reads coming on your
website more number of say people who
are dumb you know using your website to
look for the products so it should be
scalable enough to handle that kind of
user flow along with that you need to
have the kind of you know structure that
your search engine should be
document-oriented everything pertaining
to one single product should be there in
one place so these are a few of the key
features which a search engine should
have we are not discussing as of now
we're not discussing apache solr so
let's see what all really matches with
Apache Solr along with that just to
cover up there is this example which we
have taken from another very popular
website cleartrip on which basically
people would want to see that how far is
one place to another what kind of hotels
or what kind of places they can explore
when the planning the trip they can also
see the comprehensive list of the hotels
and the star rating they might belong to
the price range in the similar manner if
you would like to check the left-hand
side of the website snapshot along with
that on the right hand side the major
the map section which you're seeing in
here we have placed the tags there in
that which hotel is placed where so
maybe you would like to explore that I
mean that there is a maybe a very famous
you know spot on a particular place so
how far is your hotel is going to be
from that place or how far is that
particular hotel from the place you're
going to be traveling there along with
that you would like to see that I'm just
you know entrusted in the three star or
the five star hotels so you can select
that as well from your left hand side so
this becomes pretty easy for a user to
basically shortlist the kind of firm
places or the products they might be
looking for so again this is another
kind of feature which research you know
application would provide you so along
with that just to begin
the let me see if we have any questions
ok so the hacker has a question here
that when you say scalability do you
mean load balancing capabilities
embedded ok so the hacker when I say
scalability I say that imagine I have
I'm serving ecommerce website
application data and I have as of now
imagine I have 10 million products as of
now however in the due course of time
say in the five years or ten years down
the line I am seeing that my product
catalog is going to grow like um triple
off for what it is as of now so I'm
saying that scalable in terms of the
storage scalable in terms of performance
so load balancing is another feature
which I would essentially not count in
the scalability person did I answer your
question so Thakur so as of now I'm just
talking about the data the amount of
data my particular application is going
to handle the traffic how it is going to
handle that so we're talking about that
aspect when I talk about scalability so
when we talk about apache solr i would
say these scalability of apache solr is
unmatched considering that the kind of
giants like linkedin the kind of giants
like Twitter and flipkart and clear trip
you know such people are using it so you
can trust the flexibility you can trust
the robustness which it provides to you
the application so along with that I
would also like to tell you that
currently it is the most widely used
search solution which is present on this
planet as of now so it has established
plus it is growing it has ever growing
and you have approximately eight million
plus downloads which have been done for
this particular application you have
250,000 plus downloads every month for
this particular you know application
then you have 2500 plus open solar jobs
if you look for as a
now so every day it is being used by
giants as I've just disclosed linkedin
twitter Apple Netflix Instagram and many
more such major drum website and the
product providers so it is catering to
almost 100 million requests every day so
imagine the kind of capabilities this
this might be delivering if it has you
know scalable to that extent so Kunal
has another question here other search
api such as compassed GP text etc are
all based on leucine so am i okay is
this a question for me if you're asking
ok so well yes most of them yes they are
however we have custom you know if
you've talking about the distance ap is
if you're talking about people who are
basically leveraging the search
capabilities in terms of calculating
distance I think the one you were
talking about the compass in GP text
basically they have customized leucine
considering that it has open source you
don't have to pay anything towards
licensing of leucine so they can
basically go ahead and customize that
particular feature of leucine as if now
leucine uses have ursini algorithm to
calculate the distance between two
points two spaced points basically so
however you have few more variations as
well like you have elastic search again
elasticsearch is also using leucine at
the back end so by a large load of
applications are using leucine it's ever
seen hav ER SI n e ever seen algorithm
canal so i have another question here
from kieran how is the patchy school are
different from solar cloud so we would
be covering that in a little while
current if you can um hold on to it
maybe for few more minutes so let me
just go ahead and talk about so Prasad
we have lot of algorithms which are
there in lucene again covering them in
this particular session as little out of
scope because we have the designated
one are and I have to cover a few more
topics so when when you enroll in this
course Apache Solr along with the men at
Eureka Darren we have two dedicated
classes I mean when I say two dedicated
classes I say that we have six hours
dedicated which in which we would be
talking about leucine only so another
question from I think so the hacker what
is the relation or difference between
solar and leucine so again sadaqa the
distance is very very concrete because
solar uses leucine at the back end so
all the capabilities which are provided
by solar are basically delivered in a
min delivered through leucine we would
be discussing this in the architecture
if you could hold on to this question a
little bit maybe for few more minutes ok
so let me just move ahead we are going
to take a logical breaks in between to
take care of these sessions because
again we have to stick by the flow as
well so we would not you know go out of
context for the other guys so it you
know stays little easier for everyone to
follow so talking about leucine briefly
Lucy Knisley powerful java search
library which again lets you add these
search capabilities or the information
retrieval capabilities to your existing
or new your application anyway guys any
idea about this guy Leda pick which we
have shown in the inset anyone who knows
this guy no cutting he's the creator of
leucine any idea what else he might have
done ok canal has got it right yeah he
is the creator of Hadoop as well
absolutely kanal kannan event kieran yes
absolutely Munoz ready as well
absolutely fabulous guys so it seems to
me you guys are pretty familiar with
Hadoop and the related stuff yeah so we
would be discussing how this can you
know become
another feather in your hat I would say
so leucine is used by linkedin twitter
and many more giants once this
presentation reaches you guys you can
probably refer to this URL and explore
this a little bit more it is pretty
scalable and very very high performing
api which helps you with the indexing
and searching capabilities it provides
you with the powerful and accurate and
very efficient search algorithms again
the algorithm part as of now it is a
little out of scope considering that we
just have an hour to discuss about
apache solr so as of now i will just say
that it provides you with a lot of you
know powerful air algorithms like you
have phonetic search there in you have
ever seen algorithm to search the
distance you have say you know poorest
em algorithm to look for these similar
words these synonyms so these are few to
talk about however this all is covered
in wash to details in the course itself
so as I just mentioned we have two
dedicated sessions to ramp you up with
the with the capabilities which leucine
delivers so again the four most
important reason why it is so sought
after is that it provides you with the
cross-platform solution so you can use
it or use it on a dart in application
you can use it on your PHP application
you can use it on your java application
so choice is all yours you have lot of
variations and you have lot of you know
variants in mature leucine comes and
again it is open source as I just
mentioned a few minutes ago it is open
source and hundred percent pure Java
however the variants are available in
other programming languages as well
which our index are compatible so we
would be discussing in a little brief
that how this entire thing works so let
me just check if we have any questions
as of now okay um canal has a queer
I have customers bugging me on
integrating leucine with their DBS they
use Derby my sequel etc are there any
best practices for leucine available
around well yes and we would be guiding
you through that as well I mean if at
all you enroll in the course we have the
ways I mean people do bring up their
questions and people do bring up their
use cases along so we do provide the
assistance on how they can embrace you
know capabilities of leucine and solar
in the existing or the in your
application with various technology you
know background they might have so maybe
as of now I would say that you can
probably drop in a ticket to us maybe we
can discuss about this so for now I will
just go ahead and discuss about how
basically indexing is done and what
makes it so powerful so guys any idea
about I mean I'm just asking if you do
remember how the index in front of your
book and back of index back off book
varied do you remember the front indexes
and the back indexes in your textbooks
in your schools do you guys remember
that the the front index used to talk
about the chapter number and the page
number it belong to however the back of
the book index yes appendix absolutely
so the appendix used to basically talk
about all the important terms and the
page numbers they occurred in yarm the
front one is basically sequential and
the back one is basically referred
forward and is basically used for
appendix so let me just give you a quick
maybe flashback so we had been seeing
something like this do you guys remember
this so we had we had something like if
you're talking about seal that
apparently was referred on page number
67 and 24 if you're talking about squid
it was revered on 6 10 and 24 so
something like this so over here if you
talk about leucine basically creates
similar type of indexes for rum
the the process of indexing and
searching so instead of the terms here
these are basically the important terms
which are grabbed from the content or
the documents which are pushed in to
leucine so the important terms is
basically grabbed in from the documents
you're going to pass in so let me just
quickly take a small example here
consider it that we have say I mean as
of now we just taking example of three
documents here you could have few
million of documents as well so imagine
they're going through the subprocess of
indexing in lucene so imagine these are
drum sample you know text which we have
spoken about here we have document one
would say is I like edgy rekha courses
document to talks about Ed rekha
teachers big data courses we have
documentry here which says a Eureka
helps learn new technologies easily so
you have a few contain and few texts
here which apparently consists of in
each document there in so how leucine
basically converts them into the indexes
is similar to the back of the book index
it would gather all the important terms
from each of the documents and then it
is going to maintain the document ID the
reference document ID in which this
particular document apparently occurred
so if you if you talk about the process
of indexing it consists of two sub
processes so indexing basically has two
sub processes the analysis of the text
and the storage of this text so analysis
again has two sub processes you have the
process of tokenization and you have
another process of token filtering now
again had this been the actual class I
would have actually taken an hour or
something to explain these processes how
many different type of tokenization
processes how many different type of
tokenizer zwi have in lucene how many
different type of filters we have there
in so again just to give you a little
brief about it to give you an idea how
does that really happens
us that the the process happens to be
like the the document goes through the
process of indexing wherein it goes
through the process of analysis and in
the due course of analysis it has
basically converted the text basically
is the stream of tokens and they
basically are delimited to form a single
token each these tokens are then
delimited on the basis of certain
criterias or the rules or the algorithms
which are running at the back end so
falling for an example if you're using
any standard tokenization process and
leucine it is basically going to take
care of all the is the and such common
words it is going to take care of
removing these words from the text
because they do not really add any value
to your text or any documents person so
it is going to remove all of that and it
is going to maintain it is going to just
retain the important terms from that
document which again goes through the
process of token filtering wherein you
might have something like you might have
identified that few type of words I
would always like to filter out from my
content they might be called stop words
if you know about little concepts of NLP
natural language processing so you might
want to delimit or you might want to
remove few tokens from your text so you
might mention that in your algorithm all
together they are they worse along with
a Zonda and common auxiliaries in
English they also would be removed from
your text so the content which remains
is the most important content and that
for that content it is going to maintain
this kind of reference look up is that
clear guys are you following are you
with me okay sridhar seems to have
another question is this leucine works
only with document-based back end can it
work with our DBMS also well we would be
discussing about you know segregation
between I DBMS and what leucine does so
again as I just said I DBMS is basically
record based and leucine is basically
document based
so if I have to really you know kind of
differentiate between them if you talk
about leucine and our dbms you can
consider something like a document in
lucene is similar to a record in your
our dbms so like could you guys tell me
that what is the basic unit of a record
what is there in record what do you have
in records I mean if you're looking for
a result set which consists of few
records yes absolutely columns Thank You
canon so you have columns therein in the
similar manner a leucine document would
have feels there in so like a column
would have certain sort of properties
there in like it could be of type
integer it could be of type date it
could be of type where care here in the
similar manner in case of leucine a
document field is going to be of certain
type like string double int could be
date as well so you have lot of other
field types as well here so you can map
or you can relate to this did that help
shredder along with that so the hacker
has another question so on the
comparison the index would be equivalent
to a table and a document is a row ah
well absolutely index is actually going
to represent a database yes absolutely
so yes they're they're also stored in
the key value pair absolutely so leucine
works on this environment yeah okay I
have an application and it has a DBMS
all the product information is there an
ID BMS example ecommerce catalog so
leucine works on okay are you asking if
leucine is going to work on this kind of
environment is that your question is yes
absolutely it is basically designated
for a huge number of Records as I've
just told you guys leucine and solar are
serving 200 million requests every day
the major reason it is so sought after
is that it basically you know maintains
it basically helps you storing and
indexing huge number of records so
considering if you're talking about
e-commerce website or the catalogue
there in you will have n number of
products there in so definitely it is
the best example it is the best views
case you would you should be using
leucine so let me just move ahead so the
process of indexing basically involves a
this is again from the code point of
view as well so again I'm not I I don't
see that we have enough time to get into
the code details so just to give you a
brief about what is the process of
indexing and how does it really goes
about you have a document like you have
a record in your database in the similar
manner you would have several fields
like n number of fields would be there
in a document which goes through the
process of analysis in the analyzer you
would have the tokenizer and the token
filter and then this process or go this
process and the content the tokens which
are generated from the process of
analysis is pushed to the index writer
and each of these are tokens I then
written on the indexes and then I move
to the directory so these are the kind
of classes and this is the kind of
process which is followed by leucine in
this similar manner the process of
searching goes in the similar manner you
have the text which you enter over here
when we talk about expression and
talking about the query or the search
string you would be entering on your UI
or on your console so this query as well
is basically translated into the
expression which leucine can understand
so this goes through the process of
analysis again so this expression is
basically captured in the query parser
in the term o in the terms of creating a
query parser object a query is being
passed here and this query is basically
analyzed through again the set of
analysis tools so in the similar manner
you have the documents there in which
instead of text and was broken into the
some some sort of tokens in the similar
manner your query is also going to be
fragmented as well in terms of few
tokens then the query object is
basically formed from those tokens and
then they are passed to the index
searcher this index searcher is then
going to consult the directory again and
it is going to fetch the designated
results for this particular expression
which you have provided on the console
or the UI is the process clear guys the
process of indexing in the process of
searching do we have any doubts here
okay we have a question from Deb ashish
for solar using into a production
servers etc though it's capable of
handling huge chunk of data doesn't it
involve infrastructural costs ok so they
be sure we would be discussing how you
can leverage your existing
infrastructure if you want to adopt
solar however if you if you talk about
any traditional application you need to
have you need to have a database and the
database is going to basically work on
any commodity hardware do you agree with
me devashish you need to have a certain
you know limit off a commodity hardware
to make sure that your database is large
enough to hold huge chunk of data in the
similar manner you need to have a
commodity hardware present and you just
need to have the binary present of the
solar I mean it's as simple as that that
that you just have to pick the solar
wall and you just have to put into your
web app folders of your tomcat or any
favorite you know web application server
so again I cannot go into you know a lot
of details there in because we have
limited time frame so maybe if if you
were in my one of my classes we have a
complete topic to discuss how this fits
into your oven to fit into your
architecture so again you can probably
you know hold on to a little bit we
would be discussing
lecture of solar as well so what is
solar in the similar manner we had
discussed leucine again i have given you
way brief way more than a brief of solar
so solar also is the open source
enterprise search server and the web
application when I say you could easily
pick out the word file from the binary
distribution of solar you could use it
as the web application however you can
also use it as the search server I mean
if you just want to maybe use solar from
the existing binary so solar uses
leucine search library and it extends it
for one of the questions which we had
from someone I don't remember so solar
basically exposes the lucy is java api
as a restful services do we all
understand what is the restful services
do we all understand that so we have the
dedicated web services absolutely can
absolutely so we have the web services
there in which are exposed as the rest
so restful services so you have the
Lucille's capability which are exposed
by solar as the restful web services so
you put a document in solar and it goes
through the process of indexing so you
can choose any any of your favorite
format like XML JSON CSV or binary if
you want to send it over to the HTF
through HTTP so you can put in your
document which goes through the process
of indexing and is indexed and then you
can happily look for it as well through
the process of searching so you can
query it via HTTP GET and receive the
response in XML JSON CSV you have other
writer type supported as well in which
you can render your results like PHP you
have results which can be rendered in
Ruby as well so you have lot of
flexibility which solar provides you
okay Kiran is asking me that could you
please explain bit on restful api
services so Karen imagine you have a
capability of sick wearing I mean you
have you know leucine which exposes the
um you know say you are you talking
about querying here so entire
capabilities of querying and entire say
the feature set of querying is provided
by leucine Darren however they are
exposed by solar in the form of restful
services so if you talk about restful
services you need to have a dedicated
you need to have a dedicated custom you
know URL through which you're going to
expose the API so like the query the
querying capability of leucine is
exposed through / query in the similar
manner you have another queering
capability which is delivered by /
select you have custom capabilities
which could be delivered by say anything
like / XYZ and you can configure it in
your configuration files maybe you can
use your custom servlets here or you
could use simple you know functions in
which you can define your own arm
request handler so these are basically
the request handlers so we would be
discussing this a little more in detail
on the architecture so give me one more
minute so the key features of solar is
again for now I think by now we all have
understood that solar provides you with
the advanced full-text search
capabilities because the entire
processing is done by solar itself so
you need not to you know basically
fragment your query as well you just
type your text and it is converted in
the format which leucine would
understand so it takes care of the
entire process is barren
the the entire capabilities are
optimized for the high volume of web
traffic so this was one of the features
were sure we discussed when we said that
what all our search engine should have
so it should be optimized for the web of
web volume high wave volume so solar
provides you with that key feature as
well again it follows the standard open
interfaces like XML JSON and sgp so it
complies by that those as well and along
with that it provides you with the
comprehensive HTML administration
interface as well we're going to take a
small demo on that as well that what
kind of HTML administration interface
which is provided by solar so you can
easily take care of posting the document
querying a document may be looking at
the health of your solar instance as
well through your comprehensive
administration interface however if you
do not want to use that you can also use
jmx as well for monitoring so again we
have a dedicated you know session on
this as well in your course you have the
capability of near real-time indexing
and is adaptable to the xml
configuration so this entire thing i'm
in near real-time indexing and near
real-time querying the data this entire
thing is controlled by the xml
configuration and solar so this is this
provides you with a highly customizable
interface and the capabilities it is
again linearly scalable it gives you the
feature of auto index replication it is
you can extend the plugins you can
extend the architecture by adding on to
the plugins as well so the auto index
replication and the plug-in architecture
is basically supported by the solar
cloud instance so I think we have
another question your solar is web
application GA available for this
application absolutely killin we do have
the graphic user interface as well for
this application we would be seeing that
in a little while so how do we index the
data it is through API call or through
any tool okay Kieran you have low
flexibility in how you really want to
index the data so you have a api api
call as well available if you look here
in my architecture here we have the
update handlers which are similar to the
request handlers which render your
request in the similar manner you have
the update handlers so you can post your
data on onto the HTTP or you have the
backend capability as well through which
you can index your data off route
through the command command prompt as
well along with that you have tools like
a pathetic as well through which you can
index the data from various mime type
like PDF word xml files and say xls
files so you have lot of formats which
are supported so what apache t'kul does
to solar is that it is going to give it
a very extensible you know a feature
that you can use it and you can provide
any type of file to solar so Apache
tikka is basically going to strip out
all the unwanted content from that file
it is going to retain the text content
only which is basically the meaningful
information in any file so it is going
to retain that and then it is going to
index that information is it is it clear
is your question answered chillin so
we're going to now a briefly discuss the
architecture of solar as well so if you
look here we have been discussing about
a lot of things so maybe this light
might be kind of helping you guys to
resolve your doubts so if you look here
and another thing i would like to idea
is that i might extend with the sick
10-15 minutes is it is it okay with
everyone because again I'm handling the
questions as well in between so is it
okay if we extend for say 10 15 minutes
yes no maybe sure okay they were she
says sure okay I guess some yeah I guess
everyone seems to be okay with it thank
you so much guys thank you so much so I
promise you are going to have a very
nice experience
you're going to know lot about apache
solr so okay coming back to the
architecture here so because we do have
lot of questions on where and what all
things are there in the architecture if
you notice we have apache soo Lee as
leucine here which consists of the
course search which is index reader and
the searcher module here it has the text
analysis module here and it has the
indexing and the index writer modular so
all these things the entire and I would
say the core capabilities are provided
by leucine only however solar as I just
said few minutes ago is basically
extending the capabilities which are
exposed by leucine so on top of leucine
if you see we have request handlers here
i would start from here we have request
handlers here so we have / admin which
takes you to the admin console you have
/ select here which takes you to the
query interface you have / spell yer
which is again another example of the
request handler which takes pay a care
of the spell correction of your text
which you are looking for so you have
many more request handlers along with
that as I just said few minutes ago you
can also have your own request handlers
you can configure them maybe with the
custom servlet and you can provide the
list of parameters it is going to accept
and you can clearly have a customizable
interface as well along with that along
with request handlers you have certain
search components as well so query
highlighting spelling starts facesitting
debugging more like this okay there is a
typo here it should be more like this
here and the clustering these are the
search components which are basically
going to assist you with the a query
feature which you're going to access on
your drum solar so highlighting
basically provides you with the feature
like highlighting the term in a document
you which is basically rendered in these
search results spelling is basically
going to take care of the spell
correction of the search string and it
is going to match with the
oh you know spellings which are close by
to the search which you are with the
text you are looking for these starts is
basically going to provide that how a
particular document came on the first
position and how a particular document
came on the second position so you have
all these starts so this is the search
component which is going to take care of
all these starts visiting again as we
saw on our screen shot we had certain
grouping done so the price range say of
two thousand to three thousand was
nothing but basically your face it so
there is a little difference between
grouping and facesitting maybe it is
again out of the scope of this
presentation so again you can as of now
you can take facesitting as another
variant of grouping more like this is
again the kind of suggestion which
leucine provides you with like if you
type say maybe like maybe you have a
document in your index store which says
America however when you look for in
your search query you type some
something like in Erica so it is going
to basically first of all the spell you
can basically have the spell checker
module which can correct the spelling to
America or you could have the capability
of adding more like this feature as well
would say that this particular term
looks to me like America so are you
looking for something like America here
or are you in particularly looking for
America I'm sure this kind of feature
everyone must have seen on Google do you
guys remember when you look for
something in google it would say do you
mean this so this is the same kind of
capability which is provided through
more like this search component okay
then you have the capabilities of
distributed search here so this
distributed search capability is
basically taken care of by solar cloud
so we would be talking about solar cloud
as well in a little while okay then we
have the request writers here which is
basically going
take care of how your results basically
appear on your UI so do you want to see
your results in XML format or do you
want to see your results in the JSON
format or you want to see your results
in the binary format so this entire
thing is taken care of by schema and the
configuration file so you have to
foremost configuration files the schema
or XML and the solar config.xml which
play and you know help you configuring
eighty percent of your solar instance
behavior on your architecture or your
application then you have the update
handlers so update handlers are going to
take care of the beta updates you're
going to sorry Delta updates which are
going to make onto your server so like
you're going to add more data on your
search over so they are going to be
taken care of by update handlers so you
can provide the data in the XML format
or CSV format JSON format or the binary
format so they're going to take care of
the updates which are going to make on
your soul server update processes are
basically going to take care of the
processes like signature like you want
to ensure that this security is another
feature you want to take care of so you
can configure like you're capturing the
payment information and you would like
to match the signature of the
information so you can have those kind
of features as well you can have logging
and indexing processes as well which
would be again assisted along with the
update handlers along with that it
provides you with the query parsing the
analysis feature this these features
like facesitting filtering search
caching and highlighting are similar to
the ones we have in the search
components so these are the ones we have
added these again here because these are
the ones which are going to take care of
the content which is returned by leucine
because the results are basically
returned by leucine only so on these
results you can have the features like
facesitting you can have the features of
filtering you can have the features of
searching caching and highlighting on
these search results along with that we
have apache tika here which is basically
going to take care of the extracted
of the content from various mime type
files like PDF word is one of the
examples we have taken here you can also
ensure the information extracted from
jpeg files as well you have you can
extract the information from XLS file
files as well then you have a very nice
feature here which is called data import
handler which is basically going to take
care of the importing the data from the
existing data sources or the RSS feed
garen and you have another nice feature
of index replication which I said is
supported by solar cloud so any doubts
till here guys okay so can all say is I
see index replication with the index
size we limit it to the storage memory
available on the in front absolutely
canal I mean the amine of the index
replication definitely would be
dependent on your infra plus the
replication factor which you are going
to define so again you need to choose
the replication factor wisely for the
infrastructure which is available to you
or there is some way that solar can
distribute it mixes on multiple boxes
the way other distributed technologies
such as Cassandra does so yes definitely
you can use the capabilities of that as
well we are going to explore the
possibility of using such kind of things
as well along with solar so maybe so I
guess so we're clear about the search
process so maybe you can talk about a
user and the user basically accesses the
search features through any of the
request handler the request basically
goes through the query parsing process
which is basically going to convert the
query into something which solar can
understand which is then going to
consult the indexes which are stored on
your community hardware and then you can
have the pagination features as well so
we're here we are talking about qf here
in which you can possibly have a feature
like select maybe first name last name
and department
from employee table so query feel is
basically going to take care of how many
fields are basically wanted in your
search results so what all fields are
you basically looking for in qf so these
are the query fuse the dev type is
basically going to take care of how your
query is going to be parsed so again we
have the dedicated classes on this in
our course the start and rose number of
rows is going to be taking care of the
pagination features on your website the
fq is again another feature which works
on your complete result set like you
would want to have additional say
filtering of data so the fq is basically
filter query which is going to be a
operational on the complete result set
which is rendered by the indexes and
then you have the response writer here
in wherein you can define w t equals to
maybe XML or JSON and you can basically
return the results in the format you
would like to see them on the UI so near
real-time search as though we spoke
about so this means that the documents
are available for searching almost
immediately after being indexed so as
soon as you push the document inside the
indexes they're going to be available
for the searching as well so maybe if
you push the documents say on our
instance and immediately you query
something in which the criteria matches
with the document you've just indexed
the document basically is going to be
rendered to you so these this is
basically controlled by the again the
configuration the autos of soft comment
term property they're in so you have
soft comments and you have hard commits
so soft commit is basically going to
talk about the visibility of the
documents and the hard commit is
basically going to take care of the
durability of your documents or your
data then similarly you have the
real-time get which basically lets you
query and get the document of your
choice by a unique ID which is assigned
to each of the document so if you know
the document ID of any document which is
basically generated or
the runtime and it is done by you can do
it manually as well and if you're not
providing any document ID per se solu
scene is going to assign one document ID
to it however if you know your document
ID you can basically get that particular
document through the document ID the
latest version of that particular
document so this is primarily very very
useful when you're using solar as a no
sequel data store and not just a search
indexer so if you look here we have
real-time get here so maybe we can take
our example on this so I'll just move
ahead we can probably take an example on
this okay I think we have few questions
here let me just start a couple of them
facesitting is similar to filtering
search like brand filtering in flipcard
absolutely so you can say that face
thing is nothing but a type of grouping
so probably you would like to have you
know your results Gruden on on the basis
of certain criterias so bryan could be
one of the criterias so yes it means the
same thing so the Sri there seems to
have another question do we use leucine
or solar together or leucine could be
used with any other like solar well
leucine is used by elastic search as
well just to let you know however if
you're using solar definitely you're
bound to be using leucine because at the
back end it is using leucine however you
can use leucine independently as well
which is going to be very very tricky so
again it is kind of recommended to use
solar our joints recommended in this
architecture not really so Tucker I mean
in the similar manner Roman I'm sure you
guys are pretty you know pretty familiar
with a Hadoop so I'm sure you know that
a denormalized type of structure is
basically recommended for Hadoop in the
similar manner Solar also recommends a
flat type of structure so however you
can definitely draw the joints I mean if
you want to see and if you want to have
relationships define definitely you have
a way around so again we have a
designated session on this as well in
the course so um okay so we have another
question here my ecommerce platform will
be in PHP and my sequel so how can i use
leucine in my application and what is
equivalent to solar for PHP application
well as I just said the shredder or
shredder I guess your name is written as
say two so I'm not very sure how do i
basically address you so let me just
address you with say two only so as I
just said you can have your results are
returned in the designated format so
maybe when we take the lab excise when
we take a small demo Darren this might
be more clear that what all formats are
supported so solar is something which
can help you when rendering the results
in PHP format as well so we would be
seeing that in the demo part is it fine
say to if you still have any doubts
maybe we can clear that up after the
demo okay yes no maybe okay sure
absolutely so then we have the solar
clouds so I guess this is kind of being
waited for quite some time people have
been talking about solar cloud so what
exactly is solar cloud so solar cloud is
it gives you the ability to set up a
cluster of solar servers so it is
basically you can imagine the solar
capabilities in the clustered
environment it combines the features
like fault tolerance and high
availability of your instance so imagine
you have you want to have the
capabilities of solar in your
application and you're working in the
clustered environment so solar cloud is
something which is way more recommended
for you because that ways you can ensure
that if any of your know
get some down you have the automatic
failover and your instance is going to
be highly available I think these are
two for most things anyone would look
into when they're working in the
clustered environment so both of these
concerns are addressed in the solar
cloud environment then we have solar
cloud which is very flexible in
distributed search and indexing so it
works without master node to allocate
nodes and charts and replicas so maybe
if you guys are not very you know
familiar about the terms which sure we
might be using here just to ensure that
everyone is there on the same page so we
have the traditional master in the slave
architecture so solar cloud does not
really works on that kind of
architecture we have first of all we
have these zookeeper instance i'm not
sure if every one of us are aware of
what zookeeper is are we all aware of
what zookeeper is so in brief i would
say that it provides you with the a
centralized configuration or the cluster
state management so it is going to take
care of the leader election basically
and as of now if you're wondering what
leader is so leader is again another
type of node in your cluster which takes
care of your write requests then you
have replicas which are the copy of the
shards in a collection so maybe you
might be wondering what exactly shard is
so shard is a logical slice of a
collection so we have been talking about
few terms here okay we have few
questions as of now here so let me just
quickly cover this slide and then I can
come back to the questions so we are
introducing few concepts here guys if
you can pay a little tension we have
introduced zookeeper here so are you
guys aware of what zookeeper us
skiing again so zookeeper is something
which is going to take care of your drum
okay not sure okay so it is going to
basically take care of cluster state
management and say it's izzy centralized
i would say centralized the system which
is going to take care of your cluster
health as well then we have leaders here
before going on to the leader let me
just explain the basic part then we have
the collections iam so what exactly is
the collection we have we've been
talking about the indexes here so these
indexes are basically distributed these
basically they're basically distributed
on multiple nodes so the indexes with
the say same configuration are called
collection is it is it clear are you
guys following what collection is so
collection is nothing but you can indeed
very layman term you can call it as
indexes so collection is nothing but
index then you have shards so shard is
say a logical slice of this index
logical slice of collection then you
have a leader here leader is basically a
node in charge for writing the data
basically involves in indexing then you
have replicas here so replicas are
basically oh say Oh replicas off of
shards here so they are basically
responsible for search rendering these
search features on your application and
then you have nodes here so I mean we
have been talking about leader and
replica so these are basically the nodes
so node is nothing but a a JVM process
which is bound to a particular port port
number
is it is it clear guys okay collection
is confusing say to says so collection
is nothing but indexes which are
generated by solar is it clear now so
indexes which are generated by solar is
called collection is called indexes so
when you talk about a collection these
are basically the indexes which are
designated for a particular node
particular instance I would say and then
you have the overseer so these all these
terms are basically pertinent to the
solar cloud environments so overseer is
basically a special node i would say
which basically takes care of the
cluster administration so it is going to
take care of your zookeeper so it is
going to basically take care of this so
maybe when we are going to discuss the
solar cloud architecture maybe this is
going to be more clear then shall we
move ahead so we have the architecture
here maybe this is going to be more
clear now ok so the hacker
King where does the directory fittin in
this explanation okay director you
talking about the filesystem directories
Ibaka so I'm not sure what your question
is okay oh let me just go back to the
previous slide okay the directory
analyzer okay okay that directory
talking about okay the directory which
I'm talking about directory was
basically the class so they occur I mean
you you can have the file system
directory you can have the ram directory
as well that totally depends on where
they are you planning to store your data
or your indexes so that is what I mean
directory you can talk about is a class
from Java point of view so directory is
any place where you going to store the
indexes so you have various type of
directories there in so you can have a
file system directory you can have the
network file system directory you can
have the say Ram file directory as well
so over here i'm kailyn saves a please
explain again so killin if you look here
this is I mean maybe that is why I'm
saying it would be more clear if we look
into the architecture here so imagine
you have millions of documents here if
this the terms which I've introduced I
would not get clear you can let me know
I mean I'm just trying to map the terms
which I have introduced on this
particular architecture so maybe you
guys would be would be able to follow
this now so imagine you have millions of
documents which are coming in and you
have millions of users which are
basically playing around with these
documents so in these solar cloud
environment you're going to be accessing
the API switch are exposed the restful
web services which are exposed through
XML JSON or HTTP so the millions of
millions of documents also go through
the process of indexing through this
restful Web API ice and
the users who are querying or using
these documents also use the similar web
services now these web services
basically talked about this I mean I'm
talking about when I'm talking about are
the rest web services I'm talking about
any say solar cloud instance so this has
exposed few web services here and this
is basically going to take care of this
basically going to take the request
which comes on this particular say web
service and it is going to consult the
load balancer as to which service which
web server or which server or which node
basically I'm supposed to be sending
this request to so you have as of now
you have two servers here sober one and
server to which have instances running
on to port numbers if you look here 898
four and eight nine in five so you have
leader and you have the replicas here
the process of replication creates a
replica here for this particular
collection is it is it making sense guys
and in the similar manner for collection
too you have the leader here and the
replica here which also goes through the
process of replication so the
replication factor of this particular
instance is one ok is low
load balancer part of zookeeper vivek
asks me so load balancer is not the part
of zookeeper vivek so i'm talking about
any traditional web application as of
now here it is not part of the zookeeper
it is part of basically the solar cloud
so who is basically supposed to get the
request is basically taken care of by
the solar cloud because imagine we have
the centralized configuration management
system here so this is basically the
overseer I believe somebody had the
question Kieran had the question that
what is over here so overseer is
basically going to take care of the
centralized configuration management
which is going to have more than one
instance of zookeeper as well just to
ensure that zookeeper should also not
fail okay say to say is what are
precursors of this course we are going
to discuss that say to please give me a
few minutes we would be discussing that
as well so are you are you following say
to are you in sync with what we are
discussing as if now the architecture so
this is the overseer Kieran we have the
centralized configuration management
here which is going to hold multiple
instances of zookeeper just to ensure
that zookeeper should also not fail so
zookeeper is basically going to take
care of the leader election and it is
going to basically choose that which
port or which particular shard is going
to be chosen as leader so it is going to
basically monitor the help of each node
is it is it clear is it making sense
your do you still have any confusions on
the architecture any confusions okay say
to say a lot of confusions okay all
right so ken has another question
millions of document match with
centralized configuration management I'm
not really sure what do you mean okay
I'm really not sure killin could you
please paraphrase your question once
again okay millions of documents and
centralized configuration management
you're saying are same is that your
question is no not really millions of
document is the one which are supposed
to go inside for indexing your data your
raw data which is supposed to be next
those documents I'm talking about and
centralized configuration management is
internal part of your architecture is it
clear now are you are you clear about
the segregation or so this is this
million documents is external to your
environment they're not part of your
environment until they go through the
process of indexing and come and sit in
your collection another question from
silica Reza did not understand why shard
as a leader okay well not really shot I
mean we talking about a node if you look
here we have node one on port if you
look here this this is what i meant
shard is basically not the leader so
this is just for the correction here is
it is it fine any any confusions now I
mean I'm talking about leader no dear
leader no dandy replicon audio and
basically it is going to consist off any
I mean a logical slice of the data of
the collection so that is why we have
mentioned that information here so let's
not get into a lot of details as of now
maybe if you join the course we have the
designated three hours to study this
architecture only so let me just move
ahead I'm sure a lot of us know about
Hadoop if you still have any confusions
here maybe you can send in a male we can
clear more doubts through your mail so
considering we would have a say too it's
not the my email ID apparently that is
not part of the agreement I'm not
supposed to be sharing my email ID you
can drop in your questions on Apache
Solr a tear Eureka so so I guess a lot
of us are a pretty aware of Hadoop here
so how can we leverage the capabilities
which are delivered by Hadoop when we
are using solar so what we're trying to
do here is we are we are trying to
basically leverage the capabilities of
solar on a system which is basically
using Hadoop iam so we know that search
I mean is before most say expertise of
solar so solar provides fast and
efficient and powerful text searches and
that win very less time and solar cloud
is flexible enough for supporting the
distributed search and indexing so it is
going to take care of all the things
like automatic failover etc so when
these things are basically a set up in
the clustered environment you might want
to have this cluster to have Hadoop as
well so it can act as a very nice
substitute for the no sequel as well so
instead of using any traditional
databases you might want to use solar a
Hadoop therein so what you can do over
there is that you can leverage the
searching capabilities of solar and
instead of storing your indexes in the
file system you can store them on Hadoop
and you can basically run the MapReduce
job on your indexed data there in just
to achieve the optimized storage of your
indexes so this type of architecture
basically is used in cloudera
Hortonworks and map are so where you can
integrate solar and Hadoop easily so let
us quickly take a look at the
architecture as well so if you talk
about what I'm saying is that you have
the
put data which may come in any format
like PDF word HTML XML JSON and imagine
that goes through the process of
indexing so on this arrow imagine you
have the process of indexing going on so
these indexes are then dumped into HDFS
so on one part you will have the raw
files and on these raw files of the
indexes you would run the MapReduce
index jobs so basically they're going to
be stored into the more organized and
more optimized manner and they basically
converted into the more drum say I would
say just just to ensure that you can
store more number of indexes on one
single HDFS the file system so similarly
whenever you're going to have a feature
of say searching in your web application
you can interact through your solar
instance and instead of consulting
through your file system directory your
solar is basically going to query the
HDFS and solar is basically going to
query through lusine so your query and
responses are basically going to be
taken care of by solar and the the
indexes are basically stored into HDFS
is it making sense nice ok Underoath
says could you explain the slide once
more Sean so let me start with this
again you have the index data you have a
raw data i would say you have a raw data
which can come in any format as we have
discussed it supports a lot many type of
data formats so on blue or arrow imagine
you have the process of indexing wherein
the tokens are generated these tokens
are then pushed to the file system where
the indexes are supposed to be stored so
these indexes instead of storing on file
system we are saying that we are going
to store them on HDFS so these raw files
would be stored by solar and you will
have the MapReduce job here which is
basically going to take care of the
optimized storage of these indexes and
then where you query you
basically query through your user
interface here and this entire thing
this entire interaction has been taken
care of by solar which in turn is taken
care of while you seen so instead of
consulting the file system for indexes
you would be consulting HDFS is it now
clear I knew it you still have any
confusions yeah anytime so HDFS store
only index data it will maintain the raw
data and index datum that's what the
Killens question is absolutely so it is
not going to basically store the raw
data draw index files maybe the slide
might need a little correction here so
we are storing the raw index files i'm
going to show that how raw index files
look like so let me just quickly show
that to you guys so if you look here
these are the collections which i am
talking about so these are basically the
course cor es course so like I've
maintained a core here so every code
needs to have a configuration and every
core needs to have the data in this data
directory we have the indexes so I am
saying instead of storing these indexes
on the file system I am trying to push
these index files onto the HDFS and then
HDFS takes care of optimizing the
storage of these files is it is it now
making sense guys okay great so this is
how my normal instance would look like
so let me just take a quick demo as well
at this point in time so if you have to
start a simple instance of solar you
would simply say bin solar start so this
would the by default start on eight nine
eight three port so if you look here we
have the
which starts and it is going to load all
the course you have available here so i
have 1 2 3 4 5 i have 5 course available
on this particular solar instance one
single solar instance so I'm supporting
five cores here so five courses and I'm
supporting five different type of
collections here and these collections
if they are working on any distributed
environment oh they can have they can
sit on different nodes different shards
basically so this is what I am basically
meant so these index files are basically
distributed over different shards so a
logical range would be defined like from
this particular hash code to this
particular hash code they're going to be
there in one shard so this is basically
more relevant only distributed
environment as of x is running solar in
the normal mode so let's look at how
this looks like so if you look here this
is my solar console and this is how my
course would look like so all my course
would come here imagine I have a lot of
data on this particular core here so I
have 59 documents here and you can look
at all these starts for each of these so
you can query the documents as well and
if you look here you can push in the
documents in all of these formats you
can basically upload the file as well
here all of these would be sent to the
slash update again another request
handler so let me just try pushing one
document demo session say 18 jun and I I
need to have this particular field in my
schema you have lot of other
capabilities like dynamic fields in
which you need not to define any field
you can run solar in the schema-less
mode as well where you need not define
any
sort of field it is going to construct
the schema on fly so imagine I have
something like this and textures say
anything like demo of solar document
pushed for indexing so I'm simply going
to submit the document it goes inside so
let me just see if I want to look for
something like this I simply go to the
query console so this is like star dot
star on your normal database however it
also supports the fielded searching as
well so likewise I want to look for demo
session 18 jun it's kind of idea i'm
looking for so it is simply going to
render this document to me so did you
just see i mean i've just pushed the
document and it is readily available for
me in these search results so this is
how fast it is actually is it is it
clear guys any confusions yes we are
simply going to go ahead and cover the
yarn as well ne ne ne guys any idea
about guys what yarn is yet another
resource negotiator so this is
introduced with Hadoop true and as we
know that solar basically takes care of
the a dork queering capabilities yarn is
basically going to take care of the you
know separating the layer wherein you're
going to organize and assign resources
to a particular job and scheduling of
job so basically it is segregating the
resource management and the job
scheduling in a Hadoop cluster so you
can very well use yarn in solar as well
or other other ways I mean using solar
in your cluster which is managed through
yarn so if you look here I think I have
few questions a collection is equal to
the entire product catalog in the
application where
that's kind of the segregation you can
have say to if if you're talking about
product catalog maybe I can have I can
store it or maybe the electronics data
on one collection I can store the
apparel data on one collection I can
have the furnishing data on another
collection so these this kind of
segregation is decided by the business
or the stakeholders or I mean the
product owner I would say yes Colonel
I'm going to cover yarn right away so
we're going to get optimized storage
using HDFS or we get speed also Vivek is
asking me in terms of response so again
we make in terms of for storage I'm
saying the storage is basically
optimized for usage in HDFS however the
speed is anyway taken care of by solar
is it is it clear so your catalog is an
RD Bemis which is fine you can happily
export this catalog from I DBMS as well
through you assume by using data import
handler so let me just go ahead and
cover this light as well quickly I'm not
going to go in a lot of details so as I
have just explained that yarn is going
to take care of the resource management
and the job scheduling has been taken
care of separately so imagine if you
have a yarn cluster here and you have
HDFS which is here so imagine your
indexes are going to sit on your HDFS
and you have the java application which
is going to basically trigger the master
application so essentially if you look
here you need to have a yarn node
manager on each of the nodes in the
cluster and in each of the yarn
container you would have separate
processes so like I have solar master
rap in this particular node and I have
solar say a replica or drum solar say a
slave application here I'm taking
example of say say sentiment analysis
application so I'm also taking example
of spark here so the basic job of spark
in
storm is to basically gather the data so
we're just talking about gathering the
data so I need to have a parallel
process which gathers the data and I can
have another process which is stay or
taking care of firm indexing the data
say another process in solar which is
taking care of indexing the data and
another one which basically schedules
the MapReduce processes so if you
comprehensively look here I have a
number one job is basically to fetch the
data so this could be from your feeds
this could be from your FB context or or
other status you call them as the other
one is basically going to be the solar
indexing process or solar indexing job
the third one is basically going to take
care of the say I have HDFS MapReduce
job or say big job and the fourth one is
basically taking care of your search
capabilities again delivered through
your solar so imagine what yarn does to
this kind of cluster is that it is
basically going to manage that the
resources are correctly organized and
managed so yarn is very simple in terms
of the you know few commands which you
need to run so the main benefit of
running you know this kind of
distributed application like we're
talking about solar running in
distributed mode in solar cloud mode so
the major advantages that you can
improve the operational efficiency so
you're separately handling or separately
taking care of your data which is
getting fished your indexing is again
getting taken care of you have the
MapReduce job as well so the resources
are basically managed and the job
scheduling is not supposed to be done
manually this has been taken care of by
yarn and this entire thing has been
taken care of I mean because we're
talking about solar running in cloud
mode we need to have the zookeeper
instance which is going to take care of
solar being highly available which is
basically going to take care of all your
abilities of a solar cloud like the
replication factor and the automatic
failover is it is it clear guys so
basically what we are trying to achieve
here is that we are going to we are
basically trying to achieve a digital
computing capacity to an existing
cluster by using yarn here so solar
could be one of these sub processes so
imagine we are trying to view solar as
one of the processes and we are going to
basically use solar as in the
distributed mode so imagine you are you
know leveraging the capability is off a
solar cloud for providing for providing
the distributed search and that too is
taken care of basically in the clustered
environment is it is it clear I'm not
going to go in a lot of details here
because I see that the people are
getting kind of confused I say those
asking me is Hadoop Magna tree to join
in this course no not really say too I
mean this is just an example I mean if
you know Hadoop maybe this is again
another ad on okay say who says you're
already giving lot of details so no more
details as of now I'm not giving any
more details so and it says example to
explain yarn please well i'm not sure if
sentiment analysis example really helped
you and i wrote okay okay great so all
right again the foremost and the most
important thing for you guys so the job
train if you look into is ever growing
in apache solr so how does you know
Eddie Arreaga works i'm not sure if all
of you know how a dirac of works so we
have the classroom sessions we have
three r's on saturdays and sundays so
these classes are basically the live
classes and if you enroll in this course
i would be taking you through that so
the class recordings are basically
available in the LMS which stands for
loan
management system so they basically
uploaded them for the lifetime access so
you can anytime go back and check into
the recordings and we have 24 by 7 post
class support which is provided by
amazing you know engineers we have in
here then talking about the material the
study material we have so we have very
comprehensive study material which is
provided when you join the course so you
can access the sample slides so this is
the kind of content we deliver then we
have module buys quizzes as well just to
ensure that the learning is going on the
right pace and along with that we have
the project work like any other course
might have so you you need to complete a
project work as well and then you get
the certificate as well which you can
share on your LinkedIn and it's pretty
much verifiable you can use it and you
can look for the job as well on behalf
of it so just to give you a brief
insight of what we cover in this course
we cover as we said that we are going to
add a dedicated Lee going to study
leucine in the first two sessions so the
first session is going to talk about
introduction to apache Lucene and
exploring leucine and then you have
introduction to apache solr and how
indexing works house searching works in
solar then we are going to discuss about
the extended features as well like what
all you can really achieve then you have
solar cloud and administration which is
a separate module all together and then
you have the final project module so
these are the modules in the in this
particular course and if at all you're
taking admission into this course yes it
is the only online course say through it
is going to be delivered in the same
medium as are we having this session so
if you go ahead and roll right now after
the session I mean not a limited time
offer as such but then there is a
exclusive twenty five percent discount
on the attendees of for the attendees of
this particular session on Apache Solr
course so to avail this you can probably
reach out to these numbers
so okay let me just quickly go to the
questions okay so what are the
prerequisites to understand the course
well you need to know a little bit of
Java and databases that's about it and
obviously if you are using computer and
you are working in as engineer somewhere
I mean you would just be ramped up as
soon as you join in so the duration is
of one month eight weeks basically I'm
sorry eight classes so four weeks you
have classes on Saturdays and Sundays
Saturday three hours sunday three r's
for four weeks so eight classes total 24
hours is what the duration of this
course well you can look for the price
on the website say to I have another
question from any road thus zookeeper
keep an eye on the health in all nodes
and if one of the nodes is
underperforming it changes the leader
selection absolutely android which is
correct your understanding is absolutely
correct so say that we do not conduct
daily classes this is basically
designated for the working professionals
so this is basically arranged on the
weekends you have classes on weekends
basically so are we all good nice if you
still have any questions feel free to
drop in your questions on solar at the
rate at eureka co what time in the
weekend okay Oh Vivek is asking me so
you have two slots you have morning slot
and you have evening slot so you can
mode you can get more details on
contacting the numbers which we
mentioned on the slides all right so I
believe we had the nice fruitful session
do provide your feedback guys it helps
us improving the curriculum and
ourselves it was Grady interacting with
you guys looking forward to have you all
in these sessions and in my class thank
you so much have a good night have a
good day thank you bye bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>