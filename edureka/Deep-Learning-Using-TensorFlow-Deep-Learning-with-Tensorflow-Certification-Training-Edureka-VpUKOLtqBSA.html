<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning Using TensorFlow | Deep Learning with Tensorflow Certification Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Deep Learning Using TensorFlow | Deep Learning with Tensorflow Certification Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning Using TensorFlow | Deep Learning with Tensorflow Certification Training | Edureka</b></h2><h5 class="post__date">2018-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VpUKOLtqBSA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is your train arrival Francis and I
want to quickly introduce to you about
my background before we get onto this
session I have about 10 years of
experience in the field of analytics and
artificial intelligence I started my
career in 2007 I started working with an
investment bank Lehman Brothers I was
part of the Global Risk team and part of
my job was to actually help estimate the
risk most of the accounts were exposed
to and moved to I to decisions in
Chennai and that's where my current role
is as I to decisions I'm heading the I'm
leading the offshore delivery center
completely in terms of delivery of
projects and also in terms of the
ability to come up with solutions for
our claim our center of excellence team
is working on AI projects we are doing a
lot of projects from chat pod to text
summarization to image classification
all of that using deep learning and
tensorflow so we are at a very very
exciting stage at this point in time
where we are using tools like tensorflow
keno and Carols and techniques like deep
learning techniques like CNN our NF LS
teams and sequence to sequence models
which are very very important in terms
of being able to actually create some
very solid use cases which are very very
helpful especially in terms of
businesses and their ability to solve a
lot of problems so that is what we are
going to talk about a very small part of
it which is probably deep learning in
fact our sessions or our courses that we
do in Algeria have a focus on
understanding and going through all of
these concepts in the depth and try and
understand how this can be implemented
across use case and one of that is what
you are going to discuss today which is
basically understanding what artificial
intelligence is in the applications of
artificial intelligence most of you must
have read about it but I want to call
out some very specific use cases then
within the subset of artificial
intelligence there is this one important
part which we call as machine learning
and we want to clearly outline what are
the differences between machine learning
and deep learning and deep learning
comes through something called as neural
network and we will also look at a
hands-on use case of neural network
right at this point in time I see couple
of hands raised I just want to check if
there are any specific questions let me
see who's that our so similarly any
questions at this point and it sees even
has raised their hand and
I'm guessing at this point any questions
right Simon please feel free to use the
questions panel as well if you have a
question at any point in time just to
let the audience know if you can please
put your questions I will take them up
immediately but in case you know I am
NOT able to take the question I promise
you I will definitely get back to the
cliff's in before the end of the session
but at any point in time if there is
something you need a question even
clarification on please put that in the
questions manual I'll try and get back
to you immediately but in case I don't
definitely I will get back after having
discussed this specific point and then
probably I will probably take up the
questions so somewhat good question
which is recording be available after
the session I do have the moderator here
I think the team from Erica is also
available I'll definitely ask them to
get in touch with you some up and
probably see how we can work that out in
terms of the recordings or any other
information that you'd like to get in
terms of the current session that you're
doing all right I hope that answers
requests in decimal all right so in
brief we just discussed about the agenda
for today but let me actually start off
by talking a little more in detail about
artificial intelligence in fact that is
the reason why we are in this session
today in expect especially at a time
when yeah IPL match is happening
actually listening to the session on
artificial intelligence because the
reason is that we are at the cusp of
technology I would say or at the we are
at the beginning of a new revolution if
I might call it and this is actually the
right time because we have all the
computation power we need we have all
the techniques that we want and we have
all the data that you need that is why
you know artificial intelligence has
become this new word that is constantly
used for almost every other industry
remember the objective is to try and
automate a lot of the regular work and
you don't want to explicitly call out
all of these events and automate them
but rather you would like an intelligent
mechanism by which all of these happen
lot of people generally don't watch this
question during the start of the session
is hey why is artificial intelligence
all of a sudden such a big thing people
who come from machine learning
background usually ask the questions
that
hey aren't we doing machine learning
weren't we doing this for the last 5-10
years and how is that now artificial
intelligence takes a lot of you know
important role and has got the ability
to actually get so much amount of
limelight in the presence in our job so
the reason why that happened is
initially we didn't have the amount of
data so data wasn't being collected to
the limit or the extent to which we
wanted it for doing some of our analysis
or to do some of the automations now
with the ability to scale all of the
data using cloud you can store as much
data as you would like to so it could be
in tongues it could be in GBS it could
be in you know terabytes and Mulkey
terabytes that you could think of the
other part of it is the processing power
about it right so all this data is
available now now you need something to
process it and again clouds give cloud
as giving you the solution in terms of
being able to use all of the
computational power available within
some of the vendors like AWS or probably
as your they're giving you so much power
that you can actually do all of these
computations the remaining part is the
application of the technique itself and
that is where I think data scientists
like us are actually the need of the
other where we need people who can
actually analyze all of this data and
use all of the computational power and
that is where our focus on the session
is also to be able to help students are
to be able to help our participants in
terms of the ability to understand the
different aspects of deep learning and
how they can exploit all the
functionalities available within tools
and technologies outside to be able to
create some really cool automation and
as mentioned artificial intelligence is
the ability to you know intelligently
understand data and then be able to take
action against any provided data so
that's what we're trying to do something
that is intelligent generalized and does
not become too particular or focused on
a specific use case dependency or not
official intelligence so we're just
going to briefly pick pull out some very
important to use cases there artificial
intelligence has a lot of dependency and
one is AI for code where Stanford the
team at Stanford is actually analyzing
satellite images to be able to identify
specific areas or specific region
where there is a lot of poverty or areas
which are actually being denied some of
the basic human necessity so what we are
doing is we are using AI not for just
you know profits and whole lot of
enterprise level projects but rather for
very important things like trying to
find out through satellite images if
there is existence of poverty and how we
can overcome some of those things
Aviation is something that's been using
AI for a long time from the mechanical
part of it they've been using it to
analyze engines and predict failures and
also at the same time try and create
efficiency for airlines and be able to
you know create a bottom line growth for
most of the airline so that is one
aspect of it and then there is the
business aspect of it where there is the
ticket price determination which is a
dynamic pricing model trying to predict
what is the right price the user will
pay and make sure that you provide the
right price so that you have a
conversion in terms of aviation aspect
of it education is something where the
important thing about application of AI
comes in there because there are a lot
of projects where people are trying to
actually understand the different
characteristics of students and then be
able to actually customize some of the
content that is there for for these
students now there are a lot of things
that are happening in terms of education
in fact there are very successful
projects that have been implemented and
Google is also coming up with its own
tool that is more focused on education
for all which is basically trying to
make sure that there is a very
customized learning and teaching
methodology that can be implemented for
students to you know enrich their
knowledge through a personalized
interaction with the systems in terms of
health care I think you know I don't
even need to talk all in length about it
I just save one word which is probably
everybody's heard of which is basically
IBM Watson and that's an artificial
intelligence framework that has been
able to garner so much amount of respect
and so much amount of you know
efficiency in terms of its ability to
work on healthcare related projects in
very simple thing in fact this is
varying
testing use case I encourage all of the
participants to actually go to is to go
and read about how IBM Watson is
actually implemented at in New York for
the low-end captioning which is actually
a Cancer Research speciality and a
hospital that's focused on curing cancer
so you can actually see IBM Watson has
been able to help a lot of oncologists
in terms of trying to find out first
from the diagnosis perspective being
able to read through scans and reports
and be able to predict the chance or
probability of a person having cancer
and the next thing is once it detects
cancer it can also on the basis of your
data help and identify what is the best
path or the best treatment plan that
should be the remember cancer is very
very specific so the treatment plan that
is for a particular person changes from
people to people and you need to have an
algorithm that can identify people and
with less amount of error be able to
help out people in identifying the best
possible you know cure or best possible
plan in terms of the treatment to help
them recover fast and be able to do well
in life as well so that's where
healthcare is and I'm not going to go
much into it because I know a lot of
people must have read about it and if
you not I would encourage you to read
about it you will really find it
interesting and some of these are path
breaking technologies because remember
these are impactful projects that are
actually changing the whole mankind at
this point in time and that's what we
all aim for in life right to be a part
of a project that actually changes the
very perception of how society looks at
a particular technology or to in terms
of heavy industries there's oftentimes
the most important thing has been talked
about is automation of you're removing
some of these things and I'm very
interesting use cases about the offshore
drilling centers where a lot of metrics
are captured in terms of extraction and
failures of machines and that's where
you know you have very smart and
intelligent AI AI tools that are
available that can actually detect
failures and the other things are in
terms of repetitive projects that are
done which can be actually automated
through a learning process through data
you know you can have a model actually
understand and predict exactly what kind
of action needs to be done
from heavy industry again there's a lot
of use case and then from the
immense aspect of it I think the trading
part of it is wonder you know time
immemorial ai has been a has been a very
important concept in terms of it it's
also in terms you know
algorithmic trading and risk and a whole
lot of things that actually come into it
is where artificial intelligence has
been doing a lot of job in terms of
Finance I think there are enough in
number use cases that are there outside
and as far as we know liking tensorflow
is something that's widely expect
accepted in the in the area of finance
itself considering because tensorflow
has the ability to process such high
data or high frequency or high intensity
data if i might use the word and that is
where the you know important thing about
tensorflow and also a deep learning
comes in take a quick pause here and
before i proceed to the next slide I
actually want to quickly check with the
participants and see that a lot of
people are very quiet this if there are
any questions that you know people have
at this point and I would like to take
them up and then proceed to the next
slide before you know moving on to the
next slide itself any questions at this
time
good all right no questions I'm hoping
the students are finding it interesting
and the participants are finding it
interesting all right
or I'm guessing all of us all of us are
waiting for that one moment where we can
actually see AI in in action so I'll do
that good question here from Robert
which is when do you decide a business
case needs a I I think the the most
important thing is to look at you know
how much data is that the in the built
Rohit so I'm going to put in a important
term and condition if you might call
that particular part of it which is that
the implementation of AI depends a lot
on the type of data that you have so if
you want to completely automate an
entire process it completely depends on
the amount of data that you're already
capturing but considering now we are in
the world of open data a lot of data is
available on the bed and the north of
data is also available
on other sources data does not seem to
be the issues but rather you look at the
ROI of implementing in our way I mean
implementing in AI sorry and you have to
look at you know how much cost is going
to be involved in developing any a
solution by replacing let's say a
process being driven manually if the
manual process outweighs the AI solution
then that indicates a reason for moving
to AI base methods also remember one
more thing is that AI does involve cost
running on cloud and also in terms of
the amount of data that you're going to
store all of it takes a certain amount
of cost so both of these factors needs
to be taken in Rohit before coming to it
you know conclusion on which business
needs a IPC key okay Abhimanyu has a
very good question in fact every menu
I'm actually in the midst of this
problem so I think I'll take up this
question and I will actually answer this
you know in a little detail as well so
one of the things that I've even you
asked here is we would like to know how
a web application developer will use AI
n ml and what extent he needs to
understand it the very common problem
that we are facing in fact is the
websites have become rather static and
in fact the world of apps have taken
over a lot of the website business right
now now one thing is the reason why we
prefer apps over whelming websites is
because of the fact that they are
available in mobile and they're also
available in terms of you know at any
point in time the other thing is we
consider websites are very important is
because you know websites have the
ability to serve content dynamically
which means you have the ability to
personalize the experience every user
has this was not the use case five or
ten years back now you are here you've
got the data you've got the kind of
users who are coming in to your website
you have a pretty good idea of who they
are and now what you want to do is
personalize your website towards their
experience and the entire web experience
changes when you are actually creating a
personalization element for them and
that is where AI is actually coming in
so your ability to analyze different
features or different as
of a particular user and then be able to
serve content dynamically as one use
case could involve recommendations could
involve a certain element of changing
the interface for them or probably being
even more personalized which means if
you notice a user is coming quite often
how can you actually communicate with
them and in fact the the important thing
is nowadays most of the websites have
started to bring in chat pods so what
they want to do is they want to not only
leave the website as a static thing that
resides on your Chrome browser but
rather be a communication tool between
the backend organization and probably
the front end in terms of the user so a
lot of use cases trying to stream data
understand what are the different types
of customers and personalizations
recommendations and then the final
aspect of that is to bring the chat pods
which are trying to bring in the human
element of interface of discussion with
the particular customer as well I hope
that answers your question
Rohit and Adam on you how a I will help
in analyzing the video analysis a lotf
use cases present now there some
interesting use cases are you know
captioning a particular video
classifying the different streams of
videos as well and usually these kind of
techniques use techniques like rmm or
LSD ins that we talked about so a lot of
applications in terms of captioning and
as I mentioned the classification of
frames and then be able to correlate
different videos is something where you
know AI has been actually used Rohit the
women un present I hope I was able to
answer your question would have reached
Peter
yes or no in case no then I can probably
look at what else I could write as an
answer to you guys thanks percent now I
hope minion drove it
the questions were clarifies all right
as we move on let's a look at artificial
intelligence now remember artificial
intelligence is a macro concept and
there are micro aspects to it like
machine learning and with in machine
learning there's another Super Micro
concept called deep learning now when
you stretch the globe out and when you
flex it up a little bit artificial
intelligence is not something that
started up somewhere in the last five
years or ten years and then suddenly you
know we have a situation where you know
artificial intelligence becomes this new
thing no it's not that way in fact
earlier artificial intelligence was
available in fact an orifice were
applying it but the only thing with
artificial intelligence in those cases
was that all the outcomes had to be
outputted now let's assume a simple
example right I mean in case you wanted
to actually install a heater in your
house and you want to connect it to
let's say a software and you want that
either to behave dynamically in the
sense based on certain situations about
the environment it has to be able to
heat or cool the the the water basically
right now if I'm just giving that for an
arbitrary example but in the situation
like this what is confusing is because
there could be multiple outcomes I might
have multiple scenarios under which I
would like to have water so I could
probably want hot water on winter days
which could be a very macro concept and
I could not want a cold water on let's
say somebody's and the second thing is
you know on days when I go for a workout
and I come back it should be able to
provide this degree of water and I go to
office and come back it should be able
to provide this amount of water or this
heat level is what it needs to provide
but the problem with that is there could
be multiple preferences that I might
have and to be able to call out these
preferences programmatically or rather
hard put them in a particular program is
going to be a very difficult thing so
earlier people solve the process of
automation through some hard coding but
they quickly realize that this
is not the optimal solution because
there's a high possibility that you
might end up calling out too many
outcomes and to be able to hard-code all
the outcomes is humanly impossible so we
needed a technique which can
intelligently learn some of these things
and all you need to provide is a list of
all the outcomes and intelligently the
data which is provided as an input is
read by the algorithm and is able to
apply for different use cases and that
is where machine learning actually comes
in and not that machine learning didn't
exist in fact the concept of machine
learning has been there probably 200-300
years back if you talk about even
techniques like linear regression or
logistic regression these techniques
have now got the computational ability
as they said the algorithms were all
there the computations were all there
all we needed was the calculator to do
this and that's well now we have all of
these tools in our hands so the tools of
the war are in our hand all we need is
just a war to begin right so that is
where machine learning has come in and a
choice of the right technique helps us
to do to kind of you know automate a lot
of processes but remember we have now in
the last few years started seeing that
data itself has changed its cycle type
and the major thing is data is no longer
in the most structured type of rows and
columns or excel sheets but data has
started to move to the unstructured
aspect of it and when I talk about
unstructured aspect I think all of you
can relate right I mean earlier you used
to share jokes on text and using some
other factors but now most of us are
sharing images and videos and pictures
and you know texts in multiple different
formats and emoticons and whole lot of
things right our communication is
dynamically changing which means we are
talking in things that are not
necessarily restricted to a particular
type so that is where the importance of
actually AI or deep learning comes in
the ability to handle unstructured data
so to me if you ask me what is the
difference between other machine
learning techniques and deep learning
they simply put deep learning takes up
unstructured data like it's a it's a
piece of cake for it right so it's just
breakfast for deep learning when it
talks about unstructured data and it is
very very powerful it is very intuitive
to actually implement a plan
and the simplicity and prototyping of a
deep learning is easy so it it is
possible to work on any type of data and
ASP mentioned some people had asked how
do we apply video how do we do it on
images any use case you don't need to
worry about it
you know you have techniques that can
actually take care of all of them so a
follow-up question from our Bienvenue
actually and I will take that up here as
a web developer will mostly be using
existing libraries and incorporate them
in my application or you need to
understand ml and in great depth to be
able to use them optimally so to your
question of even you to a certain extent
you will not be using your existing
libraries because those libraries are
more specific towards the app
application development aspect of it the
library is more specific to ml nai are
on the data aspect of it which is to be
able to you know analyze the data and
then be able to look at different
computations to be able to optimally
decide what needs to be done maybe there
could be a couple of overlapping
libraries that could exist but on a
whole most of the libraries that are
specific to ml nai are different in fact
tensorflow is a very different library
although it can be applied in multiple
environments like GS or it could be
possible to implement in C++ or even
Python although our session today is
going to talk about Python but what you
need to understand is very good thing
that you already are aware about is the
kind of data that actually comes from
some of the websites but what you need
to focus on is to be able to understand
how to optimally use this data so that
is where the focus on the session is
going to be actually so another
interesting question in fact Neetu Kumar
has asked this which is how is AI
implemented in cloud computing so
remember a lot of the models are not
actually implement along CPU when I am
talking about images a simple image like
let's say you know a simple image that
our forum clicks probably it's about
thousand 2000 pixels that's about you
know huge amount of data that actually
goes in for an algorithm to actually
analyze when we are talking about text
it seems for example you know a chat pod
probably is training on a lot of chat
instances that have happened in the past
and typically a good chat what is one
that can learn on two to 3.5 million
data sets and so there is a lot of data
that is available and to be able to
process all of this data if use
definitely not possible and probably
some of the you know in-house servers
can take care of it so uncommon servers
can do that then how much can you scale
it as where the problem is so cloud
allows you the flexibility to scale
depending on the kind of data that you
have so that is where AI is implemented
using you know multiple use cases you
have a data storage within cloud you
have the processing aspects of it
although I'm not a very good close it
but I'm not an architect on that but
just couple of things that you know as
your movie flower door probably AWS
offer in terms of different packages
that help you to actually process all of
this status where it's implemented in
terms of you know yeah yes that's
correct
a Beeman you will be using ml in AI
libraries ah absolutely absolutely so
the question is a bimini is actually I
think motor in a question it's a point
which is basically is there a need for
an application developer to understand
ML algorithms to make applications more
intelligent absolutely in fact an
understanding of the amalgams is very
important because it's a it's a knife
for every problem basically right so you
don't want to use your tools for every
problem you want to intelligently make a
choice on what kind of tool you would
like to use to solve what any kind of
problem that you actually have right I
hope I was able to answer the questions
me to come early and avi when you I was
I hope I was able to answer the
questions me too I hope you can quickly
confirm if the question was answer so
now we've discussed what is machine
learning in fact I mean what is
artificial intelligence then we
discussed at the subsets of artificial
intelligences where machine learning
comes in so let's look at what machine
learning is in fact right most commonly
a lot of you must have come across this
when you actually did your research on
machine learning or on AI you would have
come across this problem this is a very
common problem we call it as the hello
world of machine learning in fact
what it does is basically you have data
on the features of different flowers and
species of flowers rather right so the
features are basically what you are
looking at over here down the tree
versus a pen lens that will with petal
length and petal width now every flower
or every class of lover has its own
unique set of features and the
combination of these features help in
determining what species of flower it
belongs to there are typically about 150
different instances and for a human main
to read through all of these different
you know different features and
different instances is going to be a big
task so what happened is is it's
actually a use case from the UCI machine
learning laboratory and what they've
done is they've actually built a model
based on the data that was available to
predict any new species of flower that
exists so all you need to do is you need
to provide just the four important
features which is the sepal length the
sepals with the petal length and the
petal width and automatically based on
the 150 instances that will provide
earlier the model is able to learn and
apply this on the new instance that you
just provided and be able to
intelligently classify of what species
it belongs to so that is a very basic
understanding of how a machine learning
algorithm works and it could be it could
be scaled up to different problems so it
could be scaled up probably if you can
identify what species of flower is there
you can also scale that up to look at
images and find out if the images of a
draw or if it's of a cat or probably
even what you know what breed of dog it
is also to a certain extent you can do
that if you can provide the training for
an algorithm now if you talk about
images the complication is that images
have multiple pixels and there are
there's so much amount of dimensionality
that exists in the images that we need
to actually analyze this very carefully
and process all of the elements
intelligently to be able to actually
classify them because remember ending
this classification of calling a dog a
cat or a cat or dog is definitely not
the most optimized use of an algorithm
so we need to intelligently help and
evolve them to make some of these use
cases not a couple of limitations that
machine
learning itself comes in lip and as much
as I had I'm a proponent of machine
learning I call out openly the
situations at which it might not work
now it's easy to make it work when you
only have the image of a dog but let's
say that the dog is a part of an image
or a family pic basically right now this
is a high dimensional image there are a
lot of factors that are to be seen in
this particular image there's probably
as you can see in this example there's
the mother there's the father there's a
kid and then he has a sister and then
there's the dog so nmi is actually uh
sorry an m/l is actually going through
all of these factors before being able
to look at this one small piece over
here so the mythical ities of the data
or rather i would say you know the
ability to look at very small features
is where an ml actually starts to fail a
little bit so when I've talked about ml
I'm talking about techniques like
support vector machines naive based
techniques like XT boost and random for
this these techniques can do it image
classification but remember the problem
is that the abstraction level on it
images or the high dimensionality that
exists on unstructured data is simply
very difficult for for any of these
other machine learning algorithms to
work with so what happens is that they
typically start to have very low
accuracies and if I'm talking about this
particular case let me bring it into a
real-life context although we are not
our objective is not to classify dogs
and cats but there could be another
scenario let's say you have a CT scan
report or a PET CT scan report and that
report needs to be actually and left as
an image to find out where exist where
there is a particular problem in the
particular scan across different
cleaning examples you can train on
different classes of problems and it has
to look at minut levels of the scan
report to be able to find out what exact
problem is this and that is where if you
know there are these more little aspects
are very few pixels that actually cover
the problem will be there
is it's going to be very tough task for
other support vector machines that is
the reason why we actually work out with
tools that probably have
to overcome the machine learning
shortcomings and to overcome that the
unstructured mess and the high
dimensionality of the data is where deep
learning comes and so you know every
sward has its own unique reason so here
is where I make a very important point
and that is I'm not saying that machine
learning doesn't work or it is not
useful at all there are certain use
cases where machine learning is
absolutely applicable now for instance
you want to actually predict a credit
list for example you want to be four
identify who are the people who are
likely to default on a loan or not
machine learning is absolutely like a
knife on a butter it can work through
very well predict on all use cases
without absolutely any
misclassifications or at least you have
a lot of control in terms of trying to
take care of all the misclassifications
that happen so that is where a lot of
machine learning is absolutely
applicable and as we use huge use case
I'm going to quickly talk about the deep
learning in fact we're going to talk
about what is deep learning and its
application but before I get there I am
going to again quickly take a pause and
check with all of the participants if
there are any questions if there are any
clarifications or even many points that
you would like to make at this point in
time before we we actually proceed to
the next part of it does so Rohith has a
good question which is does ml is ml
different from IOT and there could be a
part of a IOT bullet so in fact IOT is a
larger concept like artificial
intelligence and in AI out here talking
about connected devices a lot of the
devices could probably require a certain
level of artificial intelligence to
automate some of the tasks given the
data so ml is probably a component
within IOP is how I would like to put it
in this particular case loaded any other
questions I don't see much questions I
am opening I'm hoping that I'm actually
clear unless the students and the
participants view that this will Greek
and Latin that I am talking but I have
moving ampere but a quick thumbs up or a
quick yes if anybody is finding or you
know anybody is thinking that this was a
little different or not able to
understand what the trainer was talking
about so feel free to let me know that
that itself is also good feedbacks
that I can you know try and clarify some
of your doubts because at the end of
this session what I am looking at
objectively is that you have a good
understanding of where AI works and what
is the technicalities of a is basically
is what I am looking at and then
probably that you know clickers and the
passion to actually learn in a little
more depth about AI as well so I'm
hoping I am able to actually answer most
of the questions or clarifications that
the participants have feel free to use
the questions to panel gave the do not
worry be more than happy to answer at
any point in time but deep learning
actually starts from our origins of deep
learning actually comes from a technique
called rest neural network and more for
neural network itself is how our neurons
within our brain actually process all of
the information so if you would like to
call a deep learning is actually a
simulation of how our brains think and
analyze data so that is where you know
I'm going to quickly call about colic
very categorically how our brain
actually processes information the
general way absolutely propylene has
asked about speech recognition
absolutely provocative I'll answer that
question in a moment from now while I
just bring in this similarities between
deep learning and our biological neuron
for our brain right there are five
inputs basically is what I call the five
senses is what the inputs are basically
for our brain so I think you all of us
have the vision the sense of vision the
sense of smell taste touch and feel
perception all of these are basically
senses that are their defenses are the
senses sorry are the input data for our
brain so let's take an example right so
you look at an apple for now right an
object for example is an athlete right
now how did you know that it was an
active basically so your eyes projected
let's say a certain degree of light or
something on that particular object and
once your eyes actually looked at the
object you perceived all the features
related to that object so for an
you looked at profitably the length and
breadth of it probably also tried to
estimate what weight it is and then you
basically looked at what color it is
when all these things actually came
together your brain actually quickly
took a decision and said that it's an
apple so what happens is in your brain
basically or in our green hover the
moment you look at a particular object
there are chemicals that are passed out
from our dendrites that we are seeing
over here so they will feve all the
signals from the senses they process it
and send it to a cell body within a cell
body is where all of these aggregations
are taking place remember you can also
touch and feel the app and you can smell
it and you can probably look at other
things so when all of these five senses
aggregate together they are processing
all of this information so there is a
judgment table if you might call the
within yourself Adi that is taking all
of this in inputs and it is putting them
together and then it is saying that it's
an apple and then once it's known that
it's an apple within the cell body it
passes it on to another cell to probably
take some other action which means once
you know it's an apple the next action
you take is to cut it or probably you
take a bite of it and quickly finish it
off so that you can have a good meal so
how deep learning is working out is it
it has to take input data in this sense
probably the input data could not be the
five senses but rather the data could be
the ones that you are actually going to
provide so it's very important to
understand how our brain works because
sometimes it's in fact I have started to
appreciate a whole lot of things in
terms of how our brain is actually able
to process information things that we
take it for granted right subconsciously
our brain is doing a million
calculations and we start to appreciate
it more when we actually see deep
learning in work because you can see how
much amount of processing actually goes
in to be able to develop a very strong
model with create applications and you
know it is probably one-fourth of what
our brain does but still it has got so
much amount of computation that needs to
go and imagine the amount of computation
our brain does so that is where the
interesting concept of you know neural
networks being a kind of
simulation of car brain is an
interesting concept I have a quick
question here in fact uh are people news
question which is basically it's AI in
ml development treated as a carrier
completely different from application
development all it overlaps to some
extent with application development in
fact uh the entire thing of a menu is
that all the people start to associate
and I I certainly feel this is sometimes
the wrong Association is that people
start to think AI in ml is very much
only for data scientists and people who
deal with data are supposed to work on
this but in my opinion I think every
person who's working on application
development should be a part of AI the
reason why I say that is because you
know the only missing component mostly
in most of the application development
is basically bringing in the AI aspect
of it and it's not rocket science that
it is very complicated or it is very
very you know different it's all about
getting the components together which is
important man
that's what focus on trainings are
usually which is basically trying to
bring the components together once the
components that they're integrating it
with in your application this is
probably another component like putting
together let's say you know a component
that could probably fetch data from
somewhere in the back and I'm just
giving a simple example although i know
i'm dumbing it down really low but then
what i'm trying to say at the end of the
day is that bringing these components of
AI in ml within your application
development only enriches it and i
personally am a proponent of the fact
that AI in ml should not be restricted
to data scientists in fact every person
right from leadership to all the way
until a person who's on to development
and right on those on the floor building
applications to you know bringing things
together should be available should work
on you guys because at the end of the
day AI is all about how we train it and
we all are intelligent about our work we
know about our work and that is where a
data scientist is not a specialist in a
particular aspect of the business and
that is where people were in app
development know what customers want
know what users look for and integrating
AI in ml there tries to add an advantage
to the application that is there so in
my opinion i don't consider them as two
different streams i would rather say
it's a part and parcel of the entire
application development to the question
of flow Bukharin can you explain a bit
about speech recognition so speech
itself looks at multiple data points
actually in that particular case to be
very specific I think it looks at
different aspects like pitch and base
and the amount of different the
different things that go into a
particular speech back part of it this
is stored in unstructured format it is
converted into a structured format
before it actually goes into an
algorithm so different parts of speech
are different types of speech are
uploaded into a model to be able to
analyze the different teams and
different you know pitch and different
levels of I'm not sure about the exact
terminology is used within speech but
all of these are taken into
consideration before being able to
classify them and recognize them
basically so just like image your image
is unstructured but converting it into a
structure basically to be able to
understand you know the application of
speech recognition I hope that answers
your question probably all right thanks
for that
so moving on wait not that I have
defined how a biological neuron works it
is but obvious that we need to also
define our artificial neural work so we
call it as an artificial neural network
or an artificial neuron or a perceptron
whatever the words but at the end of the
day it signifies basically you know a
processing unit basically right so as
you see on my left side you can see X 1
X 2 and X n Viva the input data consider
them as basically your five senses right
so in this case you know for algorithm
it's not necessary that you know the
number of inputs have to be restricted
to five since you know you're dealing
with a lot of data and can have multiple
input factors that would actually help
in determining and the the next thing is
basically you have a set of weights that
are associated with every input data
points what do I mean by weights
basically right so there is a certain
threshold you have in your mind right so
you know that a combination of color and
also a combination of let's say
wait for example let's say five grams or
ten grams is an indication of that it's
an apple less than five grams is
basically what we would call as
basically in this case at least a peach
for example now that five grams is
actually called a threshold but I'll
come to that later but the learning
process of being able to associate a
certain level of rate with the input is
what the weight over here also does
subconsciously we have created weights
for every object which means we know
what is the likelihood or what is the
importance of certain features in
identifying a particular object so it's
possible when you're looking at an apple
the red color would have a higher weight
edge than compared to the actual size of
the apple probably right so these are
the there are certain input factors that
might have a higher weights compared to
certain other factors in this case the W
ones W tools and w ends are associated
with the input factors and each input
factor has a certain weight implying how
much importance it has in the final
output when you compute or when you
actually multiply the inputs against the
weights you get what we call as a
summation function so that is what your
cell body is actually doing as we
discussed in the previous life you're
aggregating all of the information that
came in from your five senses you're
putting them together and then what
you're doing is you're taking that from
the summation pushing it to a transfer
or an activation function where the
actual decision-making happens so when
you've summed it up here you've got a
final value and this is where your
decision-making is happening where
you're saying that hey this is an apple
probably a peach or it could be possible
that it's a lemon I mean I'm just giving
an example right so could be multiple
glasses so that's how our brain
processes information and that's how an
artificial neural network is also doing
in this particular use case so a simple
example of the architecture of how a
perceptron actually looks like a couple
of questions that have come in but I
will just come back to the questions
prabhakar in v2 and cinnamon in a minute
from now I'll just explain some of the
important aspects but before I move on
to the next slide I quickly want to
understand from the participants if
there are any particular questions if
anybody has any questions in terms of
the current slide
okay getting more questions at this
point but moving on to the next part so
if you notice it again we've just made
it a little more simpler in terms of the
understanding of how the entire thing
works so again the input data you have a
set of weights multiply the weights
against the input data and transfer that
to after you summed it up you pass it on
to the activation function which is
actually taking a decision now it works
well on a linearly separable data which
means it's easy on certain use cases
when you want to you know classify let's
say sunflowers from a rose features are
very very unique for a sunflower
compared to a rose but you know what
about things that are non linearly
separable which I will come back to at a
later point in time
now these weights are the most important
thing here because at the end of the day
the weights are the long-term
methodology by which you know the
algorithm is actually storing the is
actually storing you know all of the
information so a lot of activation
functions are actually in place so to
your question Abhimanyu in fact is just
moving to the activation function part
of it so before I move to that but I
just want to quickly talk about how the
way that actually determine and that way
you know it will be geared to understand
the actual implementation of an EIU
Alton now weights are typically you
would you can probably associate them
with things like a coefficient they are
like a slope in a recreation analysis
initially and remember when you run a
regression you have only a single set of
weights but not in the case of a deep
learning algorithm you start off with an
arbitrary value of weights which would
be probably as small as 0 but through I
trations or multiple processes the
weights will actually improve so what it
does is it uses the Narva tree set of
weights multiplies the input against
those weights sums it up passes it on to
an activation function and predicts an
output it already knows what the actual
output is so compares the predicted
output against the actual output and
finds out how different they are the
moment it knows there is a difference
what it does is it goes and updates the
weight and it keeps updating this weight
until a point where the weight is able
to
exactly the output given the input so
the moment the accuracy is reached it
stops the updation of which so it might
start with an arbitrary value like 0.01
but let's assume that zero point zero it
is the weight at which you know the
input is helping the matching the output
against the predicted output so it takes
zero point zero eight as the final
output so what had happens to do is
across iterations keep updating the
weights to identify the best combination
of weights for predicting output exactly
so as we can see over here what we have
is the formula is WJ t plus 1 which is
the new weight is a function of the old
weight plus n or ETA which means you
want to decrease or increase the rate
depending on the difference between the
actual minus predicted into X which is
the input data set basically now I know
a couple of questions came in which is
basically what types of activation
functions actually exist on the data but
there are multiple activation function
what you are looking at over here is a
step function during our sessions or
clearing our course we go into the depth
of what activation function is used for
kind of situations in fact this is a
step function which is zero until a
threshold is reached the moment the
threshold is passed it starts to become
a positive number or a 1 in this
particular case this is what we call as
a binary or a step function
there are activation functions like the
sigmoid activation function multiple
activation functions all of these
activation functions are available with
intensive flow to actually implement
some of these cases so depending on the
end of output or use case you need to
make an intelligent choice of
intelligent choice of activation
function I hope that answers your
question Abby when you know there's
another thing with chattin is asked as
how our weights decided dates are
arbitrary initially taken but over I
creations the weights will change which
means you know you might take a choice
of an arbitrary weight but that might
not be the best choice so you keep
hydrating until you reach that I to it
so that's how they are they are not
static rather they are dynamic
similarly is AI have any existences
without deep learning
there are hard-coded use cases similar
in which actually uses AI without deep
learning but they aren't scalable to the
level at which a deep learning is
basically yes absolutely proper current
is DL or ml you know part of is it
important in speech recognition
absolutely it does help in that case
also recognize speech context rowdy
sorry
so yes it does in that case as well and
to your question
nitu had asked a very interesting
question which is auto-encoders why do
we ought to encode now auto encoder is
basically again a family of deep
learning algorithms that are
unsupervised which means unlike the data
set where we have classes explicitly
mentioned in an autoencoder you do not
have classes mentioned so rather it's a
it's a it's a dynamic tool that can do a
certain level of automatic
classification or best known outputs
that it has to replicate given the input
it can also be sometimes used as a as a
dimension reduction ality technique in
this case me too absolutely ugly money
so are we menus point here so for a
particular problem only the weights are
changed in every I creation and the
activation function remains the same the
choice of activation function is
something that we call on it changes
from layer to layer but for a particular
problem it remains the same so the
activation function is basically how you
would like to get your output as basic
you right
a single a perceptron for a
classification now as you can see this
is an or gate I think a lot of us have
come across the concept of logic gates
here the output is called a true which
is basically 1 then both the cases or
either of the cases are a 1 the moment
both the cases are false it indicates a
false so as you can notice over here you
know you have the X 1 and X 2 which is
the input data we've arbitrarily taken a
choice of wait as one basically right
the input data multiplies with the
output data which means 0 into 1 plus 0
into 1 which is basically 0 and that
gets marked over here in this origin
over here similarly 0 into 1 plus 1 into
1 is a 1 and that gets marked over here
in X 2 the same way X 1 gets marked a 1
and then there is 1 into 1 plus 1 plus 1
into 1 which is basically a 2 which get
marks here now T is equal to 0.5 is a
threshold anything that crosses the
threshold is basically classified as a
separate group and anything that's
within the threshold is called as a
separate group now this is very easy for
a single a perceptron to classify and
all gate is simply put when either of
the cases are true or when either of the
cases or both the cases that are true
the output is a true but some of the it
is easy to do this because it's linearly
possible but the complication arises
when you are talking about non-linearity
in the data and typically an XOR is when
either of them are true it has to be
classified as a true bet when both are
false or both are true it needs to be
classified separately at the false and
as you can see the same objective with
which we built the or your logic it you
will notice that there is a certain
level of MIS classification that is
happening here now XOR is a kind of
non-linearity that exists in logic gates
and application of a single layer
perceptron typically fails over here and
that is where you know you need to
actually talk about multiple perceptron
case or what here so talking about here
is basically more than two neurons kind
of situation a single neuron has the
architecture that we just saw earlier
this architecture is what it has and we
are saying that we need to have multiple
architectures like the same which means
multiply this by n number of times to
scale up the algorithm so that is what
we're talking about when we get into a
multi-layer perceptron and its
application in fact as you can notice
over here you have the input data X 1 X
2 X 3 and then there is this y 1 y 2 y 3
each of these y ones have a summation
function as you can notice over here and
it also has an activation or a
transformation function the summation
function the formula is we talked about
is W I into X I the transformation
function applies a sigmoid
transformation here as you notice which
is 1 by 1 plus e to the power of
negative B X and what you have is
basically earlier you talked about a
single-layer perceptron if a
single-layer perceptron is defined as X
you have a 3x over here which is
basically three neurons that are
processing all of the information one
more thing that we didn't discuss about
is basically the bias and the bias is
pretty much similar to the intercept
that you come across in a linear
regression when all of the factors are
all other input factors of Z or the bias
is the value that the final output
actually takes but remember bias has a
larger function than that it is very
very important to actually steer through
the entire function so whenever we look
at the weights as your accelerators you
can call the bias as your steering wheel
that is in control of your overall
function and it's and it's you know
application basically as I talked about
it earlier the weights are basically
that unit or that particular component
within a neuron that is very important
to store information for the long term
perspective and be able to apply for new
situations and it is very important to
update the weights because in case the
weights are a wrong choice they are
going to end up predicting wrong and
that is going to create a miss
classification or failure of application
so we need to take a choice or a
conscious I mean at least the algorithm
has to take a conscious choice of
weights to be able to predict the
outcome as well as best as it can in
case it does not have the right choice
of weights it needs to go back and
update these weights and that is what we
call that's actually a back propagation
the ability to go back and update its
learning so simply put if we know that
and
of the object that we looked at was not
an apple but rather it's a it's a peach
what we do is we go back and update our
weights and we say that hey anything
that is a little smaller than an apple
and a little less red in color is
probably a peach and we can't call it an
apple so the same way the algorithm
knows that it has predicted something
wrong and it needs to go back and update
its learning so what happens is your
input data actually comes in through the
input layer passes through a set of
neurons and if you notice here this is
almost eight neurons which is eight
times of a single layer perceptron and
all these eight neurons are taking the
input data of creating certain Claire
you know computations and then passing
it on to the next year until you're
finally able to receive the output so
that is where a multi-layer perceptron
has got multiple computations and the
ability to scale it up from eight
neurons that you see in this case to
probably 100 200 or even 300 neurons is
absolutely easy especially in tensorflow
and i will show that to you in a minute
from now but the most important thing is
the ability to handle nonlinearities in
the data a very very efficiently the
training of a neural network is a very
important in fact the most common thing
in neural networks is the concept of
back propagation and in fact I was just
talking about that a moment before so
let's actually look at how it works in
this particular case right so you have
the input data which is basically zero
one and two and you your actual output
is basically zero one and four now you
take a choice of weight arbitrarily you
take the very three and you multiply it
with the input data this if you right so
0 into 3 is basically 0 1 into 3 is 3
and 3 into 2 is basically 6 now your
actual output is 0 2 &amp;amp; 4 and the
predicted output using the weight is
basically 0 3 &amp;amp; 6 so we have a certain
level of error now the absolute error
which is actually the difference between
the predicted and actual as you notice
over here is actually 0 1 &amp;amp; 2 but we
square them off because there's a high
possibility that some of the errors
might actually be negative in in this
particular case and adding up or summing
up negative and positive values might
result in a 0 so to
do that we square them up and it becomes
0 1 &amp;amp; 4 so that is where you notice that
the square is a 0 1 &amp;amp; 4 to handle this
efficiently we we feel that ok this is
not the right choice of wait so let let
us take another weight in this
particular case so what we do is we add
up a new weight which is 4 and when we
multiply that against the input we get
the outputs of 0 4 &amp;amp; 8 this time again
this is way far away from the desired
output of 0 2 &amp;amp; 4 so let's actually look
at what the squared error in this case
is it becomes 0 4 + 16 which is really
high or 4 times higher than what it was
for the weight of 3 now the model knows
that as it is increasing the weight the
error is increasing so it consciously
makes a decision rather within the model
is an optimization algorithm during our
sessions we focus about or talk about
the optimization algorithms in detail
and that is what we call this is
stochastic gradient descent and it
identifies the right choice of weight so
the moment the stochastic gradient
descent knows that the algorithm when
the weights are increasing the error is
increasing what it does is it takes the
reverse direction and starts to reduce
the weight so that the error reduces to
a point where beyond which you cannot
reduce the error is where the optimized
weight is identified by the stochastic
gradient descent algorithm as we can see
here you can notice that as you are
increasing the weight the error reduces
initially but after a point in time you
continue to increase the weight you will
notice that the error continues to
increase so it is called a gradient
descent because regardless of where it
begins whether it's on the left side on
the right side on top it has to descend
down to this point where the error is
minimum or is absolutely low basically
right so that's how the training of a
neural network actually works before I
proceed to the use case I quickly want
to again take a pause
all the part is difference I could he
want to check if there are any questions
if there are any clarifications that you
guys would like to get before we
actually proceed to the practical use
case or application of neural networks
on tensorflow
all right since there are no questions
at this point in time I will talk about
what is the practical use case that
you're actually going to look at or what
is the multi-layer perceptron use case
that you're talking about let's assume
that you've been hired by a particular
team to identify the difference between
a mine and a rock so the objective is to
to create a model that actually
identifies a particular object as a mine
or a rock now this is an interesting use
case because if you want to detect a
particular object as a mine in a rock
there are a certain level of you know
signals that are being sent out from the
from the detection tool which would
probably be a sonar and what you're
going to do is use these signals to
detect whether it's a it's a mine or a
rock so what we are going to do is we
are going to actually look at it in
terms of I'll first show you the data
set after that we are going to read the
data set into Python we are going to
define the features and labels then
you're going to do something called as
an encoding I'll talk about what
encoding is and then we will divide the
data into training and testing do all of
the assignments and all of the storing
of objects within tensor flow implement
the model train and then show you across
iterations how we are basically reducing
the mean squared error or actually
increasing the end you know prediction
capability of the model basically right
or in this use case it's the actual you
see that we have focused at so let me
quickly open the data set for you guys
so that it becomes a little more clear
in terms of how the data looks like so I
will just pull that up give me a quick
second sorry for that this is what we
call this the advantage of Python so you
love your Python so much that you
process your data all of it in Python
and not in Excel so yes
so you have 60 different classes as you
can see here sorry 60 different features
I'm not sorry about that
all of these features are basically
signals that actually come up from the
sonar - now each - each signal basically
has certain value and a combination of
these features is what determines
the final output is basically a rock or
a mined in this case it's 0 &amp;amp; 1 and our
objective is to find out you know all
through 208 as you can see 208 instances
create a model that learns all the
features that are necessary to classify
a particular instance there's a rock or
a mine and then apply for future use
cases so intelligently the moment of
future use case comes automatically the
model should have the ability to predict
whether a particular object is a rock or
a mine so let's see how we're going to
do this intense at low in fact I'm going
to quickly open up my spider and this is
a tool that is commonly used by all data
scientists especially when it comes to
deep learning any questions at this
point in time please feel free to use
the questions panel and more than happy
to take a pause and answer them at any
point in time and we'll quickly execute
this model and we'll talk about the
different aspects that are there within
this model before you know before we
actually see the final output as well so
a couple of libraries that are very
important mat clutter this for all the
visualization of the data the importance
of flow because we would like to process
all of this information intensive flow
numpy is basically multi dimensional
array and a lot of processing in multi
dimensional array tensor flow in fact is
a combination of numpy and scikit-learn
it has the important API is necessary to
run deep learning algorithms and at the
same time also got the API is to process
a multi-dimensional array so numpy is
imported as NP pandas is where our CSV
file that we just looked at is going to
be imported by and created as a data
frame it's easier to process the data
frame and it's easier to actually work
with the data frame as well so that is
where you're actually bringing that and
apart from that from scikit-learn or a
scale learn we are calling in two more
packages three more packages rather one
is for label encoding which I'll quickly
show it to all of you guys and then
there is the shuffle
remember these 208 instances that you're
seeing right in front of us are in a
particular order so if you notice the
final output
we quickly open that you will see that
initially all the ones are there and
then there are the zeros now although
the model does not get affected by it is
important that we actually shuffle the
data set up so that we can eliminate any
existence of bias that would probably be
there in this data set so that's why we
are going to shuffle the data and then
after that there is the model selection
which has the functionality of grain
base plate so automatically it split in
your data so splitting this 208
instances into training and testing what
you do by that is you train on the two
on a certain number of observations and
then you test it on the other
observations to see how the accuracy
works so let's assume that I have 208
instances I divide it 200 and 100 let's
assume for our example right now so on
the hundred instances the training
happens it learns all the features and
then applies it on the testing data set
now in the testing data set I hide the
rock and mine classification rather I
asked the model to derive that and then
what I do is I compute or I compare both
the outputs to find out the accuracy
basically right so that is where it's
important to divide your data into tray
intestines plate
now a quick function here to actually
read the data set in case you are not
comfortable with right and do not worry
we do have a Python session as well
before the tensorflow sessions actually
begin we have a prerequisite session
that focuses only on Python and how to
use Python or program using Python so
this function that we have written over
here is to use pandas functionality to
actually read the data set and then what
we are doing here in line number 15 and
16 is basically to divide the data into
X &amp;amp; Y now remember quickly opening up
the data table once again remember that
Y is basically the 60th column that you
can see over here this is what we have
discussed in our example here right so Y
is the 60th column all the columns from
1 to 60 are basically what we call as
the X basically now the rock or a mine
or the class which it belongs to depends
on all the 60 variables that's why we
separately called the massive X
and we call this as an egg white
separately because Y or the 60th column
is dependent on all the 60 columns
before it basically right so that is why
we are calling them as x and y and we've
used the functionalities within pandas
2d market what are x and variables Y is
basically so do you demarcated the
column separately we are applying an
encoder which is not necessary in this
particular use case because our data is
already in 0 &amp;amp; 1
but in case our data was not in 0 &amp;amp; 1 we
need to encode it to make sure let's say
if it was rock and a mine or if it was
fraud not fraud we will have to use a
particular example like 0 or 1 to recode
them so that is what you are doing here
not exactly not really useful in this
particular use case but we're just
showing that it's a good thing to
actually do in a neural network what so
called ammonia has a question here in a
neural network what computation is
taking place in multi-billionaires and
how does it affect the final output as
composed to a computation which uses
only a single layer
I would like to bring the concept of big
data here basically so what happens
within Hadoop is basically you are
distributing all of the tasks across
different nodes or clusters for example
the same way what you're doing in this
particular example is you are
distributing all of the work so the
neurons are freezed up if simply put if
we had to have a single neuron in our
brain to actually process all the
information around us there's going to
be a definite traffic jam so when you're
using a multi-layer perceptron what we
are doing is yet we are distributing all
of the tasks and making it easier for
the algorithm to actually work
efficiently and predict much more
efficiently so that is what we are
actually doing in this particular case
of among you so the processing time is
faster and the accuracy is higher in
terms of the final output I hope that
answers your question of the menu right
so moving on encoder as we discussed is
just a function to encode the variables
and there is also something called as
one hot encode I just want to quickly
call out what that basically means now
what a one hot-and-cold does is
basically
let's assume you have the outputs as R
and M now what a encoder will do is just
convert this into zero and basically
convert this into a 1 basically right
and a 1 hot end code creates two new
columns and when there is a rock it will
basically call it as they put this down
so let's say it is 1 and 0 so when it is
a rock what it will do is it in under
the one column it will put it as 0 and
under the 0 column it will put a 1 when
it's a mine sorry about that
apologies 0 and 1 and when it is a case
of mine it will basically call it as a 1
and a 0 so this happens across columns
so this is what we call is a 1 hot and
of course and that has been defined in
the next function that you see over here
so we've laid down the logic by which we
can not encode our data set simple
functionality but do not need to worry
about it tensorflow within itself
actually does provide the functionality
or API for one hot and cold so you don't
need to write such a lengthy amount of
code rather it's just a function that
you need to call out with intensive flow
to to execute that particular thing
remember as I did when I missed out upon
this point one hot and cold for that
matter even tens of flows actually it
has the low level api's and the high
level API so both of them are available
so which means if you want to prototype
it's very easy at the same time if you
want to customize it's it's highly
possible with tencel so big advantage is
there in terms of tensorflow
next is you're using MATLAB we are
quickly going to plot a couple of things
which is basically plot the ones and
zeros remember our data has of ones and
zeroes one indicate in a rock in zero
indicating on line so we are just
plotting that out using MATLAB function
followed by a quick execution of all the
functions that we've actually defined
here so you can see we've actually
executed the read underscore data set
function which will generate three data
sets rather too important for us which
is x and y then the plot underscore fine
the same function that you see over here
has been executed here to execute the
outputs of the data set X&amp;amp;Y followed by
a quick shuffle of data as we talked
about earlier it's important to shuffle
the data because they are in a certain
order and the order mine states affect
the biasness of the algorithm so that is
why via break we are shuffling the data
and then breaking them into two parts on
a macro level which is strain and test
as you can see tray next test checks
train why test why basically so training
data of X which is the dependent
variables available for training testing
data of X which is dependent variables
available for testing train Y is
basically for training purposes the best
Y will not be fed to the algorithm the
other will be used for validation
purposes by the algorithm to see how
accurately it has been able to predict
now the science of breaking the data is
20% for testing and the remaining 80%
for training and for Repub pre-product
ability of T results we've kept the
random state at four one five couple of
important things learning rate
I kept mentioning that the weights will
increase and decrease and the rate at
which the weight should increase or
decrease depends on the learning rate so
weights can increase by 0.3 or decrease
by 0.3 across different type regions now
in this particular use case I have given
three thousand a fox or three thousand I
trations rather so which means there are
208 observations and this model will
typically go through all the 208
observations at least three thousand
times right that's a big number rightly
so it's going to do it for three
thousand times but just for the sake of
time constrain for this point I'm just
going to keep it at thousand so that we
can see how the output looks like I'm
going to store the cost history cost is
nothing but it is trying to find out
what is the difference between the
action and predicted so we are going to
store that in an empty object so that we
can keep looking at at how it has worked
out now this is a multi-layer perceptron
the most important thing is I'm defining
how many layers I have remember our
earlier example I'm sorry about that our
example that we just saw here
now in this particular case you are
typically looking at two layers with
four neurons I am talking about four
layers with sixteen neurons that is
totally 200 and 200 240 neurons
basically right which is 240 times a
single layer perceptron remember a
single layer perceptron can handle
linear data but non-linearity is not its
strength so this data has a lot of
non-linearity and that is the reason why
we have four layers which sixty neurons
each to process all of this data we are
defining the input data which is X
already discussed as we seen it earlier
followed by the weights and biases as I
discussed earlier the weights are
something that is kept as a variable
which is an object that tensorflow
actually provides which means the values
vary right from the start to the end
they have been kept as TF toward zero
indicating that initially the weights
will start at zero and as there are
multiple iterations the value will
increase from zero to 0.3 0.6 0.9 so on
and so forth until it is able to arrive
at the right weights same concept
applies for bias and then there is a Y
which is basically the you know actual
write that we're looking at so that's
basically the weights biases and then we
come about the multi-layer perceptron
the first part of it that you see is
basically the the summation function
where you're multiplying the input
against the weights and you're adding
the biases so you're adding PF dot ad
and TF dot Matt mole so this is matrix
data so matrix multiplication between
the input and the weights and then the
addition to the bias which is basically
the summation function you take the
value from the summation function and
pass it to the activation function which
in our case is a sigmoid activation
function the same concept of the value
is basically the input layer the layer 1
becomes the input layer for layer 2 and
the same concepts happen there also
which is the summation function and the
activation function these happen for
four different layers and remember each
layer has its own activation function
here we are using a sigmoid activation
function and in the final
we are using a radio activation function
during our sessions we go into the depth
of what these functions are but
considering the other time constrain
we are not going into all the activation
functions but remember the activation
functions design your final output the
other ones that are very important to
actually help you decide the final
output basically then followed by the
output layer which is where the outcomes
are actually going to be created as now
although we've decided what our weights
and biases are it's important to discuss
about the weights and biases for each
hidden layer and that's what we are
specifying here through this particular
part of the code followed by the
Declaration of biases for all the of all
the hidden layers as well so for hidden
layers and one output layer for which we
have actually specified the biases as
well now remember what we've done until
now is just compliation so 0 to 118 line
of code is all about the compliation the
actual execution happens only when we
initialize all of these things so that
is where we are using the init function
or global initializers to initialize all
of the compiled additions that we've
done and to execute them is what this
particular function does the actual
logic of the model is multi-layer
underscore perceptron as you notice the
definition of multi-layer underscore
perceptron has been done here and it
takes all of the input data multiplies
it with me and then adds up the biases a
couple of important things with
intensive flow to declare the cost
function and as I said earlier we are
using a gradient descent optimizer to
basically identify the best possible
combination of weights to predict the
outcome basically right so that is what
we have done in this particular juice
case after having done that this is a
session this code typically helps in
executing all the things that we've just
about elicited so all this while we were
just writing quotes to execute it is
this particular function and we're just
running a for loop against the entire
data set to be able to show us how
exactly the entire process is working so
we are providing the input data the
outputs and the other things that needs
to be validated for across different
data instances so 208 instances that me
to run as an epoch basically as an
iteration over the epoch and has to
produce a multiple level of outputs so
I'm going to quickly execute this code
and there are a couple of plots that we
have in the final part of the code but
I'm going to quickly execute this and in
the meanwhile what I'm going to do is
I'm going to posit at this point in time
so that you know if there are any
questions I would like to take them up
at this point in time
please feel free to use the questions
panel to type your questions you can see
that we started off with an epoch of you
know in this case we've started off the
0th epoch which had basically very
low-level laps of accuracy but now it
has been able to improve it to almost 69
percent in terms of the accuracy
constant fluctuations yes there will be
you know in terms of constant accuracy
is being measured at this point in time
in terms of across he pops how it is
actually doing
you
so it was running across in pots it is
showing couple of use cases where it has
been able to get about 80% accuracy and
you can notice that as it was running
through any questions guys at this point
in time any clarifications that you guys
would like to get before we actually
proceed to you know the final output any
questions
we are almost
to define the epoch this is what I was
mentioning that you know this is a CPU
so considering the amount of processing
units available within the CPU is very
less sometimes that's kind of processing
that's big time that is generally the
reason why we prefer to execute some of
these models on cloth in fact the energy
record during our sessions we do have
cloud lab setup within our sessions now
so most of the times the students
actually execute or the participants
actually execute a lot of these schools
on the cloud with students to be able to
see you know how this exactly works well
definitely happening when you I will
actually asked the team at at Eureka to
send you the presentation for those who
are interested definitely please do get
in touch with also the team here the
support team will definitely get in
touch with you after the session also
now feel free to let us know if you need
anything else apart from the
presentation as well so if inquire in
case there is a requirement I'll be more
than happy to answer any clarifications
that you have after the session also but
feel free to put your questions right
away as well because while I'm here it's
easier to answer a lot of the questions
in this case
so almost about the last 15 or 20 bucks
that are actually running so if you
notice we've just about got to the end
of it so the final output is your so you
will notice that the final accuracy it
has been able to arrive is that 78 but
what is important is yes I have reduced
the box from three thousand two thousand
if I allowed it to run for another
thousand or two thousand we would have
been able to arrive at the level of
accuracy but notice the most important
thing we started off with an accuracy of
fifty two percent as you can see in this
particular object but by the time we
came to the thousandth epoch reaction
increased it to 78 percent now across
these epochs the weights have changed
and it has the ability to change the
weights to increase the accuracy from
fifty percent all the way to 70 percent
if another few epochs I would have let
it run definitely we would have been
able to arrive at a higher accuracy then
what you're seeing at this point in time
but what is important is that applying
you're applying the tensorflow here we
have continued control of the algorithm
which means I have the control in terms
of increasing the epochs are reducing
the epochs depending on the kind of
problem that we currently have
so our sessions focus on tuning a lot of
these hyper parameters in fact I know
we've rushed through most of them
because of the fact that we have a
deficit of time and during our sessions
our focus is in talking about what these
different type of parameters are how
they need to be tuned and now they are
actually supposed to be handled what are
the different aspects in fact we've
looked at at a thirty-five thousand
level right now but during our sessions
we go down to the depth of each of these
things to understand how this works and
also implement this unplowed because
that definitely saves up a lot of time
in terms of its accuracy so it is shown
of cross epochs how the accuracy has
varied couple of deposit has been able
to touch almost eighty five percent so
the model has not consolidated
definitely but just to the use case to
actually show how you're in control of
the algorithm in different aspects of it
so I'm going to quickly pause here and
we'll quickly take up if there are any
questions that are there at this point
in time not a problem so thanks for
joining in definitely I'll ask this team
to actually get in touch with you at the
email id and I will definitely ask them
to work it out with you but before
anybody leaves like this week you wanna
know I just need one quick favor from
all of you which is
at the end of the session there you're
required there will be a feedback form
that can actually come up to you guys
I just request you to fill up the
feedback form in terms of how we did
this so that you know it will be helpful
for us to understand how it went through
so definitely to all the participants
who want the recording or the things
that we discussed during the session the
support beam is there they will be
getting in touch with you and they will
be walking you through different
requirements in case there are some
other questions that you do have post a
session or at a later point in time feel
free to shoot out an email to our team
they will definitely forward the email
to me and I'll be more than happy to
answer all the questions because I know
a lot of people are actually heading out
as well so that's the reason why you
know feel free to email me but please do
not forget to fill up the you know the
feedback forms that come up at the end
of the session
anybody who's leaving are not a problem
thank you for joining us for a session
it was wonderful interacting with all of
you those of you who are leaving your
free to those of you would like to put
down a couple of questions I'm still
away laughter the session so feel free
to use the questions panel and I will be
more than happy to answer those for you
thank you so much I hope you have
enjoyed listening to this video please
be kind enough to like it and you can
comment any of your doubts and queries
and we will reply them at the earliest
do look out for more videos in our
playlist and subscribe to any Rekha
channel to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>