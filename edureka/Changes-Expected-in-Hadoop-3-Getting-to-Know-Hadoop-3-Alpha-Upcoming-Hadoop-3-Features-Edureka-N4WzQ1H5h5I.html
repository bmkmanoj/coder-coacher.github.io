<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Changes Expected in Hadoop 3 | Getting to Know Hadoop 3 Alpha | Upcoming Hadoop 3 Features | Edureka | Coder Coacher - Coaching Coders</title><meta content="Changes Expected in Hadoop 3 | Getting to Know Hadoop 3 Alpha | Upcoming Hadoop 3 Features | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Changes Expected in Hadoop 3 | Getting to Know Hadoop 3 Alpha | Upcoming Hadoop 3 Features | Edureka</b></h2><h5 class="post__date">2017-05-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N4WzQ1H5h5I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is Soros from Eddie
Rekha and in today's video we are going
to discuss what's new in Hadoop 3 we'll
see one of the things we can expect from
it as it is still there in alpha phase
so let's move forward and have a look at
the agenda for today
first we'll see how irrational coding
will reduce the storage overhead and
HDFS as compared to replication then
we'll see the changes with yawn timeline
so is version 2 and we'll also see the
changes in shell script rewrite in
Hadoop 3 after that will understand the
shaded client jars and how are they
useful then we'll focus on how Hadoop 3
is incorporating opportunistic
containers with guaranteed containers
after that we'll see how native Java
implementation is going to optimize the
performance next up we'll see the high
availability architecture which to
passive name then we'll talk about few
default posts that have been changed in
Hadoop 3 and finally we'll focus on
intra data node balancer fine guys we'll
move forward and see what all changes we
can expect in Hadoop 3 so all Hadoop
jars are now compiled targeting a
runtime version of Java 8 that means if
you are planning to switch to Hadoop 3
you need to make sure that you have Java
8 all versions of Java released after it
next up we'll see how irrational coding
will reduce the storage overhead in HDFS
so in Hadoop 3 they have introduced a
rajat coding to reduce the storage of
overhead now let me explain how earlier
before a Hadoop 3 in Hadoop tube there
were replicas made for the data blocks
that were stored in the data nodes now
by default the replication factor is 3
so we'll take that same example so if
you have two data blocks that means our
overhead is around 200 percent so now
let's move forward and see how it is
actually getting reduced with the help
of errors of coding
so basically larger coding has reduced
the storage overhead of all by almost 50
percent let me tell you how if we can
remember in the previous version of
Hadoop that is not due to boil eggs
there was a replication of the data
blocks or you could say copies of data
blocks kept across the various data
nodes now by default the replication
factor was 3 because of that every data
block that is there in the data node is
present in two other data nodes as well
now because of that
you can see that there is 200 percent
overhead and at the same time there are
certain cold datasets as well that we
rarely use because of this we are
actually wasting a lot of resources by
creating replicas for those cold data
set now let us see how our asset coding
actually solves a problem now in a
larger coding what happens there is rate
rate is nothing but redundant array of
independent or you can say inexpensive
discs now whatever the data that you
have is segmented into small blocks and
these blocks are stored in various disk
as you can notice in the diagram that I
am highlighting with my cursor right now
now for each strip of original data
cells a certain number of parity cells
are calculated and stood the process of
which is called encoding so if I take
the example suppose block a1 and block
a2 so these are 2 data blocks and we
generate a parity block out of these two
data blocks this is block AP now why we
do that let's focus on this particular
diagram and understand that so over here
if any failure happens if you can see
there's a question mark so I don't have
my block a 1 anymore now what I can do I
can actually use this parity bit along
with this block a2 and I can recover
this block a1 so this is how I can
perform error recovery using Aerojet
coding now let's move forward and see
what benefits we get with this now in
order to explain you the benefit I'll
take an example suppose if I have 3 X
replicated file with 6 blocks that you
can zoom around 6 into 3 equals to 80
blocks of this stage but with a rational
coding 6 data and 3 parity blocks will
only consume 9 blocks of this space and
it will be as reliable as replication
and at the same time they can notice
that there is only 50% overhead as well
parity block stored for 2 data block now
let me tell you what very important
thing here guys so what you can do
suppose if you want to have 5
replications so you can keep 3
replications and you can generate 2
parity block so in that way you can
actually reduce our overhead so this is
how a roger coding I solved the storage
overhead problem in Hadoop three fine
guys in order perform this a rajab
encoding we need to make sure that there
are certain changes in the architecture
of Hadoop so let us see what those
changes are in hadoo
three so the first thing is a name node
extension so we have seen earlier as
well that we have stripped HDFS files
right so whatever files that we have are
broken down into small small blocks and
these blocks can actually form a block
code in which they'll be float a number
of internal blocks now because of these
many blocks the metadata that named node
has to contain will be huge and require
a usable disk space so in order to
reduce the name node memory consumption
from these additional blocks a new
hierarchical block naming protocol was
introduced the idea of a block group can
be inferred from the ID of its any
internal bloss so basically the block
group ID or you can see the block pool
ID will be there in the name node and
using that you can actually figure out
which data block is stored in widths
particular block group and this allows
management at the level of block group
rather than the plots and an exchange is
client extension so basically the client
read and write paths are enhanced to
work on multiple internal blocks in a
block group in parallel so as we have
seen that a block group contains
multiple multiple blocks so we can
actually perform the read and write
operation on those multiple internal
blocks at the same time fine geyser will
move forward and see some other changes
in the architecture so the third change
was a data node extensions so the data
node runs an additional rational coding
worker called as EC worker tasks for
background recovery of failed is a short
coded blocks the failed irrational coded
blocks are detected by a name node which
then chooses a data node to do the
recovery work so in order to do that
there are three key tasks that it has to
perform the first is it actually has to
read the data from the source nodes
then it has to decode the data and
generate the output data so what does
that mean
whatever the parity block that we have
generated and the other data block it
will use both of them and then it will
generate the data block that we have
lost then what will happen it will
transfer the generated data block to the
target nodes now the fourth point is a
larger coding policy now basically it
determines what sort of a Rajal coding
schema will be there and on what basis
you divide or split your
it can be on the basis of base or it can
be on the basis of size so on what basis
you'll do that so this is basically what
a Raj the coding policy is now they'll
move forward and understand the yarn
timeline service version - so I see on
loader in Hadoop - jana version one was
present but in Hadoop 3 yawn timeline of
service version 2 was introduced it
addresses two major challenges first is
improving scalability and reliability of
timeline service and the other is
enhancing usability by introducing flows
and aggregation this yawn timeline
service version 2 alpha 1 is provided so
that users and developers can test it
and provide feedback and suggestions for
making it a ready replacement for
timeline service version one point X it
should be used only in a test capacity
most importantly security is not enabled
so do not setup or use timelines always
version 2 unless
security is implemented if security is a
critical requirement
John version 1 is limited to a single
instance of a writer / reader and
storage and does not scale well beyond
small clusters John timeline version 2
uses most saleable distributed writer
architecture and a scalable back-end
storage yawn timeline service version 2
separates the collection of data from
serving us beta basically it separates a
write and read it uses distributed
collectors essentially one collector for
each on application the readers are
separate instances that are dedicated to
serving queries wire REST API yawn
timeline service version 2 chooses
Apache HBase as a primary backing
storage as Apache HBase scales well to a
large size while maintaining good
response times for read and write in
many cases users are interested in
information at the level of flows or
logical groups of yarn applications it
is much more common to launch a set of
series of yarn applications to complete
a logical application so yawn timeline
service version 2 suppose the notion of
flows explicitly in addition is suppose
aggregating matrix at the flow left
also information such as configuration
and matrix is treated and supported as
first-class citizens the diagram
illustrates the relationship between
different yarn and tidies modeling
clothes so as you can notice here that
we have flows here which is subdivided
into float on one and float on two then
again it is subdivided or the basis of
applications we have the application 1 2
3 &amp;amp; 4 then I get it is divided on the
basis of attempts and finally we have
containers and we know that it is used
to just provide the execution
environment Jean timeline service
version to use a set of collectors to
write data to the package storage the
collectors are distributed and
co-located with the application masters
to which they are dedicated all data
that belong to that application are sent
to the application level timeline
collectors for a given application the
application master can write data for
the application to the co-located
timeline collectors in addition node
managers of the other nodes that are
running the containers for the
application also write data to the
timeline collector on the node that is
running the application master the
resource manager also maintains its own
timeline collector it emits only yon
genrih clive cycle events to keep its
volume of price reasonable the timeline
readers are separate demons separate
from the timeline collectors and they're
dedicated to so vague queries wire REST
API fine guys so we'll move forward and
we'll focus on the shell script rewrite
part in Hadoop 3 so the Hadoop shell
scripts have been rewritten to fix many
long-standing bugs and include some new
features as well but we need to keep an
eye towards compatibility some changes
may break the existing installations now
let's move forward and see shaded client
jumps so Hadoop client which was there
in Hadoop 2 point expose the Hadoop
transitive dependencies in the hadoop
application parts so this create problem
if the version of the dependency is
conflict with the version used by the
application so basically what happens in
Hadoop 2 point eggs whatever
dependencies for your applications are
there for that you need to import
certain jar files
so sometimes what happened the version
of these dependencies conflict with the
version used by the application so in
order to solve that Hadoop 3 add new
Hadoop client API and Hadoop Client
runtime artifact that combined Hadoop
dependencies into a single job
so whatever dependencies are there the
real application will be combined into a
single job
now this Hadoop client is the compiled
scope and the runtime is the runtime
scope that contains third party
dependencies from the Hadoop client so
whatever dependencies are there it is
converted into a single jar and then it
is checked so instead of checking it
again and again we can combine all the
dependencies into a single jar file and
then we can compile it and then we can
check it so basically this avoids
leaking Hadoop dependencies into
application path I'll give you an
example for that HBase can use to talk
with a Hadoop cluster without seeing any
of the implementation dependencies so
we'll move forward and understand a
force in a stick container so basically
in Hadoop 3 there is a new type of a
container that has been introduced which
is called opportunistic containers so
earlier we hardly guaranteed containers
but now we have a portion istic
containers let me explain you what it is
so these opportunistic containers can be
dispatched for execution as a node
manager even if there are no resources
available at the moment of scheduling
now as a moment of scheduling if you can
remember in the guaranteed containers if
there are no resources there it won't
provide the execution environment but
that doesn't happen in the abortionist a
container so what happens if there are
no resources an unfortunate stick
containers in that case these containers
will be queued as a node manager waiting
for resources to be available for it to
start and let me tell you guys
opportunistic containers are of a low
priority than the default guaranteed
containers and are therefore preempted
if needed to make room for guaranteed
containers and this improves your
cluster utilization opportunistic
containers are by default
allocated by the central resource
manager but support has also been added
to allow opportunistic containers to be
allocated by a distributed scheduler
which is implemented as an application
master
source manager protocol so basically in
the guaranteed containers after they're
in Hadoop - it has to wait for the load
manager to provide it with the resources
and then only it will process the
requests that are there but in the
poncho mystic containers it will be
queued at the node manager waiting for
resources to be available for it to
start that's the only difference here
and we know what I'd wanted it provides
basically it improves your cluster
utilization so MapReduce has added
support for native implementation of the
map output collector for shuffle
intensive jobs this can lead to a
performance improvement of 30% or more
it adds a native implementation of the
map output collector the native library
will build automatically with penis for
shuffle intensive jobs this may provide
speed ups of 30% or more so we know what
our shuffle our intensive jobs and let
me tell you guys this is only there till
the a mapping phase for reduced space it
is not present so there recently working
on the native optimizations of the map
tasks based on j9 the basic idea is that
add a native map output collector 200
key value pairs emitted by mapper
therefore sort spill can all be done in
native code there preliminary tests on
xeon e5 4 1 0 and jdk 6 you 24 showed a
very promising results and let me tell
you guys sort is about 3 to 10 times
fast as fast as our and this led to a
total speed of speed-up of about 2 x - 3
x for the whole map toss and keep this
thing in mind that the merge code is
still in development phase alright so
let's move forward and see how high
availability has been achieved in hadoop
3 i do.3 actually suppose more than two
name nodes and in the previous version
of Hadoop we know that it supported only
two name nodes one was active and
another one was passive so this was the
case in Hadoop 2 we have one active node
1 passive node and we have journal loads
so these journal loads are used in order
to synchronize both of these nodes there
is the only one active name node which
will be monitoring your data nodes and
then we one passive node which will ask
contain the metadata or the stored in
the active name node so that if it fails
it can actually take its place so that
passive can become the active name and
as I told you earlier is when these
journal nodes are used in order to
synchronize both of these and what's all
synchronization basically whatever
metadata is there in the active name
node will be there in the past in a node
and apart from that also make sure that
there is only one active node present at
the one instance so this was Hadoop 2
now let us see Hadoop 3 what are the
changes so in Hadoop 3 we have actually
three named nodes that is one active
name node and we have two passive name
nodes and apart from that we have five
journal nodes and authority orders where
these are used in order synchronize
these multiple names so this
architecture actually provides high
availability because even if two of your
name notes pain we have a one more name
node that can take its place next up
we'll see what all posts have been
changed in Hadoop tree so previously the
default ports of multiple Hadoop
services were actually there in the
Linux ephemeral ports range between 3 to
7 6 8 to 6 1 2 to 0 and we know that a
similar range is nothing but a few posts
that are required by Linux services so
what happened at startup services would
sometimes pain due to conflict with
another application so in order to solve
this conflicting posts have been moved
out of the ephemeral reach now let's see
what all ports have been changed so for
name node posts that were there earlier
in 5 zeroes 4 7 0 has now become 9 8 7 1
similarly 5 double 0 7 0 has now become
9 8 7 0 and the rest of it you can see
it on your screen right now so we'll
move forward and understand object
storage file system connectors so over
here they are integrated Hadoop 3 with
Microsoft Azure data Lake and illusion
object storage system file system
connectors so Hadoop 3 now suppose
integration with Microsoft Azure data
leak and alien object storage system as
alternative Hadoop compatible file
systems so that's all guys this is the
maximum that I can tell you about it
right now it is still in alpha phase as
I'm telling you again and again fine
guys let's move for
and see one more change so it is about
having intraday denote balancer which
was not there earlier so what used to
happen let me show it to you with a
diagram so this was the condition in
Hadoop - so what happened in Hadoop - a
single data node manages multiple disks
as we have seen earlier is whether there
are multiple discs in the data nodes and
these this contains a small data blocks
so during normal write operation this
will be filled up evenly so whenever you
are writing some data in the data nodes
it will be filled up evenly in different
disks that are present in one data node
however adding or replacing this can
lead to significant skew within a data
node but when you are actually adding or
replacing disk that can lead to
significant skew within a data node that
means some data block might contain more
data whereas other data blocks might
have less data so because of that when
you are accessing the data some disk
will take more time whereas other dish
will take a less than and this situation
is not handled by the existing HDFS
balancer which concerns itself with the
enter not enter data node skew which
means that it mostly focuses on the data
blocks to be distributed evenly among
different data nodes but not between
this different discs within the same
data so this situation is handled by the
new intra data node balancing
functionality which is invoked why the
HDFS disc balances CLI and it is nothing
but used to make sure that all of your
data is evenly distributed among these
days which are present inside one data
node fine guys will move forward and
focus on the last new feature of Hadoop
tree so it is nothing but a scheme
management the Hadoop heap size variable
has been deprecated and in its place
Hadoop each size max and Hadoop each
size min have been introduced default
heap sizes have been removed this will
allow the JVM to use auto-tuning based
upon the memory size of the host to
rename the old default configure all
global and daemon specific heat size
variables now support units if the
variable is only a number the size is
assumed to be in megabytes fine guys so
these are all the new features that are
there in Hadoop 3
when we compare it with Hadoop - so as I
told you earlier as well it is spindle
in alpha phase the dots the first go to
beta phase and then it will go to the
production phase and after that we'll be
coming out with a new videos about
Hadoop tree we'll see how to perform
hands-on using Hadoop tree so thank you
for watching this video
you can comment your queries and
questions do subscribe to our ad rekha
channel to learn work thank you and
happy learning I hope you enjoyed
listening to this video please be kind
enough to like it and you can comment
any of your doubts and queries and we
will reply to them at the earliest to
look out for more videos in our playlist
and subscribe to our Erica channel to
learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>