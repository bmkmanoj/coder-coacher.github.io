<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Is Hadoop a Necessity for Data Science? Big Data and Hadoop Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Is Hadoop a Necessity for Data Science? Big Data and Hadoop Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Is Hadoop a Necessity for Data Science? Big Data and Hadoop Tutorial | Edureka</b></h2><h5 class="post__date">2015-10-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eBApgk81Wcc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so my name is Ashok I have 20
plastic years of experience in
information technology the last three
and half years in big data and related
technologies analytics data science and
things like that
IOT and all that so I look forward to
spend the next one hour with you and
really looking for an interesting
journey for the next one hour so let's
get started friends thanks thanks for
coming again yeah
so today's topic as you can see it's
quite interesting Hadoop a necessity for
data science right so let's get started
so just a minute so what will you learn
today if you look at it let's have a
quick poll do you know the following
like what is Big Data Hadoop what is a
data product what is data science why
how do for data science is Hadoop and
necessity for data science so these are
all the things we are going to see that
in a nutshell today so I'll try to cover
cover
how much ever possible friends so I have
like 20 out slides and that also
includes a demo towards the end right so
Mohit okay voice so friends are you
anybody having audio issues
thanks for the confirm
France oh yeah everybody seems to be
perfect even Mohit also just clarified
to me so he's just doing a check at his
side all right fine
not able to hear anything so I'll coast
oh all right
others can hear me all right
so French what we will do you know what
we will do is we will the the structure
of this today's presentation right the
way I want to keep it is let's go over
the neck because I know everybody's
inquisitive and I'm sure everybody has
got questions correct so but just you
know in the interest of time and you
know and also in the interest of the
audience you know I don't want to
disrupt the flow of the session correct
I don't want to disrupt the flow of the
session so what I will do I will take
you through the session please allow me
to do that right and and you can always
bring up questions you know when when we
have questions at you know towards the
end so what I will do you know before I
go to the demo I'll pause for question
Q&amp;amp;A for some time then after the demo
you know we love Q&amp;amp;A as well click and
you know I'll try my best to wrap it up
in one hour correct that's the agenda so
so what is what is Big Data and Hadoop
correct so this is the first thing we
are going to see big data is a popular
term used to describe the exponential
growth of data can be structured or
unstructured or a combination of both
right so what we are saying is it can
you know if you look at the today's
that's right this is a no-brainer you
know the data is coming from various
sources today right
we have varieties of sources whether it
is an application server or you know a
web server or from all these devices
like feed print and you know any kind of
IOT devices or any kind of far you know
network devices correct
and all the social media sites say
everything is you know it the data is
coming out of that correct and these are
the things which we cannot we can no
longer ignore these things right because
in the in the past it was just one
database and you know very structured
the way of collecting the data now the
data collection or you know it starts
from the instrumentation all the way up
till you know visualization so when when
the data collection happens right so we
want the data to to live to live forever
right because we have realized the value
of the data we have realized the
potential of the data we don't want to
shell the data or archive the data
correct because we want the data to be
available for analytics all the time
right so we have you know thought
through various architectures that can
actually hold the data forever right so
if you look at the stats again it is
predominantly 80 to 90% of the data in
the world today is unstructured and the
data is exponentially growing as as
mentioned in this slide right and if you
look at the velocity of the data which
is the the growth you know at which the
data is growing right that the the pace
at which the grade data is growing it is
phenomenal it is just a simple example
you know New York Stock Exchange they
are saying they are generating like one
terabyte of new trade data every day
correct one terabyte of data right which
is nothing you know you if I tell you a
few other things these things get you
offed by that right the Hadron Collider
correct it's one of the largest particle
colliders they are doing a you know big
experiment in Switzerland right so those
that Collider generates about about a
petabyte of data every 3.5 seconds
correct phenomenal amount of data
phenomenal amount of data this is a very
famous definition you all would have
seen
the dimensions of big data
Representative 3 vs right what is the
velocity real-time right that the rate
that the pace at which the data is
growing and volume it's like in all the
social media if you look at Facebook if
you look at Twitter you look at tumblr
Instagram all these sites are growing
like crazy right and you are talking
about you know they are processing
millions of queries and you know and and
Twitter is like having 500 million
tweets per second a sorry birthday
correct its 500 million is staggering
right so that's the kind of an and we
have about you know 500 million seniors
we're processing called eight data
records right so everything is like the
volume is staggering correct the volume
is staggering okay alright so and the
variety we talked about it right we
talked about the morenae we said though
from what sources from what sources we
are getting and we are getting
structured unstructured and
semi-structured we have lots and lots of
varieties we are also getting this data
correct variety is also playing one of
the row and one of the fourth dimensions
which we haven't mentioned here which is
there in your IBM is this which is your
can you all guess it friends what is the
fourth dimension in the four V's
veracity okay can someone tell me what
is veracity what is veracity friends
it's a very interesting one truthfulness
fantastic truthfulness right fact-based
information can you give me an example
it's a lovely thing I mean what I feel
personally feel is no it has to be
accorded the same amount of respect what
we are core what we give for velocity
volume or variety right because
truthfulness if some field got missed in
the database it should still process the
data is that what it is sufficient a
truthfulness or T you know the validity
of the data itself right validation
correct right the validity of the data
or if there is any
certainly uncertainty in the data can
you give some kind of can you give me an
example which happened quite recently
quite recently can you give me that
example that happened in the automobile
industry I just give you all the clue in
the world fox wagon fantastic option
yeah I think you're all emission scandal
right correct
fantastic friends folks wagon emission
scandal so they just fetched a small
device right this watched for small
device so that it will you know whatever
the emission of nitric oxide it goes
undetected in the emission test you know
whatever that is done in the research
labs there right so they just fetched
that and people are you know they came
up with flying flying colors and the
same guys you know they tested BMW
upside down you know they found that to
be extremely successful and they also
certified folks wagon especially the
Passat model and see what has happened
right it is the case of veracity right
the truthfulness see if there is any
ambiguous data coming and write the
whole thing friends what I'm saying is
if the data is uncertain if the data is
ambiguous whatever coming out of the
data it may still make sense to you
because and you may be you may make a
you know you may derive some insights
and based on the insights you will come
up with a product decision or you may
come up with a major business decision
itself so that may prove to be
completely wrong if the data itself is
got some issues right so that's the
classic case thanks thanks for
explaining Fritz I think you're all
right up there in understanding this
veracity part yeah so Hadoop is a
programming framework that supports the
processing of large data sets in a
distributed computing environment first
and still the best tool to handle big
data I think there are lots and lots of
tools today right today if people are
talking about spark people are talking
about you know storm people are talking
about sparks sparks streaming people are
talking about flink and then you have
lots and lots of derivatives and you
lots and lots of flavours coming out and
out of Apache how do
still continues to be one of the most
powerful ones and Hadoop's
you know the investment is close you
know it is it will get closer to two
billion dollars by 2020 and it's growing
at a staggering rate of 58 percent
year-over-year right so there's no way
Hadoop is going down you know there are
lot of misconceptions people say that
you know spark is going to overtake it's
not like that because park is ins
in-memory computing and you know how do
place for a different purpose correct
so I mean how do can handle elephants
that's one of the reasons why they put
the elephant here right so it can just
take any amount of data right and what
is the difference friends between Hadoop
and your traditional RDBMS can someone
tell me quickly
any quick any fundamental no sequel
database okay
all right can handle unstructured data
fantastic
yeah that's again of difference I want
friends the framework yes and
distributed file system excellent
correct unstructured data distributed
file system okay that is yltp this is
all app correct find schema and reads
came on right I think lot of I think you
guys are rocking here correct works in
different cold moves to the data
brilliant uncial yeah lots and lots of
great points are coming in I mean what I
other thing I wanted to talk about was
the architecture piece right with Hadoop
right it is more a scale out
architecture not a scalene architecture
a scale up architecture right
it is kale out scale out architecture
meaning if we need to if we need to go
right I mean if if I am expanding if I'm
expanding the data size is you know
increasing and all that which is more
horizontal scaling not vertical scaling
exactly correct so that that also it's
one of the differences and of course a
lot of people mentioned that you know I
am taking the program to the data not
the data to the program that's one of
the significant differences correct
that was the first still the best okay
sure brief history of Hadoop 2004 Yahoo
creates 2514 to work on Hadoop Apache
project establish job
Yahoo begins to operate at scale
Haughton works data platform okay focus
on innovation actually even Yahoo
started it and you know later it was it
became a full-fledged you know Apache it
was became a full flesh apache platform
it was incubated and you know became a
formal launch correct and I think as we
speak I think we are running now 2.7 -
if I'm right because every day you know
things are evolving right things are
moving pretty fast so it's it used to be
2.6 then I saw 2.72 one of those formal
releases I don't know what it is today
but it should be around that 2.6 plus
right 2.6 2.7 types
correct so it provides you know how do
provides a lot of fantastic features you
know it provides a economy on scale and
it provides reliability through fault
tolerance and it provides flexibility
and these are some of the standard
features of Hadoop correct most
efficient for large skilled storage and
processing correct distributed file out
file system and self-healing datastore
correct and MapReduce distributed
computation framework that handles the
complexities of distributed programming
correct so we have this competition
co-located with data so that makes it
special as many of you pointed out data
and computation system co.design and
co-developed to work together you all
would have seen the name node and the
resource manager you know name Lord is
the master daemon for storage and
resource manager is the same thing for
processing they actually sit in the same
server in the distributor cluster
correct and similarly on a slave nodes
and all that you have you know similarly
you have data node and you know node
manager is for the processing slave
demons right they
that so it is one of the frameworks you
know where they have conquered this you
know well in a traditional system what
we do we move the data to where the
program is running that actually
cripples the network bandwidth and it
can crash the system as well that's why
we were doing the archiving and all that
part right and we take only part of the
data because we can't we cannot afford
to slow down the network right so we did
all those things because we didn't have
a proper architecture in place to handle
a high volume data correct so now with
the advent of Hadoop coming in now we
know how we can do this so because the
data and computation have become
together right so the compute layer the
date the the the program itself is taken
to the place where the data is located
so this feature is called data locality
right it's a very one of the most
significant standout feature in Hadoop
data locality really you know makes it
very powerful right the data is never
moved the program is more right process
data in parallel the other thing is the
data is stored as blocks right as chunks
in various commodity servers that made
that also makes it very special in
Hadoop right so we are able to do things
in parallel
I'm already hardware nodes self-healing
failure handled by software itself
correct then design for one right and
multiple writes reads correct there are
no random writes optimize for minimum
seek on hard drives correct and Hadoop
provides high throughput and the high
latency as well because you know I think
it is I think some of you pointed out
that you know our DBMS is more away ntp
this is Mora ola P it is by design
correct fritz okay what is a data
product a software system whose core
functionality depends on the application
of statistical analysis and machine
learning to data correct so if you look
at it okay I I want to ask you this
people you may know where do you see
these friends where do you see
feature people you may know okay
bingo all of you are saying LinkedIn
correct Facebook okay
facebook also yeah okay sure so how do
you think they are providing this
feature Facebook LinkedIn okay
how do you think they are able to come
up and and French do you all agree with
me the people you may know
it may not be so you know most of the
times at least in my case right it is 40
to 50 percent of the times I find that
link to be meaningful and I go and
connect with the people correct most of
the times so I mean I I would say 50
percent of the times correct so that's
that's fairly a a good hit rate right
how do you think they are doing this any
guesses machine learning excellent
correct machine learning says Bhushan
correct so they should do some kind of
you know they are definitely running a
machine learning algorithm for sure
correct
machine learning correct and and they
are doing it for I mean they they do it
they do things like you know for example
you might have heard about a tool called
mahute right it's a ecosystem tool of
how do correct so that one does
recommendation engine right there are so
many recommendation engines and you know
they also look at your click stream
pattern and let's say mandolay analysis
and you know come up with all these
things definitely machine learn nothing
plays a part in this correct so they do
all these things come up with
recommendation even google ads right you
would have seen amazon books if you are
reading this book it goes and suggests
okay go and okay people also bought this
book it says that right people who
bought this they also bought this book
so that also gives lot of relevance sure
so they use that algorithm correct so
look at this including results for data
science see again look at this this also
where how do you think they are
suggesting this okay regular expression
than you
anybody else friends past clicks
keywords mapping yeah there's so many
things right frequent correct correct
based on popular search criterias
pattern matching prediction that's a
same machine learning yeah machine
learning is also there pattern matching
is also there text mining fantastic yeah
they do all these things if somebody who
is doing this right it pretty well knows
that you know you're probably it's just
a typo here and it already
it doesn't auto suggest correct with
pattern matching with data mining
fantastic
sameer on right so when they do this
right they also list down the data
science things below they don't waste
time by giving you head did you mean
data science yes but they already say
this is what we are trying to do but do
you want results only for this they are
still asking this so they are they are
already gone ahead with this right so
it's all you know very intelligent way
of doing things through machine learning
pattern matching you know as you all
right Lee saying data mining extensive
data mining here right so what is data
science so we are embarking on what is
data science extracting deep meaning
from the data finding gems in the data
so data mining is hard work yes this lot
of gems hidden in there look at that
so friends just to give you a small
heads-up in good old days in the
beginning of 19th century or even little
more a little more on to 19th century
right or 20th century correct so we were
all people were running after gold right
there was a heavy gold rush everywhere
right especially people in the US mass
exodus to California I think that's how
people say you know you know people even
found California right so there's a lot
of science and a lot of history to that
I'm not getting in there my point is
that was the gold people you know this
is a big gold rush moment then 60s 70s
1960s 70s and all that you know middle
most of the Middle East prospered
because you know they call it black gold
right petroleum and all that today data
the latest gold right so without mincing
any facts I'm telling you if if a smart
analyst if that person knows how to mine
the data right how to mine the data and
extract the insights or the gems that
person can go a long distance right so
that's precisely the nature of the job
for a data scientist right given the
data otherwise you know the data will
exist right it becomes like a dumb data
correct
if you can really inert lot of insights
from the data then it goes a very long
way correct you can do wonders with that
look at some of the common data science
tasks clustering detect natural grouping
so just a give a small example of that
you know clustering is also you know all
about you know you look at some patterns
correct you look at some patterns and
group them correct that is your
clustering outlier detection so again
you know you look for some anomalies or
some bad data right something which is
outrageous so even a data scientist may
check with the business analyst even in
this case because the outlier detection
is more like you know you know just to
give you a small example friends you are
doing a you know like a mean and median
of somebody's salary correct of salary
for a particular city then suddenly the
one of the salary goes like a million
right that person may be acting or you
know you know you know he may be a rock
star there you know it you cannot
exactly classify that this an outlier
because you know you redo first you need
to detect the anomaly and still check
and take a meaningful decision again
affinity analysis look for a
co-occurrence of patterns classification
okay this is like predicting a category
right doing a regression correct you can
do a linear regression you can do a
logistic regression there are multiple
ways to regress and find out and come up
with predictions right
regression and recommendation predictor
preference what we saw just now so these
are all the descriptive ones these are
all the predictive ones correct so
building data products delivering gems
on a regular basics basis sorry about
that
pre process build model and periodic
batch processing and throw it onto the
sequel so you can I mean this is just a
model so you can use anything here
correct
why Hadoop again why Hadoop for data
science why what do you think friends
you have seen what is Hadoop correct we
have seen what is Hadoop and we got a
dip stick view on what is data science
why do you think how do where do you
think Hadoop will serve the purpose
before I go deeper I just want to put
this question to all of you for White's
economy on scale helps the data
scientist best way to handle high volume
data any kind of data efficiently right
so size of the matter size of the data
is really doesn't matter so long if we
can handle it big data in a systematic
way okay so a lot of people are giving
Hadoop can work on humans data and
QuickTime QuickTime so one thing is
friends as I told you how do provides
high latency unless so what I mean is
you know you cannot hunt down a
particular record and all that it
processes things in blocks blocks of
data correct blocks update a high volume
of data in a very effective and
efficient way so if if I want to read
something in our DBMS right if you walk
across a bank and tell them okay here is
my bank passbook and this is my name
they will be able to pull your record
maybe in a couple of seconds that is not
possible inaudible but if you tell the
Hadoop okay go and read five terabytes
of file it can still do that in few
minutes which our DBMS can probably
never do that
correct it may probably take ages and
fail and all that stuff because because
of scale-up architecture right here it
is kill out sure
so hope is very powerful and since we
are dealing with large volume of data
right so that is becoming one of the
popular choices for even data scientists
to work on write data scientist data
analyst you can have any of these
combinations to work on Hadoop explore
the entire data set if you look at this
so this is my rags and all that data
processing cycle storage data collection
he's heard of more scalable arun is
asking absolutely scalable correct
absolutely scalable so I can have I can
go on adding clusters I mean our
commodity servers to make cluster or I
can even have more Rackspace more
servers I can keep adding the service
exactly correct so explore the entire
dataset it's possible mining of large
and data sets better outcomes so the
more and more data you have write better
and better the outcome becomes correct
friends so it is like even even you know
it is for a data scientist even if you
are training a model for example a
particular algorithm
people always say we train the model
then they come and you know then the
algorithm becomes more stronger correct
so they always say that the more and
more data it always yields a better
outcome 80% of the data data science
book you will be astonished so this is
the this is the truth actually raw data
sampling filtering joins entity
resolution strip away all these things
document vectors generation term
normalization right 80 percent of the
work is actually involving the cleaning
up the data correct data preparation
they say they do lot of transformation
you know when they say cleaning up right
they they set the data because it may
come in different formats it may have
some web BOTS lying there so they need
to make the data they need to give
meaning to the data before the data gets
analyzed correct so the data has to be
in shape generally most of the times the
data will never be in shape correct so
they need to make the data make it
available
and you know make it consistent so that
it becomes good for analysis correct
what are the okay just do one question
friends before I go for that there is a
question here what are the other
frameworks like Hadoop for data science
I think
Raveendran there are a lot of you know
Hadoop is taken in conjunction with data
sense because of its ability to handle
large volume of data ability to handle
high-throughput correct
all these are plus factors so people
also do if you are only looking at some
regression and all that I think yeah you
can look at you know are you can look at
SAS you can look at you know some of
these tools as well which provides you a
statistical insight on your from the
data science perspective correct so that
is one way but you can always use it in
conjunction with Hadoop that's what we
are trying to say here correct okay our
DB misuses schema on right whereas
changes expensive high barrier for
data-driven innovation
sure speed barriers of traditional data
architecture I need new data finally we
start collecting let's see is it any
good
correct so our DBMS has lots and lots of
you know schema and right meaning
without doing anything right without
doing anything in the sense without even
inserting the data before you insert the
data right you need the schema in place
so that any change right you need to go
go to the you know DDL you know make all
those changes it is quite expensive and
you know that's not the way we work
today right because the data will come
in a very sparse format the data will be
completely unstructured so those kind of
things are DB miss beam is already
falling flat here right so so it is
really required for you for you to have
an accelerated data driven innovation
when the data is predominantly
unstructured you need more flexibility
here and you need you know Hadoop
especially support schema or read misses
c'mon right correct things can happen
a fast and very volatile scheme Andre
means faster time to innovation I need
new data is it any good no barrier
correct data driven so that's precisely
what we want right if it is a high
barrier meaning there's any changes if I
want to do it with the data you know you
need to go go and make this changes to
the schema everything right that's going
to tire down or discourage anybody who
wants to pursue or who wants to innovate
in the data front correct so this one is
it's a radical thought it completely
makes it very very easy for anyone to
move forward correct
so it accepts anything because the only
time the schema is required is while
reading it okay before I want friends
there is a question here can I update a
value in thousands of tables in 1.5
terabyte database using by dupe
he said Python on Hadoop Tanner oh is
that what you're asking Python on Hadoop
okay ah
thousands of tables okay I've used
Python at a very very surface level that
too for data science but not in
conjunction with Hadoop and Niru but in
Hadoop 1.5 can I update a value no but
what I can tell you that I do you I mean
you can you know if you are having it in
tables especially you know no sequel
database correct no sequel database it
is possible whatever you're saying if
I'm using I HBase or something database
I just wonder just a second friends just
a second yeah oracle DB or sunny Roo I I
you want to update in Oracle or you want
update in Hadoop which one are you
talking about Oracle ok 1.5 no it is
possible why no sequel DB require when
we keep data in HDFS no so men the point
is HDFS is a file storage no sequel is
actually no sequel is actually a
database correct it is a database HDFS
is a plain file storage sure there is a
significant difference
between these two things correct sure
okay so um I just want to understand the
set of Hadoop environment is very costly
or moderate Mohit is asking it'sit's
actually friends can I can I just do the
demonstration and come to these
questions correct so I I will probably
finish that because we are not done
there we can yeah required the setup is
not very expensive Mohit if you have the
expertise I think it's not so expensive
because it's open source Hadoop is open
source end of the day we can use
MapReduce one Lee saman is asking it's
not true some in there are things like
spark is there there is things like
storm is there right there are so many
processing frameworks are available
today correct okay so of course towards
the inference I'm going to do a you know
please complete the survey when once you
are done right
once I'm done well I I would request
your feedback you know that's going to
be very important for us so let us
certifications I think I don't know what
they have to say here
okay that is a Big Data certification
okay you can always came for this bat
starts from 17th October you know you
can give it a shot
sure so that is something starting here
reading and presentation okay so these
are some of the features of Hadoop perm
in the area so I I just want to
highlight this then I want to move on to
the ah this is it just a second thank
you friends once again so there is a
demo so can I finish this and come go to
those slides I think I missed the demo
part so I'll quickly show you a you know
any analyst right data scientist any
data scientist right they also they are
quite familiar with all these things
right they they are also technically
equipped most of the data scientists are
there they have a good background in
mathematics or statistics and they also
have a good
analytical aptitude so I'll just show
you a very quick you know a small
program how we you know I just want to
do a quick analysis fine so friends I am
just doing a quick you know I'll show
you this demo okay I have couple of
datasets okay I have a couple of data
set one is the customer fine
you find all the sorry customer records
are here this is a customer code first
name I'm sorry
last name age profession correct
sure and you also have you also have
transactions for that customer correct
the transaction ID and you have a date
transaction date customer code the sale
value right here is my category here is
the product and I have the city state
payment mode are you all clear with the
data set France I haven't told you what
is a problem I need to do but are you
clear with the data set what I'm trying
to say you have a customer file and you
have a transaction appropriate
transactions file is that clear
so far also okay very very simple I you
know what I want to do the problem
statement is all about for each customer
I have about 10 customers right here 1 2
3 4 5 6 7 8 9 10 I have 10 customers for
each customer I have multiple
transactions right 4 0 0 1 is one of the
customers and you know yes
that person has Christina all right
Christina has bunch of transactions here
here here
6 7 transactions are there right
so for each customer similarly I have
multiple transactions now all I want to
do is to print for each customer I want
to print the name
and I want to print the number of
transactions and I want to print the
total number of transactions a total
sale value right I need to have a sum of
all the transaction amount correct for
the number of transactions is that clear
so how would you do if I give you if I
just give you these two tables in sequel
what's your thought process friends
Howie what will be your approach here
right there join fantastic so what will
be a joining attribute here join and
group exactly joining on the customer
correct so how many lines would you do
it friends how many lines will be a
sequel code two to three lines okay sure
okay it's not the case in MapReduce I am
going to show you the program in Java
four lines okay whatever right so it is
going to be a little more you know it
since Java MapReduce right it is if you
are going to use joins and other things
right my natural choice would be to use
big or high right
they are pretty fast and you know they
are you know how big is a declarative
language and it's really quick that way
right so I'm going to show you a
traditional MapReduce so as a data
scientist right one thing what I want to
tell you is you need to know which tool
to use where because data scientists
they analyze the data that's true they
also know they should know how to
optimize an algorithm and all that they
are very scientific to write so I'm
going to show you in a very traditional
Java program like how we do the joins
and other things right so that's that's
what I want to demonstrate today correct
so look at this this is mine these are
the two files have already moved into
HDFS correct this is my HDFS so if you
look at this these are the two files
have already moved it fine so in my
Eclipse correct I have something called
customer mapper so I have like you know
let me go to the driver class my main
class right so I am saying how to move
this
miles to the hdfs okay uh I will do a
put commander up do I will just say her
HDFS DFS you know I'll just say
something like this let me just type the
command for you it's it's it's a pretty
easy step I just say HDFS DFS put
whatever is the file X file whatever is
a file right whatever file put that in
whichever location you want to put it in
HDFS correct it's DFS location is that
clear how do you PFS put yeah that's
right
correct so this is my syntax so this is
what I have done I already put it so
what I'm doing friends the mapper right
I want you to understand it's it's not
so easy to implement in Java but you
know it is quite powerful once it is
implemented so I am saying my I have got
to mapper glasses running right my first
one is like you know I'm giving
command-line argument and I'm going to
execute the jar file here once you build
the jar sure so my path argument 0 which
is 0 based right so I'm going to give
the location of the customer mapper
which is located in this location right
HDFS this is where I have sure correct
this is my this is my second one correct
this is where my transaction mapper is
running correct and the third one is the
output well what I want to generate for
because that's the problem statement
what I mentioned sure ok now Johnathon
Schaech saying he has already done Big
Data developer from Eureka surgeon they
should see data science you know you can
go to you know machine learning
algorithms that's covered in depth and
you'll also get to do lot of business
analytics correct you'll also get into
the recommendation engines business
analytics you will do not
lots of exercises in our and how to
connect how to work on our and
conjunction with Hadoop all those things
are coming in their check fish right
sure okay so from my you know what I
want to do here is I'm going to do a you
know output path like okay this is these
are all the standard ones you know I'm
just giving the output path which is my
third parameter right so in the first
mapper what I want to do I want to split
this correct I'm writing I'm reading the
file which is this file I'm reading this
file sure and I want to split that into
based on the comma right
it's a stringer I am giving a string
array I'm doing a record not split based
on the comma because comma as the field
separator there then I'm man I'm writing
the mappers output as this one parts of
0 let me just give this let me just give
this friends so that you know it becomes
very clear to all of you this is my
mappers output right so what is parts of
0 friends just curious to know what is
parts of 0 here all right you all got it
correct parts of 0 is this one
sure and what I am writing this is my
key this is my key this is my key and
this is my value this is my value what
is parts of one here first name perfect
first name right so it is kristina
correct so this is my cust this is how
it is going to be written what about the
transactions can you similarly help me
here in the transactions mapper what i
want to do a record is equal to value to
string or
similarly splitting right so before I
write Christina right friends one thing
I forgot to tell you I also write this
is how I write write customer /t
Christina this is what I do right and in
this in this part right transactions
what do we do
similarly I'm saying parts of two just
give me a minute here you go for the
transactions mapper what I do this is
the one sure this is what I write in the
transaction mapper lots of two is what
friends here what is parts of do here
customer ID correct customer ID so it is
four zero zero whatever is going to come
right okay what is parts of so I am
going to write something like this right
transactions again I'm hard coding it
here and parts of three what is parts of
three here any guesses the sale value
right 47 3 3 whatever right ok now how
many times I should be writing for this
for all the transactions right all the
transactions I'm going to be writing
this then what happens friends I think
you all may have basic idea what Map
Reduce some of you may have correct some
of you already done so can you all try
to answer what happens after the mappers
output is generated what happens prints
what is the next step in the MapReduce
sort and shuffle Genesis you know since
he has attended that right excellent
sort and shuffle and sought okay sure so
what should happen
excellent so what should happen and then
fit through the reducer correct so this
one you know what I am saying is it'll
come here right so this one will come
multiple times friends because you know
we have for the same customer I may have
85
I'm just saying you know how many our
number we have right just throwing some
numbers here right so it has four three
times let's say fine three transactions
this customer has so how will be the how
will be the values generated to the
reducer as an input after sorting and
shuffling do you all agree with me on
this like it'll create a list of values
right it will have something called cust
ste Cristina and let's say I'm sorry
and similarly transactions t / t 47
transactions d n e5 and so on so forth
right 233 whatever correct
this is what I will is that fine fritz
this is what I will have is that clear
this is what expected to go to the
reducer correct this is expected to go
to the reducer now I want to tell you in
this simple MapReduce program what I am
trying to do is this is my logic to find
out thee to print these total sale value
as well as for every customer right
that's the idea what we are I mean
that's the business problem we are
trying to solve so I will
I'd rate on the values so which one
friends which one I should be hydrating
on here exactly when I say values in the
reducer the transactions all those
things right this this done this whole
thing is my value correct this whole
thing is my value right this is my list
right so this is my and n-not and this
is the key after it is shuffled this is
my key and this is my value this is how
it is going to be fed to the reducer
correct so I'm getting this so I am
passing this so I am saying I am again
for each value
because I have hard-coded just to make a
split right because I'm going to split
it on T you know tab I could have put a
comma also you know I I'm just choosing
to you know I'm using a delimiter why I
am doing that because I want to identify
or distinguish whether it is from the
customer or the transactions right if it
is a customer if it is a transaction I'm
simply declaring a variable you know
right above this you know if you see the
cord and I'm just having a local
variable here count right so I am just
incrementing the counter because I need
to print the total number of
transactions and I am also totaling the
parts of one which is my 4785 and all
these things can you all hear me friends
okay so this is my this is my this is
how I do the simple calculation here if
it is not equal to transactions what is
the other option for its what is the
other option I will have if it is not
transactions what is the other choice
the customer correct so it has to be it
ought to be customer if it is customer
the only thing I do is the parts of one
this is the parts of zero this is the
after /t this is the one so I am simply
assigning the name to my you know again
a local variable so what I do I have to
print the name and so I'm printing the
name as the key the value as the count
and eat order sure are all clear friends
so I am meeting the objective of
printing for every customer what is the
total you know what is the total number
of transactions they are dealing with
and also along with the total sale value
or your clear totally clear okay shall
we execute this and see just a simple
file right let's execute this and see
this just a second
I'm going to create a jar file
sure and let me say advanced adv jar
sure and done okay so I'm going here
look for the adv jar it's there now I'm
going to say Hadoop jar adv dot jar what
is my input file imprints let me
maximize this input file name I have it
in HDFS mr input here is my path that's
exactly what I give sure costs mr output
sorry mr rejoin transactions mr output
and let's say join and I say data
science right help you say des October I
just want to create a folder October 30
sure
let's try this so this is where this is
the location in which I want to generate
the output sure that's exactly where I'm
coming too
done so I go to the mrr mrr output this
is where I generated the file you can
see Christina had eight transactions
total sale value similarly so many other
ten customers they are all having
transactions right so okay the relevance
is you know the data scientist has
little difference they should know as a
data scientist they are very scientific
as I told you they fix the algorithm
they optimize it and all that they also
need to know what kind of solution they
have to use in different scenarios right
so this is a simple one what I showed
you which is using Hadoop right because
the the storage is Hadoop and you know
I'm just doing a simple processing in
Hadoop so the data scientist also they
try to use various algorithms where to
use water and all that they need to know
correct so that in order to analyze or
or even cleaning up right they may use
pig to do a data transformation because
Pig is an effective cleaning of tool you
know it's it's a transformation tool the
various ways to look at this so friends
I'll go back to the deck
I think I kind of you know just so as he
told you you know your feedback is going
to be super important and you have these
certifications and you have this patch
starting up pretty soon and you know as
some of you asked I have done big data
from head to raker and you know it is
absolutely you know Danis science you
know pretty vast and you know there's a
lot of useful topics I mean it's it's a
very expansive course again and you can
check with the indica you know what is
the course criteria and all that you can
check on that but now let me take up few
questions we are we are just at 10 o
clock okay now I'll take up questions
for the next ten minutes friends so that
we can end it on here
maybe 10 minutes from now max how do
they use various algorithm in MapReduce
or in the result of MapReduce so
MapReduce is a processing framework of
each other it's part of Hadoop when I
say algorithm right you can use
different algorithms to use you know
first of all Pig also uses MapReduce
right I also use use of MapReduce in
certain cases do you really want to use
pig or hive or the raw Java MapReduce
because if the data is highly
unstructured you may still want to use
Java right because Java has fantastic
libraries and all that to handle various
kinds of data the course contents some I
know if you are talking about data
science
I think iterator is the best guys to
answer this but you know I can tell you
they definitely cover some of the data
science basics and they go in-depth into
business analytics they go in-depth into
our and all those things and they also
try to cover the Hadoop friend on few
things and you know and you also will be
talking about the big the maha's and all
that so it's it's quite an expansive
coverage there right so I'm looking at
the question window friends one by one
can we use Kafka as an input system
which is better which is better cough
curse / both are different there aren't
right Kafka it is to pry it's like you
know RabbitMQ kind of thing that it can
process messages right in an a
synchronous way where a spark is more a
batch processing but it doesn't in
memory spark streaming is real-time
processing for data science do we need
to have complete business knowledge of
underlying data bhushan I think data
scientist is go towards technology I
think people you know thanks for asking
this question because business analysts
they see there is a subtle difference
right data scientists they fix the
algorithms they optimized algorithms
right whereas business analysts they
need to know which algorithm to use for
a business domain or for a business
problem because these guys are also
masters in the business domain right
so there's a subtle difference but they
work hand in hand correct and data
scientist also you know they have a good
knowledge on some of the domains but it
may not be the case so the business
knowledge I would say more a business
analyst kind of thing data scientists
are more scientific in nature pusher
right more scientific they are good in
you know scripting they're good in
machine language they're good in Hadoop
they're good technology they know
they're good in may you know some of the
algorithms and all that so that is the
spark storm and Hadoop basic difference
so spark as I told you is it is more a
in-memory computing spark streaming is
very close to stop I think spark runs on
you know Hadoop again right runs on
Hadoop whereas Tom runs on me sauce I
could be right or wrong here
and how do passage said right it is
batch processing correct it is primarily
for high latency queries will recording
be available for other course takers
this recording should be available or
room c'mon I I you want to check with
you indirect or support but I I guess
this session is being recorded
how much execution time would it take a
value need to be updated okay updated in
all thousands of tables 1.5 terabyte of
Oracle db2 me to the point is you know
if you are talking about Oracle DB right
what I can do if you want to if you have
such a massive data I can scoop the data
I can use coop to bring it on to you
know something like a high data store
right and I can do an I can get the
values updated that is possible right
and one point five terabytes is not a
you know it's not a big data as such
right maybe for Oracle yes but for hive
it can take you know because any data
that is coming from Oracle right it's
structured data so I should quickly get
it onto hive right then I can probably
update it and send it back to Oracle so
that's one way of looking at it but if
you have heard of
use hive as the data store subsequently
then I can very well keep that in hive
itself when you say how much time it
should not take much time tell you it
should be in now if you properly you
know thousands of tables I don't think
it's gonna take much time I'm not
exactly you know because it depends on
various parameters at any row I cannot
say it he take milliseconds but should
take few seconds that's all right
why is both HBase and HDFS are require
is it either or optional it depends
Hodge it it is like HBase I am NOT
saying it's required I'm not saying HDFS
is also required in certain cases if you
want to use low latency queries go for
HBase if latency is not an issue go for
HDFS right HDFS is a pure file store you
have any other language instead of Java
apps absolutely punkish you have Python
you have big you have all these things
are there right hive and everything sure
hive is a data warehouse system data
warehouse tool it's one of the ecosystem
tool provided by Hadoop Mohammed where
is HBase used in your demo I didn't use
it to inject right I didn't use it at
all working is a support project tableau
administrator carry a change to big data
analytics with one year with cloud era
yeah it's a tough question right Kishore
it is C it all depends on everybody's
ability also so it is like yes I mean on
a positive note I can tell you can
definitely make a shift because I would
say Kishore because you are a tableau
guy right you should be good and
visualization I I feel so maybe you can
look at how you can marry even tableau
can connect to Hadoop maybe you can look
at that kind of portion right and how
you can bring the analytics overall you
can learn that analytics and how you can
bring the analytics all the way till
visualization that is one area it can
explode to show right just as a thought
Hafiz is asking is it similar to SQL I
absolutely do uses my SQL dialect Hafiz
Allah
yeah when we decided to move to Hadoop
Hadoop when PhD masters in statistics
required for data scientist that is one
way Temari right it is not a mandate
though there are folks who can also but
you know generally people come up with
such kind of degrees and all quite a lot
of people right but I would say in
somebody who has an aptitude for math
and algorithms and all that they are
fairly good candidates to get into her
can be groomed essent data scientists we
can use MapReduce for data then what is
the need of no SQL DB like MongoDB okay
that's something different right data
processing is that is processing the
data correct Mambo DB is a storage right
it's a data store both are different
storage is different processes
processing is different correct any
programs other than is there any other
programs other than word code why do you
ask that
Mallika there are millions of programs
so many programs are there for a person
better in programming what extent do you
think statistical knowledge is required
I'm dumb in math okay so friends I think
there is an interesting anecdote or
something which I saw very recently in a
data science thing right so a data
scientist jugdish you know you don't
need to be discouraged at all right my
point is a data scientist is generally
knows you know more statistics then a
software programmer or engineer would
know right and he will know more
software engineering practices than a
steady station ross statistician will
know correct so they know they have a
combination of these two skill sets
right they have a combination of these
two skill sets so they are generally
good in math they have a good analytic
aptitude and they are good in stats so
that you know they can understand
all those that complex functions
algorithms regression and all that stuff
right so but you know it is it is not a
skill which is impossible you know it
cannot be picked up or anything
sure so you can always invest on that
difference between MongoDB and Hadoop
yeah in fact MongoDB is a no sequel
again Hadoop is a file store right
nobody bean doesn't need to be on Hadoop
in the first place whereas HBase needs
to be on inaudible
MongoDB is a no sequel database correct
no Stickle database where it can store
data Hadoop can also store data but mom
would EB and all this no sequel
databases right it is primary it is
fundamentally different from Hadoop or
the Hadoop file system in the retrieval
process right the random retrieval
correct so those things Hadoop doesn't
support as I told you right it comes up
with high latency so any real-time data
you want to work on right these
databases makes it possible I need some
real-time projects is it possible for
real-time projects there are lots and
lots of projects Malika
so maybe you can check with the idea
raker support I'm not trying to you know
make any marketing pitch here but you
know there's lots of a you know I and
for you to take a quick outlook right
you can always go to you know Hadoop
powered by Hadoop and all that right
there are lots and lots of companies
they are doing real-time projects on
Hadoop most of them are using Hadoop
clusters store the data in file format
and process sure Abdul it's possible
what is hi I think I answered you Hafiz
Allah is it similar to seek well I think
I think I'm again this was answered what
language do they use for MapReduce Java
pig scanner real-time projects I think
anything can be used Java can be used
Pig can be used Scala can be used and
Python can be used can you look into
hive as a carrier in Big Data absolutely
how it's possible yeah in fact I I come
from C++ background and and you know I I
understand Java but I but there are
simpler ones so I always tried to use
the simple ones correct whenever you can
use that why you say HDFS why not I
think I've answered this Hadoop
compatible with all yes the answer is
your yes Python I think somebody told me
right I think one of you asked by duper
something which one has an advantage so
I would say our escort connectivity with
Hadoop I am sure Python also has I'm not
though I haven't used it I have used to
are with Hadoop how important is RAM
memory for running MapReduce should it
be proportional to data sets
I think RAM memory I think whatever we
have right a typical Hadoop community
server generally if it has like a you
know a GB you know this is what we are
talking about right NGB kind of thing we
are talking about so fairly good the
commodity server can do the job sure so
it depends you know how important ram
memory i am not we are not really
looking at a very sophisticated mission
but it is we are not looking at a very
cheap Oh hardware also you know you're
not looking at a through of a mission
you know which you can use it for Hadoop
cluster we have a reasonably good
terminal server which is also very cost
effective punkers eight years in Oracle
can I switch definitely Pankaj it's not
a you know any time you can switch
because people are moving into big data
people are moving into analysis
analytics wherever there is data you you
love analytics and wherever analytics
and people are gearing up to IOT and all
that stuff it's all cloud big data these
things are going to be you know the
future for sure
data brought to the memory at the time
of processing
agitates a you know a long story short
the data is not brought the program is
taken to the data right yet he can
project he can get project use cases
from major a curse upon junk they say
okay sure Jung this year I am not okay I
think I've answered this ha I can be you
can use complex transaction I know
a dumb question but I knew how long
would take Hadoop to implement the level
level from scratch it's not a dumb
question at all chase I think it takes
anywhere between just if you can take a
course write it need not be anyway I
mean you can just take it at your own
you know whichever of course you want to
take right I make an effort to do Hadoop
yeah that's all just make an effort to
read Hadoop and understand the basics
you know how things are implemented and
where are where all they are
implementing and you know what are all
the fundamental advantages all these
things you need to understand so it
takes you know for you to have a good
grip on Hadoop right anywhere between
three to six months you know just to get
a good grip because you need to at least
have a couple of implementations for you
to claim that you know you're good
correct average salary data scientist he
depends Jagannath I am I wish I knew
this but you know but they are it is
considered one of the sexiest job of the
21st century which is a you know
everybody says this but you know but
it's really true people are you know
really moving in this direction so I
think people are highly paid in this
area
sure so ok neccessity yes you didn't
answer the question yet color of s or no
no the necessity yes colored it is you
know if you are because we are talking
about data preparation we are talking
about data transformation we are looking
at the data cleansing all these things
right Hadoop is one of the platforms
that can provide seamless integration
for all these things so I I feel that
you know it's one of the one of the you
know favorite platform platforms for
data scientist as well sure
SPARC is going to replace what do you
think Chris is asking in fact spark
yeah as he told you right Krrish spark
is not going to replace as such spark is
going to complement it you know friends
as I told you right it is like you're
not I mean a lot of people ask our DBMS
is dying yes it is waning but it's not
going to die if I if I can use a DBMS
for you know for certain solutions right
which the data is never going to grow
but I'm going to have extensive querying
I would rather go stick to our DBMS
right I can user you know maybe sequel
Lite or MySQL wherever possible right
and for reals real-time queries I can
use no any streaming data I can use per
spark streaming or stop and for data
which which is like you know I I you
know a latency is not an issue which I
can get it done in minutes right I can
part it in Hadoop or one of the you know
no sequel databases lots and lots of
solutions people are looking at not
single solution right so they are not
looking at single data source they are
looking at a an assortment solution
because you may have five six business
scenarios within your organization for
each scenario you may have a solution so
they want to just put that together and
have it as a overall solution
Corrections so it's nothing like you
know one-size-fits-all model you'll
never find that kind of thing going
forward pink is a data warehousing
technology how will it be useful for
data mining
Tanny Roo the point is pig is more a
data transformation
it's a ETL tool data warehousing is more
high with data warehousing part pig is
more ETL tool to do the cleansing you
know especially when you have you can
use extensions you know the date formats
are coming in different ways and all
that you can do it you can use that tool
to clean the data right using regular
expression it provides lots and lots of
features sure it's very easy also very
simple to learn and all that so friends
I think I hope I was able to answer the
questions because I don't see any more
questions here hope you all and
the session and hope you found it useful
and I would request as usual just
provide your feedback provide your
feedback and you know we are here to
improve further right so so thanks
thanks for your your patient listening
so I look forward for your feedback once
you end the session so please take time
and you know provide the feedback
friends so for anything on the take you
know in the recording or anything on the
logistics part check with the ADA raker
support and they should be there to
support you and thanks friends thank you
so much I really enjoyed the session
with you I look forward to connect with
you some sometime in the future too so
thank you so much good luck</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>