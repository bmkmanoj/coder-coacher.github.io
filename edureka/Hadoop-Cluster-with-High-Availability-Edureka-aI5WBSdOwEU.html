<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Cluster with High Availability | Edureka | Coder Coacher - Coaching Coders</title><meta content="Hadoop Cluster with High Availability | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Cluster with High Availability | Edureka</b></h2><h5 class="post__date">2015-06-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aI5WBSdOwEU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so our very good evening to all of you a
very good evening everybody and welcome
to the Hadoop cluster with high
availability session I am Tushar Conti I
have a work experience of over seven
years in IT industry and I have been
working on Hadoop for over two years now
so let's start with our session the
objectives of this session will be
introduction to the Hadoop clusters and
at different running modes for these
Hadoop clusters we will look into the
configuration files and we will try to
talk about the unstated responsibilities
for Hadoop atoms later we will look into
the high availability features that are
available in Hadoop 2.0 and we will have
a demo after that okay below the slide
is the our mentioned our twitter handle
our Facebook page so you can ask us
questions over there as well we will try
to reply them great so guys let's start
with the session okay what are the core
components in Hadoop 2.0 okay basically
there are two distinct bifurcations
there is storage and there is processing
so one is the memory or the disk and
another is the CPU and memory the
physical memory there as I was
mentioning there are two parts of the
Hadoop hood components one is primarily
the storage that is dated to the HDFS
the Hadoop distributed file system and
the second part is the yarn that is the
resource manager in Hadoop 1.2 you have
the separate kind of a yarn that is a
Map Reduce and it is totally separate
build up for processing in Hadoop 2.0
known as yong-ok
there are other two bifurcations there
is a master and a slave configuration in
Hadoop 2.0 the master part is governed
by the resource manager and the name
node and the slave part is done by the
node manager and the data node there is
also a secondary main node mention
present in the Hadoop cluster
which acts as the passive part of the
name note okay so what is the kind of
growth you can expect and looking into
this growth what kind of capacity you
should build up in your Hadoop clusters
okay guys if you have a data growth of
approximately a couple of terabytes per
week and the Hadoop configuration is set
up to replicate the block three times
then you will have around fifteen
terabyte of extra storage per week and
assuming thirty percent of the overhead
to store the metadata part you can be
assuming to purchase five into three
that is around 15 terabytes of hard disk
space equating to our new machine almost
every week so this is the kind of
capacity planning you should be looking
into okay so what are the kind of
recommendations for a typical slave node
okay so there is two kinds of
configuration one is a high performance
configuration and one is a low
performance configuration depending upon
what type of work that you want your
slave node to work on or to chew on you
need to provide that kind of a hardware
to that node there there are two
configurations mentioned one is the
general configuration and one is a
special configuration in the general
configuration it depends upon the
requirement of your slave node you can
have a couple of terabytes of hard disks
that is and you do need to relate them
okay so you don't need any redundant
same then you need quad-core CPUs and
poopa 32 gig ram and a gigabyte Ethernet
card in the special configuration you
have one hard disk to coerce
six to eight GB RAM generally worked
well for many types of applications so
guys the totally depends on your choice
a cluster with mode modes performs
better than a few one and the slightly
faster nodes so it is better to have
mode modes than a smaller quantity of
nodes with a better performance since
the major intention is to do processing
the speed of processor
does matter but it doesn't matter in the
long run or in the processing of very
big files okay so let's look into the
memory concern of the slave nodes see
generally each MapReduce task will take
around 1 to 2 GB of RAM and slave nodes
should not be working on virtual
majority memory your page file should
not be spilled with the RAM or the
physical memory pages so that is if that
happens it will kill the speed of your
processing so just ensure that you have
sufficient ran to present on all the
tasks plus the data node the tasks are
contaminants and the OS so as a form of
rule the number of tasks equal to 1.5
into the number of core processes so
guys the main concern here is you should
be having enough amount of RAM in the
slave nodes ok so let's look into the
master node hard worker
the master node hardware needs to be a
courier class hard that it should not be
a commodity hardware it should have a
very good configuration that at least 32
gigs of ram raided hardest dual ethernet
cards for a failover and a dual power
supply for a failover as well so the
master node does all the processing it
it presents it has all the map of where
the data is stored it presents all the
job trackers or the resource managers
resource managers and then to the
different node managers so these are it
has to do a lot of processing so it has
all the metadata stored for your whole
hadoop so it has to have a very good
high-end configuration okay so what are
the different kinds of modes of clusters
that you are working with okay so there
are three distinct cluster modes or
running molds of Hadoop that are
available the first mode is a local mode
or a standalone mode here no demons are
involved everything runs as a Java
processing machine so the standalone
daemon as I was mentioning it does not
have any demons or it does not have any
Java processes running so this is
suitable for a small MapReduce programs
for during the development and has no
distributed file system configuration as
such okay so the next is a pseudo
distributed mode here a single hardware
is running all the Java demons so
basically this is a simulation of the
Hadoop file system and all the virtual
all the demons are running on the same
system okay pratik a DFS is distributed
file system does that answer the
question okay great so and the finally
there was a fully distributed mode that
is we provide a setup essentially we
have to set up a cluster configuration
of all the nodes should be there
typically presenting which should be the
master which should be the slave then
there will be a lot of other
configurations you need to do to set up
a full cluster up and running okay so
let's look into what are the
configuration files that are provided
the first two files are the Hadoop and
yarn environment files settings for the
Hadoop daemon process environment the
second is the cool side dot XML
configurations for Hadoop code such as
the IU and that of settings common to
the HDFS and yon the third is the HDFS
psychotic symbol configuration settings
for the HDFS daemons the name nodes and
the data nodes the third one is the yang
side dot XML configuration settings for
the resource manager and the node
manager the second last is the MapReduce
map inside that XML configuration for
the MapReduce applications and the last
one is the slaves file which has a list
of the machines that are running at the
slaves in which essentially the data
node and the node manager will be
running okay so this is iteration of the
same thing
this is typically looking of what a
Hadoop cluster looks like there are a
lot of data nodes there is a activity in
ohms there is a secondary end load and
there is a standby neighborhood okay so
in Hadoop one the name node was a single
point of failure
what happens in Hadoop one is 1.0 is the
lane load essentially a dance all the
metadata and managers of the FS edits
file okay
Sri we are discussing Hadoop 2.0 here
but this slide that I'm presently
showcasing is of Hadoop 1.0 that's the
terms of the question three
okay I'll guys she is asking a question
what words in of Hadoop are we
discussing here and to answer that
question we are working here on Hadoop
2.0 the demo or this webinar basically
is for Hadoop to point to how would we
are discussing the shortcomings of a 1.0
in this slide so that we can answer why
we are moving to Hadoop 2.0 okay great
so as I was mentioning name node was a
single point of failure what happens is
name nodes has two files the FS image
and the edits file now oddly it is
configured that the name node will
combine these two files and then send
back to the name node so housekeeping
and okay so these are the working for
the name node it's not our hot standby
it is a standby it is a second Union
node but does not have any shame over
feature you cannot fail over to the
secondary name world so these are the
couple of puffs the second Union what
does okay so in Hadoop 2.0 we have an
active failover to the standby main node
we have automatic failover setup and
then for the landlord okay so in Hadoop
1.0 this had to be done manually and in
Hadoop 2.0 we have automatic failover H
a we can configure for the name node
okay so what are the new features in the
high-availability the Hadoop distributed
file system H a features addresses the
Hadoop one point X problems providing
the option of running to name nodes in
the same cluster in active passive
configuration these are referred to as
the acting a node and the standby name
the stand by name node is a hot pack up
for the cluster which was was shot
coming in to 1.0 it allows a fast
failover to the moon in node and in the
case that machine crashes or a graceful
administrator
initiated a failover for the purpose of
a planned maintenance okay so we can set
up this high availability feature using
two methods one is the quorum based
storage and another is the shared
storage using NF
okay so what are the demons involved for
this configuration of high availability
of name node and resource manager
essentially they'll be having a journal
nodes this demon will beacon containing
shared edit files okay
these shared edit files they'll be used
for the active main node to write and
for the secondary or the standby name
node to sync to read it and sync its
metadata with the acting they know there
is a zookeeper service which will look
into the trail over controllers and
there is a zookeeper failover controller
which will maintain or manage the high
availability failover feature okay so
these are the demons I was talking about
guys so there is a standby named node or
a resource resource manager node there
is a zookeeper failover controller
there's zookeeper and there is a general
node and we have data nodes obviously
which will be running the node manager
the zookeeper and the journal node okay
Shanta Lata has question when the active
name node fails will the standby name
would start automatically
yes Shanta that is the configuration we
are looking at it today if the active
name node fails the standby name node
will start automatically okay
so it's time for the demo the guys I
have set up three node cluster and I
have taken the opportunity of popping
the hadoo binaries and the zookeeper
binaries here so we have a lot of
configurations to do so let's start
first of all we'll check the okay so we
have Hadoop two point six point zero
stable version that I have taken and for
zookeeper I have taken the version three
point four point six so the first task
here guys will be to extract the Hadoop
binaries all the three nodes
okay okay shoe budget you can have a
recording of this session logging to the
LMS
thank you yeah looking through that this
both are still on zippy okay then don't
sit here next concept zookeeper as well
excuse me guys
okay so we have on scepter people and
Hadoop file system here
let's
yes we need do you want to say something
of zookeeper so although this system
they are configured to do now guys are
let's first set up the profiles
okay
okay so these are the configurations
related to the Hadoop they have set
Hadoop form and the path to the Hadoop
form they have setup the zookeeper and
the path to the zookeeper have been
mentioned a couple of other environment
variables that will be required for our
processing okay that also mean a softly
- okay so this is the soft link pair
made and which we have used in our
profile ID if you see they are mentioned
here home Hadoop ado so this is the
profile ID we are using it is in the
present working latex home Hadoop so if
I present if I mark as a loop then this
will be pointing to the present Hadoop
working directory which will be using
let me just quickly set the profile IDs
for the other two environments as well
hope to Kanto and sit with us and took
okay so in the third note as well that's
the variety and we'll sit here
okay so let's also make the soft link
over here but uh or - okay that's done
here so we are ready to start with the
configuration files okay so the
configuration files are mentioned in to
a PC or the folder okay these are a
couple of these are the configuration
files that we have let's start
configuring them one by one
okay so that's first the pin tutor
so guys let's first start looking into
the poor side protection okay yes okay
so the first property is the FS dot
default FS property here we will be
telling which is our a URI for the
default file system that which we'll be
using for any client to connect the
second will be the journal node edits
directory here mostly the journal nodes
which we are talking about will keep
their data here okay these checks of of
those mentioned a question to look into
the okay okay thanks for those thanks
for mentioning somehow I skip this
looks good enough thanks for those okay
so we include the and to get to see and
to watch site protection which side okay
so we mentioned that these are the
general notes the general note at its
file will contain the path the the
journal nodes will be keeping their
metadata okay so now let's link to the
HDFS side projects in it okay the HDFS
side rhotic symbol will contain a lot of
configurations related to the Hadoop
distributed file system
yes so yes guys are
yeah so the first one is the DFS name
node name directory here the it metadata
will be stored for the name node the
second configuration parameter we have
used is the DFS replication factor which
is set to 1 since we have just one data
node mentioned here excuse me guys
we have DFS permission as false we have
not set up any permissions for the
distributed file system we have the name
service which is the name service ID
here we have used H a cluster then we
have to define the two separate and
named nodes which will be used in this H
a cluster here I have used name node and
s name node that is the second name node
then there are configurations for the
name node RPC full ethic you do an
address this is for the first active
name node this name node dot Hadoop
dot-com and the port number we have used
is 9000 the second is the second the
next configuration is for the remote
procedure call for this H a cluster that
we are setting up the secondary Union
node it's the full qualified name of the
secondary name node and the port number
we have used as 9000 the next two
parameters are the HTTP address for the
name node and the second minion node and
the port number they abused is five
double zero seven zero okay the next
configuration is important it contains
the shared edit directory so this you
general files will be having will be
running on the name node the second in a
node and the data name node on the port
eight four eight five okay
the third of the next configuration file
or the confliction parameter is for the
client failover
proxy provider so guys this is required
for a climb to connect and to the active
name node in case of a failover happens
so this is the property used for that
okay and we want to have automatic
failover enabled so be
switch this VFS automatic failover
enable to - next we have zookeeper
quorum now sue people forum will be
running on port two one eight one on all
the three nodes okay now next parameter
is the fencing method now what is
fencing here we have used SSH fence for
this methodology fencing is a process
where the second rename node in case of
a failover the second a name node who
logs into the associative into the
primary name node and then drives if it
identifies there is the daemon is still
running for the name node then it will
kill it forcefully so that it cannot
affect the reads of any client for the
metadata okay the fencing method the
private key used here is the SSH Idra I
have already setup the SSH the password
let's search for it and they I mentioned
the same ID here okay for the next
configuration we will try to configure
the zookeeper here so let's so okay so
here is a sample of the configuration
file we need to copy this configuration
or CFG file okay okay so here will
mention the part of the data directly
here data directory which we will be
using will be a 2 or 2 let's say data
zoo keeper and Google mention all the
server ports which will be running this
zookeeper service so sell gold one will
be in a mode top plate and the triplet
similarly serve but of two and what are
the tribulation to submit the server
data node one.com and
okay so we have done the configuration
part let's go to our home directory
check of the step okay so now we need to
create the directory files which we have
mentioned in our configuration so we
will make three let's say slash data
okay go to C be okay so we have
considered our they have done that
mostly our configurations set the ID
faith okay so let's move into the
zookeeper folder and here you have to
create my ID five days okay this is the
full pot see I have this is the path
which we have mentioned in the
configuration file of the zoo people
which I've shown previously if you go
back here you will see that we have
mentioned the part of the people okay
and we have to create our my ID file
here we n my ID and have to just mention
one here since this is the server they
are mentioned one and now I'll copy all
the configuration files to the separate
systems all the three systems let's
first go to the two configuration files
okay and SCP or cite that external
who s name node and how to let me see
HDFS like traffic signal here okay and
copy the same to pop either yeah
WebSockets moves so the Hadoop tilde
trend L something copied okay our power
are you not able to hear me now
guys are you able to hear me is the
audio running fine
Pawan here has mentioned that I am NOT
getting in audio okay Pawan can you just
disconnect and connect your mic sorry
speaker if you're using any external
speaker
okay good in case okay so we have to
again copy the let's move to the
zookeeper in that case let's go home
let's go to CBS we first got on earth
and they appear Sepideh
put five to again ask me more and keep
it on earth okay so the system for the
name moon let's try for the data node
one okay so looking through these things
are done now think how people just one
more thing
you have to create the same folders here
so it data different to create CD that's
a general note
okay then what does she prefer and wait
on my 85 here this is the data node so
it'll still be okay slash data will go
to the shape tick-tock it should check
okay
very popular to keep a folder and create
my and applied here and the sister
second tinium okay guys almost done
we're almost there so
yeah they have done with all the
configurations I see so now let's start
the journal notes on all the three
servers so we will be using Hadoop
demonym dot at such file the starting to
share on your notes on all the three
that's better
nope hi power our power has a question
if you can rejoin the session you're
still not getting the audio
I think power can you just log off and
log into your system and check once
again if you are able to get the audio
then okay so check all the Java
processors
yes I know nothing but I'm here say some
issue okay this little strange that's
looking to the configuration files
Jib college okay all that configuration
sky make shut this was good to see a
pull back
okay good
that's looking for that it's vehicles
cycloptic son okay down low step issue
then click off with the configuration
tab voice strange
okay
but stop cooking after joining us once
again
yes mama
that was rich sorry guys I had a block
of configuration additional and
configuration file I think that I was
the reason channel or the memo is not
coming up okay Rick so the journal loads
are up on all the theory
okay so all the three have Journal notes
started okay the next task is to format
the name node in the active name node so
we will add the name node
you just push is check the suit
so neutral slope
so this is the name mode formatting I
think to be the case somebody wanted me
to check the zookeeper
okay so we see it formatting and
still taking a lot of time so see this
is a virtual machine so in that case
that is why you are saying this kind of
latency when you are working in a
physical server built up with the high
CPU on high memory these things will run
pretty fast over there
okay yeah this is from my thing let me
discuss is extruder naming mode and
change
okay so the main mode formatting is done
and we are done with an error and able
to construct a general note on a note
twice Y location target
maybe not okay so we have hit with an
issue so this is what this
administration course is all about so
you know what Ruby bug and how to debug
so this is through channel notes entry
which we are trying to do
okay so luckily we have our archive here
in which we have its deepest semen
already present so try to walk theatres
its deepest a production man who you see
- got it let's copy this is devastated
XML with data not once
let's try to form okay
hi guys sweareth has a question did we
start the general notes on all the three
notes EF swara general notes are
supposed to be running on all three
notes and ambush has mentioned that it
is mentioned as one how to put in the
configuration files let me check
this looks good for me here
okay so this is formatting and pink I
have understood the reason there might
be a couple of things you have to check
again I think we messed up configuration
file okay so you see I took the image
the slaves and dictation file data okay
okay does again followed an issue not a
problem we need to debug this so we need
to check into the permissions of this
file system sweetie
Qi data you see to have sense on five
which looks like same configuration yes
okay so let's looking to talk about
change lying okay
seems like the high availability is
trying to play with us because the
statements as
it's good fun not like just that hip
giving or just name
oh it is crossing Messiahship this time
so the Hadoop administrator is
responsible for implementation and
administration of the Hadoop
infrastructure so like the presently
issue that we are running into we need
to look into all the logs that are
present and what are the outcomes or
what are the issues that is causing the
name rule to not format see this is one
of the things you need to work as a
administrator so this is a typical work
that administrative they'll have to do
okay then we have to test separate
things like testing the distributed file
system the high fake MapReduce access
for all the applications okay then we
have cluster maintenance tasks like
backup recovery upgrade and patching
then we have our performance tuning and
capacity planning for the clusters that
I mentioned additionally you have other
tasks like monitoring the Hadoop cluster
and deploying the security features
through Kerberos authentication okay so
how does it work what does I do recover
wait until the car provides life online
webinar classes and you get this class
recordings directly in your LMS and we
have about 24 cross 7 post class support
as well where you mention your query and
then you will get your answers as soon
as our technical person adheres to it
you have module wise questions they have
project work and in the end you have a
variable certificate from adhirata that
you have worked on do ok so we are still
working on this technical issue give me
a minute here I'll try to look into what
is the main concern for this
I'm trying to work on these virtual
machines Allah try to reboot them and
bring them back again the little more
RAM so in the meanwhile you can have a
ten minutes break we will join after 10
minutes yeah hi sir we are working on
the same issue the name load it's a
coaster remains unresolved as I have
just you started the virtual machine
while you guys are away and I think it
is same give you the same issue just
look into this issue again I just
started out the virtual machine but
probably there is some issue in the
configuration file which is check once
take your step my name's undissolved but
I did
why should it we do not have any other
games as of now so guys what I will do
is hopefully I can send you all the
instructions how to set up this bird
manager and we will send you a demo
again when we configured this virtual
machine and try to run it again probably
some issues with the file system I guess
I have just fallen occurred everything
in fresh well done probably this is an
existing virtual machine that I was
using so probably will build up a new
virtual machine and set it up and share
the video with you guys
the second guys with the firewalls are
off for the system
yeah
yeah
you
you
hi guys we have tried
the system's again but it seems that we
have run into like a BM issue the
virtual machines are not performing as
we are expected but we have already a
different slide that we have prepared
from a previous session we will be
sharing that with you and we'll be
sharing a step file how to prepare that
virtual machines and I availability on
your own I hope you enjoyed the session
however we ran into a small technical
issue and that is why we could not
complete the configuration but mostly
they will look into the other develop
info all the configuration parameters
mostly this is a virtual machine issue
the virtual machines that are built on
on a small desktop so they will not be
having that power and there was some
configuration for and in the morning to
set it up everything was running fine
but we will not they have not been able
to set it up through this session
extremely my hearty apologies for that
and hopefully but that's the daily work
for us every day we need to look into
some issues or the other and the problem
will try to fix this and hopefully in
the next session then whenever you join
we look into the full configuration
achieve for name note and the resource
manager okay thank you guys hope you
enjoyed the session I have a very good
night and hopefully we'll meet soon bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>