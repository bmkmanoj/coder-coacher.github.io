<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Tutorial for Beginners | Big Data Hadoop Training | Hadoop Certification | Edureka | Coder Coacher - Coaching Coders</title><meta content="Hadoop Tutorial for Beginners | Big Data Hadoop Training | Hadoop Certification | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Tutorial for Beginners | Big Data Hadoop Training | Hadoop Certification | Edureka</b></h2><h5 class="post__date">2017-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6H2e03tapfk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is Reema from meadow
Rika and today we are all learning
Hadoop and sparked even if you all are
new to this technologies and since this
is the first class but I will assure you
that at the end of this session you'll
be very confident about Hadoop and spark
and start feeling like an analyst
already so this session is going to be a
little different because we thought that
the best way of learning something is by
implementing it so we will learn Hadoop
and spark by implementing real-life use
cases directly and I'm sure that it will
all have a great time going through that
so these are the two use cases that
we'll be dealing with today the first is
the u.s. primary election analysis and
the second is the instant gaps use case
so we will begin with the u.s. primary
election analysis first and we'll be
talking about the primary election in
2016 so you know that in the primary
elections from Democrat the two
contenders where Hillary Clinton and
Bernie Sanders out of which Hillary
Clinton won the primary elections from
Democrats and Republicans there is
Donald Trump Ted Cruz and a few other
contenders out of which Donald Trump won
the primary elections so now let us
assume that you are an analyst already
and you have been hired by Donald Trump
and he tells you that I want to know
that what where the different reasons
because of which Hillary Clinton won and
I want to carry out my coming campaigns
based on that so that I can win the
favor of the people that voted for her
so that I can take away her votes and
finally become the present so that was
the entire agenda so this is the task
that has been given to you as a data
analyst so what is the first thing that
you will do the first thing you'll do is
that you will ask for data and you have
got two data sets with you the first is
the US primary election data set and the
second is the US demography features
County wise data set so let us take a
look at what this data sets contain so
this is our first data set which is the
u.s. primary election data set so these
are the different fields present in our
data set so the first field is state so
we've got the list of the state Alabama
the state abbreviation so for Alabama
its
we've got the different counties in
Alabama like Otto Gahr Baldwin barber
bailout bailout buckler etc and then we
have got sips now fits our federal
information processing standards Code
so this basically means zip code then
we've got the party so we will be
analyzing the Democrats only because we
want to know what was the reason for
Hillary Clinton's win so we'll be
analyzing the Democrats only and then
we've got the candidate and since I told
you that there were two candidates
Bernie Sanders and Hillary Clinton so
we've got the name of the candidate here
and the number of votes that each
candidate got so Bernie Sanders got 544
in Ottawa County and Hillary Clinton got
2387 and this field over here represents
the fraction votes so if you add this
two together you'll get a 1 so this
basically represents the percentage of
vote each of the candidates got so let
us take a look at our second data set
now so this is the US County demographic
features data set so first field we have
again the fifth the area name the
article County Baldwin and different
other counties in Alabama and other
states also the state abbreviation so
here it is only showing Alabama and the
fields that you see here are actually
the different features you won't know
what this exactly continued because it's
written in a coded form but let me give
you the example what this data set
contains and let me tell you that I'm
just showing you a few rows of the data
so this is in all the entire data set so
this contains different fields like
population in 2014 in 2010 the sex ratio
how many females males then based on
some ethnicity how many Asians
how many Hispanic how many black
American people having a black African
people then there is also based on age
groups how many infants how many senior
citizens how many adults so there are a
lot of fields in our data set and this
will help us to analyze and actually
find out what led to the winning of
Hillary Clinton so now you have seen
your data set you have to understand
your data that you have to figure out
what were the different features or what
are the different columns that you're
going to use and then you have to think
of a strategy or think of how you're
going to carry out this analysis so this
is the entire solution strategy so the
first thing you'll do is that you need a
data set and you have got two data sets
with you the second thing you need to do
is that you need to store that data into
HDFS now HDFS is Hadoop distributed file
system so you need to store that data
the next step is to process that data
using SPARC components and we'll be
using SPARC sequel SPARC ml Lib etcetera
the tanks task is to transform the data
using SPARC sequel transforming humans
filtering out the data and the rows and
columns that you might not need in order
to implement or in order to process this
the next step is clustering this data
using SPARC ml lip and for clustering
our data will be using k-means and the
final step is to visualize the result
using Zeppelin now visualizing this data
is also very important because without
the visualization you won't be able to
identify what for the major reason then
you won't be able to gain proper
insights from your data now don't be
scared if you're not familiar with terms
like sparse equal SPARC MLM k-means
clustering you'll be learning all of
this in today's session and so this is
the entire strategy this is what we're
going to do today this is how we're
going to implement this use case and
find out why Hillary Clinton won so now
let me just give you a visualization of
the results so I'll just show you the
analysis that I have performed and I'll
just show you how it looks like so this
is Zeppelin which is in my master node
in Maha dope cluster and this is where
we're going to visualize our data so
there is a lot of code don't be scared
this is just Scala code with sparks
equal and at the end you'll be learning
how to write this code so I'm just
jumping on to the visualization part so
this is the first visualization that we
have got and we have analyzed it
according to different ethnicities of
people for example in our x-axis we have
got foreign-born persons and in y-axis
we're seeing that among the foreign-born
people what is the popularity of Hillary
Clinton among the Asians and the circles
represent the highest values the bigger
the circle is the bigger the cow
so we have made a few more
visualizations so now we have got a line
graph that compares the votes of Hillary
Clinton and Bernie Sanders together
again we have got an area graph also
that compares Bernie Sanders and Hillary
Clinton votes and hence we have got a
lot more visualizations we've got bar
charts and everything and finally we
also have a state and County wise
distribution of votes so these are the
visualizations that will help you to
derive a conclusion to derive an answer
whatever answer that Donald Trump wants
from you and don't worry you'll be
learning how to do that I'll explain
each and every detail of how I have made
these visualizations so now let us take
a look at our second use case which is
the instant gap fuse case so now there
is a cap startup in US which is called
instant gaps and again since you did a
very great job in analyzing the u.s.
election so they have hired you again to
solve their problem now so basically
this company wants to know what is the
demand of caps at which pinpoint
location during which peak hours and
what they want to do is they want to
maximize their profit by finding out the
Beehive points where they can get a lot
of pickups and getting caps there during
the peak hours so this is your second
task so again the first thing is that
you need a data set so this is the uber
data set that has been given to you in
order to analyze and find out what where
the peak hours and how much caps are
expected in those locations during the
peak hours so this is just a date/time
stamps so this represents a pickup time
and pickup date for a particular uber
right so this is a January 2014 and and
it's around midnight and then you have
got latitude and longitude so this
represents the location of the pickup
and then this is space which is a TLC
based code so this is like the license
number of the driver so now again we
have to make a strategy of how you are
going to analyze this data so at first
you have got your data set in CSV format
so this is your first step you have got
the data then the next step again is to
store the data in HDFS like we did the
first time then again we have to
transform the data because this data set
actually is really really large contains
a lot of rows and columns and maybe you
don't want to analyze all of them at
once so you will again filter out some
of the rows and after it's transformed
you'll start clustering again with
k-means again I'm going to tell you that
don't worry I'm going to explain you
k-means from the start and how to do it
and by cluster we will find out the
center points of each cluster which will
represent each of the pickup point or
each of the beehive point so that's why
we're performing clusters in order to
find out the cluster centers which will
represent a beehive point where we'll be
expecting maximum number of pickups
during peak hours so this is your entire
strategy now let me just show you the
visualization of this one also like I
did it for the US elections so here is
our code again which is a scholar and
sparks equal code so let me just jump
directly to the visualization part so
this is again we have in our x-axis we
have the count or the number of pickups
and the y-axis we have ours that is the
time of the day and then we have grouped
it according to the size of the count
that is number of pickups so you can see
the largest size that you can see here
are these ones and this is found in the
fourth cluster and it's found at the
17th hour which is around 5:00 p.m. so
this is what we found out after we have
analyzed and visualized our data set our
uber data set so again I'll be talking
about how many clusters are to be made
and how to make different clusters and
how to find everything out so this is
again the visualization of the uber data
set and then this one represents the
location in order to identify the
Beehive points
again we have classified the number of
pickups according to the different hours
of the day so we have got 24 slices over
here so you can see the biggest slice
you can see is in the 17th hour and the
16 dots which is around 4:00 or 5:00
p.m. so this is the visualization of the
data set so let us go back to your
presentation now we have seen that what
we have to do but now let us understand
what it takes in order to perform all
this what are the things that you need
to be aware of or you need to learn so
here is what you need to learn in order
to perform the analysis on the two use
cases so we'll start with the
introduction to Hadoop and spark so
we'll understand what is Hadoop and what
is spark and then we'll take a deep dive
into Hadoop to understand the different
components of Hadoop for example the
storage unit of Hadoop which is known as
HDFS and then yarn which is the
processing unit of Hadoop and then we'll
be using different tools like Apache
spark which could be easily integrated
with Hadoop in order to perform a better
analysis and then we'll understand
k-means and Zeppelin
because we have used k-means clustering
in order to cluster our data and we have
used a plant in order to visualize it
and then we'll finally move on to the
solution of use cases where we'll be
implementing it directly so this is what
you need to learn so let us get started
with Hadoop and spark so we'll start
with introduction to Hadoop and spark so
now let us take a look at what is Hadoop
and what is part so Hadoop is a
framework where you can store large
clusters of data in a distributed manner
and then process them parallely then
Hadoop has got two components for
storage it has HDFS which stands for
Hadoop distributed file system so it
allows to dump any kind of data across
the Hadoop cluster and it will be stored
in distributed manner in commodity
hardware and for processing you have got
yarn which stands for yet another
resource negotiator and this is the
processing unit of Hadoop which allows
parallel processing of the distributed
data across your Hadoop cluster in HDFS
then we have got SPARC so Apache spark
is one of the most popular projects by
Apache and
this is an open-source cluster computing
framework for real-time processing we're
on the other hand Hadoop is used for
batch processing SPARC is used for
real-time processing because with SPARC
the processing happens in memory and it
provides you with an interface for
programming entire clusters with
implicit data parallelism and fault
tolerance so what is data parallelism
data parallelism is a form of
paralyzation across multiple processors
in parallel computing environments a lot
of parallel words in that sentence so
let me tell you simply that it basically
means distributing your data across
different nodes which operate on the
data parallel and it works on
fault-tolerant systems like HDFS and s3
and it is built on top of yarn because
with yarn you can combine different
tools like Apache spark for better
processing of your data and if you see
the topology of Hadoop and spark both of
them have the same topology which is a
master/slave topology so in Hadoop if
you're considering in terms of HDFS the
master node is known as name node and
the worker node or the slave nodes are
known as data node and inspires the
master is known as master and slave
they're known as worker these are
basically daemons so this is a very
brief introduction to Hadoop and spark
so now let us take a look at spark
complementing hadoo so there is always
has been a debate about what to choose
Hadoop or spark but let me tell you that
there is a stubborn misconception that
Apache spark is an alternative to Hadoop
and that it is likely to bring an end to
the error for you it is very difficult
to say Hadoop versus bar because these
two frameworks are not mutually
exclusive but they are better when they
are paired with each other so let us see
the different challenges that were
addressed when we are using spark and
Hadoop together now you can see the
first point that SPARC processes data
100 times faster than MapReduce so it
gives us the results faster and it
performs faster analytics the next point
is that spark applications can run on
yarn leveraging Hadoop cluster and you
know that Hadoop cluster is usually set
up on commodity hardware so we are
getting better processing but we are
using very low cost hardware this will
help us to cut our cost of
not so hence you also achieve cost
optimization third point is that apache
spark can use HDFS storage so you don't
need a different storage space for
Apache spark it can operate on HDFS
itself so you don't have to copy the
same file again if you want to process
it with spark so hence you avoid
duplication of files so how do forms a
very strong foundation for any of the
future Big Data initiatives and Apache
spark is one of those big data
initiative it has got enhanced features
like in-memory processing machine
learning capabilities and you can use it
with Hadoop and Hadoop uses commodity
hardware which can give you the better
processing with minimum cost so these
are the benefits that you get when you
combine SPARC and Hadoop together in
order to analyze big data so now let us
see some of the big data use cases so
the first big data use case is web and
eat a li so the recommendation engines
whenever you go out in Amazon or any
other online shopping site in order to
buy something you'll see some
recommended items popping below your
screen or at the side of your screen so
that is all generated using big data
analytics ad targeting you go to
Facebook you see ads of different items
asking you to buy them then you've got
search quality abuse and quick fart
fraud detection you can use big data
analytics and telecommunications also in
order to find out the customer turn
prevention the network performance
optimization analyzing network to
predict failure and you can prevent loss
before the error or before the fault
actually occurs it is also widely used
by government for fraud detection and
cyber security in order to introduce
different welfare schemes justice it has
been widely used by healthcare and life
sciences for health information exchange
gene sequencing serialization healthcare
service quality improvements drug safety
let me tell you that with big data
analytics it has been very easy in order
to diagnose a particular disease and
find out the cure also so these are some
more big data use cases it is also used
in banks and financial services for
modeling true risk fraud detection
credit card scoring and analysis
and many more it can be used in retail
transportation services hotels and food
delivery services actually every field
you name no matter whatever business you
have if you are able to use big data
efficiently your company will grow and
you will be gaining different insights
by using big data analytics and hence
improve your business even more nowadays
everyone is using big data you have seen
the different fields everything is
different from each other but everyone
is using big data analytics and big data
analysis can be done with tools like
hadoo spark etc so that is what big data
analytics is very much in demand today
and that is why it is very important for
you to learn how to perform big data
analytics with tools like this so now
let us take a look at a big data Eustis
solution architecture as a whole so
you're dealing with big data now so the
first thing that you need to do is you
need to dump all those data into HDFS
and store it in a distributed way the
next thing is to process that data so
that you can gain insights and will be
using yarn because yarn can allow us to
integrate different tools together which
will help us to process the big data so
these are the tools that you can
integrate with yarn so you can choose
either Apache hive Apache spark
MapReduce Apache Kafka in order to
analyze big data and Apache spark is one
of the most popular and most widely used
tool with yarn in order to process big
data so this is an entire solution as a
whole so now since we have got
introduced to Hadoop and spark let us
take a deep look at HDFS which is the
storage unit alpha 2 so HDFS stands for
Hadoop distributed file system and this
is the storage unit for Hadoop and here
is the architecture of HDFS so an HDFS
you have got and since I already told
you that it is a master/slave
architecture so the master node is known
as name node and slave nodes are known
as data nodes and then we have got
another node here which is known as
secondary name node
now don't get confuse that secondary
gnome node is just going to be a
replacement for name node because it is
not so I'll tell you what a secondary
name node does so now let's go back and
understand the entire architecture
property
you can see the little icons over all of
these different nodes so basically a
name node is the master daemon so you
can think of it as the king it has got a
helper daemon which is a secondary name
node which has the icon of the minister
and then the pawns represent the slave
nodes or the slave demons data node over
here
it contains the actual data so whenever
you dump the file into HDFS and it gets
distributed your data is stored in the
data nodes and the best thing about HDFS
is that it creates an abstraction layer
over the distributed storage resources
so when you're dumping the file in HDFS
it gets distributed in different
machines but you can see the entire is
DF as a single unit because it is laid
out in such a structure so now let us
view each of this components one by one
so first we'll take a look at the name
node the name node is the master daemon
and it maintains and manages the data
node so what a name node does is that it
preserves a metadata a metadata meaning
data about data so whatever file or
whatever data is stored in the data
nodes the name node maintains a proper
sheet a proper file where everything is
mentioned that what data is stored in
which data node and it serves any kind
of request from the clients also and
since it acts as master node it also
receives heartbeats the little hearts
you saw popping in the earlier slide the
data nodes are actually sending
heartbeats the name node which is
nothing but signals just to tell the
name note that the data node is alive
and functioning properly
now comes the secondary name node the
secondary name node does a very
important task and that task is known as
checkpointing so checkpointing is a
process of combining edit Logs with FS
image so now let me tell you what is an
edit log and what is an SS image so
let's say that I have set up my Hadoop
cluster 20 days back and whatever
transactions that happen whatever new
data blocks are stored into my HDFS
whatever data blocks are deleted every
transaction is contained in a file known
as FS image and FS image resides in your
disk
and there is one more similar file which
is known as edit logs now edit laws
won't keep the record of transactions 20
days back but just a few hours back so
let's say we'll keep the record and the
transaction videos that happened in the
past four hours and checkpointing is a
task of combining the edit log with the
FS image and it allows faster failover
as we have a backup of the metadata so a
situation where a name node goes down
and the entire metadata is lost we don't
have to worry we can set up a new name
node and get the same transactional
files and the metadata from the
secondary name node because it has been
keeping an updated copy and check
pointing happens after every one R but
you can also configure it so let us
understand the process of check pointing
in detail so here is the FS image and
the edit log so the FS image in the disk
and edit log recite in your RAM so what
the secondary name node does is that it
first copies the FS image and the edit
log and adds them together in order to
get the updated FS image and then this
FS image is copied back to the name node
and now the name node has an updated FS
image and in the meantime a new edit log
is created when the checkpointing is
happening so this process keeps going on
and hence it helps the name node in
order to keep an updated copy of the FS
image of the transactions every hour so
now let us talk about the data nodes so
these are the slave daemons and this is
where your actual data is stored and
whenever a client gives a read or write
request the data node serves it because
the data is actually stored in the data
nodes so this is all about the
components in HDFS now let us understand
the entire HDFS architecture in detail
so we have got different data nodes here
and we can set up different data nodes
in racks so we in rack one we have got
three different data nodes and in rack
two we have got two different data nodes
and each of the data node contains
different data block because in data
nodes the data is stored in blocks so
we'll learn about that in the coming
slides so the client can request for
either a read or write so let's say that
the
and regrets to read a particular file so
it will first go to the name node since
the name of contains the metadata the
name node knows exactly where the file
is so it will give the IP addresses of
the data nodes where the different data
blocks are out that particular file and
it will tell the client that you can go
to this IP address you can go to this
data node and you can read the file and
then the client goes to the different
data nodes where the data block is
prison and finally the read request is
served and now let's say the client
wants to write a file again it will
contact the main node and the name node
will check the metadata and we'll see
where the space is available and it will
check whether space is available or not
and then again we'll give the IP
addresses of the data nodes where client
could write the file and similarly the
writing mechanism also happens so this
is how entire readwrite requests is
served by the data nodes and the name
node so now let us talk about HDFS block
and replication and since I've been
telling you that HDFS is a
fault-tolerant system let us see how so
each file is stored in HDFS as a block
so whenever you dump a file into HDFS it
breaks down into different blocks and it
is distributed across your Hadoop
cluster and the default size of each
block is 128 MB so let's say that I have
a file of 380 MB so it'll be divided up
into three blocks so the first block
will be of size 128 MB the second will
be 128 MB and the third will occupy
whatever the remaining size of the file
is which is 124 MB so now tell me that
if you have a file size of 500 MB how
many blocks it will create all right so
as I said for real head says for of
course guys you are right we will have
four blocks so the first three blocks
will be of the size 128 MB and this last
block will just talk about the remaining
file size which is 116 MB so now let us
know what is block replication so
whenever you dump your file into HDFS
first it is divided up into blocks and
then each of the block is copied to
types so now you have the original block
and two more copies of the same block
the replication factor equal to three
means that there are three similar blogs
in your Hadoop cluster so you can see
that I have a file of 248 MB 128 MB and
120 MB so my block one is there three
times and block two is also there in
three times in three different data
nodes so we use this replication factor
so that if any of the data node goes
down so we can retrieve the data block
back from the two different data nodes
so this is how data blocks are
replicated in HDFS so now in order to do
the replication properly there is an
algorithm which is known as RAC
awareness and it provides us optimized
fault tolerance so RAC awareness
algorithm says that the first replica of
a block will be stored in a local rack
and the next two replicas will be there
in a different rack so we store a data
block in one rack so that our latency is
decreased so now these are the commands
that you use to start your Hadoop
daemons your Hadoop daemons like your
name node your secondary name node and
your data nodes in the slave machine so
in order to start all the Hadoop daemons
HDFS and yarn so I have not explained
you yarn yet but yarn is a processing
unit of Hadoop so it will also start all
the yarn daemons like the resource
manager and the node manager also then
this is a command to stop all the Hadoop
daemons and with GPS you can check what
are the demons that are currently
running in your machine so let me just
show it to you so the first thing that I
need to do is I need to change the
directory to my hadoop directory so I'll
just do CD Hadoop and now I can run all
the command and you remember the first
command was dot slash has been
stardoll Sh they'll ask for password
we also tell you that you can also
configure it to be a password list
process so that you don't have to enter
passwords when it wants to run certain
demons so let us now use JPS so here are
the demons that are running in my master
so I've got my node manager my secondary
name node
JPS itself is a demon then data node
resource manager and name node so tell
you all about resource manager and node
manager in the coming slides so don't
worry about that so these are just the
demons that are running in my master
machine now let me show you what are the
demons that are running in my slave
machine so this is the terminal of my
slave machine so I'm just going to run
JPS here and these are the processes or
the demons that are running in my slave
machine so node manager and data node
both are slave demons and they are
running in my slave machine so if you
want to stop all this demon so you can
run the same command only instead of
start you can just put a stop here so
since I'm going to use my HDFS so I'm
not going to stop it and show it to you
but the process is same so these are a
few commands that you can use to write
or delete a file in Hadoop so if you
want to copy a file from your local file
system to your HDFS you can use this
command Hadoop FS - put and this is the
name of your file so you have to type
the proper path of the file so that it
can copy to HDFS so you can also mention
the destination folder in HDFS where you
want to copy it if you're leaving it
blank it means that it will just copy
onto the master directory in hadoo then
if you want to list out all the HDFS
files you can do that using this c'mon
and you want to remove that file you
move the same file again you can use
this command hadoop SS - RM which is
used for removing now this is also the
first step that you need to do when you
are starting to analyze something so
there's the same way that we'll copy our
datasets into our HDFS first and then
analyze it so we have seen HDFS now let
us take a look at yarn which is the
processing unit of Hadoop so what if
your yarn is nothing but it is MapReduce
version 2 so when Hadoop came up with
this new version Hadoop 2.0 it
introduced yarn as the new framework and
it stands for yet another resource
negotiator and it provides the ability
to run non MapReduce applications and
begin the yarn we
able to integrate different tools like
apache spark hive pig etc and it
provides us with a paradigm for parallel
processing over hadoop so when you're
dumping all your data into HDFS and it's
getting distributed and all this
distributed data are processed parallely
and it is done with the help of yarn so
you can see over here the architecture
of yarn so it is again a master/slave
topology so the master daemon here is
known as resource manager and slay
demons are known as node manager so let
us take a look at this components one by
one the first up is the resource manager
so this is the master daemon and it
receives the processing request whenever
a client comes up with a request he
comes to the resource manager first
because the resource manager manages all
the slave nodes are the node managers so
whenever a client comes and he wants to
process some data the resource manager
takes that request and passes the
request to the corresponding node
managers now let us see what is a node
manager now node managers are the slave
daemons and they are installed on every
data node so you know that our data is
divided up into blocks and are stored in
data nodes and they are processed in the
same machine so in the same machine
where the data node is set a node
manager is also present to process all
the data present in that data node and
it is responsible for execution of tasks
on every single data node so this is
where the actual processing of data
takes place now let us see the entire
architecture in detail so the client
comes up with a request to the resource
manager in order to process the data and
then the resource manager passes on the
request to the node managers so there
are this important components that I'm
going to talk about and you should pay
attention to it so the node manager we
have got a container and an app master
now an app master is large for every
specific application code or every job
or every processing task that the client
comes up with so the application master
or the app master is responsible to
handle and take care of all the
resources that is required in order to
execute that code so if there is any
requirement for any resources its the
app master who asked for the resources
from the resource manager
and then the resource manager provides
the app master with all the resources
and then it asked the node manager to
start a container and the container is
the place where the actual execution
happens so now let me see the entire
yarn workflow in order to understand
things better so here is the client and
this client wants to run a job so in
this example I'm considering a MapReduce
job so the MapReduce code is first
displayed as MapReduce job and then the
client wants to run this particular job
he submits the job to the resource
manager and also resource manager to
execute this the resource manager gets
back to the client with an application
ID for his job then the resource manager
starts a container where the app master
is launched now the app master is also
launched in a particular container the
app master then gathers all the resource
requirements in order to run that job
and ask the resource manager to allocate
all the resources after that when all
the resources are provided the node
manager launches a container and starts
the container and this is where the job
executes now let us take a look at the
entire Yarn application workflow step by
step so the first step is the client
submits an application to the resource
manager then the resource manager
allocates a container to start the app
master and then the app master registers
with the resource manager and healthy
resource manager that an app master has
been created and it is ready to oversee
the execution of the code many app
master ask containers from the resource
manager the app master also notifies the
node manager to launch containers and
after the containers are launched the
application code is executed in the
container which was the application code
of the particular client and then
decline contact the resource manager to
ask for the application status whether
it is executed properly or not and after
it has been executed successfully the
app master unregistered with the
resource manager so this is the entire
workbook now let us take a look at the
entire Hadoop cluster architecture HDFS
with yon so here you can see that both a
CSS and yarn follows a master sized
authority and the master in HDFS is
named
and master in yarn is resource manager
the slave demons in HDFS are the data
nodes this is where all your data is
stored and in yarn it is node manager
this is where your data is processed in
a container and the app master takes
care of all the resources that is
necessary in order to execute your
program there is one important thing
that you should know and you must have
noticed it that my data node and my node
manager will lie in the same machine so
this data node and this node manager
will be in the same machine and this
data node and this node manager will be
in the same machine but it is not
necessary that the name node and the
resource manager would be the same
machine they could be but it's not
necessary now a name node could mean a
different machine and a resource manager
can be in another machine all right so
don't get confused that these two will
also be on the same machine which is not
the case so now let me tell you about
the Hadoop cluster hardware
specification some of the hardware
specifics that you should keep in mind
if you want to set up a Hadoop cluster
so for the name node you need RAM with
64 GB your hard disk should be minimum
of one terabyte the processor should be
Zenon with eight cores the ethernet
should be three by 10 GB per second the
operating system should be 64-bit CentOS
or Linux the power should be redundant
power supply because you don't want the
name node to go down right because the
name node goes down your entire HDFS
posture will go down and for the data
node you made 16 GB of RAM hard disk
should be 6 into 2 terabytes because
this is where you'll be storing all of
your data so it needs to have a lot of
memory the processor should be set on
with 2 cores
Ethernet 3 by 10 Gbps and 3 by 10 gene
per second and OS should be again 64 bit
century for Linux and for the secondary
name node your RAM should be 32 GBS your
hard disk should be one terabyte
processor then on with 4 cores Ethernet
3 by 10 GB per second oh it should be 64
bit CentOS Linux and power again should
be redundant power supply now you might
pause your screen and take a look or
take a screenshot of this image and
don't worry this present
and this recording would be there in
your LMS as well so this is what you
should keep in your mind so if you're
setting up a Hadoop cluster so these are
the hardware specifications required to
do that now let me tell you about the
real-time Hadoop cluster deployment so
let us consider our favorite example for
just Facebook so Facebook has got 21
petabytes of storage in a single HDFS
cluster and 21 petabytes is equal to 10
raised to the power 15 bytes and they
have got 2,000 machines per cluster and
32 GBS of ram per machine they run 15
MapReduce tasks and each of these
machines run 15 MapReduce tasks and
1,200 machines have 8 cores each and 800
machines have 16 cores each and there
are 12 terabytes of data per machine so
it has got a total of 21 petabytes of
configured storage capacity and it is
larger than the previous known Yahoo's
cluster which was known to be the
largest to do cluster it was a 14
petabytes of Facebook has beaten Yahoo
with 21 petabytes and now let's talk
about another use case which is Spotify
so how do you feel is music in Spotify
alright so it looks like some what do
you do so even Spotify users Hadoop for
generating music recommendations because
when you're listening to music you'll
see that some of the music some new
songs are recommended to you which also
belongs to the same genres that you have
been listening to right so it is done by
big data analysis with Hadoop and
Spotify has got 1,650 note clusters and
they have got 65 petabytes of storage
procs and Spotify has 70 TB of RAM and
they run more than 25,000 daily Hadoop
drops and it has the 43,000 virtualized
course so it is even a larger cluster
than Facebook right so these were two
use cases who use Hadoop cluster in
order to process and store big data so
now you have learned all about Hadoop
the HDFS and yarn both the storage and
the processing component of Hadoop so
now let us take a look at Apache
part so party spark is an open source
cluster computing framework for
real-time processing and it has been the
thriving open-source community and is
the most active Apache project at this
moment and spark components are what
make Apache spark sauce and reliable and
a lot of sparkle phonons were built to
resolve the issue that cropped up while
using Hadoop MapReduce so foggy spark
has got the following components it has
got this forest core engine now the core
engine is for the entire bar framework
so every little component is based on
and it is placed on the core engine so
at first we've got sparks equal the
sparks equal is a spark module for
structured data processing and it can
run unmodified hive queries on existing
Hadoop deployment and then we've got
spark streaming now spark streaming is a
component of spark which is used to
process real-time streaming data and it
is a useful addition to the core spark
API because it enables high throughput
and fault tolerant stream processing of
the live data streams and then we have
got spark and the lip this is the
machine learning library first part and
we'll be using spark and live in order
to implement machine learning in our use
cases - and then we've got graphics this
is the graph computation engine and this
is the spark API for graph and graph
parallel computation so it has got a set
of fundamental operators like sub graph
joint vertices etc and then you've got
spark R so this is a package for our
language to enable our users to leverage
spark power from our shell so the people
who have already been working on R and
they're comfortable with our so they can
use the our shells directly and the same
time can you as far as using this
particular component with the spark are
they can write all of your code in the
our shell and sparkle processes for you
now let's take a deeper look at all this
important components so we've got spark
core and spark core is the base engine
for large-scale parallel and distributed
data processing the core is the
distributed execution engine
and the Java Scala and Python API offer
a platform for distributed EDL
application development and further
additional libraries which are built on
top of the court allowed diverse
workloads for streaming sequel and
machine learning it is also responsible
for scheduling distributing and
monitoring jobs in the cluster and also
interacting with storage systems now let
us take a look at the spark architecture
so party spark has a well-defined and
layered architecture where all the spark
components and layers are loosely
coupled and integrated with various
extensions and libraries so first let us
talk about the driver program so this is
the spark driver which contains the
driver program and spark context so this
is the central point an entry point of
the spark shell and the driver program
runs the main function of the
application and if the place where spark
context is created so what is park
context spark context represents the
connection to the entire spark cluster
and it can be used to create resilient
distributed data set accumulators and
broadcast variables on that cluster and
you should know that only one spark
context may be active per job our
virtual machine and you must stop any
active spark context before creating a
new one so let's talk about the driver
programs the driver program that are
runs on the master node of the spark
cluster it schedules two job execution
and negotiates with the cluster manager
so this is the cluster manager over here
so the question manager is an external
service that is responsible for
acquiring resources on that spark
cluster and allocating them to a spark
job and then in the worker node we've
got the executors so the executor is a
distributed agent that is responsible
for the execution of tasks and every
spark application has its own executor
process the executors usually run for
the entire lifetime of a spark
application and this phenomenon is also
known as static allocation of executors
but you can also opt for dynamic
allocations of executors where you can
add or remove perfect
dynamically to match with the overall
workload okay so now let me tell you
that what actually happens when a spark
job is submitted
so when a client submits a spark user
application code
the driver impulsively converts the code
containing transformations and actions
into a logical directed acyclic graph or
dad and at this stage the driver program
also performs certain kinds of
optimizations like pipelining
transformations and then converts the
logical dag into a physical execution of
plan with a set of stages and after
creating the physical institution plan
it creates small physical execution
units that are referred to as tasks
under each stage and this talks are
bundled to be sent to the spark cluster
so the driver program then talks to the
cluster manager and negotiates for
resources and the cluster manager then
launches the executors on the worker
node on behalf of the driver and at this
point the driver sent tasks to the
cluster manager based on the data
placement
and before the executors begin execution
they first register themselves with the
driver program so that the driver has
got a holistic view of all the executors
now the executors will execute the
various tasks that is assigned to them
by the driver program and at any point
of time when the spark application is
running the driver program will keep on
monitoring the set of executors that is
running the spark application code and
the driver program here also schedule
future tasks based on data placement by
tracking the location of the cache data
so I hope that is understood the
architecture of spark any doubts
all right no doubt now let us take a
look at spark sequel and its
architecture so Swan sequel is the new
module in spark and it integrates
relational processing with sparks
functional programming API and support
squaring of data either be a sequel or
be a hive query language so for those of
you who have been familiar with our DBMS
so smart sequel will be a very easy
transition from your earlier tools
because you can extend the boundaries of
traditional relational data processing
which parts people and it also provides
support for various data sources and
makes it possible to weave sequel
queries with code transformation and
that is why it's press equals become a
very powerful tool so this is the
architecture of spark sequel so let's
talk about each of this components one
by one the first we've got the data
source API so this is a universal API
for loading and storing structured data
and it is still no support for hive Avro
JSON JDBC CSV purckett etc it also
supports third party integrations
through response packages that is called
the data frame API so data frame API is
distributed collection of data that is
organized into named columns and it is
similar to a relational table and sequel
that is used for storing data in tables
now it is also domain-specific language
of the cable or dsl applicable on
structured and semi-structured data so
process data from kilobytes to petabytes
on a single node cluster to a multi node
cluster and it provides different API
for Python Java Scala and our
programming so I hope that is understood
about the architecture of smart people
so will be also using spark sequel in
order to solve our use cases so these
are the different amounts starting spark
demons so these are very similar to the
Hadoop commands to start these HDFS
daemon
so you can seem to start all these funky
months since barking is our master and
worker so you can use this command to
check if all demons are running on a
machine you can use JPS just like to do
and then in order to start the spark
show you can use this they can go ahead
and try
it's very similar to the Hadoop part
that I just showed you earlier so I'm
not going to do it again so we have seen
Apache spark also so now let us take a
look at k-means and Zeppelin
so k-means is the clustering method and
Zeppelin is what we're going to use in
order to visualize our data so let's
talk about the k-means clustering now so
k-means is one of the most simplest
unsupervised learning algorithms that
solve the well-known clustering problem
so the procedure of k-means follows a
simple and easy way to classify a
dataset through a certain number of
clusters which is fixed prior to
performing the clustering method so the
main idea is Devine K centroids one for
each cluster and these centroids should
be placed in a very cunning way because
of different location causes different
results so here let us take an example
so let's say that we want to cluster
total population of a certain location
so we want to cluster them into four
different clusters namely Group 1 2 3 4
so the main thing that we should keep in
mind is that the objects in Group 1
should be as similar as possible but
there should be much difference between
an object in Group 1 and 2 it means that
the points that are lying in the same
group should have similar
characteristics and it should be
different from the points that are lying
in a different cluster and the
attributes of the objects are allowed to
determine which objects should be
grouped together for example let us take
in the same example that we are using
the u.s. county so let us consider the
second data set that we have used so
there are a lot of features that I
already told you there is like there are
age groups and they are categorized by
professions and they're also categorized
by their ethnicity so this is the thing
that we are talking about so these are
the attributes that will allow us to
cluster our data so this is k-means
clustering so here is one more example
so let us consider a comparison on
income and balance so in my x-axis I
have got the gross monthly income and in
the y-axis I have got the current
balance so I want to cluster my data
according to this two attributes so here
if you see this is my first cluster and
this is my second
so this is the cluster that indicates
people who have got high income but low
balance in their account they spent a
lot and this cluster comprises of the
people who have got low income but high
balance they safe so you can see that
all the points that are lying here have
got the similar characteristics that
they have got a low income and high
balance and here are the people share
the same characteristics where they have
called low balance but high income and
there are a few outliers here and there
but they don't form a cluster so this is
an example of k-means clustering so
we'll be using that in order to solve
our problems so does anybody have any
questions alright so here is one more
example and one more problem for you so
you guys will tell me now so the problem
is that I want to set up schools in my
city and these are the points which
indicate where each of the student lives
so my question to you is where should I
be building my school if I have students
living around the city in this
particular locations and in order to
find out that we will do k-means
clustering and we'll find out the center
points right so if we can cluster and
make groups of all these locations and
set up schools at the center point of
each cluster then that would be optimum
isn't it because that is how the
students have to travel less it will be
close to everyone's house and there it
is so we have formed three clusters so
you can see the brown dots are one
cluster then the blue dots are one
cluster and the red dots are one cluster
and we have set up schools in the center
points of each of the cluster so here is
one and here is one and here is got
another one so this is where I need to
set up my school so that my students do
not have to travel much so that was all
about k-means and now let us talk about
Apache Zeppelin so this is a web based
notebook which brings in data ingestion
data exploration visualization sharing
and collaboration features to Hadoop and
spark so you can see that so remember
when I showed you my Zeppelin notebook
you can see that we have written the
code there
we have even run sequel codes there and
we have made visualizations by executing
code there so this is how interactive
Zeppelin is and it supports many many
interpreters and it is a very powerful
visualization tool that you can use that
goes on very well with Linux systems and
it supports a lot of language
interpreters it supports our Python and
a lot other interpreters so now let us
move on to the solution of use case that
this is what you have been waiting for
set first will solve our US County
solution so the first thing we'll do is
we'll store the data into a CFS then
we'll analyze the data using Scala spark
sequel and spark Emily and then finally
we'll find out the results and visualize
them using Zeppelin so this was the
entire u.s. election solution strategy
that I told you I don't think I should
repeat it again but if you want me to I
can so should I repeat all right so most
of the people are saying no so I'll not
go through this one again so let me just
go to my VM and execute this for you so
this is my Zeppelin and I'll open my
notebook here and let us go to my u.s.
election notebook and this is the code
so first of all what I'm doing is that
I'm importing certain packages because
I'll be using certain functions that are
in those packages so I have imported
Sparx equal packages and that I've also
imported spark and Mallette packages
because I'll be using k-means clustering
so vector assembler enables me certain
machine learning functions so over here
the vector assembler package gives me
certain machine learning functions that
I'm going to use so I've also imported
the k-means package because I'll be
using k-means clustering now the first
thing that you need to do is that you
need to start the sequel context so I
have started my SPARC sequel context
here and the next thing that you need to
do is that you need to define a schema
because when you want to dump our data
set or we want to dump our data it
should be in a particular format and we
have to tell Spock that in which format
that it should be so we are defining a
schema here so let me take you through
the code so I'm storing schema in a
variable called schema and we have to
find the schema in a proper structure so
we're going to start with struct type
and since you know that our data set has
got different fields as columns so we're
going to define this as an array of
fields so this is an area and struck so
we're defining the different fields now
so we'll start with the first field by
defining it as struck field inside the
bracelet we should mention what should
be the name of that particular field so
I've named it a state it should be a
string type and true that means it is of
string type the next we have got FIPS
which is again of string type now I know
that Phipps is a number but since we're
not going to do any kind of numeric
operations on sips so we're going to let
it stay as string then we have got party
as string type candidate a string type
then votes as integer type because we're
going to count the number of votes and
there is going to be certain numerical
operations that we're going to perform
that will help us to analyze our data
then we've got fraction votes which you
know is a decimal type so we have kept
it as double type the next thing you
need to do is that spark needs to read
the data set from the HDFS so for that
you have to use this command spark read
option header true header true it means
that you have mentioned and you have no
spark that my data set already contains
column headers because state and state
ABV are they are nothing but their
column headers so it will not so you
don't have to explicitly define column
headers for it or neither will spark
choose any random row as a column header
so it will choose only the column
headers that your data set has then you
have to mention the schema that you have
defined so I have defined it in another
variable schema so that's what I've
mentioned it so that is why I've
mentioned it and my file should be in
CSV format and then I have to mention
the path of the file in my HDFS and this
is the path and I've stored this entire
data set in my variable DF now what I'm
going to do is that I'm going to divide
up certain rows from my data set because
you know that my data set contains both
the Republican and the Democrat data and
I just want the Democrat data right
because we're going to analyze the
Hillary Clinton and Bernie Sanders art
so this is why you divide your data set
okay the first thing that we have done
is that we have created one more
variable
called DFR and we have applied a filter
where Part II is equal to Republican and
then we're storing the Democrat Party
data into DFT so we're going to use D F
D from now onwards and D F are the
Republican data this is going to be your
assignment for the next class so I'm
going to analyze the Democrat data and
then after this class is over I want you
guys to take the Republican data this
data set is already available in your
elements you have got the VMS also with
everything installed so please when
you're at home when you have free time
just analyze the Republican data and
tell me what we're the reasons that
Donald Trump won so I want you all to do
that analysis and come up with that in
the next class and we'll all discuss
about it that whatever results and
conclusions that you have made after
analyzing the Republican data and that
way you'll also learn even more and then
we also practice for you after today's
class so all right so we're going to
take D F D now and the first thing that
we will do is that will create a table
view and I'm going to name the table
view as election so let me just show you
what it looks like and what it has so
this is the command that I have run in
Zeppelin so this is the sequel code that
I've run in step one and you can see
that I've got sage state everywhere and
I have only got the Democrat data all
right and let's go back all right so
after creating the table view now every
of the Democrat data is in my election
table so now what I'm doing is that I'm
creating a temporary variable and I'm
running a sparse sequel code so what I'm
actually doing by writing this code the
motive of writing the sequel code or the
sequel query is that I want to refine my
data even more so what I'm trying to
analyze here is that how a particular
candidate actually want so I don't have
to do anything with the losing data
because you know that each of the fits
contain one of the losing candidate
member and one of the winning candidate
member it contains the data of the
winning candidate and the losing
candidate also because my data set
contains both the data of Bernie Sanders
and Hillary Clinton in some parts Bernie
sander won in some counties Hillary
Clinton won so I just want to find out
that who are the winners in a
particular County okay so I'm going to
refine that data and for that I'm using
this query so I'm going to select all
from election and then I'm going to
perform an inner join with this query so
this is one more query inside this
entire query so let me tell you what am
I actually doing so first what we have
done is that we have selected Phipps as
be so you know that now you have got two
entries for each Phipps
so each Phipps actually appear twice in
my data set so I've named it as B now
and then we're counting the maximum
fraction votes so you know that in each
fit we have the maximum fraction vote
and then we can find the winner by
actually seeing who has got the maximum
fraction votes and then we have named it
as a the maximum fraction votes column
is named as a and we're grouping by FIPS
so now each of my sips will be selected
with which has a maximum fraction vote
and I have two columns for that v which
is 1 0 0 1 and 1 0 0 1 so only that we
row will be selected which has the
maximum fraction votes so now I'll have
the winner data and I've named this
entire table inside this query as Group
TT and then I'm validating it as we're
election dog Phipps the main table view
dot sips should be equal to the B column
that we have created in group TD table
and the election that fraction vote
should be equal to group PT a so any
doubts on this query about how I have
written this all right so now what we
are doing is that whatever data that we
got here I'm storing that in election 1
so let me just show you what is an
election 1 now so this is my election
table only and you can see that I've got
two flips so 1 0 6 7 1 0 6 7 so now let
me show you election one
so there if now you can see that I don't
have reputation of FIPS I only have one
entry for Phipps and that is the row
which tells me who won in that County
or in that particular fit or in the FIB
associated with a particular County so
you can see for Bullock it was Hillary
Clinton for calla who it was Hillary
Clinton Taraki also Hillary Clinton and
then Statehouse district 19 as Bernie
Sanders so in Alaska it's mostly Bernie
Sanders so this is what we have done now
and then you can see that we have also
got additional columns as B and a so a
tells you the maximum fraction vote and
B tells you the fifths so the data in
sets and the data in B are same and data
in fraction rose and data in a is say
right so what I'm going to do now is
that since my columns are repeating and
they have the same value so I don't want
a and B now right so what I'm going to
do is I'm going to filter out the
columns that I donate and in this case I
don't want B and a so what I'm going to
do is I'm going to make a temporary
variable again I'm using the temporary
variable to store some data temporarily
so I'm running the SPARC sequel code to
select only the columns that I want so I
want the states today will be our
country's EPS party candidates both
faction votes from election one and I'm
storing everything Indy winner I've
created this new variable and whatever
there was in Tampa I'm assigning it to D
winner so now I have got only the winner
data so I've got all the counties and I
have got who won in that particular
County and by how much fraction votes so
what I'm just doing till now is that I'm
just refining our data set so that will
be easy for us to make some conclusions
or to gain some insights from that data
right and also let me tell you that it's
not always necessary that you refine
your data set in the exact way that I am
doing if you have something in mind
after you have seen your data and
understood your data and you found out
that what actually you need to do you
can carry out different steps to do that
also this is just one way of doing that
and this is my way of doing that so I'm
just telling it to you and then we're
creating a table view for D winner and
we're going to name it as Democrats
so let me go again and let me show you
what the Democrat table view looks like
so you can press shift enter
so there you have we don't have a and B
column that we had an election one so
I've just got the winner data
so now let us go back and now what we're
going to find is that I want to find out
that which of the candidates won them by
state and then whatever date and
whatever result I'll get it will be
stored in the temporary variable and
then I'm signing everything that will be
stored in the temporary variable to a
new variable called D state and then I'm
going to create and then similarly I'm
going to create a table view for the
state with this state let me show you
what my state table view actually
contains
so there it is so I've got stay
netiquette Hillary Clinton won 55
counties Florida Hillary Clinton won 58
counties so this is what we have come up
to for our first data set so now let us
see what we can do with our second data
set that contains all the different
demographic features the first thing
again you have to define a schema and
this I'm naming that schema as schema
one and since you know that we have got
almost 54 columns so I have to mention
and I have to define all those 54 column
cells so you remember what those each
columns contain so this is exactly what
I've done and I don't need to go through
every line but I already told you how to
define a schema you can have this code
in your LMS so you can take a look at it
so the next thing we're doing again we
have to read our data set and the NAM
story my data set into a new variable
called df1 and this is the path in my
HDFS where my data set was and then I
had created a table view for my dataset
which is called fact now let me show you
what facts contain
so you can see that it can do
state abbreviation population 2014 so
instead of using the code now or the
encoded form that was actually there in
my dataset I have given a very meta name
that will describe what it contains
right into the BST - 1 4 I have got
population 2014 so that makes sense
right
it contains all the 54 demographic
features or different features that was
in my dataset Y below not Hispanic or
Latino living in the same house one year
and over foreign-born persons language
other than English spoken at home high
school graduate or higher so it contains
basically all the different features or
all the different columns that actually
wasn't my data set and that I have
defined it in my schema so this is what
facts have so now what I'm going to do
is that I'm not going to analyze my
whole data based on all these different
features I'm just going to choose some
specific features in order to analyze it
I'm going to just take a few at one so
these are the different features that
I'm going to use I'm going to use Phipps
I'm going to use States I'm going to use
state a BB RS then area name candidate
and people who are over 65 years to
senior citizens and then female people
white alone black or african alone I'm
choosing Asian alone Hispanic or Latino
so basically what I'm trying to do is
that I'm trying to check what is the
popularity of Hillary Clinton among the
foreign people or people from different
ethnicities some choosing white people
black people and Hispanic people so I'm
just trying to analyze it okay and then
and you know that I stored this in a
temporary variable again and then
whatever result I will get by running
the sparks equal code I'll store it in a
different variable called GFX and then
I'll store it and then I'll make a table
view for dfx as winner facts so let me
show you what winner facts look like
so it's winner and underscore facts
so we have got the feds the state is
Alabama's the state abbreviation is al
for Alabama the area name is Otto Gahr
County the winner was Hillary Clinton
and the people over 65 years in that
particular County is 13.8% female
percentages 51.4 white alone at seventy
seven point nine and so these show you
the data so black or african is 18
percent and then i've got the different
fields that I've selected Asian alone
Hispanic or Latino foreign word so I've
chosen 14 features to analyze it from
alright so now what I'm doing again is
that I'm going to divide the Hillary
Clinton data and the Bernie Sanders data
so that we can analyze only why Hillary
Clinton won or why Bernie Sanders won in
some particular counties so we are
applying filter the same way we divided
democrats and the republican Ischl
primary results data set so this is what
we have done so you know that's stored
india fact so we're putting a filter in
DF sax and we're candidate is equal to
Hillary Clinton so that will be stored
in 8c and then whatever the data of
Bernie Sanders will be stored in BS so
after that what we're doing is that
we're doing a one hot encoding so we'll
add two more columns in our dataset as W
underscore B F and W underscore at C so
in this case we're going to do one heart
encoding and what we are going to do is
that so we're going to include or we're
going to attach two more columns in
winner fax as W HC and WBS and it will
just contain either one or zero set it
in the way that in whichever County so
if you're considering a County let's say
autograph County so if Hillary Clinton
is the winner it will have one in W HC
and in WB s it will have zero similarly
where in whichever counties Bernie
Sanders won so Bernie Sanders will have
one and then W HC so WB s will have 1
and W HC will have 0 and then we're
creating different views for both of
these two together so this will only
tell me where ever
hc' is one that means they'll only show
me the counties where a Hillary Clinton
won and this will only show me the
counties where Bernie Sanders won okay
and we're creating a view for both of
these so for Bernie Sanders the view is
WBS and for Hillary Clinton it's whc and
then finally we're merging both of them
together using Union so select all from
whc you need all select all from WPS and
finally have stored it in results and we
have created a table view known as
result so let me show you what those
result contains
so their attendants for all Turk
accounted with Hillary Clinton so we've
got the Bernie Sanders data over here at
the bottom so there and I have got all
the different fields also from my second
data set the different features that I
chose from a second data set to analyze
it so now comes the actual analyzing
part this is where we're going to
perform k-means but first we have to
define the future columns actually you
have to define what is the input that
you're going to feed so that you get an
output so this is actually the input
that you're going to feed to the machine
so that the machine learning goes on and
finally it gives you some kind of
results right so this is where I'm
defining so again I'm using an area to
define all the different fields for my
data sets so I'm using persons 65 years
and over female person percentage white
alone black or african-american alone
Asian alone Hispanic or Latino
foreign-born persons language other than
English spoken at home bachelor degree
or higher veterans home ownership rate
median household income persons below
poverty level and population per square
mile and whc and WBS and then I'm going
to use the vector assembler this will
enable me some machine learning so this
is what enables different machine
learning algorithms where we're using
k-means so my input column is feature
call so this is going to be the input
and my output column will be called as
features or whatever result that I'm
going to get is features and we have to
transform results so this is the final
table view that we have created and you
know what transforming means
transforming again means so in our
strategy we already saw that we have
transformed the data first
so my updated data set was result
sometimes transform result input columns
is going to be DS which is feature calls
and my output table view will be called
features and then we're going to perform
the k-means clustering and we're going
to store it in a variable called k-means
so we are using different functions so
we're going to use different functions
from SPARC
and a lip library and we have chosen
spar gamma clustering cami
and you know that it game means we
already define that how many clusters do
we need and we need four so we have
selected four clusters and then we are
going to set feature columns as features
and then set prediction column as
predictions so after that we're going to
make a model so we have defined our
input output columns in rows so we're
going to use game inset row so whatever
predictions that we'll get we're going
to store it in model and then what we're
going to do is that we're going to print
the cluster centers for each cluster so
let me show you what my cluster centers
are so after we have run this code so
you can see that these are the different
cluster centers just so that I can make
understand about what we're going to do
after k-means clustering how to analyze
it so the numbers are present there
plays very happy surgery so what I have
done is that I have picked out each of
the cluster Center points and then I
have made a new table yes so this is it
so you know that we have got four
clusters we have got the zeroth
clustered first cluster second Questor
and third cluster so zero one two three
okay so four clusters and we have found
out this cluster centers according to
different features that we set into my
k-means algorithm so what we observed
here in wh CN WBS is that the winning
percentage or the winning chances of
Hillary Clinton was 0.9 whereas winning
chances for Bernie Sanders with 0.1 and
then if you observe the differences in
the cluster centers for each feature
here you can see that there is not much
difference not even here say it's 50 49
49 51 and then in white alone again it
is not much of a difference but if you
see here that it's nine and it's going
to 60 so you can do a more detailed
analysis on black or african-american so
if you want to know the real support of
black or african-american you want to
see how what was their voting pattern
how popular was Hillary Clinton among
them so maybe this
would be a good feel to analyze because
you see the variations in number
similarly you can check out other
features too you can check out here at
16 8 9 and 36 so maybe again Hispanic or
Latino feel you should make some more
analysis on it and even here you can see
in veterans there is forty seven
thousand eight hundred six whereas we
have got one hundred and eighty two
thousand dollars and there is also a lot
of difference here is only two thousand
seven hundred fifty nine we've got in
ten thousands we have got numbers in
even hundred thousands years so this is
how we can identify that which are the
fields or we can find out the main
reasons or the main points where we
should make our analysis on so let us go
back to our suppli notebook so here it
is so now what you are doing is that
we're going to visualize the result
first so we're counting from prediction
so you can see that in cluster one so
the prediction means prediction contains
my cluster since you know that I have
stored my clusters my cluster
information in prediction this is my
output after the k-means so I have got
these many clusters so this is the count
of my counties or account of different
areas that lie in my particular cluster
so you can see that in cluster one I've
got 1917 in cluster two I've got 750 one
so maybe I should pay more attention on
analyzing cluster one right so that's
why I have selected cluster one here and
we are making different predictions so
you can see that we in x-axis I have got
foreign-born people and in y-axis I have
chosen language other than English
spoken at home and then we're grouping
it by candidate so you can see the
lighter blue is for Bernie Sanders and
the more dark blue is for Hillary
Clinton
so all this light blue is for Bernie
Sanders and you can see that as the
number of foreign people increases you
can only see Hillary Clinton in the
scatter plot here so there might be a
few outliers like here and the size is
defined according to black or
african-american alone so you can
number that this was the feature where
we find a lot of variations in the
numbers so that's why we grouped it
according to that and you can see the
bigger the circle it represents more
black or african-american alone so what
is the conclusion that we can find out
from this scatter plot we can see that
as the number of foreign people
increases the popularity of Hillary
Clinton is more in larger groups of
foreign people you can also choose
different parameters out of all the
different features that you have chosen
if you remember that we have also seen
the variation in veterans so let us
choose veterans in y-axis so let us also
change x-axis let me just choose white
alone here so what you can see here is
that this is the x-axis that has white
alone and this is the veterans so you
can see that Hillary Clinton is popular
among veterans also in a smaller group
of veterans since we have decided the
size on black or african-american alone
so the size also represents some values
so she is popular among the African
American veterans and then as you go
ahead and as the count increases you can
see actually there since it's a scatter
plot and it almost represents that this
is a point as the number of people
increases or as the number of white
people increases the votes are equally
kind of distributed between Bernie
Sanders and Hillary Clinton because
there are a lot of points in the scatter
plot over here so you can go ahead and
drag and drop different features and you
can make different visualizations on
that so now what we have done is that we
know that there are 1917 counties in my
cluster one so what I'm going to do is
that I'm going to see that how many that
among this 1917 how many were in favor
for Hillary Clinton and how many were in
favor for Bernie Sanders
so in cluster one you can see clearly in
the reclaim ten is the winner and Bernie
Sanders only has got seven or sixty-four
whereas she has got eleven hundred and
fiftyThree
similarly in clustered three again
Hillary Clinton is the winner with nine
and bernie sanders with one then in two
also she has got 388 and bernie sanders
was 363 so this was a very close call
and again in zero you have got one one
nine and thirty and then we we went
ahead and created a line chart also of
the vote distribution for hillary
clinton and bernie sanders so in keys we
have selected prediction and the values
here are wh CN WBS to sum that we have
got over here
so definitely Bernie Sanders is lagging
behind so even though you didn't have
that table for you you can also find it
out according to this line chart so you
can see that in cluster zero so you can
see here that in cluster zero even again
Hillary Clinton was ahead of Bernie
Sanders
so in cluster two there was a very neck
to neck competition and you can see it
in the graph here so this represents
cluster two so you can see they have a
neck to neck competition and again in
cluster three they have got a neck to
neck competition so this describes the
distribution of votes for Hillary
Clinton and Bernie Sanders and
definitely in the Clinton ahead and that
is why of course she won the primary
elections so again you can go ahead and
make the same we have created the same
graph it's only just a area graph
instead of a line graph so the keys here
are estate and candidate so I've got
States and candidates over here and then
the values is counties one so if you
just hold on to this bar chart you can
see that in Connecticut Bernie Sanders
won 115 counties in Connecticut Hillary
Clinton won 55 only since Florida
Hillary Clinton is 58 and in Florida
Bernie Sanders is nine so here you can
see in Maine Bernie Sanders won 462 so
Bernie Sanders got majority votes from
Maine so you can also classified it
state wise you can also find out that
which are the states so as Donald Trump
now you'll know that which are the
states that you can target right so you
know that in Maine a lot of people vote
for Bernie Sanders so maybe Hillary
Clinton is not popular
so you can go ahead and lead out as
Donald Trump's party member you can just
advise him to go and main and carry out
different campaigns because Hillary
Clinton is not so popular there so maybe
it would be a little easier to get the
votes from the people of Maine so this
is what you can make conclusions this
might not be very accurate but this will
be very close so the thing is that you
can make different charts you can make
bar charts you can make pie charts so
whatever counties one that we have made
in the bar chart so this is they're in a
pie chart so this looks better but maybe
it's not insightful I just placed it so
that I can show you that you can make
pie charts also so these are the
insights that he can make after
analyzing your US County data see this
is what he can tell Donald Trump so
these are the different suggestions that
he can actually go in till Donald from
that she is popular among the
foreign-born people and the people who
speak different languages she is popular
among the Hispanic people and then in
Maine she lost in a lot of counties she
almost lost in all of the counties in
Maine so these are the different
insights that you have got and then you
can tell your superior or your employer
who has hired you to do that so this is
what you can present right so this is
for a very beginners level way so there
is some more analysis that you need to
do I just showed you a few options you
can go ahead and try more in the
democrat section also and you remember
that you have to do it for the
republican party so so so let me see
that what you have learned today is a
but if you have any questions right now
you can just go ahead and ask me so does
anybody have any questions all right so
a Jay is telling that I will give it a
try
and ask you in the next class of course
a Jay you can I'm very happy that you
found it interesting and you are going
to try so all right Thank You AJ so now
we'll move on to find out the solution
for the instant cab use case so remember
that we have got the uber data set which
contains the pickup time and the
location by two columns latitude and
longitude and then we have also got the
license number for a particular boat
driver and what
we have to do is that we have to find
the Beehive locations that is the point
where we'll find the maximum pickups and
then we also have to find out what is
the peak hour of the day so this was the
entire strategy so we have got the uber
pickup data set and then we will store
the data into HDFS we'll transform the
data set and make predictions by using
k-means clustering on the latitude and
longitude to find out the beehive points
so now let me open my other notebook the
Bur notebook so again the first thing
that you have to do is copy the uber
data set into your HDFS sub well we've
done that before explaining you the u.s.
county analysis so again the code is
kind of the same the first thing is that
again we're importing some spark sequel
packages and some spark emilich packages
because we're again going to use k-means
clustering so you can see the vector
assembler here against bark and we'll
clustering k-means and other spark
sequel packages so then we have got we
have to start our sequel contacts and
we're doing it the same way then the
first thing again we have to define a
schema so now I don't have much fields
I've got only four fields if you
remember so so the first field was the
date and time stamp that defines the
time so we're defining it as DT then the
next field is the latitude the longitude
and the base and then I'm going to read
my data set and this is the past and my
HDFS where my uber data set is there so
I have to find the schema schema here
the header is true because of course
again my data set because again my data
set contains column headers and I'm
going to store it in DF so my future
calls here is going to be latitude and
longitude because I'm going to find out
the Beehive point the point where I'll
get the maximum pickups from so again I
have set the input calls as feature
calls here and output calls as features
so I'm using the assembler to transform
my data set and then again I'm using
k-means and we use the same elbow method
and we found out that we should make
eight clusters for this data set okay
and then we are selecting the prediction
column or the output column as
predictions and then again we have
printed
cluster centers for each cluster so
definitely whatever result we will
define the cluster centers will tell me
the exact location so this cluster
centers that we'll find after k-means is
actually the Beehive point so this will
be the point where I'll find the maximum
pickups right so here I have printed my
cluster centers so this defines the
latitude and longitude so this is going
to be my location where I'm going to
find the maximum pickups and I've got
eight results like that because I've got
eight clusters and define the Centers
for a different clusters so this is
exactly like the school problem that I
explained you in caming so this is what
exactly happens and since we found out
the Centers of each cluster and that is
where we're placing the school are
building the new school so similarly
this is going to be my beehive point
this is where I'll place maximum off my
caps okay so we found the Beehive point
so the next thing that we need to do is
we need to find the peak hours because I
also need to know that at which time
should I place my caps into those
locations so what we're doing now that
we're taking a new variable called Q and
we're selecting our from the timestamp
column and then the alias name should be
our and we're getting it from our
prediction or from the result that we
got after my k-means clustering so now
we're proving it it will have the
different hours of the day and then it
will just show me the pickups at the
different hours of the day in the
location that we found out are the bee
hive points and then we're going to tell
the how many pickups are we going to get
from that place right so and we are
doing it by descending so the smaller
pickup count will be on first and then
the larger would be at the bottom
similarly again we're creating a new
variable called T and we're doing the
same thing so here what we're doing is
that we're selecting the time the hour
of the day the latitude/longitude
prediction and then we're filtering it
by hour which is not null so we're
filtering out the null values from here
so now we have created a table view for
categories so let me show you what
categories contain okay let me just go
down so I've done some few operations
here so
let's scroll back up and I'll show you
and again we have created table views
for T and Q also which is again T and Q
alright so and then I've made some
visualizations for each so then we have
created a value P where our is not null
so again we filtered out the null hours
and we have created a new view called P
so here is my hours there is my account
in the x-axis that how many pickups were
there and this contains different hours
of the day and then I've grouped at the
prediction the sizes according to the
count so you can see that the bigger the
circle it means more the pickup so you
can find out the biggest circle and you
know that you can find the biggest
circle as you go along the x-axis
because this is where the count
increases so you can find out the
biggest circle would be here and
enlighten my fourth cluster you can see
that there are eight thousand nine
hundred fifteen pickups at the
seventeenth hour of the day which is
around 5:00 p.m. and so you know that
the maximum pickups are around four
o'clock or five o'clock and lies all in
my fourth cluster it means that my peak
hours are around four or five o'clock in
the evening alright so this is what's
inside that we have gained so you can
tell your instant cab CEO that I found
out that so your cabs should be ready
around four or five because maybe that's
the time when people go home from
offices or they are going out for dinner
or something and then and this is what
my another table view looks like which
is T so here we have latitude and
longitude so this is where we are
finding the Beehive locations so if so
I've got the distribution and a scatter
plot again so you can see that we have
got very dense points over here it means
that these represent the bee hive points
so what you can do is that you can just
put a US map and scale it according to
this scale over here and then you can
exactly find out that what is the exact
location where you need to put your
calves around the 17th hour or the
sixteen hour of the day alright so and
you know that we had a lot of rows but
the results are only limited by ten
thousand you could just run ten thousand
rows
but we obviously had a lot more and then
you can check in different clusters so
we have selected from the cluster so now
we're analyzing cluster zero
so here if you see this point over here
this lies in cluster for this lies in
cluster five and this lies in cluster
zero so you can just analyze each
cluster also
so here I have just laid out the
latitude and longitude for my 0th
cluster so you can see here where
prediction is equal to 0 and selected
this from the table view of T so there
you can find out the exact latitude and
longitude for so here the latitude is
forty point seven two two and longitude
is negative seventy three point nine
nine five so this is how it in pinpoint
location where your cab should be during
the peak hours so again if you see this
distribution this is a pie chart that I
have created which just tells you that
what is the count of pickups at each
hour of the day so there are 22 starting
from 0 to 23 there are 24 slices in this
circle so you can see that these few
slices are the bigger chunks so this is
the 19th hour of the day which is around
7 o'clock 6 o'clock 5 o'clock 4 o'clock
3 o'clock so and you can see in the
midnight maybe nobody travels more so
maybe your caps can rest or you don't
have to place more cabs during this part
of the day so these are the insights
that you can gain so any questions on
that so I think after doing the u.s.
county election this was pretty easy to
do and this is also pretty easy to
understand and the results were also
much much clearer right ok so let us go
back to a presentation so now let me
tell you about the ED Eureka elements
that you have so you will have
everything in your elements all these
classes are recorded and they will be
there in your elements so even if during
this class if you forgot something that
I said so you can just take a look at
this recording of the class again in
your LMS so you can just go ahead and
learn on your own see how the all your
course can tend your self-paced course
so whatever documents that you want
related to your subject so you'll find
it here so let me just show you that so
even the entire course content is here
so whatever documents whatever
presentations and all the recordings
will be here in the course content
according to different modules so you
can also have access to different
projects related to your course
signatures download all the problem
statement all the requirements that you
need to do in order to do this project
you can just download
and after solving it you can just submit
it here and again you have the area VM
so there is also proper guideline
mentioned about how to use this VM and
if you have any doubts you can call our
24/7 support team also let me show you
the elements you just have to go to Ed
Eureka CO so I'm already signed in so if
you're not finding so there will be
assigned an option here at the right
corner so you can just sign it and then
just go to courses go to my courses so
here are my different courses that I
have bought from at Eureka so you can
just go to course so here is everything
that you want to get started with Hadoop
so you have got everything mentioned
here the software and hardware
requirements so you can click here to
access at Eureka a VM installation guide
then this is the entire guide for your
LMS also then you've got all the pre
recorded sessions over here class 1
class 2 so you can watch anytime you
want and the best part of your elements
is that you'll have a lifetime access to
it it is not like that
once your classes are over once you have
learned how to do when your batches were
over and you're not attending any more
classes you can still have access to all
the older classes that you attended and
you can view it anytime you want then
there is the course content there is for
module 2 there is the class recording
there is the module presentation the
quiz assignment and everything
and again I'm telling you if you have
any doubts about how to use the LMS or
how to use the virtual machine you can
always call our support team so does
anybody have any questions alright thank
you for attending this session I hope
you had a good time in learning about
Hadoop and spark you can rate us to let
us know that how much you have enjoyed
learning with Eddie Rica you can comment
your experiences you can even give us
some suggestions don't forget to like
this video if it really helps you you
can also help us to get some more ideas
about what we should come up with next
and then you can also take part in our
survey so that we can improve us more so
thank you for watching this video and
I'll see you next time till then happy
learning
I hope you enjoyed listening to this
video please be kind enough to like it
and you can comment any of your doubts
and queries and we will reply to them at
the earliest to look out for more videos
in our playlist and subscribe to our at
Eureka channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>