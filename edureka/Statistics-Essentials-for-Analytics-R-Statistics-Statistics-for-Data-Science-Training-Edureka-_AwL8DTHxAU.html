<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistics Essentials for Analytics | R Statistics | Statistics for Data Science Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Statistics Essentials for Analytics | R Statistics | Statistics for Data Science Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistics Essentials for Analytics | R Statistics | Statistics for Data Science Training | Edureka</b></h2><h5 class="post__date">2018-04-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_AwL8DTHxAU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Lorien right now we are going to start
with module number one so let's start
with the module number one we are we are
going to discuss very basics of
statistic our objective here is to
understand the data types and what are
different types of data that will be
available with us and in fact if you go
and talk to data scientist or if you get
any chance to read two articles related
to data analysis and a statistics you
will see all these jargons are used very
heavily so we're trying to explore what
different kinds of data are available
with us how can we broadly classify the
data into different types and then we'll
go into further analysis of different
data that we are going to classify under
quantitative and qualitative data then
we'll we will try to define what is
population and sample because these are
the two terms that you are going to use
very frequently throughout this whole
course and in fact not in this course
when I will get the chance to explore
more into a statistic world or if you
did get a chance or if you already had a
chance to explore all those areas you
might have already seen the usage of
population sample so frequently so we
are going to discuss what exactly is
sample and what is population how can we
make sample represent the whole
population then as we are proceeding we
will try to see what are techniques or
is it only the blind way that we are
taking a sample out of a population or
is there any logic behind that so all
these things we are going to cover in
topic number 3 where we are going to
discuss understanding sampling
techniques and then at the end we'll see
from the techniques in order in the data
we are going to cover the confusion
matrix how can we represent the data in
conclusion matrix actually understand
the whole model and how it is behaving
so let's start so first of all the
question is what exactly is data so if
you go by the definition the data is
nothing but in general terms refers to
the fact and the statistics collected
together for reference or analysis so
those facts and statistics may vary as
for the organization or as for the field
that you are talking about it may be
data related to banking systems
financial systems the data may be
related to any of the other varied areas
that may be of any kind of research
related data that can be any of the
general data related to the population
of the country things like that so it
can be any facts and when we take all
the
most facts for any kind of reference or
that's been shared for some kind of
analysis in order to come up with some
kind of a result then we termed that
whole collection of facts has it so if
I'm talking about those types and data
or those facts or figures or statistics
can I have different types of tags and
different types of statistics let's see
so we broadly classify the data into two
types we call it as qualitative data and
quantitative data if you go by the
meaning of this term data you will see
qualitative tilted more towards where we
don't have any of the numerical stuff
what we have the properties of the
characteristic of the data will agree
with us that is characteristics or
descriptors those that cannot be easily
measured but can be observed
subjectively such as smell or taste or
textures so all of these kind of things
comes under qualitative data second
category is quantitative data where as
the name suggests we are talking about
something major River because we're
talking about in terms of quantity so
quantitative data deals with numbers
things you can measure dimensions such
as height width length temperature of
room temperature of the country
temperature of an area humidity like
that so further if you go into detail of
qualitative you can broadly divide
qualitative data into two types one is
called normalize data another is called
ordinal data so if you talk about
nominal data so as the name suggests it
represents some kind of characteristic
let's say if we're talking about gender
of a population or a race then we can
say that the gender can be of male or
female you can say name or
characteristic that and theta represents
it is sometimes called as categorical
data aspect that has one or more
categories but there is no intrinsic
ordering to the categories that you can
see here there is no ordering it doesn't
matter where that male comes before
females come after male or before me it
doesn't matter the ordering here doesn't
matter so these kinds of data is called
as nominal data so if you try to
understand this the same example with
the help of our you will see that it all
depends on what kind of input is entered
by the user let's say for example it's
being prompted to enter input number one
so that is being assigned to answer
number one and again the input is
a sign to answer to we have taken two
variables and we are just putting a
check if you have pleased as yes to the
answers then we can simply say print a
message that welcome to the girls club
other than that then you just print a
message saying sorry boys you can not
join the club so all of these results of
the print statements are executed on the
basis of answer given by you which is
nothing but kind of a simple mentioning
of any of the characteristic or we can
say just as entered by the user
it's very eatable example with illusion
that what exactly is nominal data
when you say ordinal data that means
there is some kind of inherent order
associated with that kind of data so
there is an ordering of the variables
now if you see this example here we're
talking about the customer IDs and a
reading associated with those customers
so let's say we have customer ID number
one which is having ID of 1 0 1 and the
rating given to that perlier so I
strongly agree similarly T not to
disagree and similarly the other neutral
will be and strongly disagree so if you
see there is a inherent ordering is
already in place there and that ordering
is starting with very strongly agree
which is the customer ID or not one till
it goes to the another customer with the
rating skin off strongly disagree so
there is an order because strongly this
is V one will be pretty much in low
priority compared to them strongly agree
one but the neutral one will build it
somewhere in the middle so the value
which is associated with particular
variable or the particular data having
an inherent order it's called as ordinal
data so if you want to take the example
of ordinal data it's pretty simple and
straightforward here if you talk about
the marks so the value of the marks has
some inherent order because the check
that we have placed here is the value of
the marks is less than 40 then that
student is considered as field if it is
between 40 and 50 then it is a second
class if it is between 50 and 60 it's a
first class and if it's more than 60 or
equal to 60 then message is
congratulations you got a distinction
that means the value of marks has an
inherent order because if it is less
than 40 then it has some different value
and it has some different consequences
if it has something else between 1415
has something else and respectively for
other values it has some other
significance so because of the order
precision with the data we call that
will
or Lolita let's go back to quantitative
data so as I already told you
quantitative data is nothing but the
data that we can measure or the data
that has some kind of quantity
associated with that data so the data
that deals with numbers are broadly
classified into quantitative data and
the further can be classified into two
types one is called as discrete data and
the continuous data when I say discrete
data that means the data that you can
count that has a specific defined value
of it that involves integers for example
we just saw about the mask so how much
marks you got is the discrete data how
many runs scored by a batsman is a
discrete quantity that we can measure
take a look at this example we're
talking about the return later to the
mobile phone companies and the mobile
phone company is being represented by
the kuala lumpur organization having
samsung after lochia algae in sunni and
the number the product given by those
organization is being present in the
column called number of products so if
you see Sampson has a definite or
discrete value of five Apple has a
definite and discrete value of 30 so
these kinds of data is called as
discrete data because we have finite
number of values possible with that
particular data or you can say product
you cannot further subdivide it you
cannot further change the unit's or
something else because that's a discrete
quantity we're talking unlike the same
thing when you talk about continuous
data that is also a huge measurement but
that can be further divided or we can
say that can be further Pinal increments
and modifications or you can say further
levels let's see here if we're talking
about the patient in the weight of the
patient then weight of the patient is
represented in the form of eighty six
point five kg but the same weight can be
represented in pounds as well that can
be furthered represented in the
quantities or the grams and to whatever
level so we can further modify or reduce
to any level we want that kind of data
is called as continuous data which can
be measured on a continuum or SK
so why I'm telling you all this because
based on these categories whether you
are talking about the discrete of the
continuous later on you will see we have
a different distributions related to
these kinds of data sets when you talk
about discrete data you will later on
see a binomial distribution is coming
into the picture
Poisson distribution is coming into the
picture
these are different representations of
that data later on with the continual
you see probability density function has
come into the picture so different kinds
of data normal distribution is going to
have different kinds of data projections
or you can say distributions arise from
these kinds of data it's important to at
least understand the meaning behind all
these kinds of different chocolates so
apart from the data now we are talking
about variables and we all know in the
mathematics we have a habit of taking
imperials right if you wanted to solve
any of the equation let's say y equals
to MX plus C then you have yn X as two
evils so it is nothing but it represents
an unknown value or the value that
varies that's called as B the variable
can be further divided into so many
categories
it's like categorical we even control
variable independent variable
confounding variable independently so
when we talk about categorical variable
that means it fits in some kind of a
category and does not have a numerical
value associated with it it is unlike a
normal variable that we normally see in
mathematics right because mathematics we
normally define a variable say for
example eggs and we assign a numerical
value for but unlike that we have a
different category symbol that holds the
kind of normal to it and let's say when
you are talking about the categorical
variable that value can be of English
type or let's say if I'm talking about
values associated with the college
subjects then we can store those values
in variables by lose like English
mathematics and polygon signs that value
is stored in a variable and that
variable is called as indicated then
after that we have another variable
called control a control variable is a
variable which is meant to be kept
constant now why it is meant to be kept
constant let's say for example we're
talking about growth of the plants in a
simulator we can see controlled
environment because most of the
experiments of the research they all
being done in a controlled or a
regulated in part now why we need to do
that is because whatever the figures or
whatever the resource analysis that will
be proposed at the end shouldn't be very
based on some external effects or
external quantities all right so we need
to make some other things constant let's
say for example if you are trying to
measure the growth of the plants in a
simulated environment so let's say you
can keep the amount of sunlight as a
constant or you can see control me
because
if you keep on varying the sunlight then
what kind of analysis whatever analysis
that you're doing on your plants let me
has some of the very ill things
wait and those waivers are called as
control aids then you have independent
variables we reverse which is changed by
the researchers or the person who is
doing all that research or analysis call
an independently for example if you are
investigating how changes in size of
water faucet of an amount of water flow
the poster child size is your
independent variable and you have no
control over the amount of water which
is flowing or let's say for example if
we're trying to measure that's a very
simple example if you're trying to
measure the length of a wave versus the
time in a day so that time in a day is
your independent variable because you
have no control or that right but that
time has very significant effect on the
output of your whole analysis of getting
the relationship between the time and
the height of the waves so the height of
the waves is your dependent variable and
the time that you are talking about is
your independent weight that is how
before the class go into two categories
in Mendel dependent then we have one
more quantity or you can say one more
type of variable called confounding
variable in order to understand
confounding variable let's take an
example let's say we have two hundred
volunteers what are those two hundred
volunteers we have hundred as men and
hundred s : at a lack of exercise
basically leads to wait but we are only
considering one way lack of exercise
what about how much you eat we are not
taking into consideration that or when
you're talking about volunteers we have
hundred women and one hundred men there
is one more saying that men can eat more
than wood
we are not taking into consideration
that organs at that point so nothing was
mentioned about starting weight nothing
was mentioned about the occupation
nothing was mention about the age so
these are called as your confounding
variables because here men can eat more
than a woman so here sex can be
confounding anyway and we are not taking
consideration all the other factors that
may lead to significant changes in the
output of your whole exercise of
analysis so those kinds of variables are
called as and
let's take a look at a simple
experimental research example now here
in this experiment of the search the aim
is to manipulate an independent variable
and then examine the effect that this
change has on dependent behaviors let's
say there are 100 students completing a
math exam we the live Indian variable
was exam marks an independent variable
over reason Tainan intelligence now if
you try to relate the revision time and
IQ with exam score you will see a linear
relationship between these two variables
that means the more the revision time
the better they have exam score the less
revision time the less exam score so
what they have done they have divided
the growth their 100 students and the
group is divided into two one is having
50 another atomically students and then
instructions given to the group number
one is that they have to revise for 20
hours and the group 2 students shouldn't
be allowed to do any in the region and
then if you try to compare the results
of these two since you already know
there is a linear relationship then you
will see how if you varying this
quantity the result is getting affected
giving you a result of analysis that how
the dependent and independent variables
changes the whole scenario another
example there is non experimental
research in norm experiment the
researcher does not manipulate the
independent variable we're not changing
the independent variable this is not to
say that is impossible but it will be
impractical let's say the researcher who
is interested in the effect of the
illegal recreational drug use which is
independent variable for your
information in this case on certain
types of the behavior that means what is
our objective the dependent variable
that is going to affected with the
inclusion or exclusion of the
independent variable it will be an
ethical to ask individuals to pay the
legal drugs in order to study what
effect this had on certain behaviors so
this is kind of a non experimental
research if you can modify cannot modify
the independent variable that has its
own effect on dependent weight now let's
talk about the more technical sense of
or technical evidence and mostly we will
be talking about population and sample
so what exactly is population so
whenever you start with any kind of data
analysis or any kind of analysis that
you have to do on some kind of input
sets then whenever you get the whole
data set for which you have to provide
your analysis let's say we are giving
with say for example last ten years
whether in form
of a particular region and those the
last areas data can be huge data so
whatever input data given to us and we
need to come up with some kind of
analysis that whole dataset is called as
population so normally speaking you
normally won't go and apply the whole
analysis or whole mathematics on the
whole population the idea is to come up
with some kind of a sample out of that
population so let's say if you have 10
million realistic values and take sample
out of it you will start but 10,000
maybe 1,000 but there is again a science
behind how to get that thousand results
on how to get thousand results from the
10000 data set points that God
assembling dig so sample is nothing but
so the end of each use in points or
again city assets from the wholes
operation set so the idea here is take
the sample out of whole population
put your analysis on that sample come up
with the analysis results and try to
represent that analysis result on the
whole population set there is a whole
idea behind sampling and publish so how
to get that particular sample because
that sample shouldn't be biased
shouldn't be blindly chosen sample
because that Blandy choosen sample may
have some of the biased related terms
which may affect the results right so we
need to choose the sample pretty
logically so that is why we have
sampling technique so there are two
types of sampling techniques available
one is called probability sampling
another small probability sampling in
problem will be something it is
basically based on fact that each member
of the population has a known or equal
chances of being selected but I am
saying selected that means for becoming
a part of the sample so for example if
you had a population of 100 people each
people would have odds of 1 out of 100
of being chosen so if you choose 100
people so each and every individual has
an equal and likely chances of getting
choose there is what you call as
probability sample another is
nonprobability sampling with an all
probability something those odds are not
equal for example a person might have a
better chance of being chosen if they
live close to the shore or have access
to a computer so there are some
unforeseen in circumstances that may
actually affect the sampling of the data
big as I told you if you want to choose
a person or a 100 sample so you might
choose your neighbor before
the sample other people because of the
closeness of the factors that is far
less non sampling or nonprobability
sampling probability sampling can be
further divided into three types we
called us random sampling systematic
sampling and stratified sampling random
sampling is very basic simple technique
in this one we select a group of
subjects while study from a very large
group and each individual is choosen
entirely by chance and each member of
the population has an equal and likely
chances of being included in the sample
that's what is a very basic sampling is
this as a first representation of your
probability sampling and called as
random
another sampling is stratified sampling
in the stratified sampling there may be
some factors which divides the
population into sub population like some
kind of a groups or stratum the
meezerman English may vary among the
difference of populations a stratified
sample is operating by taking samples
from each stratum or subgroup of the
population the proportion of each
stratum in the sample should be same as
in the population that's what the logic
behind it is being very frequently used
when you have a population of
heterogeneous results when I say a true
genius that means mixed records are
there and we can say let's see in this
slide where we have divided the whole
population into male subsets in the
female subset so we divide into category
we use our subject and then further we
can start with our random sampling in
the same part of stratified sampling as
well systematic sampling is very very
simple something in this one what we'll
do we'll start with some kind of first
repeating technique so say for example
we always start with T in consideration
every fourth record or every second
record right so it's pretty simple it's
kind of a you can say ordered sampling
we are doing here that's why the names
it is a systematic because we have a
perlier system for that so we need to
take every second or fourth or six
whatever the recording you take and you
have to keep on repeating the same step
again only this one as system at exactly
now let's come back to non-probability
sampling so nonprobability sampling can
say it's not a product of randomized
selection persons there is no
randomization process here here the
subjects are selected on the basis of
their accessibility or by the purposive
personal judgment
the researcher some of the extreme
effector there is no mathematics there
so basically these kinds of sampling we
have a problem of sample representing
and Diet operation is not confirmed so
therefore they cannot be used in
generalization of the entire operation
so each has a technique not that
frequently used in order to generalize
for the whole population we can further
divide nonprobability sampling into four
or five types sometimes using four
different regions and sometimes you see
fine
so we broadly classified into convenient
a consecutive type quota a judgment in
and snowball so what exactly is
convenience not probably something so as
the names of this the samples are taken
based on the convenience of the
researcher or the person who is doing
that analysis that is cordless
convenience sampling then we might have
consecutive as well so whatever this is
a sample that you have available as a
part of the Cutler process you will
include all those samples we call it as
consecutive then we have Kota this one
we need to actually the searcher it
shows that equal or proportionate
representation of the subjects depending
on which trait is considered as basis of
it let's say if we are talking about
college students so the college level
can be considered as your quota so that
we can divide into first year second
year third year things like that that is
called as quota sampling then we have
judgmental sampling in judgmental the
soldier should be having a specific
purpose in mind where based on that
special purpose some of them may be more
deserving than others
we used one let's say whatever the
samples were taking if we're talking
about english-speaking people so the
people who are native English speakers
might be given favor
compared to other right so if so build
on the judgment of the research or the
analyst you can include some of the
samples then we have snowball in this
type of sampling the researcher asks the
initial sub to identify another positive
potential subject who also meets the
criteria so that is called as snowball
sampling after that we take another step
in statistics we will talk about
descriptive statistics so when you talk
about descriptive statistics and
descriptive statistics we try to
represent the data in the form of graphs
or maybe some kind of different
graphical formats
maybe so grams maybe lying Lourdes
Landgraf scatterplots right and based on
that representation we'll come up with
and that put the data being represent
based on some kind of central tendency
okay means that put the graph might be
representing distribution of the mean
might be representing distribution of
the mazes of the spread right so it all
depends on what kind of data what kind
of analysis you are dealing with so in
order to do that we have through the
mazes of the center or against a central
tendency that means are nothing but the
mean median and mode so when you talk
about mean mean will give you basic
central tendency about the whole group
let's say if I am talking about the
average marks is cold by each and every
student in school is ninety percent then
that particular data will give us a
meaning or you can say an information
about a whole school that on an average
the each individual student of the
school scores around ninety percent so
it gives us a strength or tendency that
the school performs normally well so
like that we have mean we have median we
have both data that can use in different
kinds of analysis in order to get the
central tendency of the whole population
okay so if we try to understand the
meaning of all those terms let's take a
look at this example here we have a
sample data sets of empty cars
containing the variables called cars
miles per gallon cylinder displacement
cause power and lift like that
okay now one by one try to come up with
these measures of the centers and let's
say for example we need to find out what
is the average of the horsepower how to
find average of horsepower how to find
common type of cylinder or find center
value among miles per gallon increase so
let's say start with an analysis number
one here we want to find out the most
common type of cylinder so when you see
most common type of cylinder and you see
the value of the most common type of
cylinder divided into two types like
four and six and if you try to calculate
the mean man which is nothing but taking
the average of those old records so
coming back to the first analysis we're
talking about the most common data
centers we got to two values like four
and six so if you see the frequency of
six is more
so you can say we can put it there to be
most common terms under this six head if
you want to find out the average
horsepower of the cars and with the
population of the cars now in this case
we calculate the average or against a
mean of the all the values of the hot
spot
so we'll get the average value of the
hot spot in the whole circuit then let's
say we need to find out the center value
of the miles per gallon on the
population of the cars how to get the
central value we can give this interval
you with the help of median of the whole
data set which is nothing but we'll try
to pay the two if the number of the
entities are even then you need to
consider two entries if resort and make
it pretty the middle back okay this is
what they don't care now in order to
represent the same thing in our the
skeleton please let's take a quick
example with the help of our suite if
you try to see
we just have some pearl for me
see we can do that we take this sample
right we cut the sample we if you want
to cleave the mean this temple add
function is available so you can
directly create the mean like that if
you want to change over I mean you can
simply see the value of millions
okay you can see the mean is something
are in Bernal so that is how you can
actually calculate me that is how we can
calculate the median there is a little
bit the mode we have different functions
certain for that weight if you try to
see for median mode you can see so mean
you already know how to create median
also direct action available in order to
get the mode value which is nothing but
the most frequent values present of the
whole data points we can define our own
function in our with the help of like
this get memos name of the function and
we can get the unique sort of values we
can get a maximum of for the principal
values and then the result will give us
the mode of the whole data scale and we
can actually represent the data set with
the help of histogram so I will do that
by in our you just have to use the
function called hist and pass on the
parameters so that's how we can
calculate the median in our so that
function will be so if you go by that
mathematical definition you can see it's
nothing but take the two values middle
values and get the if it is odd then you
can get the direct value for this even
you can ever take two values anywhere
but generally mean to either some of the
all the values you had put some values
and more is more is nothing but the
number the figure highly or you can say
the most frequent of that's going to
come as more of the voltage six like
measure to the center we have another
thing called measure of the spread right
how can you measure the spread of the
data
so there are four terminologies used
there one is called as range one is call
it the quartile range others variants
another a standard deviation so range is
the name suggests it is giving you some
kind of or we can say a whole sort of
values between two points like maths and
minimum so if we take the difference
between Max and minimum we ideally will
get the value of the whole range and
that is called as range of values so in
some of the term or in some of the
libraries you will see there are dead
functions available in order to get the
value x1 value of greatest points at the
same time to get the minimum and once
you tell you the difference between a
maximum value in the minimal value you
will get a full spread of values and as
range
similar to the range we have quartiles
as well so instead of getting difference
between Max and minimum we will try to
divide the whole data set into some
guards of quarters and those are called
squat toilets if you see the slide we
have divided the set of ten points into
three quarters okay that is how we can
divide the whole of quarters if you see
this example we are again dividing two
different quarters because we have
hundred values the quarters will be
lying between two different 36:58 could
be worst and 75 and 76 that's how we can
calculate the chordates now there is a
forgotten fall into quartile range so
range we already know the difference
between Max and min in the Wardle range
as the name suggests it's the distance
between the quartile east side so say
for example if you wanted to have a what
I'll interquartile range between quarter
three and one so we'll probably take
quite a three value - quadrant one that
will give you a book interquartile range
apart from interquartile range there are
two more important terms one is called
as deviation another is called as
variance so both of these terminologies
revolves around the calculation of me so
basically they tell us how much each
data is varying from me and we called as
variance and difference between each
value mean is called as deviation and
different Oregon's say the deviations
and India's computing square of
deviations called as variance so
basically how does being calculated
since we need to calculate the variance
of the whole data set or the whole
population so first of all we need to
calculate the difference between the set
point and the mean value and then one by
one need to add for all the data sets
points available and then we need to
square it so that we cannot cancel out
the difference which may be a sign
results
right this way we square n the mean to
the fourth example divided by n minus
one in the key so of sample variance why
a sample values n minus one because
later on you will see in order for the
classes there is term called degree of
freedom and that degree of freedom we
need to subtract from the number of
samples to take into consideration and
again say the units of error against a
traditional theory will be that mean
missed if you take all the excess under
the population measures is nothing but a
set of population in the population mean
and the same formula as it is for the
sample variance so as I told you
is a means of the spread between numbers
in a dataset it means us how far each
number in the descent is from me
similarly for deviation is a measure or
a dispersion of a set of data from its
mean so basically variance is an
individual level and the deviation is
the whole point if the dataset points
are far from a mean they're pretty much
far from the mean that the deviation is
very high let's take a look at this
example
you know when the stand standard
deviation so say for example Sam has a
20 rose pushes the remove the flowers on
each bush is 9 to 5 for 12 7 8 and 11
and few other values now we need to
calculate the standard deviation so
first of all in order to as I told you
that centered deviation and sample
variance revolves around the mean values
so first of all we need to calculate the
mean value so here in this case the mean
value is 7 then we need to calculate the
difference of each point with the mean
value square it and then sum at all so
that's what we are doing in this stabia
squaring agent individual value last we
are summing it all and then we are
dividing the N minus 1 sample or total
sample and then you will get a result
that sum means the square difference is
8.9 and I'll get the standard deviation
you will get as to 0.98 3 is a standard
deviation of the whole data set in order
written the same thing in R we already
saw we take the sample values and then
we greater Instagram holders and s so
let quickly you run this example
get the histogram this is something like
this histogram you will get so as the
initial start is to come then we can put
the mean value in order to print that
mean value will print the mean variable
will give a mean value when the medium
value and then we define the function
forgetting a motive and you okay and
then we store the value in the result
function result very well you open that
little the mode will get the more values
like this right so that's how we can
actually do the same stuff of separating
mean median mode in our okay and the
same time we can draw the graph as well
so in order to tell you the range as I
told you where most of the languages
supports the calculation of maximum
value and some of the eBay's so those
epa's are Max and min here in this case
so max data - min did I will give you H
and if you wanted to calculate the
variance of that value which is called a
bear function will give you the variance
of the whole data support alright so
another important term in this versus
information gain and entropy so before
we go into the details of for
information being an entropy first of
all need to understand what is mean by
purity or what is mean by interfere from
the scientific background you might have
heard this term philanthropy in case of
physics where entropy is a measure of
uncertainty in the data so what this
physics term is doing here so this can
be explained very beautifully with the
help of an example so let me explain you
that example and then we will come to
the formal definition of these terms
let's see I have used case where a match
is going to be played on specific days
the task is to forecast weather the
match will be played or not according to
the weather conditions so we need to
make a prediction and for that we have a
data set available with us which is
having like putting values for each day
and that has few of the parameters like
Outlook which may be sunny overcast way
and we have humidity which may be high
normal and but we may have been like
strong mean we win and based on all
these factors whether we can play on so
if you take a look at this one
there are total nine yes and five knows
that means there are nine outputs that
suggests that the play will happen
and there are five known that such as a
plane won't happen so based on a
different properties or the outlook we
divided the head into three further
divisions right like sunny over castle
rain now you must be wondering why did I
choose only Outlook right so let me
answer that question shortly let's first
try to understand what exactly is
information here and Rafi I'm talking
about and then gradually you will
understand where we can use in drop the
information game and why we have chosen
only Outlook as a and there's a feature
to be take it not humidity impact let's
imagine be somehow good burn to know but
to know that outfit has to be take so
you take outlook we got three different
results it may be how to look maybe
sunny it may be overcast and rain so we
talked about how to took a sunny you
will get again three different results
or against a five different deserts
where two of them are suggesting that
pave can happen and fear suggesting that
click may not happen then you could go
with overcast all other intercept or all
the entries suggesting that play will
happen if you go over the reins and
there are three additional play will
happen and to the happen now if you see
if you go into detail this is an example
taken from the decision field on right
so if you see initially we started with
something of a mixed data we don't have
current information available with us
based on that we can see the play will
happen because there is a mix data
because nine years we started that kind
of data we told us that has impurity
because there is no final resolution he
can make on the basis of that well those
values like spying yes knows similarly
if you take sunny data there is the same
problem with this one because there are
two years and three nodes similarly if
you take the rain the same problem is
there as well three years into but if
you take the overcast the overcast is
completely period when I say completely
pure data that means there is no chances
of confusion here and you can gladly go
and say okay okay that kind of data is
called as your data now if you think how
can you calculate the intro of your a
disturbance that is a very simple
formula where we try to calculate the
probability of getting those values so
the probability of getting yes and
probability of winning no we divide with
all those karate
simple mathematic formula is there and
once you get into it we give you a total
entropy for that combination which is
0.94 which is very high so please don
all this calculation so if you go back
to the formula if you see the same form
that we have up right there so as I told
you it is a measure of uncertainty and
in fact based on the value of entropy
you can calculate the information gained
now if you see in this example we have
started with entropy of point nine four
we have just calculated so here we have
you can say and just represent interview
with E so here we have entropy of 0.9
for right okay and if you see here we
have pure data set so we have represent
information gained here so how can
hinder person information gain which is
nothing but ID some value of entropy and
- something value of it properly here we
don't have any interview value because
the number of knows a zero here all
right so we'll probably get zero that
but here if you see if you calculate
entropy here you will get some value of
entropy a and then differentiating that
initial value of entropy with this value
of entropy you will get some differences
that will give you the information
gained back rather usually just now why
I told you all this about information
gained on this one so I believe you
always want hundred person information
gained we don't want to lose our data or
the entropy values so we need to choose
a better one that will give up under
person information this we should not
lose a little Daisy procedures so out of
all the combination like sunny outlook
windy and humidity and other things
right why we're choosing Outlook and
that too with all these values right so
if you will start with calculating the
information gained in each and every
case so information here in case of
windy you can see here so in Krabi for
the initial set we all know which is
nine years and now five knowledge is
common to all of the combinations will
get that entropy of initial system is
0.94 and then we calculate the entropy
for the other combination that we are
taking right so you see the difference
information gain is point zero four
which is very very less right so you
want the more information game you go to
them in another formation and here you
will see 0.94 - so other combination
we have taken for Outlook right and you
will see the hair in here the difference
of or you can say the information in her
getting his point to 4 which is speaker
compared to point 0 4 8 that means in an
Outlook if we consider Outlook as a
parameter we're getting better
information came compared to the Wendy
similarly in case of humidity in case of
humidity the calculation of information
as to the formula gives us point zero
one five point zero point one five so
that is again better than your Wendy but
is still less than your Auto so Outlook
is the one similarly we take temperature
as well but still has a very low value
so Outlook is the one which gives us
very good information that is why we
have chosen Outlook as a major to start
with the decision tree right because
we're getting maximum information game
now let's come back to the last topic
called confusion matrix Oh confusion
matrix is again very straightforward
method to calculate the performance of
an algorithm and basically that is
classification because I might use
pretty heavy and it falls on the concept
of true and true positive to negative
false positive and false negatives
let's try to understand confusion matrix
is the simple example now here what we
are doing we are having in function
about 165 cases of patients or of 165
one not five have the disease and 60
patient will not have this each and the
classifier predicted one 110 times as
yes and no as a 5-time so now we have to
come up with the measure of the
performance of this algorithm or the
model that we have model that you have
fit for that so we write and create a
matrix of yes true positive through
negative false positive and false to get
that means where it is predicted no and
there is actual know how many cases
anyway I have done that which is 50 when
there is predicted yes but there is
actual not that means ten times when
there is an actually yes and we pretend
no we made it like five times and there
is an actual yes and we printed
yesterday so we create a matrix
something like matrix structure and we
call those matrix values with period and
expected values based on our prediction
is correct or wrong so we talk about the
true positive that means your cases
where we have predicted years and it was
actually yes that means we have
predicted that there yes they have
disease and they actually have disease
to negative and being predicted as they
don't have disease and which is true and
they actually don't have a C's when is a
false positive when we predicted that
yes they have disease but they actually
don't have disease that is an error or
mistake we did and we have forcing it
evaporated no actually they do have
disease right so this another way around
we again made another that so because of
all those calculation values or two
false positive false together we come up
with the two terms called sensitivity
and specificity if you see these are the
terms that we talked about right to
negative is nothing but it didn't know
an actual true positive is portrayed as
an actually is like that we divided into
some kind of matrix and then you
calculate the sensitivity is nothing but
probability of detection of some fields
that the measure of the positive right
which is nothing but true positive
divided by the combination of two
positive and false negative okay and
that means it gives you a fraction of
all those with the disease who get a
positive result
similarly specificity gives information
about ruining eaters right which is
nothing but 2 negative 2 negative at the
false you get a false positive and you
get a some value of that give you a
fraction of those without the C so
basically while doing all this we get
some of the additional problem because
if you go with the other equation users
we just kept it there and we see how
many of them is politically correct and
how many of them operated wrong instead
of getting those increasing users with
confusion matrix you will get an
additional benefit of seeing the
behavior of your model how many of them
imprinted 30 or how many do brittle
wrong how many of them are usually third
but you got a brittle
how many of them original wrong but you
will never get away like that with the
help of these terminologies you can
drill down to the further left so
somebody is that in this whole session
we talked about it
types so this mirror for summarizing so
Gandhi is his brother it is divided into
two types one is called as quantitative
this qualitative and then quality it is
further divided into normal inaudible
type quantitative further divided to
discrete and continuous then we have a
population sample population is the
whole data set if you talk about sample
is a sum of the values taken from that
and that to be is also the technique so
we talked about the population
probability sampling technique and
non-probability sampling technique
probability sampling
can be further divided into random
sampling stratified selfie and
systematic sampling nonprobability can
further divide it to convenience
sampling judgmental sampling snowball
sampling right and there is one more and
then we have a representation of the
numerical data which is nothing but the
measure of the center a measure of the
spread and them in the end very
important topics of informational game I
hope we've got information related to
that thanks a lot for your time take
care see you next time bye bye
I hope you have enjoyed listening to
this video please be kind enough to like
it and you can comment any of your
doubts and queries and we will reply
them at the earliest do look out for
more videos in our playlist and
subscribe to any Rekha channel to learn
more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>