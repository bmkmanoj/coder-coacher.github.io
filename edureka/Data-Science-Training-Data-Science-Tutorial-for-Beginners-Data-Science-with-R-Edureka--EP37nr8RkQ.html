<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Science Training | Data Science Tutorial for Beginners | Data Science with R | Edureka | Coder Coacher - Coaching Coders</title><meta content="Data Science Training | Data Science Tutorial for Beginners | Data Science with R | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Science Training | Data Science Tutorial for Beginners | Data Science with R | Edureka</b></h2><h5 class="post__date">2018-04-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-EP37nr8RkQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in a world where 2.5 quintillion bytes
of data is produced every day a
professional who can organize this
humungous data to provide business
solution is indeed a hero now this hero
is none other than the data scientist
the mix of personality traits experience
and analytical skills required for the
data scientist role is considered
difficult to find and thus the demand
for qualified data scientist has
exceeded supply in recent years now data
scientist is a four hundred thirty
billion dollar industry and it is
considered one of the sexiest job titles
of the 21st century now radio scientists
topped the list of 50 best jobs in
America by Glassdoor in 2016 and 2017
based on metrics such as of satisfaction
number of job openings and the median
base salary and I'm sure it will be on
the top for 2018 as well so guys this is
Krishna from ad Rekha and today we'll be
discussing about data science now let's
have a quick look at the agenda of
today's training so I'll start off by
talking about the various paths one
takes to become a data scientist next
I'll discuss briefly the data science
training offered here at EDA Rekha and
moving forward will learn about data
science its peripherals and the various
use cases we'll have a look at few of
the projects offered here at at Eureka
and finally I'll finish off this video
by showing you a few demos on the data
science now with experts predicting that
40 Zita bytes of data will be in
existence by 2020 the other science
credit opportunity will shoot through
the roof there's no doubt in that now
there are various ways of becoming a
data scientist one could be coming from
a mathematical background some could
come from economics some could come from
computer science as well as information
technology and there are various
programming languages one can choose to
start this path like Python our Hadoop
spark and many more now all of this
might seem very confusing and hard to
work with so let me do the analysis for
you and make things easier so according
to Google the top languages as you can
see here from the graph are Python and R
while Hadoop and spark are relatively
low in numbers they are used a lot to
solve many big data problems
now there is a particular way of study
required to learn and master those
techniques to become a successful data
scientist now a dareka as we speak
provides a very detailed and
comprehensive training on data science
it's available in both r and python
format
whichever you find yourself comfortable
while coding and creating more lists to
train and test your data so the training
is divided into ten modules now upon
completion of each of the module you
will also gain expertise in the areas of
technology and will also gain knowledge
of the topics along with some real time
implementation of these topics and many
practicals to work with now data science
training encloses a conceptual
understanding of statistics time series
text mining and an introduction to deep
learning as well throughout this data
science course you will implement
real-life use cases on media healthcare
social media aviation and HR so let's
have a look at the training structure
here offered in the data science program
so module one is introduction to data
science in this module you will get a
brief introduction to data science and
we'll see how data science helps to
analyze large and unstructured data with
different tools we'll be starting with
the basics of data science what it
involves the life cycle introduction to
our Python Hadoop and spark now coming
to module 2 which is statistical
inference in this model you will learn
about the different statistical
techniques and the terminologies used in
data analysis like measures of center
measure of spread probability normal
distribution binary distribution and
much more now coming to module 3 which
is data extraction wrangling and
exploration in this module we'll discuss
the different sources available to
extract data arrays the data in
structure format analyze the data and
represent the data in graphical format
some of the topics include data analysis
pipeline data extraction the types of
data which are raw and processed data
now coming to the fourth module which is
introduction to machine learning you can
get an introduction to machine learning
as a part of this module you will
discuss the various categories of
machine learning and implement
supervised learning algorithms in this
module we'll learn about the various
machine learning use cases and the
machine learning process flow coming to
module 5 which is classification
techniques
in this model you should learn the
supervised learning techniques and the
implementation of various techniques
such as decision tree random forest
classifier and much more
we have algorithm for decision tree we
have a perfect decision tree confusion
matrix we have random forests and knave
paths now coming to the sixth module
which is unsupervised learning here we
learn about unsupervised learning and
the various types of clustering that can
be used to analyze the data some of
which are the k-means clustering the c
means clustering
canopy clustering and hierarchical
clustering now coming to module 7 which
is the recommendation engines in this
module you should learn about
Association rules and the different
types of recommender engines which are
the user base recommendation and the
item based recommendation the module
eight is text mining we will discuss
unsupervised machine learning here and
the implementation of different
organisms for example the tf-idf and the
cosine similarity now coming to module 9
which is the time series in this model
you should learn about the time series
data different components of time series
data we learn our time series modeling
and exponential smoothing models and few
models such as ARIMA and time series
forecasting now coming on to our final
module which is the deep learning module
you will get an introduction to the
concepts of reinforcement learning and
deep learning in this module these
concepts are explained with the help of
use cases and projects you will get to
discuss artificial neural network and
few artificial neural network
terminologies now to know more about the
training you can visit Erica's website
just go to the website and search for
data science certification training
now on this page here you can find the
details of the training like the topics
covered in this training the various
projects and also you can find the batch
timings so when you enroll for this
training you will have access to the LMS
which is known as the learning
management system
now here you will find all the training
related content like the presentations
the case studies the projects the data
sets and the class recordings in case
you missed any class and here you also
have a personal diary section where you
can upload any document or any data set
you find relevant for your training for
your own purposes
now that we have seen the whole
curriculum of data science training here
at adhirata
let's go ahead understand exactly what
data science is and what are its
features now data science also known as
the era driven science is an
interdisciplinary field about scientific
methods processes and systems to extract
knowledge or insights from various data
forms either structured or unstructured
it is the study of where information
comes from what it represents and how it
can be turned into a valuable resource
in the creation of business and IT
strategies data science employs many
techniques and theories from fees like
mathematics statistics information
science and computer science
now redesigns can be applied to small
data sets also yet most people think
that data science is when you are
dealing with big data or just large
amount of data now let's have a look at
some of the peripherals of data science
we have to know about statistics there
are some programming languages like ah
Python and SAS we have some software you
also need to know about machine learning
and finally we need to know about Big
Data as well now analyzing data has a
long history it's not something new they
have been many terms that have been used
to describe such endeavors like
statistics artificial intelligence
business intelligence and data analytics
now additionally the process of data
analysis used to start with the
collection of data and creating
structure repository in the data
warehouse and using the business
intelligence tools to find out the
predetermined outputs this used to hurt
the data analysis and businesses because
the data was isolated the storage was
very expensive and there was slow
processing which in turn led to short
and irrelevant insights and also many
human resources were involved now the
major part of their role was to prepare
and cleaning the data as you can see
here it took a whooping 65 percent of
the time in data preparation and
cleaning while only a small amount of
time was devoted to an
strategizing modeling and tration and
making decision on top of that every app
and website started to store different
types of data in different formats here
we are talking about the outburst of the
term big data now the spectator was
unstructured coming at real time in
various formats and the traditional BI
tools just when designed to handle all
of these and therefore they fail due to
the emergence of big data then began the
era of data science for solving the
challenges companies like IBM Microsoft
and Oracle started to build their own
frameworks this gave a new way for being
insights and business solution
automation of several tasks related to
analysis such as pattern recognition and
prediction are not possible now with the
introduction of data science
things got a little easier and different
now now the data preparation and
cleaning only took 10% of the time a
loss of time could be devoted to
modeling and iteration some
decision-making and also a good
percentage of time was spent on analysis
and insects which is very important now
there are many real-life use cases where
data science plays an important role
let's have a look at few of those now
starting off with recommender engines
today many companies use big data to
make super relevant recommendations and
growth revenue among a variety of
recommendation algorithms data
scientists need to choose the best one
according to a business limitation and
requirement collaborative filtering is
one of the most commonly used
recommendation algorithm when we want to
recommend something to a user the most
logical thing to do is to find people
with similar interest analyze their
behavior and recommend our user the same
items or we can look at the item similar
to the ones which the user bought
earlier and recommend products which are
likely to them now these are two basic
approaches in collaborative filtering
which are the user based collaborative
filtering and item based collaborating
filtering in both the cases this
recommendation engine has two steps find
out how many users or items in the
database are similar to the given user
or item and access are the items to
predict what grade you would give to the
user of this product given the total
weight of the items that are most
similar to this one now we'll learn more
about
collaborative filtering in the upcoming
videos so let's move ahead with object
detection our object detection is a
computer technology related to computer
vision and image processing that deals
with detecting instances of semantic
objects of a certain class such as
humans buildings or cars in digital
images and videos every object class has
its own special feature that helps in
classifying the class in recent years
classification models have surpassed
human performance and it has been
considered practically solved while they
have plenty of challenges to image
classification there are plenty of
relatives on how it's usually solved and
which are the remaining challenges now
object detection is one of the areas of
computer vision that is maturing very
rapidly
thanks to deep learning every year new
algorithms and models keep on
outperforming the previous ones in fact
one of the latest state-of-the-art
software system for object detection was
just released last week by Facebook the
software is called detect Ron that
incorporates numerous research projects
for object detection and is powered by
the cafe to deep learning framework
nowadays Royce technology is everywhere
and we are undoubtedly in an era
dominated by voice assistance part of
wise assistance are
speech recognition and one part is
searching and acting according to the
command that was asked now data science
is assisting the speech and talk
application by recognizing the voice
messages effectively given by the user
and produce accurate text output in the
response
this technique is widely used by most of
the popular tech giants like Google
Microsoft Apple and so many more in
their products to detect the input voice
waves and then convert it into text
messages to make the digital
communication easier and faster than a
traditional typing method the most
reliable technique used to make an exact
an accurate speech designation result is
deep learning
it made speech recognition methods
accurate and reliable enough to be
applied to the outside environment in a
controlled manner now you might have
heard about many voice assistants like
the Google assistant APIs 3d
you have Microsoft's Cortana Samsung
spic V and I'm sure you might have also
heard about the new artificial
intelligence voice assistant Alexa which
is developed by Amazon now coming on to
our final use case which is the
sentimental analysis
soon as opinion mining it refers to the
use of natural language processing text
analysis and conceptual linguistics to
identify and extract subjective
information in source materials using
sentimental analysis techniques
companies can respond to negative or
positive brand perception when a company
releases a new product monitoring and
analyzing social media content can play
a large role in quickly remediating bugs
and error now generally speaking
sentiment and analysis aims to determine
the attitude of the speaker writer or
the other subject with respect to some
topic or the overall contextual polarity
or the emotional reaction to a document
the attitude may be adjustment or
evaluation effective state or the intent
of an emotional communication so let's
go ahead and look at the various phases
of a data science lifecycle now starting
off with phase one which is the data
acquisition now before you begin the
project it is important to understand
the various specification requirements
priorities and the required budget you
must possess the ability to ask the
right questions here you ask us if you
have the required resources present in
terms of people technology time and data
to support the project in this phase you
will also need to frame the business
problem and formulate the initial
hypothesis to the test now coming on to
the second phase which is data
preparation in this phase you will
require analytical sandbox in which you
can perform analytics for the entire
duration of the project you need to
explore pre process and condition data
prior to modeling further you will
perform the ETL T tasks which are the
extract transform load and transform to
get the data into the sandbox now you
can use R or Python for data cleansing
transformation and visualization this
will help you spot the outliners and
establish a relationship between the
variables once you have cleaned and
prepared the data it's time to do the
exploratory analytics on it let's see
how you can achieve that now here comes
the third phase which is hypothesis and
modeling here you will determine the
methods and techniques to draw the
relationship between variables now and
these relationships will set the base
for the algorithms which you will
implement in the next phase you will
apply exploratory data analytics using
various statistical formulas and
visualization tools
although many tools are present in the
market but I is one of the most commonly
used tool now that you have got insights
into the nature of your data and have
applied the algorithms to be used now we
can apply the algorithm and build up a
mod you will develop data says for
training and testing purposes you will
consider whether your existing tools
will suffice for running the models or
it will need a more robust environment
like fast and parallel processing you
will analyze various techniques like
classification Association and
clustering to build a model now coming
to the fourth phase which is evaluation
and interpretation after all this will
evaluate and interpret the output
received in this phase you will deliver
final projects reports briefing quotes
and technical documents in addition
sometimes a pilot project is also
implemented in real time production
environment this will provide you with a
clear picture of the performance and
other related constraints on a small
scale before deployment
now next comes the deployment of the
file module which we have prepared now
this can be in any language like our
Python or even Java as a matter of fact
now the final phase which is the
operation and optimization it involves
developing a plan for monitoring and
maintaining the data size project in the
long run performance downgrade is
monitored in this phase and the model is
retrained whenever a new dataset is
added or there is a downgrade in
performance
now let's have a look at some of the
projects being used in the data science
training here at a Drakkar the first one
is movies collection it is of the
entertainment industry and the goal of
this project is to explore the movie
data sets given the parameters like
duration movie title cross collection
the budget the title year and many more
you need to explore and analyze the data
sets and find out some insights like you
should know the top 10 movies with the
highest profits knowing the top rated
movies in the list and average IMDB
score you need to plot a graphical
representation to show the number of
movies released each year and grouped
the movies into cluster based on
Facebook Likes
coming on to a second project which is
recommendation system for grocery store
now it is of the food retail industry
and this project scenario is to create
recommendations for customers of a
grocery store based upon historical
transaction data which could recommend
preferable articles coming onto a third
project which is the Twitter analysis
project it is of the social media
analytics industry and this project
focuses on social media analytics the
problem can be defined as measuring
analyzing and interpreting interactions
and associations between people topics
and ideas the data set to be analyzed is
captured by live critter streaming and
you have to perform sentimental analysis
on the tweets obtained and visualized
the conclusion one of the situations
could be comparing two football clubs
based on the trees that they are
receiving from their fans next we have
the air passenger forecasting it is of
the aviation industry and this project
is about analyzing the data and applying
time series model to forecast the number
of bookings and a landform can expect
each month the data set we will analyze
contains monthly totals of international
airline passengers between 1949 to 1960
and you have to make informed decision
on staffing hospitality and the pricing
of the tickets now that we have seen the
various phases of the life cycle of data
science let's understand it with the
help of a use case so as you can see
here John wants to put a baseline
pricing for his real estate company and
needs our help so now to help John let's
see what data we
collect from different location and how
it affects the pricing of an apartment
this is the data acquisition phase now
data acquisition involves acquiring data
from all the identified internal as well
as external sources that can help answer
the business question this data could be
logs from web server social media data
census data or the data stream from
online sources we are api's now as you
can see here that the data we have
collected is not clean there are some
errors which needed to be cleansed also
we may need to change the values of the
columns as per requirements now data
wrangling is the process of cleaning and
unifying messy and complex data sets
data after reformatting can be converted
to json csv or any other format that it
makes easy to load into one of the data
science tools now based on the
requirements a model is creating using
the data set it involves forming and
testing hypotheses about the data and
the processes that generated it requires
writing running and refining the
programs to analyze and derive
meaningful business insights from the
data mostly written in languages like
Python R and SPARC now this model is
evaluated using test data set if the
accuracy is low the above steps are
repeated until a good model is found
model performance should be measured and
compared using validation and test
datasets now model should have a high
accuracy for implementation as well the
data scientist might have done this in
Python or SPARC but if the production
environment supports only Java then he
needs to record it in this step the
model was created and is deployed into
the market and models generally have to
be recorded before deployment when after
the model is retrained we evaluate the
model and deploy it it involves
developing a plan for monitoring and
maintaining the data science project in
the long run the performance downgrade
it is monitored in this phase and the
model is retrained whenever a new data
set is added or there is a downgrade in
performance
so let's have a look at the data set
first so as you can see in the data set
we have 14 number of columns we have per
capita crime rate we have proportion of
the residential lands owned we have the
average number of rooms we have nitric
oxide concentration full value property
tax and finally we have minion value of
the owner occupied homes in thousands of
dollar and many more so I'll be
executing this practical in the our
studio but you are free to use any of
the format like spark Python or R so let
me load these libraries so the libraries
which we are going to use out the
forecast library the cat tools the mass
the core plot and the metrics library so
let me load these library now that we
have loaded the library let me load the
data into a data frame a
now as you can see here on the right
hand side in the values section global
environment we have Boston which is the
data set and a which is the data frame
containing the Boston data set so now we
will find the correlation in the data
set we'll see which variables are more
interrelated as to get a better
understanding of the situation which we
iearn so let me plot the correlation for
you we'll use the core plot here
now here we can see that the final price
this is MATV has a higher correlation
with RM which is the number of rooms
which makes sense as more the number of
rooms more will be the price of an
apartment and similarly you can see that
ma TV has no relationship with the PTR
ratio and it has very less relationship
with the NO x which is the nitric oxide
so let us V our Boston dataset so you
can see here we have all the rows which
which are the five and the six entries
and we have the forty number of columns
here now to perform any kind of analysis
we need the training and the test
dataset so let us split this dataset
into training and test dataset in the
ratio of 70s to 30 so here I'm going to
use the function sample dot split so as
you can see here we have split the data
set into 70s to 30 ratio and we'll
assign the 70% of the area set to the
training part and the rest 30 to the
testing part now LM is used to fit
linear models it can be used to carry
out regressions single stratum analysis
of variance and analysis of the
covariance so now we need to create a
linear model between the variable MATV
and rest of the data set now it can be
done in two methods as you can see here
I've commented out the first part which
is MATV
and then I have added all the remaining
variables with the plus signs and the
data set match I'm choosing to create
this linear model is the training data
set otherwise you can do this MATV and
we can use the till sign and the dot to
represent all the other variables other
than the MATV
this is much more simpler and easier to
use so now that model is created let's
have a look at the summary of this model
so as you can see here we have the
residuals which have the minimum value
the first quarter value the median value
the third quarter valley and the maximum
value so you can see the maximum value
is twenty six point seven eight and the
minimum value is minus twelve point zero
eight now looking at the other
coefficients in the intercept we have
all the crime rates the zone and all the
other remaining variables then we have
the estimate then the standard error the
T value and the probability at the
bottom part we have the residual
standard error the degrees of freedom
the multiple r-squared value adjusted
r-square value the F statistics and much
more now these are all statuses terms
and you must know each of these terms
before doing analysis now we will use
this model which we have created to
predict the prices in the testing data
set so here we will use the function
predict
as you can see here now we have the
predicted output now to get a better
understanding of how our model is let's
plot the prices in the test data set and
also plot the predicted output according
to our mode
so here you can see that the green line
and the blue lines are coinciding a lot
and not days not much deviation between
the two this means that our model is
very much accurate so as you can see
here there's not a lot of deviation
between these two lines
it suggests that our model is very much
accurate now we can also improve this by
creating another model or by changing
the number of factors which are included
in the analysis so guys this is it
similarly you can also play around with
the data set get to know them clean the
data set and perform analysis now it's
important to know that not all the
analysis you do will help you some might
not make any sense at all and some might
not have any logical explanation but as
data scientists one should always look
for answers to describe the analysis so
guys I hope this video was informative
and has helped you to get started with
data science thank you I hope you have
enjoyed listening to this video please
be kind enough to like it and you can
comment any of your doubts and queries
and we will reply them at the earliest
do look out for more videos in our
playlist and subscribe to any rekha
channel to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>