<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simon Belak - Doing data science with Clojure: the ugly, the sad, the joyful - Curry On | Coder Coacher - Coaching Coders</title><meta content="Simon Belak - Doing data science with Clojure: the ugly, the sad, the joyful - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simon Belak - Doing data science with Clojure: the ugly, the sad, the joyful - Curry On</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6tZQqwil1c0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh I think we should probably get
started so I don't know if you guys have
been in the other chest time or talk I'm
just going to explain how it works real
quick and then give it to Simon but the
way that this this this whole chest
timer format is supposed to work it's
supposed to put a little bit of pressure
on the audience to interact with the
speaker so he is limited to only twenty
minutes of speaking time and you guys
have to speak the other twenty minutes
of this slot so if you guys are just
sitting here and sleeping through his
presentation you're not doing the chest
timer thing correctly you're a part of
this okay so it's up to you to you know
like focus and try to come up with some
good questions to ask him he might be
sneaking stuff in where he leaves a big
hole and you guys have to okay well hey
you didn't explain that thing can you go
into a little bit more detail about that
that's a valid thing to to raise your
hand and ask the only rule is that you
can't ask him questions for the first
five minutes because he's got a start
somehow he can't just drill him the
first 30 seconds so um
wait five minutes before you ask
questions the chest timer is right here
it's this little black thing I'm not
sure how many people can actually see it
maybe Torsten has to like wave it around
every now and then but there's one thing
to note about this chest timer speaker
and audience have only 20 minutes the
chest armor unfortunately only operates
in 25-minute chunks so if you look at
the time subtract five minutes I'm
really sorry about this so do a little
bit of arithmetic when you look at the
chest timer but that's that's it so
please please please help them out ask
questions get involved and you know
don't don't really be afraid if you
leave something and like unresolved or
not clear maybe that's like deliberate
maybe you're supposed to raise your hand
and ask so so be on the offensive and
ask questions but I'm going to let Simon
start so second chest number the day
thank you
I said hi I'm Simon I lead the data
department data science department and
go opti so very briefly what we do we
are on demand share transport service
you can think of a sort of like uber
pool except we focus on airport
transfers to on longer distances so we
try to pack as many people into a van
and then send that went 200 kilometers
out to some Airport so basically the
core of what we do is very intimately
tied with working with data the three
core things my department does is we do
routing we do packing meaning how can we
pack as many people as possible into one
van and we do demand projections or
rather risk management so those things
are in various ways packed with data and
this talk and and before that I worked
as a consultant doing data science
helping companies become this driven so
this talk is going to be sort of a
distillation of all the lessons learned
mostly the hard way by doing stupid
things and I'm try to empower mom to you
so just a very very brief overview
because if I'm doing this right we won't
be we only have time to cover all the
things so we have a rough idea of what
still in store because at some point you
need to ask where you want me to drill
deeper into something or you want me to
move forward so basically the three main
topics I need to cover are just some of
the design constraints I think a good
data science environment should have or
and just and just about the process of
doing the science the second one is
going to be talking about an environment
for doing so here going to talk about
mostly notebooks versus R apples and how
we can have more programmable
environments and the last biggest chunk
is the tools here one big part of this
is dedicated to talking about the side
design decisions behind my data science
library called hood it built in closure
and then some things about having
feedback loops and important feedbacks
the importance of feedback loops and
last but not least how we can extend an
ecosystem by building small compilers
that target other language runtime such
as building a small compiler to scifi or
to ggplot and so forth right so before I
think
it can have quick show of hands how many
of you already know clover okay and how
many of you have actually worked in
closure okay cool thanks so just I can
sort of adapt to the level so I won't go
too much into details about closure okay
so um let's start with the design
constraints well the first thing is that
this talk is very much targeted at
working on a single machine this is for
two reasons first of all I think that
most of our data cases are actually can
be done motions people from Google and
Twitter notwithstanding obviously and
the second reason is that even when not
we can often be smart about it
beforehand and sort of do a dividing
conquer of complexity and try to massage
our data before hand and turn it into a
shape where we can then work with it on
a single machine meaning just having a
sample or having some form of pre
aggregation already done and then just
work with this data on a single machine
and I think such a separation of
concerns greatly increases developer
productivity because I think it's quite
horrific that there are now more and
more environments coming out where their
main selling point is this seamless
integration with SPARC yes this is a
good way for some tasks but very often
you're approaching something wrong
you're just making including massive
additional complexity in form of big
data kind of data pipelines where you
should focusing on how I can shake my
data in a such a way that I can working
up on my computer so here's the first
sub question do you want me to go more
into details of how we prepare data in
advance to make it more feasible to work
on a single machine or and this is
DevOps sitting or should we go on with
the language design and actually working
with it at once you have it on your
system
like okay so we're skipping this so the
process of how I think the science
should be done is I think the core thing
is it at some point is going to boil
down to the question of communication
you can build as fantastic models but if
you can't communicate what they do and
how they work it usually won't work in
context to the whole company so and this
is an area that's often I think
overlooked so I'm just going to briefly
a couple of core points I think the
science system should cover the one
thing that I've very optimistically
termed analytics chasm I think they're
separate in terms of just the duration
from posing a question to provide an
answer with data they're this kind of
categorical differences you can provide
an answer in two minutes or less this is
the ideal because it means that in most
situations when you're brainstorming
something we just want something you can
immediately pull out the data have an
answer and continue that once you're at
this stage this means that you can
actually for me this at this point a
company becomes really data-driven
because then any decision can be
supported by data obviously that's not
going to always be the case but then the
next curve group is answer down in 20
minutes or less and obviously the
numbers are not super precisely
described like a guiding point and here
the idea is something that again you can
squeeze into your day without having to
allocate too much time to it
again the benefit here is if you go over
this it's something that needs to be
budgeted in your time and you're going
to incur this huge delay in how long
it's going to take from the question
being posed to provide an answer and
this is a problem because then often
times what happens is that the question
won't even be relevant or is going to
change or something on that and just
your results won't have as much impact
as they could if you tried sooner so a
lot of things that I'm going to talk
about are kind of are optimized towards
developer productivity in the sense that
they allow us to provide fast answers to
the questions that any stakeholder in
company my post and then
yeah just stop me anywhere in our all
analytics created equal
I've had to answer questions that have
taken me half a day they're ready query
even though we have all the data on hand
and I have the same data and I can
produce a different answer to a
different question in 30 seconds so it's
every single things the same ah no but I
think at the point where you have a
problem that is a question of writing a
single query which takes you half a day
that's a problem of tools that's a
problem of the express ability of your
language and exactly the kind of
problems which I would like to squeeze
into the 2 million gap there obviously
some and this is on the your right side
or whatever of this timeline the project
size questions there sometimes you just
don't have the right infrastructure in
place or it takes you need to scrape
some date or whatever so those are
obviously can't be solved through a
short time but at the point where you
have data enough relatively well formed
and stored in the database I think that
we should strive towards providing
answers quickly
now obviously the more machine learning
to involved the less this is possible
but front of analysis this should be
kind of the target frame and exactly
basically this question was perfect this
I hate right spending half a day writing
a long way this is something that in
most cases we shouldn't be have to do
should have to do either big and I think
this is soft on two levels one is in
terms of just the tools we have and the
other is how we how much and how we
prepare data in advanced so and maybe
just a quick then reverse leave although
you didn't want me talk about this but
one thing that yeah the question is why
closer should be different from the
other languages like Python for the same
thing that's one of those where I was
expecting the question fundamentally
we're using closure because I love
closure I've been a list programmer for
12 years now almost exclusively and I
picked Lisp when building up this
department because I knew I was going to
be most back to vated and I knew I have
a belt I know the path of how to make
people fluent in Lisp in a relatively
short time so it wasn't a problem
so the staffing wasn't such a big
concern but most of the things I'm going
to talk about here aren't exclusive to
closure there I think that a lot of
insight design decisions I was
influenced by the clover ideology but I
think that a lot of us can also be
either directly translated to other
languages or at least the core inside
can be transferred and used somewhere
else so this is we're using closure
because I like closure not because I
think that closure is the ultimate
language for doing their signs like the
Julia people are doing lovely things
it's a very interesting language to
check out and they're some of the things
that they're doing numerically with
Haskell are very interesting so no there
is no reason for accept it or maybe yeah
reason is that it's why I like closure
is because I it seems to resonate with
the way I think now in some ways this
might be just using lisper salon has
shaped how I think but just thinking in
terms of structure and something and I'm
going to get into eventually hopefully
is that it matches very well with how I
think of problems and how I analyze
problems are looking for languages which
are very strong in terms of manipulating
variously nested or complex data how
important to have immutability in the
language I think it's it prevents a
certain class of problems or bugs which
in term aids faster development every is
anyone who's worked with respondus
you've probably got bitten at some point
where they have this sometimes it's a
copy and sometimes it's nothing it's a
mess at least with closure
it's very unified in terms of
performance it's it works well enough as
long as you're like most of the data
seven data sets I work with are around 1
million data points 10 million data
points and all that stuff can be works
readily on my machine so it's fast
enough for for the use cases that I have
I suppose that if you want to have
absolute performance you want to do more
miscibility but even with closure this
is becoming less and less of a choice
because for instance with introduced
interaction of transducers which is kind
of a
you can think of them sort of as a
stream fusion enclosure but what they
did is behind the scenes most of the
then it's other bodies done with
mutations so you don't even have to
choose but this mutation is not exposed
to the end-user so still I think that's
kind of the best situation you can have
but language isn't that the language
design language itself is not afraid to
use mutation but it's not something that
should be readily exposed to the user
okay just a quick note on preparing data
in a readily way I think that this can
in some ways it might seem that I'm just
kind of pushing the problem in the
problem if Iraq but what we do is that
we are very aggressive in just
automatically building various views of
our data so what we do is we would have
an ontology of our events and the
attributes of those events going in both
a male ontology and some of it inferred
mostly from just the data types of
events and then we built automatically
various views for that you can think of
like for instance the every time that
you have you see a date
will build various irrigations on a
daily level hourly level monthly level
and so forth and then we also we do
seasonal detections so we know we kind
of have this rough idea of which kind of
frequencies will in time make sense to
combine them or where you want to make
build the drill down it doesn't matter
so you can you have readily available
all types of pregrated data and not
having to code for them explicitly so
every time a new data source comes in we
just basically classified in terms of
our ontology and then we get a lot of
things just for free and this immensely
helps with such a development cycle
because we then don't have to pay as
much in terms of just preparing our data
and it's readily available available at
any point that we want to use it right
so continue on in terms of environment
assuming that it has to be visually
enough that you fund the main way of
conveying information is through
visualizations rather than from numbers
because there's way less information is
lost that way and
it opens better any questions you can
very soon in you know all those problems
with averages and stuff like that and
the moment you're looking at the
distribution of those problems just
follow away so we want an environment
where this is where visualization is
encouraged as much as possible for this
type of design issues do you think it's
better the wrapper or set of the
notebook style so what do you think
about it and how so um I think or let's
start at the end ultimately I think that
a notebook is better means for
specifically doing data analysis but it
shouldn't be an either/or question
thinking that both should be used
together because what fundamentally what
you gain with a notebook style of
representation of two things you gain
spatter grouping and you gain more reach
in from you can sort of incorporate more
information easier than with repla
although if you're using Emacs that's
kind of a moot point what is just that
I'll start with one it when the repple
is good and this is the kind of inverse
with rapid what you'll have usually is
you just you type in one thing and after
the other of the other so you have this
completely FML way of working with and
we're just adding new things and even if
you just have some throwaway code to
just stay somewhere there and then you
refer to something that you've done
previously and it's now lost in two
screens of rappelled down down the
screen so what the notebook on the other
hand allows you is that you can group
together similar functionality or
similar and analysis even if you do it
sometime later so you do it's a notebook
allows you to structure your code more
clothes more as you would in a texture
file rather than how it looks like in a
rapid session that's obviously you can
that you could just use a code file but
again for it's always a problem with
what you do if you just have some
snippets of codes you want cry out so I
think ultimately the best solution is to
have both your end result is going to be
catch it's going to be incorporated into
a notebook but while working it
sometimes makes sense to her Apple also
available otherwise you're just
constantly deleting various small crap
experimenting with things and looking at
functionality especially with languages
which allow you various forms of self
describing and self documentation you'll
be using that Latin rap a lot and it's
um it's not an output that you want to
be to be included in your end analysis
so and in further what a notebook in
such environment allows you and this is
why I'm also a big fan of notebooks is
that there is this kind of a seamless
way between having a notebook and then
turning to a dashboard because first you
start with notebook and then what we do
is we add this Auto refresh
functionality where each day or each
month or however you want to specify it
with we recompute all the things in the
notebook so you never have you never end
up with stale data that was one of the
big problems that we always have before
taking on this approach was that you
have this kind of some stale analysis in
the worst case may be written as an
excel sheet and sent over email and then
you have five different versions
floating around which slightly different
results and nobody knows what the
canonical version is while having a
single canonical point which is a
notebook and then having it was
refreshed it's always going to be
up-to-date and it's oftentimes also
works as a wonderful kind of early
warning mechanism but you broken
something because the auto refresh
breaks and you have you done somewhere
something wrong and then you can track
the changes to do you did to your data
scheme or whatever and also set kind of
having notebooks is just in terms of
current tooling it allows you much
better sharing and also it's in a lot of
ways it's easier to build things around
them you don't need to be either a Emacs
hacker or Eclipse or whatever so for
instance what we do is we just add a
simple annotation plug-in to our
notebooks and the other nice thing is if
how did you make the trade-off between
commercial products like tableau and
there's a few products you can use for
this kind of stuff and building yourself
what were the things you thought about
how did you take that decision decision
was based on two criteria I think that
kind of most of at least four are meets
most of the commercial tools covered
this is kind of a middle ground where
very simple things take too long to do
and you can't do very hard things so
just kind of okay for some diminutive
stuff and that just wasn't a good match
in terms of what my department was doing
and you're still doing so we still do a
lot of very simple analysis and/or we do
quite complex things which is that you
just can't really visualize and some
person we do a lot of custom
visualization stuff like I didn't just
can't do that you can't have that kind
of a level of either interactivity or
complexity in tableau on the other hand
doing something simple just takes too
long and here is just one line of code
to get a decent in our visualization so
it just didn't mix well with how what we
were doing and so I think somewhat this
question depends on what's your what's
your team composition personally because
my background is more in the programming
side then on the data science or math
side so I approach this in other ways as
in from an engineering perspective and
then the hope the tools that we're
building and how the team works reflects
that
but um probably the trade-off is going
to be different if your science
department is primarily composed of
people who are maths or science part
first and engineering later so I don't
think that this is necessarily the right
decision for everyone but it definitely
works well for us and also one of the
strong key points that I want to get
through is that once you have such an
environment and if for instance this
notebook is well written in closure then
what you can do is you can actually
change your running environment at
runtime so what happens here is and just
as a brief overview for those who don't
who aren't fluent in closure what we do
is we just we extend the rendering of
our output on the fly in renderings and
incorporate in the same notebook so
basically we can use the same cells that
we're using to do our analysis we can
also use them to program actually
the environment in which it's executing
so we can add new types of
visualizations on the fly front or
something else this example here quickly
what does is it just allows you to print
infinite sequences and stops them at the
first ten elements but this is just a
poor example but the main point here is
that if your execution environment is
done what I would say actually right is
that it's exposed enough that you can
actually program it from within itself
but you don't have to have separate
basically module system which you
program like an iPad or something but
you can modify most or all of the
functionality at runtime this is again
an idea for people who are from the
idealistic tradition this is nothing
special but I think it's in terms of
notebook environments it's not readily
accepted idea and that's a big shame and
a lot of games will be made in the
coming time with more people hopefully
hopefully embracing this approach okay
so enough about the environments for now
let's maybe just wear right now what
you've seen is it's called gorilla
repels some closure notebook which is in
a pretty bare-bones state right now so a
lot of work here should also be at some
point done with making it better and
make and more powerful and feature full
but this is something that were slowly
also starting to open source on things
we already have done internally but
think of in just in terms of what the
closure ecosystem offers here it's kind
of enough but it's far from good and
still even though I like having my
notebook very programmable I would still
oftentimes love to have a much more
mature environments and like Jupiter or
something so here the situation is not
ideal but it's getting better
what's a data processing libraries
that's the question you didn't know
before so what we do is for data ingress
we stuff all our data into Kafka and
then we use onyx to build various views
in either post we SQL or elasticsearch
and from that on we're using the library
I'm just going to start talking about to
actually manipulate data right so going
back to this dichotomy I already talked
about between a lot of people see data
science is kinda either/or between
engineering into the science I think
that there shouldn't be such a clear
delineation and that's this is in a lot
of ways hampering the tools that already
have and the libraries that we already
have but the core offender here probably
is the data frame and it's going to be
somewhat controversial but I think that
oftentimes that's simply not the right
abstraction why because it serve
conflate students representation and
abstraction and curve forces you into
this matrix like way of working which
sometimes makes sense but oftentimes
especially at the beginning stages of
analysis and when you're cleaning up
data what actually have is a very
heterogeneous data and in those cases
data frame is a relatively poor
abstraction of what you actually want to
work with and on the other hand this is
somewhat clover specific as that closure
excels in structural manipulation and
coding computation into structures and
so it's a shame to move from that how
much do you think it's the capability of
closure to using immutable immutable
data so there's a big difference between
between fightin or R big R makes the
difference your work because we we said
it excels in the in the structure
manipulating now so I mean from from the
just data but it's just like I think
that it's not so much about mutability
per se or immutability but it's a
question of how you organize your
problems
and I think this is not just just by
being functional what you usually tend
to do in closure is you just have a
basic like a pipeline of transformations
one from the other and once composing
programs in such a way that actually
isn't
it's not even natural to do mutations
because it's just kind of a
transformation from one to the other to
the next next next and you don't so you
have no need in just in terms of
thinking to do an explicit mutation
because I know sometimes it's a there is
a problem if you have a very pure
language and just some algorithms don't
hash very well together with not having
any sense of mutations but concretely
for kind of analysis type of tasks and
data clinic I think that's not the case
that most of it can be defined as kind
of serious of transformations and so
it's its ability is even less an issue
it saves you from sometimes the bugs and
that's it but it doesn't impose a
cognitive burden on me and also looking
at on some of the jury people on my team
have never before programmed in a
function language they have Python
backgrounds of a gnat or MATLAB and they
don't have big problems with working
around clover being mostly mutable it's
a I think it's a very small step to
start thinking with immutability in mind
and then you completely forget about
what but the mutable approach so and now
back to this kind of include in encoding
computation in structure I think that's
a very powerful idea I that's going back
to when I was saying why I picked
closure because of how it kind of meshes
with my thinking and all times I kind of
for me it's easier to think of the
problem in terms of kind of various
nested structures one with the other
which encodes some step of the
computation and then basically doing at
the end some reduction which turns the
structure and the more normal or data
frame ish form rather than have it being
forced with full steps having a
transformation from A to D data frame to
the 2d data frame so basically adding
additional dimensions
often times makes I think problem more
easy to think about just in terms of how
you want to approach it and solve it and
also gives you yeah it also gives you
niceties such as it's very easy to then
encode something like a distribution
which can just be a map if discrete
distribution is just a map between kind
of keys and values and get the
distribution out of it and that what a
more elegant way of representing that as
and you'll turn this you would have in a
more data frame ready to play
occasionally yes because especially when
you're working with relatively big
datasets and you're on the edge of what
your memory can can handle but at least
in my working it's not such a huge gain
I should have in Haskell for instance
this is just due to the nature of
language overall enclosure laziness is
nice at some times it can allow you to
express something elegantly but for
instance if we want deferred evaluation
we'll use force to use macros anyway so
you don't get those niceties so but and
also oftentimes what would happen in
what actually happens have sorry happens
in closure is that even algorithms which
are or operations are nominally lazy you
would then have them collapse into a non
lazy transducer operations which are
more efficient anyways you'd have so
there are two halves fusion instead of
just laziness and again so it's not it's
not as apparent as in maybe some more
hard core functional languages okay back
to our story and the tools that and
that's the library that I've built so
the idea is that I wanted something
where which can be mixed together freely
with existing closure abstractions so in
closure you have a lot of functions
which operate on sequences and I didn't
want to lose this whole ecosystem and
also third-party function so forth so
again this another argument against data
frames that once if you have a proper
data frame usually the rest of the
language that's but to do with working
with sequences just doesn't work on that
so you have to really build all the
capabilities that are otherwise already
and present or you're lacking things so
what I want this kind of an easy way to
make vanilla closure with my more data
data oriented the different functions
the other thing is that I'm very big on
compressibility this is composability in
some ways even going against what would
be considered a geometric closure but I
kind of think that here this is one were
doing data science or working with data
it's one of the cases where sometimes it
makes sense to go against what's
considered best practice in terms of
purely a genetic site this is again this
dichotomy between doing data science and
engineering and sometimes you do have to
make some concessions and some things I
would never write in a purely production
code makes sense when doing exploratory
analysis so what my library does is that
it's allow these offers most of the
functions have current versions which is
not something that's always done in the
closed ecosystem because I think just
composing functions together is a very
powerful way and this also gives you
kind of a generalized accessors because
in clover also would be data structures
can access functions so it's basically
there's no difference between reaching
within a data structure and then just
applying some function if you look at
the example above so what we do is first
we reach into a data structure and the
the slot holds slot and then we apply
one function to it and then apply a
different function to it and all this is
basically acts as a unified accessor
where we just kind of roll up then based
on whether a day of the week is an odd
or even number and so you have this kind
of what this kind of composition then
introduces in a way is kind this virtual
structure where even though you don't
have a specific structure within the
number let's say seven which is the day
of the week but if you apply our
function to it
it kind of yields to this virtual
structure and this little provides a
very unified way of working with various
nested or otherwise complex structures
and that's something that I've grown to
love and most of my library's kind of
designed that words with that in mind
now maybe the last kind because we're
compressed the time the last thing I
think that's important or helps a lot is
that still closure system even though it
can reach out to the whole Java system
is limited in terms of the libraries you
have available especially with in terms
of machine learning so what we do is
that we just then farm it out to
scikit-learn or whatever we we have
small dsls that compile to Python code
and then use that because if you look at
like something like Sai Sai pie it has a
very unified API so it's super simple to
just create the necessary Python code on
the fly send data to it with the data
from it or from a file or whatever and
then move back to closure and then
immediately you have access to the
entire basically scientific Python by
the ecosystem from within closure and
closure is a very nice wrap because as a
lisp
lisps tend to work very well with
building small and dirty compiler since
it's exactly such case the other example
of that is that I'm using DSL to
regenerate the ggplot plots so we can
here see relatively complex plots done
in closure with very cold so we have a
small closure DSL which then gets
compiled to 2r which produces an SVG and
then that sent the notebook and you have
a very expressive visual library
available enclosure with very code yeah
can you maybe tell us a bit more about
interaction with other languages and the
thing you were telling us before
especially in the live approach in
rehave a link from this in this repple
and yeah the examples you see here for
instance going out to arts is happening
on the fly so basically at the point
where you evaluate for instance the bar
chart invocation
what happens is that our program is
built from this description it's sent to
basically an interactive our session is
spawned the result is an SVG which is
then taken as the output and inserting
notebook and you can do that the same
thing with psych it or whatever so you
just have to define some simple
information interchange protocol and
that's it and then it's the lag for
something simple like using ggplot is
negligible and when you're doing some
machine learning the time the algorithm
actually runs is so much greater than
the time to marshal information in now
that it just doesn't matter and I think
we're out of time one minute a question
to you hypothesis you mentioned earlier
how do you integrate it with your
notebook and with your workflow do you
just use I don't know how hypothesis
works right now because I supported it
several years ago on Kickstarter or was
it IndieGoGo is it is it ready to use
for academic projects we have our own
instance running getting hypothesis
running on your own is an endeavor it's
possible but it's not something that you
can just start up and going to work but
it's doable talking with the hypothesis
guys they intend to improve this thing a
lot because they had a couple of use
cases similar to ours and they want to
support that further but right now it's
doable but it's not very pleasant
so we have our own instance of
hypothesis running which is then and the
data that's stored in hypotheses and
also available to the rest of our
systems which then also helps with
things such as discoverability so what
we do is we would write various either
annotations or just tag things in
hypothesis and then use that to make all
of the analysis that we
the report in notebooks kind of
discoverable so we have someone new you
can they can then just browse various
topics and find out analysis that has to
do with that which helps with onboarding
a sort of conserving all the knowledge
that we've produced up to date for
querying are you using just regular
culture functions or some kind of data
log some kind of more formal language
currently we're just using standard
closure libraries either for doing the
SQL just ODBC reps and for elastic
surgeries in the elastic search client
and closure but it's it's an area that I
want to investigate more during future
is just various optimizations because
most of the times the if you look at the
structure of the code it has this kind
of very unified look so it's basically
using just the kind of pipeline or the
freighting macro the starting with the
arrow and then you have various
transformations one after the other and
that's something that could be very
easily transformed into a more efficient
form so something for instance in Common
Lisp world this is very disquieting
common practice called compiler macros
where you have specific macros which
catch various patterns in the code and
form into something more efficient and
that's something I would like to do here
as well and then be able to on the fly
delegate some of the computation to the
database and some to the code because
right now just the workflow is that you
basically pull the data from the
database and then work it completely in
closure and then maybe at some point
save it back but it would make more
sense and be more efficient if we could
do more computation in the database but
at the same time I don't want to
sacrifice accessibility so but I think
there's a lot of low-hanging fruit that
can be solved very simply with just
looking at the structure of the code and
then transforming that into the actual
database
so your case looks like that your you
can handle data you're just one machine
and your machine only no what about
using closure with a dupe or spark do
you think it's something that is useful
yeah of course if you look the storm
architecture which was one of the first
real-time processing systems was written
in a closure and bought by Twitter and
you're sometimes a new Twitter
concretely we're using onyx which is
kind of a simple similar system but also
from what I gather the story with
working with spark is very nice word
enclosure so definitely that's something
that can be done but as I said we're
trying to not do it do not use those
systems too much and it's doing
day-to-day analysis but like if look at
the all the functions that marshal data
around and either write to calf guard
and read from caffeine's of that
everywhere in between
it's tends to be closure specifically
onyx which then does all the
calculations or one more question
you say that you think that closure yeah
and sorry you say the closure is very
good for expression expressing certain
kinds of transformations can you give a
concrete example for such such a
transformation that you would think
would be harder to express in another
line which is say there's also trap here
that I'm just going to show how ignorant
I am the capabilities of some other
language but what I think is that where
closure shines most is when you have
nested data structures when you have
something like a vector of maps that
contain further Maps or vectors or
whatever that's something like that and
you want to work with that here closure
is very good and it allows very concise
code in how you then take that structure
apart and build it back together and I
think the language is built with that in
mind so it kind of it's also produces
very idiomatic code you don't have this
kind of feeling that you're fighting
against the language or some horrible
hack but it's just natural and also
think what this produce
this positive feedback loop where even
if you look at closure programs in the
wild and not having even having to do
with data analysis you'll see they just
encode things into various factors and
then kind of work on that so it's kind
of a better and it's Prive Civ through
the closure worlds to kind of try to
upload as much computation as you can
into data structures ok yeah so it's
kind of hopefully this talk wasn't too
chaotic I didn't really have a good look
but I just going to sum up what I think
that the main part the main points would
like to impress on you are is that when
doing data analysis speed of answer
matters in turn because you're never
doing data analysis in a vacuum but
during in a context of other
stakeholders and for them this matters
and second of all it's about
communication so look for tools that
allow a good communication not just
solving the problem or rather your
problem is not just producing a number
or a model but it's also communicating
that and that oftentimes is left out and
we don't have to reinvent every wheel
enclosure or whatever language you want
almalexia or whatever we oftentimes are
very good especially if you're a
relatively powerful starting language
you can just build mini compilers and
leverage other platforms and this can be
done surprisingly robustly and
surprisingly concisely and well and as
we've booked a lot I think that one of
the core assets or strong points of
closure is how good it is in with
working with structured data and this is
something that's it can be very nicely
applied to problems in data science
sometimes it might be forces you to
switch in thinking from a very matrix
oriented view to a different kind of
view but I think that overall it aids in
how fast you can just imagine the
solution and then obviously type it out
and last and something that's I would
wish to spend more time on or maybe and
some other talk is just kind of this
idea of a blurring the line between your
development environment and the world
we're actually doing and how melding
those two more together can yield much
better and much happier that development
experience rather than having two
separate systems which don't really talk
to each other so thank you for your help
your questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>