<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ali Shoker - There are no BFT Fans Anymore... About Secure Eventual Consistency | Coder Coacher - Coaching Coders</title><meta content="Ali Shoker - There are no BFT Fans Anymore... About Secure Eventual Consistency - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ali Shoker - There are no BFT Fans Anymore... About Secure Eventual Consistency</b></h2><h5 class="post__date">2017-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iXxiefIMG5w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's better now shallower start on
there no yeah so so I was I was like
yeah I was like that when I moved to
eventually system I brought with me the
sense of fault tolerance and I've noted
that eventual consistency seems actually
they trade consistency for every for
availability and then they almost run in
the white and they core about just the
what we call crash recovery model or and
I just wanted to to design some solution
for these systems and I thought that
because I've worked it on both areas so
it might be a good trade-off to do to
the to design that thing and I'm gonna
propose probably this one so we've
already this was presented in the iosys
pop workshop I'm gonna give an overview
of that system but a friend of us an
expert in both areas as well have seen
that people and this led to a long
discussion with him so the discussion
was like the end of the session was like
there are no giftie fans anymore so just
like you know when you have a good work
and you invested time on that and
someone says that you just throw your
work away so I just like he boxed me in
the corner and I wanted a killer answer
like well my answer was like at least
Leslie in output is still living so if
you know that Leslie Lamport is the
founder of buy some time photos or and
he's defined this in the his paper owned
by some kind generous problem in the
nineteen M so but then I think that then
I realized that this was a bad answer
actually it was like kind of surrender
but because he is actually the proposed
a Byzantine generals problem and he
probably will be the
sponsor of that line of that problem on
the other hand add to the complexity of
base and time photons actually Lamport
also is the author of time clocks and
this paper that's that considered the
seminal paper of discrete systems and
it's very complex and the axis paper
which is also very complex so he started
working on that in 1990 and after eight
years the people just grasped the idea
and even Nancy Lynch proposed that yeah
it's that's very complex and it's worth
another paper and then he published
another paper which is that is made
simpler in 2000 so even just the kind of
naive application like its document
editing he made that complex so less
example is a bad example to measure on
that soil and then I realized that this
is not a good answer and I tried to
analyze what was the reason behind why
EFT is not being adopted in the industry
so I had some thought some thoughts that
I would like to share with you along
with my design and I would like to get
your feedback so this work was done with
the my student who Sam Black team and
colors back arrow with the wars on
conflict free replicated data types if
you know him and within a project
computer age computing project called
light-cone which we will continue
developing c oddities there if you want
to follow so in this talk I'm gonna give
a brief background on BFT and eventual
consistency and what are the problems
with buzzing tines in these kind of
systems and what is my design that i've
proposing potaters design and I ramped
up and throughout this this talk I'll
give my highlights on why if T is not
being adopted in the industry so let's
with this and sorry for the harsh
probable image so this kind of story
that used to happen in the 19th century
like whenever someone dies and you ask
the physician this is that it just it
happened or and currently we we don't we
don't we don't accept this kind of
argument in this century because it
should be something scientific like it's
it's a brain stroke it's a heart failure
it's something like cancer and this
allows an analogy also goes forward
computer systems so you often see like
system is down the system is unreachable
at the moment we are sorry about that
there are there was leakage in the
system that's embarrassing and
unexpected failure caused something and
these are also no longer accepted
arguments in this century so what I want
to say that here you probably might have
expected that to bring you like compiled
a list of Byzantine failures that
happened and then I would've ate my work
so yeah this is a list of failures
actually in real systems and that's why
it's important to talk about by sometime
fault tolerance but I want to go beyond
that actually so I think that there is
some barrier in bees and time filters
and and between bison plane photons
research community and the industry
actually and if you notice that from the
proportion of the investor people here
who know about waiting for tolerance so
it's ten percent so the message the
first message that I want to invade that
we couldn't we couldn't figure out if
they're always n thankful was n time
faults or where are they and or how do
they behave if you don't have access to
the systems so we need your help from
industry and you have to be more
educated about buy some time for parents
so you find problems
and we solve them juster phrase it looks
very bad
like phrase like you do bugs and we fix
them so but you have to discover or
describe what happened and if you don't
know about miss and time folklores or
even the definition and how do they
behave so you couldn't help us okay so
the my first message is that I think
that developers and they need to be more
educated about this kind of failure so
then let me start with the overview or
the background of peasanty for tolerance
so the base on time for tolerance model
is the strongest model in the fourth
class so and it's the strongest because
the nodes actually behave arbitrary so
you kidding you couldn't expect how do
they behave so just a couple of
different types of errors like it could
induce errors it could induce bugs
it could anti Registry delete a memory
okay another thing is it could behave
maliciously or it may collude with other
nodes to do something like even errors
or bugs or could even behave correctly
so this is also a challenge I will I
will come to this later on so actually
the main approach that that I'm
intricity in here is something called
state machine replication in which your
system like the cluster a couple of
servers they work as if they are a
single deterministic machine with the
same input it should give the same
output and it works with the majority
consensus so if you know access it's a
kind of close or it's a Coulomb system
actually and in this system the right
the right column and the right column
should intersect in a non design plan
fall so it's not enough to have one
intersection the intersection should be
numbers and time because it wasn't time
replicas could lie okay so you can't you
couldn't rely on on the result and it's
known that you need for the right one
for F fault you need three F plus 1
nodes it means today to tolerate one
faulty server you need four servers in
the cluster ok so and the one of the
most known probably and it's the first
practical protocol is pbft which was
built in the paper was published in 2000
by Castro and Liskov
so the main challenges in this system
are so we have its first it's it's
impossible to distinguish between
buzzing Byzantine replica and a slower
replica so here you're going to think
about the availability so when you
consider that you have a response or not
how much would you wait or not in a
synchronous model the second challenge
is being paid independence of failures
so nodes must be diverse but they also
must be deterministic it's a kind of
controversy like you need all the
machines to be deterministic but they
have to internally be different like and
here it might be challenging because you
need different software and different
hardware installed in the same cluster
and they should work together as if they
are one machine and also it's impossible
distinguish between well behaved
behaving design time nodes and correct
nodes and I will come to this later to
show you it's mainly on the client side
you will see so I come to the second
highlight which is Ballantine fault
tolerance may be costly so we need
additional servers actually to build
your system you need a cluster of Forces
servers to tolerate only one by some
time for a server and I had before a
talk with the open led community was
trying to
to integrate this in open LD and LD AP
and then they discover that we need four
servers they got re crazy like and
they've escaped so also there should be
hardware diversity otherwise all the
servers will fail at the same time and a
kind of inversion programming like you
need the same system it's better to be
implemented by several groups several
developers probably in several languages
with several libraries and that's very
costly so this is the second highlight
problem so now coming to the eventual
consisting that you are more familiar
with this model so in this model you
usually you trade a consistency for high
availability you you care about AP in
the capsule if you know the cake calcium
which means you care about availability
under partition network partitions so
the model V when you have you have for
example here you serve the requests of
the clients on each replica without
doing any synchronization with the other
so it's super fast because you are
immediately giving the clients the
answer and you have to accept the
application here it has to tolerate that
it camera it will read in the past okay
so it wouldn't read probably the last
version that's written and then each
replica then propagates the updates to
other replicas and of course since you
are doing updates at the same time there
will be some conflicts like a kind of if
you are using github or something okay
so you have to fix this data so fixing
this you probably you have to fix it
manually or there are new solutions like
C oddities conflict free or petty data
types that are mathematical based
solution that can do conflict resolution
by themselves so this kind of the
eventual consistency model and its
derivatives like causal consistency and
all the relaxed
consistency models so what happens if
there is some bison time fault in these
systems actually what happens is that a
lot of problems
I'm going to focus on two of them so one
of them is on the clients and you will
give the clients wrong values so this
wouldn't happen in another model like
the numbers and time on you wouldn't
expect that but if the server is
compromised like it's malicious or it
has some bug for some reason if going to
give wrong results to the client side to
the clients on the server side you
should be expecting eventual consistency
but your system will not will never
converge okay so and this work because
different servers are executing
different operations so as long as these
operations are commutative or you have a
way to reconcile them it's okay but here
you have a server that is actually
changing the values so you will never
converge if you change the values just
give you a practical example like this
is a simple replica of a counter that's
replicated in two replicas one of them
is bezel time which is the replica D so
the state of the counter is three
starting to see in both of them
and then you have an increment operation
a on the first and then it becomes state
becomes four which is correct a
decrement on replica D should lead to a
value 2 but because it's the same time
it's it's working as arbitrary so the
value is now 7 then when you coordinate
the server's you'll need synchronize and
you send the operations to each other we
propagate the operations you will end up
with different values at the end so on
replica a will have a value 3 and all
replicas view will have value 8 and
hence your system is divergent okay so
this kind of fault on
on the server side multi client side of
course you've got the value s equals 8
which is rank okay so why don't just use
DFT to prevent this to allow for
convergence and this is the non story of
the debate between strong consistency
and revenge consistency because it's in
bit BFD BFD embraces strong consistency
so you have total order you have
synchronization you have you you're
going to block the clients and you can't
afford this in eventual systems so my
third highlight is the bad timing of BFT
so actually the first practical
Valentine photons protocol appeared
around 2000 and this was the exact date
of the cat theorem and so there was
probably most of the system or become
like a scalable and high availability is
a major concern so all the focus went
there to the cap and there wasn't so
much interest in buzzing time fault
tolerance
probably probably so this I think that
this is one of the other reasons as well
so and this probably one of the reasons
why I left
EFT to avenging insistence to work on
Avenger consistent as well so what we
what will be the approach what will the
approach to build such a system like how
how could we do if if version 10 photons
is strong consistent and eventually
assisting is not so how could I come up
with a trade-off so my work is actually
to add some Valentine fault currents
feature to eventual consistency and then
my protocol would look like this
so add to the complexity of bezel time
for Florence D complex to V virtual
assistants and so that would be a crazy
system and of course this is not what
I'm gonna present
and we have already spotted some
complaints from even of the research
community like this paper which is
called the saddest moment and the
saddest moment this is a research from
Microsoft the service pond is when he
attends a conference where a BFT
protocol is presently being presented
and I hope he is not here Sarah so this
is becoming frustrating even for the
research community so and also if you
won't need something that's more
practical some people actually have
tried that so there is this paper in by
sometime for tolerance for services with
commutative operations so even they've
tried to integrate this VFT stuff in the
event assisity system themselves and
they figured out please the optimization
mechanisms outlined in Section C which
is about these technical details are not
yet implemented due to their complexity
and this is expected because even when
time for turns by itself is complex so
integrating it with EC will be like
additional complexity and this brings me
to the highlight for which is B if T is
really complex actually so I'm not sure
I just put but maybe not that much
because because probably because I've
worked it on by sometime fault tolerance
I've not that I've got more familiar
with that but I often compare that to
pax was like and I think that praxis is
even more complex than 50 particles on
the other hand like if B if T is complex
why just not use it in an intelligent
way so and here comes my proposal so
here is an eventual consistent system I
have four servers and the propagation is
being done by a reliable broadcast
between them and each one has allowed
the green green light is a lock and
that you have different log for each one
because everyone is executing clients
requests at the fly on the fly okay so
you expect different different
operations but eventually they're going
to propagate these updates to each other
and then what do I do I need to just add
a plug-in which is a Byzantine photos
proxy for each one and this proxy
communicates with a BFD cluster this BFD
cluster is a VFD protocol running on
four servers say if you have four on
four servers that are not integrated in
the service itself but just the proxies
they issue requests to this cluster and
they get response so if you're not is
here that I have that the deity runs at
a consistent offset so here I have a
problem so I've said that we have
different logs for every server so now
if I've sent these logs to the Byzantine
cluster VFD cluster so it wouldn't be
matching so the values wouldn't be
matching because they are clearly
different so I can do this matching it
will it will not it it will not approve
the request the BFD cluster so how could
I I need something that is consistent on
all of these replicas so here you have
to think about the consistent offset so
for all these logs since they are
propagating the updates across
themselves so at some point I will find
in my history in my log a consistent
offset for this operation consists of
offset means that you have all the
operations executed on all the replicas
but it doesn't mean that the same order
should be there because it's eventual
consistent like the order might might
differ but since we have we have like
ways to reconcile this probably using
theologies
or if the operations are commutative so
it's okay from me but what I want to
guarantee that this yellow color is all
the operations on all these replicas and
it's already being executed so now this
yellow colors should be the same if the
if the service is correct if none of the
server is Byzantine so what do I do
adjust in the background I look at the
tail of the of the log and just I push
it to the VFD cluster and I get the
answer whether they are matching or not
so so you can see that BFT runs on a
consistent offset of the critical path
so if you see the upper side the system
is just working like eventual
consistency okay so there is no blocking
by the way of the cluster but you are
just verifying the consistent offset of
these logs so this we say that this is
an accountable model so like for example
here you would expect from the client
for example to see some errors because
you are not verifying the head of vlog
you are verifying the offset which is in
the background but in an accountable
model the client issues requests to a
replica and then the replicas
immediately replies with the last
certificate so what's the certificate
the certificate is a proof of
correctness which says that this
consistent offset is actually matching
on all replicas okay so how this
precedes so as long as the client is
receiving an up-to-date certificate from
the beef thirdly beef tea cluster it
knows that at some point in the history
the service is working well ok so this
means that the client should have some
tolerance for four faults
at least for for like them for example
let's say you have a metric K so a
client could tolerate for example 1000
requests without being verified by the
BFD cluster okay so if it has this
tolerance so as long as it's receiving
new certificates it's okay please if not
or if the certificate is bad it needs to
roll back the client needs to roll back
and in a kind of like the client here
could be a cache server or proxy or
something actually it might not be the
end user okay so it might have some roll
backs or you can just even notify a
client so even without rolling rolling
back at least you've knit if I decline
that there was some error so I think
this is the best that can be done on
this front how could you keep high
availability of the system but also
verify it using by some time fault
tolerance so this design actually I
think that is very practical because
it's modular right and it gets all the
nice properties of modularity actually
so among these for example you can use a
beefy solution of the shelf so I don't
want you to fiddle with this in time for
balance protocols which protocol you can
use pbft you can use blockchains you can
anything so just regardless of this part
you can just use something of the shelf
as as long as it has a clear API that
you can fire your requests across the
second thing is it's easy to test and
maintain and this is a property of
modularity actually you can just even
turn off that service using a
configuration metric and test this
system aside and the other system aside
because simply this is running in the
background so it's not critical very
critical to it change things or stop
things
and then you have this safe integration
because you've gotta keep your current
system running so you don't need to
change your code you just need this
plugin for the proxy and you just can
for example test the BFD cluster and
then you publish it and finally there's
a nice thing which is there's a spectrum
between spectrum of solutions for BFT
and and eventual resistancy options so
as I've said that depending on the
number of requests that the client could
tolerate without being certified by a
big cluster you can have for example if
you don't tolerate any requests like k
equals zero it means that you are
running on the BFD cluster exactly so as
if you are running a DFT protocol on the
other hand if you have k equals infinity
it means that you tolerate everything
which exactly what is eventual
consistency now so your event because
this now is doesn't care about this in
time for terms which is K equals
infinity it means that whatever the
replies I'm getting you don't want to
certify them I don't want to verify them
across the network and this means that
you have this text from between these
and probably the for example it depends
on the application and then the servers
like if I I might think like for example
K equals 1000 might be a good tolerable
so depending on the so it's roughly
depends on these servers actually and
the load so but you have a big spectacle
spectrum in this so in this approach so
why do we use this approach because you
need to care about if you care about
consistency in eventual consistency so
you have to use it because as I've shown
that if you have a single buy some time
server or even client so you will end up
with wrong service you will not have
conversions which is consistency
in the event consistency also you might
want to achieve this consistency by
using PFT itself but that's useless
because you can't give up availability
so you need some kind of trade-off
between them so also you need to care
about your legacy system so it's not
practical at all to do to go and choose
a new system with integrating it with
Valentine fault-tolerance and that would
be very risky so in this way you just
keep your system running and you just do
it as a plugin for your system and also
if you care about your customers you can
give them the different options like
among your clients who wants to use like
eventual assistance e he can set that
parameter of K equals infinity and he's
died and it's done he still used he
still he is still using eventual
consistency considered and then if your
client another client is very
conservative then he just can put K
equals zero and can be using present
time for tolerance itself and the
spectrum that I've just explained so
also there is a cost for this these are
there are trade-offs actually so here
I'm assuming that the client should
tolerate some faulty history so and I'm
not sure if this is realistic or not but
I just compared like even in now in the
AP systems you trade consistency for
availability and actually consistency is
a correctness property and for tolerance
also Detroit fault is a correctness
property this is about integrity so it's
a correctness property so probably it's
similar like yeah let's trade for
tolerance for high availability but at
least eventually
in the case of event consistency you
will be consistent and in our case
eventually you will verify your system
across the Byzantium photos and so this
is the first challenge but and as I've
said that the client could be not the
end user it could be like cache server
or proxy servers or it's within the same
classes so it's client to another
service so also there is trade of VBF
the assumptions that we have seen
already so you'll need more servers you
will need to use cryptography and this
kind of stuff you will need to have
diversity in your system like as much as
your system is failing independently
it's better in this kind of system ok
and that's costly actually but probably
recently I think that every like system
or micro servers or something has
different implementations good
implementations in different languages
so it's getting easier so and at the end
the clients are not allowed to talk to
multiple replicates at once in my
solution and this brings me to that
point where the Byzantine nodes can
actually act in a correct way so this
was also this confusing because now for
some in the eventual consistency model
like clients can send requests to
different servers so what if you send
like if you have on a set add an item a
add a and then you fire this request
across different servers as well a day a
day
so from the perspective of eventual
consistency it's actually it's a
different request so it's not an error
it's a different request and if you see
that from this perspective of based on
time fault tolerance it has nothing to
do with the system what BFD cares about
is
consistency actually if the note is by
sometime and it's acting correctly
that's fine okay so that's the from so
but how to stop that on the inventory
system because this probably a problem
with semantics like you should not allow
clients to follow the same requests
across replicas different replicas and
thank you and and that's why we
currently the solution for us just like
every client has to stick with one
server and this makes sense in the
eventual consistency model because often
like clients access to the closest
servers server to them and I think in
general the solution for this is
actually it's not like Byzantine fault
tolerance for this problem it's like how
to figure out the patterns of clients
that are attacking okay and this has
something to do with machine learning or
something like this so I come here to
the this fifth highlight which is this
last point that that I've just got that
actually there are limitations in by
sometime folders and and buy some time
filters is not a panacea so it will not
solve this problem exact problem when
the buy some time
nodes are behaving well so even if you
have a dozen pine server who is behaving
well you will never know but and even
you don't care so in the bus on time
fault tolerant mindset you don't care
about that
okay so just final takeaways like here
in my system I have like the event
consistency community needs the puzzle
time fault tolerant community why
because you gonna stay highly available
and you will give the option for the
client to be secure or not okay so it
has this flexibility and you also have
to focus if you are an aversion
persistent engineer you just focus in
your vendor consistency
just leave the BFD experts do their
stuff and the opposite way is also the
same so be if these nifty experts have
to focus on their work and just you can
just compose these systems and then you
never compromise your running system
because it's just a plugin just plug in
BFT and you go and then here are I
ramped up these five highlights here in
front of you so I would like to have
your feedback if I'm missing some of
them or why do you think that 50 is not
widely adopted in the industry and if
you have any any questions I am happy to
answer thank you so you mentioned that
the eventually consistent system is not
impacted as long as the BFG stays in the
background yeah but eventually isn't it
a problem that the VFD system won't be
able to catch up with the the offsets
will stay far behind with the eventually
consistent system and eventually you
have to you run out of log space and you
have to do something which it might
introduce some blocking somewhere in the
system right so so so the blocking
depends on the configuration of the
client so it turns you have to think
about how many operations or what's the
size of the log that the client accepts
without being verified but at some point
if there's a parameter and it's met so
the client has to be blocked and then
you have to take a decision whether you
stop or you make a request to change
that number for example but simply
putting this to infinity it's as if you
accept everything you accept Valentine
folds and that's that's why this is a
trade-off solution so I'm not allowed to
do
you to do what you have already been
doing on eventual consistency on the
pure avenger consistency model which is
just you can accept anything that's
coming okay so what I'm saying that your
model your application should accept at
least a number of operations that are
not being verified okay but progress lee
the the history of your log is being
verified so progressively it's being
variable so you always know that the
last operation number 1000 is very fine
okay so just later you know later that
these operations historical operations
were correct okay and that's why you
need to roll back actually if you
figured out that some certificates in
the past was wrong it means that some
operation was exhibited drunk on that
replica it was buy some time and you
have to change that that's why you need
to raga and yes you have to block the
client in this case and that's the trade
off or you just use eventual consistency
but one thing that your system will not
be consistent eventually if you have a
single Ballantyne fault your system
won't be consistent and you have to
fiddle with with the solution yourself
you change the database values on this
kind of stuff to fix that
um if you have a high throughput system
can the verification keep up with the
load or if you have a lot of requests
incoming in your eventually consistent
system if the BFD can well work with the
firm machines
I think the question is the load balance
so even if you have a great load on the
system as long as there is kind of low
dense propagation between the replicas
so you would have this offset will
increase quickly and then you are very
fine quickly so they're flying across
pbft cluster is costly but it's one
second or something so it's not that big
number okay but the problem is when you
have much load on one server and not on
the others so you will have difference
in the offset and the clients who are
firing requests on that replica quick
replicas will have to wait or will have
to be blocked okay so I think we have to
think about this also to find like what
could we do in this case thank you for
your question yeah and another question
did I understand correctly that for the
system to work correctly that also
you're eventually consistent surfaces
needs different implementations across
different languages different libraries
yeah what well this is because that for
the industry that gets very very costly
yeah that's correct and that's actually
the assumption for resin timing fault
tolerance and probably for other systems
like even like axis or something you
will you will need to have some
diversity otherwise probably like some
hardware if you have a bug in some hard
work all the hardware all the servers
will be down at the same time yes so
here we are talking about software not
only how do so also software it's better
to be different so in some cases there
there might be some for
that's not related to the implementation
itself so one server might fail but the
other or not or you would have a
malicious attack and attack on some
server but on the others not
but in multiple cases if something is
related to the implementation or the
libraries that you are using it's likely
that they fail together and that's
actually an assumption for certain it
basically becomes an issue of what kind
of errors can you catch and what kind of
errors do you accept do you accept the
software errors but do you want to catch
the hardware errors or something like
that so actually when we say that bazan
time replicas we mean everything so even
the network so the behavior of the
network the behavior in the memory on
the CPU on the libraries on everything
so that's the kind of diversity that we
want you don't need this kind of
diversity probably in systems like axis
because you assume that the replies are
correct so what could happen what what's
they don't think that what could happen
is just turn off their server or its
unaccessible or you don't have the link
network so because it will be just like
crash fault like you don't have access
to the server so it's simple but in our
case now you have access but it's giving
wrong values how to detect that you have
you have to have witnesses other
replicas that are applying and if they
are all coming from the same developer
group with development there's likely a
chance to have this same error and you
will not figure out that error yeah so
this is a fundamental assumption in
Valentine photos but I think that
recently we are seeing a lot of
implementations for for every important
service actually in different languages
so this might not be a barrier and
another thing to add is faults are
costly so what do you do when you have
failure
so you are taking the cost when the
failure happens so why not just think
that before so I think it makes sense
thank you I can take questions offline
yeah thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>