<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ron Pressler - Why Writing Correct Software Is Hard and Why Math (Alone) Won’t Help Us - Curry On | Coder Coacher - Coaching Coders</title><meta content="Ron Pressler - Why Writing Correct Software Is Hard and Why Math (Alone) Won’t Help Us - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Ron Pressler - Why Writing Correct Software Is Hard and Why Math (Alone) Won’t Help Us - Curry On</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dWdy_AngDp0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay um so how do we write correct
software if you guys read hacker news or
read it the answer is simple you just
use Haskell but Haskell is really so
2015 if you ask people now you it's also
very easy just need to use address now
in this talk we'll try to understand the
difficulty of writing correct programs
understanding of problems thoroughly is
a prerequisite to finding solutions and
as we'll see this problem is very very
hard but without it there is no
computation my name is Ron Pressler I'm
the founder of a company called parallel
universe and in the past year I spend a
considerable amount of time working on
formally specifying and verifying
complex distributed algorithms which is
how I became interested in this problem
uh the story of theoretical computer
science begins about 100 years ago with
the growing success of mathematical
logic the mathematician David Hilbert
laid out a program to formalize all of
math he wanted to prove the formalism
consistent and complete and to find a
decision procedure an algorithm for
determining the truth of every formal
statement Hilbert believed that anything
that can be formally reasoned can also
be fully understood and by that I mean
all of its properties formally proven
the 19th century view on the limits of
knowledge was expressed by the maxim
ignoramus at ignore a Bemis meaning we
do not know and we will not know but for
his retirement in 1930 Hilbert gave a
lecture in which he challenges this you
he said for the mathematician there is
no ignorance and in my opinion not at
all for natural science either the true
reason why no one has succeeded in
finding an unsolvable problem is in my
opinion that there is no unsolvable
problem in contrast to the foolish
ignorance our credo offers we must know
we shall know and those words are
written on his tombstone but unbeknownst
to him
his program actually had been demolished
one day earlier when Kurt gödel in
tentatively announced his first
incompleteness result good old
incompleteness theorem demolished the
first two pillars of Hilbert's programs
and in 1936 Church and Turing broke the
third that is a decision problem with
proofs of what is today known as the
halting theorem the halting theorem
which is computer sciences first ever
theorem was proven in both churches and
Turing's papers in 1936 but the theorem
is so fundamental to the essence of
computation that it's no accident that
appeared right alongside both
definitions you defined computation and
halting ensues it basically says that
there can't exist an algorithm that
decides whether any given program ever
terminates the proof which is given here
in a very shortened form is based on the
idea of diagonalization or
self-reference so we assume such an
algorithm H does exist and if it does
then we can easily construct a program G
that inverts the result of H and then we
do diagonalization we pass a G to itself
and we get a contradiction between what
G does and what H says it does if H says
that it never stops and we stop if H
says it does it does stop so we loop
forever therefore there are no solutions
to the halting problem now this theorem
has some immediate corollaries they tie
it more directly to the problem of
software verification the first is a
simple result about reach ability there
is no algorithm to determine whether a
program starting in one of a set of
initial States ever reaches a certain
state and the proof is a simple
reduction from halting a more
interesting corollary is Rice's theorem
now programs can compute what's known as
partial functions if they stop given
some input we consider the output that
they write on the tape to be mapped to
the initial input that's the function if
they never stop for a given input we
consider that input unmapped so
the partial bitch so Rice's theorem says
that for any non-trivial property of
partial functions and by that means any
property that isn't true for all partial
functions or for none of them so for any
part for any property we cannot write an
algorithm to decide whether a program
computes a function with that property
so for example there's no algorithm to
determine whether the output of a
program is always the square of its
input and this theorem is also proven by
reduction from halting but for our
purposes I think that the most
interesting corollary of the halting
theorem is what we'll call the busy
beaver theorem and I think it's a most
important because it captures best the
notion of the complexity of analyzing
programs giving it some quantitative
measure and also because this result
applies to both dynamic and Static views
of programs namely their behavior as
they run or their global properties
which we can maybe prove using logic so
it helps to demonstrate that those two
views are really two sides of the same
coin in the dynamic version let's define
the following function on the natural
numbers the maximum number of steps a
halting Turing machine with n control
states may take before halting Turing a
control States that's that's basically
the program for the Turing machine and
it basically says the program of size n
this function is well defined because
there is only a finite number of Turing
machines of size and a subset of them
terminates we count the number of states
for those subsets and that's the
function and the theorem says that this
function is not computable the proof of
that is simple we say let's suppose that
it were computable and if so we could do
the following you give me a Turing
machine I count the number of control
states it has the size of a program i
compute f of n because our assumption
says that f is computable i get a number
and then i run your program up to that
number of steps and if it doesn't
terminate by then I know it never
terminates because that's the definition
of F and we get a decision procedure for
halting there is no decision procedure
for halting therefore F is not
computable now not
if not computable any function that is
known to be greater than F for all then
is also non computable because that
would give us a lower bound and upper
bound on the number of steps a halting
machine takes and we can apply the same
procedure and solve halting so F is
greater than all computable functions
the very same argument applies to the
static views of view of programs now to
understand this result first you need to
know that any for any given logical
theory with a finite set of axioms we
can write a program that takes those
axioms and start applying deduction
rules one by one and therefore enumerate
all theorems of course it never finishes
because it could be an infinite number
but we can start doing that so this time
we'll define the function G of n to be
the maximum size of a minimal proof of
termination for a halting machine of
size n so again we there's only a finite
number of machines of size n subset of
them terminates each of them has at
least one proof of termination because
if they terminate the trace the
terminating trace of the program itself
can be used as a proof so each of them
has at least one proof of termination
there may be smaller one so for each of
them we take the smallest proof we take
the maximum over all the terminating
ones and that's G of n so by the very
same idea if G of n were computable then
if you give me a machine of size n I
just compute G of n get a number and
then I use that the description of that
machine as a set of axioms and I start
in numerating proofs because G of n is
finite eventually i'll numerate all
proofs up to that side size and if none
of them is a proof of termination of the
program i know that the program never
terminates by the definition of G and
they have a procedure for deciding
halting which is impossible therefore G
is also non computable so this corollary
of the halting theorem captures this
fact there is no computable function
tying the size of a program to the
difficulty of proving that determinant
regardless of whether we're interested
in dynamic proof technique or a static
proof technique by the way while we
can't compute F for all then because
that's what we just proved we can
compute it or lower bounds for it for
some values so for N equals six the
program just six instructions assuming a
two symbol alphabet for the Turing
machine F of n is greater than seven
point four times ten to the 36,000
that's a difficulty of showing a program
of six during instructions terminates
now another question we may ask is this
so now we know that we can't come up
with an algorithm to decide whether any
arbitrary program halts
but are there specific programs that we
know of that we can't tell whether they
halt or not even if we tailor a decision
procedure just for them well it turns
out that as a result of girdles second
incompleteness theorem such a program
should exist we just didn't know how big
or small it would be and just a couple
months ago a Turing machine with less
than 2,000 control States was found that
cannot be proven to ever halt or not so
that Turing machine would comfortably
fit in a 4k challenge it's comparable to
a Python program of about one or two
hundred lines and the result can
actually be improved further so this
means that there are very small programs
100 lines of Python whose behavior
cannot be proven by math and I don't
mean that the proof is too long or
laborious to be feasible I mean that
mathematics is not powerful enough to
ever prove the behavior of that small
Python program even in theory so in any
event with the work of good old Church
and Turing Hilbert's program at least as
originally planned was laid to rest 80
years ago by simple compact mathematical
objects that can be formally reasoned
about but not fully understood those are
computer programs the church-turing
thesis as it is presented today is that
the conjecture that any computation done
by any realizable physical process in
the
universe can be computed by the
universal computational models described
by Church and Turing so in a census
means that anything in the universe in
the universe could be formalized and
simulated as a computer program now as
mathematics as an academic discipline
not as a tool is concerned with what's
inside that small circle and computer
science is mostly concerned with what's
outside it and actually as we'll see
right away even those programs that do
lie within the small circle almost all
of them are certainly most of them are
only provable in principle not in
practice so I think it's safe to say
that computer science isn't math this
actually was said by Turing but but
we'll get to that all right hmm so
whether verification halting and
corollaries is decidable or not doesn't
quite capture the difficulty of the
problem because Turing machines can have
infinite behavior the problem of
verifying them could in a sense be
considered similar to the problem of
sorting an arbitrary list of either
finite or infinite length or finding the
maximum element in such a list of finite
or infinite length so both of them are
equally impossible but we know that when
we restrict finding the maximum element
or sorting to the finite case their
difficulty is very different so let's
try to figure out how hard verification
is so I took another 30 years for the
issue of algorithmic difficulty to be
properly defined the theory of
computational complexity was born in
1965 in a paper by URIs Hart Manus and
Richard Stearns which begins so in his
celebrated paper Turing investigated the
compute ability of sequences functions
by mechanical procedures and show that
the set of sequences can be partitioned
into computable and non computable
sequences one finds however that some
computable sequences are very easy to
compute whereas other computable
sequences seem to have an inherent
complexity that makes them difficult to
compute
now in his 1993 turing award lecture
heart mana says that Eve you'd
complexity was a like physics
he viewed complexity as a law of nature
and he said it's a law of nature just
like in physics now before I go on it's
absolutely crucial to understand that
complexity theory is concerned with the
inherent complexity the difficulty of
problems not in analyzing the complexity
of specific algorithms of a solution so
the complexity results they provide
bounds for any algorithm known or
unknown for solving a problem one more
thing we need to to make clear is that
questions have quantum computing aside
it's important to understand that
complexity results apply to humans or at
least we've never encountered any human
violating any complexity bound ok so it
was no surprise I think that it was our
very same question on the precise
difficulty of the halting problem that
allowed heart Madison Stearns to show
the inherent difficulty of some problems
and prove what is now known as the time
hierarchy theorem which i think is the
second most important theorem computer
science which basically says that
computational problems form an infinite
proper hierarchy of difficulty so the
problem as I'll presented here is
sometimes known as bounded halting and
asked the following what is the
complexity of deciding whether a program
M stops an input X within n steps so if
we just simulate the program and we run
it and assuming that our simulation
procedure has low overhead we can
certainly do it in less than an order of
n but can we do it faster so let's
assume that we can and that we have so
given some n we have a program H of n
that does exactly that and it runs so it
answers whether M halts our acts on X
with intercepts but it does so faster
than for the sake of this proof we need
to do n minus 2 you'll see why by the
way this is a an approximate proof it's
more similar to
proofs by cook on Ram machines rather
than Turing machines anyways we take the
exact same proof the exact same proof of
the halting theorem we use
diagonalization we define G which
inverts agent H ends result and we pass
G to itself and we get a contradiction
because if H of n says that G running on
G does not stop with in n steps it
completes doing that in n minus 2 steps
and then it takes us just one more step
to return true and we have finished in
with in n steps and we get a
contradiction the same thing the other
branch so the complexity of this problem
is an order of n we can't do it any
faster so put simply you can't know what
a program will do any faster than
actually running it's an interesting
question is can we generalize from one
input to another so this is for a
particular input X can we work really
hard much bigger than n 2 to the N on
some inputs so that later on will be
Fastenal other inputs like study the
state space of the program and then and
then give fast answers for the rest of
the inputs so I call that amortized
bound halting unfortunately I can't show
the proof it's an immediate corollary of
bound halting and the answer is no so
this result actually answers a question
on the complexity of the halting problem
now because we can't answer any question
about a program faster than running it
as soon as a program reaches a state
that it's visited before
and again neglecting the the overhead of
actually writing down which states we
visited we can immediately tell the
program never halts so in the worst case
the complexity is at least the number of
states of the program and if we're
talking about Turing machines so I'm not
talking about the control States the
state of the program including the tape
so in the general case so this is a
complexity of the halting problem in the
general case a Turing machine can have
an infinite number of states so the
worst case complexity is infinite and
we get the undecidability of the general
halting problem as a special case of
bound holding this is a more general
theorem and as we'll see and this is the
interesting bit this result is extremely
robust it holds even if we strip away
the computational power of our machines
from Turing completeness to pretty much
anything else this holds for any
computational model almost another
question is so far we talked about
Turing machines which is really a very
general object and it's a computational
model it's not really a programming
model in practice we write our programs
in Nice programming languages with
abstractions maybe even semantics so can
we exploit the language semantics to
answer the question faster so we'll
address this question
head-on later spoiler alert
No okay
before we go on I'd like to states the
verification problem in the most general
way the the holy grail of verification
what what we want if we want most if we
can have it so what we really like is
given a program m and a statement Phi
and some logic that can talk about the
properties of M we'd like to know
whether M satisfies Phi so it's easy to
see that this very general problem
contains within itself halting and rise
and reach ability and all that so it
certainly can't be easier now in
mathematical logic if M satisfies Phi we
say that M is a model for Phi so we need
to check whether M is a model for Phi so
this problem is actually known in
literature as the model checking problem
not to be confused with model checkers
are specific algorithms for solving the
model checking problem so how we
determine whether M is a model for fire
or not is up to us we can use deductive
roots if you want we can use time so
whatever we want the results of the
compare computational complexity bounds
that I've shown and will show more
applies to all of them so no specific
algorithm can be faster than the general
case the the general complexity bounds
again the results apply both to machines
and humans so to try to tackle the
verification problem let's start
restricting our machines so one way to
do it is to restrict ourselves to
programs that are easily proven to halt
how do we do that we can write them in a
style or even in a language that does it
for us that requires that all loops or
recursive calls have have a counter that
is decremented with every call
so these languages are called total
languages and they allow writing a
useful subset of all total programs but
if we look at our diagonalization proof
for bound halting we see that even if H
n could only accept programs M that were
written in total languages H and itself
would still be able to be written in
that total language because it only
needs to run for up to n minus 2 steps
so we can have a counter the same thing
goes for GG would also be written in
that language so we are allowed to pass
G to H and the same result there's
absolutely no change so verification is
still the order of the number of states
of the program
don't languages make no difference we
can still ask what is the relationship
between the program size and its number
of states in two languages before we've
shown that for general Turing machines
it's non computable I'm sorry to say I'm
not exactly sure about the precise
answer I do know that it must be greater
than all primitive recursive functions
and it's definitely greater than the
Ackermann function but it may still be
computable in any event for our purposes
it's still as good as infinite and total
languages actually don't help
verification at all
now this 10 line Java or C example is
written in this total style all loops
have decrement encounters and it clearly
always terminates it also doesn't do any
tricky mathematical operations just
addition and subtraction yet
mathematicians trying to prove this
program
never crashes never each is that line
there for 270 years and that's because
this program cache is if and only if
goldof conjecture is false actually for
32-bit integers we know that never
crashes because we tried all of them but
we don't know the answer for yeah I
think we know up to 10 to the 14 we
don't know for 64 bits there are other
ways we could restrict our computational
model there are languages that only
allow writing programs that run in
polynomial time another model is
pushdown automata which is very
interesting in itself but we will jump
directly to finite state machines the
weakest computational model that's still
useful because there we can find the
most interesting results so automated
methods for full verification of finite
state machines called model checkers
have existed since the 80s and they're
probably the most common form of formal
verification techniques used in the
industry and it is precisely because
practical automated methods exist that
the complexity of the problem has been
studied extensively again the results
I'm going to present are about the
inherent complexity of the model
checking problem not for specific model
checkers they apply to any proving
mechanism human automated etc and these
are to the complexity bounds for two
different logics that can be used to
describe the properties of program of
the property of I and you'll see that if
we treat Faiza constant then the same
result applies to find state machines
it's the order of the number of states
but the familiar complexity classes are
sometimes too crude we talk about worst
case and sometimes you know the
real-life cases are easier so
parametrized complexity is a theory
relatively recent
that tries to address this problem in
some cases by introducing finer
complexity classes perhaps the most
important important of them is FPT fixed
parameter tractable which says that a
problem may be intractable for example
exponential more in some parameter K
which is a function of the input but it
is otherwise polynomial in the size of
the input so if in real life the the
parameter K can be kept small then the
problem would still be intractable even
though it is intractable in the worst
case so an example of a problem an FPT
is sat boolean formula satisfiability
right a boolean formula and we want to
know whether it is an assignment to the
variables which true so this problem is
the quintessential and np-complete
problem but the not even the naive
algorithm that tries all assignments is
only exponential in the number of
variables and that's the parameter K
otherwise it is linear in the length of
the formula another example is the
Hindley Miller
type inference which is why it is
efficient in practice so hindley-milner
is FPT and that's why for most programs
it is linear so a 2005 paper explores
this question and they try to
parameterize verification complexity by
modularizing programs so the idea is
that maybe each module or components of
the program could be verified in
isolation and then we put them together
and verifying the composition of
component well may be intractable the
number of components but that number
would be low like five ten or fifty it
would be tractable in each component
size and if that were so we could
basically compose programs from smaller
modules prove each of them in isolation
then combine a combined a small number
of them and we have efficient
verification but the result is negative
and the proof actually is very cool
the authors note that K finite state
machines can actually simulate any
non-deterministic Turing machine with K
tape squares and that problem is known
not to be an FPT actually it's pretty
high in the arcades it's very non
decomposable so verification is a not
tractable even in the parametrized sense
of being FPT and we do not think the
difficulty can be solved by considering
other ways of treating K as a parameter
so this 2005 result is pretty profound
it is proof that we cannot in general
decrease the effort of proving a
program's correctness using modular
ization correctness does not decompose
okay so now we finally get to the
question of language usually we don't
write a programs by drawing a huge state
transition diagrams we use nice
programming languages with nice
abstractions like I said maybe even
proper semantics and we write programs
there are in general very succinctly
recites pace so can we exploit the
structure in the source code and that
question is known as the symbolic model
checking problem and Felipe's know blend
we also wrote the previous paper show
you who studies the computational
complexity of reification writes that
from a complexity theoretic viewpoint
there is no reason why symbolic model
checking could not be solved more
efficiently even in the worst case so
there's hope there symbolic model
checker is only deal with a very special
kinds of huge structures the structures
are the transition diagrams those that
have a succinctly presentation okay so
the result is that restricting ourselves
to programs that have a 16-4
presentation does not help at all
the problem is pspace-complete for
finite state machines and it happens to
be an order of the number of states we
cannot reduce that bound which is so
pspace-complete for fine state machines
is the same as simply expanding the
state diagram of the program and
forgetting about the code so indeed it's
not only the worst case this has been
observed in many instances from your
ability to equivalence and can now be
called an empirical fact it's not only
the worst case by the way and see the
Kripke structures there that's the
abstract the transition graph just the
way of talking about programs in the
verification community without talking
about source code representation just as
a quick example to give you intuition
for why very fine finite state machines
is so hard
so consider a programming language that
has nothing but boolean variables and
function calls there are no higher-order
functions no recursion and no iteration
at all so such a language will clearly
only describe finite state machines by
that I mean that it's easy to tell from
the source code how many say Sarah
that's what I call a finite state
machine program so you can see over here
equals B equals C etc no loops no
recursion no higher-order functions in
the end there we have six boolean
variables they're all tried once with
true once with false so we have two to
the six number of states so yet very
verifying this not even a useful
language is still pspace hard because
this trivial program you can write a
program like this and it directly
encodes this problem of determining
whether this predicate calculus formula
is true or false it's known as it t qbf
problem true quantified boolean formula
and it is pspace-complete
again it's a quintessential
pspace-complete problem and unlike Sat
solvers that for some magical way work
we don't have good solutions for that so
I think that in light of all these
results we can state this informal
theorem there does not exist a generally
useful programming language where every
program is feasibly verifiable and there
cannot exist one okay so
now we know why writing correct software
is hard because it has to be but while
we can't verify all programs all the
time regardless of how they're written
there's nothing stopping us from
verifying some programs some of the time
for some properties now cheering
understood full full well the difficulty
in writing correct software in a lecture
to the London mathematical society in 47
he said the Machine interprets whatever
it is told in a quite definite manner
without any sense of humor or sense of
proportion less in communicating with it
one says exactly what one means trouble
is bound to result I need a solution and
well the stirring solution types we
shall need a number of efficient
librarian types to keep us in order so I
mostly mention types because I imagine
that they're popular in this crowd
that's interested in programming
languages I also want to talk about
doctor proofs and model checkers sadly
there's not enough time but as a
motivation for discussing types I want
to ask the final question
this is Rice's theory Rice's theorem
again and yet not only is it possible
but it actually quite easy takes no
effort on our part to ensure that say a
a C or a java or a Haskell program
returns an int how is it possible any
ideas anyone share just shout
yes but white yes no but but how but
Rice's theorem says it's impossible how
is it possible okay all right so some
people I that has to do the right some
people I was hoping to hear that answer
they seem to assume that does anyone
think by construction no good all right
so some people think I Niecy it's if you
reread it and and hacker news that there
is no violation because the property is
correct by construction this is
absolutely false Rice's theorem says
absolutely nothing about construction
yeah so so it doesn't say anything right
about construction for a good reason and
that is because if we could prove things
by construction then we'd have the
following efficient verification problem
I would write my program in Python and
then because it's so easy you translate
it to Haskell or Idris and we'll a
problem solved so this is impossible
because the complexity applies also to
humans so the real answer must directly
address Rice's theorem so a property
would either need to be trivial or it
may not be a property of a Turing
machine up the the function the partial
function will not be computed by a
Turing machine so in the case of an
integer what the type means and you said
correctly is that either the program
never terminates or returns an integer
but this is true for every program
because every program either never
terminates or it turns something and
that something can be interpreted as an
integer so not even a single bit of
information is given to us now some of
you recognize that this is not always
the case
so for example in high school you can
relatively easily show that a program
returns a string of numeric digits so
this specifies a lot of bits because we
restrict every byte to ten out of 256
possible outcomes now is that not a
violation Rice's theorem this one is
clearly not trivial
well that's because the property is not
generated by a Turing machine but by a
very small finite state machine as I'll
now show before that we should say a few
words I wish I could talk about this
more about abstract interpretation which
is an elegant and mathematically very
deep solution to rafin which was
introduced in the 70s and tools and
employ are usually called static
analysis tools
this technique is fully automatic and it
avoids the intractable complexity bounds
by being imprecise so the very rough
idea is to consider so given a program
that we want to verify we consider
another program that is a sound but less
precise abstraction of our input program
and that abstract program would be a
small finite state machine and then we
could model check that so the complexity
would be an order of the number of
states of the abstract program so for
example in this in this program on the
left this program has an infinite number
of states and but if we only care about
the parity of X to second we can
abstract that program to this small
finite state machine program and we
still know X is parity in two out of the
four labels in p2 and p3 yes what what
is it
the narrowing here well we'll discuss a
technical I'll discuss that with you
later let's not get too technical here
it's just this is an introduction to
types anyway so one one of the one of
the several ways of looking at types is
as an abstract interpretation so we in
the in the concrete program we have a
variable X of type int that can take the
values 1 2 etc and we abstract all of
them as the value called int in the
abstract program so the values of the
abstract program are the names of the
types in the program so how many states
does the abstraction have it's about as
many as there are concrete types in the
concrete program or the particular chain
of function application so every state
in the abstract program would be the
name of the function we're currently
evaluating and it's either input or
output type so maybe it's the number of
concrete types times the number of
functions depending on the structure of
the program now the number of concrete
types can be greater than the number of
types that we actually spell out because
people write programs with generic types
and polymorphic functions but that this
is not a problem because just like we
can have a concrete function that takes
3 and returns 5 the abstract program can
take int as a value and return list
event as a result so you using nothing
but our result of bound halting namely
that the complexity of verification is
in the order of number of states and now
that we understand abstract
interpretation and the relationship to
types we can immediately obtain our
bounds for type inference algorithms so
the lower bound for type inference
algorithms is the number of concrete
types in the program so in a program we
and we got that result using nothing but
the halting here so in a programming
language without dependent types if you
don't know what those are so it's not
important so that number is relatively
small
and the result we get what we get is a
finite state machine and the types don't
express properties of programs the
express properties of a finite state
machine so that's why there is no
contradiction with Rice's theorem and in
fact you can take any high school
program remove all the function values
replace them with functions there are
finite state machines you get a program
that's completely wrong but it would
still type check so for any Haskell
program there's a finite state machine
Haskell program the type set completely
any IDE is completely wrong on the other
hand in languages with dependent types
so the number of concrete types can be
infinite and therefore type inference
which is still the order of number of
states is of infinite complexity and
therefore it's undecidable and it's
really as simple as that all right
so in his follow up to a very famous
article no silver bullet
where he argued that will never again
see another order of magnitude increase
in programmer productivity he wrote that
in 84 he was right so far at least Fred
Burks writes is Einsteins statement that
nothing can travel faster than the speed
of light bleak or gloomy the very nature
of software makes it unlikely there will
ever be any silver bullets okay so now
we have proof of that but what can we do
so we've seen that computer science
studies objects computer programs that
are composed of very basic components be
they beta reductions in in lambda
calculus or Turing machines Turing
machine instructions that on their own
each of them is easily subject to full
mathematical reasoning but it is their
interaction that is essentially
essentially essentially intractable now
I think this is similar to physics where
for example the gravity equation is very
simple and that equation composes
beautifully you can have multiple
instances of it for multiple bodies it's
all very equational equational reasoning
everything composes but still more than
one instance of the equation more than
two
gravitational bodies and gravitational
interaction the problem is already
chaotic and there's no closed-form
solution so I think we should take a
page out of physics his book and embrace
empiricism so let's consider the
verification problem again now we must
restrict either the left hand side or
the right hand side so I hope that by
now it's clear that we'll never find
generally useful category of programs
for which verification is always
affordable but we can make the following
obvious observation all programs we care
about are created by people and if you
like we can consider humanity to be one
large Turing machine that generates all
machines m and it is very likely in fact
we know it to be the case that there are
patterns in those machines that humanity
generates and and if we study them we
would be able to maybe get some
affordable verification now we can't
analyze humanity as a program the only
viable method is empirical study of the
program's humans right and I think we
should begin with a taxonomy of program
types size cost etc now this observation
also makes some useful restrictions on
the right hand side possible so far all
the properties we've seen like halting
reach ability were global ones but in
practice we know and we know that
empirically that some local properties
are also very useful to prove a great
example of that is memory safety which
is known to be a source of common very
common and very costly bugs and we may
be able to find other useful local
properties but this too would require
empirical study a great place to start
would be a taxonomy of bugs by cause
prevalence association with program
types and maybe most importantly cost
such empirical studies would require the
cooperation of academia and industry
with that empirical knowledge in hand
we'll be able to tackle the interesting
special cases using ad hoc mathematical
techniques just as physics does
simplifying assumption
heuristics approximations etc just one
such a really beautiful example of
empirical research is a recent study at
the University of Toronto that
discovered that a vast majority of
catastrophic failures in large
distributed systems is YouTube
programmers not thinking enough about
error handling code the code is there
they just don't put much thought into it
and this is clearly a psychological
effect now I'm not saying that computer
science is physics your sort madness
actually wrote a paper explaining why it
isn't physics and obviously it isn't
math but he said he says that computer
science differs from the known sciences
so deeply that it has to be viewed as a
new species among the sciences I'll
conclude with something extra Fred
Brooks wrote complexity is the business
we were in and complexity is what limits
us um some people seem to be
philosophically offended by this notion
and they try to fight the proven
limitation they are under the impression
that the world and now I'm not just
talking about computation is essentially
simple and it is stupidity of people and
institutions that needlessly complicate
it and if we were only to apply careful
mathematical reasoning we could find
solutions to anything now computer
science is the very discipline that
proved that essential complexity arises
even in the smallest of systems some of
them are unfeasibly proven and some of
them just are beyond the power of math
even small 100 line programs yet
sometimes it is computer scientists and
I think more software developers who
attempt to challenge the very foundation
of their own discipline complexity is
essential it cannot be tamed and there
is no one big answer the best we can do
in society as in computing is to apply
lots of ad hoc techniques and of course
try to learn about the particular nature
of problems as much as possible
thank you
yeah time for questions
you uh you first
oh we do we do the question is how is
related to your business uh no I it's a
kind of tangential but we had to use
some of our algorithms were too
complicated to just write in debug and
we had to use formal methods so we're
using formal methods pretty much all the
time and that's how I got interested in
the topic
with what more color yeah
they can't be formalized they can't be
proved yeah it is what okay but the
four-color theorem is kind of going the
other the other way around it's starting
with math which is a lot simpler than
computer science or physics and applying
a techniques that I've been studying
computer science and also in math to
solve a mathematical problem well if you
want to axiomatically call computer
science if you want to call computer
science math then yes actually I think
something nice here that Paul Cox shot
he was a computer scientist and
political economist at the University of
Glasgow and he said that actually
something very similar to what you were
implying Turing 1936 paper in the
computable real numbers marks the
epistemological break between idealism
and materialism in mathematics
prior to Turing it was hard to get away
from the idea that through mathematical
reason the human mind gained access to a
higher domain of platonic truth during
his first proposal for universal
computing machine is based on implicit
implicit rejection of this view his
machine intended to model what a human
mathematician does when calculating or
reasoning and by showing what limits
this machine encounters he identifies
constraints which bind mathematical
reasoning in general whether done by
humans or machines Turing starts a
philosophical tradition of grounding
mathematics in the material and hence
ultimately
what can be allowed by the laws of
physics the truth and mathematics become
truth like those of any other signs
statements about sets of possible
configurations of matter now so I don't
know if I necessarily agree that
mathematics has changed in that way but
it's certainly different so I wouldn't
call that math but if you want to call
that mass so now math is different
yes yes is a social up math is whatever
mathematicians do yeah okay yeah yeah uh
yes so what we're using teal a plus the
entire program is will over fifty
thousand lines of code maybe even a
hundred thousand specification is about
a thousand lines fifteen hundred would
three different refinement levels so
there's at least at least a 10x if not
100x difference between the size of the
formal specification and the actual
algorithm and its scale a teal a plus
actually specifically scales very nicely
to very very large real-world systems
and it's also very easy to learn which
is basically why we adopted it
the
the Venn diagram
yes that's that's because of girdle
Church and Turing they showed that
Hilbert thought that they're equal and
what why is it bigger there are more
things that cannot be proven the things
that can there's actually a simple
cardinality argument why that must be so
but everything that can be proven
formally must be formally reasoned at
least
so the formally reasoned must contain
the proven because completely outside
yeah I don't know other things in
physics so the church-turing thesis
basically says that there is nothing
outside you know in a way you can think
of and but that's a that's a conjecture
hypothesis we don't know if it's true so
far we haven't found anything but so far
anything we know in the universe would
fit as a computer program simulated as a
computer programmer
it's kind of hard to say because the
whole idea of formal specifications is
that's the part where you think so
once you think about the program and you
think and you write it down and you
reason about it formally then it's very
easy to write afterwards if you just
write it supposedly you also need to put
in the same amount of thinking in
practice you don't so you postpone some
of the thinking later to debugging which
is fine by the way it's a valid
technique but you can also do debugging
on your formal specs it's actually very
nice it gives you a counter examples so
on the whole it's definitely safe sign
but it depends if you want to do formal
proofs deductive proofs then that would
be very expensive but if you use model
checkers and other automated techniques
some may be automatic proofs then
actually it is very affordable and it's
faster for some problems it's not if you
want to write a very very simple program
then I will use formal methods but for
stuff that you say ok this can be subtle
or certainly for distributed algorithms
or anything concurrent and it turns out
to be easier but it depends also on the
tool you choose some tools are easier to
use than others yes
sure so the question is whether the
machines could do the empirical studies
of course if we have machines that
gather statistics and machine learning
is basically statistical algorithm the
clustering that's what they do so if we
could use machines to study all github
and some people are already starting to
do that so people are working in that
direction it's just that I sense a
little bit of a slight disrespect among
people in programming language research
towards those empirical programs
empirical studies and but that is
absolutely essential because otherwise
you're not focusing your solutions and
you must because there is no general
solution it's a simple proof anything
else all right so thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>