<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Michael Steindorfer - W/ Age Comes Beauty: Past, Present &amp; Future of Efficient Immutable Collections | Coder Coacher - Coaching Coders</title><meta content="Michael Steindorfer - W/ Age Comes Beauty: Past, Present &amp; Future of Efficient Immutable Collections - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Michael Steindorfer - W/ Age Comes Beauty: Past, Present &amp; Future of Efficient Immutable Collections</b></h2><h5 class="post__date">2017-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uyrY9UNO19k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right thank you do you hear me like
you know better
hello hello okay in YouTube
all right thanks for coming Altima talk
with age comes beauty today I will dive
more into into the past present and
future of immutable collections as you
already heard the talk will be chest
timer talk so the idea is that I have
like twenty minutes of speaking time and
that there's 20 minutes of discussion
time luggage throughout and as you heard
like in order to make it work I have to
count on your support so well yeah so
interruptions are not only like on there
even mandatory how did this talk came
into place well maybe some of you
remember that last year at curry on
David Nolan gave a keynote address which
entitled how do how to win big with all
the years and in David's talk he dived
into like four key concepts that in his
opinion made closure success in the long
run and the four things that he
mentioned were like Lisp Trice data log
and regular expressions anything idea
like what I'm talking about today well
it's trust so what are twice the name
try comes from the word retrieval and
it's actually a data structure that lets
you search like store search and delete
paper so you can imagine it like set
data structure and tricep like one
particular property that make them
interesting and that's the de encode the
prefix of the search piece that you put
to them so on the left is like a three
birds that start all of the letter C so
the try data structure will account for
that the day have become
prefix but further on two of the words
have like a longer prefix they start
work with three little Co and the same
accounts for the words in arrived like
all of them start with F some of them
have a bit longer prefix and some of
them have another one
so try data structures were conceived in
the late 50s early 60s like
independently by two different
researchers and if you look at the
papers how did you describe that and
conceptually yeah device try data
structures you can see exactly that you
have a tree where you encode the common
prefixes and in this case we have a tree
of tables that always map from one
character to the possible subsequent
characters that can follow the character
so all these words that we have here
occur in this table as well and like
letter by letter we we say like which
which letter can follow the next one so
the second person who devised the tries
on simultaneously was a at work fret and
as you can see here it's a different
view for the same problem like instead
of like having nested tables in the tree
form with like a a really table
representation were on the top we have
like the alphabet and in principle what
used to be before and noting the tree is
more like a row in the tail well so to
summarize the properties of try data
structures it's on the positive side a
general purpose search tree and that can
cope with like variable length input
strings so all the words that we had
before Mike had different lengths and
they can like draw arbitrary large and
by design try data structures group
common prefixes so what's not so good
about them is that they are relatively
made
at least like the state lack of the late
50s and how do you conceive and because
we encode character a character in the
tree it's also like a linear time
operation so like look up assertion
deletion they will also like all take
linear time so if we skip forward
nowadays could try data structures
I used heavily in standard libraries of
functional languages like closure as I
mentioned before Haskell for scale and
so what do you mean by collection if you
look at the standard library or of the
documentation of like language like Java
you will find that a collection is
nothing else than just a representation
of a group of objects and these
representations of group of objects can
be like different data types like lists
sets maps decks multi maps it can be
have different like orderings or be
stored unordered they might be like
mutable or immutable and they can be
processed either like sequentially in
parallel or confirmed so as you can see
like collections like yeah like really
broad but try data structures can be
used to implement like most Craven of
all of them in languages like closure
rest or scale so what makes collection
data structures interesting is that they
did their general purpose tool box so
they are at your disposal at the
disposal of FL of every programmer
because the ship with the standard
libraries of these languages and as such
they should be also like balance the
time of the operations and also the
memory consumption but if you looked at
what we said before about try data
structures well they have some good
sites but you also have some negatives
attached
and that's that they are relatively
memory efficient inefficient and that
they have a linear time complexity of
the update operations just a
clarification and it's linear time in
the in the length of the key length yeah
not linear time in the number of
elements yeah right important
distinction hmm are there any other
questions so far
regarding try to destructors the past or
the principle to sign up so okay yeah so
what well you can also like sometimes I
get away with just comparing parts of
food so yeah but let's say like if you
just interested like if it's not in
contained in the set of like keys that
you have stored let's say you're looking
at a work with the letter set and you
just store words as before like we've to
let us see and F then you can like
immediately distinguish that that prefix
is more present there so it's not always
like in the full length of the keychain
also be linear in the number of
yeah that's another dimension yes right
yes
all right so in in the context of
collection data structures like what
changed would change the adoption of
hash fries and made them actually like
feasible emember
are for implementing fast immutable
collections was a paper by field back
billion in the year 2001 and actually he
proposed changes that address the two
weak points that we saw before so how to
make them fast and space-efficient and
in order to make them fast what you
introduced this are what he merges like
is to use caching for these tired
try data structures and what hashing
does is in principle instead of like
having arbitrary length keys you just
make them fixed length keys so in the
context of Java for example or Scala or
any like language that runs on the JVM
hash code is usually out 32-bit locks
and it doesn't matter if you're looking
like for for like search key found or
something longer or something short you
will always have like the same length
few key but hold on before we looked at
character a character when we encoded
the try data structure and the prefixes
of that so what shall we do here
okay select using bit by bit and making
a tree out of that okay
good idea very fine based again okay by
five like a number as and with three
okay
any other ideas okay what turns out five
is a lucky number
now essentially what you can do is you
just chunk up and divide the wrong
screen in bigger groups and again each
of these individual like five bit
elements are actually now becoming
letters in the alphabet so each letter
is actually five bits long and like can
add up to like thirty two different well
is that we can take and well if that's
already after that twisted we will build
up will actually use that alphabet on
the edges so we will encode the tree not
based on the crumb directly but on the
hashes of the content and that gives us
like nice properties because the hash
code is like thirty two-bit locks and if
we check the top like in sequences of
five bits we limit essentially the
depths of the tree so to make it a bit
more visual I will switch back from from
screens that we had before and to
integers why integers because like
integers the hash code of the end is the
value itself so that makes like sings
for demonstration purposes and if we now
take the concept before and build a
retreat out of these integers we get
something like that and if you remember
the two are the two illustrations that I
showed you in the beginning of like the
tree tables and the second one was the
table representation itself that comes
very closely but that's not memory fish
in the door is it
so if I want to store how much elements
do we have here
seven elements and I have four times
32-bit arrays what can we do about that
how can we do that like how can we do
that how can we make it more memory
efficient
so what Becker's paper proposed was to
use bitmaps in order to designate which
of the slots is actually in use and by
using deep maps we can then subsequently
compress that and remove all the empty
slots and by doing that yet we get like
a concise representation on what we had
before so in the code skeleton we have
like a circle with a bitmap and then an
array that can grow up to 32 elements
more you can't hear me back there okay
all right
so the properties that you can expect
from such a search tree is that it's a
wide but like very shallow tree and
essentially takes you at maximum like
seven steps to find your element and in
terms of like complexity it's it's not
like a constant time operation as like
in the hash table but like the seven
steps is essentially concept so as the
scallop people for example will call it
mm-hmm
what your campus is that a hash table
sorry to me blow you up or catch dishes
that a hash table so it has people
you'll have if you're lucky you're going
to get a hash but rather the first price
right yeah that's like one of the
differences and I've become later to the
point in why these three structures are
actually like in favor and like
immutable collections compared to let's
say flat tables but in essence it's that
you can copy smaller fragments and by
copying smaller fragments like for
deriving a new data structure from your
old one
you're winning something there so like
your update operations get much faster
but you paying in sort of like cache
misses in directions for example that
you have to get through that's right so
it's under the structure II also have to
store the original hash keys just as
you'd imagine
yep right that's true there's another
disadvantage also with respect to
ordinary tries which is the you don't
get the new donut store records with all
the prefixes that you encounter a large
way to the word edit some algorithms you
actually want to get the reason that you
surprised yeah you like vegetable
prefixes for example in some pattern
matching algorithms yeah
I agree and the differences so there's
also like there tries using in databases
or like yeah as indexes like it for
databases especially in the context of
in memory databases and they're the ways
that they don't soar or they don't hatch
because they won't range queries for
example yeah so this is exactly what
you're saying that but for collection
purposes for example like hashing gives
like different advantages and they are
also like nicely compressible as as you
saw before like in the picture and one
of the advantages as well is by by using
the hashes you can shortcut for example
that's what what elaborate before so if
you know that one prefix doesn't support
for example in the whole try data
structure you just skip that you know
it's not there
some other questions
all right
so that's the basis of what was
introduced in 2001 by feedback box so in
a sense using hashing for try data
structures and that means like making
variable length keys - fixed length keys
and the second core idea was using
bitmaps in order like to compress the
empty space out of the tweens so but
recently there was also like lot of
research going on in order to yet
improve these data structures because
like collection data structures they're
like at the heart of programmers and
they're like widespread and widely used
so they based on like that
representation you can make them even
faster like an exporting cache locality
for example of iteration or even like
using structural operations for equality
checking and and then get yet other
speakers
so he first showed us the Amish person
that representation was a gala
smartmemory vision was just as the part
with some transfer about cash
so is it as a cash a position to do
hashing it at all are ya using that
approach or just American that they
regret putting branching on every
possible bills
oh you mean branching about every
possible building yeah but so so for
immutable data structures there there's
a there's a trade-off between like hook
up time and time for for updating the
data structure so if you want to modify
it and empirically it is shown that ah
there the sweet spot is around like
grouping four to five bits for example
so you can for example improve the run
time by just the using larger sequences
so like six seven or eight bits for
example but then you pay once you your
going to update notes because then you
have like 128 like large arrays or note
that you have to copy for example so
it's it's really trade-off and and the
design that you can get both like fast
look ups and also like relatively fast
update operations at the same time
so in principle yes and that's also like
a subject I'm interested in but in terms
of like how you spread out that's that's
rather complicated because try data
structures big spend lazy so as I'd like
if the prefixes differ of let's say with
a set of two elements that we insert and
both have like different prefix on the
beginning both of scored in the root
node actually well only if they like
only if they differ in the prefixes I
grow the tree and the same goes if I'm
adding elements like the tree grows as
needed for example and exactly that
growing the lazily growing of the tree
makes it very complicated to have these
variable lengths for example sizes of
nodes so there probably other ways but
like not in terms of how much notes you
group together should be together that
you're giving a talk about this data
structure because it's currently the
best choice on CPUs for immutable
collections of strings I think I can
think of five or six other data
structures for this patrician trees and
pad arrays and judi trees and permit
fascinating yeah so is this the correct
letter ah I would say it's a current
winner because it's a it's it's an old
concept that was uh easily let's say
refresh let's say with new technology
and the adopted towards uh some of the
news and so one things that makes or
that make these data structures popular
was actually CPU support for bit
counting operations that was introduced
like in the way in the mainstream let's
say we've endured has word processors so
yeah I agree like there were plenty of
other candidates but these are like a
viable default divorce let's say
they get all of the a lot of like checks
mark there's a good explanation for good
locations for requests like it ah yeah
right so so happy replies that's like
what my girl proposed are they are
widespread in many languages so first
they were implemented by closure then
later they came back to scanner and now
they are spread like throughout like a
large ecosystem like including Haskell
airline and plenty of like other
languages so and most of the
implementations are actually based on
backwards Richmond paper from 2001 and
just recently let's say from 2010 2011
on they were there was like much more
research interest in also like making
these data structures concurrent for
example also like in investing in
parallelism working these concepts to
vector implementations for example so
immutable sequences and also improving
the memory efficiency for example of
this concept
how do you do that well it's yep what's
the baseline
well the baseline the baseline is closer
since Carlos implementations for example
of immutable hedge price as they are in
the standard library and here
particularly I'm talking about equality
checking so you're comparing two trees
to each other or like two sets two maps
to try based data structures so you you
want to figure out are they equal in
contact and what you usually have in
collections is this iterator based style
traversal and abstraction so you iterate
over one collection and semantically do
we look up for each element in the app
and tries offer you also like structural
operations so for insertion deletion and
lookup structural operations are the
default see because that's by design how
the data structure operates but in the
context of collection libraries it's
also important that you look at the
largest set of operations like iteration
for example equality checking mapping
each element of a collection like you
are applying a function to each element
of an collection and here particularly
it was it was the case that the
structural operations were not
implemented are for equality checking
and the reason for that is that in order
to perform structural operations you
have to keep the data structure
canonical selected two data structures
that have the same content are also like
structural equal that's something for
example it's more natural in Haskell
then maybe like in other languages and
here it's like an implementation detail
of the Tri data structure itself so by
making both data structures canonical
you can apply structural equality
checking and structurally quality
checking you are much faster than like
doing it logically
right
yeah yeah but that's the same symbol
back of operations like a Union
intersection equality checking and
there's like plenty of more things that
you can do structurally on the tree
itself and these things are not that
widely like a spread and implemented for
example also like new languages out
there
so coming from like beggars Hamed
implementation uh so one thing that you
might note color-coded berries that
there's a mix of data elements and like
SAP or like references to sub tables so
and these are actually occur just based
on the hash holds the defeated so with
different content you get like different
orderings and if we talk about iteration
for example cache locality these data
structures just like one particular
drawback because if you want to iterate
let's say over this set and give all the
elements what you would have to do is
either traverse each node twice like
yielding all the elements per node or
what you otherwise would get is you
would like to go up and for like a down
and up a lot and that also like would
result in a lot of cache misses because
like you switch in context a lot your
trust in order to reversing the tree so
something that are like changed recently
is the color encoding was adopted in
order to group likewise elements
together so all the references to sub
trees stay on the right for all the data
elements stay on the left so if you now
iterate over the three data structure
you can use all the data elements first
before you traverse into sub trees and
that gives you for example like the
speed ups of 6 6 X what I mentioned
before for iteration for example
compared to the traditional like
Headroom triangle mutations so what do
you need in order to make that work well
essentially yet another bitmap because
you need to make explicit what notes are
data elements and what are what cells
represent data elements in what cells
represent like sub trees to distinguish
the prefixes further
so well they also like got much much
smaller memory footprint compared to the
previous state of yeah and how comes
that what a show to here is it's just a
permutation of the elements so it's a
permutation of the elements grouping
things together if we now go from a set
data structure to a map data structure
as it is for example implemented in
closure the convention is that key value
pairs are either stored in line so like
next to each other or if I'm happen to
have like a subtree the convention is
that the key slot is left empty and the
value slot is like we use as a reference
to sub trees and by doing the same trick
of permuting the elements we actually
like can encode like different lengths
slices in our array so like all the key
value pairs of like lengths to ones like
the the references to sub nodes just
occupy like one reference how do you
know if you permute array how do you
know which
yeah that's something I'm skipping over
because I'm keeping it visual but
essentially for the original try data
structure sorry
you've one bit 1 bit per slot and the
bandipur slot really let's just access a
for compression so let's you indicate if
it's there or not and by doing that you
can like have a function that with the
help of the bitmap and your position
that you want to look up you can recover
for example let's say that the physical
slot in the array that you want to
address well in order to make the
permutation work it's actually a
function that takes two big maps into
account and what you can do is then
either like do sort of like offset
addressing that you say I would want the
data map I can determine the bounds
where the second one starts so that's
like one way in order to do it so it's
like a compression and permutation at
the same time or you can even do it like
more cleverly in order so it's also
indicated here we see these numbers of
swap
so like index 0 and 24 swap so like the
end reverse order and essentially it's
like s hidden second row together like
you have like two different offsets you
say like slot 0 is my offset for my key
value pairs and like the length like the
other end of the array is the offset for
all the subtrees so once you do that you
don't even like need any kind of offset
addressing you just need to count like
the offsets based on the data elements
or on the node elements so you do the
case distinction based on like what
you're looking for you do the addressee
like here
so so far I didn't talk about the
immutability adjusted like these data
structures are like the state-of-the-art
like for immutable collections out there
but essentially try data structures you
can use them for all kinds of data
structures like a concurrent mutable
immutable data structures and
essentially the only difference is when
and how much you copy copy so for the
immutable cases why would you care at
all about immutability and some of the
reasons are that immutability gives you
like a simpler mental programming model
they allow you also like to worry less
in the case of concurrency because you
can't corrupt your state like from other
threads and further on like you cannot
apply like several optimization based on
the constant ends of your data so how do
you implement like a mutable version of
a trial well if I knew tably would like
to edit the data structure let's say to
a potentially huge huge data structure
there and add the elements 3 4 . I would
just like put it here in the node once
I've located like where to store it I do
that and so that would be by mutation
and a simple copy and write approach
would copy the whole data structure
before modifying it in order like to
keep immutability guarantees alive but
that's not a smart approach like if I
have a arbitrarily like huge data
structure like doing a copy on write
before modifying it that that's not
performant and that doesn't also liked
it they can't like the context of
collection data structures where we want
something compact like relatively like
efficient in terms of runtime and also
relatively efficient the memory
footprint so what three data structures
allow you is actually to do is sort of
like Delta like to produce a data of
your update so once again the same
I'm going to let insert element sir
before here but I leave the old data
structure intact so I'm not going to
modify it but I'm yet producing not a
version without or like with as little
updating as possible and in this case I
would just follow the path down to where
I am like going to modify the element
flow these nodes on the way and add the
element there and reference everything
that was not modified yeah from the
previous data structure so essentially
what I get is like yeah so I I just I'd
like to copy a fraction of of the whole
tree and that is actually what makes
high data structures are good for
implementing immutable collections
because if you look at libraries out
there like Google's gravity like a
Google Scholar it's also like a
full-fledged collection library you have
immutable collections but they are not
they are not incrementally update able
as the data structures that you have for
example in total in Scala
you know they look at the lookup happens
like based on the prefix so what I
showed before is like in the traditional
trial you would do it like based on the
content here you do it like based on the
hash code like prefixes and all the
operations like lookup insertion and
deletion will make the same traversal
based on the prefix and once you're at
the node and you know like the offset
where you should store the element then
you make a case distinction like based
on if an element is already there or not
and if it's not there like you know that
you can like update it for example and
otherwise yeah
yeah
so but I not mostly talking about this
and how to like how the original
proposal like from field Becker for
example was subsequently refined but
esse there's also like similar
approaches that like adopt the same at
the same data structure to make it
concurrent and to make it suitable for
arbitrary like sequence data structures
so how performative these data
structures or at least like what's
challenging to address about them so as
we already discussed this is there a
difference between let's say flat array
data structures and trees well there is
a difference because like there are like
more in directions happening for example
in order like to look at an element and
yes it's it's you have like a small lock
operation instead of a constant time
operation so essentially what one can do
is improve the locality between the
nodes so if you would get as fast as in
the radiator structures why not like
making your tree more like an array so
instead of like having this
representation of the tree you could
also like to think about that sort of
like serialized or linearize the
representation of the tree where all
your data is like continuously in memory
and you just made like small chunks
lower ripping chunks forward in order
like to locate your data item that you
would like to have and the other thing
is also true in once you have different
data structures that structurally share
a part of their content you can also
think about how to represent the
debtor's in contrast to the tree like to
the original tree itself so
betting putting that roach does make it
does pay off to keep the pocket
yeah now the issues are so you're just
like always having a point look up
the only time you're interested in all
the elements group together is actually
once you want to iterate or like a batch
process your collection let's say you
want to iterate over all the elements
you want to perform stream operation on
them and otherwise if I have a
particular key that I want to look up
for example I really like to jump to a
particular section and then like
continue the jumps there so there
doesn't really make a difference in how
let's say inside of the node the
elements are arranged but it's more it
makes much more difference how the nodes
of a tree are located like in memory so
like the locality like between the notes
themselves
maybe I'm getting a problem let's see
that important senior settle down
right yes yeah okay in that case because
there's a PowerPoint to see okay and and
send your question select something yeah
so if I understand your question
correctly is what you're saying is
because we lazily grow the tree so just
if needed and that we have to like to do
less chumps or no
okay so so and these kind of behaviors
and like how you like deal with locality
between the three notes actually like
various like really like from language
to language and also like really it's
dependent on like what your language for
example explosives to you in terms of
like memory memory control so in terms
of Java for example or closure and
scatter like languages that are
implemented on on a runtime that like
takes care of memory management you can
actually do much dead of yourself but
these optimizations can be implemented
for example like in the memory
management subsystem or also like in
terms of like a copying garbage
collector it takes care of these kind of
optimizations so another thing that was
also already addressed is that there try
data structures are also used for in
memory databases but they're hashing is
not used
it's like Rada you encode as in the
original approach the data elements
themselves in the search key but
recently there was also like much more
focus on on how yeah like how to improve
in memory databases and generally with
advancements in the context of like high
data structures in general and in my
opinion in memory databases and
collections are also like growing much
and much more together in terms of like
concepts that they're sharing and like
where one can learn from each other for
example so can learn from each other and
so something that was keeping before as
well as I was showing you like how quite
physically like how the tried a
destructor is represented internally but
I skipped over the code is what you need
actually to get to your data elements
and in terms of like the original octet
remember try approach it's a mix of like
bitmap processing operations and also
like runtime type checks and that's
something what I consider is the beauty
of the approach in how changes over time
because not only conceptually let's say
these are like more beautiful to look at
because similar like type elements are
grouped together but also because of
that like the code that you have to
write in order like to process such a
hash Drive gets much easier to read and
to write so if you're interested so if
you're interested in learning more about
these data structures you can look at
closure for example on the JVM its color
or also like one capsule that's like a
library where I'm also involved that
they're implements like some of the more
recent concepts of hash traps and to
conclude it with the words of Pat Helen
immutability changes everything so there
is a cost attached stream usability but
most of the time I we can afford to pay
it and it's also like a pays off in
different ways
thank you
it's going to be in five years</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>