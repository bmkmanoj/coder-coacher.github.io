<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Yuan Yu - Distributed Iterative Computation in TensorFlow - Curry On | Coder Coacher - Coaching Coders</title><meta content="Yuan Yu - Distributed Iterative Computation in TensorFlow - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Yuan Yu - Distributed Iterative Computation in TensorFlow - Curry On</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8rgd7X_E6v4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll be nice I have you can use it at my
use so that make it a sake make it safer
okay
hello
okay okay let's get started so I'm doing
you and a member of the Google brain
team so mainly working on scalable
infrastructure for deep learning so so
that my talk today is about distributed
control flow in tensor flow so first
what is tensor flow the tensor flow is
Google's deep learning system it's very
large-scale deep learning system and the
system has been used very widely inside
Google and has been used for search for
ad for for instance the deep mines
alphago and a lot of other things and it
is incredibly useful to us and so we
thought the community will actually love
it so we we open sourced it last
November and within six months or seven
months it becomes one of the most
popular github project so and so the
license for the for the tensorflow
is very liberal is to computer it's
actually the upper pocket 2.0 so what is
deep learning so deep learning
effectively is to learn a completely
complicated function from data that maps
from one space to another space for
example and you can have in a space in
the in the space one in the first place
you have a space of images and on your
for your space queue is as a boolean
space is cat or no cat so the function
is going to tell whether given an image
whether the the image contains a cat or
not
so the function itself is represented as
also as a network of neurons
and they're trying to emulate the human
brain so the a neuron is effective a is
a very simple mathematical function that
takes some output of some other new some
neurons and it generate a new value and
that value is going to be used consumed
by some other neurons so the what is the
the deep learning so is main are also
this for this function there's a lot of
a lot of variables and for some of the
models we have the very we have like
billions of variables so the job of deep
learning is to train those variables so
that I can give me an image and I can
tell whether the image can hit the cat
or not so a one standard way of training
those variables is to use gradient
descent so it's just very very primitive
calculus concept so therefore all the
functions involved in the in the neural
network are differentiable functions and
then you can actually apply the chain
rules to get to compute the gradients of
this function so why do we care about
this whole thing's and why this
community should learn something about
this so many of the key challenges we
face in tensor flow are about
programming language design
implementation so here's here's a few
examples we need to come up with the
right attractions to pro12 program those
neural networks and also because of the
computation the the the the deep
learning the training aspect of it is
extremely data and also compute
intensive so therefore we are we're
using all kinds of computing devices you
know using your using CPUs we using GPUs
and also we Google actually and some
and also some other companies we
actually work on cousin hardware's and
so the a big problem is a big question
is how to compile computations to run on
those devices of course also we want to
support distributor and apparel
executions on cancer of this kind of
computing devices you know imagine the
cloud contains tens of thousands of this
kind of computing devices and how do you
get all those computing devices all the
CPUs GPUs of GPUs work together to do
something to do some computation it's at
a very large scale that is that is the
problem we need to solve and also
there's a lot of program transformation
optimizations and it's believe me it's
very very challenging and also it's very
very relevant to the programming
language community so basically
tensorflow is a software stack so it
runs on all kinds of devices CPU GPU and
the CPU so keep us our google cousins
hardware which is what it stands for
tensor processor processing unit so and
on top of it is a layer that is the
essentially is that is execution later
as an engine so it's a an on top of the
execution engine so we have a lot of
math a lot of mathematical operators so
we implement all kinds of money advanced
mathematical operations that is needed
for deep learning and the soul and and
those mathematical operations typically
have multiple kernels or multiple
implementations and it runs on either
CPU GPU or keep you so on top of this so
this is all written in C++
and then we service different language
bindings and they solder the Python API
is the most popular one because of this
is actually the one so if you talk to
the machine learning researchers and the
sort of the language they use is Python
so therefore we have to service a Python
API and no and there is also a C++ API
and also people are working on Java
bindings and go bindings and some other
some other languages and this'll work
and on top of it is just application
user models and people people jet people
create a lot of models for translation
for speech for all kinds of things image
recognition and also they the date is as
good as part of the ecosystem and people
can use it and people can play with it
so what is this talk about so this talk
is about when interesting aspect of the
system so that's that how to support
control flow constructs in tensorflow
and so yeah so everyone knows you know
we we want to do the conditionals that
means we want to be able to execute part
of the program or part of the model and
then ignore some other part of those
based on some condition and also we want
to do while loops and we want to provide
some higher-order functions that allows
them to program allows the people to
write program so so why this is hard so
the first problem is we want to support
distributed in a parallel computation
execution so that means for instance in
a while loop the loop body can be
actually partition onto a set of devices
and so therefore that the commutation
has to be that they each device is going
to run it's part of the computation but
there is going to be a coordination
to collectively decide when the loop is
going to terminate and whether there's
going to be a next iteration so and also
and also we like to have multiple
iterations to run in parallel so
effectively this is this is a
effectively is a data flow model so we
want if there is no data dependency the
the multiple iterations the operations
in multiple iterations should be able to
run in parallel and we want to make sure
they can actually write in parallel and
so there's a another very challenging
problem which is I'm not going to talked
about in this talk is how to do
automatic differentiation that means how
to compute gradient for this control
flow constructs so the basic idea is do
a lot of program generation so we take
the control flow constructs we generate
a program that automatically generate
the program that does the Duster does
that does the gradient commutation and
so when we put the two programs together
and that becomes a very large program
and so the entire large firm can be
actually running a parallel mode so that
means effectively the system allows you
to compute gradient you know you know
very parallel and it's distributed way
so why do we need this control flow and
typically if you look at the
literature's of deep learning so there's
a few layers of the your network and you
only fill layers of neurons and the
stack they stuck together so that is
what happened in the past few years but
there are new classes there are some new
classes of deep learning models and so
that is very cutting edge and advanced
but in the meantime is
showing a lot of promise so there's a
recurrent for instance the recurrent
Network and there is some kind of a tree
and graph network and also reinforcement
learning we just but you know that the
kind of things are used by the alphago
and the soul that requires you to have
more and more programming constructs and
so in particular that requires use of
control flows so let's give you a very
very simple example so there's a
recurrent Network so the requirement of
network basically is trying to model
sequential data so sequence of data
sequence of things something so it could
be a sentence it could be user action
and it could be your history of Google
search and it could be a lot of things
and there is very useful in translation
is this the state of state-of-the-art
translations is actually the the
inference is using a recurrent in your
network and the solder image caption and
also video zones all use and order so if
you look at a code so that the real or
recurrent network is very simple this is
effectively the code pattern and so we
have a sequence it could be a sentence
or it could be something else and then
we basically keep calling a function
cell is a complicated mathematical
function and we hit calling the function
that's that's it so but but this is
something we don't know for instance the
length of the sequence so that's why it
is a while loop nada and there's all the
previous way people try to solve it by
static and rolling so that means
typically you have you need to have some
sort of Max lands of the sentence you
can handle and if your sentence end up
actually shorter than that you have to
pad the thing and you get this longer
and you truncate so but with control
flow you can express it very very
elegantly so
so for the rest of the talk I'm going to
give you an overview of the tensorflow
and I'm going to introduce the control
flow primitives and I'll tell you how
those control flow constructs I just
mentioned it's going to be translated
into data flow graphs and now solder and
I've talked about a little bit about how
those things are going to be executed in
a distributed fashion so in tensor flow
computation is the data flow graph so
nodes are basically operations and so
here is a matrix multiplication is a
matrix addition there are some very
simple mathematical function and this is
the cross entropy and is something like
that so it's just a bunch of
mathematical operations and the edges
are essentially the flow of values the
value is is effectively just in
dimensional array of some type typically
as a float so so the rule of the
execution rule for this thing is
extremely simple and for any operations
if the input already available and this
thing can run so is completely data
dependent is that so that's why as a
data flow execution model so the way we
execute this thing is modular some kind
of collocation constraints the tensor
flow can just partition this graph
arbitrarily based on some cost model of
where to be up where do you run that one
particular up up optimally and so is the
system is going to decide to try to
partition it in some optimal way so
therefore we can take advantage of all
the computing devices so so the system
is going to is going to do this kind of
partition based on some cost model
and obviously one of the problem is if
we partition something on two different
devices and so that means the data has
to be transferred from one device to
another so the system is going to be
it's going to automatically insert a
pair of notes for any cross device edges
send and receive and also the receive in
the current implementation the receive
is very proactive so the receiver going
to start is trying to try to pull the
data from the sender so this send and
receive a sternotomy is a very nice
abstraction that encapsulates all the
communication of tensorflow and so under
the cover so for instance even is two
GPUs on the same machine so we'll do all
just a CPU to GPU copy and if it is a
CPUs on different machines so we'll do
it just on you know cross machine RP a
RPC and if it is GPU on different
machines and we'll do our DMA so so this
is all under the cover is is implemented
encapsulated and so most of the people
don't see it so but Allah a lot of the
kind of book graph optimization and the
so for instance they decide the
placement ability of operations on
different devices assignments all that
things are can be done without really
understanding all the details about how
the things are going to be transferred
so so that's just let me just briefly
talk about them the Python API so the
python api is divided into two parts so
the first part is a way to construct
this kind of data flow graph you can
think think we can think about is the
sell for graph graph beauty so for
instances is a is that you know they
declare some W is a variable which is
the variable is the central is a
two-dimensional it's matrix with the
first dimension is semi 87-84 the second
dimension is can and so here is a the
variable B essentially is okay it's just
a vector and so Y is some kind of
computation matrix multiplication and
then do element ideation and they'll do
some kind of another mathematical
function called softmax so so this is
this is basically describes the graph
but the execution there's no execution
so the execution is defined by another
thing called session api so the session
api exposes a method called run that can
run any of the sub graph the created in
the graph you know by the graph building
api so the sub graph is basically
defined by the feed and also the fetch
servants i want to feed some value to
act and i want to fetch some some value
from y so the tenth of all run time is
going to take the feed and a fetch and
create this sub graph and optimize it
and then partition it on two different
devices and there and then run it so
that the python api for control flow is
it's just a very very straightforward
extension of that that the current
python api so we introduced something
called count which depending on the
predicate we either so the F 1 and F 1
and F 2 are basically lenders when you
call it is going to create a sub graph
so this is basically says so this is a
sub graph that is going to be executed
when the birth when the predicate is
true and this
the graph is going to be executed when
you appraise boss and similarly and we
have for the while loop we have a lambda
that computes the condition the
termination condition and also we have
this the B basically is the body of the
the while loop so so for the rest of the
talk I'm going to talk about how this
thing is going to be the the the the two
things we are just mentioned here is how
these things is going to be compiled it
down to data flow graph and execute it
by tensor flow so let's just recap what
we have what we know about data flow map
this is data flow model of commutation
the commutation is expressed as a data
flow graph and knows our operations and
address our data dependencies and also
the and so the the basic execution rule
is very simple and an instruction is
enabled on operation C anoleis enable
the if all the operands are available
and and the solder the tensorflow
minus the control flow I'm going to just
going to introduce follows this rule so
we borrowed very heavily from the data
machine data flow machine research all
of in the 70s or in 70s or 80s you know
in the 70s and 80s there's a some very
nice piece there were some very nice
piece of work about trying to figure out
how to do something different from a
normal machine and so they invented up
something called data flow architectures
there are basically two kinds of
architectures in Brian's static the data
flow machine and there's a dynamic data
flow machine so there's some limitations
of the static data form and machine so
therefore we actually adopt the dynamic
dataflow machines so that trying to give
us the maximum
possible potential parallelism so so
that debate the fundamental notion in
the dynamic data flow machine is
something something called execution
frame so that means each wire loop is
going to have its own execution frame
and frame can be nested in order to
handle nested loops and so there's also
it also needs to introduce a bunch of
feel control flow operators so that is
going to memmio manipulate the the
execution frames so this is what I'm
going to show you which is essentially
of this five operators the five
operators is the first why is a switch
that allows you to forward the data
either to the false branch or the true
branch depending on the of some
predicate and emerge basically says I
have Q inputs or maybe multiple inputs
and also its and if there's any input is
available it's going to power it up so
and the so there's a enter operator that
tells you to enter up frame and this old
exit operator that helps you to exit the
frame and also the next iteration tells
you to go to the next iteration within
that frame so all those operators are
for control flow purposes and so there
is no data
there's no computation so they are
basically forwarding the data from the
input to the output
so now let's look at how the the control
flow constructs can be compiled to this
so this is how we compile for instance
the conditional so the conditional
basically says I have to so so this is
my predicate and so if it is true I'm
going to come here so the axis is going
to be X comes in because of its truth
the X comes out here and it's going to
the l1 and an emerge and if if it is
false and we come here and to this add
and comes back so for the merge the
merger is going to see only one tensor
and one value and so how about a while
loop
the wild loop a basically is compiled
into a form like this so we we entered
that this is the initial value and so
it's going to enter and because for the
merge you know once there's a value
available it's going to forward it and
it goes here and the base based on this
and so it goes here and it goes to the
predicate the predicate hell's it goes
to the swage and if it is false and we
exit if it is true we do that add and we
comes back and so that
so the value flows here again so this is
so circular it there's a circular graph
here's a graph contains the cycle so and
how this quarter is going to be executed
so as we talked about the you know the
the partitions and also the different
devices and the so effectively we have
one executor on each device and the so
which has effectively manages the
execution of the subgraph assigned to
that
device so the state of the local
executor is very simple and as there is
for you to know there is a pending count
what if the companion count reaches zero
it is enabled and we can execute and
also there is a and for each node there
is a set of available inputs when the
colony when the pending count is zero
all the input must be there so the
execution of a ready node is also very
simple
yes videos only is ready we perform the
commutation we update the pending and we
update the inputs for the successors and
then we execute the new new ready nose
and all can execute in parallel we can
execute in whatever ways we want to
execute so how but with control flow we
have to be a little we have we have to
be a little we have to do a little bit
more so one one of the question is what
happened if how to handle the untaken
branch of the switch so we introduce a
notion of dad a notice dad if it is on
the untaken bred and taken branch of
load switch so so for that node so the
what we do is we actually we don't
ignore it
we actually propagate but we skip the
commutation so there's no commutation on
that note but we do actually propagate
this this darkness so this is actually
quite useful as needed it's useful for
better garbage collection but it's
necessary when we get to the distributor
execution and the yeah the propagation
is actually quite nothing it's quite
simple and any node and you know the is
that if if one of its input is that and
the accepted merge and the merge is that
only if all its inputs are done it's
very simple rules
sold it so how do we deal with while
loops so the unit in the case of the
wire loops so we have we need to execute
the same node multiple times so because
of your cycle so so that so the way we
do it is by basically introduce the
execution context for each loop
iteration so this so the this execution
context is uniquely identified by the
frame name and also the iteration cut
iteration counter so it's exactly in
terms of the state of the context it's
exactly the same as before so we have
the pending we have the inputs but but
now instead of a global thing but now is
for each context we have this state so
the execution is exactly as before but
only executed in this in this context so
so the so now let's take a look at a
little bit closer look at the the
control flow context the control flow
contacts effectively as I said you know
that all the primitives are used that
you manipulate some the control flow
context so the and here basically create
a new context so that means we're
starting a new while loop and so the
next generation is going to create a
context base
counter plus one and our contacts is can
be garbage affected when the computation
is done so so effectively what when a
context is done and there is no
outstanding operations in that context
and also the context the previous you
know the context for the previous
iteration is done so this is actually
highly optimized for parallelism so that
means not all loops can run in parallel
and multiple iterations of the same loop
can also run in parallel so yeah so it's
all data dependency so let's now get to
the interesting part of this whole thing
is how to do this review the execution
of this so let's just recap what we what
we know about it this thing is a
grumpiest partitioned into a set of
partitions and and the soul and each
partition is runs on a single device and
managed by this thing the executor we
were talking about and so there is a
execution for the for the entire
computation is done when all the sub
graph is are done so so this is actually
managed by a master service that gets
some kind of signals from all the
partitions with all the partitions
ordering on the entire partitions the
entire computation is done
so how do we do the distributive
execution of conditionals so so the
first problem we run into is suppose I
do a partition on the untaken branch so
remember we talked about the receive the
receiver is going to proactively pull
the data if this is a if this is an an
taken branch
so there's if we don't send anything
here so the poo is going to wait forever
so so the so as I said before so that so
that this is where we need to propagate
this this is the the time is token so
what we do is we actually even though
we're even though this this part for the
even for the untaken branches we
actually send we propagate that the 10
is and so there is actually as a token
which is a boolean token as sent from
the send from the send it to the receive
so the receiver is going to receive this
is that token and it's going to keep
propagating this token and you know for
instance maybe there's a third device
it's going to keep propagating and
that's so how do we deal with the while
loop so the so an ie partition will say
to partition it to partition the blue
body on to a different device but this
is going to be also bad for this night
if we do it naively so so the problem is
the the device P has no context about
what exactly the execution context this
this part of this sub graph is in so for
instance it doesn't know you know it
needs to start the next year
also it doesn't know when to terminate
so the trick the trick is to do graph
rewriting so the basic ideas also vary
the idea is very simple but it's turn
out of the implementation in stern I'll
be quite tricky so the basic idea is to
add a controller as a finite state
machine to each partition of the loop
and this is actually as a equivalent
transformation of the original graph so
this we're not changing with the
semantics of the graph and then so the
resulting graph is going to be partition
and execute about tensorflow
so the beauty about this approach is we
can use whatever the local executor we
use before so this is the whole
distributed execution is done by this
very simple graph rewriting and
transformation so the cost of the
communication is also quite good so
there's one tiny message for every
iteration we need to send one tiny
message from the device that is
controlling the loop termination to each
of the device participating devices for
the loop so so let's take a look at how
it how it is done so this is the this is
a loop and is now it's on two devices
and what we do is we write after the run
right after the placement of odd up ops
two devices and we go through the graph
and the put and introduce our add a
little finite state machine that
controls the execution of this off or
the the part of the OP in you know each
partitions of the participating in the
loop commutation
so and then we go through this and then
we do the partition introducing the send
and receive and now the graphic is it
looks like this so I'm not going to go
to the detail but so this one actually
allows you to execute and they sort of
the tube the two executors in fact runs
completely independently communicating a
little bit of the termination condition
and right so and in somewhat more
general way and so you can if you have a
bunch of send and receive or there is a
lot of cross device edges and you
basically there's a bunch of send and
receive it's all controlled by this
little state machine and so on it on it
on device a and also we also have this
the little state machine that also
controls all the receives so so what
happened if there's an essay loops and
we just a stack the two state to veto
state machines together so that would
actually give you the same machine that
controls nasty loops and this whole
thing is released as part of the
tensorflow
open source and as is widely used both
internally externally by I hope by a lot
of projects and a lot of people so
that's pretty much is my talk
yes given that you partitioned the graph
using graphical writing
I assume it's done statically right so
how do you do load balancing and how do
you minimize compute communication cost
so so this is this is a little bit of
beyond the scope of this this talk but
what I can tell you is for this kind of
computation is there's a lot of a lot of
repeated computation so we can learn
from the from the history of the
computation that we can generate some
very very accurate cost model and so
that is that is typically that is used
to us to to to optimize the placement of
the assignment of the operations to the
devices so it's basically an offline
profile gyrate optimization for the
right now it's offline but you can
imagine so you so this kind of is a
premium so for this kind of training is
a training loop yeah there's a lot of
data you need to train you use a lot of
years of training so you can say for
instance after few thousands of steps
and you calculate the cost model and
then doing doing a better trying to
optimize the placement and then and then
do it again so it's a it's a it's not
completely offline and but this it's not
really you know the real-time attack it
adaptive things you may want to see
thank you how do you deal with failure
is there some kind of fault tolerance
abilities
we also it's very interesting question
so the failure for this kind of system
the failure is is handled by checkpoints
okay so there are some states and so we
periodical checkpoints the system and so
and you've something crashes and we just
we just you know go back and start from
the previous check bomb thank you
could you also use this for other stuff
machine learning of course so this was
this is actually becoming so initially
is when the project started so and the
the application domain is focused purely
mainly on the on the machine learning
deep learning and but once we now the
system is getting more and more general
we start to introduce for instance
shared variables and we introduced a
share the state and we introduced this
also the control flow stuff so that make
it more or less the general-purpose
programming language in some way so yes
the answer is yes and so we are
certainly trying to push it more and
more towards general purpose
applications yeah
connecting to this so the user has only
consumes this API to set variables and
the steps are the transformations on the
variables but and you don't have control
on the actual formation of the of the
graph right you you do have control so
my question will be do you have an
intermediate form in which the user can
see how the graph looks like and there
is a graph visualizer so that allows you
to realize the graph and but but it's a
little bit more complicated because of
the graph there are many stages of the
graph and it's going to be transformed
many times us you know optimize the
optimizing compiler does you know the
code is becoming more and more obscured
so the same thing is true here so and so
the graph actually end up executing is
quite different from the graph you
initially constructed for instance if we
do constant elimination or we do some
kind of load common sub-expression
elimination and so that the graph being
executed
as an executor as can be quite different
from the graph you you contract is there
a public API to control this graph as
well no there's a is mainly as a graph
visualizer but there's no API that
allows you to - there's no what I should
say there is no GUI can you speak to the
limitations can you speak to the
limitation say problems issues that you
have encountered so the the limitation
so that's a very good question so I mean
I haven't really thought about much
about the limitation so that if I want
to talk about something the limitation
is so that I would there is a so as a
mention you know this is it so this
thing is started us some very inert
somewhat narrow scope and then it has
been generalized so as part of doing
this and I think the programming API
needs some work so and the problemi api
is not so there's a there's a few things
which is when this is the this mannix of
this api is not very well defined and
and also the the the use of this api if
you only do deep in europe programming
deep in your network is turned out to be
is quite is quite adequate but if you
want to do some more general purpose
programming so that it's so this is
actually as a very good question - the
programming language community is to
come up with the right abstraction to
program not only the newer network and
also do you know we we the execution
engine with providers execution
that if that allows you to do to run
something a very large scale and using
all kinds of computing devices and with
all the support for control flow and
other things and the soul the
interesting question is what exactly we
can the programming interface or the
programming abstraction on top of it and
so we don't have we are actively
thinking about these kind of things but
we are it's nowhere near the we got some
good handle we have we haven't really
got a good handle on this yes so I'm a
bit curious about the predicates in
particular so how they're built up and
where they're executed up for example
where if X is less than Y yeah so you
were talking about Heinicke here yeah
I'm curious where that's evaluated and
how that computation is built up and
distributed the product can be
distributed to sold it can be
partitioned so then are the contracts
for building up the conditionals then
limited to things that can be expressed
in the graph as well or can they be kind
of any arbitrary computation it's
arbitrary commutation that is composed
by the operations okay we defined so
things like it's not Python so there's a
we're talking about the tensorflow
operations yeah if if you end up using
the Python operations that means you you
end up having a round trip from the from
the server to the client so which is
which is exactly the thing we try to
avoid thank you
okay any questions oh there's one more
yeah so just look at like looking at the
while loop for example it seems like
using the word while loop doesn't really
match what it's actually doing like in
the end when it's executing possibly
parallel and possibly over several
machines
it doesn't really resemble a while loop
anymore and so you would think about it
so I was wondering if you thought about
what kind of language you would use to
describe this kind of distributed and
parallel operations so so the way I am
inclined to think about this is this is
our this is your instructions that for
your machine or processor right so and
and so that it could be a while loop the
while loop can be compiled into
something like this kind of data flow
graph and and for instance some other
higher order operators like fold or map
or parallel map and also scan and also
can be compiled into this so and so this
the the the instruction set is
reasonably general and but we so what
exactly the programming the programming
construct we want to surface to the user
is it's wide open
oh okay so the so the spark so those
spark stars as batch processing system
and they started as something about
streaming and and I think for the kind
of applications that we were talking
about so the this this is a this is I
would say a so it's more scalable and
also high-performance way of doing
computation so and so there is actually
some work going on and trying to trying
to integrate tensorflow with spark and
but you know so so the way for instance
the spark doing streaming is quite
different from the way we do so we do
streaming so with a while loop and and
also something I haven't mentioned is
and also the in the tensor flow we have
distributed a queue implementation the
combination of a while loop end of the
queue gives you effectively a streaming
way of doing commutation so but that's
something we haven't really explored a
lot because we haven't really found any
compelling use cases for this but but
that's actually is the direction we are
actively exploring
okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>