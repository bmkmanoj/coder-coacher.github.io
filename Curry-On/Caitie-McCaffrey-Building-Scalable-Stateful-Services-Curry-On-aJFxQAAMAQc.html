<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Caitie McCaffrey - Building Scalable Stateful Services - Curry On | Coder Coacher - Coaching Coders</title><meta content="Caitie McCaffrey - Building Scalable Stateful Services - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Caitie McCaffrey - Building Scalable Stateful Services - Curry On</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aJFxQAAMAQc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hi welcome back from lunch I'm
really excited to be here so I'm gonna
talk to you guys about building scalable
stateful services so like heads up this
is not a programming languages talk this
is more system space so it'll be
something a little different
I'm Katie McCaffrey I am an industry
person and I'm also a systems and
distributed systems engineer I've spent
my career building large-scale
distributed systems that sort of power
entertainment industries or social
networks so I spent a lot of my early
career building video games for the Xbox
and most notably out of that I worked
with the team at Microsoft Research to
make productionize a distributed actor
framework called auro means and we built
all of Halo 4 and the future Halo games
on top of that most recently I've been
at Twitter working on observability as
the tech lead and then I recently just
switched to building distributed build
systems so all really fun things I'm on
the Internet if you would like to talk
to me please do that I do that my Dan's
are open on Twitter if you have like a
question or something afterwards if we
no time I also wanted to just point out
since this is a nice bridge between
academia and Industry that I'm also
associated with a group called papers we
love which tries to do this as well
papers we love is super awesome if you
don't know about it
there are chapters around the world and
the idea there is we have meetups
generally monthly and some of the bigger
chapters to talk about academic research
and and facilitate discussion there and
I'm in the San Francisco Bay Area
chapter and so we have a lot of good
hybrid and discussion going on so I
think that's also a really amazing group
and there's a conference in st. Louis in
September if you want to come okay so
let's get started so we've been building
systems in this world of the stateless
service paradigm so this is sort of like
the idea that everything's like
horizontally scalable and so when I get
more users I just add another machine
and this keeps happening and then things
just scale and that's okay because
either our workloads are partition
enough that they can easily be
parallelized
and you have concurrency problems and so
like typically that's been fine but the
implicit assumption here is that
basically we're assuming that our data
storage also scales and that's actually
non-trivial to do and so we're sort of
hitting this world where maybe this is
also a little bit slow and doesn't meet
our needs so this paradigm basically
realizing the data shipping paradigm and
the idea here is a client comes in it
makes a request and then it'll go to the
database or whatever you're caching
layer is it'll get that data pull it on
to some machine which will then process
that data and then send some use to the
client and then that data gets tossed
after the end of that request and so
either of you like explicitly memory
managed or cleaning up memory or via a
garbage collector and so then the next
time a user comes in and we have very
chatty clients these days comes in it
makes a very similar request and needs
the same bit of data it's gonna go pull
it from the database it's gonna answer
that request and it's gonna throw it
away so this is may be wasteful
depending on the how your application
behaves it's very wasteful for things
like mobile games it's very vey wasteful
for things like so all the apps which
are very like user centric these days a
lot of times you're operating in the
same set of data and you have these
sessions that are long-lived and so can
we do better so the idea here is we're
going to talk about maybe how we can do
better and this is actually an emerging
trend that's happening building stateful
services this is not databases this is
not caches this is actually having state
in our services and I'm not going to
talk about replication and this is
basically focused purely on data
locality and some of the latency
benefits we get there but a lot of
people are doing this in industry and so
I think it's an interesting problem
because we're all I'm gonna walk through
the model of how we build this but I
think it's maybe something that could be
solved at a lower level with a better
level abstractions so I think that's an
interesting conversation to have in this
venue so we're gonna talk about some of
the benefits of this like why are people
doing this
how are people building these systems
some real-world examples that you
understand just the use cases and where
people this is actually happening and
working and you know whatever we're just
doing in an industry and then some of
the pitfalls inside traps that are
people are falling into there so the
biggest benefit of stateful services is
this idea of data locality
and I think that's pretty obvious right
like if you have data in one place and
you don't have to go pull it from the
database every time then that saves a
lot of latency so for interest for
instance in the team I'm currently
working on we're doing distributed build
and so like taking advantage of data
locality is really huge because we have
a mono build at Twitter so if you have
to pull the diff down and like sync that
to a machine and then you just throw it
away after you run like one test case
that's pretty wasteful so we want to
take advantage of data locality same
thing in Halo when we were you're online
and you're doing something and you're
very sort of narcissistic as a halo
player in terms of how you interact with
our services you're constantly asking
questions about yourself or like what
you're doing or what are my stats or
what are my teammates doing and that's
all very session based data that is
being operated on and we wanted to
respond very quickly so data locality
revolves around function shipping
paradigm this is just the idea that I
move the function to the data so in a
real world system would actually end up
happening is you have these services I
probably still have a canonical source
of truth that is the database story and
all this durably for me but then when I
make a request the first time I start a
session I will pull that into some
machine and then every subsequent
request I will just keep going to that
machine and so then I don't have to keep
going back to the database and I save
some round trips and I save some load on
my database as well so that's super nice
and then you just figure out how to
clean that up later when the session
ends and usually a timeout is fine for
that okay so the way we build this is
this idea of or one of the other
benefits is that when you build these
sticky connections or these long live
connections to a machine that has the
data that you're on you get some
different consistency models so just to
sort of like quickly define I think it's
clear but like the sticky connection is
just always going to talk to the same
machine in the cluster that has my data
for the length of my session state this
is a diagram that I borrowed from Peter
Bayliss about consistency models and so
like I don't wanna like harp on count
because I think it's not necessarily the
most interesting model but the idea here
is that a lot of things that are
considered CP and not available when you
have just generally like no sticky
sessions can actually be made more AP
and you get easier consistency models
that are easier to reason about I think
Vernon Vogel made this point from Amazon
and a paper
in a blog post in 2007 where he talks
about eventual consistency and this is
around the Dynamo paper which is their
key value store and he basically says
that like stickiness is this idea or
that you can get stronger levels of
consistency with stickiness and it's
sort of a nicer model to program for for
the developer because if you think about
it it's logically easier for me to think
about as a client-side developer which
clients are part of a distributed system
to say I'm always sort of talking to the
same machine for the length of my
session I don't have to think about all
these race conditions and concurrency
issues that can result because I'm
talking to a variety of different
machines so that's sort of where all of
this started or this terminology came
from okay so how do we build sticky
connections to take advantage of this so
once again we're just trying to do this
thing where we talk to the same machine
in our cluster every single time and
there's a multitude of ways to do this
and that I've seen this done in industry
and there's different trade-offs and
problems that come with each so
persistent connections is honestly the
easiest dumbest thing you can do and
it's the way a lot of people start I
mean this is why we have streaming
protocols right so this is just that
like when you start a session your
clients gonna open up a persistent
connection whether that's TCP or like
pick your protocol like it should be a
long pulling or speedy and you're just
gonna talk to that machine via just
because you have a pipe open basically
so this is easy to do it's problematic
because well obviously once the
connection breaks you lose any advantage
that you had and that happens very
frequently with mobile clients since
they go on and offline and lose service
pretty easily it's also problematic in
the sense that if that machine like if
you're just doing like round-robin load
balancing it just connects to any single
one you're also making this implicit
assumption that the services or the
requests and every client is basically
the same amount of load on the server in
reality that like one percent never
happens and there are some that take up
way more load than others and that's
okay because that means like some people
are just more active users of your
service but if you don't build in
backpressure and allow your service to
make or break these connections and say
like hey I'm overwhelmed you will
actually just see cascading failures
throughout your cluster so you've to
build some more stuff into make sticky
connections work at scale
not just take down your cluster and be
unreliable the other option if you want
to get a little more advanced is routing
logic so when you do routing logic it's
just this idea if I talk to any machine
in the cluster to sort of start start
the connection and then they'll send me
to the machine that I want to talk to
you and this is definitely more involved
and I sort of break this down into two
problems you've just solved there's this
idea of cluster membership like what
machines are in my cluster and then in
work distribution how do I determine
like which machine and the cluster gets
assigned to this session or this
workload so like cluster membership is
pretty simple there's either static or
dynamic static is obviously easy it's
bad for a lot of reasons because if you
have a machine outage you basically have
a partial service outage because
anything that was getting around to that
machine can't get robbed at anywhere
else because you have no dynamic way to
update your cluster membership it also
just means that you have downtime to add
capacity so in in the real world this is
like or an industry I should say this is
generally not acceptable anymore having
downtime is generally seen as like
pretty bad and embarrassing and if you
say like if you put it over like we're
down for maintenance page people like
definitely pick up on it and there are
like negative press headlines so that's
this is sort of not really a solution
for most highly available user driven
systems today the other one that's
obviously more complex is dynamic
cluster membership so this is more fault
tolerant because just as machines come
and go and my cluster I can add them and
they can start doing work and then as
like one goes away it's fine it'll just
rebalance the load so that's that's more
what we want generally and so the way we
can do this is basically via there's -
there's another choice here cuz sort of
one of the fundamental problems with
distribute systems is where as always a
trade-off so there's no no perfect
solution but we either start with gossip
protocols or consensus systems and here
the choice you're basically making once
again going back to the cap theorem is
this idea of availability versus
consistency and what do I care about in
my cluster membership protocol and
there's different decisions that we'll
walk through like why you may might make
this decision so consensus systems are
just this idea
of like an industry that's like
zookeeper or @cd and then like you know
like the algorithms underneath are
either Paxos or raft or Zab and these
are just pretty standard things that
everyone uses people use them for
service discovery today that's sort of
like what we're talking about it's not
anything super fancy here gothic
protocols are more interesting or like
epidemic protocols and I think this is a
super nice thing to think of and like
once again I want to point out like none
of this is new like obviously consensus
systems have been around for a while and
gossip protocols are also used in a lot
of no sequel databases but basically
gossip protocols are nice because they
assume non reliable networks they use
like information dissemination
techniques and pairwise communication
and this is just sort of an easy walk
through what's going on but basically
they'll start up with no state and then
they'll sort of communicate with each
other to figure out like who all is in
the cluster and they'll just do this by
sending pairs and there's this is the
most basic form of this there are more
optimized versions that so that you can
minimize network traffic and stuff like
that but the ideas at the end of the day
everyone in the cluster now knows who
everyone in the cluster is and so they
all have a consistent or they eventually
will converge on a consistent view of
the world but obviously if that was
going through they didn't have that and
so that could be problematic if you need
really high levels of consistency in
terms of like how you're doing work
distribution throughout the cluster okay
so work distribution is another idea of
how we do these and I think there are
different trade-offs along the lines of
here each and this is the idea of now
that we know who's in our cluster and I
get a request like how do I sign it to
that machine to do the work this one's
own super dumb but there's actually a
real-world example that does it very
well and so I just I talked about it a
little bit and it's random placement so
maybe this isn't directly sticky but it
is this idea of we're keeping everything
in memory so I get a request I write it
to anywhere in the cluster just via load
balancing and so what this means is that
what I read it back I have to read from
everywhere in the cluster so this is
kind of seem silly because you're like I
don't know anything is but for very high
write workloads this is great and then
when you have read generally if you're
doing a very expensive read query you
have to fan out throughout the entire
cluster to read it anyway
or you're pulling in a lot of data this
is really nice because it automatically
sort of like parallel lies as your reads
this consistent hashing is another
pretty standard way to do what load
distribution and this is used in a ton
of databases it was most I think it was
like first talked about in the in a
paper in 1997 it was used for
distributed caching to relieve hotspots
on the worldwide web and then Dynamo
obviously made this very popular when it
was released as it uses it to figure out
like we're it shards work so basically
what's happening here is that you
essentially hash all your nodes in the
cluster to the same space as your
requests and so these big like solid
colored nodes on here are basically our
servers like node A through D and then
the little ones are our requests that
come in and they get hash them to the
same hash ring and so basically wherever
it falls you just walk right and that's
the note that the request gets to and so
what's nice about this is unlike a hatch
where you would just take like a normal
hash when you remove a node from the
system it's not like you have to rish
are Deven and move everyone around again
and all the requests because that's very
expensive like so if node C just failed
all the requests before it would just
get load balanced to node D and things
like that so you were doing a minimal
amount of restarting when nodes fail or
when they come and go and so that's
really nice one of the problems here or
maybe it seems this is deterministic
placement right like I always know where
a request is gonna go give it a stable
view of my cluster this seems really
nice because you sort of like can just
calculate anywhere where a request is
gonna be in like what node it's gonna be
on but it's actually a fairly
problematic because once again you're
making this assumption that all
workloads are exactly the same and that
also really happens especially when
you're talking about like running a
query on a database so you can get
hotspots and and in practice what this
means is you run these clusters with a
large Headroom of capacity in turn to be
able to handle the fact that you have to
run at the max hotspot right so that's
that's less cost-effective I think
another really another way of doing this
is a distributed hash table this is
pretty easy obviously you just come in
and then I'm gonna like the first time
you come in I'm gonna hash and figure
out like hey you should talk to node a
and then the next
it happens that's fine and you would
talk to you today but the nice thing is
because it's not in deterministic and
I'm just keeping a hash record of where
I'm routing your requests to I can
update that on the fly so that allows me
to write a program that alleviates
hotspots within my system and I don't
have to do that or I don't have to like
run at this extra Headroom so I can just
move stuff around should it become too
hot in one hour more machine okay so
that was sort of a lot of information so
let's put it into practice
how are people actually combining and
using these things in the real world so
scuba is the system from Facebook it's
basically how they do a lot of their
real-time analysis enquiries of metrics
and debugging and they can go and ask
questions and this has sort of been very
very widely successful for to them and
they use this fan out on on bright and
they write randomly and so then when
they read they actually go to every
single machine to ask data they run the
query on every single machine in the
cluster and so like this may seem silly
but it's actually really nice because
generally the the data that they're
pulling in is very large and so then
they just like parallelize the reads out
and then you're not just like totally
swamp een water machine with a very
heavy workload
and they and so this is could be
problematic in the sense that if one
machine goes down then obviously you're
missing data so what they do is they
just say we'll return the results we
have in a correctness value of here's
the like percentage of data that I think
is here and correct because we we can
basically say with a normal distribution
of machines if one is down then we know
that we're missing this much data and
then the user can make the call of is
this enough consistency for me is this
enough correct or correctness I should
say this is enough correctness for you
to make a decision or do I need to like
wait and run this query again and try
and get a better more correct view of
the world so I think that's actually a
really nice real-world solution to this
problem there's a paper on this as well
that you can go and read if you're
interested in more scuba Guber ring pop
is this thing that uber developed it's
in nodejs and they have this problem of
when like drivers and riders are going
around you have your phone it's very
chatty and it's also a session state
that's like how the determinants are
it's not deterministic it has a
I'm in which you were being very chatty
with their services and they made his
observation that like hey it would be
great if I didn't have to round-trip to
the database every single time that I
wanted to update something slow and
that's a lot of load on their database
and so they put everything they wanted
to take advantage of data locality so
they went and implemented routing logic
and they use the they use the swim
gossip protocol which is a paper that
came out of Cornell for cluster
membership and this is actually a pretty
popular way to implement gossip in
industry it's one of the more more the
favored ones and so they and then they
use consistent hashing for method
routing so when you have like a session
ID and you're always rounded to the same
machine so one of the nice things about
this is they take advantage of data
locality this is open source on github
if you actually want to go look at it or
try it out one of the problems they
actually do have though is that because
you have a consistent or because you're
using a gossip protocol for cluster
membership and you can have a partition
in your network then you could have you
know split brain mode where some
requests are getting some of your
requests from your phone are going to
like one side of the partition the other
are going to the other end so they have
to write code to handle like merging all
that state back or dealing with that so
that's nice but it's also nice because
then there's basically no time at which
like uber will be like sorry like I
can't like too many nodes are down or
whatever Weaver partition and you can't
get a ride and so over and a lot of
businesses choose availability over
correctness and we'll deal with the
consequences later and fix it up because
like if you can't get a ride with uber
like you're more likely to pull open
your app or your phone and look at
another app or stop using them and
that's that's bad one of the other
problems is obviously this has to
deterministic placement so for whatever
reason you get a hot spot or a harsh hot
shard there's nothing you can do besides
add more capacity okay and then I will
talk briefly about our aleene's because
I think they do this in a really
interesting way this was a product that
I was involved with so Orleans is an
actor model and a distributed runtime
that came out of the extreme computing
group at MSR and so they basically
actors are stateful services because
actors can
you know they're basically little state
machines that run and they can receive
async messages and they can create new
actors and they can send messages but
they can also have their own internal
state and they can modify that so every
time you make a request to Orleans and
Orleans cluster it'll route it to the
right actor and I'll talk more in depth
about how we do this next but I just
wanted to mention that Orleans is
available open source on github so you
can go and play with it today and
there's a paper and then it's also like
the core tech that we shipped Halo 4 &amp;amp; 5
on so this is like a very real-world
system that works ok so how are the
instance its routing I think is actually
pretty interesting and allows Orleans to
do a lot of the magic it does so it uses
a got so it uses a combination of
basically like a gossip protocol to
propagate the cluster changes and I
actually uses a more consistent story
there after storage or zookeeper or
whatever your plug-in library is to say
like these are the nodes in the cluster
but then it is an eventually consistent
like the model who all the nodes know
and who is in the cluster has to
converge because they use a gossip or
protocol to propagate those changes out
and then they use a distributed hash
table as well so what is actually
happening when you make a request to
Arlene's as you come in and it's gonna
say like we for instance in Halo we
hashed on your player ID a lot and so
you look at your player ID and we would
say ok do a consistent hash on this
person's player ID and that would tell
us where in the cluster your information
in this you're entering the distributed
hash table was so you have to do a hop
and then and then it'll eventually route
you to the machine we're like your actor
is running and now like you're good to
go and like you can run whatever
computation you had to run and so
conversely like if you came in on
another request you might go to another
machine and say for instance like this
is a you know a couple days later or
whatever or there was a failure and so
your actual state your actor has moved
to another node in the cluster you're
consistent hash or distributed hash
table lookup it's still the same this is
a consistent hash and then you're gonna
get routed to wherever your node happens
to be so I like this pairing because an
actor reference basically are where the
machine lives is basically the same for
every request because it's just like 80
bytes of memory that says like this is
like the user and this is the machine
where it lives on so that's actually
the same for regardless of like the low
that aren't users going to do so there's
no hot spots or deterministic problems
with deterministic placement here and
then because they can actually move
these things around when you get a hot
spot and it says you should rate hash
table and non-deterministically place
where your actor runs it handles failure
very seamlessly because if a node goes
down then it'll just like figure out
it'll just remake it somewhere else and
it also allows or means to do really
nice things like if that mission one of
your machines is running too hot it'll
just move nodes it'll kill nodes on it
and just move them somewhere else and
there's nothing wrong with that and that
actually works really nicely in practice
because we ran halo the halo services at
about 90 to 95% CPU utilization in
production so that's really cool because
you could use the whole box so I really
like this method of how Orlean sort of
distributes load and and does that
seamlessly for you okay so what are some
of the challenges associated with
stateful services this is sort of I mean
it's like a new paradigm for web
developers right like database people
and cache people who've been doing this
for a long time
but there are some gotchas if you
haven't been exposed to a lot of this
before basically you now have unbounded
data structures and like you didn't
state less services before but they only
like lived for the length of one request
and now they could live very long terms
for like you know minutes or days or
hours and that could be problematic
because if you don't make your
assumptions or explicit in the
distributed system it's Charlie I caused
a really bad time for someone down the
line so the canonical example of this is
that people do this with with
distributed queues and distributed
systems you'll sort of hear that you
know all these horror stories about from
Twitter about like how we have the Ruby
on Rails monolithic app and then we used
queues for comma to do RPC and then the
queues would back up and the unicorns we
get sad and like all these horrible
things would happen in Twitter would
have outages and that's just basically
because we're assuming that like our
queues were unbounded and like nothing's
actually unfounded there is a limit you
just don't know it because machines have
real memory and so the same thing sort
of happens with with stateful services
another example this is in
Twitter on observability we have an
indexing service it's like basically all
in memory and we sort of just assumed
and it was made that things would be and
the number of metrics someone was
indexing per service and how we were
doing this would you know be a
reasonable size whatever reasonable
happens to be and then the observability
service at Twitter grew by an order of
magnitude going from hundreds of
millions of metrics ingested per minute
to billions like on the order of 2.8
billion and remain order of magnitude
exposes to all these implicit
assumptions that were made and so that
service used to struggle very hard
because some services were adding tons
of metrics and it would blow out memory
and so we had to go and add some
explicit bounds and help people
partition their data a little better so
starting with this idea of you know
making implicit assumptions harder to
make actually is a really like making
people forcing people do it explicit
assumptions is really good industry for
making reliable distributed systems
another thing that's a gotcha and seems
really obvious but a lot of people don't
think about it so you get into the
depths of these is memory management
becomes a problem garbage collectors are
not really good at dealing with long
live state typically at least not on the
JVM or c-sharp side of things we ran
into this problem at halo where
statistic service which does all the
aggregation of your stats per player and
is long-lived for however long you
happen to be playing Halo online you
have an actor running in our service and
like just your such and say it it goes
away like once you log offline but it
could be hours like people played halo
at launch for like days straight it was
crazy in during a tournament they played
it for like 48 hours straight because
we're like giving away a truck or
something but that's like a whole thing
um so anyway the C sharp guard or the
CLR garbage collector got very sad
because it's like hey you're still
wearing all this data in memory and when
I and depending on how your daughter
scripters implemented so I know a lot
about how the C sharp corner ones
because we had a to knit it and you had
all these references going around we had
to go and tune that and sort of make
friends with our garbage collector and
do some object pulling and things like
that so that does get harder when you
have these along with things so you know
an interesting question is is like if
this becomes more important going down
the line and people really care and want
to build more saiful services is there a
garbage collector that is optimized for
long-lived objects basically I haven't
seen a
research in that area yet because I did
people don't really do that all that
often and then like databases are
written in native code a lot of the time
although more being written in Java and
it seems to be okay
so we should talk about how they're
doing that
the other interesting thing here is that
once you reload state from a from like a
user perspective I don't want anyone to
know in the background that I like a
staple service I don't want them I know
what the first connection to be really
slow or in the case of when a failure
happens and then we have to spin up your
your actor or your state on another
machine I don't want that to appear
really slow in time out it should be
kind of transparent to the user what's
going on so you have to be really
careful about when you re on boot when
you reload state you want to only load
the minimal data set on restart another
interesting thing is that if it fails
you should keep loading that state
because there's like a timeout or
something because they're probably gonna
come and ask you for it later and that's
totally different than you know you
cancel everything once a stateless
server request fails another really
interesting thing that happens in
industry is that typically you know
adding capacity or like replacing nodes
is not super all the time it doesn't
happen that often but deploy new code
actually happens really frequently and
so that is really expensive because
reading the entire system with all this
state is noticeable to your users then
that's that slows down your development
team and that's really problematic so
Facebook actually ran into this problem
when with scuba so they published this
paper and I actually thought was really
cool because it's it's not a hard
concept but it's just like this key
observation that was just like totally
paradigm shifting and and what they did
is they found that restoring all the
scuba nodes and deploying to deploy new
code like took them like 12 hours or
something so developers literally just
sitting there babysitting this deploy
for like 12 hours because of how slowly
they have to roll the cluster so that's
awful and like no one wants to do that
and it also disincentivizes your team to
ship code really fast which is bad so
what they did is they made this key
observation that basically so scuba is
written in C++
I think or see one of those so they
memorize their own memory and they
decouple the memory lifetime
the process lifetime right so we know
that memory is bad when a process
crashes and so obviously we have to
basically reload everything from disk or
some durable storage but when we're
doing code deploys which happens way
more frequently usually then a process
crashes where you hope to get to that
point
the memories goods we don't have to
throw it all away so basically what they
did is they copy all the memory that's
running into a shared memory and then
they killed the old process and they
bring up the new one and then they copy
all the memory back and that took like
two or three minutes instead of three
hours per machine and so deploys took
like two hours which is really cool for
this simple trick so it's sort of when
you start thinking of services in this
way it sort of is very it's worth
reevaluating a lot of assumptions about
like Oh like when a process crashes or
like we sort of process but you can't
trust memory and they published a paper
on this that I'll link to my reference
section at the end so yeah so like sort
of in summary like data locality and
availability are like very big concerns
from an industry perspective and like as
we went through like there's all those
choices that you actually do you and
that's actually a lot of code to
implement and actually fairly
challenging to get right and I think
there have been a lot of successful
real-world systems which means this is
gonna keep getting adopted as a solution
but it's hard and so like can we think
of better abstractions can we have
better runtimes or programming languages
that maybe do some of the work for us
this idea of you know moving state
around in the system is not new right
like it's happened and other papers like
unrolled and things like that and the
past and so maybe that's an idea worth
revisiting that we bake this into a
language somewhere so that we can just
have all these advantages and not
everyone has to write all this messy
code that's that's easy that's time
consuming to write and easy to get wrong
okay so I would say questions I also
want to point out I have a repo up on
github if you have if you want about all
the papers and things referenced in this
talk or any of the links and the slides
are also up there so if you--if you want
to do that check that out so thanks
hi are you aware of any practical
real-world problems of using consensus
for membership it should be the the
protocol should be pretty rare and why
would that be a problem
so using consensus for membership is
problematic if your consensus system is
down it's also problematic in the sense
that like what you have to make a
decision then to say like either I
totally kick out notes from a cluster
and then like the state gets smaller or
like how do I like what if like some
nodes are partitioned from the consensus
system and they're off doing stuff how
do you how do you it's not it's a
non-trivial problem to solve and the
option basically is like you stop
serving requests at that period in time
and that's generally not a solution that
like your business partners will
tolerate most industry clusters cannot
survive half of the machines failing
that I know of because you don't
provision that high I mean some can like
a bigger systems but most people do not
run it with that much elastic capacity
that I have seen and so like so for
instance like with uber and in
explicitly at halo we made the choice
that we always want you to be able to
talk to a service and we will take the
problem of serving you stale data and
deal with the consistency problems that
generates versus you not being able to
talk to get like have a ride schedule or
be able to see your statistics or play
Halo right because that was just mission
number one is you had to be able to do
the thing that the business wanted you
to do I thank you for the token can you
give more details about how the state of
an actor can survive across so Orleans
doesn't so I'm assuming you mean in
Orleans right okay so Raeleen actually
doesn't do anything along those lines
right now and the reason some of the I
don't know if people are familiar with
are liens there's actually like a couple
papers that are not like what I call our
lead-ins Canon because they were like
ideas and then we start
with them where they were gonna try and
do this like whole transaction model and
like roll back and detect when there was
a consistency violation even in the
super available world and then like that
problem is actually really hard so I
don't think it's like totally abandoned
but what we cared about was availability
so what we did in the system and the
reason they didn't want to do a generic
solution is actually really really hard
so cuz there's a bunch of different
options so for some services we didn't
care if we persisted state between
activation because like it's a heartbeat
service or our present service was
telling you what you're doing right now
and you're gonna talk to me in like 30
seconds later or like a second later if
you could connect a device and so like
if I lose that data no one cares
statistics we actually had to persist
that and we that was one of the the more
complicated systems because that also
had to be very correct
you couldn't double count things we so
you had to persist we did it right we
treated them like right through caches
and then detected inconsistencies in
case you were in split brain and then
did a whole bunch of other stuff that I
could talk about offline on how to deal
with that and then some things are like
some services will may want to persist
you know periodically like just
checkpoint State and so right now
there's like a bunch of I believe with
or liens there's actually a bunch of
open source code on like how you can do
a bunch of different or patterns on how
to do this but they sort of leave it up
to you as a user and I think last time I
talked to them they were investigate and
how do you use like CRT tease with
Orleans because that would be actually a
really nice solution to this problem if
you don't have look you always have
Murderball state right so what do you
think so kind of the key assumption is
that the database layer or the storage
layer is slow yeah so what do you think
of something like safe for distributing
on the database layer instead of adding
all this caching logic so I actually
know stuff but I'm assuming it's
something along the lines of the the fad
now which is like everything's part of
the database so surface says it's the
storage layer so basically like this but
where the clients can go to any node in
the cluster like the clients can figure
out where so it's out I mean so this is
doing like
that except for like I still have a
database the reason a lot of the times
people do this in in an industry right
now and then you don't have to deal with
a lot of consistency problems because if
you notice I wasn't talking about
replicating data and we didn't any in
this part we just relied on our storage
systems replication and fault tolerance
and also some consistency you run into a
lot of really nasty consistency problems
which is why just throwing a cache in
the middle also off honestly doesn't
work all the times because once you
start having concurrent access it
becomes problematic and land one of the
things that will point it out with the
sticky consensus is it's very easy to
sort of reason about like and a lot of
the ways these actors are implemented or
actor models or some of these systems
like the uber ring pop is endowed so
there's obviously only one thread and
you don't have concurrent updates to
things happening so you just get to
write simpler code that doesn't have to
deal with compare modifications on your
data oh and the other thing I'm
wondering is what do you think about
Erlang language so I think so I think
our lines actually really cool so I
don't think I pointed this out but I've
like I'm super polygon a program like 10
languages in production so I'm like less
any one language versus more of a
there's just tools I think really super
cool obviously like Orleans building on
the actor model is drawing a lot of
inspiration or whatever you want to call
it from Erlang or building upon that
knowledge or liens makes a nice sort of
like or they have a programming model
associated with it that's like has types
and interfaces and stuff so it's a
little easier than life blows up because
like it doesn't accept that message and
then and then you know or leans did a
runtime which obviously like helps you
distribute this stuff which is also run
or run the cluster which like the Erlang
runtime has it as well
I think the actor model is actually
super powerful for building distributed
systems and I don't think it's you know
an industry it hasn't gained as much
adoption as I would like or leans in
akka or have some widespread adoption in
their various communities and like
obviously Earling has adoption as well
but it's by far not the most prominent
paradigm and I'm not sure like why I
think it's really great
yeah in elixir does also against age
pressure mmm yeah elixir school - I mean
played with that one as much just
because I haven't worked in like I'm
rolling or elixir shop do you think that
for implementing such an airy solutions
you always needed out from the visual
machines or can you think of systems
that like our generic at work across a
number of programming languages orbital
machines so I have yet to see one that
works across like any kind of language
right like everything's sort of like
either built into language or as a
runtime like or leans at least works
with like C sharp and F sharp and I
guess like a car works with like Java
and Scala but that's cuz it's like ah
it's like they're all still like sandbox
to the CLR or the JVM I don't know what
that would look like doing compiled
multiple things way out of my mind or
main of expertise cool thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>