<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Marc Shapiro - Encapsulating replication, high concurrency and consistency with CRDTs - Curry On | Coder Coacher - Coaching Coders</title><meta content="Marc Shapiro - Encapsulating replication, high concurrency and consistency with CRDTs - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Marc Shapiro - Encapsulating replication, high concurrency and consistency with CRDTs - Curry On</b></h2><h5 class="post__date">2015-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rVRegyQvHqs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so just a little bit of outline I'm
gonna explain the motivation for CEO
duties I'll tell a little bit about the
basic mechanisms of distributed systems
and then I'll focus on conflict-free
replicated data types what are they how
do they work
how do you make sure that they are
correct and I'll show you a couple of
examples interesting examples and then
I'll talk about composing sir duties and
the kind of invariance you can enforce
I'll tell you a little about about work
in progress of course if I have time for
all this
ok so motivation what is consistency and
what are the different degrees of
consistency so we talked we talked about
strong and weak and either weak or
eventual consistency so about more or
less the same thing there's not a
continuous spectrum between the two but
I'll try to make it simple give you a
little historical perspective and also
wonder why I bother with all this so the
the current motivation is about Geo
replications so let's say you have a
bunch of users you recognize this kind
of slide you see I'm sure you've seen a
hundred times before you have a bunch of
users who are interacting with some
distributed system some you know
Facebook or whatever and these people
are reading data but they're also
executing code or they're requesting
code to be executed and which will make
changes to a database so here we have
three data centers some geographically
separated across the world and usually
users which will interact with the
closest data center because that gives
them faster interaction and you see that
each it each one of the data centers we
have some code running and we also have
a database a copy of the shared database
right the whole point of this thing is
that people will be able to share their
data their information so the problem
here is that even the latencies here are
maybe okay
in the order of tens of seconds but
Layton sees between the data centers are
often of the order of 100 200 500
milliseconds so you need you cannot
centralize everything also for reasons
of scale there too many users you cannot
centralize everything also for reasons
of fault tolerance and availability if
something goes wrong in the network you
want to be able to redirect the user to
a closer data center or a different data
center for instance so this is why we do
do replication but of course and and and
there's another reason also is more and
more of these applications are actually
running on the mobile computer itself so
this mobile computer may be completely
disconnected from the network in which
case the latency goes to infinity so and
and in order to continue to interact
that is to maintain availability the you
need to have a again a replica of the
data and of the of the application on
the mobile computer so the problem is
how do you make if if this guy makes an
update how does this update become
visible to that guy over there or to
this guy over here who may be
disconnected at the minute when the or
update happens so usually you start
little start with the single data center
you have a company in Paris with a
database of its employees and you can
you know you can do updates to this
database and making employee nearly
purple whatever that means a blue sorry
then I'm adding a new employee Bob and
well someday my company becomes richer
and now I want and I want to open a
branch in California so here I do this
replication thing I open a I make a copy
of the database in California and now I
want to do exactly the same thing right
so I want to say add new user Bob in in
in California the problem is that if you
want the two databases to the two
replicas of the database to be exactly
equal that means that every
we do a an update on one side you have
to synchronize with the other side right
and you're not allowed to return to the
the the the users may be making this
call to add Bob but you cannot return
this call from this call until you
finish the synchronization okay
and I just told you that there's a
hundred or 200 or 500 milliseconds wrong
chip between these two so update on on
the California side update on the Paris
side every time you have to synchronize
you have to wait 500 milliseconds
whatever it is so you whereas when you
hand you your single database you've had
lots of processing power and parallelism
in cetera oh and you maybe you could run
tens of thousands of transactions a
second now you're down to two
transactions a second or 10 in
transactions a second right so all your
expensive hardware is being completely
wasted so this is the reason why people
are looking at a different approach why
don't we just do it differently
that is when I want to make an update
here in Paris I just do the update
locally return to the client and
propagating the background whatever time
it takes so this means that even if the
if the network so in the previous case
if the network is down I see everybody's
stuck right because there's no way to
synchronize here even if the network is
down I can I can synchronize so that's
that's I think sorry I can make my
updates so the data to my database is
always available and furthermore it's a
lot faster because I never have to wait
okay so that I put in a lot more
parallelism and I can use my hardware
completely but of course that raises
another question what happens now since
I'm not synchronizing I could have two
concurrent updates
Tonelli right in California they're
trying to make Nellie read in Paris of
trying to make Nellie blue and when I
try to propagate I'll see hopefully I
will realize my system will realize that
there's a problem what would be really
nice is that there was some if there was
some magic that
and combine the two updates right so I'm
going to propagate and ah blue and red
becomes purple right now you know it's
easy to put in the slide but how do you
do this in a program how can you ensure
that it always works I mean there's
clearly there's some magic here
underneath and well you'd like to have
some kind of guarantees that this is all
going to work so this is what the talk
is about how do you guarantee that
eventually the first of all that your
operations are going to be mirja belen
the first place and that it's going to
converge so this is called different
things usually eventual consistency a
little bit of historical perspective
there different approaches there's
different reasons why people want
eventual consistency and there's
basically two classes one is because
you're doing some kind of cooperative
work right so you're used to using SVN
or git or whatever which is one form of
intervention consistency you disconnect
from your friends you want to want to be
able to make your updates to your local
copy of your code independently of other
people without being bothered by them
right so you do your updates take your
time and when you're ready your
propagate and you merge okay now
this there's a human in the loop here
right the time it takes is very slow and
the human is very tolerant of you know
anomalies and is willing to fix things
by hands if it doesn't work out so
there's a lot of work on eventual
consistency that is along this line but
this is very different from the line I
just described earlier where we where
you have concurrent updates happening to
databases at a very high rate and of
course you want to you want that you
want the merge to happen automatically
with no human intervention so you have
sort of different takes on this and I
will concentrate on on this one today if
there any questions just asking right
and so the the sort of the crux of the
problem or one of the most obvious
cruxes of the problem is how do you
reconcile how do you merge these
concurrent updates and you can see from
this history that it's it's taken a very
long time for people to understand the
problem so if you don't understand it
don't worry you in very good in very
good company right so the the first
paper on last writer wins is 1976 see
our duties is what no 2011 and of course
there are lots of people who tried
different things in the way a lot of it
was at hawk some of it has principles
behind it but unfortunately these
principles are not always correct
so operational transformation which is
very well known most of the operational
transformation algorithms out there are
have been proved to be incorrect
they will not converge there's been lots
of work in the 80s 90s etc on file
systems most of it is incorrect for a
reason that I will show you and you know
what the first person to actually
understand sort of put down the theory
is a back arrow and moron in 1999 but
their paper went completely ignored but
there there are now co-authors on on the
CRT T papers so since this apparently
this is also complicated why do we
bother
well there's a very good reason why we
bother this you know this is a
performance slide I'm not going to
explain what what it means but it looks
nice right but the important thing here
is to compare you have you know
different experiments so basically we're
running some workload of updates to a
database and using different underlying
consistency models and here we have
let's look at this one for instance here
on the left you have the strongest
consistency model and here on the right
you have some of the weakest ones all
implemented using the same code base so
they're comparable and the the absolute
numbers don't don't really matter what
you have to see this each point here
represents some workload latency sorry
to put in latency so at this point you
have a throughput of whatever maybe 500
transactions a second and each
transaction takes hundred 50
milliseconds to terminate right and as
you increase the load your database
saturates in some point it goes bad
right but here's the difference between
strict sterilize ability which is the
weakest sorry the strongest consistency
model that guarantees that everything
happens in this trick serial order
consistent with real time and this one
here which is
yeah but I'm looking for this one here
you can't see it oh it's there no I'm
looking for I'm looking for are see
there are see yes
so are see is a very very very weak
consistency model and right so you can
see that at this sort of the knee of the
curve which is the interesting point the
latency is three times better for RC and
the throughput is four and a half times
better right so basically with the same
kind of the hardware you get twelve to
fifteen times better performance right
so you can spend twelve to fifteen times
less buying data centers right so it
makes a big difference okay so I'm gonna
tell you a little bit about the the
basic mechanisms so typically people use
what's called read one write all okay so
here the this graph is this animation is
going to explain what this means very
simple it means that every time you read
you just read from your local copy since
it's a replicated data so you just read
from your local copy but every time you
write you have to update all the all the
copies and here's the difference between
weak and strong consistency right so in
let's say you have this up so client you
so these question marks just mean this
is query and this just means it's a it's
an update so in order to do some some
transaction you queer that the state of
the database and you say ok now I want
to make this change this effect right
and you propagate the effect to all the
all the different replicas now the
difference between strong and weak
consistency is that it's under strong
consistency you cannot return to the
client until all or at least a majority
of these guys have answered so you have
to wait it's slow and in strong
consistence in we consistency you can do
that and therefore you you end up
overlapping different updates so now you
see that this guy has received update
you before update V
and this guy's received them in the
opposite order so of course if you're
not careful the results here and here
are going to be different the important
thing here is that you cannot it's
important you cannot talk about global
state there is no global state right the
best thing you can get that is the
closest to global state is what happens
when all the updates have been received
so all there's no updates in flight and
so that's a local state and what you
hope is that all these local states are
going to be the same that is it
converged but you know it's not going to
come for free so there's different ways
of implementing replication basically
you we distinguish the state based and
or data shipping from operation based so
state based basically as the name
implies means that every time you want
to propagate your updates you just
simply send your state look at the
content of your memory or some some
representation of the content of your
memory so here I have two concurrent
updates U and V having a two different
replicas they make their local change
right and then you just send the whole
state to the other guy and the other guy
merges in the state somehow and from
this state you may take the merged state
and send it to someone someone else will
again merge it into his own local state
and if you do this you know often enough
and randomly enough eventually everybody
will get everything right which is what
you wanted but of course the problem is
how do you ensure convergence I will
come back to that in a minute
the other approach is called function
shipping or operation based here what
you ship is not the content of the of
the data
it's the code for the change or the
change itself the Delta so here I'm
sending to the change u that happens
first on the middle replica to the other
two
and here I'm so now this other operation
X which happens we'll see the result of
you and therefore we say that you is
visible to X even though you happened on
a different replica and here we have a
concurrent update V and V does not see
the result of U right it's not visible
so again the question is how to ensure
convergence but before I get to how do
we ensure convergence I want to come
back to this fact that X has seen the
result of U so you would in most systems
most systems just ensure first-in
first-out delivery or sometimes even
less than that which means that
sometimes X could could be delivered
before you to some places and that's
anomalous and I'll show you what that
means so let's take let's take a
specific example here we have three
replicas of your you know social network
application where you post photos and
Alice has a replica at home on her home
computer and she she she she executes
the operation that says in the future I
don't want this directory of photos to
be visible to my boss Bob and then she
goes and and and she propagates that and
then she goes to her phone and post a
photo but if things are sent just in
FIFO order that means that Bob or Bob's
computer might receive the photo first
before receiving the change to the to
the permissions and therefore Bob will
see the photo even though he's not
supposed to
again so what happened here is that the
system did not realize that there is a
dependence between V and U because
because that dependence is sort of
indirect transitive through through
through through Alice's replica right so
this transitivity is tricky and it's you
know most systems do not actually
implement that so what you would like is
what's called causal order delivery in
which since V is dependent on you then
you want V to be delivered after you
everywhere okay so call order delivery
says that if if you have two operations
you know U and V VC observes the effect
of you therefore V should be delivered
everywhere after you but if u and V are
concurrent then there is absolutely no
no no no no single order right so causal
order delivery is a bit tricky to
understand a bit tricky to implement but
you can implement it without doing
synchronization right all you need to do
is to have some kind of metadata
attached to V that says V depends on you
right and if u is delivered first sorry
if V is delivered first the system sees
oops it depends on u there's a gap I
need to wait right and you just wait
until you arrive and then you push the
afterwards right so you just buffer V
for a while but that requires some
mechanism and some metadata and metadata
has a cost there's another piece of
metadata which is simply the log of your
updates right which you need to keep
this log of updates in order to be able
to deliver your updates and as long as
there's somebody out there who hasn't
received the update you have to keep it
right so you have to have to do some
kind of garbage you can you can do some
kind of garbage collection only once you
sure that everybody has received you
update and that is also costly okay so
what is eventual consistency you may
have read definitions there's a very
well-known definition that says eventual
consistency is when a client's stop when
all clients stop making updates then all
replicas have the same value so by that
that's really bad definition because
clients never stopped giving updates and
furthermore they the just saying that
the that all the replicas have to
converge to the same state doesn't tell
you what state that is gonna be doesn't
say whether it's a correct state so I
have what I think is a better definition
so the top here implies the the earlier
one so I say correctness conditions and
distribute systems usually have a safety
part in aliveness apart so liveness
means something has to happen and safety
means something good has to happen or
only good things can happen sorry so the
lightest part is that if I deliver an
update to some replica then all the
replicas have to eventually hear of it
and apply it and the safety part is that
any two replicas that have received the
same set of updates even if they're even
if there's more in flight doesn't matter
if they've received the same set of
updates than they should have the same
state and I don't care I explicitly do
not care about the ordering of those
updates and I don't care about anything
else than the set of updates right
because that's the only way if if you
depend on anything else than the set of
updates then basically it's
non-deterministic so these this is this
is eventual consistency and but there
are other other properties out there
that you often want in addition to just
eventual consistency so first of all you
want to make sure that the state
satisfies some kind of specification
so I'll talk about the specification of
Siri keys in a minute
so it would be if you just say they all
have to reach the same state well that's
easy just make everything know and empty
the database and everybody will have the
same state but that's not very
satisfactory so you want to have a
specification of what the state changes
should be there are some systems that do
this by doing some kind of tentative
update and then rolling back and then
another tentative update and then
rolling back that's really bad
so you don't want that you want it to be
monotonic and as I explained earlier you
don't want these anomalies these message
delivery anomalies you want causal
delivery okay I'll skip this one and
I'll talk about certainty so what are CR
duties
er duties is basically so it's a it's
data that's replicated and which is
updated without any synchronization so
just like I explained earlier but the
two new the two sort of key ideas of C
or D T's are one that a CO DT is a data
type and therefore you encapsulate all
the complicated things inside the data
type so that programmers can use it
without thinking but the second key idea
is that you can ensure that your data
will converge to a correct value by
construction and there are some very
simple properties that ensure this which
we will look at now so first let's take
a state-based ERD T so here we send
state as I said so here we have some
update at the at the top replicas F and
G can cook concurrently at the middle
replicas and we send the state and you
merge etc and you merge just like I
showed you before now how do you ensure
that this converges well when you think
about it what you want is is is is is a
few things let me bring up the next
animation so first of all you don't want
to go backwards right so what that means
is that there
some somewhere in your data type in the
definition of your data type there's
some implicit order right once I've done
an update I never want to go backwards
there for each each new update should
dominate the previous one so there's
some kind of order implicit I I haven't
told you I told you that you just send
stayed around and it's basically it's
random so you might receive the same
state several times you may receive or
you may receive different copies of
different states that I think that
contain the same update
therefore you want that to be idempotent
and the thing is you never know in which
order the merge is going to happen and
therefore the merges have to be
commutative right and also emerge may
contain multiple updates and therefore
the merge has to be associative so if
you look at math theory and something
that has the partial order and an
operation that is idempotent commutative
and associative is called the semi
lattice so the the only property you
need here for convergence is that your
data type is a semi lattice and you also
want this monitor monotonicity property
so we call it the monotonic semi lattice
so it's a semi lattice where the merge
operation is the least upper bound or or
join so let me take an example let's
consider a left-right of winds register
so less Rider wins
is so less red wins register is a data
type where the register contains some
value and there's an Associated
timestamp and when to it when when you
merge two concurrent when you merge two
updates you just compare the timestamp
and the one with the highest time stamp
wins over the other one so it's just
like if you consider a sequential
execution it's just like a normal
register but when you have concurrent
updates the one with the highest time
stamp wins over the other one so how
does this work here we have a write of
some so initially we have value a with
times
zero and now we're at the first replica
replica one we're gonna write value B
and the time is one and we're also going
to add the replica ID in order to
distinguish concurrent times concurrent
updates so it's B width times to have
one point one okay yeah no this is
logical time Lamport clock it's logical
time you just keep the local counter
every time you do an update you increase
your local timer okay or every time you
receive it send a message send a message
but of course real time works as well as
long as you have the two parts your
local time plus the the ID so that you
can distinguish concurrent updates okay
so here we have the B time stamp 1 1 and
here we have another concurrent update
time step it happened at the same time
so the time stamp is the same one but
the second part is different too all
right anyway third one here and when you
merge them well
time stamp one one loses over I mean
times have one two wins over time stamp
1 1 and therefore the final value here
is gonna be C yeah okay
and same thing here D with time step 1 3
is is gonna win and therefore it will
converge so I'm going much much too slow
but if there are questions go ahead so I
will skip all the animations and go
straight to operation based CR DT so as
I said operation bastes the operations
can arrive in different orders so the
final state here if I've received F
first the final state will be G of F of
the initial state and the final state
here will be F of G of s right so if you
want to be sure to converge or
is that operations gnf commute and I
will skip the last rotor wind example
but the truth is that most data types
that you're interested in do not
naturally commute so how do you make
this converge anyway well you just have
to define away the non commutativity so
and and have some way of resolving
concurrent operations so here's an
interesting data type the set um the
sequential specification of the stet
sets is that whatever the initial state
if i add e then ease in the set whatever
the initial state if I removed e e''z
not in the set anymore then no
duplicates okay
most operation pairs commute you can do
the exercise that's easy add e with
removed F at F with movie and with ad
etc all this works okay so I don't need
to worry about them but there's one pair
that does not commute right if I say add
e followed by remove e it's completely
different from from saying remove a
followed by Eddy so now if I want to
have concurrent at E with remove e I
have to do something what do I do
well there are different approaches you
could say well let's try to synchronize
right so the way that we all agree on
which one is going to is going to come
first
but of course that's going to cost you a
synchronization we tried to we were
trying to get rid of that so let's
forget that which is a perfectly good
solution if it's fine for you but right
now we're not we're going to forget that
and so there are other approaches the
only thing the only thing you need to do
is to make sure that you're the result
is going to be deterministic right it
only depends on the set of operations so
if the set of operations is at you
remove e I need to have some
deterministic outcome and that could be
well I might have a timestamp and say
the one with the highest time stamp wins
or I could say concurrent add and remove
of e is an error and just put some error
indicator in the in the set or I can say
I always want the ad to win or I want to
remove two wins or I always want my boss
to win or whatever right as long as it's
just
monistic you're okay deterministic
independent board of a delivery
independent of local state and of course
you don't want any synchronization so
for instance ad wins seems like a cool
thing to have so let's see how can you
implement
Edwin's so this is what we call the our
set means observed removed set so I'm
gonna have to go pretty quickly through
the animations so basically the the the
behavior is what's down there so let's
say at the beginning my set is empty and
I add element a at the top so what I'm
gonna do
the key thing here is I'm gonna just
distinguish elements by the replicas
where they have been added or removed
okay so even though it's a set and
there's only one a okay even if it was
added 27 times I will still distinguish
internally in my concrete implementation
this a as being a blue a it's coming
from the top replica and concurrently
the middle replica inserts a brown a so
when we propagate the state this is
state based if you propagate this stage
from the middle replica to the bottom
replica and merge you take the union of
the local states and you have brown a
and then you Road propagate the blue one
and now the state at the bottom replica
is both brown and and blue a but of
course there's only one a it's a set now
if I remove a well what a could I remove
at the top replica the only one I can
observe is the blue one therefore I
cannot I cannot remove the brown one
okay that's why it's called observed
remove sets so now we'll add a deleted
marker what's called a tombstone to my
element eight and again if i propagate
and merge now the final state is brown a
inserted and blue a has been removed so
a's in the set right because it's
this one doesn't exist and you can see
that indeed we have a concurrent
concurrent ooh remove an ad and the ad
wins because we still have the brown
eight so it works it does what it has so
the the semantics here is that so the
sequential semantics is the same all the
concurrent pair all the commutative
pairs remain the same
but this one which is not commutative
you just decide that's my decision
whatever the initial State when I have a
concurrent add and remove I decide ease
in this end that's why it's called ad
wins
that's called or a set because you only
remove the instances that you observe
now you can see there's something not
very nice here is that I have to keep
this tombstone around so can I get rid
of this tombstone indeed I can if if I
again and so if I have causal delivery
then I can take an advantage of that so
here I'm not going to have tombstones at
all when I say remove a at the top
replica you just take a out of your
representation and when you propagate
you're gonna propagate this empty set
and when it gets here this guy's gonna
say well I saw a blue a that was
inserted and now blue tells me there's
no a right and because of causal
ordering I know that they cannot be
swapped right so if I saw a blue a and
now there's no a it means that a has
been removed I can take it out so I
don't actually need the tombstone I want
to tell you about another very cool data
type which we didn't invent it's called
a replicated global array RGA it's not
well known but it's very cool basically
it represents a linked list of elements
so it's something you might use for for
a concurrent editing for instance and
each element has a value so i or n here
and a timestamp and just like the for
timestamp
as a time part and a local replica ID so
here's the my initial state of my linked
list so beginning and end guards and
then to two elements in the middle and
now I'm going to I'm going to insert a
new element to the right of n ok so I
just update the linked list so you
remember this is replicated on so this
can happen these inserts and removes can
happen anywhere and now I'm going to
insert something else
to the right of n and so this this
insert happened on replica two in this
insert now is happening on replica three
and just notice that replica 3 is going
to insert two things after n so first it
inserts I right so I comes after N and
now it's insert something else after n
which is our and therefore it comes it
comes first right because it was they
were both inserted after N and you
notice that the timestamps are when this
happens the timestamps are ordered in
opposite order
right so the higher timestamp is to the
left so we're going to use that fact to
resolve concurrent updates in the same
way so now if I have an insert let's say
at the beginning I have two concurrent
inserts at the beginning with timestamp
43 and 42 right well I will resolve them
in the same way and I'll say well the
higher timestamp goes to the left and
this is the result so it's a very simple
construction that allows you to maintain
a sequence of elements and it's very
useful so we have many CITT designs and
you can use these different designs too
you can compose them to build
bigger programs so registrars less
writer wins different kinds of sets once
you have sets you can do maps when you
once you then we also have counters and
graphs which are just compositions of
sets and sequence such as RG that I just
showed you but I want to show you some
problems that you have with CRT T's so
what happens when you compose CRT T so
let's say we want to write let's say you
want to compose you want to create a
file system for instance so you have a
file system tree you have directories
each directory is a map map is almost
the same thing as a set you can use or
set to create your map and each file
could be some kind of CR DT data type so
let's say you have a sequence and
therefore in each file you can do
concurrent edits because you're using
RGA for instance ok so as long as you're
just looking at individual files of
directories everything seems fine right
you just you just have to apply the
semantics of series or C or D G files
but it doesn't work that way all right a
file system is more than just individual
files of directories there's also this
tree relation between them so what
happens for instance if you delete
something let's say a delete a while at
the same time I'm inserting something
under 8 so you have to come up with some
reasonable answer for that it seems that
most people in file systems in this case
if you delete a directory and insert
something underneath or edit something
underneath they will recreate the
directory fine but there's a harder
problem what happens if you try to move
ok so let's say here I want to move
directory a under D
on the top replica and in the bottom
replica I'm gonna move DeAndre so I
can't do that in sequential code because
in sequential code there's a
precondition to move that says you
cannot move under yourself right but
here you're not moving under yourself
because it's all happening concurrently
so I could move a under D and I can move
D under a and when I try to merge whoops
I get something that's not a tree and
there are millions of distributed file
systems out there that have this error
so what happened what happened well as I
told you there's a precondition to move
that says you're not allowed to move in
a sequential code you're not allowed to
move something under itself what
happened
and so neither of these replicas moved
under itself right but because the other
replica was doing something at the same
time it ended up it ended up moving
under itself so what happened is that
there was a precondition precondition
that is violated by a concurrent update
right and that's that's the typical
skate and that's sort of the general
case of invariant violations so that's
something you have to worry about so I
will since I'm out of time go straight
to my Easter egg for you guys if I can
bring up my slides
so let's say we're composing some some
CEO duties and we want to maintain some
invariant some generic invariant it
doesn't matter what it is okay and so I
have a specification of my system where
every update is specified in a
sequential way so it has a precondition
and the the precondition is necessary to
maintain invariant so if I just if I if
I if I just run this code sequentially
so I assume that every operation has an
in very condition if you vary it is true
initially the invariant is true pretty
initially and the precondition of the of
the operation is true initially then the
invariant has to be true after you so I
mean this is typical sequential code
sequential composition if each
individual operation maintains the
invariant then any sequential
composition maintains the invariant but
I need to check that of course I can I
can if I have the specification I can
run this in a proof tool and prove that
this is true okay but I also want to
make sure that my precondition is stable
that is is not violated by concurrent
update so first part of the of proving
this environment is is doing this
sequential proof so invariant is true
initially I run you and I want to be
sure that the invariant is true true
afterwards so that's my proof obligation
and if that's true on this first replica
well it's also true on the top replica
right so by by did by since I prove that
on the first one for generically I can
assume it now on the SEC on all the
other ones what happens now if I have a
concurrent update so as I said I want to
make sure so this means concurrent
alright so if they have if if I allow
these two updates to be concurrent then
I must be sure that the precondition of
one is not invalidating by the second
one right so I need to prove that
running V Dupree the invariant of you is
still correct right and if that's true
just by applying the other one
well after applying view you the
invariant is also correct right so if if
I can prove this proof rule so what's
really neat here is that I don't have to
look at all possible combinations of of
updates I only need to look at pairs
right if for every pair the precondition
is stable under that pair then for any
combination it follows that the
invariant is is true so we're actually
applying this now and we sort of
applying the rule backwards so we say if
well if if if we can prove this then
we're well then we're happy
iron variant is true if we can't prove
it we so we give this to them too if
your improver and if it's incorrect your
improver will tell us will give us a
counter example and that tells us well
here we have a problem with our either
our invariant is too strong or our
precondition is too weak so I just need
to strengthen our precondition and
strengthen the precondition might be
well I'm not going to allow you and V to
be concurrent and therefore I have to
put in some synchronization so the
upshot is now you can take a completely
concurrent system based on certainties
combine the co DTS and infer the minimal
amount of synchronization you need to
prove to to maintain that one invariant
that you care about right so instead of
sort of the old way was okay if I have
Co duties I have lots of concurrency I
can can't do it I can't maintain it in
invariants like the tree invariant so
therefore I will use full sequential
phases so full sequential consistency
reliability or whatever right so you
take the similar one of these very very
strong models from your library of
models here it's the way around we start
from something that's completely
concurrent right and so therefore
available etcetera and fast
and we add just in and if you need to
maintain a specific invariant we had
just enough synchronization for that
okay I guess I'm really out of time so
I'll better stop here and I hope this
question or two yeah two three four yeah
you thought that clients never really
stopped updating but the safety rule you
said is that if many clients receive the
same updates they will have the same
state in reality I think that never
happens - so is that will strong enough
to express what things like the auditors
are able to achieve that's the best we
can do so we cannot stop clients from
sending updates but we can sort of
isolate a subset of updates and have two
imaginary replicas that receive just
that subset of updates and if those
imaginary replicas are equal then we're
okay right and then we do this for all
any any combination of updates and
therefore it'll converge yeah there is
no good solution there is no good
solution that does not use
synchronization so you're so you're
you're you you can either accept that
you lose the thing that is not part of
it has been disconnected from the tree
there's some systems that do that many
systems that do that actually without
knowing it it's called a book you can
sort of try to recreate these you know
replug these things back into
directories but the probability that you
can get something that will actually
converge by doing that is very low or
you can introduce some synchronization
so move the problem is not trees the
problem is moving
no no the synchronization is if you have
one move that violates the precondition
of the other move then you're then
you're in trouble so you have to check
the preconditions so what are these
what are these so if if I'm doing the
move within a left subtree and a
different move within the right subtree
right and they don't interfere there
okay I had a second question could you
show us slide 65 this is it that's like
fifty nine oh I guess I changed the
numbering yeah so you say eventually
consistency is the only thing that you
can really achieve in a distributed
system without I mean getting bogged
down by synchronization and all that and
we are preaching to I mean telling
everybody the same message so what the
question I get back many times is from
people who are used to strong
consistency because they have told by
have been told by Oracle that that's
what you get they are used to a
programming model that just wastes
synchronization in a sense now you've
sketched an answer here that you might
figure out the minimal amount of
synchronization needed but so is that a
practical answer that we can give today
or is it still too early I would say
it's work in progress so we have we have
the rule the rule we have the tool we
have we have proved a few of these
things and we have derived a few
protocols that's the best I can say but
it's obviously a lot more work right I
mean if you if your system gives you you
know serialize ability or whatever and
that's good enough for you by all means
do that I mean eventual consistency is a
lot harder so it's only because you you
want 15 times better performance that
you do this
yeah when you are talking about merging
changes for example in the set example
you repeatedly refer to concurrent
updates how do you distinguish
concurrent updates from non concurrent
updates and aren't all updates in
principle concurrent instances so I
skipped tons of slides no that's why
causal ordering is important so cause
lording tells you this thing depends on
that thing so if this thing depends on
that thing it has to come afterwards and
therefore I don't care about concurrency
right and I better apply and if this
thing comes after that thing I better
apply the sequential semantics or else
I'm going to confuse my users okay so
when things are causally ordered by all
means apply the sequential semantics
that users understand Thanks but but but
that means your system has to give you
closer ordering and there very few
systems that give you calls ordering so
we can do the last question okay so you
said that on this slide if you want to
have two operations that can work
currently you have to like for each pair
of operations that want to be able to
have concurrently prove that like the
invariants and the preconditions are
stable thank you
respect is what most occurs of all of
magnitude of the number of proofs you
have to do there I mean is it one or two
or thousands or does it really depend on
what you're doing the the hard work is
writing down the specification right so
writing down exactly what your
precondition is what your post condition
is what are your invariant is that's
hard work and then once you give it to
the proof engine that's easy something
it takes a few seconds or order thank
you know we had we did a couple of me to
examples right but fairly complicated
examples and we can do the proofs on in
every time okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>