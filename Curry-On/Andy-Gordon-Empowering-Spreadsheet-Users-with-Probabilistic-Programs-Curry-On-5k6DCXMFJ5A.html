<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Andy Gordon - Empowering Spreadsheet Users with Probabilistic Programs - Curry On | Coder Coacher - Coaching Coders</title><meta content="Andy Gordon - Empowering Spreadsheet Users with Probabilistic Programs - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Andy Gordon - Empowering Spreadsheet Users with Probabilistic Programs - Curry On</b></h2><h5 class="post__date">2015-08-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5k6DCXMFJ5A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I love this this event that were
bringing together academics and Industry
types and I work for Microsoft in about
em both I work for Microsoft in
Cambridge in UK and I'm also a part-time
at the University of Edinburgh so so
this talk is about a hot topic in I
guess programming languages for machine
learning at the moment this idea of
probabilistic programming and that's the
idea that you can write a probabilistic
model of data as a bit of code and then
and if you were to run it you'd get sort
of synthetic data but then if you
compile it in a certain way you can turn
it into inference code that will learn
parameters and make predictions from
actual actual data sets and we've looked
at this problem and we said well look
where should were you doing where should
we be doing this we should be doing this
where the data is we should take the
code to the data and an awful lot of the
world's data lives in spreadsheets so
this project which is code named tabular
is all about taking what this sort of
the research that's been done on
probably programming languages and
embedding it within a spreadsheet
environment so that people who are not
necessarily developers are able to do
the machine learning using public
programming within the spreadsheet but
to begin with I want to give you an idea
of a probabilistic programming and and
what I'm going to do is give you a
little sort of probability puzzle and
i'll tell you in words and then i'll
show you how we can write it in code so
imagine your English English country
country house and has been a murder
let's say professor plum has been found
dead you know who has killed him and
Alice and Bob are in the frame and you
know a priori you know if I'm the
detective coming to the house I think
it's most likely to Bob done it so this
is a seventy percent chance Bob done it
thirty percent Alice done it and then
there's two possible weapons may have
been used we don't know which at this
point but we do know that Alice would
really prefer to use the pipe
ninety-seven percent and Bob would much
rather use the gun so that's our sort of
probability situation before we've
actually visited the scene so Hank how
could we write that as code
here's a little bit of code that does it
so I first of all have a boolean
variable that is true for Alice then it
falls for Bob done it and so we flip a
coin so I assume I've got some sort of
random primitive that takes a
probability and it tosses a coin so so
this is going to come up thirty percent
of the time true during the gate Alice
done it and seventy percent false to
indicate that bob has done it and then
the second variable is whether the gun
of the pipe has been used and that is
conditional on the the first variable so
if Alice has done it she's going to got
a three percent chance of using the gun
whereas if Bob's done it he's got an
eighty percent chance so this is our
model of the situation and the way that
you get the really simple way to
understand how we're just defining a
probability distribution by a bit of
code is if you think about just running
the program many times and then counting
up the different possibilities so this
is called sampling and so if you were to
run this program many times this is the
distribution of the four possible
possible values you get so it's most
likely that it would be Bob using the
gun so it would come up here and then
less likely Bob using a pipe and I think
something like three percent chance it's
going to be Alice using the pipe and
very unlikely that its Alice using the
gun so that's a sort of prior situation
so this is and then we actually visit
the scene and it turns out that a pipe
was found at the scene so this now lets
us update our model of the situation in
light of this data and the way we would
represent that is imagining we have a
sort of assertion within a programming
language called an observation it's an
assertion that some that constrains the
variables in this case we constrain that
with gun is false because it's the pipe
that's been used and you can again what
does this mean operationally well we're
interpreting a publicity programming by
running many times and what we're going
to do is Mark some of the wrongs is
invalid they're incompatible with the
data so we're going to chop out those
runs and in this case we're going to
chop out the situation where with gun is
is false so we're going to we're going
to chop out yeah although the case
Square the where it was the gun that was
used and so you get this sort of thing
so basically have basically removed the
you know these possibilities and then
renormalized and now the situation is
flipped around so to begin with we
thought it was most likely that bob has
done it but but in the in the in the
world in the light of the data it's much
more likely that alice has done it an
improbability notation we'd say at the
top what this graph is showing is this
is the child of these events the
probabilities of C and W and down here
it's the probabilities of scene w
happening given that the the width
variable that the actual weapon and was
the pipe so that in a nutshell is
probabilistic reasoning sometimes people
call this Beijing reasoning about a
situation where we we represent we have
some sort of sometimes it's called a
generative model or you know we have a
promise it program that represents our
review of how the data is generated and
then we place constraints on the actual
generated data in light of the data so
you can do probably sick programming
lots of different languages and I mean
you can be functional you can be
imperative and there's really three
things you add to your language to
represent probabilities one is some sort
of randomness so you can you can draw
you can toss coins you can you can roll
dice you can also draw from say Gaussian
distributions to get you know numbers
you have constraints like my my observe
equal operator and generally what you're
observing is that some synthetic value
of a data equals the observed data and
then finally you're trying to infer some
some variables in this case it was
whodunit so you have the some way of
saying which variables it is you want to
infer and you know it in any of these
systems that the kind of the rallying
call for why are you doing this is that
it's much easier to represent to write
code that sort of simulates a situation
and then have the compiler generate
inference code that will learn your
parameters and make predictions then it
would be to do all that from scratch so
typically the programs you write very
short maybe 10 20 lines is very very
typical and but the equivalent inference
code would be thousands of lines and the
compiler will generate that
automatically for you now there's lots
of systems out there most of which are
sort of in the research community but I
call that this one bugs so bug stands
for bayesian inference using gibbs
sampling and this is the granddaddy of
the field and that goes back to the
early 90s and this is actually quite
widely used in practice and then there's
a great many other systems like church
is like probabilistic programming in
lisp what else hand side is sort of
problem stick programming or camel the
system that we're building on is called
Infernape and it's probabilistic
programming in C sharp and we did a
version uses a sharp but but I'm going
to talk about today's is using Excel and
a range of other such languages and if
you want to understand the semantics
there's a bunch of nice work using the
probability monad so people are
interested in one ads and what have you
the dangerous topping to bring up those
some good papers to go and read about
how that is done but i'm not i'm not
going to go into the details of that at
all and to date and this is a slogan
from kathleen fisher really that about
the the general aims of the field that
we want to sort of make you know
development of machine learning systems
as easy or as easy just writing a short
bit of code you know it's as if you know
we've been in the world of assembly
language of machine learning we want to
kind of raise the level of abstraction
so finally this talk and I like to get
onto a demo very quickly to show you
doing machine learning inside excel but
I want to say a bit about our intentions
so you know that there's a there's a
range of populations of users we'd like
to reach out to so there's a small kind
of group down here the machine learning
PhD someone who actually is a specialist
in machine learning who is capable of
writing the inference code from scratch
then there's a larger population of
users the ML developer someone who's you
know maybe a c-sharp or Java programmer
who would like to you know who's build
trying to build machine learning systems
we'd like to make you know enable them
then there's the the data scientists
someone who's maybe a statistician by
training whose's are and maybe he's not
so much of a programmer but it was a lot
about stats and then there's a date
enthusiast and so this is the sort of
the person the business person or the
scientist maybe a doctor who is curious
why it is that some patients returned
quickly to the hospital and not others
or the sales the car sales person wants
to know why some cars do well auctions
and others
someone who cares seriously about the
data and has some sort of business you
know professional reason and
understanding it better there's lots of
those people and we'd like to reach out
to them I'm afraid at the moment our
system is probably best used by machine
learning PhDs so we've got a bit of a
journey to make it more usable but but
the first step is getting into Excel and
then we're going to try and make it more
automatic so we've got three guiding
principles we r you can't do machine
learning where that data and therefore
you're going to have some sort of data
schema typically relational schema a
bunch of tables with foreign key
relationships between them and the basic
idea of tabular is it's a
domain-specific language that starts
from that schema and lets you mark it up
with probabilistic annotations that
explain how the data is generated and
then and you know and and then we can do
inference we do inference we choose
which variables to query by just leaving
blanks so I'll show you that in a moment
so we just leave blanks in a column in
Excel and then the model will try and
fill them in and finally I won't say so
much about this but this is our
aspiration we think at the moment our
system is usable by the kind of by this
sort of the darker kind of hughes down
here we'd like to go up to the lighter
ones and to do that we won't tell
auto-suggest models that will be useful
in different circumstances and we've
made some progress than that but we're
not going to done all right so Excel
okay so here's a tiny data set and and
this is a problem of ranking okay so
I've got a table of players Alice Bob
and Cynthia and I've got some matches
between them so Alice placed Bob and
this is saying that player this is a
boolean saying whether player 1 has one
or not ok so in this case it's saying
that Alice lost a bob and then this is
saying that Bob lost is Cynthia and I've
left a blank I'd like to fill this in
perhaps Allison Cynthia are going to
play tomorrow and I like to infer how
likely it is that alice is going to you
know be Cynthia perhaps I want to decide
whether to take a better not so that's
the that's the data I have and I will
start up exhale start up tabular I
should say that tabular is a download
from the Microsoft Research website and
you're free to download it i'll give you
the URL in a moment so tabular is up
there we start it on the side and one
thing i should say is that we are using
a feature of excel that's fairly
recently been introduced that it builds
in a relational data model so we're not
extending excels formula language to do
probabilistic modeling we're sort of
extending the relational model that's
built in so this UI here is just this
data model so you see it's got a table
here of the players and a table of the
matches but I'll just put that to one
side so the first thing i could do is
ask tabular to fill in a default model
okay so tabular knew i had pre-loaded
actually i told it that there were two
tables the table of players in the table
of matches and if i make this a bit
bigger you've got a chance to see okay
so this is a simple tabular program so
type of the programs are sort of fortran
like they sit in four columns you know
that the this column here Colome has got
the name of the columns in your database
so the players database at the moment
has got no columns apart from the
primary key the name of the the player
and the matches table has got em three
columns it's got player 1 player 2 and
win one which is a boolean and we filled
in a default model which won't do a very
good job of inference so I'm not going
to use it so what i will do is well i
can show you it let me run it so what
it's done is over here it's done
inference and it's put the results back
into the spreadsheet okay so and in
particular it thinks is that there's a
quarter chance that Alice will beat
Cynthia and it's done that just on the
basis that player 1 has tended to lose
to player 2 so it's got no kind of
knowledge about the actual skills of the
the players ok so let me just let me
just delete the results
just go there and then we come back here
and so what I'm going to do is a model
called true skill and the idea is that
every player has got a latent variable a
hidden variable that is their skill it's
a number and let's say that's the number
with mean 25 in standard deviation 10 so
roughly it's between nothing and 50 and
then the model is that when two players
meet the one who's got the higher skill
is going to win probably okay so let me
write that so I add a column called
skill I say it's real I say it's a
latent variable and I'm going to say it
a gaussian and it's going to have mean
25 and standard deviation 10 which and I
you write the square of that the
variance okay and then down here i'm
going to say i'm going to predict who's
going to win or not by saying player 1
skill if that's bigger then player 2 dot
skill cool so we just made it a bit
smaller so let's run inference so it
thinks a bit and during it as what it's
done is it's compiled down to the info
don't net system which is down to a sort
of probity programming c sharp run that
got the results and then put them back
in the spreadsheet and let's go look
more closely
is it going to a bit small let me first
of all look at the skills it's a wee bit
bigger so these thought I'd out as being
everyone was 25 but Alice lost a bob and
bob lost a Cynthia so what it's learnt
is that is that Bob is in the middle
whoops how did that happen interesting
who did it again what's that right okay
live demo let's never had before okay
let me just point so whoops so Bob is
stuck in the middle still at 25 but
notice that the variance has gone down
right so initially I was very uncertain
about Bob's or everybody's skill it was
like you know it was like spread out
between nothing and 50 but you see that
this is dropped down to 45 from from 100
so this is and where we will save Asian
or probabilistic inference what they
mean is that the answers you get at the
end or not yes nor it's not like a
specific number it's more like a
distribution right and in this case it's
a it's a Gaussian distribution a bell
curve and so your answer is it still
mean 25 but it's become more precise the
variance is dropped and you'll see that
a light alice is down at 16 and cynthia
is up at 33 now this is a very crude
model because and well hold on the force
ok and then the other thing is we've
made a prediction so we think there's
just a five percent chance that alice is
going to be Cynthia and that's because
Alice's skill is much lower than
Cynthia's skill and this is really
rather crude because I all I've said I
said that that there is an exact skill
everybody has and that that exactly
determined determined who's going to win
in the particular match but we all know
that almost every kind of game has got
some element of chance and we're over
people's skill it's a bit of a fiction
you know maybe you maybe you had to skip
lunch because things are running late
and so your your skill is not quite what
it was so we need to take that into
account so the way we can do that is by
putting a bit of noise into the model so
if I go in here what i can do is instead
of just using the exact skill what i can
do is
whoops yeah i can put gaussian so
basically add add a bit of noise so i'll
add the same amount of noise goes with
noise gaussian with mean it as a skill
and the standard deviation being a tan
again okay and so now I'll get a tech
checking error wow thank you I've lost
it now oh yeah thanks okay so now if I
rerun inference you see that then so now
we're a bit less certain about
observations because we've added a bit
of noise so what you see is that the the
skills haven't moved so much and in
particular that the variance hasn't gone
down so much because it added some noise
another other thing you see is that the
prediction is much less certain it's no
longer a five percent chance has gone up
to a thirty percent chance okay so that
was a quick demo let me see I'm not
going to I've got a pre-prepared
spreadsheet for the more complicated
thing to show you that we can't just we
don't just do very simple things like
that so this was one of our early users
did a model of american college football
so the same kind of idea that college
football teams were given skills and
this was used to predict how they did in
there in there in there in there leaves
and the model was i don't think this is
going to be very as usable visible and
like but the model is basically a little
more sophisticated but it has the same
kind of structure and is written in
twenty two lines of tabular so so one
that one of the differences take my
chance of getting with this well
whatever one of the differences and i've
got a table of leagues so we and every
team belongs to a a league and so we can
have a skill assorted associated with a
league
and then and every team skill is a sort
of noisy copy of the league's skill so
that means that and the reason you do
that is you expect that different
leagues are going to have reached league
is going to have teams that are sort of
comparable with one another so once you
start to learn about the behavior of
other teams in the same league you'd
like to learn something about you know I
another team in that league and the way
we do that is to have the shared league
level skill so that's one difference
with the simpler model and the other one
is we also take into account whether a
team is playing it's at home or not and
we have an additive home skill advantage
that we add to the skill of the other
player that was the team that was
playing at home um so this is actually
an external user who was an MLP HD but
he was able to use the system and he did
some nice visualizations and this is
really and this sheet kind of shows you
why I think it's so nice to work with in
Excel because you're going to the data
there you can have the model there and
then you can visualize the results and
it's all just something you can save an
email to someone so so these the results
he got heavens and I should stop doing
that the top you see that's the the
skills of the teams sorted by their min
but then more interestingly just to
compare to human-derived judgments
there's a I think an Associated Press
set of experts that ranked teams as well
and this plot here is plotting the skill
against the rank of the from the from
the experts and you can see that it's
broadly speaking a downward sloping line
which is what you'd like what you'd
expect you know correlation and between
the two so basically the model is
recovering largely what the human
experts considered was a was a
reasonable ranking okay so let me return
to the talk
ok so I'm self cheering it's half past
two and i started at twenty ten past so
got until about quarter to 22 quarter to
ten to are there any questions at this
point you've been remarkably silent
post-lunch I suppose okay ah right so
let me let me skip on so we've been
through that these are my backup slides
in case things crashed this is to give
credit that the underlying system we use
is called info don't net that's been
developed for about 10 years in MSR
Cambridge developed by john wayne and
tom minka and it's a language that lets
you see sharp to describe probably stick
programs and then they build various
infants algorithms and what we're doing
is building on top of that okay and i've
shown you this this sheet so generally
i've given you the demo and what I was
showing is this recipe for how we use
tabular you start with a schema you add
latent columns that describe qualities
of the data such as the skill of players
and you write models for those latent
columns and output columns such as the
thing and outputs the things you trying
to predict such as who is going to win
and then you then by running tabular we
learn these the columns and the
parameters and predict missing values
and we the tabular language itself
focused really just on this part of the
the cycle of developing a machine
learning model but I mean the whole
experience is is much more than that and
we decided to embed in Excel because we
figured Excel is a great place to do
visualization and data and data
wrangling and so forth are all sort of
very important to you know building
machine learning systems okay so let me
go more slowly through the different
components of the language so and the
this linear regression here is going to
illustrate the idea of parameters which
I haven't shown you so far so what's
linear regression well you you have a
bunch of points and you want to find the
best fitting straight line that goes
through them and if we phrase this as a
problem
the question your model of the data is
this that you every for every pair X Y
you expect that y equals x times a plus
B plus some noise so b is like a global
shared intercept and a is like the slope
of the line and e is some noise you know
there's always a you know the
discrepancy between the the line and the
actual point and you you want to you can
phrase this is trying to minimize the
the total square errors would be if you
were if you're not being probabilistic
that's how you would you'd say it's an
optimization problem you're trying to
find the a and the B that minimize the
square of all the errors and instead we
formulated probabilistically and i'll
show you that in in tabular so for this
data set the only actual observed data
are the the blue and the orange lines
that we have a we have a bunch of x's
that our wheels and we've wise they're
also reals and these gray columns are
all the rest of the the model that you'd
have to add and the crux of it is that
them here we say that in every row that
there's a Z which is that your point on
the line which is ax plus B and then y
itself the observed item is is a sort of
noisy copy of Z so just as I need noisy
copies of skills in the previous example
here i'm making a noisy copy of the
point on the line and how do you find a
and B well they're declared as
parameters and the difference between a
parameter and the latent variable is
that latent variable is added in every
row of the table whereas the parameter
is like a global it's like a static it's
sort of the difference between static
and instance variables so to speak and
here we declare a and B we don't know
what they are so we say there are noisy
copies of some what are called hyper
parameters which are both in this case
just going to be 0 so if it's some sort
of model about run certainly about a and
B are that we stipulate here and then we
use them to define the rest of the model
so that's linear regression and you can
visualize this as something called a
factor graph and this is essentially
what we send to infer net and in this so
this is like very much a probabilistic
model
so the circles are random variables and
the square boxes are what are called
factors and they're basically and
computations that will compute one of
the random variables from one of the
others so you can see at the top where
we're computing B and a by doing
Gaussian draws and then down here inside
this thing called a plate we take an X
and then as an input and then do the
computation to produce a Zed add some
noise and get the output and this this
this box here is known as a plate and
basically it's a for loop or it's a map
over all the items in the in the table
so this is like an equivalent well a way
of rendering what the the tabular model
is okay so how might you use this well
here's an example of using it to do
price determine price elasticity which
is the which is the property as or it's
the property of whether or not when you
put the price up the of some item that
you're trying to sell the sales volume
goes down so here I've got a bunch of
data and this is from some classic sort
of management science study or marketing
study where they had data on sales of
cheeses in various American cities and
in different retailers and they've got a
whole lot of observations of when there
was a particular price like 2.5 seven
dollars the volume is 21,000 at a
particular retailer so you've got a
you've got a table of price volume and
retailers and then you've got a table of
retailers and if you visualize the data
inside Excel you get this sort of
picture where those are gently is a
gentle slope going down but you haven't
learned anything about what happens at
different retailers and what the
relationship is and then the point of a
sort of probability model of this and in
particular one that is hierarchical in
the sense that we group the observations
by the retailer is you can see the
differences between retailers so we can
write this in tabular as follows where
this is the tabular program for the
retailer's table this is the tabular
program for the sales table
and the key thing is that I I've got a
slope associated with every retailer and
that the slopes the mean of the slope
itself is a shared parameter okay so so
we're sharing the the means between the
it's a sort of baseline slope and then
we add a bit for every different
retailer and and because of that we can
see the differences in the slopes and
then down here this is very similar to
the model I had before for a linear
regression with a couple of tweaks one
is that we're taking logs because we're
interested in the sort of difference
rather than the absolute value of the of
the sales as you put the price up and
the other difference is that down here
so this is the basic formula for doing
regression that we're saying it's the
price times the slope plus the baseline
B plus some noise but the difference is
that we've got retailer dot slope so
we've got a different slope for every
retailer that we're going to learn so if
you go ahead and run inference we get a
latent variable added to every store
which indicates the sort of the
difference from the sort of the the mean
the mean slope and you see most of these
are negative but some of them are
actually positive and then you can go
ahead and Excel and plot it go ahead and
accept and plot the result so this is
showing how the differences in in
different cities so I think up in Albany
the I think this is all bini here and
the there was right there's only they're
rather insensitive that there's not much
of a decrease as you put the price up
but in some other cities this orange
band like down in New York it's actually
higher than the national average so you
see you can within Excel you can easily
sort of visualize these kinds of
differences yeah
we've been living doing a great many so
so we've been looking at stealing ideas
from our so our has this these packages
for a linear regression that can mean
they can do sort of hierarchical linear
models and generalized hierarchical
linear models and we were looking at
stealing the rotation to put them into
two tabular I don't have any slides
about that at the moment but that's what
i mean so when i said earlier that we
were conscious that there was a bit of a
gap here between you know what a machine
learning PhD could do and what the data
enthusiasts can do I'm very conscious of
that and it seems ours done a nice job
of nice notations for quite
sophisticated regressions and so we've
been stealing those to put them in but I
we don't have that ready for a talk yeah
I mean in general there's a I mean here
I'm talking about fail a limited set of
distributions but we have a wide range
of things like my soul and distributions
and and you know whole range for doing
publicity modeling okay so the the next
example that I'd like to do is another
sort of this is a variation on the true
scale model but looking at a kind of a
very interesting model that looks that
sort of is a kind of intelligence test
where we've got a population of users in
this case it was 120 participants and
there were 60 questions posed to them
and multiple choice questions and the we
want to sort of learn the sort of
ability of the the participants with
respect to this these sets a question so
it's a test but we also want to learn
what the hard questions about the easy
questions and we're over we even want to
learn what the correct answers are for
you know perhaps not all the answers
have been have been labeled and so this
model is rather interesting in that it
can do all that at once and you can
think of it it's a bit like it's a model
it's a bit like the way
some professors might operate that you
said a test for your your class and you
may be a little bit tired and don't want
we're going to work out all the answers
so what you do is you know you know who
you're smart students are so you go and
look at their their answers then use
them check them and then use that for
the rest I guess we're in a mixed
company here between you know a comedic
echo academics and an industrialist so
I'm afraid I'm a part-time academic so I
sort of born the professor's cover a
little bit there it please forgive me so
anyway in this database we got we got a
table of responses and for each response
it's by a particular participant and to
a particular question okay so here is
the difficulty ability model where we
every participant has got an ability
which is done as a Gaussian as before
every question has got a has got a
correct answer and and which we which we
can print which we can predict and it
also has a difficulty which is another
number and then the model I may be
getting the idea by now that we've got a
model of responses so we want to predict
what the answer is and we we come in and
we calculate the advantage so this is
the advantage of the participant over
the question and it's it's whether the
ability is bigger than the difficulty of
the the question and then we and if
that's greater than zero we're going to
assume that the responder actually knows
the answer otherwise they don't so we we
have something called a probit function
which is a sort of noisy way of passing
from a number 2 to 12 it's sort of like
a sinusoid you know as the as so this is
0 the chances of the the output being
true is sort of zero and then it kind of
bumps up to being you know and it at
zero it's sort of 5050 and then once you
get a bit above one it becomes one okay
so this is of noisy way of turning a
number into into a boolean and then we
and then we have a guest and then the
basic model is if the respondent knows
the answer then we give the correct
answer otherwise
we just guess the answer um so so this
is a nice model and you know it performs
well this is it represented in plate
rotation and one reason I'm putting this
up is to give it a comparison so I want
to evaluate this language versus writing
it directly conventional probabilistic
programming languages so this cord out
here is what you would what you would
write if you directly used the
underlying system info net so it's about
hundred lines you know written in in
tabular it's much shorter and I'm going
to put some numbs up in a moment well I
put them up now but we don't need to
parse them in detail the main point is
that we get the same statistical answers
with the same performance but in much
less code so this is basically evidence
that you know you might as well use
tabular rather directly using C sharp
okay so I start by saying on the
language enthusiast's actually put up a
grammar this is the grammar of tabular
and like I said okay like I said it's
we're basically annotating the schema of
the data you can't do machine learning
without data you have a scheme of the
data we're going to mark it up with the
probabilistic model so the schema is
just a collection of table names with
with table descriptors big t is the
table descriptor table descriptor is a
list of columns they've got types and
then and that will be it if it was an
ordinary database of course but now we
also have annotations and I've gone
through these different annotations in
the talk that you can have inputs and
outputs which are the observed data and
you can't you need the inputs to be
present and we're going to predict the
outputs from the problem suit model n we
can have latent variables like skills
weaving the parameters like the slope on
the intercept of a linear model and we
could hyper parameters which are just
constants that are used to configure the
the parameters or other things I if if
this is what where did your appetite for
tabular please download it and play with
it and read our papers and there you see
how how we have well we explain the sort
of the full language which has got
function definitions and various kinds
of conditional models but I won't try
and go into detail about them and I
think come
see I skipped a slide I wanted to just
show you just it to give you the feel no
where is it
oh I know what happened
yeah this is if you download it it's a
zip file so if you have Windows and
Excel you can go ahead and run it if you
don't have windows download any way you
can open up going to open up the files
and there's a whole range that we've got
a bunch of different classification
problems we can do this is Beijing
linear regression we've got logistic
regression if you know about these
things we've got naive Bayes there's a
whole bunch of different classifiers
have been written in this language we
got various clustering algorithms we've
got principal component analysis which
is a way of taking a database with you
know a table with a great many columns
and scrunching it down to a more
significant representative sort of sort
of narrower table I'm got various
rankling models like true skill and find
lots of regression models as well so
it's at that URL please download and
enjoy let me finish with this slide
which really just summarizes where we're
going that were we want to empower users
to learn from the data we are models are
just annotations on the relational
schema data and finally we think that
Excel or spaces in general are a good
place for manipulating data anyway and
so surely that should be the place where
we're doing machine learning and I think
I've shown you how tabular as a syntax
designed to fit into the shape of
spreadsheets and let you you saw what I
was doing if you just simply use the you
know the editing facilities to mark up
your model and to you know run inference
in place without having to go out of
that environment to you know programming
c sharp and whatever so that's tabular
do consider downloading it enjoy it and
I'm very happy to take questions thank
so for your hypothetical data
enthusiasts sure do you think that they
have the statistical sophistication to
understand I mean assuming that these
sort of user experience is improved and
everything like that like what is the
sort of baseline knowledge they would
need to be able to use this and not
mislead themselves sure that's a good
question I I think to use tabular
seriously you would need to have some
understanding of probability
distributions so you know you look out
there I mean there's a tool called at
risk for Excel which is a sort of Monte
Carlo inference system that assumes the
user knows about normal distribution
well Gabbar Singh dissolutions normal
distributions and so forth so I guess
we're going at people you know of that
sort of sophistication who you know no
bit about statistics yeah
Oh so let's go ahead thanks it seems to
me that tabular isn't quite as
integrated in the model of excel as one
might want so so typically in a
spreadsheet it has this sort of
recomputation model where you change a
value or you change the formula one
place and then those changes sort of
percolate out through the rest of the
spreadsheet sort of until it's fixed
point whereas here you've really got
sort of a batch processing paradigm
where you highlight a region of cells
and then say tabular do your thing and
then the output appears over here and
then if I go change it I have to say
tabular do your thing and then I get my
help again yeah that's true um is that
something that you'd like to change in
the long run I mean is the idea to sort
of build Excel out to have recomputation
for these models as well and I would
love to do that we haven't attempted
that in this version we've had some
earlier versions where we attempted to
do that and we use user-defined
functions to build models within the
cells so yeah I mean that would be a
terrific way to go so yes
so it's a tabular program a
representation for a factor factor graph
yes and I understand that markrob logic
networks are shorthand for representing
factor graphs or something like that
that's true yes what's the difference
what's the difference um that's a good
question so Markov logic network is like
a is a particular kind of logic program
they I'm not sure right now if I can
give you a night i'll try and consider
that and tell your flame yeah okay if
there's no more questions thanks for
attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>