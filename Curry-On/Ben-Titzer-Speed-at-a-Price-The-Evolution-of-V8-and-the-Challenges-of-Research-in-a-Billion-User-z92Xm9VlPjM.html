<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ben Titzer - Speed at a Price: The Evolution of V8 and the Challenges of Research in a Billion User | Coder Coacher - Coaching Coders</title><meta content="Ben Titzer - Speed at a Price: The Evolution of V8 and the Challenges of Research in a Billion User - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ben Titzer - Speed at a Price: The Evolution of V8 and the Challenges of Research in a Billion User</b></h2><h5 class="post__date">2015-07-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/z92Xm9VlPjM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Ben I work in in Munich on
V age I've been working about two years
on this VM and I lead the compiler team
and I'm going to talk a little bit about
v8 and I'm going to talk mostly about
how v8 and production systems relate to
doing research and kind of give you at
least one perspective on that I've been
in and out of research I went to grad
school I I published papers and I
actually enjoy doing research I'm going
to give you a somewhat sarcastic picture
of v8 it is actually a really cool
system there's a lot of really cool
technology going on inside a lot of cool
optimizations there's not been a lot of
attention from academia and part of that
has been a lack of publication on our
side
and also just a general lack of
information about how JavaScript themes
work in the literature this is kind of
an example of the divide between between
academia and industry and I'm gonna give
you a bit of a perspective on value
systems and why that might be and also
this wouldn't be a worthwhile talk if
there was not some you know deep
technology and cool optimizations
presented and sort of I'll try to end
with some rhetorical questions about
where do we go from here a little bit of
background about v8 it was the really
really the first fast JavaScript VM one
of the key ideas was introducing this
idea of hidden classes which are
essentially turning JavaScript objects
which are basically hash tables and to
things that are have a sort of constant
time field all set is a compiled only VM
there's not very very many production
VMs that are compiled only and that
means that it basically to generate code
you have to generate code fast so the
sort of baseline compiler just walks the
est in it's machine code it launched in
chrome in 2008 I was a major
differentiator between chrome and other
browsers and it was kind of from like 20
to 100x faster than other JavaScript
games at that time in 2010 through the
first optimizing JIT in v8 was designed
using type feedback and that launched
that launched in quite a short time that
was a very very quick effort and since
that time we've actually built another
JIT compiler which we launched at the
begin
this year that gave rise to a horse race
so to speak pretty much today you can
say that all the four major browser
vendors are really fast on benchmarks
but there is still quite a lot of
performance cliffs and the performance
landscape for javascript is still quite
uneven
that means that there's lots of work
still to be done so all of that
production work in making really running
towards getting a fast VM right away has
a cost it has a price and what part of
that trade-off is a research potential
and I use the term shipping this and
kind of a it's not that's not a real
English word as far as I know but that's
kind of this idea that favoring things
that are products over ideas and doing
things that worked in the past as
opposed to taking risk on new approaches
that also favors incremental
improvements and all also means that we
build fewer platforms and frameworks
which are sort of internal things that
make our lives better or as we pursue
optimizations it also means that we
don't have much time to write papers and
part of shipping these things also means
that you can't really go backwards on
benchmarks so once you get to a certain
level of performance you can't go back
that builds up to some architectural
debts or a technical debt and that means
sometimes the VM is complex and also
fragile and we tend to favor things
which fit in with the existing approach
fit in with the existing code is easier
to either a hack on the side or to sort
of work around how it is now the really
innovative ideas take a long time to
stabilize that are risky and they mean
that you know you get bugs coming in
from from production and in chrome that
means coming in from new people
unfortunately you get crash reports it
also means that new language features we
have to be in conservative in the face
of so we don't typically do long we
don't do things that are not necessarily
standardized despite the per special
perception in academia resources are
still limited even at Google engineers
cost money
interns cost money
and insurance cost more than students we
can't really afford to waste
quote-unquote waste resources on
constant exploration of ideas that means
that we time box experiments that means
that we cut things off when they seem
like they're not going to be fruitful
means we have to commit to a direction
you have to pick a direction and commit
to it a bad direction means some cost
later but at least you've committed to
something also that means that past
solutions even if they're suboptimal are
less risky and this the other thing
about this that I'd like to communicate
is that maintenance has a continual cost
that you don't see in academic projects
in particular fixing issues branching
releases and dealing with perpetual
churn from other software systems this
actually costs engineer time and gives
people dark thoughts like please kill
our project kill me something like that
and I think I cover this a bit research
is sort of an afterthought at least in
v8 we've had the approach of only
publishing after we've shipped ideas so
we've sort of proved that they work and
that can take more than a year and this
is only sort of if we hat still have the
interest and still have the drive to
publish a paper about that and this is
partly because of this sort of drip of
stabilization engineers are not rewarded
the same way in in industry as an
academia
it depends exactly where that tuning is
on which maybe if you're an industrial
research lab it's one way where as a
product group it's a little bit more the
other way and writing a paper takes time
you have to justify that to the
management somehow and of course there's
a lot of new things coming in you're you
want to write up an idea that you've
finished but there's bugs
there's users asking questions usually
they're wrong but they're still asking
questions and there's features coming in
and there's having a life so I want to
share some observations from doing work
in production systems and also just
doing work in general I think that at
least in a production system one really
good idea is not enough anymore
especially when performance start to
plot starts to plateau which I think it
is start to starting to plateau for
JavaScript an example of that is we have
this optimization in VA
whether we assume an object gets to a
certain representation and after that
it's not going to change anymore so
using this we can actually
elide some type checks once you do one
type check you can assume that it's not
going to migrate away so this only gives
you a small bit of performance it's
probably not an idea that you could
write up and put into a research paper
and get published maybe it'd be a
paragraph of page two pages in a tech
report but that's not a research paper
another example is how we have
techniques for lighting prototype change
checks which are important in JavaScript
you know there's a tangle of pointers in
the runtime representation of JavaScript
classes of JavaScript types in the VM
and again that's kind of not quite
getting to the paper threshold so that
doesn't really come out as an idea that
penetrates in the Accademia simile for
assuming that certain constants Global's
or constants all those are little ideas
but you start to put them together and
you start to get the percent after
percent after percent improvement and
that's how the engine gets faster and
complex systems need to sort of progress
using these small steps and these these
don't even a collection of these little
ideas don't really make a good paper
another one another observation I have
from my experience is that when we do
build patterns and frameworks inside a
VM the emerg bottom up so we get many
examples of a particular optimization or
a particular pattern on how we structure
the code and then once you see the
patterns then you can actually develop
something that actually covers the
patterns I'm gonna give some examples
and do a deep dive into some technology
to show you why I think that this sort
of bottom-up actually works better but
this is actually unfortunate because
those patterns are the most
transferrable parts what you learn in 1
p.m. you see many instances and you sort
of collect that into a reusable piece
and that's the piece you want to
communicate and it's also it's hard to
get to that level and it's hard to write
papers about those so these kinds of
ideas they're tough to pitch in a
research paper actually I think it's
really important to publish research
papers of that kind because they are
peer reviewed and there's sort of this
threshold there is a tech report
mechanism but honestly Tegra
courts don't get this sort of fanfare
that research papers or conference
papers do and they're often ignored and
also they don't get rewarded in the same
way so I'll talk about a little bit
about the flow engine so this is a
essentially a framework for doing
compiler optimizations in crankshaft
which is the sort of middle compile that
we launched in 2010 we noticed that
you're writing compiler optimizations
sort of one instruction at a time and if
you get new people come in and have an
idea and they sort of think of it in
terms of what's right in front of them
which is the IR and they will do some
very simple instruction at a time thing
examples are like load/store elimination
type check elimination we can see the
flow from one instruction to the next we
can do very local things it's really
easy to do those and again it's
incremental because you can add them you
can see a small increase but then after
awhile you want to extend them to work
on the whole control flow graph to do
dataflow analysis and after a while you
notice that basically they start to
actually look like the textbook
algorithms so you read the Muchnick book
and it shows you solving all these data
flow equations it's very abstract it
doesn't really nice necessarily show you
how it could be applied to a compiler
that you have you only see that after
you see many examples and so that's one
of the things that I didn't crankshaft
was developed slow engine to solve the
dataflow equations for for you that
wouldn't have made sense to do unless I
had at least seen three or four
optimizations that could fit into that
so I'll give you another example of this
these frameworks appearing bottom up and
only after many examples sort of come
into play this is actually a paper that
we published it is mem this year this
idea which we call allocation momentos
is a way to dynamically instrument
objects and learn about them so this
arises because javascript has arrays
which you can essentially put anything
into you can delete items out of the
middle of an array you can grow arrays
and so the way those are represented in
the VM is different depending on the
contents and depending on which
representation you pick it changes the
cost of all those operations so you sort
of want to pick an optimal rate
representation that's very related to
also picking a good initial size
in a growing strategy and we also
noticed after a while that we want to
predict object lifetimes so there's sort
of a there's some benchmarks for example
and in some programs in the wild where
some objects a little really long time
and other objects die very quickly and
if you assume that everything dies
quickly but you're wrong
then the garbage collector actually has
to do more more work because typically
you would copy once maybe twice to
promote those objects to the old
generation so I'm going to give you a
deep dive into sort of how allocation
mementos work and give us the ability as
a platform to do all three of those
optimizations it's actually very simple
so instead of putting some information
directly in the object we put the
information directly after the object in
memory so and this is optional too so we
can actually have an object that
essentially does not know if it's been
instrumented and the the instrumentation
is added to the end so we can optionally
check for it and you can store you could
think of storing anything that you want
into this instrumentation but for now
we'll just look at allocation site
that's basically the place in the code
where the object was allocated and this
is assumption that objects allocate from
that site have some of the same
properties over and over so arrays are
represented with an indirection in in
the VM and they have different
representations so in particular this
example we have the elements arrays all
storing 64 bit double numbers the
allocation site actually records the
fact that all that all the objects from
the site so far have only needed to have
doubles the one thing that can happen is
that you can actually delete an element
of array and of an array in JavaScript
which is horrible it actually means that
you see through that hole to whatever is
on the prototype chain we actually need
to sort of set a bit in the map to
remember this array actually has holes
so you have to do it a second check when
you read so you can lie that check do
you know it doesn't have holes so we
actually will use the memento to update
the allocation site to be able to show
to be able to remember that objects from
this site eventually had holes put in
them
same thing on even more expensive cases
if you had doubles and then you added a
string or over wrote one of the elements
with the string so this is actually
represented differently in memory we
actually will make a copy of the
elements so on a 32-bit platform those
will be 32-bit pointers versus 64-bit
doubles and then we actually have to box
the doubles that are there this is
actually very expensive operation that
you want to avoid in the future so
recording a little bit of information in
the allocation site helps you sort of
skip over that transition step so you
sort of start out with a more general
representation so you can also use this
this platform for doing pretending this
is basically getting a signal about how
long an object is going to live so we
instrument objects with mementos
depending on whether we want to learn
about them and then when the garbage
collector runs it basically collects
mementos for surviving objects and
knowing the number of allocated objects
from that site you can compute a rate
the survival rate and with that you can
sort of trigger at some threshold you
can trigger pretending and that makes a
big difference for the objects future
objects that are allocated from that
site will go directly in the old
generation and never go through to
copying to copies it also means that we
have a chance to pretend yearning
basically means allocating objects
directly in the old generation so that
you don't copy them twice if they're if
they're living through young generation
collections we can actually learn
something and then discard the mementos
that means that we don't pay the
overhead for them and they will gin so
we only actually have to use them for a
short amount of time so I want to show
you a graph from the paper not to say
how awesome this optimization is but to
say how weird this graph looks and
actually how awesome this graph is so
you you see that there nothing gets
slower that's the most important thing
of this optimization is that we tuned
all this sort of learning phase so it is
really low overhead and only when you're
really sure that you learn something
that you turn it on and you get this big
speed-up so this is actually very very
weird from an academic perspective or
you often see charts where some things
get slower some things get faster and
some two or three sentences where they
wave their hands and say it's fine
that's not fine for our situation we
rather see grass like this where nothing
gets slower also one thing that doesn't
show up that we did present in this
paper is that pre tenuring can make
wrong decisions and you don't want to be
stuck with them so we implemented a
backup makers mechanism that sort of
recognizes when something is horribly
horribly wrong and then backs everything
out so that we can relearn so this does
not happen on benchmarks but this
happens in real world role applications
in particular Gmail for example it's
important is long running you don't want
to get stuck with the wrong optimization
decisions also this is I don't know if
you've spotted the bug bug form that
that's created but you're actually
looking at memory after an object in
memory so that if that's trash then
maybe it was some low probability it's
trash that looks like a memento and then
you start to get confused
it's okay maybe if you're just using it
for a heuristic it's probably okay but
if you're actually hunting around in
that trash memory it's something's gonna
crash so this is a new system-wide
invariant of it took quite a long time
to find all the places nobody did ever
thought we would do an optimization like
this so we sort of had to find those and
stabilize those and that's kind of like
a more than one year stabilization
effort like actually getting crashes
from the wild I'm tracking down the
place that's missing something that
needs to overwrite the trash so we don't
get confused and this is kind of a
crucible that every v8 engineer kind of
goes through you ship an optimization
everything's great it took you a week to
write the code it took you a week to do
the code review and it took you three
months to of turning on turning it off
after you get some crashes from
production obviously you can improve
that with some testing techniques but
this is always there I want to give you
one more third example considering our
work mostly on compilers I thought this
is it might be interesting to people who
work on compilers so again this idea of
frameworks and patterns emerging from
many instances and this is how we do
optimizations in turbofan most of the
optimizations work by doing a kind of
local graph reduction
so we use an intermediate representation
which is we call the soup of nodes which
is kind of a play on words from see of
notes it's a slightly more relaxed
version of this he had notes and mostly
optimizations work on this um and
actually we have ship two turbofan like
more than six months ago so this is
again stuff that's been already in
production so just now all the compiler
IR is coming right at you so I hope
you're prepared it's basically a graph
all the computations are just nodes in
the graph that means constants
parameters arithmetic and all and the
edges basically represent dependencies
between computation so they can
represent value dependencies they can
represent as you'll see affecting
control dependencies and we do ssa
renaming so there's no such thing as a
local variable in your code in the graph
basically we rewrite the references to
local variables with whatever value you
actually used assigned to it last
that means the graphs actually are not
trees they're not expression trees like
you would see in a textbook they're
actually DAGs and actually they will
have cycles to once you get into having
loops the key idea is that this kind of
a representation it does not dictate an
ordering like a control flow graph which
has instructions one by one in order it
constrains an ordering it basically
represents all legal orderings so
basically illegal orderings are ones
where the edges go the wrong way
so we use effect edges to order
operations that deal with mutable state
so anything that loads from immutable
memory location or anything that stores
and those effect edges basically make
sure that those things stay in the right
order that they were specified in the
program there's only it's and rights
have to be ordered with respect to each
other we use control edges to represent
control and these are also dependences
they all go backwards in time so you can
kind of think of this as end is the most
important thing in a function that's
kind of the goal of calling a function
is to get something out so end
represents where you want to go and then
it just depends on everything you sort
of need to compute to get to end so that
can be control that can be effect
and that can be an actual value that the
return value of the function okay so we
started doing optimizations on on this
representation and it's actually really
natural to see these as local kind of
pattern matching you see something which
is obviously suboptimal and you can
change it to be something cheaper so the
constant folding is the obvious example
strength reduction is if you're adding
zero to something that's a no op or
replacing something with the cheaper
version of that and similarly for SSA
renaming by simplification is basically
reducing the complexity of the data flow
relation you do reassociation on this
form to so it doesn't necessarily have
to be one note at a time the other thing
is the turbofan is designed to compile
JavaScript code to machine code and
there's a big gulf in between and so we
use type information as part of that
lowering process and this is actually
another instance of how reduction works
on the graph we basically have these
static types associated with nodes since
we know so we have a node that
represents a JavaScript ad which
includes all kinds of overloading rules
but knowing the types can actually
statically determine what overloads
those are and we can boil the code down
to a lower level lower form this is
actually good for code that has latent
static types in particular there's this
as MJS subset of JavaScript which has a
lot of type annotations which basically
give you this for free you can extend
the reducer idea to doing common
sub-expression elimination that's
redundant computation in the graph so
you have two computations of sign they
will eventually look something like this
where you actually have a node that
represents the input and then two notes
that represent two independent signs you
can just collapse them together and this
works even for effect falah per a
shion's like loads so if all the
dependencies are the same the operation
is the same it will compute the same
value okay so we went even further step
along this this road in all control
reduction all sort of removing dead
branches actually can be expressed in
the same way so if we have a branch that
has a condition which is a constant then
we
actually replace that the one-armed of
the that's not going to be taken with
dead and the other one with the actual
sort of assuming taken and you make more
rules and more rules and all of them are
local which is great and it basically
means that you only have to reason about
the sort of notes that you're modifying
and then we put this into an engine and
I'll show you that in a bit it basically
gives us the sort of dead code
elimination as a reduction and the great
thing is is that you can mix all the
reductions together and sort of one sort
of pass over the graph so this is the
kind of framework that emerges from from
this sort of seeing all the local
transformations we do this reduction of
the inputs and then we're at a node and
it's got some edges coming in and then
we're going to apply some rules and
that's going to either produce new nodes
or it's going to rewire the edges to be
some of its inputs or maybe even
constants so there's an ordering problem
and actually you want to do this in a
way that's the most efficient because of
visiting the graph many times it's
expensive so it's compilation time
so we basically realized that this that
you want to do the rules and postorder
and the way we implement that is with an
iterative reduction so you can think of
starting at end and you only want to
essentially excuse me look at the code
which and really depends on you don't
look at code that it doesn't depend on
because obviously that would be dead
code so we use an explicit stack for
this where every every time we want to
go one deeper we push something on to
the stack and now it's actually really
easy to see what the local rules are you
have the note on the top of the stack if
it has any inputs which have not been
reduced then you then you recurse on
them and finally you're either gonna get
to leaf or you're gonna get to a cycle
when that happens you apply reduction
rules and you can apply all the
reduction rules and hopefully if they
don't fight with the fight with each
other then you actually will get the
right result so this gonna happen you
can have a cycle and that's if you would
have a loop in the graph and then there
what we do is basically you have to
apply some of the rules you do a partial
reduction and then you sort of allow
that node to be popped eventually when
you get back to the place where that
cycle is completed then you can
reduce it so this competes a fixpoint
over there over the reduction rules I
think when we started so I didn't think
that this would be so so abstract when I
started doing turbofan but actually
seeing all these instances we saw the
commonality between them I just want to
give one more anecdote about how
shipping this is hard
so we implement its escape analysis in
v8 in late 2013 and shipped it or so we
thought
right there was the main the main
blocker here was bugs and D optimizer
bugs so that's basically when the sort
of optimized code has made an assumption
that doesn't hold do you have to go back
to unoptimized code and if you have
escaped analyzed objects those have to
be essentially materialized they have to
be turned into real heaps on real
objects on the heap and their values put
in well there's this language feature in
JavaScript where if you have a
JavaScript function you can get the
documents property and what that
actually does is it walks the stack it
finds the last function activation
reifies the arguments passed to that
function at that call site and gives you
an array back and you can modify that
array if you like that's actually not a
language feature that's not spec by ACMA
but that's part of what v8 gives to
chrome and the web uses that so there
are websites that depend on this of
course we knew about this feature it
causes it the optimization it's a slow
path is fine we're fine punishing those
people but those websites so it worked
fine but we did we would actually
accidentally materialize two copies of
the object sometime sometimes and that's
fine if you don't modify them or don't
compare them but actually they were went
even a step further where they were
modifying the contents of that arguments
array and expecting this to work so we
had to do you know engineering solution
you have a hash map on the side which is
like you make sure that you make sure
that you only materialize one copy of
those and sort of that hash map is you
know has to be known by the GCE and has
to be popped off the stack and some
you know memory leak and stuff like that
so this is kind of like an ID you're
getting idea what the sort of round trip
to stable is for a future you find
something like this and it's a complex
system and yeah we could we could blame
this on some horrible language feature
but this is always going to come up when
you have a complex system it could be
your own creation and actually I would I
would say this is V its own creation by
offering this language feature to to
Chrome so I want to kind of bring the
the technical discussion to an end and
talk a little bit about what research in
v8 looks like today so there's a lot of
there's things going on turbofan is a
you know it's kind of my my baby we've
got a nice team working on it a lot of
bright people working on how to rephrase
optimizations to get the the best out of
the this representation it turns out to
be way more graph theory than I was
prepared for it's somewhat frightening
at times to think about the complexity
there but turbofan also gives us a
chance to rethink some architectural
ideas that that the v8 had from the
beginning about how it uses type
feedback and how aggressively it uses
that type feedback so there's some rearm
we architected there there's always a
tough task to find an incremental way to
do that and of course there's compiler
optimizations they were always pursuing
it's hard to say whether or not they are
researchy until you see what the actual
solution is so our very own andreas
Rosberg who is here is working on strong
load which is a subset of JavaScript
which is more controlled and hopefully
we'll we'll get the ability to optimize
code better with strong strong load and
also you can have programs that have a
little bit better astaire static
guarantee v8 is a production system it's
hard to build these these new features
in so that maybe is harder in B and then
it should be for that's also part of the
challenge and then along those lines
there's an even further proposal to add
static types to the language which is
called sound script which is also Andres
I'm also involved in another project
which is a collaboration with Mozilla
and Microsoft and Apple is called
webassembly and made the news a couple
weeks ago maybe you heard about
but it's essentially a native code
format for the web and more and we hope
that it can be much much more but
there's actually quite a quite a big
effort behind this and I think this will
sort of solve some of the problems that
that web browsers don't really support
use cases like compiling C++ games to
the web very well there's actually a lot
of ideas that were exploring in GC in v8
one thing that we do now is that we try
to hide GC pauses in between frames so
when you're running some graphics thing
we actually there's some slack time
between when you compute what a frame
looks like and we actually show it we
try to find a way to actually squeeze a
small GC amount of GC work in there so
we hide it from the user so you don't
miss a frame this actually works really
well you don't change the amount of GC
work that you do it doesn't actually
make it more efficient but it makes it a
better experience for people that's it
actually really important we're also
working on a fully concurrent GC and
that's also that's really tough to do in
a production system I think Tony could
probably vouch for that
and there's a lot of interesting things
going on there and also we are working
on studying object locality and things
like that
I think this fell off the end there this
doesn't really fit so Virgil is my baby
language project which I wanted to plug
that's not really in v8
maybe Virgil to webassembly and I could
claim that it's in v8 but there it is on
the slide so now it's kind of a
rhetorical question time where do we go
from here I don't necessarily expect you
guys to answer that I don't have any
particular expectation about what that
means for v8 but the question is kind of
like what is the right forum for
discussing these secret sauces and these
all these tricks that are in in VMs I
think it's important to share those
things I think it's important that
language that that is part of the
cultural understanding of how to build
VMs and also good ideas can be
reinforced and bad ideas can be maybe
also shown to the light VM implementers
tend to talk shop when we run into each
other at conferences like this I think
that's good
but I think there I think there needs to
be sort of a literature there's needs to
be a paper record about about these
things there needs to be places you can
go there you can read about how the
engine works that's important for
application developers to to know about
how the VM is going to deal with their
code JavaScript VMs are not documented
well maybe research papers of the right
vehicle I don't know I like writing
research papers personally I think
that's valuable to show something that's
on the cutting edge but also tech
reports are probably valuable maybe your
experience is different I don't know how
many tech reports see people typically
read I think maybe blogging also could
be important too
so we will launch a v8 blog here in a
week or two and we'll talk about some
more details there also word-of-mouth
probably also works interns really work
so we've had a couple of very successful
internships and we're happy to contain
that those programs so that's also a way
that we can sort of share knowledge by
sort of bringing someone into the fold
putting them through the pain of the
eight and then sending them back in sort
of getting them in the back door to
academia alright so thanks for your time
sorry for the whole hiccup situation but
yeah I think so in addition to tech
reports and papers I think design Docs
which maybe could be more or less formal
could also help and maybe implementation
guide maybe that's what you're looking
for which is like an annotated sort of
here's what you'll find in this file
here's why this file is separated from
this other file that kind of thing those
are those are really important it's it's
hard for us to find time to do that and
I actually wish that you know I had more
time to do that but I think that maybe
that would be a way it forward maybe we
should hire some people to do technical
documentation I mean that would that
works at other companies in other
projects that's not a joke
I have a technical question regarding
this location and mentor yeah so how do
you identify the allocation sites let's
have a one helper function map program
let us all allocations will this helper
function like whether you take the
context the calling context into account
so currently we don't take the calling
context into account so I I didn't
explain a bit about the policy but our
policy is to admit allocation mementos
and unoptimized code and unoptimized
code doesn't have any inlining it
basically has an allocation site boiled
into the unoptimized code that says this
is where it came from but you could for
example have a different policy where
you take one stack frame into account
and then you find you know you have that
means you have more allocation sites
because you have more context for
research sheamus sorry research so cliff
asked why did we do read after write and
write after read dependencies well so
technically we they're all treated the
same and actually our scheduling
algorithm treats them all the same but
it doesn't have to I think we can treat
effect edges slightly differently in the
scheduler to get better schedules we can
go into an offline but we basically
treat all edges as requiring dominance
relationship other than splitting the
splitting out also happens in the in the
scheduler so we will replicate some
computations in the schedule and the
schedule or algorithm probably we should
probably talk so yeah so we have a
little bit of aliasing out so the
question is of what level of alias Al
and information do we have like to tell
different memory locations apart so in
crankshaft we have something that's
relatively coarse grained it's like the
offset and the object and those don't
interfere it there's no aliasing
relationship where those could be and
then it's for the actual objects we can
tell to allocations apart an allocation
apart from something that comes in from
the outside so it's it's not super
detail we don't do really heavy weight
pointer analysis for that you showed
some note reductions and you quickly
said that sometimes these note
reductions could collapse could both
apply on the same situation how do you
how do you resolve that very carefully
by hand so it be so what can happen so
in particular one thing that can happen
is that one of these reductions that is
lowering so we'll take JavaScript and
reduce it to something else whereas
another reduction will reduce it to
something that's cheaper so if you are
producing a lot of new nodes there's new
opportunities to find
common sub-expressions and so you can
sort of get confused you could
potentially if you had if you had
reductions that could cause a cycle one
reduces at one way one puts it back we
have to be very careful we don't have
any safeguards to well there is one
small safeguards we don't apply a
reduction to the same node twice the
same reduction to the same node twice we
will sort of at least let one other
reduction run on it first so there's
some safety fences maybe it'd be nice to
have a theory about that there's this
this is idea of confluence in
programming language reduction which is
like you will no matter which order you
go and you eventually get to the same
result it'd be cool to have theoretical
guarantees about reductions like that
but that's something that
that's that's a research that's more
question as as I tell you have
requirement that all organizations that
you introduce none actually give a
penalty for existing code
you shouldn't have regressions on your
benchmarks though you'll raise say that
you have optimizations like global value
numbering that is known to in some cases
introduce computations that weren't
required yes so let's say you have a
match example and you have several
branches two branches you say Express
and if they're wrong dozens yes so I
know exactly what you're talking about
that was I alluded to that a bit so
global value numbering actually does
introduce redundant computations right
because it canonical eise's and what the
scheduling algorithm does is we actually
will compute whether that is redundant
computation on some path and split it so
that it's no longer Mihama so you have
the undoing no global yes yes and that
means that the graph that you get after
scheduling is different than the one you
had coming into scheduling which is kind
of bad excuse me um we can run it in a
mode where you don't duplicate anything
and that means we actually get a
schedule on the side which you can throw
away if you wanted to but what's your
general approach for creating new
optimization so in most cases if you
have some optimization and it obviously
can be a do optimization in some cases
at least because you can spend more time
actually applying it if you measure the
compilation time yeah so that's always I
think in terms of doing local reductions
they're essentially free more passes
over the graph cause you're a
compilation time to slow down so we try
to keep the number of passes down so
originally C 2 did a couple passes that
like you're done with graph reductions
and eventually went to one where we kept
back edges and did a completely
incremental and so adding a new
optimization would cost no more than the
time to execute it if it ever executed
at all it would never
cause of other optimizations to run
slower for any reason so it was always
free to run local optimizations like
that you're really close to that already
yeah we actually do very few passes over
the graph and we schedule only once a
few D optimize you're leaving the code
you will go to unoptimized code maybe
you come in
maybe you recompile alright so</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>