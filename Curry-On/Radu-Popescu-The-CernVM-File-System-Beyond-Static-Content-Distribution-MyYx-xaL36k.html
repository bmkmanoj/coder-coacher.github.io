<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Radu Popescu - The CernVM File System: Beyond Static Content Distribution | Coder Coacher - Coaching Coders</title><meta content="Radu Popescu - The CernVM File System: Beyond Static Content Distribution - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Radu Popescu - The CernVM File System: Beyond Static Content Distribution</b></h2><h5 class="post__date">2017-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MyYx-xaL36k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you so before I begin I would just
like to thank the organizers of Korean
this is the first time for me so they
really did an excellent job of putting
this all together and also all the
speakers who gave talks at Korean so I
really feel like I've learned many many
new things I'm really happy to to be
here so I'm a radical based NGO I work
at the CERN where I develop software and
today I'm going to talk about the CERN
in VN policy change the project I'm
working on there so as you may know the
CERN which stands for the European
Organization for particle physics
research is the lucky home of the Large
Hadron Collider which which was involved
recently in the discovery of the Higgs
boson this is the world largest particle
accelerator right now so for the moment
and of course with point this image is
already seen it takes very clearly the
the accelerator is outlined by the by
the yellow circle so just to give you a
sense of scale this is this gives
saturator the tunnel and here you can
see the city of Geneva this so we could
put it in put it inside the circle
easily and here you see the Lake Geneva
Lake level and behind you see you see
the moon block so if we go inside the
tunnel we we see that it's 27 kilometers
circumference and it's about 100 meters
underground
and the accelerator is eating up around
118 megawatts of power so again for
scale all the combined households of
Geneva are are around 180 so it really
the biggest consumer of power this is
not very eco-friendly maybe I don't know
but this is this is it so the bees there
are two bins running in opposite
directions around the around the
accelerator each one is
around seven tera electron volts of
energy so the energy of a particle is
around that of a mosquito buzzing around
but the combined energy of all the
particles in the stream is equivalent
that of a speeding train
so as a fun fact so at four points
around the length of the accelerator
there are the so called detectors so
there are four major major experiments
that are operating and maintaining these
so-called detectors either these are
very large devices they're there they're
located in caverns so let's say the
height of a man is about this big and
the the one for Atlas and CMS so these
two these are general-purpose detectors
that are able to do all sorts of physics
experiments and maybe they're a little
bit more well-known because they were
involved in the large I mean in the
Higgs boson discovery but the other two
are also very interesting for instance
Alice here with with it's very
characteristic magnet they are the only
ones accelerating very heavy ions metal
heavy metal ions in an attempt to do we
reproduce the conditions right after the
Big Bang
of course like if you want really
details about these things I'm not the
person to ask because I'm not the
physicist but there's a lot of resources
available for days now each one of these
detectors have one of these devices
inside this is about this big which is
the actual silicon tracker so the the
digital detector this is very similar to
what you have in a digital camera but of
course the performance is very different
now if this image is familiar it's
because it was used in 2013 by the
thrash metal band Megadeth for our cover
of their album super collider which I
haven't been listening yet but I thought
it was
interesting trivia Mustang at all now
back to the ER to the city contractor so
in principle is similar to a digital
camera sensor but you have 100 million
channels manage detector and it should
be able to react to punch crossings of
an event every 25 nanoseconds and the
idea is not just the ability to do
recording these things these detectors
are backed by a very complicated
hardware which which is supposed to
reduce the size of the data going going
into the hardware because trying to
store the raw data coming out of the
detectors is impossible so right now
we're seeing something along the lines
of 1 petabytes per second
internal data rate and the experiments
are storing around 5 petabytes of data
per year so you see the amount of
reduction happening so this is including
the derive data set and by the by the
end of 2025 which is the the latest
planned experiment that the CERN is
planning we estimate around 100
petabytes of data so 34 will increase
now the regarding the size of the the
code used by each one and experiments
we're talking about 5 million lines of
code okay this is not negligible but
I've heard that that it's a time it may
be tiny
by-the-bye enterprise standards but this
is this is what what there is there now
an important thing about the CERN is
that while the site in Geneva is the
house of this this accelerator and
detector equipment is not just there
basically the CERN is a worldwide
collaboration spanning 42 countries and
around the
seven dissenters so you have physicists
all around the world which which are
taking part in this science and that's
why the CERN is operating the so-called
worldwide LHC computing grid so you have
computers from all these sides which are
pooling their resources and giving this
this interface of a view of a worldwide
grid so when you're a physicist and
you're doing simulations or analysis
you're submitting jobs to this grid
hoping that they work like like they've
said and you don't know actually where
they go they could go to another
continent and - and this is actually
this this visualization is is the wal CG
light map which is available through
Google Earth and this is a snapshot that
I took last week so you see at that
particular moment there are around
300,000 running jobs in the grid and
using around 460 thousand CPU cores and
this all this amounted to 70,000
gigabytes per second transfer to the
through the grid so it's a not
negligible now about the software stacks
so each experiment maintains his own
software to do that to do the specific
physics of there natira collaboration
and on on each collaboration you have
hundreds of developers ranging from
novice more physicists who who do a bit
of coding to to very experienced
programmers and I think right now if you
look to these repositories you have
something but 10 to the power 8 binaries
which which which have accrued and the
the experiments are outputting around
one terabyte and they automatically
builds everything it is rebuilt every
night and actually the physicists really
like using cutting-edge tools so so
basically the the continuous integration
in nightly builds out their of their
experiments involve or start with
compiling the compiler usually the
latest stable version of GCC and you
bootstrap from there so there's a lot of
stuff coming out at the continuous
integration and all these those two
around a hundred thousand machines
worldwide and of course I've nightly
builds get pruned after this point not
not everything has to has to be archived
but still the daily production releases
are stored also due to these attempts of
doing reproducible research and so it's
important that they are kept around now
in these conditions there there was the
origin of this certain VM file system so
in around 2008 or 2009 there was there
was the idea of creating the so-called
certain VM a very configurable appliance
to be used on the grid and on on laptops
of physics is and basically giving them
access to the entire software stack of
the CERN wherever they go and this was
this was very impractical due to the
large size of these VM appliances they
were like a few gigabytes of size they
decide you had to remake it every day if
you if you had different configurations
you have one
they say one gigabyte for each
configuration so the idea was to solve
this problem so the problem of serving
the software fax we're not talking about
the data
talking about the executables so the
idea serve these things through a file
system and users userspace interface so
this was the birth third gamerfest
around ten years ago so it's implement
there's a fuse module I think I don't
know if you're familiar with the fuser
interface of Linux and it makes
available the content of the software
repositories which are around one to ten
terabytes of size so not not really huge
by say big data standards but the
terabytes a size full of very small
files and these are made available
lazily so on on-demand through a global
HTTP cache all the way down to the
individual clients and each one of these
repositories is mounted into the
/g VMFS file system namespace and of
course the important thing is like these
are these are read-only views so this is
a content distribution system is they're
not made made due to all make changes to
the repositories and synchronize also
important is the fact that HTTP is used
as a transport everywhere so you have
good chance of being being able to pass
through the through the firewall and
through two different corporate systems
or I don't know enterprise networks or
what about happening this is very simple
and reliable now the main components as
I said are infused modules so this is a
single process running in user space
which is which is talking to the to the
fewest kernel component and this can be
complemented with different cache
plugins so these these make the
performance this increase the
performance in different use cases for
instance you can imagine that that on
the on a supercomputer or on a cluster
where you have very fast Network in
between the nodes of the cluster but
relatively slow slow connection to the
exterior you would like to exploit this
and it's possible to exploit it through
a specialized cache plug-in so data is
brought in into the cluster once and
then it is distributed as needed to all
the workers then you have the server
tools which are implemented as stateless
command-line tools so on the single
writer machine that is assigned to a
specific repository you will run these
command line tools and publish new new
new changes to the to the repository and
then for the distribution so for
everything from the repository to the
client we are able to use standard HTTP
tools so we are able to use Apache nginx
and things like squid proxy you don't
need to write special code for this and
you can rely on on existing very
oh solution down here I mentioned
starting here in stratum one so
distracting zero is the authoritative
copy of each one of these repos and
clients will not connect to it directly
they will connect to the strata one so
these are full replicas of this
repositories and and they are globally
distributed now a few important data
points regarding the design of the file
system so for storing data
the core idea is immutable blobs which
are content addressed so here I put the
start over to remember it since we uh we
are at the programming language
conference these are these are concepts
that are seen inferences in functional
programming languages where you have
things like referential transparency you
know that the name will always refer to
the to the same data and it will never
change so these are the principles that
that that we have also used in certain
VMFS and first is things like
deduplication in this context come come
come for free because you end up with
the same thing I already have it and we
also do compression for this and then
for the metadata you have this so-called
catalogs where the state of the entire
repository at a given point in time is
encoded the numerical tree so the idea
is that we can iterate ibly hash the
contents of the repositories and we
build up a tree of hashes where the root
hash if any change in the repository
occurs this will be reflected all the
way up to the to the root hash of the
tree this is similar to get in principle
in implementation and this
- that comes out to the marquetry is a
put in a repository manifest which is
imagine it as a text file which gives
you the entry point into the into the
repository so this is the first thing
that the client downloads when he wants
to access the repository this is put in
the manifest and the manifest is
digitally signed so there is this
verification step you know that the
client has guaranteed that what he will
receive from from the server's is what
he asked for
so this is very useful and then because
of the design revolved revolving around
you need to book content addressed
storage you also you need to implement
things like versioning and snapshots and
we have features for this now the last
point is definitely not the least
important the fact that everything is
poll based so all the connections and
interactions between the client and the
servers is initiated from the client
these are the HTTP connections coming
from the client so this means that we'll
never run into trouble with firewalls
this system works as reliably on a grid
side at the grid side on a cluster as
let's say in a hotel on on spotty Wi-Fi
connection so there will not be any
problem in connection of core
differences beam but this is another
thing now the fact that we are we are we
are passing through a layer of caches to
the stratum 1 this gives a lot of a lot
of resilience to the reading operation
and it also makes it very scalable and
if any cash node let's say goes down or
becomes unreachable the client has logic
implemented to route around this and to
find another suitable candidate in order
to re-establish the connection
way to the stratum one server and also
because of all these caches which exist
cache is also on the client if the
connection is severed the client is
still able to do continue based on the
on the contents of his hats and hot
cache is just that doesn't receive
receive any changes anymore
so as I mentioned earlier the clients
are always talking to the stratum 1
servers never to the stratum zero the
stratum zero is where the writing takes
place and just to give you an impression
this is the the current network of
stratum 1 servers all around the world
or in the UK and in the Netherlands and
Germany in Asia in the US all of them
are serving their planet my closest
clients so I haven't covered reading
let's talk about publication about
writing into these repositories so I
mentioned that if the system is designed
around a single right there so you have
a centralized machine which is close to
the to the repository so storage which
is doing this the writing operations
with the help of these stateless command
line that I mentioned and once we're
entering the some sort of transaction
phase we are constructing a readwrite
view of the repository for the for the
sake of this transaction using Union man
such as overlay at I saw really a ufs
and at this point files are compressed
in hash and they are written to the to
the repository storage so on the writer
machine the the repository storage is
either accessed through NFS or through
for instance an s3 API and we are able
to use any back-end for the storage as
long as this back-end is able to export
this contents to standard HTTP so that's
why this tree is very useful for this
and after the files are compressed and
hashed and written those so then you
would have
the metadata catalogs are created and
published and finally after all this is
done all the data is present in the
repositories we are able to switch the
the manifest of the repository with the
new one and this in a single atomic step
the the new state of the repository
becomes available so this is important
because at no point the clients will see
will see the repository in an
intermediate state they either see the
old version or the new one not not
anything in between so to give you an
idea of the actual workflow so on the
centralized release management machine
you can add to it who SSH and then you
won't see me unless I serve our
transaction to initialize the
transaction you can also specify a
specific repository if you have multiple
repositories installed on the machine
then you start editing the files that
you that you want
and finally you publish and at this
point all the heavy work is done a
fashion compressing and lighting so this
sort of approach has pros and cons pros
it's very straightforward to use because
you just log on in any runs of a set of
commands and it also hides somewhat the
distributed nature of the system so your
may not be aware that they actually here
you're talking to a different machine
but it was the saying that the
distributed system is when a machine you
didn't know existed is is crashing and
work impossible so and of course this is
good for scripting to the fact that you
have command-line tools this is 99.9% of
the updates happening to these
repositories are in an automated setting
in a continuous integration system so
the cons is not really support for
concurrent writing you have two people
logged in or two users logged into the
same machine that will be it will run
to trouble and the thing is like we
actually have performance issues for
very large change sites we have clients
which are reporting this so right now
with these tools you are able to make
use of all the course on a single
machine it's it's paralyzed it's there's
no wasted resource there but you're not
able to scale out and they are seeing
that they are trying to publish an
amount of files where they're reaching
the limitation of this and they like
more so we set out to improve this and
of course like the moment you have
people you have users logging into a
machine whether this is unsafe and
should be improved with some way now
before setting out to do anything it's
important to note the properties and the
constraints of the system we are working
in so we are talking about the file
system and power repository so in in our
case consistency is key so nothing we
should do with this repository should
leave it in an inconsistent state this
is something we cannot afford and given
the large number of clients that you can
potentially see we also like the highest
availability possible but always given
point one something highest availability
possible as long as consistency has
maintained but the design of our
software gives us some some potential
potential avenues to explore when per
certain currency so we have the
availability of our data and the fact
that that is addressed by its content
and this this means that you will not
have to two entities trying to all right
to the same power
and then you also have the fact that
pushing copying these objects into the
repository storage is idempotent so if
it happens multiple times doesn't should
not change anything and of course the
data we are working in is a directory
tree so this gives you clear branches
where you can operate in parallel
without without setting on the toes of a
neighbor let's say and of course this is
not embarrassingly parallel you have a
critical section in your algorithm which
involves updating the metadata catalogs
so updating the Markel tree that
represents your repository and at the
end swapping the manifest but luckily
this is not a computationally heavy job
that takes a long long time so this is a
short critical section at the end of the
the heavy work so this summarizes the
existing architecture with and without
me describing our user from his machine
he goes the release manager over SSH and
there we have the fuse module and the
server tools and this machine sees the
central storage in the repository over
NFS or s3 and it publishes its contents
to the side of one servers over HTTP and
how we improve this is by doing this so
now we see that the central release
manager machine we cut it in two into
the user facing part and then let's say
the storage gateway and now each user is
able to operate its own release manager
machine which can be can be generated
on-demand it's it's there's much less
State there and does the heavy work of
processing the changes to the
repositories there and then finally this
is released
is publishing this changes to the
storage midpoint but it's talking to a
well-defined service API that is defined
now and afterwards is the same thing of
course in this approach we can also we
can also run multiple storage gateways
with the replicas of the session session
data to increase the availability of the
system and in our improved workflow we
see that but the change the steps are
the same but now we have two users on
set each on his own release manager
machine the requiring transaction
they're starting transactions and
acquiring leases on on separate sub
paths in the repository and finally
they're able to push push there without
any problems now in this new approach
the certain vnfs release manager machine
so this one is the consumer of our new
storage gateway API and his job is to
try to acquire release from the Storage
Gateway to the Dilys being given on a
specific subdirectories of the
repositories if he acquires the lease
he's then free to hash and compress and
send the files over and of course this
is an important thing it's a separate
machine to the gate and ok this is a
media logistical matter but we're giving
more more freedom to the user of
managing the resource and deploying
based on the size of their of their load
and then the second part is the storage
feed point which mainly serves as a
distributed lock manager for for all the
release managers and its job is to check
the rights of clients whether they have
these rights to the rights to to modify
the repositories in managing and then
give out handout leases to the different
repositories of that and of course
in the case of say like a high
availability deployment you have
multiple multiple replicas of the
session data and I guess in the in the
cap theorem classification is with your
CP system so we are okay with losing
availability to asset or system if some
if some sort of network trouble happens
we wait for report early start o to
maintain the consistency of the
repository and of course the storage
data is the one that receives the the
payloads from the release manager client
and it's writing them to the
authoritative storage now for the
implementation of this new component for
for the Storage Gateway we actually went
with with Erlang OTP the I didn't
mention it but all the other code is C++
so it's a C++ is actually the the main
language in use at the CERN from from
systems all the way to to the Pacific's
who are doing their analysis in C++ too
so this is the one thing to keep in mind
now it shows Erlang an OTB because it
seems very suitable for this sort of
concurrent and distributed applications
and you can think of this also as sort
of pilot program for us so if we see
that for disk use case it's working well
we may deploy it into other into other
sections too so I don't know if you're
familiar with Erlang and OTP
the language and it's built around the
concept of the around the module on
actors actors are basically lightweight
likely to processes managed by the
pannier MDM and they they are they have
complete memory isolation so the only
way to after scan can
interact is by sending messages to each
other so did there's no shared memory
made available to them the language has
beautiful immutability of values which
in certain cases makes things very very
clear or expected but I think if you
were at the keynote this morning the
keynote of Jersey Valley lodging he did
me the favor of explaining already these
things so a super vision tree is if you
have a tree of these these are actors
these processes which implement your the
logic of your application you will only
focus there on the happy case so error
handling you will not implement it there
you will let these these these processes
crash because it is very cheap to to
also restart them and you will implement
a parallel tree whose sole purpose is
implementing recovery and an error
handling logic so this error start at
the bottom of the tree and basically the
the lowest level supervisor says ah
there was an error with the guy I'm
supervising what should I do if it knows
how to do it will just restart it if it
said that I don't understand this error
it's it's it's above my pay grade then
he will he will say I'm dying too and
let the error propagate up to his own
supervisor and in practice this this
gives you very very very zillion systems
and it's also good for example to work
with it with with this so we thought
that there wasn't a lot of risk by by
trying this new technology the fact that
the language and the framework and the
virtual machine are battle tested at
this point so they've been in use for 30
more than 30 years at Ericsson for for
running system which are much more more
critical let's say and of course last
but not least
the system gives you many ways to
interact with C++ code so since most of
our code code base is C++ this is this
is a really important to us because we
cannot afford to not really use it so
the architecture of this application is
very simple and with the exception of
the HTTP front-end which we implemented
with with the cowboy Erlang server so
this is the standard let's say standard
Erlang server that is used by bye-bye
Erlang developers right now I think also
the the Phoenix framework mentioned
bye-bye host Valley is also using this
this Erlang server so they don't
implement their own Alex your server
they're using this one but with the
exception of that everything is
implemented with with the base
distribution of Erlang so with the
included batteries of the distribution
we implement the main business logic
using OTP generic servers so the the
generic behaviors are are say a way to
implement some sort of behavior
polymorphism inner line and we're using
a media which which is the built-in
database or key value store for the
persistence of session session data and
then the the computationally heavy parts
are implemented in a C++ program which
is coupled to the main line application
through a so-called poor driver so think
of it as a KO process so you have pipes
going in and out of the verb the process
and and you have a request reply cycle
happening so the same thing you have the
on top you have the HTTP cowboy
front-end it's receiving requests from
the release managers and it's extending
them to the back end and the back end is
the one who to dispatch is them to go to
the roller
model so all these things are basically
confirmed concurrent processes in our
applications you have all have handling
the access control list and then the
least module which is which is checking
the availability of these is for
different subclass and finally once the
time to receive payload please go to the
receiver which which which she passes
them which forwards them to the
synchronous bus workers which are
usually meant to be run 1 / 4 or
something like this and it's also
important to note what sort of the
experience this was for us so we're
coming from from C++ so the fact that
you have so GE the framework for
building application comes with a lot of
stuff let's say so your you have so so
many tools that you can use there to
build your application and the fact that
you can do like tracing and inspection
again it was it was shown in the morning
keynote that you can connect an observer
to a running node and see exactly what's
happening with the message queue all the
processes and this is really really
great thing we're not used to it like
that you have the mutability and
functional programming concepts again
this discrete disguise simplify certain
algorithms the certain parts can be can
be made very very clear and of course
the entire language since it's focused
on concurrency it really feels like a
domain-specific language for writing
concurrent code it's you do this
effortlessly and it comes it comes with
things like unit testing and integration
testing frameworks quick check my jump
use and dialyzer dialyzer means you can
annotate your your functions so i forgot
to mention that
so the Erlang is dynamic language you
can annotate it with with type
information annotate all your function
then this is a checker it's I think it's
using such success typing to see if what
you have their electrical changeable and
we have good integration in C++ now of
course for the given are seekers past
experience which is not the most they
say the strictest type system but still
it's a bit more strict than the dynamic
typing experience of our life it's a bit
weird but I have to say that you can
immutability of the data in practice it
it's it's very manageable even though
you have dynamic time typing if this is
not your your thing you will you are
helping immutability and there are some
things which it's more of a lack of
polish such like the default errors are
very verbose and it takes a while to get
used to it and some parts of DP API are
not really well documented so this
summary we were able to get more
concurrency and handle a lot of like
some more traffic for our clients and
also give them more more flexible
configuration for their knees now to to
end I would just like to mention a few
other projects and activities a certain
LMS there's a there's a technical
student in our team which is now working
on a darker plugin so if you've ever
used docker and you have to download an
image that contains something like ten
layers and you have to wait on a slow
network for for all the layers to
completely download before actually
running your your 5 second operation or
what what it may be the things like this
will come in handy too it's a plugin you
install it and then it makes available
the contents of these images from
certain VMFS repository so let's see
downloading the the layer at the
beginning is instant and then the files
are fetched on demand as you are using
them so in the case of an image where
you have one gigabyte image size and
you're using two files this this would
be really
so he's working on this and can take a
look at this code at that link we also
tried are trying to improve the
experience of cbofs on HPC machines
which have special configurations reduce
connectivity from from the nose to the
outside and we actually managed to do
the work with the guys in CICS in in
Switzerland in Lugano which is the swiss
supercomputing center to get it running
on their machine place tank which is the
number three of the top 500 and there's
going to be a talk by by my colleague
here I got a blower in Frankfurt two
days from now so maybe it will be made
available online and next year we are
celebrating 10 years of this project and
there's going to be the seventh EMFs
workshop I discern so at the end of
January if you are interested and would
like to join us you are more than
welcome
so this is this is the King working on
this so there's me there is a lack of
bloomer my colleague and he's his PhD
thesis was the origin of was the start
of the certain VMFS project so he knows
it the best jargon is our team leader
but we have a former technical student
who was working more on the CERN VMO
pious and Nikola hardly the current
technical student working on the docker
integration so thank you very much and
if you have questions
the engine on it means the compression
hashing and transmission phases and all
these things and also the fact that
there's the duplication so could you
speak a little bit more about the
precise order in which hashing
compassion lead replication happens
because I was matching that with large
data volumes it makes a big difference
this is a good question but I wouldn't
really know how to how to transfer it
precisely right now I would have to
check because obviously just this this
can have a great impact well I think
right now the default hash is sha-1 but
I think for this use case we're still
safe because the the recent exploit
wasn't a pre-image attack but we have
full support for switching the the hash
it's just that most of the repositories
right now are using sha-1 but you can
can switch this and you can even have a
single repository half of the object or
sha-1 you half of them are inner another
hash so we can change it as we need if
you have multiple hash and you have the
same contents wise
get the idea is that you have oh yeah
you could have it once in one hash and
one the other things there's something
to do that takes care of this but I
think having half-and-half probably
isn't a the easiest way to go for it's
probably to try to avoid it but
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>