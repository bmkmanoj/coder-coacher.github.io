<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jörgen Brandt - Functional Programming and Foreign Language Interfaces - Curry On | Coder Coacher - Coaching Coders</title><meta content="Jörgen Brandt - Functional Programming and Foreign Language Interfaces - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Jörgen Brandt - Functional Programming and Foreign Language Interfaces - Curry On</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/E6fpICLZ1cw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is one of these one of these
chests timer talks in case you haven't
been in one of them before the rules are
pretty simple the idea is basically that
the speaker has 20 minutes of speaking
time and the audience has the other 20
minutes which means you guys actually
have to make some effort ok sorry thank
you so you guys actually have to make
some effort to participate in this you
can't just you know let him time out at
20 minutes and then just walk out of the
room you guys should be raising your
hand and asking questions he's designed
us talk to try and get you guys to you
know do them so I put the responsibility
on you guys to raise your hand if
there's something unclear at all and
then ask the chest timers a little bit
weird it's supposed to be 20 minutes for
each speaker and audience but this Jess
timer only operates 25 minutes so
subtract five from this but I will raise
I will raise these signs to let
everybody know when they have two you
know five minutes for the audience or
five minutes for him so okay so my name
is Jurgen and I'm going to talk to you
today about functional programming and
foreign language interfaces which are I
think the essentials of distributed
computing but we'll see ya and a little
bitty words about myself and currently
on a PhD position at the hamodia versity
in berlin i am the creator of the
scalable workflow language cuneiform
which i'm also going to talk about today
and the major field of application for
this language is is bioinformatics but
it turns out that the language can be
used also in very general scientific
applications but we come to that and yes
this language cuneiform as I said is a
scalable work
language and it borrows from functional
programming a lot and allows the reuse
of existing suffer which is quite
important I think because people in the
who are willing to do scientific data
analysis don't want to first pour their
software or write wrappers for using
them but a little bit more about the
problem domain so what we're facing
today in the field of bioinformatics and
especially in next-generation sequencing
is that there's a coming up a new kind
of hardware which is capable of
producing data extremely fast extremely
much and also you can operate it very
very well in parallel so it's natural to
say that if we can generate this data in
parallel we also have to analyze it in
power in order to keep up with it with a
pace yeah and what is also quite
interesting is that that the cost at
which we produce this data is dropping
even faster than Moore's Law so it's
it's not not only a halving of the cost
every 18 month but it's even even even
better yeah so the challenges are even
more so this graph reverses the view a
little bit here we look at the
throughput of these the sequencers which
are the yellow and the red curve and the
blue curve is how fast the the capacity
of our heart this is growing and it
looks a little bit this is a little bit
of misleading graph because the EE you
see this at this point where the lines
crossed this is not really the problem
point is just where it's just um yeah
the lines intersect there but that has
nothing to say but what is what is
important about this graph is that
the cost of the throughput at which we
get these machines to work it grows much
much faster than the our capacity to
store this data yeah so and the the next
problem is that if we look at this
scientific domain and all the people
that work in there and all the
universities that contribute software it
turns out that this is a very diverse
field with many people and many sources
of contributions and and this is a
screenshot from a website sec answers
which which enumerates here only the
categories of tools that are available
in this field of research and in in
parenthesis you only also see the the
number of tools that are categorized in
there and is this is a huge amount of
software that is at our fingertips and
we also need to be able to use that it's
not if you if you come up with the next
data analysis platform you you can't
just say okay and everybody migrate
their suffer through to our platform you
have to have a way to incorporate what
is already there because these people
that won't sit down and rewrite their
software just because you have a fancy
tool yeah and this is a kind of the
problem setting to reiterate we have a
lot of software that we that we need to
reuse we can't just reimplement it and
also wrapping it is a kind of costly we
have machines that generate data at a
very great pace and that it can operate
it can be operated also in parallel and
this is kind of the what do we do now
yeah so this is a question I asked
everybody this is kind of the point
where you can interact with me so
yeah okay well this was five minutes
okay and to keep you two to put you a
little bit on the spot I will ask this
question to ask this to myself this is a
very important question i got from also
from from the users why would why won't
you why would you use anything like
generalize of general-purpose
programming language not up to the task
well what is what is wrong with Python
why can't you why can't you solve this
with with Python or pearl or whatever
yeah so the thing is since you're using
all the kinds of tools I guess the tools
that won't don't get faster just because
you drive them from Python or from from
anything so the tools are just as
efficient as they are if you don't
reimplement them or touch them in any
way so it turns out that exactly as you
said there was this this blog post or
the paper from Osterhoudt in the
nineteen nineteen eighty ninety eight
where he said that stripling language
was set out to be the the gluing of
applications so you could say that that
for Python offer pearl or these
scripting languages it is exactly this
is this is what they try to achieve
right to to be the gluing code for all
these foreign tools and but it turns out
that there's other things that there are
complicated and I think that that of
course it's possible to do in Python or
in pro or whatever scripting language we
have but but the hassle is is a little
bit disturbing so what you need to do is
first of all you need to have a way to
import all these all these tools in
let's say Python for a command-line
tools this is not so hard to do because
you have this command OS like you do
further the control to the operating
system and you get your answer back but
what if you have for example a co step
in our then you have to first wrap your
our code in command line and then you
have to
call your command line to from python
and then gets it's not impossible but
perhaps we can do better as so so the
cost of wrapping is something that you
can reduce at that point and also what
most of those these scripting languages
give you is there's libraries where you
can paralyze for example a map you don't
have to do it like sequentially what you
can do it a multi-core and use all the
capacity of your multi core system but
what is what may be even harder is if
you want to do it on multiple machines
right then there still may be libraries
in Python or are or pearl that give you
that but it doesn't come out of the box
the user needs to know these libraries
you need to drive these libraries and he
needs to know what to do it doesn't come
automatically and this is where we can
improve and the second remark perhaps
that I'm curious to hear what you have
to say about is by infinix is built on
unix tools and the philosophy needs to
make its way to the big data set so if
i'm going to provide a tool that does
big data and i wanted to work in this
spy on phonetics environment I just need
to I just need to have a way to import
all these eunuchs tools and then I'm
done is that so
a big data tends to happen in in big
corporations that they are very special
about their IP so sometimes I've seen
internal tool that are built on top of
the current open-source ones and those
are never going to give the light of day
because you know that lawyer team is
probably more powerful than restaurant
and put together so I think that's one
of the reasons why that the feel maybe a
little bit underdeveloped because most
of those like high-end tools i kept
sacred or indoors yeah that's that's
true I think so true i also have doubts
about the first part of your claim is it
really build on unique tools exactly
that's that's exactly what i think to
its if you look at this plethora of
tools you just can't expect them all to
be come online to install some of them
will be our libraries some of them will
be Python libraries some of them will be
have a different ap is the other is so
easy to to to end up with with an
arbitrary API and you still need to
drive that so the the multi-language
aspect is very important here
yes I just have a thought about the the
first part when you're talking about the
speed of hard disks or like the storage
capacities it's not as fast that's the
amount of data that you getting yeah and
I mean clear at some point the the
storage is going to be totally
insufficient there's no like you can't
use compressor so maybe the the idealist
need is another unix console which is
like pipes and streams where you just
have to look at the data is the hose
data that you used you can't though you
start to dip in and look at stream live
kind of that model like you have to have
to shift of models instead of storing
that and then looking at it yeah this is
this is valid approach if you allow to
throw away the data after you analyze it
you can get rid of the raw data quickly
but but only keep the analogy you get
out of it what is what I think is a
sense a little bit in the way of
streaming is that oftentimes these tools
get replaced so the standards also
change from time to time this is often
but from time to time the the standards
of how should I analyze this data a
change and then it would be extremely
valuable to still have the the raw data
at hen and and redo the computation
because as you go as the algorithms
improve also the knowledge that you
could get out of this data that you
might have been throwing away you still
have that so it's also a worthwhile
activity to do to store the data now
even if it's costly and another approach
you could have is like as you said
streaming not to build hard disks but to
build refrigerators where you keep all
the samples and and sequence them as you
as you need them but that's
different kind of thing it's not that
worse yet so we were still getting
bigger hard drives and we still and it's
still cheap to store stuff but we we
might run in a problem soon so it's like
yeah something that drives cost
eventually okay so so here this is again
yeah command-line tools wouldn't be
sufficient in the end we also need other
languages to be supported and when we
look at how the scientific community
approaches this problem there's this
keyword of scientific workflows and a
scientific workflow is essentially a
description of an analysis task as a
graph is a directed acyclic graph in
which the nodes are tasked to be
computed and the edges are data
dependencies and and the good thing
about scientific workflows is they they
already some of these central workflow
systems that that that are available
have a very good focus on integration of
tools so they already do that and some
of them are also there's also scientific
workflow systems out there that scale
and that use distributed infrastructure
and but what they will with these
systems most sort of time focus on its
reproducibility so the aspect of can I
repeat what I also said earlier can I
repeat a scientific study because in
science is always a necessary that that
you yeah that you can reproduce
everything and this was a major aspect
for the scientific community all along
and and then when you go and try to find
a system that has all of that it turns
out that some of these workflow systems
especially the ones that scale
are are limited to come online tools or
and also what is to say there is most of
these even if they even if the
scientific were closed system scale they
were conceived at a time where this all
this Big Data technology wasn't
established yet and this is also
something you can feel and you when you
use them and what is perhaps the most
important cornerstone here is that that
that this representation as dag is a
limiting representation so the
oftentimes you have data analysis tasks
that just don't fit in this model of of
representation for example if you have
an iterative algorithm that that
consumes the output of the of the
previous iteration then it is something
something that you can easily represent
in the deck and what I think is a
worthwhile approach is the combination
of these two ideas so functional
programming and foreign language
interfaces if we have these two together
then then this can bring us much further
as compared to the point where we are
right now and from functional
programming this has two things we can
get and the first is a functional
programming as a programming paradigm is
very expressive it's universal yeah most
of the functional programming language
is you know can express any program so
this is something we want to have here
in it in order to get rid of this
limitation that Dex can't expects all
the programs we might need and also what
is what is very natural and functional
programming languages the state of type
of lists and operations on lists and
also you have the possibility to to use
recursion to express
iterative programs even if you don't
have any syntactic sugar on them is you
can always use the recursion right so
what functional programming gives us is
a very general way to express data
analysis and the other thing we get from
functional programming is that we have
the I mean if if we can assume that the
all subtasks the sub functions are
side-effect free and then we can
paralyze the independent parts of a
functional expression and we if we
additionally don't execute each and
every expression as soon as we get it
but but try to collect as many
expressions as we as we can before
actually a computing one so lazy
evaluation then we we can what we can
get as automatic parallelism and this is
something yeah we want to use in data
analysis basically you're using like
your small tools as sub modules and
those are eager dark there are no lazy
so how does this play together yes so it
turns out that it's not so much a
problem that you have sub perils of your
program that are eagerly evaluated as
long as the overall computation so in
these tiny bits here you don't have
control so your your foreign application
it might be of lazily evaluated it might
not be you but in any way you cannot
paralyzed automatically in a piece of
code you don't know right so this this
is not an opportunity anyway so these
are following bits of code they are
always the like the granary granularity
level at which you can do this and you
can't drill into these but as up to
these foreign code set see you can do
that
and you might have problems with that
will grin right also because even if
he's an external call it could be faster
very short and the overhead of
concurrent execution could be not pay
enough so automatic perilous means
always suspicious because you need more
information to know if a it makes sense
yeah I don't I don't know if I got the
question right but how do you know if
when it makes sense to parallel life I
just always paralyzed I just assume that
all the tasks i have in my workflow our
side-effect free and cannot get in the
way honorless not because of the defect
because a concurrent execution have our
head yes yes and I in corona head that's
true but I assume that the chunks of
work that i am that i paralyzed are big
enough to to not make a big impact a few
if you also count the overhead so I
don't paralyzed additions and
multiplications so that would be very
stupid but but you can if you have a job
that runs for one minute or 1 hour then
it perfectly makes sense to take the
effort that is necessary to paralyze it
and write creo que but you can use a
library because the algorithms
complicated but fast so in that case he
is going to be as lower because you
don't need to paralyze very fast calls
exactly you shouldn't do that then and
it is up to the user to not use the
system in a way that that makes it
actually slower but yeah sorry
so we don't have control over it so you
have to use it in the right way so a
little bit back to your problem
statement right you mentioned this
problem with represent different tools
and the reproducibility problem right
and this is not exclusive to academia
industry deals that a problem as well do
have solutions we have containerization
the reducible builds infrastructure is
cold so I did you to look into this kind
of approach encoding this whole
infrastructure this whole workforce and
making it repeatable in this way and in
the way industry does yes so if you
refer to the way industry desert as
controller ization so to use docker or
chef or I don't but the one thing is the
infrastructure so of course you need the
right infrastructure to to run this wet
floor but the other thing thing is first
how do you how do you organize
distributed infrastructure how does
contain X Y know when it has to start
when all the input data is ready for it
and the other thing which is the most
important thing is how do you want to
express this kind of dependency graph
and so the it's it's much much more
about how do i write down a program that
distributes that automatically paralyzes
that uses all these foreign applications
and not so much about the infrastructure
then you then we can map this to any
kind of infrastructure and we we method
to Hadoop and we also it would be
possible to do it on different
infrastructures also so we the answer to
your question would be a it's it's
possible and I think it's also a useful
thing to use all the infrastructures
like that come from industry and our
open source to execute these kind of
data analysis applications but what
these infrastructures don't give you is
a way to express the data analysis
itself it's just a way to execute it
yeah there was another yeah this is
related to the the other question and
and let's pretend that I'm a straight
hustlin consultant now and I'm just
saying okay you're doing it wrong you
should you do microservices with docker
like he said and rest services and you
send all the data to rest and what's
what's your response to that and do that
if you done first yeah yeah and if you
if you're successful doing it with micro
services and I don't know then go for it
yeah I mean nobody substitute I i have
my own project also i mean if you look
at under the hood yeah when I come to
present my language turns out will be in
Erlang and he had and you will have
services and stuff like that but that is
not the point here the part of the
question is how do you express your data
analysis tasks and not how do you
implement all the organizational stuff
right now on this high level yeah and
then when you when it comes to executing
it and run it comes to distributing
stuff then then microservices and all
the software will be a very very nice
thing to have but ya know it just
doesn't seem very I mean sending sending
all these data however rest seems very
inefficient to me yeah exactly yeah and
that is there's also a point exactly so
you don't send the data of rescue center
to a distributed file system and you use
data locality to leave here that okay
thanks
okay and the and so these are these two
things yeah the functional programming
cool stuff you gain a very general way
to express programs and you can
automatically drive parallelism and the
other thing is for in functional
interfaces so what we how they would
this look like in a functional
programming language well you just need
to have a way to say for each function
that you define okay this will be in
bash and then you call you a command
line two oh this will be in Python and
then you call your python library and
and and this way of describing your
following task or for you for in a
function yeah it gives you a very
natural way to embed foreign code in an
organization language and the cost
essentially what you're doing the
activity of writing a function in a
foreign function language is the
activity of writing a rapper but we want
to have this rapper as as low cost as
possible yeah it should be a one-liner
to wrap a foreign tool and this is
simply what thank you I wanted to ask
how are you leveraging the GPU power
when you're writing things on those
languages how is the tooling because
right now when you're doing heavy
computation like crypt equipto keep the
coins and all that stuff is supposed to
be running under GPU but I don't know
too much but right now about the tooling
that you're using is it running CUDA is
it running like opencl any of those so
it just runs on the CPU so no any not
not any of that so if you're it all
depends on these foreign tools you you
include so if these run on the GPU then
we run on the GPU and if they don't we
don't how many of them run on the GPU I
don't know any just a question
is the particular reason to have looking
at this all these tags I I see a mix and
match or algorithms infrastructure
databases and you have GPUs as well we
doin fine but again the world why do not
characterize your workloads and say okay
this is infrastructure this is general
algorithm so what are you looking to
achieve as a kind of system to perform a
orchestration between all these tools
exactly or just to have a better way to
handle your resources no it's it's a
little bit of both yeah to have the
assistant that lets you use all the
tools and also who and what I described
earlier is we want to use parallelism
and this is this is exactly what you
said to to read to use the resources in
the right way but on a very high level
so it's not like okay here we have this
machine and it has a GPU and perhaps it
has as X inside yeah and we want to we
want to get the leverage of this general
and special purpose hardware but to if
we say okay we have a cluster where
where every machine is homogeneous yeah
and and we know we can do this and that
at the same time then we want to be able
to use these resources okay do you think
about using measures including sort of a
scheduler in mazes to end all these sort
of things because you need to make it
scattering and orchestration of tasks
then you get the result and combine them
so you can write your own language to
define what your schedule should do and
leverage meter skaters also
me yes so yes we thought about using
missiles this is essentially this very
good point because my sauce is exactly
it it is a system that gives you
scheduling and distribution right away
and we have an implementation on her too
but there's it's not the only thing out
there that emergency resources meses is
a very good example so I think one of
the important distinctions between like
mesas and like what I hear you talking
about is like Mesa just gives you
resources it doesn't actually like it'll
run it and it doesn't give you any
result from returning it so like working
a similar thing or you run into this
bottleneck of like i tell you i want
this many cores and stuff in it like
it's a nice tool for giving you that but
it doesn't actually like help you
construct anything useful on top of that
or scheduling it or stitching any kind
of result back together so i think
that's an important distinction to draw
if you aren't used to working with mesas
and just sort of like so the word you're
right and this is why we we just we're
not done by just using mesos we have to
put something on top of it in order to
achieve exactly what messes cannot give
us so the the piece of code that we are
running on like messes or any other
resource management system is has to do
all that it has to talk to a distributed
file system it has to give us return
values and we have to have a way to to
know of these values and this is what we
have to do here in order to do it that
way okay so this is my call oh my god so
what is cuneiform it's essentially it's
a minimal way of achieving just the two
things I showed you earlier so this
gives you distributed execution right
now we have an implementation on HD
corner and Hadoop messes would be nice
it's a it's driven by a functional
programming language in which we assume
that all the tasks Ida factory they
terminate their deterministic and it
gives you the possibility to draw in
libraries from our from Python whatever
and then we can set out and and take
this scientific work flow where we have
all these problems yeah and we can
petition the data infer the parallelism
and run it distributed so that is that
is the core of selling point here and
let's look at an example so this is a
task that this is a very yeah that
unzips a file so we have this the name
of the task is aji unzip don't get
confused by the Deaf task thing is just
a definition of a function and in
scientific workflows we're talking about
tax so it has one output and consumes
one input both of files and I now I say
okay I want to write this function in
bash and then I step out and and put in
some bash code and and then I'm done and
then I can just call this thing like a
function and when I when I call this
function on a list of stuff then the
then the language interpreter is able to
find out that the unzipping of my
archive one dot jeezy has nothing to do
with my archive to dodge easy and they
can do this in parallel and distributed
on the cluster and whatever very
simplistic but this is essentially how
it works and then we can compose out of
this all kinds of workflows right now on
the website you can find examples these
are only from there is a question
I'm a bit confused because you say if
you unzip a file that's a side effect
yes that's true on the file system in
the in the UNIX thing but but what what
you get here is so I at some point I
have to introduce side effects right so
there's all the tools they they access
memory they access a hard disk but only
for the workflow system it has to look
as if there was no side effect and when
you look at the layout of this function
it it consumes a string which represents
a file and it outputs a string that
represents its fire fire and as long as
this doesn't interfere with any of the
other tasks and cool but of course at
some point I have to allow something
like a side effect it's not likely the
Haskell way of things there were you we
try to keep everything side-effect free
until the bitter end but to you these
tools are they then are constrained in
that very sense so locally they produce
files in the consumed files and they may
be even talking to the internet but but
eventually it has to look like yeah as
if they only produce something and and
didn't do anything to to their
environment it's just I kind of have the
same problem I don't really understand
how so you're saying that you're
assuming that all of your tasks like all
of these building blocks are side-effect
free and parallelizable but that seems
like such a big assumption like you're
gonna have shared resources that they're
gonna have to cooperate around like how
do you have a way to let's talk about
that yeah that's true this is the
biggest assumption I have and the of
course if this assumption fails then
that then I'm done and I don't have
anything useful to offer but it turns
out and from experience here this is
only because we look at the tools and
look at how they work and they they they
work exactly like ji anzi yeah they
consume a file they produce a file or a
set of files and they most of the time
they don't talk to the Internet and if
they do they don't change any stage
which could interfere with another state
so this is where I draw the validity of
this argument from because all these
tools do it's okay to have this
assumption but it's not generally the
case that's true I want and I need to
raise to the end of my talk I guess so
there's a lot of examples they are all
from bio informatics check them out this
is how this can look like so here we
have a workflow in a description and
essentially this is a commented code so
further down is we find the code and it
has a beautiful rapel and to wrap this
up what cuneiform is it's a functional
programming plus foreign languages it
gives you distributed execution on
multi-core a few condo a dupe automatic
parallelization you have a very
expressive way to express your data
analysis workflows and you can use all
the libraries you have already have and
I want to I want to conclude this talk
with paper from 75 and their Frank
deremer said by large programs we may
mean systems consisting of many small
programs possibly written by different
people and if we take this literally we
also have to take into account that base
programs are written in different
languages and yeah usually I would ask
four questions
I mean you can come up and ask questions
but we should start changing rooms
because there's another talking five
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>