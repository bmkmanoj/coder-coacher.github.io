<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mira Mezini - Modular Composition of Reactive Applications | Coder Coacher - Coaching Coders</title><meta content="Mira Mezini - Modular Composition of Reactive Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mira Mezini - Modular Composition of Reactive Applications</b></h2><h5 class="post__date">2017-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NQCc3SWROJ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
okay um I'm supposed to introduce myself
I was told I'll keep it very short is my
name professor of computer science at
Tim Duncan in Germany I'm doing research
with my team they're basically in
programming language design and
implementation and static program
analysis and security so today my talk
is about our work on programming
language design and I will start with a
very brief summary of the talk so that
you can decide for yourself whether you
want to stay to the end or not so I'm
saying that today's applications and
most of them and what I mean by this you
see a list of some examples of what I
consider as typical applications we'll
build today are controlled by the
environment rather than controlling the
environment as traditional ones which
means that they need to react that fast
to data changes and the currencies of
advance and the other property of them
is that they are mostly about
correlating data and events correlations
are the units the more interesting units
of abstraction and Composition rather
than individual units data so the
question is how well does existing
programming technology mainstream or not
so mainstream supports these two things
reactive computations we have deep
updates and event data correlations
let's start with reactive updates well
we're still on mostly have this old call
back way to handle reactive updates to
some interesting changes and basically I
will tell you that that's not good and
to explain why to make it
hands on our explanation of what I mean
let's look at a very small piece of code
what does it do because it's kind of not
very straight and it's a mainstream
language in which it is implement
implemented we declare three variables
our J and week is kind of an embedded
language in Scala and then we declare
three events so we have a language here
that actually supports events first
class on kik is somehow triggered let's
say from hardware while new day a new
week are defined in terms of check by
opposing team with some predicates and
then down here we attach callbacks by
this plus equal operator
Comex implemented as closures in Scala
to those events so far so good let's see
what this application is piece of code
does well every time a chick is
triggered will execute the callback
which will change this variable hour
which may cause the predicate here to
get filled which will cause moodily to
be triggered which will cause the
callback associated to need a to be
executed which will change the day
variable which might call the new week
event to be triggered the associated
call that to be executed and so on and
so on and I hope that you agree with me
that this is way too complicated way too
complex control flow for a simple thing
as basically expressing the time weeks
days and hours which could be expressed
in this three very simple lines defining
our internal subjects and they meet in
terms of cheeks if only we had the
programming abstractions that allowed us
to declare these relationships between
variables and make sure that those
values where the variables are kept
up-to-date so every time cheek changes
our day and week also
automatically get up to date so um
forget about the much of the text on
this slide basically we all know I'm not
seeing much of new things here that
callbacks are problematic in the sense
that we need a boilerplate code and we
net abstraction their heart premiums
that broken home beds are hard to reason
about arrow to learn how to hard to
maintain and compose and basically there
are also onerous director there was this
report by Adobe explaining then their
application on the production code one
third of the code is about handling
events and how for three bucks reported
during the lifecycle of their products
is basically located in this code
take-home message from this slide August
two three last slides on events and
callbacks are a workaround we can use
them but not as a substitute for real
proper support for reactive computations
over time changing values what about
data and event correlations well there
are tons of the kinds of technology out
there different silos that do
correlation in different ways so we have
complex event processing systems stream
processing stew systems we have systems
of the big data processing vector
programming and hidden concurrent
programming languages are somehow about
correlating events or have some support
the thing here is that we have a reach
or high feature and semantic variability
if anything goes to find joins but they
have different semantics and the
question is is unclear how these
features how these variables semantics
are composed and interact across domains
so I think this is too much complexity
for the core program in the middle that
might
a mixture of these different features
and I'm claiming that we are in high
need for unifying generalizing making
those teachers composable and to explain
what I mean by this I will use this
analogy which unfortunately is not by me
I borrowed it from yen store Stoica
basically says well we started here with
the mobile phones it's big language
basically but people realize this mobile
communication technology was interesting
for a lot of things and what we saw in
the next step we're a lot of specialized
devices that did one thing with the
technology which eventually finally
ended up with our smart phones which
provide a unified platform on top of
which we can plug all these different
specialized functionalities so in the
rest of my talk I will basically
represent the what we are doing in a
children's on with 9 team to address
these problems and I will do so in two
parts so in the first part I will check
this what I call the pragmatic path
where we took the pragmatic path which
basically corresponds to the language we
are developing risk Allah as it is today
or as it's almost today this is an
embedded language in scala is the main
almost ethic so by pragmatics I mean
with Skype start with existing concepts
for yet values and event halls and
things that combine those and improved
and bring them to this ugly world of
distributed concurrent systems that are
not are reliable then in the second part
of the talk which is the shorter part of
the talk I will start moving at more
systematic paths towards what I call
unified unified foundations here we use
the world back to essentials and add
built in support or extensibility
variability customizability and this is
kind of how we initially Scout very
scholar to be in the future okay first
part the pragmatic part signals and
events in risk are so signals risk color
teachers these two concepts of VARs and
signals VARs are variables they are
empirically imperatively set input while
signals are basically these time
changing values and from well known from
some flew program reactive programming
languages they are values the change
over time they are composed from VARs
and other signals and for the
composition you can use any
general-purpose computation expressions
including combinators higher-level
combinators like fold map zip and so on
so basically the code that I showed you
the three line is basically called in.we
Scala so here we have to be a variable
that is changed in an imperative way why
the less average a and we are defining
signals that depend on G in which case
or they can depend on each other so I
can also depend daily is depending on
out the number of hours a week depending
birthdays the thing is here every time T
changes the changes are propagated
automatic excluded of the graph
consistently and this is done by the one
time so you don't see any distinct
management in the code what I mean by
done by the runtime is the following
let's look at this code what does it do
it defines a bar X and then values that
depend on X decrement of X then is even
if
when X is even and false otherwise um
while the other even is the kind of this
conditional if X is even it returns
sorry is even returns to when X is even
well this even value is basically either
X or the decrement of X depending on the
value of this even and then the last
expression just prints the value of even
which is all this an even number now
this code is basically compiled into a
dependency data dependency graph where
all the variables and computation units
are connected by edges based on their
data dependencies now let's assume we
change an X on the second to three this
means that this change will be
propagated automatically from the X node
to all it followers in the graph and
further on to println and the thing is
that this update is done consistently to
ensure that it's called in functional
reactive programming languages glitch
freedom and clear astraying what I mean
is look at this even node sooner than
the change on X lives along this edge
immediately and even propagates really
evaluates you need to see after seeing
the change of X and propagates its own
change to println which in this case
would cause that we print 3 which is not
an even number V invariant here is only
even numbers are printed um that's why
even only evaluates reevaluate itself
updates itself when all three or its
predecessors that are affected by the
change have already evaluated so if the
guarantee is that each node is evaluated
once per update time and the other thing
that this example illustrates is
that the craft can change dynamically so
X becomes three decrement becomes 2 X is
not even in the morsel now establish
that if then else statement will
establish a dependency between Jack and
debug previously we have the dependency
culture and X 2 even so far so good
in risk Allah we also have support for
events the simple code are not so simple
with the complex control flow that we
saw was actually written unit race color
so you can declare events in the
interface of some module you can compose
the events you can filter them basically
with the Combinator's that are available
in functional reactive language is the
thing that I want to emphasize here that
both events and reactives are integrated
into all so they are subject to
inheritance and some type for democracy
and the other more important thing is
that we have made them and composable
basically they are both special cases of
a higher level concept called reactives
and they are made composable
interchangeable on by operators for
moving between the two in changing lives
on even an event we can create a signal
out of it by taking the light where
latest by applying the latest operator
to the event which basically creates a
signal with the latest values exposed by
an event the other way around
even a signal we create an event stream
which basically triggers events every
time the signal changes and there are
more operators like this one a snapshot
here there is an event and a signal
declared clicked and position and then I
create a new signal last click which
Volken basically is kind of a subset of
the values in the position signal only
those values are taken when we have an
event click triggered if I didn't have
this higher-level operators for
composing those two things I will have
to resort back to using callbacks and
get again this in interwoven and arrows
control errors to sunrise so we have
both a vengeance in Scylla and we made
them composable that's all I've been
plugging in so far and they are kind of
living area so we can even a signal we
could recreate an event stream given an
event stream we can create a single out
of it but they have different flavors in
terms of what they support so um we've
seen us only the change signal is
propagated all dependent computations
need to be a long way from scratch so if
we have let's say collections of objects
and only one was edit or removed then
this is propagated as the collection
changed but not in a fine-grained way
individual elements change on the other
side but here the thing is that the
change is propagated automatically and
computations reevaluate themselves
without as triggering this revaluation
in the application part on the other
side we have events on which notify a
change about a change but also material
the Delta of the chain so we can use
events in a more fine-grained way to
signal changes but we have to explicitly
encode in the program the incremental
update what it means to get some other
values up today and this could for
example be used to call for missing here
there's incremental computation but
remember that what I told you was events
we need them they exist in the world but
we shouldn't use them as a workaround
for time changing dollars so in risk
honor we also have support for
incremental collections for example I
will not go into more details about this
all right so far so good with what you
see with when you read papers about meta
programming you see claims like improved
application design quality easy to
compose abstractions relative style due
to this automatic state management and
less error-prone improves program
comprehension revising authors of some
other reactive functional reactive
programming system also are not sure
about this last claim assuming that it's
not easier for the first reader so we
thought we should evaluate this
empirical keep in mind this is in a
university academic setting so there is
so much we can do about empirical
evaluations but at least we did it we
did two empirical studies the first one
was on this report research question do
composable events and signal
abstractions improve indeed application
design what we did was to take to
implement several case studies and two
versions one using our events and
comments on the other one using pre
scanner with its advanced signals
composable Wow on five thousand
not so much but that's why it says
preliminary and integral and turned out
that the design is better because better
by the measure of God s convex and we
can easier come compose extraction the
other study that we did was a user study
uses our students um so we took ten
applications implement it in with our p
with risk honor or with trouble and the
observer pattern we split the students
in two groups each group twenty students
and then we give them tasks
understanding the about understanding
things in the programs and found out
that our peer group the answers there
were significantly never correct when
the or group and basically they were not
slower so in half of the task they were
significantly faster and the other half
have not significative slower what I
have to say here is that we all would
God are all training from the first
semester in the computer science 101
they learned about Java and all the
years after this while we introduced
them to our p and risk Allah only in two
lectures and nevertheless so this kind
of shows that the results are not bad
are significant actually so art with
this encouraging results we thought we
should bring these concepts to these
more complex distributed concurrent and
10-30 world out there and there are
because basically we find callbacks
everywhere in those systems and the
hypothesis was that distributed reactive
would reduce complexity
in those systems but of course the our
key challenges to take there and I will
briefly talk about this challenges we
need a decent
propagation control I will say both I
mean concurrency control and fault
tolerance for those systems and to see
what I mean with decentralized
covariation control propagation update
propagation let heaven let me summarize
the main components of a reactive system
of risk Allah so what we have is on one
side we have the user-defined
computations on the other side on the
right hand side is the reactive
programming lantern and users enter
change input values it run again in an
interactive way we have the empirical
world over here and the reactive world
down and they also define signal
computations as we saw the runtime
consists of this dependency graph and
the propagation algorithm that kind of
consciously nodes in the graph to be
evaluate while revaluations going on
these logs can invoke functionality
defined as signals and signals can have
side effects
back to the imperative goal and in
variables and also read we have
operations for reading the value of the
signals that are super good when we go
to the distributed world of course these
things get to soon have only two hosts a
and B then you have the world up there
be split it in two pieces the signals
will live in different hosts and also
the graph will live in different hosts
and their movies dependent the edges
that cross
hosts Kinder's so we need the
propagation algorithm nevertheless
although things are distributed to
globally and take care of this
consistency property of the glitch
freedom that the updates happen
consistently even if
in this distributive world logically
this propagation algorithm is shown is
is a global thing it means logically to
ensure the property globally the problem
with the existence of negation Heinekens
there are mainly two of them two kinds
of one is topological sorting so as I
said we need to ensure that each node
only re-evaluates
when it inputs have evaluated so
basically we can have a topological sort
of the graph and then we have this
priority queue that controls when each
node can reevaluate the thing is here
every node that is affected by some
update needs to talk to the priority
queue to see whether it can reevaluate
or now and this is obviously not good in
a distributed setting the other version
of the global fluting of ELQ overcome
basically whether some input changed or
not so the green one change aim sense
change or no change signal so the nodes
that didn't change just say I didn't
change to the rest and this is
propagated through all the graph
floating noting and of course this
doesn't scale in a distributed setting
again because there is so many messages
that are sent around for nothing this
all this yellow can't even change do not
need to be affected by the update
nevertheless they are affected and there
are messages going through them on
riscuta and the doctor the decentralized
propagation i will not go into the
details it because i want to have time
for questions so perhaps nice to explain
some more details if you want afterwards
but basically the idea is that each node
in the graph is enable to autonomously
decide whether and when to reevaluate it
only needs to know about the neighbors
and the propagation algorithm is
adjusted to propagate chair knowledge
about what changed which nodes were
changed
when these two pieces of knowledge the
neighbors and the kind of the set of
input nodes that are behind the scope
that came from which I can reach a
certain node and are enough for deciding
autonomously okay so we fertilize this
algorithm proved it correct in terms of
glitch freedom termination completeness
we compared it its complexity with the
two other algorithms and also
empirically showed that indeed it's
better in terms of completion time and
in terms of the number of messages
exchanged next concurrent updates and
here to update visualizes yellow and
creeks and they run concurrently you
know how to credit environment and even
if each of them might be consistent
whether one can currently will get
inconsistencies due to data races at
shared nodes may happen so we want to
isolate them to keep this property even
in a multi-threaded environment what we
do is basically we want to ensure
serializability all these updates turns
which in terms of the other eight many
minutes will contain this ESP in
transactions on the data affected by
transactions out these signals and event
and then there are some populations that
we need to kind of serialize um the good
thing and we want to do this to avoid
abort and to avoid rollbacks
so when concurrency control hidden in
the runtime so that the application
remains declarative and composable and
it doesn't have to care about
concurrency control and the good news is
we were able to do so thanks to some
specific features of the reactive
paradigm the thing that the reactive
radical manages the
so flow of the application gives the
runtime enough knowledge about the
transaction behavior it will think of
propagation algorithm as being the
transaction profile that's why even when
change has happened that otherwise would
cause the transactions to abort our
propagation algorithm then the
integrated sensibility part in it can
change things such that the world is
brought again in a consistent scale okay
um since I was signaling that yeah the
next thing we are doing now is looking
at fault tolerance because disability
systems say anything you hear are not
reliable for college it can change
clients joining and leaving messages can
get lost and clients or nodes can crash
and will really lose stake so very
briefly kind of high-level what are the
aspects that we are looking at in this
space first this is also related to the
distributed part so we are looking at
defining constructs primitives to talk
about remote nodes in a heterogeneous
multi-tier application so we are working
towards a clear less reactive
programming to make miss calculus
reactive programming language and to
have things like place this or signal on
this node and do so in a type safe way
without being able to give you much
details the other thing is how to do the
placement automated question mark we
don't know whether this so far it's a
manual but the thing is that reactive um
where time again gives us some leeway
because if I ensure that side effects
happen only at inputs and outputs
and the middle part of the graph is
side-effect free when I have a lot of
freedom to move those nodes as I wish
between the holes to minimize for
example Network hops or to distribute
load or to replicate computations for
part of this graph is not replicated on
this this host one thing we are doing is
to default handler existing reactive
languages do not support fold handling
at the programming extract API level um
so we can handle folds in the graph
itself on signals and events have now in
their API methods to recover from
exceptions or we let them go to the
output nodes and be observed either
handles at the absorber nodes or throw
to the color and we had basically
integrated exceptions in a very natural
way in the propagation algorithm they
probably the same way as values
propagate through the graph and this can
be also used for propagating your values
delays values and so on and so forth the
last piece is kind of looking at
snapshots and recovery and the only
message to take home here I mean this
client may crash and we want to store
all the states and restore it well but
when we move for example the
complication somewhere else or the
client or the host comes back the
message to take home here is again due
to knowledge about the way computations
know through this graph we can manage to
snapshot a minimal amount of data so to
conclude those this part the whole take
our message so far is you can see it
what I talked so far from two
perspectives one perspective is risk
Allen bring those declarative concepts
of our FRP functional reticle
the signals and events to the imperative
concur anticipated world the other way
to look at what I talked about is if you
model your computations in a reactive
way for example whether it language is
this kind of this gives you
composability for free even even
distributed multi-threaded environment
second part of the talk the principle
path towards what I call the unify unify
foundation you remember this picture
that I showed and I said we need to
unify things if we look at reactive
programming because this is what I am
talking about super and consider
correlations there basically we have
very good support for doing things like
filtering events in an event stream
grouping there by some criteria zipping
two streets together counting and life
but this world fails short in doing
other kinds of correlations things like
timing or partial order patterns
negation or trust referencing and there
is another domain that excels at this
kind of patterns this is the complex
event processing world where you can
express things like report the five most
profitable uber routes in New York City
within the last 24 hours
it's not easy to express the same
functionality RP the program here is
Solaris is not as well
and the challenge is semantic
variability what I mean by this is
illustrated by by this colorful pictures
here so assuming we have this
correlation pattern and these are the
observations this is the event stream
degrees and now we need to correlate
according to this pattern and the
measures are shown here this is the list
of all pairs that match this pattern now
the thing is which one do i select i
select first received according to this
criteria and then the result would be
this to legend or I can take the most
recent and then this will be the result
both are may be valid especially in
particular contexts the thing is that
it's CP engines forced me to into a
fixed semantics for the correlation and
that's why we started working on this
core language it's really a colon which
is more like a foreign language read
knowledge called coral and here we
express declarative language embedded
event correlations as comprehensions
what a surprise and here's how it looks
like I mean don't take this syntax for
for the given we are changing it this is
just to illustrate what do we have here
we have a correlates expression and
basically we correlate events coming
from these 3 streams shop basket tables
and authorizations to derive a stream of
purchase events composed events the
upper part defines three binders
a which bind events from these three
strings the middle part defined
constraints on bound events the first
one says that causally baskets events
should come before pavement events we
need a publication to happen five
minutes after the payment occurred and
there is a I identity constraint here
that says that they all have to have the
same transaction ID and if this
constraint are fulfilled that we produce
we put the events that match the
constraints into a data type and yield
it as a new event what is interesting
about to note here is the two things
first of all this constraint they are
just in this case Scala expression for
their expressions in the language
nothing
domain-specific or external to the
language the more interesting thing is
bad Allen this might look like direct
style pool based semantics which is it
is not it okay so everything is
push-based
it does write whenever they want they
arrive in in an asynchronous way that
means that this comprehension is not
kind of the mlady comprehension style of
functional programming languages this
you can see is a very lambda abstraction
and anyway with any inputs that come in
parallel they can be interred in the
water meter live with each other and we
progress the evaluation at any time when
one of the inputs comes in we can decide
earlier to cancel the selections because
one of the constraints for example is
violated now what about this variability
the semantic variability
talked about let's illustrate this with
an example assume we have these two
events dreams in alb that produce events
over time now the simplest way to
correlate them is to say select one from
me and one for B and produce the pairing
of them which is basically the set of
all these associations everybody every
event is every other is the Cartesian
product which might not be the best for
all cases so how do we can we refine
this correlation semantics in this
second example what I do is for this
correct expression which can be
first-class salt will there two
so-called e-text handlers those are
first-class context objects get put into
the semantics into the correlation
semantics complications as they unfold
and change them modify them in this case
the most research effect handlers make
sure that all the most recent event
pairs are paired together and this is
done is the correlation unfold that
means it's not that we produce all and
then select those that fulfill but
during the control flow the correlation
the second exchange and basically this
is the semantics of really Scala this is
both what risk Allah I always kind of
correlate input reactives all events but
we can have more like here I put all of
it into another effect handler which
basically once an event is paired it's
it's done it's not paired with other
events um at a more abstract level what
we get is basically a simple stream
light through learned an extensible
runtime special no correlation semantics
are libraries the language is scalable
and one can choose between defining
basic correlation semantics as libraries
that come with the language but the
correlation semantics users can also
define no effect handlers to customize
the correlation semantics for their
particular needs um the key ingredients
the building blocks of the language are
a call for a functional language on top
of which we have added an imperative
narrative part for supporting a circle
and timing and binding which comes with
a synchrony and then on top of this
there are these algebraic effect
handlers which go back to blocking they
are kind of a flavor of delimited
continuation pets interpret sign
defecting commands locally we can think
of them as exceptions try catch blocks
but they resume back to the control flow
of the trailer when this I want to
conclude my talk on with some pictures
of people involved in the development of
risk a large coral and beyond those that
are shown here in each gesture number of
students master students that have been
involved in the work so far thank you
and I'm happy to take questions okay
Wiress kala is a language not a library
which of the feature requires you to
actually change the language and can be
expressed in scholar don't change the
language the scholar is basically a
library uh okay there's no changes of
the compiler the embedded language we
use SCADA features that allow one to
embed the language as a library that
looks pretty much integrated into the
scanner itself everything is a library
okay so it's a library with the own DSL
right right okay I mean
okay okay okay okay what's the next
question of what is the core difference
between the arabic scholar and the risk
our well I think the core difference is
in a way I think reactive ax does it
support reactive values natively
it's basically event correlation with
the semantics I showed in the RP domain
um but then when it comes to model the
reaction to those events and event
correlations you basically go back to
observers I would say well here the
values propagate very smoothly through
dependent computations in a very
declarative way so RX doesn't access
support and then when it comes to the
distribution and concurrency I think
risk ala excels compared to reactive X
and I emphasize that we see our main
contribution in taking those constantly
bringing them to the in the pragmatic
part bringing them to the distributed
concurrent world but maybe we can take
that other questions
sure and then we can take offline way
the rest
Hey and you started this project as a
research project and so there was a
problem you wanted to so use effective
um let me go back to the so this project
is basically founded by that's this icon
on the left hand side by the European
Research Council with an e rc advanced
crane and when I sat down to write this
project the post I thought what is the
problem I want to solve and basically
and this was the problem statement
I thought about applications that are
built today what characterizes them and
then looked at the technology we have
and identify the problems and I showed
in the first five minutes this was my
project proposal it confounded and we
have been working since then on this
project</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>