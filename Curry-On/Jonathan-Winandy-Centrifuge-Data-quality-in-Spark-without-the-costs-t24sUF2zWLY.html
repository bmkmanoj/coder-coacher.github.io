<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jonathan Winandy - Centrifuge: Data quality in Spark without the costs | Coder Coacher - Coaching Coders</title><meta content="Jonathan Winandy - Centrifuge: Data quality in Spark without the costs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jonathan Winandy - Centrifuge: Data quality in Spark without the costs</b></h2><h5 class="post__date">2017-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/t24sUF2zWLY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and so we will talk about data quality
and and spark and also ways to have data
quality and check for data quality
without the cost the usual cost of the
dye quality if you want to ask question
you can ask questions during the talk if
it's fit to answer this question why no
I will do it then if it's not we will
have more time to discuss about it later
try to to take a large chunk of time at
the end to answer your question so a bit
about me my name is Jonah Tommy Nadi I'm
a functional programmer like I spend
most my my years in Scala but I also do
closure as well and my day job is mainly
to do data engineering in France where I
coach teams around italics so I'll help
a lot of teams in France to build better
data platform in in large corporation
I'll so go for it and followed a couple
of companies around data and predict
Whitman maintenance else care solutions
and currently P I and so business
intelligence and big data so we will
first talk a bit about that equality and
and do a short introduction to spark
very short and then present two
solutions to improve a lot the the data
quality in pipelines and also improve a
lot the lead time of operation to run
pipeline so how to have the same day
delivery I mean like can i push the
prediction right away after coding my my
new feature so why data quality it's the
thing is its subject is a bit like not
common because it's highly related to
the data sources we collect so
the body as that equality problems but
they are always different so it depends
of the kind of businesses you are in the
field you are and and the kind of tool
you are using and it's not a very common
discussed topic because it's usually
perceived as some kind of thing that is
nice to have but on quite the contrary
it's not very nice to have its monetary
because it slows down the project so
much that some project may fail if they
not they are not incorporating that
equality at rest so it's a major concern
nowadays in big data so the thing with
big data is thanks to the
democratization of distributed solution
we are now able to to store a large
quantity of data in the data legs so we
can differ a bit the data quality
concerns in the future so we don't need
to solve them now but we still need to
have some strategy because the problem
is growing so you don't need to solve it
right away but you still need to solve
it to be able to provide data services
in your organization so there are two
kind of like qualities the check and
compliance and the anonymous anomaly
detection I will not talk about animal
detection because it's highly
domain-specific and we mostly talk about
check and complain compliance but once
you implement the check and compliance
part you can run better anomaly
detection because you have more features
to detect on so the impact of bad data
quality is obviously there is a lot of
impact in in machine learning because
when you learn on on average data and
you run on bad data you make bad
prediction but also it's it's bad for
any kind of business intelligence
activities such as reporting and it slow
down the project so some project might
be stuck
into this QA phase where you need to
assess the quality of the project but
you cannot run the product in production
because the numbers are not right and
and you can be stuck in this phase for
weeks or months like the god is done but
the data produced is not okay so we are
not able to deliver any this effects a
lot the team morale because you did your
job but because of something that
doesn't concern you like directly you
are not able to ship and you are not
able to understand properly what's going
on and you don't have any kind of
estimation when the data quality would
be good enough to relate the project so
it has a huge impact on on on the team
morale also I believe in same-day
delivery
that's my closure side like when you
actually change your program in memory
and and sorry so when you change your
program in memory and and you deploy new
feature on running instances I really
think that in 95 percent of the job we
do we could start something in the
morning and ship it in the evening and
it's a bit challenging and a bit
counterintuitive and running against a
lot of the last-known best practice but
this is something we can do thanks to
distributed system and the fact that we
can manage failure more easily than
before so this is doable and but the
thing is that data quality better that
gets in the way like you release
something but well it's it's not good
and it doesn't depend of what you do
with depend of something else so so
there are a couple of anti-pattern that
that can cripple you in this kind of
undertaking and one very common task in
that equity is
to check the aggregate of being in the
like tens to maybe a non-word of meeting
where we are talking about the fact that
the aggregates are okay or not so okay
and then we start a detective work and
this lead to to waste of resources
because there is no way to check
anything based on aggregates well even
if it would be the same something could
underlying could be changing and have a
huge impact on some other metrics so any
kind of a locality effort should be row
based and based on the smallest quantity
of data because it's it's what will be
done if we are not able to find the root
cause of any of the problem so instead
of trying to shortcut on the data
quality effort by checking aggregates
and so on and then maybe you have some
silent data bugs which will always check
for the conformance of rows and and it's
it's better to do it like fully right
away it's may be a lot of costs up front
but then it pays off easily the other
issue with data is this kind of topology
so this is common in data pipelines and
this is something that that not a lot of
people think this is an issue because
they don't say it but if there are some
topology where the data has two ways to
to to move in the pipeline so there are
two paths leading to the end result on
certain data sets and this is something
that can be easily avoided and and this
is something that that that decreases
the performance of the pipeline is
usually related to the to the use of
sequel because sequel encourage is kind
of topology of jobs and and also it
doesn't allow to centralize the checks
of data because like your data is
divided into into several paths
and depending of the past you don't have
the same rules concerning good and valid
data and then it creates a lot of
problems so this is one one trick
actually I do a lot of audits of data
platforms and usually I go I go look for
that like the paths that are not trees
like that are real DAGs a direct
accessing a cyclic graph and when there
is to pass between the result and and
some of the sources I know there is a
problem there it's very easy to check
like there is always an issue in this
kind of topology and usually you can
like make it pop and then you have like
some some form of 10 X 2 20x of
performance in the pipeline so if you
want to do data audit this is a very
good like pattern to to look for so base
of those of those issue I I created
centrifuge
so it's an about this project and the
idea is to generalize a couple of
techniques I've been using the the past
few years and the main idea is to
provide performance rule-based analytics
to allow to check volumes of data and as
well to tag the data with with data
quality information so let's talk a bit
about SPARC
so SPARC is the generic cluster
computing farm arc and what's great
about SPARC is you have a like coherent
set of libraries that can work together
to to build pipelines you running
different kind of programming model so
you can start your pipeline with generic
distribute computation then switch to
sequel then switch to graph and go back
to the generic disability computation so
you are always using the best tool for
the job in SPARC like if you think about
it that way because you could do
everything in sequel and then I try to
push some of the computation in sequel
but what's good about it is you have the
choice
and and it is a very active community so
thanks to the SPARC architecture and the
key component in SPARC is a driver to
the fact that you you are not your
program is not run by something it
actually asks the master to run
computation so you have real program for
example you can um you can integrate the
driver in reporting application you can
integrate the driver in non running jobs
and so on you don't need to you are not
rent into spark like you would do in Pig
or I've classical MapReduce actually you
control the cluster and you can run
dynamic dynamic workload as well as
tuning the execution of your program
after like collecting metrics and so on
so you have a lot of control and you are
not locked to one of Elif abstraction
and we see why this is important because
SPARC
actually all those different ways to
tackle the same problems and we will see
that with the two that I've IT solutions
so there is two main favors for
distributed computation there is the led
which is the stoical one two relevant if
you want to do for example complex event
processing like it's very good for that
or imperative programming on your data
and there is a sequel and the sequel
like part of SPARC allows you to run
very efficient job because you define
your job using sequel like expressions
and then they are optimized to to run on
on the cluster so it's you manipulated
some sort of intermediate language and
then it's rent so it's very efficient
but it's gone while you're going to do
whatever you want so the idea led IP is
still is still useful
so let's introduced two ways verifying
data quality the first way is to check
volumes of data
so the problem is the following is like
I want to update my problem my program
that from some data but then I want to
make sure that I did not break anything
so the classical way to do that is if
through a unit test but the thing with
unit test you are only checking or I
confirm like some part of the behaviors
but you don't check the behaviors for
real data and especially what happened
on the real datasets and the proportion
that are out
I for example if you start to reject row
did you reject too many rows or not and
so on and so on the second the second
way to to integrate data quality is to
use annotations so with annotations you
can tag elements of data with data
quality information so let let's check
for the first one so the the idea is
very simple is you have a function f
that you map a value data and you
provide a new version of this function
and you want to make sure that this
function behaves correctly and it's it's
not easy to do that and I think it might
be possible to compare two functions but
this is not the kind of problem I know
how to solve so maybe some people know
how to solve this kind of problem but
what the thing we can do is we can try
to understand the behaviors of F with a
sample of data with sample of data of
all the data that you have depending off
your computing capabilities at the end
so if you ran the two functions and then
you compare the result you may
understand something about the
difference between F and F prime but
that's not very tractable anyway so the
the idea of data Delta QA is to actually
run the two function side-by-side and
then to compare the results but it's not
practical in real life because we are
not able to
replay the existing functions once you
deploy a function is very difficult to
keep it Ren able because your
environment will change your you may not
manage your binaries properly it
requires a lot of infrastructure to be
able to run all version of the same
program so that time transition might
cost a lot and might only be useful for
this kind of experiment so what can we
do and something that that some teams do
is to actually store the source data
with the result of the transformation
and it's doable
thanks to columnar formats it's highly
efficient and it doesn't decrease on
query performance and by doing that we
are able to compare the result of F
prime next to the result of F the thing
is it's only the technical concern that
I'll stall here because we still need to
define a function that can compare our
and our prime and to make this
comparison in a way that can be
aggregate after so you could solve it
like that but it then requires like
trained engineers to be able to define
those comparison function and
aggregation function it's not that easy
so the way to solve that and to and to
confirm two real-life problems is
actually to project the data set into
episode a ring so absurd a ring in just
a structure that define a couple of
operations like plus minus zero and x
and then if you do that you are able to
compare with your result so you don't
compare the result of the data but you
compare protection of it given that
comparing the projection of it will give
you enough information of the evolution
of the
data so this is one way to solve it and
also we use key and joints to be able to
do it on real data sets because we are
the descendants released all source data
only a couple of the data sets are using
this kind of technique so so how does it
work let's take a very simple table so
it has like four fields so the ID the
name the age and the phone numbers which
is a an array of phone numbers and then
we project this structure into a
structure that that as these soldering
properties so the ID the age and the
number of phone numbers the person have
okay based on that we can compare the
sorry the produce and pass results so at
the left you have a certain data set
with the same structure at the right you
have the the produced previous version
of it after a transformation and then
you are able to to to join those two
data set based on key and on each key
you're able she generates value
depending of the nature of the drain for
example if you have only one value on
the left you will put the value in
certain columns if you have one value on
the only values on the right you put the
values in certain some other columns and
so on and so on and when you have both
values you can compare them so what
happened when you compare them is you
are adult you have the difference for
example under line one there is the
sorry so the number of phone is
different so you have one more phone
number the age is the same and the toy
so the phone is different it's it's over
there so it's here with the Delta phone
and once you map all those twins results
you eyeball to
sum them up column by column and you get
an aggregate view of what happened with
your data okay
so it started at a Z as an
implementation in Scala only so there is
a easy I would say simple way to do it
but not easy way to do it in skies to
use a shapeless spire and and spark
added ease and then it's better to use
the data from version because it's more
performance and it's more co-worker
friendly so the the spire vation over
there it's it's searching lines long and
it's very short and it's relying heavily
on type classes so it's not very funny
but you get everything out of it it's
like you do the join and then for every
type of drawing of drunk results like
you have the result on the left only the
result or the right only on both you are
applying different different operations
what okay
please IntelliJ what are you doing okay
thank you
so you apply different like aggregation
and and then you compare when you have
two values and you are able to generate
this large role and to reduce it to a
single role so using our money we'd like
transformation and so on like in that
everything is defined in spire so it it
works you should work with no setups on
generic structures sense to type class
regeneration I mean derivate Auto
devaluation but it's not working right
now because SPARC is not able to sell
you lies so structure right now so we
are using that on one that like of a
data set that have a couple of inverter
fields and and it takes like five
minutes to to check the difference
between two data sets so it's day of
data it's one load of gigabyte of data
it takes five minutes to check if
anything meaningful changes so based on
on so we implemented so the production
is using like standard KPI is like a
number of clicks number of visits number
of research pair uses kind of like
information like that you you you
actually care about in visit engines so
we use this same definition as the BI
team and we are able to tell them well
with this new version of the program we
are sure to not change anything about
the number of clicks because we checked
on every line for the past few days and
it changed nothing so it's not very
practical to use an LEDs and and type
safety are a great when you know how to
use them and especially if you if if you
know a bit about shapeless and type
classes so I did the data from version
as well so this is the the complete the
the end result of the computation with
that of the data from version you get
for every metric that we define so the
first metric is just one like you put a
one in every row and then you have the
number of rows so you have ten rows on
the left that have not been matched by a
row on the right five row on the right
and one was rule that of that of
corresponding values in in both sides
and then you have this M tag summed
matrix for just left just right both and
then the Delta between them and then the
number of rows that are actually
identical for this the for this metric
for for example that means that over 100
rows 90 rows we're having the age
identical between left and right so with
this kind of a view you can easily find
what's going on with your data you see
that
one of your metric is off and you can
you can say well the valuation is is
okay because it's increasing so let me
that's better
oh it's not okay then you can apply a
business rule to two to every metric but
you are able to find the impact of the
change of your function on every metric
you define and you're able to know how
much how much did you lose
where is that is most of the identical
or not because it's a very good
properties like let's say 99 percent of
the rule are identical but only 1% of
the row are different and they are
generating that much difference and so
on and so on so to use that with data
frame you import the centrifuge yes
well on this version with the data frame
we use only that aside with numerical
numerical attributes for now so to get
the putter ring with the long is easy so
but on the spire version is you have to
make sure that there is a type class
additive abelian group plus semi mono it
of multiply something like that like in
spire on your class but thanks to share
plus it can be also divided so you don't
have to do it yourself
but usually like a case class like a
record with only Long's
is the ease of soldering like you get AB
sort of written definition of it with
every every field so with this version
you import the library and then we with
two data frame with the same structure
you can generate this this table by
doing DF 1 dot Delta we is DF 2 and
that's it your job is running and and
checking with the structure you define
before all of these metrics and I think
we can add more metrics especially
concerning the new values and the
boolean logic ways
nobs so the delicate world map is the
following so I want to add the partial
support of all the types boolean strings
and engines
I mean partial support just good enough
to to be to be useful I mean it won't be
as complete as a support for lungs to
export the results to excel because
Excel is great when you have to have
like shiny colors in the cells and to
and to really like expose the result and
also I think it's possible to build an
online online query generator for other
databases so based on the schema based
on turntable with certain schema we can
generate the sequel queries for so that
databases as well the problem is they do
to the structure of sequel to have a
meaningful wizard at the at the end you
will have to destroy the performance
using a lot of unions so I'm not sure
it's it it's possible but is it very
useful I don't know okay the second
techniques is annotations so for
annotation the kadhi is to use the
standard way to do arrow processing in
functional programming in the direct
data environment
so in functional programming when you
want to manage arrows you have the
validations or either monads and you
have the type result that is for example
the either of arrow and and T or the
validation L&amp;amp;T in scala so when you want
to pass a row of data and you want to
gather the the error and not to use
exception file support on the GBM you
will use the the result line type and it
define monadic operation as well as
applicative operation the problem in
data processing is like we have non
working errors so the types
using functional programming are not
enough so we need to add warnings so we
get a type that is much more complicated
so we either between a value T or T we
use a couple of warnings or an error
with a couple of warnings until we have
these three alternatives the thing is
like in Big Data that they are actually
close to zero tools that support some
types
acha products so we need to encode that
into a product type so this is the way
to encode that is to have the value as
at the optional of T and then to have
the sequence of annotation and the
annotation to have a flag for four
arrows so this is one way to encode that
and then you have the the applicative
operation on it
so you can use it to to create your
structures and so on and so on if I
forget about your thing so we know the
ball applicative in the room
Piketty functors so not much it is very
useful it's key to to to understand
fully like type functional programming
but it's very foreign so it's not
something you would go right away so
what we did is we define a macro to win
place the applicative to and instead use
generated functions so for example if
you want to construct a person with
values that are encapsulated in this
validation scheme you can use the build
function and pass the details and then
you get a person and all the body of the
of this function is generated by by the
compiler so you can build your your your
person nicely we
Lda applicative syntax it's a regular
syntax adjust your values are completed
in in structures that capsulate the
validation okay
so what happened is once you do
everything like the annotations are
rising to the root of the structure so
you able to accumulate them at the at
the at the top so if we take the first
line we have John of age five and the
age like we see that the past is age so
it generated by the macro to to capture
the the pass of of the data for the type
name int is out of range so the range
might be like search into 100 110
something like that and then you see
that for this world
the age is invalid so clearly out of
range like it's it's negative age and
also the name is empty which should not
happen so you get for the same role a
list of things that that are wrong or
that could be wrong
and thanks to columnar format again we
are able to store that and without
impacting query performance because when
you query your data normally you would
not read these values so they are not
they don't get in the way but when you
want to analyze the quality of your data
you can do it and you don't need to do
joints which actually slash down all the
costs related to data quality because
usually we do data quality after and we
need to rejoin the data we still data
and so on it is not the case you have
the tags on the same row as at the other
values
so this video is satisfying it's slow
but that's okay we are we are running in
in your in your in your main ETL
pipeline again it's not coworker
friendly I think it's a great way to to
actually talk about shapeless and
inspire and Mashka and so on it's great
but well you need to do a step towards
your coworker and make tools that that
they can use and also you cannot track
the provenance of values for example you
know that you have an impact on the
output field H but you don't know I mean
where this value is coming from and and
that can be like how to track you need
to go back to the source code to see
well the field age is construct with
this data and so on so I bought it
recently the annotation to data frame so
we don't have to use any kind of the
applicative sandbox and so on so you use
like normal sequence sequel and it
increase also a lot of performance so
let's let's take an example program you
would start to your SPARC program
normally with this partition you will
define your your operation
transformations like to age that takes
an int and an output a result of int and
you see that on certain values like when
it's okay we we just put the value but
when it's for example below zero and
other under certain or a certain value
we also add annotations and then we just
tells it transformation so thank you to
the framework of centrifuge and we can
run the query as if we were doing normal
sequel and asked to include the
annotation so you could when you your
your sequel query as if you don't care
about that equality and it will Chan
nothing about what you did but you
should care about it I want to solve
that equalities on every roll you can
increase the annotation and
automatically that will collect all the
annotation values to an end the
annotation column so the result is this
one so for example like you get the age
12 it's under 13 you get the edge minus
one and it's below zero so if it's below
zero it's an error so it could clear the
data it's it's set to nil like it's
better to have noodle than minus one so
for free
by by defining new transformation with a
data quality component you can you are
able to collect all of it after query
definition and even through aggregation
or whole food joints yes I'm finished
actually it's it's last two slide okay
okay thank you so then the key listen
fall for the varieties like those tools
we have written first in into these type
safe forms with shapeless with
applicative phone calls and so on but
then you can revert them to to Spock
sickle and and this is something great
about particle is it's truly extensible
like it's not something that the the
guys that erect said like sparks people
will be extensible it is like we are
able to extend it
we are bolts you to pass the query to
reinterpret them to to be to be base on
on the optimizer and so on to extend it
in a way that we can like enlighted
equality and have performance
measurement in lining and also it's
easier compared to scalar macros so for
those of you that does color and try to
dive a bit into the macro it's not that
hard compared to you to square macros
and that's it so do you have any
questions
no question you have one yes okay okay
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>