<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Daniel King - Building Tools and Languages for Terabyte Scale Biology: A Call to Action | Coder Coacher - Coaching Coders</title><meta content="Daniel King - Building Tools and Languages for Terabyte Scale Biology: A Call to Action - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Daniel King - Building Tools and Languages for Terabyte Scale Biology: A Call to Action</b></h2><h5 class="post__date">2017-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/B5l5H0LlBww" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right now but the first dog and also the
third one this morning session will be
slightly special because we call it the
chest time talks the idea is that the
speaker has 20 minutes and the audience
has 20 minutes to build up the pressure
here is the clock you do have to ask
questions you can only ask questions
after the first five minutes okay so
that the speaker can introduce himself
and so on so like asking a question what
is your dog gonna be about does not
count okay so whenever he escapes over
details as something interesting you
want to elaborate even you disagree
please do interact and without further
ado thank you thank you very much hi I'm
Dan King thanks for having me here I'm a
member of the hell team at the Byrd
Institute and they'd like to tell you
about terabyte scale biology and our
experience is building tools and
languages for that so let's break down
terabyte scale biology what is biology
well in our context we focus a lot on
genomics but we try to build tools that
are more broadly applicable so what is
genomics it starts with the genome
there's 23 pairs of chromosomes in each
of us one for my mom one from our dad
and these comprise sequence of 3 billion
bases meaning a is T's GS and C's or a
two bit number if you prefer that
within this genome there's 20,000 genes
and we each have our own unique genome
and it's shared by all the cells in our
body
however the genome does not stand alone
it stands at the beginning the essential
dogma of biology holds that DNA is
transcribed into RNA and then the RNA is
translated into proteins the environment
of each cell in these different levels
of RNA
during the transcription process even
though the cells all share the same DNA
and this results in different protein
levels in the cell proteins are the
fundamental molecular machines that run
the low-level processes of biology it's
on top of these
cease alive many ladies you've
increasingly abstract biological systems
and finally sitting atop all of this is
your ability to read the words on the
slide unfortunately the system sometimes
works against us it causes human disease
and lucky for you it's the mission of
the Byrd Institute and my primary job to
leverage an understanding of genomics to
treat human disease to do that we start
like any good scientists by running
experiments taking bedrooms all right
in particular we measure the source code
of life the DNA so the modern way to
probe this sequence of life is called
whole genome sequencing it's a
systematic way to measure every single
base of the genome we start by obtaining
or making several copies of the genome
under study we tear those copies into
millions of short little pieces we
measure all the pieces and load them
into a computer and then essentially we
compute edit distances from a reference
genome and choose from where on the
reference that piece most likely came
and we end up with this alignment here
and finally we take all these data and
estimate which two letters are present
at every position on every pair of
chromosomes in addition to a point
estimate we also fully record the
probability distribution that this
process produced that's maintaining a
high-volatility view of the stochastic
process finally we take pairs of letters
from the pairs of chromosomes and throw
the whole thing in a matrix humans on
one axis genomes on the other and this
is where the computation would begin
unfortunately as I said earlier this is
just the beginning of the stack we also
want to study the RNA understanding
disease with just the DNA is kind of
like reading assembly language so each
gene is transcribed into different
levels of RNA but we don't really
understand why different cells produce
different amounts of RNA so we can go
and measure all of that information try
and understand why different cells
produce different amounts of RNA and the
data type that we end up with again is a
matrix of gene
by cells we can also measure the RNA at
many points in time producing a
three-dimensional tensor of data so now
we have gene cell and time and we can do
a lot more we can actually take a
picture of the cell before we blow it
apart and measure the RNA and include
these pictures a part of the entries in
the tensor unfortunately I don't have
any pictures of human cells on hand but
this here is a photo of a tree so
clockwise from the top right there's the
endoplasmic reticulum actin filaments
chlorophyll endosomes and finally the
mitochondria each one was caused to
fluoresce in some ways that we could
take a picture of it and as an aside the
flowers of this tree are really pretty
this tree is called la ohia Lihua and
it's a native of hawaii
besides imaging we can also perturb the
cells by introducing a drug or knocking
out a gene and this produces some kind
of four-dimensional tesseract with
fourth dimension being the drug that we
administer to the gene that we disabled
and the point of going through all this
is to give you the sense that biology is
filled with data that's rich with many
axes and high dimension and much like
Saint Paul's Cathedral biological data
is impressive not just for its details
but also for its immense scale how did
biology get to that scale well this plot
shows the cumulative number of sequence
human genomes on log scale against time
on a linear scale this sustains
exponential growth that we see here is
caused by improving efficiency in
sequencing and investment in sequencing
the red dashed line
extrapolates the last seven years of
data growth which is a doubling every 7
months unless my cursor and the blue
dashed line is a Moore's Law a doubling
every 18 months the gap between the red
and blue projections means that
understanding biology is no longer a
problem of biological engineering but
one of software engineering
and to really drive home this point take
a look at this linearly scale plot it's
the predicted genome is a predicted
amount of data storage that we'll need
in 2025 across different fields genomics
will have more than four times as much
data as astronomy and the world store
pet videos combined two of the larger
recent data sets are nomads and the UK
biobank Nomad contains 150,000 human
beings and weighs in at around 40
terabytes compressed and on disk the UK
biobank contains a whopping 500 thousand
human beings weighs in around 12
terabytes of genetic data has very deep
phenotypes with over 8,000 measured
traits per individual including a lot of
medical images which are themselves
really massive and we want to load into
our tensorial data so to tackle these
problems we need to bring to bear the
ideas of statistical genetics the core
of statistical genetics is the
Association study these studies seek to
associate mutations in one gene with a
trade or disease in the individual for
example if we have the genomes of many
people with high in the levels of LDL
cholesterol we could arrange them like
in this slide without any mathematics
you can look at this and see the
mutations in the sonic hedgehog gene
don't seem to correlate very well with
high or low levels of LDL cholesterol in
stark contrast that pcsk9 gene seems to
have an excess of mutations with low
level correlated with low levels of LDL
cholesterol in fact it was an
association study just like this that
encouraged biologists to explore the
pathways associated with pcsk9 and these
indications ultimately led to the
development of a drug
Rapha which targets the pcsk9 protein
this drug represents a fundamentally new
kind of cholesterol drug and statistical
genetics and the computation that powers
the physical genetics played a
foundational role in its discovery we
would never considered looking at this
without statistical genetics
so I want to dig into the computational
aspects of finding associations like
pcsk9 and cholesterol because that's
where I find all the interesting parts
of statistical in genetics are
yeah so uh what was the comment from the
other time I so the question was there
must be a lot of duplication so that's
true at 99% of our DNA is exactly the
same that's why I said 40 terabytes
compressed when you uncompressed it like
so the genome is what did I say 3
billion base pairs long
so I mean take 3 billion and multiply it
by 4 bits that's like 12 billion bits
and then you have a hundred thousand
individuals so the actual like
uncompressed form would be gigantic
luckily we never actually realized that
uncompressed form because there most of
that variation is not interesting to
genetics like what makes our hearts pump
like it's interesting to genetics in the
sense that if you didn't have that you
would die before you were born and you'd
be like stillborn or not even viable
yeah but at our level it's not that
important yeah I have like 150 slides
I like prepared as much information as I
could so yeah dig in as much as you want
I think I have information for a lot of
questions that may come up yeah yeah
sorry
so from that not amount of data how much
of that data is really available for
research because I guess a lot of it is
maybe you know instead of bio banks and
stuff like that so it's hard to say
there are private companies like 23andme
and Craig Venters company that's the guy
that was competing with the human genome
project to do it before them and then
copyright the genome
they yeah they have we don't know how
much data they have um all the data that
I talk about in here like the biobank
and nomads are publicly available
datasets to anybody who is you know in
the realm of statistical genetics or
population genetics in the sense that we
don't want to give away genes for free
like we don't to give genes to the
public because it's way too sensitive if
I gave away your genome no insurer would
ever insure you I mean unless you happen
to have the genome that makes you live
forever then maybe they would yeah it
should like most of the data that I'll
talk about here is public and a lot of
the data available is public and is
collected by national agencies so like
Matt might talked yesterday about the
precision medical precision medicine
initiative and maybe he also mentioned
the million veterans project both of
which are attempts to sequence 1 million
human beings and will be public datasets
used for public research hi these two
datasets which you show a why second one
has more people and take West storage ah
that's a very good question and
unfortunately I don't have a slide for
that one uh so see we're already this
one so the second one the UK biobank
it's what they call whole it's called
the genotyping which is different from
sequencing the entire genome this
process that I described where you take
the genomes and you break them up and
you unwind them that's like a very
detailed process it gets you all like
every single letter on the genome
biobank is using a genotyping array
which measures specific positions on the
genome that we think are interesting and
then it turns out that there's a lot of
correlational structure on the genome so
we don't actually have to measure every
base pair to sort of guess what the rest
of the genome contains but even
including the data like even after we
expand it we we guess how much we guess
a bunch of other
the genome we have a fraction of the
whole genome like I said like there's
all that stuff it makes your heart pump
and that's kind of not including all
that extra information it adds a lot of
data and maybe isn't super duper value
valuable depending on what you're
looking for okay it's a really good
question though
so okay so that what I want to talk
about is like our data representation so
when you're doing statistical genetics
you basically have a matrix where the
columns are people the rows are
locations in the genome or Llosa in the
genetics lingo
the cells are genotypes and you might
think that they're just the pair of
letters that you have at that position
for that person but in reality there's
this like rich structured data so in
addition to the point estimate we have
all these probability distributions
about the process that measured that and
that's really important not just for
quality control and cleanup of the data
but also for kind of getting a finer
resolution microscope on what's causing
the disease and the story doesn't end
with the genomes we also have to have
this metadata about the individuals to
look for the correlations with the
disease status or with other traits so
we have a table that's indexed by the
individual ID and we also have a table
that's indexed by the genetic position
and this contains lots of metadata like
is this position in a gene or is it not
in a gene and if it's in a gene is it
predicted to like prevent the protein
from working correctly that's associated
with the gene or is it totally not a
problem it doesn't matter that you had
that and I think the most interesting
thing about the kind of data that we
work with is that it's not just sequel
you know piles of data in a database
like we also want to do linear algebra
we want to multiply it the transport we
want to multiply the matrix by its
transpose so that we can understand the
correlational structure between
different parts of the genome and
understand how different parts of the
genome are inherited together or odd and
we want to multiply a kind of in the
opposite direction which gives us a
relatedness between individuals between
humans we can use that you know plus a
bunch of more linear algebra on top of
that to actually go to humans relevant
measures of relatedness like
parent-child sibling avuncular pair and
so linear algebra is really key to the
kind of work that we're doing especially
when it comes to computing principal
component analysis this allows us to
actually recover the ancestral
populations of our dataset so this is
the nomad data set they ran a principal
component analysis each one of these
points is a person in that data set so
hunt is 150 thousand points here the
colors correspond to the reported
ancestry so when they sign up to give
their genome away they write down like
oh I'm Asian I you know my parents came
from Finland and you can actually see
that the principal component analysis
recovers the ancestral information yeah
okay
so I think like a really key part of
what we're doing is marrying linear
algebra to sequel and our goal is kind
of go further than that we also want to
bring in Bayesian inference and deep
learning and make them available to the
community of biologists that we work
with and we certainly don't want to
reinvent the wheel we want to really
take advantage of things like tensor
flow and probabilistic programming and
try and bring those into our world and
connect them in a first-class way to our
data yeah so when you're combining these
two worlds that means there's a lot to
know and a lot to learn I guess one of
the things that programmers tend to
think we can do is so long as we know
how to write programs in our favorite
programming language we can pretty much
just like bust our way into whatever
other field and just start like herding
the crap out of it does that work here
like can you genuinely make a
contribution by being a programmer what
do we need to be start to be spending as
much our time learning the fundamentals
of the domain like biology as well as
coding when you ask about having an
impact do you mean
so I guess could someone contribute
positively to the project without having
gone deeply into understanding
biological concepts just by being you
know good enough at the you know the
coding or just approaches a coding
puzzle rather than understanding biology
oh absolutely
like so what one of the things that is
shocking to me is that there doesn't
exist like a distributed sequel plus
linear algebra engine that has nothing
to do with biology it is nothing like
it's just a thing that you want if you
do any kind of science and it as far as
I can tell and maybe somebody here can
tell me I'm wrong that doesn't exist and
basically we've been building it
piecewise because that's what we need to
solve the biological problem so if
somebody could come along and just be
like here's a like super badass
distributed tensorial sequel plus linear
algebra system that's performant like
that would be awesome
so yeah it sounds like you're saying
kind of the state of the art is so kind
of primitive at the moment that there's
plenty of easy wins that maybe later on
we'll need to get like more things in
order but for now you could just live
pitch in yeah for sure I like it seems
like most databases are built for
business cases where you have like one
dimensional data you don't have a matrix
yeah it would be great to see more
databases built for this sort of use
case thank you I think there's a couple
in the back hello um can you go back
like three slides where you had the
previous lab why is the siblings off
axis so much compared to wrists I am
like so glad that you asked that
question
because I actually have a slide for it
okay so maybe I'll start with this last
slide so I took away the axes because I
didn't want to explain the axes on the
slide so okay the first thing to
remember is that when we talk in English
about human relatedness we use actually
two numbers so you have like cousins and
then like first cousins and then second
cousin's and third cousins and in some
sense you can think of your siblings as
like 0-4 cousins but there's this other
thing that we pretty much never talk
about called removals right so you're
like my first cousin once removed my
first cousin twice removed the reason
why we need two numbers to precisely
describe a relationship
is because we're talking about both when
was the most recent common ancestor but
also what is the difference in
generations so like if you if our common
ancestor is my great grandparent but
he's your father
then you're removed a few times from my
generation and that's where that
terminology comes from and in a similar
way to fully describe the different
kinds of relationships you you need
two axes to describe it so this axis on
the the y-axis here is called kinship
and the axis on the bottom here is
called identity by descent zero which is
a complicated term that I'll try and
explain and basically if all we knew is
kinship we wouldn't be able to
distinguish parent-child pairs from
sibling pairs and the reason for that is
that when you're born you get your
mother half of your mother's content
genetic content and half of your
father's genetic content so if we were
to just compute the similarity between
your genome and your mother and your
genome and your father it would be 50%
because like literally that was just
translated through the whole gamete
process now it turns out that in
expectation your siblings should also
share half of your content because ah
you're going to share about 50% of your
father's DNA and 50% of your mother's
DNA with your siblings just because of
the way that the biological process
works and to kind of give you a sense of
how that works like if you have two
parents you can think of a parent as
just one copy of the grandparents genome
and one copy of the other grandparents
genome and then these two people mate
and they produce a child which is
basically a mixture of each of their
genomes so you take a little bit from
one grandparent a little bit from
another grandparent and so as I said
earlier like obviously a hundred percent
of the left genome here at the left
chromosome is shared with part one
because it all came from parent one if
you have two children that are both
produced by this process like in
expectation you would expect that most
of the time they like here on this
bottom bit the left chromosome is shared
exactly the same but the right
chromosome is different on this bit
actually everything is exactly the same
and so like in expectation you expect
this to come out to about one half and
she get this plot I hope that okay cool
yeah there's a lot of like really cool
algorithms in genetics there's another
question yeah hi so sort of related to a
previous topic can you elaborate a bit
more on what what you mean when you say
that you want tensor algebra plus sequel
so you you know it's clear why you would
want linear algebra operations but what
specific kinds of database queries would
you imagine would be most useful to run
as you're doing biological
investigations yeah so I just left with
that so here's like an expression in our
language and I'll walk through like what
it means to execute that expression and
maybe that'll give you a sense of why we
want to do kind of sequel lease tough
and I guess I'll stay at the beginning
the reason the high-level reason is we
want to like I guess I can enter it just
in words a big part of doing genetics is
like slicing your data down to the BART
that's interesting to you and running
your algorithm on that interesting part
so like if you're trying to understand
how Europeans are related to a certain
disease in Europeans you don't want to
look at all of the data you have about
East Asians and South Asians and
Americans and Africans you want to throw
them out
the other thing is that you when doing
quality control you need to remove data
that seems faulty or is caused by a
measurement artifact things like that so
you need to have a way of sort of like
querying the data and asking for
aggregations like how many people are
way outside of the norm what's the mean
what's the standard deviation and if
something looks funky then you have to
drill in and like oh can I fix this or
do I have to throw it out so the sequel
is very much about like pruning down to
the data set you care about and then
making sure that that data set has
high-quality data
this was actually the next part that I
was going to do so maybe we'll just
switch back if that answers your
question okay
yeah so this is an utterance inhale and
we're basically trying to say so
AMR over here refers to admixed
Americans which is basically the
population of individuals living in the
Americas that are the result of actually
Spanish and Portuguese colonization
mixed with local Native Americans and
we're going to count the number of
heterozygotes in that data set and so
what a heterozygote is is somebody who
has a T rather than T T or a a just
means you have two different alleles at
a particular position so we can walk
through this query on some like toy
dataset like who if we have the VDS that
that that word just refers to this data
set then we have this filter samples
expression which filters the data set by
columns based on information in this
metadata table so right here we have to
do a join to get that data into the
right spot so that we can then ask this
question is this column a AB mix Native
American and then if they are we're
going to keep them and if they're not
we're going to remove them and I've
denoted removal is like a dark gray and
then this query genotypes call so this
is all written in Python the query
genotypes is a kind of aggregation
expression so you want to compute a
final result so this says so the little
G's here is referring to all the data
that survived to the previous filtering
expression and then we have this like
filter expression and so this is saying
only keep the data that's that our
heterozygote so that removes a bunch of
cells may be like little runs across the
rows and then finally we just want to
count what data remains after doing that
filtering query so what's all the like
right here so that's that's kind of how
our language works and that's the kind
of sequel queries that people do
after something like this you might want
to do some linear algebra as well where
you treat the missing data as maybe you
mean impute or you said it's a zero or
it kind of depends on the biological
domain oh yeah so sort of as a kind of
zoom out exercise I was thinking that
the it's possible for physicists to go
around the world and measure you know
the size of every grain of sand and that
kind of stuff you can you can gather a
lot of data so it seems like it could be
symptom that this is a developing
science and maybe this huge amount of
data is a temporary phenomenon and after
a while we have some principles and
basically we just look at a little bit
of data there and a little bit data here
and use those principles to to reach
conclusions that this person needs to be
careful about that particular thing
because of this predisposition etc so
what do you think is happening with
those principles is that something that
is is something that is sort of emerging
in a way that you can see or do you just
have you know huge amounts of data for
an unknown amount of time that you're
just looking into this sea of data I
mean we're very much at the beginning of
understanding biology like physics I
think have done a wonderful job of
distilling things into these Beautiful's
equations and stuff like that and the
standard model of particle physics and
like that's beautiful mathematics that
like describes the real world we're way
way way at the beginning in biology like
we have no idea this this central dogma
like we understand that this is what
happens but like the arrow from DNA to
RNA totally unclear and maybe someday
we'll be able to say like this sequence
of letters when it's in this position
influences the nearest gene to like
all the production of the protein but or
double the production of the RNA which
will double the production of protein
but like we have we don't understand
that fully at all right now
we have like some very basic ideas and
then the next step going from RNAs the
protein is like totally crazy because
now you're talking about taking a linear
structure something that's very
comfortable to computer scientists and
you're actually creating a
three-dimensional structure so that can
be okay and then it can like fold itself
in all sorts of crazy ways and so you
know someday we're going to have data
about that and we want to like
understand you know like there's people
that study how proteins fold it's like a
whole nother domain of like totally
crazy science that we don't fully
understand so it's all at the very
beginning there's like so much
computation to be done so much data to
be collected okay so another thing I
wanted to say something about is kind of
like how do you Co develop with
biologists or scientists I think this is
something that a lot of people have
tried and not a lot of people have
succeeded at and I think we've succeeded
pretty well so I think there's like heat
at three key tenants basically you have
to go to these scientific lab meetings
and student science like I in some sense
have slowly become a biologist that's
really important for being able to speak
their language for being able to
actually understand what it is that they
want also you got to distill their needs
into a more general framework because
they're going to come with you like
basically the way that science works is
that some PhD student needs to get his
PhD or her PhD and she writes the
software to get the PhD and then doesn't
maintain it and in the process of
writing the software she wrote a bunch
of parsing code and a bunch of code to
distributed across their particular HPC
cluster and none of that work gets kind
of consolidated into a common place
where it's maintained so by building a
general framework we prevent people from
doing all this stupid grunt work
but also which accelerates science they
can do more science faster and also it
allows it to be more reproducible and
then the other kind of like really
important thing is that we have to
become an engine for science that aligns
our goals with their goals right they
want to publish paper and discover
better science and find these laws and
if my goal is to publish a paper in PLD
I then I'm not going to focus my
energies on helping them do their goals
and we're not going to build the system
that they want and so we've actually had
a lot of success and a lot of science
has actually been done using hell in
fact that Nomad dataset I described the
40 terabyte won the whole quality
control was done using Hale and it would
have been very difficult for them to do
it without it yeah okay so now is when
you guys get to flame me a bunch because
I'm going to complain about all the
tools that I use 17 minutes okay so I
tried to use quick jet
I was really tickled to hear Johnny's is
going to be here because when we needed
to test our expression language I ran
straight to quick check I think it's a
wonderful paper standing 17 this year
which I hope means that it's done
telling me how wrong I am about
everything so we tried to use quick
check to generate random data so we have
like pretty straightforward types
structured sets raised dictionaries and
then base types and those base types
include genetic types like variants and
alleles and other stuff and I wanted to
generate data like values of that type
and then run my expression language and
the compiler and make sure that I'm not
generating bad Java bytecode or whatever
and I was expecting this like beautiful
structure that totally explored the full
generality of biological data and found
all the great bugs in my code and what I
actually got was a structure with a
bunch of empties and I hope you saw it
there was a double in there so needless
to say this wasn't finding bugs in my
code and I think led me to this question
like what what distributions of test
cases tend to find bugs
kind of the those are the test cases
that I care about like why am I not
using the kind of distributions that
find bugs and I made a number of
mistakes some of them are simple I think
some of them are where it's sharing with
other people like if you're running into
not being able to use quick Chek
effectively so just to refresh quickly
on quick check you write down a function
that is a property so here we're saying
the sum function distributes across a
list append so you have yeah and then
you have a generator so Jenna Vey if you
don't know how school it's totally fine
just read Jenna Vey as some type that
takes an integer and a random number and
produces a value of type a so it's like
a function the integer here is supposed
to be a size bound so you're not
supposed to make something any bigger
than the integer and the reason for that
is like you might want to focus on small
test cases or you only have so much time
to run your test so you don't want to
run them forever and then so this is
what a generator looks like this one
generates lists of a's so it takes a
generator for generators of a's and it
creates a list and in particular it
chooses the length uniformly from zero
to the amount of size so I kept
generating these huge arrays of numbers
and like these are not very interesting
it's just like a really long array of
numbers in the same way I was generating
huge arrays of fields and to understand
why generating really long array is not
great I felt that if I had an array of
size 10 right so I'm saying my generator
make things of size 10 so it says okay
here's an array of size 10 I filled it
with elements also of size 10 that's
really a size 100 thing right there's 10
things of size 10 so I wanted to like
split my size across all of the elements
unfortunately I was using this stick
breaking approach which I won't describe
but it basically ended up putting all
the weight on like one or two elements
so I got a bunch of
jeez and then I got double so the sort
of lesson that I learned I think is that
the default length of a random sequence
should probably not be chosen uniformly
but that's the way it is in quick check
both in Haskell and in Scala one
distribution that I found that worked
pretty well is a mixture of beta
binomials so these are just their their
finite distributions they have finite
support and they're discrete so you can
use them to generate numbers between 0
and n I like a mixture of the red and
the blue I think that makes pretty
interesting over a sizes another problem
was this whole partitioning thing so I
don't have a size 100 thing I want a
size 10 thing and I want it to be kind
of even that kind of sounds like a deer
so a distribution which gives you kind
of it breaks the value 1 into multiple
pieces that sum to 1 so it and this is
like a triangle until like every point
on this triangle the three values also
into one so the generalization of the
deer Schley to kind of finite things is
the deer shade multinomial so you get
this kind of triangle with a grid and
you can get you know any number of
numbers that sum to some integer that's
positive and so I started getting data
like this and values like this and I was
really happy of course I'm still the
there's this whole problem with
generating this giant matrix which I
still don't really have a good answer
for I see them over time I think I'll
just stop you and if I think we have a
couple of minutes for questions actually
well okay yeah now so we check one of
the other things it does is it minimizes
the test cases that it produces have you
experimented with that at all or like
how does it go for bioinformatics um yes
we so when you're generating something
that looks like this it'd be really
great if you could shrink the test case
but I haven't really looked into
shrinking yet I'm still trying to get it
to generate data that's useful and
interesting we actually had to implement
our own quick check library because we
didn't find the Scala one to work well
for what we did
so okay thanks so let's thing I think is
silly question and now that we don't
have you know lots of questions we're
asking what do you think about
explaining to people what you're doing
in a way that would just bring us one
step further to being able to ask really
good questions what do you do when you
talk to people like us who are not
experts in the area and you want to sort
of you make those things one step more
understandable yeah you mean like the
genetic stuff or like well one thing I
was thinking about that would be nice to
understand a little bit a little bit
better is those diagrams of the
relatedness mm-hmm so if you had a few
remarks on on what kinds of relatedness
that you see that you know amateurs
might not know about that could be a
very good way to spend a couple minutes
okay um yeah okay let's see so I mean
these are really cool diagrams yeah I
don't know exactly how much to say and
so I guess I never explained what this
identity by descent zero thing is so
that oh those ones yes sure yeah we can
talk about that so this is the result of
like doing some linear algebra
yeah so that box there is so if you
multiply the genetic matrix by itself
you get a genome by genome matrix and
it's telling you the similarity between
two positions in the genome so the so
it's symmetric because it doesn't you
know you doesn't matter which pair you
look at it doesn't the order doesn't
matter and these blocks that you get
around the diagonal basically correspond
to areas that are so the way that Mayo
sis works and when you when you create a
child as you take your two genome the
two genomes that you got from your
parents and they cross over and they
might cross over twice and so all the
spots where you don't see a lot of red
I'm pretty sure this is a picture of a
single chromosomes worth of data the
spots where you don't see a lot of red
are spots where you're very likely to
cross over and if you think about it if
that's a spot where it's very likely to
cross over that means things below the
crossing point and above the crossing
point are unlikely to get inherited
together because when you when you cross
them the next thing you do is tear them
apart so you get two new chromosomes and
you give one of them to the child so the
by looking at this diagram we can
actually start to learn where it is
likely that you are having crossing
points in the chromosomes which maybe
you would hope can start to teach you
something about the chemistry and
physics of the chromosome and then this
sort of thing is just a similarity
between individuals which you would
expect to be really hot on the diagonal
because you're similar to yourself and
then anything off the diagonal is sort
of telling you about other ancestral
populations like you know if you're both
European or the telling you about
families so people that you're very
closely related to have a have a shared
common ancestor that's very recent okay
so I think let's thank the speaker
and we'll start again here in actually
five minutes from now
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>