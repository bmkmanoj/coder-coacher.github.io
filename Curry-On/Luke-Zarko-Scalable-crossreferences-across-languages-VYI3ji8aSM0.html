<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Luke Zarko - Scalable cross-references across languages | Coder Coacher - Coaching Coders</title><meta content="Luke Zarko - Scalable cross-references across languages - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Luke Zarko - Scalable cross-references across languages</b></h2><h5 class="post__date">2017-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VYI3ji8aSM0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in the open-source heist brought our
kites project kite aims to establish
open data formats and protocols for
interoperable developer tools and I'll
be explaining to you what that means in
the following way first I'll give you an
introduction to the project give you an
overview of how we effects the project
I'll talk about testing a little bit and
this is interesting because as it turns
out because of the way the components of
kite fit together we're able to abstract
testing from particular programming
languages then I'll talk about how we do
cross language support between protocol
buffers an interface description
language and I need an eric programming
language and then conclude so first
here's the problem that you generally
are trying to solve when you're doing
programming language tooling you have
something you want hope you have
something you want to do and you have a
language that you want to do it to but
you don't really want to re-implement
that thing for every possible language
that you use so at Google we use a whole
bunch of different languages right and
we have a whole bunch of different tools
and we don't have the bandwidth to
implement the cross-product of those
things so you start to look for
abstractions right you say well maybe I
can write an analysis that works on
languages that are mostly compatible to
C++ like C maybe you can do cross
references as long as your language is
supported by the client front-end maybe
you can generate documentation provided
you can write R ax for languages that
have curly braces in them or maybe you
can do code review or code search tools
based on programs that are stored in
plain text what kinds of which kinds of
leaves are languages like squeak which
are image based in alert but really
that's not the whole problem right you
have some kind of source code repository
that you're talking to you're maybe
using git subversion CVS your company
file or your local disk or you're just
pulling code off of someone's port 80
right you're using different build
systems you're using see make G make
basil maven a bunch of shell scripts and
maybe ant and you have some code
generators involved 2 protocol buffers
captain proto yak antler maybe the j'ni
and your tools have to understand all of
these things to be maximally effective
so what we propose with kife is this
sort of plastic our last thing right
where you have support for some common
interchange format that you implement
for all of the languages on the input
side and all of your tools consume it
right but it's not just the programming
languages that you want support for it's
your build systems it's your other tools
and the tools that you want to consume
this data or your cost reference servers
your editors your documentation
generators this is slightly different
from extant technologies like the
language server protocol in that more
concerned about the semantics of the
intermediate representation here not so
much about the protocol that the tools
use to communicate with it well we do
provide some protocols we're really
interested in how we represent programs
and how we can represent the semantics
of the programs at a deeper level so how
do we do this how do we build this
intermediate format we've factored the
idea of analyzing source code into two
major phases first we start with the
build system and then run it through a
process we call extraction where we pull
all of the information we want or all
information we need to do semantic
analysis over source code into build
data which I'll explain shortly here you
can stop and pull the bill data into
your existing analyses and not have to
worry about repositories or build
systems if you want to keep going though
we provide an indexing step that takes
this build data and produces our
intermediate representation that you can
then feed into a database and then read
with other tools what does the build
data look like right so after we start
with the build system we produce
something this build that has an
interesting property in that it's
hermetic it contains every dependency
your compiler needs to do semantic
analysis
it also gives files identifiers that can
be used to locate them in those
repositories so this is where we
abstract away the necessity to
understand your build system or your
subversion instance together this allows
for the distribution of analysis tasks
in the following way you might have a
bunch of machines that are allowed to
touch your source code write in a bunch
of machines that are allowed to do
analysis and these are at the same set
of machines because source code is
precious right so once you build your
database of compilations you can pass
those to your cluster of machines that
are allowed to do analysis on them that
cluster no longer needs access to the
original repositories in the same way
you can pass those
compilation units to someone else
entirely and have them do analysis on
them because of these names that we
associate with the build data and with
all of the files in them we can go back
and look at where they came from the
original repositories without actually
having to touch those repositories in
the first place this is what one of
these compilation units looks like for
C++ it has all of the the transitive
closure of the headers that you need to
compile a particular source file as well
as the arguments you need to invoke the
compiler on them and the source file for
the particular compilation action for
Java it looks fairly similar in that we
have your Java source file to build a
particular jar and then we also have all
of your dependency jars included into
this single file but we do some special
stuff to them we don't really need the
bytecode to analyze the bytecode of your
dependencies to analyze a particular
Java project we just need the public
signatures inside those jars so we
eliminate the bytecode
and just store Public signatures on any
details that are public in the class
files so once you have these things on
you've taken your build system you've
taken your repository you've built a
whole bunch of these compilation units
that describe all of the build actions
in your in your project um what do you
do with them to produce cross references
for example um you produce our
interchange format the interchange
format is a big directed graph all
programs are represented with nodes
which are just named maps from strings
to bytes and we'll call these facts
sometimes so a node might have a fact on
it that says it's kind is record and
these names so the name of a fact for
example is namespace so we have the kite
node or the kite node and this name
space is sort of the route um you can
define your own fact names and do your
own analyses and put them in the
database then we have edges between
nodes that are triples of source names
labels and in target names in our graph
we have nodes that represent both
semantic information and syntactic
information so an example of semantic
information is the thing I just showed
you right we have had they node in the
graph that represents some records so it
came from a structure a class in Java a
minute says this is a
this has some unique name but we'll also
have a representation of the binding
site in your source code that causes
that record to come into being so
somewhere you will utter Class C we have
a node in a graph that says this span of
text in a source file gives rise to this
node in the way that we represent that
is with an edge between the syntactic
node in the semantic node and we call
that edge defines binding also we
sometimes call these spans of text
anchors just like in HTML you have
anchors so as it turns out in one of
these graphs we store information about
a whole bunch of different programs at
once so naming becomes very important
right you don't want two things from two
separate programs to have the same name
because they're indistinguishable so to
start out we have structured names for
notes there are you know about five
fields in one of these structures what
they're actually called are unimportant
what is important though is that some of
these fields are fixed right Wow we have
one field for example that says this
node came from this particular
programming language but the indexers
the things that take the compilation
units and build type interchange format
from it are left to build note IDs for
themselves there are some Maxim's that
we give people to help them come up with
good names for example things that are
the same should be given the same name
this has important implications because
for example the graph can get very very
big and if you have unique names for
things that aren't unique say the
representation of a type then you're not
going to be able to store the graph so
we say when you have a representation
for our type try to hash cons it right
you should have one type for the type
constructor for a pointer you should
have one type for the built-in void and
you should also have one representation
of the type for void star which is the
type application of void - pointer on
the other hand it's important that
things that are different be given
different names and this can sometimes
be surprising for example users like to
see that different forward declarations
of a function in C++ are different
things in the graph so for example they
can ask where are all the forward
declarations of this thing if we merge
them all into the same node we would no
longer be able to distinguish them it
also turns out that writing down the
qualified name of something as the nodes
name is insufficient right the canonical
example is void mere sorry it's its main
and C++ because we have a whole bunch of
programs stored in the graph if we had
only one name for int main we would no
longer be able to distinguish them and
we would no longer be able to tell those
programs apart as it turns out in a big
multi program graph are even qualified
names in Java are not enough to
distinguish between different program
artifacts so for Java we name things
after their qualified name plus a hash
of the structure of those classes so
once you get your names right and that's
probably the hardest part of integrating
with the system um you have to start
describing the relationships and the
particular nodes that you've generated
so what we do is we provide a base set
of nodes and edges are in the kite
schema we also provide name or rules for
naming certain kinds of nodes right so
if indexers are allowed to come up with
their own names for things and really
they are and we say this should be a
part of the implementation of the
indexer and it shouldn't be public you
have to be able to get in the graph
somehow you need to be able to address
your objects the easiest thing to
address is a file you can just ask the
system show me all of the things that
are declared in this particular source
file and you can do that because we
dictate how source files are supposed to
be named the schema is extensible so I
mentioned that identifiers for edges and
identifiers for facts are sort of path
structured so stuff under the kite
namespace is ours and we dictate how
that's supposed to be used but you can
use your own paths and you can propose
to add them to the schema so this leads
to the maximum that you should be
conservative and what you send to us at
but be liberal know what you accept in
that when you talk to kite some data may
be missing right some people just might
not be able to produce data maybe
they're analyzing a dynamic programming
language there may be more data than you
can
stand on this means that some people
might be writing their own analyses
under their own pads
some people might do the wrong thing and
that's okay you should be robust and in
the end type is not a formal system it's
meant to be a scalable system though
that you can use to drive stronger
analyses for example you can ask our
system show me where all the references
of a where a particular thing is you can
pull down the compilation units that led
to those references being generated and
then run your language-specific analysis
over them it's also meant to be
extensible in the sense that if you
start with a really really simple
implementation of an indexer like you
write something for python that can only
look at local references in a single
file you can improve on that indexer
without having to change the schema
without having you change this the tools
that consume the intermediate format by
just emitting more date so here's an
example of a section from the schema
it's not really important what's written
here I'm just showing you the general
layout of it will give you for each
artifact an informal description of it
so this is four that defines binding
edge and it says sort of like where you
can expect to see this thing come up
armed from what kinds of edges there
from what kinds of nodes it should come
like maybe syntax to semantics nodes are
and then we'll give you some checked
examples about how this thing is used in
different programming languages and the
languages that we have in the schema
document for examples are C++ go and
Java so with all that in mind so we
start from your build system we build
the compilation units we index the
compilation units we get a bunch of
graph artifacts how can we actually
connect a project let's say based in C
make to a web browser to get cross
references so the first thing that we do
is we run extraction on it um see make
has this nice property where you can ask
it to build a compilation database which
is a big JSON file on that contains main
source files working directories and
invocation arguments for your compiler
this differs critically from our
representation of compilation units
because it doesn't contain the source
text so you still have to have a local
check out of your source code
in order to use one of these compilation
databases so after you run it through
the extractor you have this whole bunch
of hermetic build data that you can use
to pass to your indexer on the in
Victor's construct data in the
interchange format and remember this is
just a big graph of nodes and edges and
will dump all that information into
something that we've titled the graph
store which is really just a big on disk
database services then use the graph to
answer queries on they do code browsing
code review documentation generation and
you can have different services in
different our PCs to talk to the same
store this design is known just scale so
what we consider a strong are a small
data set is what we have power in
chromium code search and actually what's
running is the second version of this
system this is the third version of the
system that we've built on this data set
has about 36,000 C++ compilations that
generate about 50 gigabytes of serving
data our internal instance of code
search is much larger it runs daily on
over 100 million lines of code and a
bunch of different programming languages
and other internal tools actually do
make use of this build data so they do
get off the train early and pull down
compilation units and run their stronger
analysis to do language specific things
now I lied a little earlier where I said
that things talked to the graph store
directly that's too slow because the
graph is just too big and the queries
are too complicated so for each client
so say for a code search client or
something that follows cross-references
we go through a D normalization process
so we pre-baked some queries over the
graph for example
we will pre-baked a source file with all
of the links in it for definitions and
references so that when users go to
query a file it's be able we are able to
give it to them right away one of the
more interesting steps that we go
through during d normalization is the
generation of clusters for
definitionally related notes and what
does that actually mean
practice well as it turns out users like
to think of things like for declarations
as distinct and as the same thing in
different contexts right
imagine that you're asking for all of
the call sites of a function f and you
have some forward declaration of F that
some dot C C file C or some source file
C and the implementation of F the full
definition of it that other source file
see if the user asks for the call graph
for F and they only see calls to this or
only C calls to this that user will be
upset and rightly so so what we do is we
have canonical ways to say that certain
nodes are associated with one another so
we say that this forward declaration is
completed by this definition when we
have those relationships in the graph we
can build this cluster around them and
blame calls to this on the cluster
instead of on each individual node now
we maintain that relationship right we
can still say to the user
oh here all the calls to this particular
thing all of the calls to this
particular thing but we're able to give
them the merge site or the merged calls
very quickly we have support in our open
source project for a bunch of different
languages already and we keep adding
more we have great support for C++ go in
Java we have a nascent indexer for
Haskell for rust and for typescript and
internally we have support for a bunch
of other languages so I'd like to talk
to you about testing a little bit
because this turns out to be kind of
interesting um at a high level if you
even go higher than in the first couple
of slides you can think of our system as
something that takes in source text
passes it through this extractor
composed with an indexer and produces
graph artifacts how would you actually
test one of these things in our earlier
the earlier versions of the software the
early revisions of the software sorry um
we would write tests in each programming
language that we built in index report
and these tests all kind of followed the
same pattern
you have to find something in a source
file like the Declaration of a function
on you have to identify what parts of
the graph that should generate and you
should make sure that you disregard
other stuff in the graph that you're not
interested in because otherwise if you
do that thing earlier that I mentioned
where you can progressively improve your
indexer by emitting more data you don't
want a better indexer to violate your
previous tests um this really is a
search problem so let's put it there
let's cast it that way let's say we'll
build a generic component that can look
at the source text can kind of clamp in
here and can look for various graph
artifacts based on user provided
expressions not in a particular
programming not in particular
programming language is defined by
source text but by a language defined by
this testing tool so we call this the
verifier and a test that is so
instrumented looks like this here's our
function declaration and we want to make
sure that it produces a graph that looks
like this so progressively we'll say
there's something that is that is
printed as f like the token F should
exist on the following non assertion
line so here's an F and this is looking
for it we want an edge in the graph this
defines binding edge that connects to
something else that we don't know about
yet some function so here's that note
it's an anchor node and it has some
start and end location which are offsets
in this source file and we have an edge
that we specified that connects to
something else we don't know about here
but we're going to call it function f in
the scope of the test we'll say that
this function has the proper node kind
associated with it so we're looking for
something at the end of that edge like
this so how we actually solve this is by
lowering this test to a tiny constraint
language what's actually printed here
isn't so interesting it's just to show
that yes we do some lowering um and we
produce logical variables or existential
variables for each of these things that
we're searching for
so we have a variable that represents
the syntactic node for F remember the
anchor in the source text and that shows
up where we check to see that it exists
on an edge and we check to see that it
has the right range and then we also
have a variable for the function f and
we check to see that it has the right
node kind when we go to actually process
this test case we run it through a
really naive but still sufficient for
our purposes logical solver so here's
the graph that I showed you earlier and
we'll walk through what the solver
actually has to do so the first thing
that it's going to do is try to find an
assignment for this variable and it gets
lucky and it says well yeah we'll say
this is what we'll assign to a def which
remember is the syntactic representation
of the functions binding site and it has
the right start in locations and it has
the right kind and it also has the right
edge here um because it has this edge
and because it has the source it can try
to unify or assign this function
variable to this node and as it turns
out it gets it right if it gets it wrong
it does kind of the obvious or not maybe
not so obvious backtracking thing where
it keeps trying to find satisfying
assignments for all of the logical
variables in your tests if it can then
the test passes if it can't then the
test fails this is what the output of
this actually looks like and my point in
showing this to you now is that you can
have a lot more stuff in the graph than
what you're actually interested in but
the testing framework allows you to draw
a box just around the stuff that
interests you and here this is where
we've unified the existential variable
for function f and here's where we unify
the variable for the anchor so with all
of this in place we're actually able to
do cross language support in a really
easy way ah this is something that a lot
of these tools don't actually support
and we're able to get away with it
because we have this consistent
representation and reification of syntax
in the graph so I mentioned earlier that
in our data store we have graph notes in
edges for lots of different programs in
lots of different programming languages
but there are little islands because one
of the things that we've said is well
one indexer isn't allowed to know how to
utter the name of another indexers notes
this is actually comes from some
experiments because as it turns out in
previous versions of our tool people
were writing queries against the graph
that had special knowledge of say how
the C++ index are generated names this
kind of ossified an interface that
should never have been an interface in
the first place and made it harder for
us to make changes without breaking
people so now we say well you're not
allowed to do that at all and
furthermore we're going to hash all of
our names you can't even try anyway the
problem with this approach is that
sometimes users think that things that
are actually distinct are the same in
the same way that users sometimes want
to see for declarations and definitions
to be the same thing they also want to
see um things written in an IDL like
protocol buffers which are used
extensively at Google um with the code
that they generate for example you might
write in your interface definition
language the definition of some message
type M this generates implementations of
the message type in a whole bunch of
programming languages for example Java
at the top right here and C++ at the
bottom right as it turns out users
really want to see all of these things
as the same thing right they want to see
them all unified in the same cluster
like we talked about before it's still
important to maintain the differences
users might want to slice the program
only by a particular programming
language but in the general case you
really want to say where is message M
used across the whole codebase how can i
refer
message m without breaking anyone in any
language can I remove a field can I
rename a field without knowing how to
utter the name of em in these
programming languages though how do we
droll in those edges that tell us
through all this cluster around them
it turns out that we're allowed to do
this without violating overalls let's
look at how the process works so first
we have our source code in the protocol
buffer IDL this just defines the message
type and because this is a programming
language just like any other programming
language there's an extractor in an
indexer for it that takes in that source
code and produces a graph artifact so we
have this representation of the message
type in the graph it also has a name it
has some signature that the protocol
buffer indexer generates the protocol
buffer compiler also generates Java
source for example the implementation of
class M but we have instrumented and the
code is there the code is public arm the
protocol buffer compiler to also produce
this kind of sidecar file
it's a metadata file that says in the
Java source that I just generated this
token M is associated with this
signature the protocol buffer compiler
an indexer are in on it with one another
this is allowed but it's just contrary
to the direction things normally working
right usually the indexer is completely
subservient to the compiler but here the
indexer dictates to the protocol buffer
compiler here is how names are generated
it would be unwise to make the Java
indexer understand how these names are
generated because it would mean that we
introduce a strong coupling between two
components that previously were
uncoupled it would also mean that for
every new code generator we would have
to implement special support in every
language that the code was generated
into so we take this metadata file which
again is just a bunch of source text
ranges that link up to addresses in the
kite ref and pass it through the Java
extractor indexer and will produce a
representation of the Java node for
public Class M but during indexing we
have associate
this metadata file with the Java source
file so the indexer knows to look for
this M when it sees the M it knows to
paint in that additional generate edge
in this edge we know internally during
our denormalization process should cause
one of these clusters to be built so now
we can say to users anytime you see a
reference to the protocol buffer message
m or anytime you see a reference to the
generated code for M consider those
things to be the same thing so now
users can ask show me everything that
uses this cluster or users can ask show
me anything and everything that just
uses this note or just uses this note
show me anything or show me everything
that I would need to touch to change the
definition of message m now one last
thing
are you the one problem that we've run
into it's all is that this metadata file
is a new weird binary thing that has to
be passed around with the rest of your
code right the build system needs to
understand it and sometimes writing
build systems support can be complicated
build systems are very smart they'll do
things like for C++ look at all of the
includes in your transitive closure of
includes or in your source files and
only allow the compiler to touch those
that you've actually included or
declared that you've included by doing
ray X base parsing over your over your
source files this doesn't work if you
try to say I'll attach a metadata file
to your source file because it's not an
official include and you can't just use
an include because it's a binary file
and the reg X parser will fall over so
let's think of a simple code generator
that say takes a file that just has a
bunch of names in it and then generates
void functions for them for that it
might build this implementation file of
a function that just is called test it's
not a very useful code generator but
it's a useful example because it doesn't
do anything interesting this generated
code file will include a public
interface that is also generated and
that looks like this
for C++ this is how we associate
metadata with a particular compilation
we have a special preprocessor
definition that we defined during
indexing and the reason that we do this
is it allows us to do more expensive
things when we're running the indexer
than when we're doing the compilation
and as it turns out when you have a
whole bunch of different protocol
buffers in your file
this really has significance we have
this pragma which attaches the metadata
file to the current source file so this
is how we say to the indexer when you're
looking at different definitions in this
source code also make sure you look for
these special ranges to generate the
additional generates edges and then we
have a regular include and this is there
to satisfy the build system the one
final thing we do though is we have to
make our binary metadata look like just
plain text otherwise include scanners
get upset with us so we base 64 it on
this looks like an ordinary header file
that just has a comment in it the
indexers in on the game and it knows
that it can just decode this to a
metadata record this just says test
base64 strings so there's no use typing
it but when the header scanner goes
through everything is ok and it
understands already that header files
our dependencies are the header file
that's part of a generated code
dependency should be passed to all of
the dependents of that thing so
everywhere that the generated code is
used so too is the metadata available
we've had two groups already internal to
the company that have built custom
support for their code generators based
on this thing where they didn't actually
contact us first they just went and did
it which is really cool on so anyone on
internal external like can just pick
this up builds support for their their
bespoke code generators and not have to
change kite and not have to change the
indexers and not have to change the
compilers or the build systems they just
produce this metadata which is open and
fully specified
so to conclude kite is a set of open
data formats and protocols for building
interoperable developer tools we give
you this graph schema that defines a
common vocabulary but allows arbitrary
extensions to it so this common
vocabulary is actually really important
and in fact with the the verifier test
language that constraint language it
becomes a way to talk to one another
about the shape of the graph right
instead of walking to a whiteboard and
drawing lots of boxes and arrows and
things you can write down in a really
simple concise and understandable way
what you expect to get out of a
particular program in our interchange
format you don't have to adopt a whole
system for the system to be useful um if
we have support for your build system
you can pull out the compilation units
and use them to drive your special
analyses in we're running this stuff in
production every day over a very very
large code base and the more important
thing is that our users report that the
cross-reference is that we generate our
their favorite internal developer
feature so I will leave you with links
to the project into our mailing list and
please visit and ask any questions there
or ask any questions now
thank you for your talk a quick question
if I understood for your system to work
correctly its relies on being able to
identify the things which are literally
the same to make sure you don't
duplicate it because then you'll say
that you refer to it twice in practice
how does it work with languages which
have undecidable subtyping so when know
the notion of equivalence is commonly
defined it's a is a subtype of B and B
is a subtype of a if one other thing
happens to you know B undecidable it's
not clear how you join them if a is a
subtype of B and B is a subtype of a
then isn't a a equal to B yeah so but if
subtyping is undecidable then well
equality for such languages would be
undesirable so for those we can actually
represent the we have a representation
of the subtype relationship so we can
use that to unified things together even
if they're undecidable from the
theoretic perspective we can still
record in the graph that people have
established those relationships and then
use that establishment to build the
clusters and give people useful cross
references so if I go through your
approximating subtyping but something
which is decidable but maybe less
precise I'm sorry I didn't catch that
part your approximating subtyping with
something which is decidable but maybe
less precise exactly yes so this so we
don't produce representations of
programs that you can compile down or we
don't produce representations of
programs that you can type check we
produce glosses of programs you can
think of it that way like it's a common
interchange format that is lossy but is
still useful enough to supply a whole
bunch of different developer tools with
the information they need to be useful
thank you Thanks
and so did I understand correctly that
you build your own craft database and if
so how is it different from RDF style
graph databases so we have not found an
RDF style graph database that can deal
with our data on the database and the
denormalize represent the the
representation of the graph is built in
open source world on top of level DB
and then we have another level DB based
implementation of our denormalization
pipeline that's all that I thought I Oh
if you go to we have our stuff is up on
github if you want to play with it and I
noticed your test spec thing was in a
way quite similar to the query languages
often use for graph databases was it an
inspiration or is it's like
independently so it was no I agree it's
it's it is very similar to graph queries
on it was independently developed um I
mean it's not so it's not a new
groundbreaking theoretical development
it's just something that we said like
here's the kind of data that we have
what is a concise way of expressing
constraints over it um there's like a
how-to up on our website like that
describes the language in full there's a
bunch of neat little things in it like
you can have groups of rules that are
negated and other things like that but
in the end it's we've tried as hard as
possible to keep it simple keep it small
and keep it focused on testing I mean
the the best thing about a programming
language is the constraints that it puts
on you right
the stronger language is the less you
can do in it
so following up with a question that
asks before have you had any examples
where your approximation was too crude
and the information which user was
getting back let's say all the coal
sites was too imprecise and not
informative and did you have to tweak
how do you simplify the data and your
loss how much information do you lose um
let's see yes there I mean plenty of
examples when you're developing these
things and sometimes it's it's hard
until you get a bug report in or until
you're browsing around so for Java for
example um it turns out that one of the
important properties of naming is that
one you're generating a name internal to
a source file it has to be the same as
when you're generating a name external
to the source file and because we do
this trick where we suck out all of the
bytecode and we suck out all of the
private members from a class before
putting into the compilation unit it's
imperative that the hashes that we build
of those classes don't include details
about those things so one of the
problems that we had was we were not
properly unifying names external and
internal the source because the the
hashes that we were building included
some private implementation data which
turned out to be some flags in the byte
code that got eliminated thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>