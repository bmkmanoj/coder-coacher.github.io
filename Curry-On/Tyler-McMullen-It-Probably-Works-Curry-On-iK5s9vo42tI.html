<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tyler McMullen - It Probably Works - Curry On | Coder Coacher - Coaching Coders</title><meta content="Tyler McMullen - It Probably Works - Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tyler McMullen - It Probably Works - Curry On</b></h2><h5 class="post__date">2015-08-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iK5s9vo42tI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I hope that's interesting view if
it's not that's probably okay because
every other talk of this conference
looks amazing so Who am I
Tyler I'm the CTO of fastly what it's
vastly vastly is an awesome content
delivery network we'd all the things
that the content delivery network
normally does and then do things like
real-time purging and analytics and
whatever else and that's the end of my
sales pitch so what makes me qualified
to be up here talking to you about this
absolutely nothing I didn't go to
university everything I know I have
learned from reading books and papers
and picking the brains of amazing
engineers over the last 15 years I've
been doing this so I guess that's not
nothing but i guess i have written
implementations of these things that
we're going to be talking about and i've
used them successfully in production
that said there are undoubtedly people
in this room who know the math behind
these and know the algorithms themselves
better than I do so if I say something
stupid please tell me also I'm
incredibly jetlagged so also if I just
fall asleep up here just let me let me
sleep please right so what is the
probabilistic algorithm I guess formerly
of be an algorithm that incorporates a
like an element of randomness to it
right there's a few different ways that
randomness gets into these algorithms
for some it's literally a ram call for
others you'll take the input data you'll
convert it into a form that is uniformly
distributed so hashing it essentially
and for others the randomness is
actually intrinsic to the data itself so
why am I appear talking about this at
all probabilistic algorithms exist to
solve problems that are either
impossible or unrealistic for us to
solve precisely so they're too expensive
too time-consuming etc to those who
either deeply understand probability
theory or who are like me and just have
like use and observe these things in
production for a long time not only
early is like that precision trade-off
they're like you know accepting less
precision in order to like make the
problem easier not only is that like
acceptable but we actually seek out
chances to use them this is entirely
because it lets us solve problems that
we couldn't solve otherwise and create
systems that are less expensive more
predictable and so on
not normally one to read from my own
slides however I really really like this
quote that comes from the structure and
interpretation of computer programs so
I'm going to do that in testing
formality a very large numbers chosen at
random the chance of stumbling upon a
value that fools the firm at test is
less than the chance that cosmic
radiation will cause a computer to make
an error in carrying out a correct
algorithm considering an algorithm to be
inadequate for the first reason but not
for the second illustrates the
difference between mathematics and
engineering I mean it's obviously
tongue-in-cheek but I think it makes a
good point in a way the other thing is
that like speaking of cosmic radiation
the world is probabilistic the universe
is probabilistic or at least it's way
too complex for us to actually model
100% correctly networks and especially
the internet are especially
probabilistic whether or not a
particular packet will get where it
needs to go is not something that we can
actually have complete knowledge of
which paths it will take when traversing
all the networks that it needs to
traverse to get from like point A to
point B is also something that we can't
possibly have knowledge of and so
systems that run across a synchronous or
lossy networks are necessarily
probabilistic or at the very least they
have to deal with the probabilistic
nature of these systems so you can find
it which is entirely reasonable and like
many systems do that like they try to
build like reliable things on top of
lossy probabilistic things underneath
that and that's a totally reasonable
thing to do but there's no avoiding the
problem right and I feel like this this
slide is probably like totally wrong for
the audience you look like it doesn't
seem like there's a lot of junior like
developers here however in the past I
have had to like in working with like
uninitiated developers I've I think the
core of the issue that they end up
having is that they believe that like
the idea behind a probabilistic
algorithms is just guessing in a way
that ideas of course not right probably
probabilistic algorithms incorporate an
element of randomness and the randomness
I refer to here is something that can be
quantified and analyzed in fact it's
crucial if that's the case because
otherwise it really would be just
guessing furthermore I've never seen a
paper or on a probabilistic algorithm
that did not quantify and
prove the error rate of the algorithm
this is incredibly important it's the
first thing I would look for like if we
were developing the first thing I would
do if we were developing a new one right
so it becomes incredibly important so
like if we look at bloom filters for
instance right depending on the context
in which they're being applied the input
can be large enough and the necessary
error rate low enough that like this
particular data structure this
probabilistic data structure becomes
entirely useless to us right you might
as well just use a set at times so the
point is that like you need to consider
what your input sizes and what the
acceptable error rate is in order to use
these things effectively so when should
we not use them right actually now so
many real-world systems rely heavily on
algorithms that have a probabilistic
component because in fact the input data
contains its own element of randomness
GPS navigation system self-driving cars
missile guidance systems all of these
use estimation algorithms which arguably
are using a probabilistic component in
themselves so then when should I use
these I'm probably more often than you
think consider the acceptable error rate
and how that affects the runtime or size
of an algorithm or data structure and
then make decisions based off of that I
found it to be surprisingly rare that
the acceptable error rate for real-world
systems is actually 0% even though
that's the intuitive response as soon as
you ask someone like what's your
acceptable error rate oh nothing no it's
it's not actually considering the fact
that we already talked about the fact
that the world is probabilistic you know
network cards die cables are cut
configuration errors occur machines
burst into flames like the acceptable
error rate for your real-world problems
is almost certainly not zero so let's
look at the first problem we're going to
consider there's this true that we're
going to consider this is the first one
this is the count distinct problem it's
the the problem of counting unique items
in a stream that may contain duplicates
or more formally like finding the
cardinality of a multiset the real
problems the phone of this category
there's a bunch of them you know how
many unique words are in a large corpus
of text
how many different users visit a popular
website how many unique IPS connected to
a server how many URLs have requested
through an HTTP proxy for instance or
what if you're trying to do the same
thing but across an entire network of
proxies just like as a concrete example
here right so this is our log of IP
addresses we would say ok for this one
the count distinct is five right I think
everybody gets it and so does a naive
solution to this it's really super
trivial you probably already wrote the
code in your head as soon as I started
talking about it you take a set and then
for every item in the stream you add it
to the set and the output is the length
of the set very straightforward it's
entirely possible we've already written
this code in production before and so
you know for that naive solution you
know adding an element is an amortized
constant time operation finding the
length advertising on some time or just
regular constant time Jesus the space
however does grow linearly with the
number of entries that you add right and
so as with many problems the
difficulties with count distinct
actually come with scale it's really
easy and cheap to count a thousand or a
million unique users ip's URLs or words
but how about a hundred million actually
that's still not too bad how about a
hundred million per server across
thousands of servers now that's more
interesting so if we let's make that the
problem that you know it's born
performer counts of stink across a
thousand servers which could have each
as many as 100 million unique items and
find the cardinality of the union of
their results in other words distributed
count distinct so how would that be done
well let's look at another naive
solution you know great it's really
super simple again each server can keep
a set of its seen items just as in the
you know previous example and when each
finishes their individual council stink
to run they can just send that down to a
central server and we'll just combine
them all using like a Union operator
great it's super simple and obvious but
of course that precision comes at a cost
if each server seeing like as many as
100 million unique items the size of
that seamless is going to be significant
even if you were to like hash each the
inputs down to like a 64-bit integer the
size would be about eight hundred
megabytes you know potentially per
server each of those sounds of servers
would then send their scene list down to
the central server even in the case of
just a thousand servers that 800 gigs of
data that we would be pumping into that
like combined cardinality function right
this clearly doesn't work you either
need to like install one of those big
data systems I think they call it now
and then hire a team to run it or find a
different solution to the problem so
this is where a hyper log log comes in
April autoblog takes a probabilistic
approach to the count distinct problem
it's an extension of the earlier log log
algorithm and attempts to like improve
the improve the accuracy of that and
it's been like pretty widely used over
the past several years the core of the
algorithm actually arises from a couple
simple simple observations which we can
easily show by walking through an
example well so let's say our input data
is just a stream of URLs right so we'll
take those URLs and we'll hash them
using our awesome 8-bit hash function
here and I'll spare you the lecture on
bad hash functions I assume you're going
to use something reasonable and so what
do we know about this number well we
know it's part of a uniform distribution
right ideally anyway it's part of a
uniform distribution another way to say
that is that the probability of any
individual bit being set is fifty
percent which would then mean but the
probability of you know one bit being
set another specific that being set is
twenty-five percent and twelve and a
half percent for three of them right
gotta see where I'm going with this and
then so we can take those numbers we
turn them into an expected number of
trials right so one bit being set will
be expected trials of 22 bit set is 43
is eight right so in this case expected
trials is equivalent to on average the
number of unique IP addresses or items
or whatever that we're looking for that
we have that we have seen until we
excuse me in this case expected trial is
equivalent to the number of unique IP
addresses until we expect to see this
particular number of set bets again jet
lag sorry so the first step then is to
take the input item and hash it down to
this uniform
distributed set of bits then count the
number of leading bits that are set keep
a record of the maximum number of
leading betsa demand sets so far this
would give a really very rough estimate
of the number of unique items that each
of these things is seen if there's one
leading bit set than you'd expect the
number of unique items is too if you've
seen three leading that's set that on
average you'll have seen eight again
this would be a very rough estimate so
the unique idea behind log log and hyper
log log is to buy is to divide the
incoming items into a series of buckets
based on their trailing bits right we're
looking at the leading this before and
we're going to look at the trailing bits
and then remember the maximum number of
leading bit set per bucket right so by
dividing up the incoming items into
buckets like this you can develop a much
more accurate estimation of the total
cardinality if you have em buckets and
end unique items then on average each
bucket will have seen n over m unique
items so taking the mean of those
buckets gives you a reasonable estimate
of the log base two of n over m from
which you can easily generate an
estimate so that's actually a pretty
complete implementation of log log in
about 15 lines of Python and I'm not
actually going to like try to walk
through this or anything but like I just
kind of put it up here as like a
demonstration the fact that it's
actually a reasonable i'll grow them to
write like it doesn't actually take that
much work to make this happen
furthermore a hyper level look has this
really neat property where if you take
separate instances of it with the same
number of buckets and then excuse me
right take separate instances of this
hyper log log with the same number of
buckets you can actually just take the
maximum of each of the like
corresponding buckets and it will give
you a pretty good union of them as well
so basically we just did this in place
of our set that we had before where now
the amount of space is actually the log
of a log of n rather than linear and
this is like actually the neat part so
according to the allesseh analysis done
by the authors of the paper you can
expect an accuracy of about you know
within about two percent
it while using roughly one-and-a-half
kilobytes of space for the buckets so
each server keeps its own one and a half
kilobyte hyper log log then sends it
down to that central server again let's
say for 1000 servers the central server
is now processing one and a half of
megabytes of data instead of eight
hundred gigabytes in our light
incredibly naive example before so
you're processing like 0.0002 percent of
the data you had to look at before and
the thing is this like this completely
changes the economics of a problem like
this right like suddenly you could
actually like run many of these in
parallel you could run them more
frequently you wouldn't need a big data
system or a team to run it you wouldn't
even really need more than one server
honestly like and all that for a cost of
the two percent skew and your estimates
now whether or not thats coo is
acceptable is entirely dependent on your
particular use case of course but
ultimately what this probabilistic
algorithm has done for us is made it
possible to do something that we would
not have been able to do before right
being able to give like potentially like
being able to give our customers right
the ability to see like this these are
the number of unique objects that you
actually have stored in a CDN right no
one's been able to do that before and
like so if we can do that with a two
percent error rate that's still totally
worth it so let's look at another
problem Oh put an animation and that's
great cool so a second problem reliable
broadcast right so the system that needs
to send messages to a known set of
processes reliably temples that and the
use case that I've had for this in the
past is is being able to send around
purged messages so essentially what
people do is you know they're storing
things in the CDN and they go hey this
thing has changed in the origin so get
rid of this particular URL that you've
store it everywhere around the world in
this particular case one of like one of
our unique competitive advantages is the
ability to remove that content from our
network really really really quickly and
so latency is thus an important factor
here but reliability is always paramount
we can't just lose purge requests that
would be really problematic
so the obvious and naive solution since
apparently I'm going through those for
each of these is to have a single source
of truth right so if we had a single
sink that all purged messages could go
through that would make it really easy
for us to like monitor the system and
make sure that it stays consistent right
in the very early days and we were just
trying to like launch a thing and not go
bankrupt that's exactly what we did
however that simplicity comes with the
phone instead of pains right having a
single source of truth for this is also
a single source of failures for this and
anyone who knows the first thing about
distributed systems can see like why
having that single point of failure is a
problem and so our central sink didn't
last very long we pretty quickly had to
build something else build something
better so first we had to figure out
what something better would actually
look like and so you know at first
glance this kind of looked like an
atomic broadcast problem to me however
it doesn't quite fit atomic broadcast is
an all-or-nothing sort of thing and if
we can't get consensus for the whole
network then the purge would fail and
that's not at all what we actually want
right getting the message to ninety-five
percent of the server's really quickly
and then eventually getting the other
five percent would be like far superior
for us and so the promos to reliable
broadcast protocols so this includes
things like SRM and a bunch of others
actually so most of these algorithms
were designed in the 80s and they use
what's called a retry strategy by Reese
try strategy what I mean is that the
sending process is basically responsible
for making sure that the receiving
process receives and processes the
message unfortunately this kind of
limits the scalability of a system like
this means that during partitions load
on the sending processes can skyrocket
right and if I know anything about the
internet I know that partitions happen
all the time so that eventually led us
to gossiper two phones so things like
plum tree and sprinkle the main
difference between these and the
reliable broadcast papers that we were
looking at before is that they're
designed to be much more scalable so
scalable like tens of thousand
servers punches us out hundreds of
thousands of messages per second and the
research on like gossip protocols is
still continuing down to this day and
probably people in the room who have
been working on this actually to get
this higher scale though what these
systems usually provide is actually
probabilistic guarantees rather than
like very firm rather than strong
guarantees in this case that the message
will be delivered instead of hearing
excuse me they provide probabilistic
guarantees the message will be delivered
to all servers rather than strong ones
and so what we ended up deciding to
implement in this case is actually
bimodal multicast which came out of
Cornell several years ago so it's like a
quick break down to how this actually
works this is essentially like three
phases but in this case basically just
too so we broadcast the message to all
of the processes that we want to say we
do that like you know we send that out
as quickly as possible and we gossip to
make sure that everything continuously
stays in sync so right so if we break it
down into phases we send the message to
all the other servers it doesn't matter
if it's actually delivered here you
could use IP multicast if it's if that's
available you could use UDP in a for
loop which is what we did initially you
could use carrier pigeons it doesn't
really matter like this is just a
protocol not an implementation the next
phase is that every server every so
often we'll pick a server at random and
send it a digest of all the messages
that they have seen so far right so in
this case a pix BPB pixie and so on so
the server looks the digester receive
and says god does this match what I've
seen so far oh no I'm missing a few of
these ok I'll send a recovery message
saying I would like these particular
messages and then the server that you
sent the gossip to will send this you as
well which is what happens in this space
does that make sense so far well good
cool and so for some reason I decided to
put this like neat little thing in here
basically just showing how gossip works
it's an epidemic protocol you all know
what that is right so it exhibits
behavior so the paper includes a bunch
of math to predict the expected
percentage of server's receiving a
message after some number of rounds of
gossip and so basically in this case
what we're looking at is like after 10
rounds with 0% packet loss we would
expect 96.5 percent of servers to have
received a message good enough right
sorry again J like right so essentially
the problem is like ninety-seven percent
is okay i guess but like really the the
ultimate issue is that like we see high
packet loss all the time and so even
after 10 rounds we're not actually
expecting to see ninety-seven percent of
servers to have received it we would
actually expect in case of fifty percent
packet loss something more like thirty
seven percent of service you receive it
and so one of the neat things about
generally with probabilistic algorithms
with this one in particular is that so
many of the things are tunable like you
can actually change what the probability
is and so in this case we don't actually
just have to keep it for ten rounds even
though that's what the paper recommends
to us instead we could keep it for I
don't know something like eighty
thousand rounds and so this is kind of a
neat graph in my opinion so if we can
see down there we're actually assuming a
ninety-nine percent packet loss but if
we keep it around for 80 thousand rounds
the probability of us losing a purge
becomes smaller than the probability
that i'll be struck by lightning
multiple times during this talk and yes
i actually look that up to figure out
what that probability is so the point
really is that like with high
probability is fine as long as you know
what that probability is so how does the
system work in the real world well
usually typically we're seeing like zero
percent packet loss just like at a
normal like normal operating procedure
across the internet it's something like
zero 0.1% packet loss and so thus the
95th percentile delivery for purchase
for us where messages for us is actually
just one way network latency which is
kind of cool most purges are sent from
the US and so like the 95th percentile
for latency for
verges is actually well below 150
milliseconds across the world it's
pretty cool and so I have an example of
like what happens when we do a packet
loss though and so on the left is
completely unreadable but on the left
you can see the number of purges that
have been recovered over time and on the
right you can see the the throughput of
machines that were affected by this
packet loss instance and ones that were
not and if you're trying to see what the
difference between those two graphs are
like on the right side there there isn't
one essentially this system despite
having like a on a between thirty and
fifty percent packet loss was keeping up
perfectly or as close to perfectly as it
could and recovering messages very
quickly so kind of the idea what this
section is a good systems are boring
right I don't really like this like
trying to debug distributed algorithms
at two in the morning I don't think you
do either it's nice to be able to like
you know sleep through the night and so
that's kind of the point of that that
second one but what was the overall
point again there's two really
probabilistic algorithms so in the case
of hyper log log you know it allows us
to build things that we couldn't or
wouldn't build otherwise in some cases
it can completely change the economics
of like building a particular system and
then the second thing is that like we
can build systems that are more reliable
the case of bimodal multicast you know
this you know we've ended up building a
system that is highly reliable and
ultimately boring right so rather than
battling Network partitions and outages
we go to system that just accepts that
they're a thing and is eventually
consistent without requiring infinite
memory in this case so that's all I got
thank you
that the last point and made me think of
the work of peter baelish and others oh
yeah probabilistically bounder staleness
which is about the same thing it's
instead of going for strong consistency
which gives you all these scalability
limits you just allow certain deviations
and then you calculate how how likely is
it to see a stale read for example from
my database and they and most people for
most people they they intuitively say it
must not ever happen and then you need
to dig hard and say okay what what if it
happens right what if you haven't sold
it for these other reasons and then you
find okay hundred milliseconds might be
fine and then you gain a lot of freedom
so this this is a pattern that I see
recurring in many places yeah the the
work that Peter Bealls and others have
been doing out he's not a Berkeley right
yeah just tremendous I I can't remember
the name of the actual lekha of some of
the principles that they were coming up
with that have been following their work
for quite a while now it's it's really
Julia really cool anything else
just a general question don't you think
that connections happening I mean
messages being transferred between the
nodes in the gas gossip protocol that
does that turn out to be an issue with
the number of nodes being very high I'm
sorry I didn't quite follow that could
you repeat the question I guess so let's
say the nodes are very high so they have
to send a lot of messages to each other
sure so does that turn out to be an
issue so currently that's not an issue
for us however I have been like
pondering what to do when it does become
an issue there's a bunch of different
things we could do including like
essentially like dividing up the the
purges such that they like only go to
machines that are likely to have that
content as well as dividing customers
into like certain subsets of machines so
purchase for particular customers were
only like go to those particular
machines as a bunch of different ways
you can partition the problem so
basically sharding you again on this
more global level yeah yeah basically
yeah I have a question because this all
works and I saw it working in for
example some risk management and so on
where all the things are probable and
everything is you have a chance of
losing that amount of money but what
about for example systems which kind of
have to be more reliable I was having
these problems some time ago but it was
very important for me to get the
communication Oh to ensure communication
won't fail the problem with this was
that actually after first
acknowledgement when you have to get
acknowledgement that you get
acknowledgement and so on and so and so
on right now do you have some better
idea for this problems I have some
better idea for those problems um no I
don't unfortunately like I mean there
certainly there there there are certain
problems where like you know a
probabilistic algorithms just doesn't
actually make sense for it right like if
you actually do need to be a hundred
percent if you do need like you know to
you know be able to like go to sleep at
night and know that it's never going to
drop a single message I can't actually
think of a problem where that's the case
but like if you have one then like I
totally get that yeah one more remark to
that there are some results like the flp
result that consensus is impossible to
achieve if you have a faulty process and
so on so also i worked a completely
different industry before operating
satellites and well things must not fail
right it costs billions of it fails well
losing a life is also something you do
not want to contemplate but in the end
there is a failure probability for
everything even if you think that you
covered all the cases I mean cosmic
radiation is not the only thing that can
kill you so this probability of 0 is has
been an illusion always yeah and indeed
nothing exactly the point I was trying
to make their early on in the talk so I
appreciate it
sure hey I I missed the beginning Tyler
I apologized but I was running around
but so you might have answered this
already but I'm just curious if you
evaluated other epidemic like protocols
such as like epidemic broadcast trees or
something like that given the
complexities in debugging bimodal
multicast just because it's like the
combination of two protocols and just
what your thoughts on that word yeah so
I think when we when we started building
the system sprinkler for instance I just
come out he goes right around the same
time and in retrospect I totally would
have probably built the system based on
that instead but we did look at some of
the others and ultimately we decided
this one was like the simpler like we
this is the one that we could that we
have the best understanding of after
reading the paper so some of the others
were really cool but on the other hand
like if we couldn't like really
internalize how exactly they works like
it made it much easier to build the
system ultimately so when you're fine
tuning these algorithms that you pick on
and you need to know some other failure
probably parameters what kind of failure
probabilities do you usually consider
besides like latency and stuff like that
what are some examples and where do you
get your numbers from 0 so essentially
how do you decide you know what what
what error it is acceptable oh man dude
that's totally above my pay grade i
don't know like ah no so so deciding
like what error percentage is is
acceptable honestly like the the way
that I've gone about doing this in the
past is actually just talking to our
users about it like going like hey so
here's the thing that we would like to
build we would like to be able to show
you what the total number of objects you
have across the world is and so on what
if that were 1% inaccurate 2% three
percent 5% inaccurate and just seeing
like what they actually would find
useful I think this is I think it's this
is essentially a UX problem having that
UX yeah that's how I got one last
question you upset out the error rate
doesn't match prediction of the theory
that's I can say again if you care for
example IQ formula predict two percent
error rate do you observe it or the
heavy observed for come significantly
higher rate than predicted by sea or by
theory for example super hyper log log
in particular or you mean for hyper log
log in yeah we've got a logo for example
yeah on for a second problem you know so
yes I have I've seen like totally like
off-the-wall error rates with hyper log
log and then it turned out I was using
it wrong essentially like the number of
buckets that you have in the amount of
space that you create for like the for
the particular for the particular
instances makes a huge difference as it
turns out and then of course like it is
a probabilistic algorithm right like
you'll there are times when you'll see
something that's like completely off the
wall but it is incredibly rare all right
okay let's think I speak
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>