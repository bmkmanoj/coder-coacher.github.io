<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Santiago M. Mola - Babelfish: Universal Code Parsing Server | Coder Coacher - Coaching Coders</title><meta content="Santiago M. Mola - Babelfish: Universal Code Parsing Server - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Santiago M. Mola - Babelfish: Universal Code Parsing Server</b></h2><h5 class="post__date">2017-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/E-ZzIXXHjAU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so early after lunch I santiago mala and
i'm presenting bubble fees which is our
project to build a universal called
cluster so I'm the lead engineer at
sourced this is a company building on
open source stack or to apply artificial
intelligence to code and I'm working on
a bi data pipeline to analyze all open
source code online so all open source
code for us includes every public git
repository every public repository or
some bit bucket on grid lab or
self-hosted and that also includes not
only the last version of the code but
also every single commit in every
repository and that's not focused only
on a small number of languages but also
means arising source code across
hundreds of languages so whatever we do
is sort code within with massive amount
of source code and in our wide diversity
of it and as you will see that kind of
influences a lot of design decisions on
this project
well my note I'm not a problem in
languages expert so what you are going
to see is the work of a bunch of deaf
engineers and other scientists trying to
make sense out of the field of of source
code analysis so health the story that
lead us to to this point a couple of
years ago we were working on an
application to help recruitment of
developments so we were building a
database of developer profiles and in
order to that we did some shallow
analysis based on git history and so
basically we're just gathering some
metrics on developers and we did this by
fetching every git repository we applied
a PageRank to two contributors we
calculated some metrics such as the
number of bytes contributed by each
person per language so quite easy things
from the point of view of
and then last year we started facing
some problems with this with the system
for example we have to ignore comments
to get some more accurate metrics and we
solve that with regular expressions then
we needed to detect the usage of some
libraries or with is that with moderate
expressions at some point we did with
easier to detect more than 170 libraries
and Microsystems where ecosystems are
may be a set of libraries and API is
something more raw than a specific
library or framework and we did that
with a generic tokenizer with some
butter matching up to this point we're
using like this pretty adult stuff
pretty ugly solutions but that they did
the work for this simple stuff but then
the designers came in we created a data
science team and they started doing the
design stuff mainly training deep
learning models on source code for
different applications and so we would
exposed to a whole class of new problems
for example extracting a data set of
identifiers used in every file in github
that includes function names variable
names and so on across every language
actually the story with revision and so
on we saw that with pigments so pigment
is a Python syntax highlighter it
contains like 400 lectures so we just
reuse that in order to buy the lectures
like the NT fires okay that kind of
works then we got asked for a data set
of all tokens inside small goal
functions where small is any arbitrary
measure of small space and we solve that
with our we wrote an extractor based on
they go ast standard library that must
quite a specific but also did work
pretty well and the next request is
extract a data set of every token in
real language arranged
per block whatever block or scope is in
its language it's a definition so at
this point we just bang ha
any more adult solutions for for this
kind of problem and this is the data
engineering team trying to cope with all
these crazy data science requests so we
have to take a break and stop doing this
kind of very adult solutions for each
request and try to come up with a more
general way of extracting this kind of
information we realize that all the old
data we needed so far can be easily
extracted for from majesties so if we
had a way to obtain an ast from any file
any language on github we could extract
we could fulfill all our previous
feature requests kind of trivially so
the first problem we faced is that or
something pretty obvious every language
is different and every language has very
different ast definitions when it's part
served on similar languages or even each
part set of the same language have
massively different elastics for example
we tried $3 parsers to realize that all
of them produced three absolutely
completely different things so we need
we need some kind of normalization to be
able to work across languages but
creating a single is the definition that
could work across any language and
without what will develop little doesn't
seem possible to us so we came up to
this conclusion that we shall deal with
this identity but using language
specific ast s where we can annotate it
snowed in the S T with some generalized
concept that we can use across a wide
variety of languages so our first
approach was using existing grammars for
we we pick the unblurred project which
is a great password generator and
contains existing implementations for
more than 120 languages
we said okay maybe we can tune them to
industry lommers to produce these
annotations maybe they they can prove
the job first thing we realize this well
something pretty specific to to to pass
him with llama but they produce a too
verbose parse trees which require
further normalization and adapting and
maintaining in them seems too costly for
us in some cases we will need to rewrite
these grammars from scratch so that's
where we reach our current approach
which is after realization that most
languages have an official parser
producing this end AST if not an
official in a library they have a de
facto standard so we should be able to
reduce this parsers and then are the
support processing step some minimal
structured normalization which is also
reversible minor detail at this point
and annotate with these general concepts
on this language the specific ast the
result of this processing step is what
we are calling in this project universal
after syntax trees or UAS TS so let's
see an example of how we can go from
code to one of these you just defined us
days so the process of creating this new
edgiest is have three steps first is
passing parcels create an eggless t that
its language the specific its in json
and it has preformed so this dashing
kaha absolutely any form it can be a
list of lists in using this like a sts
or it can be in Java or Python they are
they are JSON objects with arbitrary
fields and values and then we have a
normalization step where we take that
free foundation and we feed that into a
normalized structure but without
touching the relation between notes so
whatever it is a child or a sibling in
the regional industry it still
it has the same relation in the you AST
so we just keep the three structural
cities and then there's the annotation
step where we annotate its note with
language independent rows short roles
are at at this point the only language
independent annotation that we have and
a role is defined describing an aspect
that is generalized on one now in the
est so let's take as this simple example
in Python and Java of adding a and B
just actually the same code in both
honestly I put this example because it's
the only one that fits in the slides for
the next size so so on the passing step
without a two-part set for for each of
them they pee review something
completely different of course you can
find some some similarities as for
example in Python you have this AST type
which is no anybody note and which has a
left property which has also another
note we know that we know it's a note
because it has the ast type - it's a
name as a feel file filled within the
file and we generally see a similar
thing it's known as an internal class
expressing which kind of note it is
property with a plus and so on so
completely different things and then
that goes through a normalization step
you meet at some fields actually the USD
contains more information such as the
position in the original source code
where things start and end what I
omitted secondary information for for
brevity now so I just know the values of
stuff are still very different but the
structure this is the kind of structure
normalization I was talking about so we
have the same kind of fields internal
type the children of the of these nodes
are talking with corresponds to the
original token in the source code of
that node and some arbitrary properties
so this is a lossless transformation
because eventually what what we cannot
normalize we just put it in this
properties map we
released after and then we have the
annotation where we add the rows field
as you hear here that defined that this
node has this a specific function
that is actually the same across
languages so the by nope in Python is
stacked as binary expression which comes
from the USD specification the names are
simpler in the failures that are also
qualified identifiers and we have the
left hand of the expression as by an
expression left and so on can you see
something really weird on the
differential that's good this this is
just the same but I omitted everything
that this language is specific and I got
only the roles of its node and
children's on the top so it's a loop to
do the same for a simple thing right the
fact that we keep the original relation
between nodes from the original a DST
means that while we have generalized
what the annotations of its node means
the relation between different nodes is
not normalized so for example in this
case the operation ad which is a part of
the binary expression is a child node of
the binary expression in Python while in
Java it's the same note the same note
that listen effectively binary
expression is the one defining the
operator we have to deal with this kind
of variety of structures even if we have
the c'mon annotations so now that we
know where do we want to wet or what do
we want to attain from this process we
can introduce bubble fees bubble fees is
a self-hosted server for so-called
passing it can pass any file any
supported language extract an ast from
it and convert it to a common
representation called the UST
providing providing assign a single
interface to analyze source code in
different languages so as the title
representation cells the universal code
server so here's a overview of the
architecture bubbles barfy's is a
client-server architecture the client is
whatever application that this short
code passing it can send request to uh
to a server that then delegate the
parsing to different drivers it's driver
providing support for a language and
these drivers are split in two parts the
parser which produces this language
specific ast and the normalizer which
normally the structure and annotates
with language independent annotations so
in a driver the parser is implemented in
any language usually the source one so
for example the Python parser is
implemented in Python and the job of one
is implemented in Java 2 it's not
mandatory so for example the bus driver
has the pastoring provisioning Java 2 so
we try to just reuse the best library or
the most accepted library that there is
for passing a language or at least there
were the one producing better results in
order for us to be for this to be a an
effort that we can that we can manage
and then the normalizer is implemented
in goal
implementing the normalises of every
language in the same language allows us
to reduce a lot of code so we have the
bubble fish SDK providing a lot of
utilities that make the tree
normalization annotation process much
easier then the whole thing is packets
shadow pyramid and that's all loaded to
the to rocket pop the bubble teas
organization that serves as a central
repository of drivers and then we have
the server the server exposed side the
RPC protocol to pass files and get the
UST a manages the execution of drivers
these drivers are executed in a
lightweight container runtime its own
runtime not actually docker so it's
written in go and using live container
which is a library that docker uses to
execute containers under the hood so the
we're executed Ibis has containers based
on docket images but doesn't really
depend on a locker Damon on one time or
actually it has no other runtime
dependencies which is pretty convenient
for for deployment and then we have the
clients just you can generate the client
code with ERP RPC for whatever language
you are using and we are not working on
a C++ library with meanings for multiple
languages to make easier to manipulate
the UST and then the protocol it's
pretty simple
right now we have just one type of
methods with support requests you send
the content of the file and optionally
you add the path and the language so for
in our case we are passing arbitrary
stuff we don't know the language with e
beforehand so what we do is that we send
a pass request without the language the
rubber fist server uses entry which is
under library with we created for the
Technic language to detect language and
then draw to the appropriate driver
depending on this detection so the
status of the project so we start like
six months ago the project has a team of
three full-time developers plus a few
more part-time contributors and we have
an initial UST specification and I work
in several implementation you can friend
run them with either a standalone binary
or with a docker container which is
already published then we have 2 beta
drivers for Python and Java Python is
the most complete so ok
Python is most complete there are second
more complete and then we have 30 more
working progress drivers this includes
include JavaScript
relics elong our well we're working out
of them these are still not ready to
being used but it's the LR coming and
then we have published some showcase
tools we are at one providing this just
for you to have a feel of always working
with bubble fees what can you do
so these are still not to believe like
really polished tools they are just
demos I will have this tokenizer
a cyclomatic complexity calculator and
impact complexity cup later too and this
can work exclusively on the on the UST
they work on the UST and I'm sure they
work across every supported language so
that's it so you can check if you want
to know more you can check the
documentation and this unpronounceable
address dock at vvl f dot s8 so model
fish with the bubbles
we have released everything as open
source with Apache 2 license or GPL
license depending on the component you
can check out on our github website you
can check out also the Bible peace
improvement proposals which is our
community process to standardize the
Verge's protocol and the USD
specification and we're also hanging out
all day in slack so you can chat with
after so thank you
so there's plenty of time for questions
so I will you mean
cases like like the dacha later or
comida to learn so so the question was
how are we solving the problem of the
relation between nodes not being
standardized so um currently the
specification each floor has a
documentation what does it means to be
an article with digital and draws are
also documented with what their relation
to other roles so currently and this is
a pretty early approach we are defining
what are the possible relations how you
should look for the next node for
example then the binary expression we
have defined that you should look at the
operator in the same node and then in
the next and then in the next node
before finding another another world
that blake's that kind of relationship
that so so far that worked for Python
for Java to work for some application
that we are building and for some
specific cases so now we are discussing
if nothing that we have to do you have
to another kind of annotation related to
the role indicating indicating close
references in the UST so I think that
somehow soon we'll add some kind of
mechanism for you to say this is a
binary expression and you will find the
related note for operation on
it's tile or it's Palin or something
like that thinking most multiple on
relative references but that's something
that we are still thinking about all for
the Fall
I'm cool thank you thank you for the
talk um one of the things I noticed is
you showed Python and Java as examples
and I guess in terms of how of the many
language paradigms they are those two
are relatively similar how well does the
U ast approach work when you're Napa to
say logic programming or other paradigms
that are somewhat different yeah so we
are pretty early pointed on that but
that let's say that that's something
that we are still exploring so we are
having a lot of philosophical debate
about add up to which point so we create
generalized concept because if you want
drawers that generalize across every
language we are probably getting just
like four or five right not makings on
the other hand if you go and create
specific roles for every possible
concept in programming language we are
possible creating just the union of
every concept within every primary
language without any generalization
right so abnormal we are we are still
exploring up to which point we want to
generalize in in any case so now we are
pretty sure is that we need somehow to
have some kind of hierarchy of of of
annotations so that it's important to
know that a node can be annotated with
many concepts and our role doesn't
necessarily in fact it usually never
combined it describes the full meaning
of the node in the original language but
only an aspect so there should be
drawers are indicate something that an
aspect of a note that is actually
generally stable across different in
between list and Java there will be
other in fact there are other other
roles that work or that that will be
present only in a class of languages but
we are still exploring whether what's
the sweet spot on that so okay I am like
the talk you mentioned a few things
you're looking to measures such as
cyclomatic complexity or n path
complexity that kind of stuff yeah do
you have longer term goals for the kinds
of information that you're looking to
reveal from this kind of tool so for
instance you're looking to compare
programming languages like with regards
those kind of measures so sorry not
fully get the question so what kind of
an insight see you have any particular
aspirations or insights they are trying
to reveal from using label theory so our
most pressing need in fact currently we
are building
couple of demos one of the things that
one of the first columns that we want to
solve with our fists internally at
source for actually what we are
producing will be also construction is
to solve this kind of request so so this
kind of data set that we are extracting
with other fees are served to we're
using them to build models that one of
them will be for matching similarity of
projects across different languages and
other will be for a source code source
code completion we can build a single
model or have a single training process
to build models for code compression on
different line
witches or even or even to to use
patterns from one language to help in
the training of our different language
so these are actually just pretty simple
things you have the est but at the time
that this request were made the absence
team just didn't ask for more ambitious
thing because just they couldn't believe
that we were possibly able to extract
them right if we have some kind of
normalization the est now they can start
asking for more ambitious things and and
they are now working on on on how on
sorry the problem how to adapt the the
the word embedding algorithms that they
use for deep learning adapting them for
trees instead of instead of list of
tokens in order to be able to build more
accurate deep learning models so sources
accompany this is the kind of things
that we want to start from from bog
Rizzo we want to extract we want to use
it to extract many different
representations of source code in order
to build models or making but we hope
that with the help of the community we
can see I'm pretty sure that we can see
many other kind of applications that are
so possible with this thanks by the way
the kind of showcases ah
fortunately this tokenizer is kind of
like the first proof of concept for this
kind of usage in the news cases
cyclomatic complexity and impact
complexity were just stuff that we don't
need currently internally but they came
up at us hey if you can calculate these
kind of things on the UST then you are
not doing it that what I said there are
more like validation cessation scenarios
or okay we can do this we're on the
right track hey I wonder what wrote the
decision to use : for the normalizer
yeah so there was an easy decision for
us because for more than two years our
company has been using go for almost
everything so it was kind of the company
language that's basically the main
driver for for this decision this the it
being a language that was easy to build
a self-contained contained binary was
also something that was a good fit for
this for this architecture what were we
aim to build drivers that are docker
contained docker containers that are
very small but that was not really
determined terminal the main thing is
that it's our language so we don't
discard that if in the future when if we
get we get contributors to drive it
creators and writing analyzers in other
languages is something that people feel
like doing I guess it will be okay but
since now we are the ones that writing
all of those drivers so generally we are
writing all of them in their industry
language
the server on the other hand will will
stay go prolly in there on the
foreseeable future
mainly because of the usage of
containers because all the tooling on
containers is written in go from docker
banging go we can introduce a lot of
these tooling to for the front end not
has been very valuable with questions
I forget the author but I remember I
think it was a paper about comparing
very similar programs written in Python
and Ruby and the other father who sort
of compiled step by step each of these
programs and found that they became more
and more similar the closer to bytecode
they became so I was wondering if you
have any plans or have thought about at
least four languages that generate
bytecode maybe comparing them on that
dimension as well as the syntax trees so
you mean an icing on the bytecode
generative bytecode instead of the
original source code right or in
addition to organization - yeah so I'm
glad you asked
so that our main problem here is that
they said initially we are building this
on a pipeline to analyze source code in
massive amounts we want to analyze all
public source code so one of our
problems is that we cannot rely on the
build system to cooperate we cannot rely
on a build system being present because
all the from posters up do not use any
build system at least they do not say
which one they use and even more we
cannot rely on the code to compile
because even a lot of code has the build
system but at least by with the default
commands it doesn't build and we cannot
rely on being able to solve the
dependencies so one of the things that
because so I think that usually comes up
when we are doing this per file analysis
of source code is hey if you are looking
at individual files you don't have the
context you cannot do things such as
resolving a symbol and that's right it's
a conscious decision to limit ourselves
to this to this scope because it's a
scope that works that works everywhere
it works whether you can cannot fetch
the dependencies I can think for example
the go case which is a mess you are
take a point in the past of any gold
repository in six months you are almost
100% sure to eat Garlan teeth to that
code not building so unless you are on
Google that with me some more rep or
something like that and so since we want
to analyze the whole history of source
code is very hard for us to rely on
anything that that means being able to
to build but on the other hand we do not
discard that that we could add some of
that functionality or integrate with
other tools of source code analysis that
they do this kind of analysis at at at
real time working out guide for example
but that will have to be always an
addition to be used by users that have
control of the repositories and kangaroo
grantee that that this kind of paralysis
is going to work that the that the price
is going to build control I would like
to to go to that approach in the future
but by now we are focusing on stuff that
works for 99% of the code on github
nope Boop thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>