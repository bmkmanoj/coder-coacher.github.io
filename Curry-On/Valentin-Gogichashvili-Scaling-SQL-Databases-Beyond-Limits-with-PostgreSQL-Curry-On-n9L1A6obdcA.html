<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Valentin Gogichashvili   Scaling SQL Databases Beyond Limits with PostgreSQL -Curry On | Coder Coacher - Coaching Coders</title><meta content="Valentin Gogichashvili   Scaling SQL Databases Beyond Limits with PostgreSQL -Curry On - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Valentin Gogichashvili   Scaling SQL Databases Beyond Limits with PostgreSQL -Curry On</b></h2><h5 class="post__date">2015-07-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n9L1A6obdcA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yes I mean it was quite difficult for me
to actually define the scope for this
shock because more people from academia
are here then usual in my talks usually
I'm used to give talks to in front of
database administrators or developers
who are close to databases and I decided
to talk about the way how we scale
postgres and how we use sequel because I
think this is a cool language and we
have to somehow promote it and
understanding how it works and kind of
how can one use the system that is one
of the best in this area can be quite
interesting I suppose so yeah that's me
I was reading database engineering team
for quite a long time now I'm ahead of
database of data engineering yeah that's
enough so tell how many people here
knows how what solano is actually oh so
I can actually skip the whole set of
slides so we have so it just some
numbers yeah every every store every
online store is not just the nice kind
of page it is driven by something that
is sometimes quite complicated so we
have very typical layout of our
applications we have services and we are
moving
with a full speed towards the micro
services we have a possible sequel
database as the core storage for our
data and so are clusters for
implementing searching on the front end
so some more numbers something what can
be interesting here is can kind of we
have more or less more than hundred
database master instances so that
multiplies by the number of slaves and
in different data centers we have a lot
of people working at zalando already
it's 700 people when i started at
zalando it was only 55 years ago it was
one of the biggest growing companies in
the world for some time and yeah it was
quite interesting to see how we were
managing to and sometimes not managing
to keep up with the pros when i started
at zalando it was growing one hundred
percent per month and it was quite scary
so and we still exist so that means that
we managed to kind of cope with this
game so we have not very big but quite a
big cluster of post presses we have now
six terabytes of possibles data in our
databases accepted the our business
events store database that is growing
quite fast and I have to update every
time the this slide equilibrium yes yeah
big biggest challenges constantly
growing fast deployment cycles no doubt
times are tolerated and actually the
problem was that developers I used to
stability of the system so they kind of
don't write code that can have
database down times so we had to keep
databases running all the time so now we
are rolling into the direction of
scholar and reactive and all this cool
stuff and I hope our developers will
become more kind of will write more
resilient code so that database people
who be able to restart databases much
faster and much much much easier than
now for example now to shut down the
databases preparation
to do so and we are going into the
direction of autonomous teams so every
team gets now autonomy to choose their
technology their language their data
storage so kind of this is quite
interesting and tricky so we are looking
how it will work out so we are now
rolling out this change and we this
change is called radical agility at
zalando so it would be quite interesting
so yeah probably hear about this in a
year or something if it flops or if it
works well so the scope of my I already
talked six minutes it didn't want to
talk so long so the scope is to define
how it says data and how we manage to
change data models without doubt times
in the in the postgres and how we
actually shard our data and manage it to
scale without minutes I call it without
limits so a typical excess per train so
typically so what you have in more or
less every commercial organization we
will have some tree of objects kind of
you have some domain of objects that are
interconnected and on the borders of
these domains you have some things to
other domains but in most cases if we
see for example this customer and order
kuraki then this is very typical for
many organizations so usually in not
usually but in 70s the relational model
raised and it gave birth to relational
databases and the sequel was invented in
our systems I think and this became the
de facto standard to access a real date
the databases so the way how we access
data and how we actually query data from
our applications is usually nest and
removed from the kind of from the site
of the developer by creating some object
relational members they have some
advantages of course because kind of web
developers can stay in the in their
comfort zones they can kind of develop
business logic directly in the in the
environment that they are used to
develop develop business logic and but
there are some disadvantages of using RM
for example the transaction management
can become a horror you have to kind of
map everything that you have in the
database into your vacation code all the
tables and schema changes can become a
trivial there is another approach the no
sequel approach no sequel approach
actually says completely no to the
relational kind of modal everything the
whole key or key is serialized in one
document the right vente jizz of course
kind of you don't need transactions you
civilized if they're lies directly your
code but there are some hidden usually
hidden problems so implicit schemas are
a horror because when you start it was a
kind of evolving your schema the
migrations can become quite complicated
all the migration code will come
eventually your business logic in the
form of if case or some
horrible constructs and of course we
have no sequel in these structures so we
cannot really easily clearly data that
you store so there are several
alternatives one alternative that for
example Scala went they wrote wonderful
or a map or like slick it's an
impressive piece of code and if you
haven't seen it just look into the
source code it's kind of mind-blowing
closure went into the direction of a
kind of SQL where they map directly
sequel statements and allow you to
access sequel statements as if there
were functions during the period when we
were rolling out everything the we
didn't have this technologies so we went
we were purely Java store actually we
were moving from PHP and Magento based
system to these five years ago to
completely Java shop and postgres based
so we chose to go the direction of store
procedures and there are several
advantages of using store procedures so
for example are you can retrieve and
send the complete object trees like in
no sequel you can you have very clear
transaction scopes you have very
consistent data checks so you move in
all your data logic inside the store
procedure and you are close to your data
so you can really very easily operate it
inside the store procedure using sequel
and you are not related to data schema
so you can negate your data schemas and
change the tables underneath your store
procedure API without actually even
telling without the need to tell about
it to the application module so this is
a typical java application I'm talking
about java
because it's kind of yeah because I'm
talking about Java here and what we did
in the Java thing so one can actually
implement everything in some other
languages so the kind of work this is
what we did so we kind of job
application jdbc layer and so procedure
API but we had a horrible problem so the
marshaling and extracting and sending
data to store procedures was very
difficult so what we implement it was
the rope wrap up this kind of the this
is more or less an RPC layer that makes
the possible database your application
server and exposes all stop procedures
as if there were Java store procedures
so just to demonstrate very fast so you
have the store procedure that it insert
something and then you define you
interface with the kind of some
annotations and you define more or less
everything that you need here and the
system automatically marshals everything
in and out this made it so easy that
it's kind of now it is more or less
piece of cake what to write the mapper
so you can access your data using this
kind of very small signature in your
interface you can actually be used not
only in store procedure will directly be
some kind of SQL query and it will map
everything up and down but there are
some of course disadvantages so proud
operations for their yep to generate
reports from this great quote or just
generate backgrounds both this great
yeah you just write this code so because
this is a very simple interfaces we only
think that you have to write is this
honest and then you have this no
procedure
it automatically maps it so it's a not
booster so with some guys wrote the
bootstrapper when when they would strap
the big kind of project and write the
set of a store procedures they wrote the
boots trappers but it's kind of it's
needed only once then you are not adding
more store procedures all the time so
yes so developers can write bad sequel
and this is a big problem and it's a
very pity for me because sequel is very
cool language it's so kind of it's a
logical language it has execution plan
or it can do smart things but developers
don't like to write sequel and I don't
know why but I I suppose that in this
room people who are kind of more
academia base they all love sequel and I
shouldn't talk about this at all but
learning simple if you don't learn it it
will let you understand many things
especially in functional programming so
yes advantages use case driven and yeah
horror stories I usually put on some
horror stories but I'm kind of out of
time or less yeah I will skip it another
thing is that how we do versioning of
the store procedures you can think
that's kind of okay they have to put the
straw procedures all the time in the
database how do you version them and we
had this nightmare from the beginning
but then we came up with a very kind of
I think smart idea that uses the feature
of phosphorus of search path so you can
actually set the search path in your
connection and every object that is
being looked up in your system catalog
is looked up in the order of
search part in this case for example
every object that is being looked up
including stop procedures is looped at
first with this schema and then in the
publicity so you roll out this kind of
tables in this schema you use it then
you roll out another schema from the
from another version so it's kind of
completely all store procedures together
in parallel you can actually test them
separately without touching this area
and this is in production and then roll
out the kind of new version test it see
that it works then more or less removed
the previous versions and kind of throw
throw this garbage collect these things
away if you are sure that you're not
coming back so this gave us really a
huge speed because kind of the
development of store procedures became
as easy as development of Java code I
would say but of course it's not about
problems the kind of yeah the positive
thing test set out to the whole API
version so there are no into dependency
problems know what the immigrations are
needed and deployments are completely
automated so the deployment tools
tooling that we used is rolling out the
new IP is completely independently from
the kind of from the developers so it's
just taking it takes the tag of the
branch and names it's like this so any
questions so far
so yes the Postgres database is a kind
of very I like it because it saved us a
lot of problems especially during the
data migrations and data migrations we
know that for example problems it's not
bashing but it's kind of problems in my
sequel lat companies like Facebook to
implement the whole new systems just to
kind of they couldn't extend the user
table with a new column and they had to
kind of rewrite everything and to write
a new kind of database for that they
decided to do that in importance you
don't have this kind of problems you can
more or less extent your data schemas
without locks and without any yeah
problems by deployment mostly the number
of full locks that you need in
phosphorus is becoming a lower lower
with every you have released what what
does it give us the API layer it can
film data on the fly so for example if
you are kind of adding new column and it
should be already prefilled you can
pre-fill it in the store procedure and
not updated somewhere in your kind of
business logic up there in the
application it we manage to several
times to change the underlying data
model completely without the application
knowing it so just by kind of changing
the underlying and store procedures yeah
this is about this but I'm out of time
for this I think
yeah some more details about how we do
the roll out the model changes because
it's not so easy as tow procedures you
cannot just roll out new tables every
release and kind of forget about the
previous data so we kind of have some
format it's probably not so important
but it's just a small defile i call it
diff file because from the beginning we
were writing the determining the
difference between two schemas in
development database and the production
database and generating these things by
hand by automatically but it was not so
old idea but the name so we didn't
manage to do this but the name state and
this thing's are generated by but people
themselves were changing the models yes
as a kind of interesting thing an
interesting fact we didn't have a single
downtime of the system kind of that was
due to problems with migrations so and I
think this is quite an interesting fact
that you can operate relational database
doing a lot of changes into data models
or for understanding we do up to 100
data model changes so we have up to 100
Demidov files per week so it's going to
have weekly weenies and we are managing
to kind of developers are not limited by
the fact that they're using relational
database
this is something that is quite
difficult to achieve with old-style
systems like Oracle for example where
they have to look quite much more during
the integrations and you can imagine
that the actually it's quite fluid the
whole kind of data structures so from
the beginning in the database team was
trying to control the database models
much more than we could actually but
then we understood that with growing
number of developers it's not possible
so we were doing a squirrel trainings we
teach our developers how to write sequel
and how to write a kind of this change
files and this helps a lot so yes and
then probably the most interesting thing
kind of how how do we actually short our
data so the problem with so with the
advantages of big data base one big
database is very difficult to not to see
because if you have all your data in
your one database then you can access
all data you can correlate all the data
it's wonderful for our business
intelligence it very simple as exit
strategy and I don't recommend to
anybody to split your data a way into
several databases if you don't have to
do it because it's kind of it's so much
wonderful so so much fun to have it in
one database but unfortunately when data
doesn't fit into men
then you have usually a problem because
you are starting to hit the bottleneck
of the input output performance you need
much longer times for regression tasks
you may need much kind of more expertise
in operating such a big database this is
a prosperous so what what we do we
actually we short our databases so
sharding gives us the possibility to
still make the data fit into the memory
it widens the aisle bottleneck and kind
of all the all the negative things that
I mentioned before kind of gone with a
smaller database but of all TV oh I
should have looked it up I'm still
online transaction yeah online today I'm
using this terms for years now without
kind of really knowing me whatever
affiliation thank you so yes and the
disadvantages of sharding is that you
cannot join objects that are located in
different shards of course so it's kind
of if they are located physically on
different machines and that brings you
to the situation where you
much more tooling to kind of to operate
such a system so one kind of a tooling
would be to move everything to Cassandra
because Cassandra is kind of a system
that actually provides you with the
tooling that does it but unfortunately
cassandra is still not yet white as
sequel database as for example of
postgres and in the times when we kind
of we're thinking about how we will be
solving this issues Cassandra was a very
kind of not very advanced system so but
the cassandra is very cool i think so
one should definitely you use it but we
have store procedures we have the
possibility to access data kind of with
we had all the new layer with this
sprach rapper so what we did we extend
it for brand new rapper and kind of we
spoke rapper can now access a kind of
parallel databases so it's really very
easy and the only thing that you have to
do is more or less is to define which
key that we are use is your partitioning
key and knowing the partitioning key the
SPRO prepa can use the defined starting
strategy + +
I more or less recalculate to which
short it should go so yeah we
implemented several hook up strategies
like searching on all charts in parallel
for example if you don't know the primer
yeah just helping is one sharp
proximately we thought that we will keep
them less than 100 gigabyte but now the
our biggest charts in 16 charred cloth
store is something like one terabyte
risen so but yeah but it's it's
difficult to say what what is the size
because I'm just saying that yeah so
it's it's it's 60 so on one of the
biggest shark kind of clusters we have
kind of 16 charts and every shark is no
I think it's half a terabyte or
something yeah so I'm it's it's working
so fine that i even don't know how how
it
how big the database is a yes so Hassler
cups and kind of a unique shirt so
Charlotte where IDs so this order
worries we monas use the idea from
tumblr and we use md5 hashes a kind of a
partition key to extract the kind of to
calculate just the bit number and then
we use if you have two charts then you
use the first bit to kind of to address
it if you use for sharks than two bits
whether the advantage of this thing is
that your data is completely Charlotte
so you can everytime split your chart
and just change the strategy to use one
another bit and then kind of you have
two times more kind of charts that you
can use already and you don't need to
move any kind of objects between the
shards so the only thing that you have
to do is to delete the objects that are
not anymore kind of addressable by this
strategy
yes additional thing that we are now
working on as kind of the new feature in
Postgres that is a logical replication
and we are working on using this feature
what we want to do is to kind of you
know I forgot the slide what we are
working on is a project psyche is also
open sourced on our github the we decode
the logical communication stream from
all the possibility bases push them into
the queue system and then kind of decode
them back into a different system so now
we have drivers for postgres so we can
extract kind of all the changes from all
the databases and put them into one big
Hospital database for example or to
cassandra or two or a call so this makes
it even much more interesting in from
the point of view of kind of
accessibility of data and especially in
the context of in the context of micro
services and teams that are kind of
keeping their own data for themselves
and not giving access to the databases
to other teams so we need to somehow be
able to extract data from there from
from this remote data bases so to say
and to collect this data back somewhere
for business intelligence and business
at data science people of course so we
also are moving away from our data
centers to AWS that we hope that it will
give us more flexibility in building up
services and they're kind of we have
another project that is a kind of
liability appliance for full posterous
where we are using our framework for AWS
to bootstrap post via starter basis
inside inside AWS yeah so do we have
time and actually other slides are more
about kind of tools that we built for
the infrastructure so any any questions
so far yep
to question about Charlie I don't
understand what is partition key what
what I use include and do have some
problems or tasks connected with join on
this shark shall Charlotte databases yes
as I said it's not so easy to join
between the shorts so usually you try to
put your object so that they don't have
any anything to do with with each other
so for example customer so customer
information so customer and its orders
they are all they all have to be located
on one chart and customer to and her
orders should be located could be
located in another short so if you can
if you manage to define these domains of
objects that are related to each other
and use the partitioning the unique key
of the kind of the father of the of the
root of the QR key as the partitioning
key then you are mall as safe in this
case did I answer your question so the
partitioning key is usually in our case
is the customer number or article or
product ID if you are charting on
products
let's say you have bad partition key and
will your system still work if let's say
customer and orders of this customer
somehow are in different databases I
suppose that this could be caught by
wonderful tests that our developers
right yeah because yeah it's the the
problem with this thing is that it's a
it's a logical idea yeah so it's kind of
this partitioning key is nothing that is
physical yeah so it's a it's a logical
entity fire that tells you that this set
of objects should be located co-located
in one place so and it they're the same
idea would be kind of like with for
example in no sequel databases you will
also have some kind of kind of document
tree that you will aggregate together
and put it into one kind of under 1ki so
this one key in no sequel databases is
the partitioning key in our case
um
yeah I can kind of go through so these
are a lot of tools that we open sourced
for monitoring and for so this is the
list that I kind of compiled yet the
question was how many replicas do we
have in our system so usually we have so
as we had two data centers we had one
master in one slave in one data center
and one or two slaves in another data
center so the posterous works like this
that it has a master kind of instance
that accepts rights and stendhal stand
by slaves that accepting the right ahead
logs from the master and right ahead
locks are streamed kind of through the
kind of intro data central connects
connected to the another data center and
we'll play there so we always have a
possibility to promote databases in
another data center if we lose
completely liquor status and
the question is how do we handle master
failure so master failure is handled by
now semi-automatically and so we have
tooling that allows us to kind of with
one click to promote this slave to
master and this project is exactly this
your project is exactly the idea there
is that we will kind of completely
automate the whole process so it's kinda
we are now experimenting with ecce
console and actually with zookeeper to
use this system to elect the muster and
kind of this prototype works already and
we are now doing the experiments with
the experiment with the storage systems
and LWS of course one you can use the
RDS in the alws but it's kind of they
have already the support for slaves and
so on so it's kind of when we started
they didn't have it and we need to apply
a very strict German auditing rules so
we have to stay in two in this concept
in this scope of our stops structure
infrastructure that is a kind of insert
structure for I would say bigger
companies to be able to roll out
applications to AWS system yeah so that
it's auditable monitory bowl traceable
and so on yes
and what are your thoughts on the newly
introduced Jason be data type oh yes I
completely I constantly forgets to put
this on the slides so Jason be some JSON
data type was introduced already several
years ago and it was already used by
guys in Heroku for example quite
extensively for prototyping especially
the same way like like you would use
JSON inside mongodb for example with
Jason be there is a wonderful there are
wonderful advantages of kind of indexing
of the Jason be document and very fast
access to it so the at least on one note
installation the this Jason be
outperforms Mongo for example I think 30
times or something this so and
ninety-five percent of mangu
installations are single no reservations
so kind of yeah and you still with
phosphorus and Jason be you are still a
set compliant and you still have all the
advantages of kind of open possibilities
to split your JSON documents when when
you know that you're interfaced
otherwise you can actually split
Eurasian to comment interrelations if
you need it or to keep it
you motivated the user ceclor because
it's such a nice language for
formulating queries to figure out some
data that's one side of the coin the
other is you just mentioned it acid
which of the letters and how much do you
use them no no it's another tricky
question about the meetings of the
letters simply feel like how much do you
use transaction Alateen in so the we try
to stay as transactional as possible so
at least we claim that we are completely
acids compliant the kind of from from
our understanding so where we could lose
transactions would be during the
situations where we promote the master
and the application wouldn't kind of
understand that we kind of switched the
kind of we switched to a new master and
then and go one writing to old master so
it's kind of these situations can lead
to the partition so network partitioning
could lead to kind of animal it is that
would lead to data failures or losing a
fit of the data but we hope that this
network partitioning actually these
problems exist for any system more or
less and and having one master is quite
a big advantage so I think we are more
or less acid compliant I don't know did
I answer your question or
um unfortunately that's all the time we
have thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>