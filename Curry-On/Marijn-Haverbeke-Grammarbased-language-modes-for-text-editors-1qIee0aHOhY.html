<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Marijn Haverbeke - Grammar-based language modes for text editors | Coder Coacher - Coaching Coders</title><meta content="Marijn Haverbeke - Grammar-based language modes for text editors - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Curry-On/">Curry On!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Marijn Haverbeke - Grammar-based language modes for text editors</b></h2><h5 class="post__date">2017-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1qIee0aHOhY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the ders to look specifically the
syntactic elements of the language that
you're writing are color-coded so to say
are highlighted in different colors
giving you an additional source of
context as you're reading the codes
whether a given word is a variable a
type part of a command part of a string
and how these kinds of highlighting code
like this in an editor is a problem in
itself is people are getting quite
expecting quite a lot from their editor
these days like they're not just
supposed to highlight strings and key
words anymore they should also do
context sensitive things like whether
something is a type or a regular
variable whether it's a local variable
or a global and so on and so on
so I'm yeah
defining these is not entirely trivial
especially since if you are writing an
editor you don't have to deal with the
quirks of one language you have to deal
with the quirks of every language and
while programming languages do tend to
follow a certain kind of discipline
grammar structure most of them at least
every single one has some weird
exceptions and weird rules that don't
fit into most generalized parsing
frameworks so um this isn't trivial code
mirror comes with support for which is
my editor comes with support for over
120 different languages with some of
them having a bunch of sub-dialects as
well that is quite a maintenance burden
and as I set out to try and find a less
error-prone and more convenient way to
define these and in a way these are
parsers as they they do a lot of the
same work that a normal partial that you
might find in the compiler does but they
have very different constraints so a
file might be big but instead of parsing
it in one go you'll have constants
localized edits usually around one place
and the like it's not necessarily parse
tree but a token stream it has to be
updated for
every change quickly preferably because
you don't want to freeze the interface
that the user is using people also find
it annoying if it's like black for a few
seconds and then flips into coloring so
it has to be able to respond to
localized changes immediately the input
will often not be valid as you're
editing a text file you're not going to
have it in a syntactically valid form
all the time and you still want
highlighting to more or less work even
if you make a mistake so the I think
every editor has a solution that relies
on incremental parsing so you have your
partial state as a value that you can
save and that you can resume multiple
times so that you keep your partial
States different positions in your
document and s it is edited you just
resume the nearest States and parse from
there instead of always starting from
the start
and of course saving all these states
has to be relatively low memory you
can't save a giant data structure x
times it's just it's going to take away
too much memory one requirements that is
more cumbersome than it might initially
sound like is it has a have a low
granularity many editors use end of line
as a like resumption point which is not
an ideal a situation for the region that
a line can be several megabytes long if
you have a weird minified file or
something but that's this what a lot of
editors do so if you have say comments
there's a thousand lines long you want
your parser state to be you're going to
parse to be stoppable even in the middle
of this which you would conceptually
consider with token so you can't just go
one token at a time basically in the
traditional sense and of course it has
to be error tolerance which I'll get
back to later one way to conceptualize
such a highlighter is to
you see it as a function from a given
state and an input position to a token
which is like a little piece of this
input with styling applied and a new
state after that token and then you can
just repeatedly call this this function
as you iterate through the document and
occasionally set aside of state for
later assumption made a little animation
which is trying to show this so here you
have a document the blue stuff has been
highlighted the black rectangle is your
viewport so as you start up you're only
seeing a little piece of the document so
you don't want to go off and highlight
the whole thing because it might be 20
megabytes in the user might might never
look beyond the fifth line so um you
start by just highlighting the first
screen full and then as you scroll down
you can like slowly continue
highlighting this is all still
relatively like incremental you don't
have to do a lot of work in one shot and
when the user makes a change say the red
line changes you throw away all the
information below that change and take
this the nearest state above the change
and continue to the end of the viewport
from there this has a nice property that
if changes happen within the viewport
which is the case at least 90% of the
time you're the work you have to do is
bounded by viewport size so you don't
have to do and you can usually just do
it synchronously as you refresh the
screen because it doesn't always hold up
say you're looking at the bottom of the
few and you're have a collaborative
editor and someone else is changing the
first line so you do want to set things
up so that this work can be done in the
background and the user can keep on
editing as some background processes is
catching up to the culture currently
editing you can usually even change
teeth like highlighting is somewhat
state dependent but not hugely state
dependent so if you are at the bottom of
the document that you want to draw
something but you don't have a suitable
state you can just make up some state
and start highlighting and then later as
your background
catches up with your position you can
fix whatever is whatever is different so
if someone added a block comment marker
on the first line then yeah the whole
document will look differently but
usually it looked almost the same so you
you don't need to immediately get the
accurate state the techniques that exist
for this are like carbon one which my
editor uses is just write a function
like the one I described before by hand
which maintains the state manually and
is a lot like like souped-up tokenizer
basically one the only other system that
seems to do this is that the IntelliJ
family of IDs they also allow people to
write the Java program that does well is
basically tokenizer with the states and
conditions unlimited Slyke has limited
access to the input stream and needs to
return one token at a time and the
downside of this is that it's like
cumbersome to write a new Mouse
error-prone you can't really expect the
average user who has a problem of
needing to highlight a new language to
write this and it's one of the reasons
why this huge massive Mouse for code
mirror is quite a maintenance burden
they're often copy-paste it messes but
people who didn't really understand what
they were doing the upside is that it's
very powerful as you it's hard to to
write a framework that captures all the
possibilities that you have if you just
write a program by hand then the older
editor systems like Emacs in film they
they kind of show the history of this
where at first it was like state of the
art to highlight just key words and
strings and maybe comments and then
expectations increase today they started
with some ad hoc clutches to do just
that and then they added more clutches
and then you it's not pretty
I don't understand them entirely because
they're just
weird but these are not systems that we
want to be replicating a more modern
editors they all have very similar
format some even are compatible among
each other like sublime text visual
studio code Adam and I think the one
that pioneered this approaches is text
mates they have a formats which
basically allows you to define a number
of contexts and from each context you
have a number of regular expressions
which define if you are in this context
then you see this regular expression
it's dead to open and in addition to
like defining a token leaves these
regular expressions can say and then you
have to enter this context or you have
to leave the current context something
like that
this is relatively nice format to work
with a lot better than the alternatives
and relatively 16th but it's not
entirely general in that it's really
painful to - I was showing an example
actually to express sequences of things
so this is a very simple like operator
expression languages which has like
numbers variables and operators and
parents parenthesized expressions so the
starting context this is expert and it
says if you see digits call it a number
if you see a letter s' call it a
variable if you see a parent seasons
move to there parenthesis parenthesis
context an efficient operator just
continue here and then the parent expert
context it's as if I see closing parent
go back so it is this tracking of
context is kind of useful for things
like indentation and sometimes also
styling but what this for example
doesn't Express is that an operator is
supposed to appear after a variable or a
digit so it couldn't for example
distinguish between unary and binary
operators you can kind of make it do
that but things get for both very
quickly because you're basically
building up a complex graph and you're
doing it manually like allocating names
for all the edges and
building the transitions manually it's
not a lot of fun so my idea was we have
a relatively boot ID of how to write
grammars in like a context-free grammar
style grammar notation this has the
advantage that the kind of hierarchical
relations between things are naturally
expressed you just get like a new
production which is for example
parenthesized expression and you can at
any point know whether you're inside of
this it's much better about expressing
sequencing because the remnants tend to
have a sequencing operator and much more
succinctly you don't have to mess with
reg X so did this these are trivial regs
we usually you'll be doing all your like
clergies things in array X and that just
doesn't scale they get hugely unreadable
complicated and big and so we want
something where usually can write the
grammar and then there's a little tool
that compiles it into a machine
something like this and in the editor
can run that and it should become a lot
nicer to writes such grammar so this is
an example of what my syntax look like I
pasted it on a partial expression
grammars rather than context-free
grammars which are somewhat different in
that in a context-free grammar if you
have a choice operator to the pipe
usually it means a basically these are
all tried in the same you really have to
start talking by the way because at some
point I'll just be like finished and
there and there so if you have something
to say to stretch your hands the the
choices are all basically conceptually
parsed at the same time and if multiple
ones match this isn't an error and
ambiguity whereas in partial expression
grammar it's more like algorithmic it
says try this one first if it matches
that's that's what we got here
otherwise try the next one and so on and
similar with repeat operators like
Pleasant star and question mark
it will greedily match as many as
possible and then stop rather than
trying to like see if one matches if the
like farce multiple variants end and see
if there's an ambiguity this is only one
hand nice because it allows you to treat
a grammar more like a program and then
heck around with this and without
getting ambiguity errors all the time on
the other hand this is kind of a food
gun because it's very easy to
accidentally have an earlier like if you
put your identifier rule before your
keyword rule then all your keywords will
be seen as identifiers because they
match the same like lexical syntax so
it's kind of a trade-off but for my
purposes it seemed to work pretty well
another thing that that this style of
grammars have I didn't think load anyone
any here because they didn't need one
but they have a look ahead operator
which becomes really useful when you
want to disambiguate things that kind of
looks similar and then probably the most
important thing is that there's no
separate tokenizer this grammar is the
whole grammar there's no need for a
tokenizer it expresses the tokens in the
grammar so it's scanner lists grammar so
this is usually called and that's a nice
because it removes one content context
you don't have to worry about writing a
tokenizer and a grammar you just revise
your grammar and you're done and on the
other hand this has the property that
grammars without a tokenizer can be
combined they compose like they're
closed under extension and embedding and
it's much easier to mess around to these
for example if you have a see grammar
and you want to oil the C++ grammar you
can start from the c grammar and just
add more rules and extend some existing
rules yes
so one of my first grammars had tried to
write through this is Cianna c++ grammar
and it's not a shtetl in that way that
you can keep a symbol table so I have to
use a terrible hack where I try to
syntactically distinguish them by
looking ahead seeing if there's type
expression and then an identifier and
then it's going to be one of these
horrible C type declarations this is not
hundred percent like there are things
like foo space star space something
where you just don't know what it is it
could be a multiplication it could be a
declaration of a pointer but so I use
like if there's a space on both sides
it's probably an operator if not it's
probably a type and it works for most
code but it's it you could even if you
had a symbol table you don't want to go
ahead and read include files from your
mode somehow and parse those yep that
would be impossible anyway so this
you're basically stuck with ugly
approximation for to see
like something where there is an editor
not work
I might must walk or be able to fire the
whole grammar like this if I just wanted
to get the keywords and maybe some I
don't know maybe some other things are
simply numbers all right is there
I would say that this is also a
relatively nice format for this you
would just write the grammar that is
basically a stream of tokens so you
would say token equals string number
variable something keywords well keyword
before variable of course or an
underscore which is like any character
and then that's with a pleasure or star
after it and it'll just continue
matching tokens throughout your file and
it's pretty much what you need in this
case and I think it's still more six
things than the Reg X syntax for that no
it doesn't do that like that requires
you to define kind of templates with
holes in it and you can't really do
rightish for my grammar I mean I mean
you could extend it to do that I haven't
thought about it
I mean I've written some imitation but I
yeah it's it's very good question so you
can see some of these extra annotations
after some of the rules like for parents
it says context which means as long as
you're in this this production save some
context about where it started and what
kind of rule it is and then there's
style number and style variable which
are obviously just style these with this
given a name which then gives them a
color in the actual output but yeah I've
written annotation functions from as
basically as functions from context to a
number the amount of imitation and ever
is relatively well you still have to do
quite a lot of obscure things like pay
if an opening parenthesis is followed by
some text you'll probably want to indent
the rest like aligned suit appearances
instead of in entity bits relative to
the start but that works quite well so
that's why I said that this tracking of
context of which rules you're in is very
useful also for other kinds of cooling
um so where was I
yeah and yeah so since I think I'm the
first one to try to share it we use
these for highlighting notes there's not
much tooling and they are a bit of a
pain to the bird so I've been writing
some ad hoc helper things I think they
would lend themselves to relatively good
tooling where you can just like provide
an example where it goes wrong and see
exactly how it parsed it and then you
can usually figure out what you did
wrong but doesn't exist yet oh yeah yeah
I was inspired quite a lot by that for
this the that's really a partial in the
classical sense that they're they're
doing something quite different I'll get
into why my partial works very
differently in a moment so yeah I took
inspiration from them but it's not
something you can directly apply to the
kind of grammars I'm using
yeah yeah yeah I'll get to that later I
need some more context to explain how
that works yeah all right I think that
needs more heavyweight tooling and very
very language specific tooling because
it's like it involves from any languages
type inference and stuff like that so it
gets out of scope for what I'm trying to
do here I think it's usually you do want
a very quick simple highlighter and then
you can build stuff on top of that that
shouldn't necessarily want like be
responsible for the highlighting because
if you want to build up real accurate
abstract syntax tree trees for example
you don't want to do this thing
initially as the users editing you you
prefer to do that in the background so I
think these are best conceptualizes two
differents tasks
49
conceptually the editor doesn't actually
do it at every line it just does it
every end lines and then scans backwards
when it needs a given state depends on
the state that I haven't really founded
a problem so far okay so one issue with
this is that a standard parsing
algorithm for partial expression grammar
is a kind of backtracking thing where if
you see X or Y you first like you
remember where you are you try to parse
X and if that's else you go back to
where you started then you try to parse
Y so you have arbitrary backtracking
effects most grammars top-level rule the
loops on something like statements star
and if your whole file is a single
statement you aren't sure that you're
actually yeah that your current parse is
the right part until you first the whole
file which of course unacceptable
because you want to immediately start
outputting Styles without yeah without
first parsing the whole file and every
change reporting the whole file so we
need to change the semantics of partial
expression grammars in a very crude and
scary way namely what I do is I am I
first distinguish between parts of the
grammar that are regular language so
which could be matched with a regular
expression and does it aren't like if
you have a recursive rule there's no way
you can match it with a regular
expression also these rules that
actually denote a token they need to be
separated because they need to be their
own thing because they have to and
that's this this token information with
all the other things things like digit
and letter it can just be in lines and
then you have in expression we can
inline the whole content of num and of
for to just be part of that like be a
single edge in that expression so we
compile it to
of something like this where like the
small details lexical level things gets
in lines into bigger rules and that is
already quite close to the format we saw
before but you do have the issue that
you when you write these grammars and
this is like the main issue I have with
my solution still you have to keep in
mind how much of this is regular this is
actually disambiguate from the current
point and if not you have to manually
insert Luka heads to to figure out to
make sure the right branch is taken and
I try to look into solving that by
automatically finding ambiguities but
the problem is that some ambiguous are
actually intentional because you do want
to sometimes you do want to have this is
like how partial Russian grammars work
something we do intentionally needs the
first one to match even though another
one could match and sometimes it's an
accident so I'm still looking about for
for like syntactic changes to make this
more obvious but it's workable I only
had one search perk as I'm actually as I
was actually working on a grammar so
it's practical even without this but
this is definitely suboptimal but if you
have this like simplified graph you can
output it in a format something like
this though - more graph oriented rather
than stick oriented and then you can
have an interpreter which keeps like a
stack of active which are like notes in
this graph like points where you are and
so it's like not a regular state machine
whether state machine the Bills have a
stack as it runs through a single rule
like this each of these sub graphs has a
read at a read note where it's like
returning from the current rule to
whatever pushed it on to the stack and
alongside that the state needs to keep
track of the context so for example I
specified that this parents
production is a context so whenever it
enters that it pushes some stuff onto
that context stack and that's that's all
the state it keeps and each time it
needs to really token it just moves
forward until it finds an edge that
actually advances a stream and then at
that point it commits to the edge so
that's considered like the direction
this part is going ablaze the state and
returns a token which is good because I
wouldn't have gotten off through the
talk and there's 20 minutes no that's
hard to determine where whether you
could like reuse an all-state at a given
position although actually if you if you
have said so far codemirror states were
just like controlled stuff from from
widely different programs so it was hard
to compare States but if you have a
compare state function you could
actually it would be a really great
optimization you could say okay I've
arrived at the same state so I'm just
going to stop working okay
and it could save a lot of work in
performance the answer I think a
tokenizer is probably always going to be
faster than than matching a bunch of
separate drag expert one by one because
you can usually dispatch in the first
character and do much less work so that
is faster like in my benchmarks about
three times faster than then reg X based
solutions they just get a few lines of
input in my system because you're
supposed to tokens shouldn't be bigger
than a few lines anyway so they're kind
of limited because also JavaScript
projects this is all written in
JavaScript are terrible in that you
can't match them lazily like start on
some substring and then when they reach
the end feed the more input so you
always would have to concatenate the
whole document which is a tree data
structure that it's a practical concern
I can't give you the whole document so I
want to quickly get back to the to the
error tolerance but I'm doing here is
that when when it can't make progress
when it for example gets stuck in and
out and none of the edges that go out
from there match the currents input or
the first do is it'll try to like
basically insert tokens by skipping
edges like it will skip an edge and see
if from the next node out it can it can
match some of the inputs so that in that
case for example if you have a missing
closing parent or missing closing brace
it'll randomly happen to skip that edge
with well usually you'll it'll be one of
the nearest edges it will skip that and
see if you can make progress after that
and if so it will do that so like it's
basically inserted a conceptual token
there if that doesn't work it will try
the other way like skipping pieces of
input so
you can mark some of your rules as like
base tokens in the grammar and it will
synthesize a big rule with all these
base tokens with or operators between
them or any character appended at the
end as a kind of fall back and it will
just push one of those onto the stack
when it's can't make progress otherwise
which means that at that point it will
just parse something that may look like
a string or a keyword or whatever and
color it properly and then it will try
again to make progress after that so it
will just basically discard pieces of
input but still color and that works
relatively well so I would also solve
the closing quote for the string thing
because strings are usually built out of
multiple edges and the closing edge
could be discarded too
if it's not there although if it's a
multi-line string it would prefer to
just barge the rest of document as a
string but that's maybe the expected
thing to do if you don't close a
multi-line string yeah that was my next
slide it's a big problem with scandalous
rumors that white space has to be in
there somewhere so you could either in
every of your rules
insert white space something white space
something white space it is very
cumbersome another convention is often
done is to take it to the end of all of
the like leaf rules that token style
rules which doesn't work in my case
because I have to like I use the
matching of a rule to color something as
a say keyword and I don't want to color
the white space after a keyword as a
keyword so I took hints from on there as
well they have a special kind of rules
like they distinguish between syntactic
and lexical rules and in syntactic rules
they just insert a given Y space
expression which is this like skip
whitespace index that I have here all
the rules that are inside the skip
whitespace block have this expression
whitespace inserted between every like
sequence operator and in repeat
operators after the repeated expression
and that's causes it to virtually end up
in the right place and be skipped at
every point where you want it to be
skipped
not currently but that's a big design
problem in my editor and a bunch of
other editors that's line granularity is
just problematic it's it's not a good
way to do to like limit work this is my
last slide so I did this quick like file
size comparison between like the basic
50 lispenard is very very crude so I
searched for like third-party signals
first much that also tries to highlight
things a bit more thoroughly and they
came to 82 kilobytes sublime text
standard C++ no 260 my C grammar is 4k
and my extension on that is another 4k
so this is definitely more sixteenth
whether it's also more pleasant to write
I'm not entirely sure yet but you do
have to write a lot less and if I
compile it with my tool I get a 22 K
graph which is unreadable but also very
compact so that's looking relatively
good I put it on github the little lower
mouse link it's not very polished yet
but it works and I'm using it to to
deliver some outs for our customer so it
should work better in future yeah
it just sort of looks at it sideways
I don't get the questions very consider
extending this to be able to write a
journal right
no I've written a code formatter in the
past it was a huge pain and I think it's
a different problem again because the
grammar doesn't really say where the new
line where the new line should go or
stuff like that like if you have the new
lines already rhiannon tation is kind of
in scope so I usually write an indenter
along with each mouth but finding places
to put new lines is another problem I
think
that movie the ends yeah yeah yeah so
it's it's I'm not currently using that
but it would be a rudder of his
extension to like track scopes as you're
running the grammar Alfred it that and
keep it like data structure hierarchical
I'm already building up the data
structure but I'm currently discarding
it again and you could keep that around
and use it to do things like this in
these it's would make it's easier to to
match some non-trivial things rather
than running a red X backwards and
things like that
but for example if you want to select
expressions usually these kinds of
things
ignore operator precedence because it's
a pain and it's not necessary for
highlighting and you'd have to write
your grammars differently I'm not sure
how hard that would be to get operating
operator precedence rights in terms of
expression sighs all right I said we
have a few more minutes I think I am out
of flight and
yeah I can't try to show it so it looks
something like this
these are shared shared edges and then
here you have your uh no deterrent these
are statements like unab tokens this
like I thought that you can mark things
as being a token so this is the token
fallback is it counts match anything
then here we get a preprocessor line
whitespace and this is the non compact
readable format the other one uses
numbers for node names and and the
duplicates edges so you can't read it at
all but here you can kind of see a
statement can be a class and then it
goes to statement one which is back here
it matches whitespace after class goes
to statement two and then it's a figured
out some align as syntax continues to
state and four and so on and so on so
the grammar itself four so it starts by
saying that it extends the see grammar
and then it's very especially defining
this statement that we just saw the
Grover
probably is just JavaScript so I guess
yeah I think so so I mean they're
definitely useful even if you don't use
it in IDE like just providing a decent
code editor or web page is often a
useful thing to do
I mean web technology has a long way to
go but on the other hand it also does a
lot of things writes that desktop
editor's often still have problems with
like advanced Unicode magic and stuff
like that you just use the browser
framework I hope that website continues
to get better but it's it's definitely
slower because you're limited to and one
big problem is also that you can't
really ship a compiled version of a
Unicode library to a browser so you
don't really have a Unicode database
which you actually need to provide a
good editor so I think that's the main
stumbling block but there's also
standards work on going to actually I
mean all browsers have a Unicode
database that they could just export it
to the JavaScript somehow I think we're
out of time or can we do another
possibly yeah if you have just the
stupid interpreter running a simple
input format that's probably a great
example of something that can be
compared to webassembly currently
marshaling data between javascript and
web assembly is a bit of a pain so that
might still make it's relatively
inefficient but they're working on that
it might might at some point become a
usual strategy done okay
thanks for listening
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>