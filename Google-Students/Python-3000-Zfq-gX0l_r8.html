<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python 3000 | Coder Coacher - Coaching Coders</title><meta content="Python 3000 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Students/">Google Students</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python 3000</b></h2><h5 class="post__date">2008-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Zfq-gX0l_r8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I know you've all come here to see Guido
of course but I wanted to first say that
this is the latest in our series of
talks about programming languages topics
at Google the goal of the series of
talks is to have everybody who knows
something about programming languages
that Googlers in general don't know come
up here and give a series of talks
obviously we're very lucky here today to
have we don't then rossum the benevolent
dictator for life of Python but you
don't have to be guido van rossum to
give this talk if to give a talk at you
to give this talk you do but to give it
time but but to give a talk at the
series you don't have to be so pleased
if you have ideas for talks if you want
to give a talk come up and see me my
name is or email me my name is Jeremy
Manson and again my ok so now I should
move on to the actual meat of the talk
here again we'd ova an awesome is the
benevolent dictator for life of Python
he is the creator and father of Python
and we're very very lucky to have him
able to give a talk to us to here today
and here he is Thank You Jeremy thanks
for giving me the opportunity to do a
preview of my talk which is going to be
a keynote at python by the python
conference next week reminder for the
Googlers we're going to put this up on
google video so please don't ask any
google sensitive questions quick
overview of what I hope to be talking
about and I make shall make sure that we
will not skip the last two bullets what
you can do today and questions what
happened since last summer there mean
I've been I started giving five and
three thousand talks well I really
started giving Python 3000 talks about
seven years ago in 2000 for a very long
time it was purely a daydream it was
purely conceptual was going to be the
next big thing last early last year we
decided to really go make an effort fix
a set of features and actually start
implementing and so gradually over last
year the plans became more solid and we
had various revisions of a schedule
give a little bit of a timeline I'll
give some highlights if you have to
leave in 10 minutes stay until the
highlight slide is finished then I'm
going to have a long laundry list of
various things that will definitely or
most likely or in one or two cases
potentially make it into this new
release and the things that are sort of
most interesting from the developers
from the from the end users perspective
I'll try to say a bit of them how you
turn your Python to code into Python 3
code which is not completely trivial but
also doesn't have to be a tedious
completely manual process and I'll start
giving some hints and over the next six
to nine months those hints will will
probably improving quality on what you
can do to your code today to be ready
for Python 3000 basically to make the
final transition as easy as possible so
we started with having lots of
discussions at some point I actually had
to say we've had enough discussions
let's get down to implementation work
and I had to say that several times and
I think the last time I said it was
around Christmas 2006 and since then
it's it's really been very much nose to
the grindstone work out the details on
features that we know we're going to
have and work on the implementation and
sometimes the implementation actually
informs the specification as things go
we did write quite a few peps still not
enough in my view and I think we were
pretty much on schedule in terms of
writing code but it's certainly going to
be a big effort between now and June
which takes us to the timeline slide I
hope that by April this year will really
be done with the sword of the feature
proposal process in the feature
selection process should be done soon
after that because that typically goes
hand in hand we don't collect all the
proposals and then there is a long pause
where somebody selects them we discussed
them as they are being proposed and as
they are sort of finalized that
also means they are accepted so I hope
to be to be able to complete that by
April then by June I hope to actually
have a first alpha release then I'm
giving myself and the developers another
year to sort of work through the
feedback shave of the the sharp bits
improve performance because at the
moment we're really feature driven and
performance sometimes goes by the
wayside increasingly get users to
actually try the new Python with their
source code with their applications and
then hopefully in 2008 in June were
somewhere in the middle of the of next
year we'll actually have a release that
we can be happy with that doesn't mean
that at that point everyone who is using
two point X will be forced to upgrade
there's going to be a python 2.6 release
actually somewhat earlier than the
planned Python 2 3 point overlies
although you never know release is tend
to sort of fluctuate a bit 2.6 is the
first release that is going to make an
active effort to also incorporate things
that will help you transition to Python
3000 it will in some cases have options
that turn on warnings for things that
are going to disappear and in some cases
port features from Python 3000 will
actually be backported into 2.6 and
unless the transition goes really smooth
for everyone immediately it's Syria
likely that there will also be 2.7
release at the usual schedule for the
two point X releases so the highlights
and I have I have more slides on each of
these but print is going to be a
function that is sort of I just
implemented that last week and I really
have to get used to it still but it is
the right thing to do dictionary views
are even fresher
oh by the way a single star means that
there is some working code but it's not
complete and two stars means that it's
currently completely vaporware but we
know we're going to do this question
marks means that it's not just vaporware
we're also not sure that we're going to
do that but there aren't any ? so this
slide dictionary fuse is another thing
that will impact many people's code
basically dick the keys and items and
values will return something that smells
like a set route ER than like lists
comparing objects is also changed at
least a default comparison will sort of
be more type safe and less lenient
probably one of the biggest things
certainly in terms of implementation is
unicode we're going to move to a more
java like model where all strings or
unicode and we have a separate bytes
data type which is more like an array of
small integers than like a string now
that means that we also have to
implement a new i/o library which I'm
actually pretty optimistic about some
things that have already been done
integer unification which means that
there's only one integer type there's no
more long no more long literals and you
can be pretty you can get pretty close
to that even today in Python 2.4 you in
2.4 you almost never have to cast things
too long anymore and you don't have to
cast them back from long to int because
most of those conversions are taken care
of by the system in 3000 the long type
will completely disappear integer
division will return a float that's
being a long-standing wish of mine
actually you can turn that on in Python
too since I think probably since Python
21 or 20 even you can turn 21 Thomas
knows everything
but not too many people use it and then
of course there's lots of other clean up
like string exceptions no longer exists
classic classes no longer exist we're
changing the race statement and so on
and so forth so a little bit more on
many of those items and a bunch that
didn't make it to the highlights page
prints print is a function we had a
discussion and there were a couple of
competing proposals one of the proposals
was that they were going to make it a
function we should also drastically
change what it does maybe not insert
spaces between items maybe have printf
functionality in the end we decided to
actually go with very simple
transformation where we have a print
function that is just as convenient as
the print statement is currently so in
most cases all you have to do is put
parentheses around it by the way you
won't have to edit your code yourself we
have a conversion tool and while the
conversion tool is far from perfect this
is one of the things it can do really
well there's this funny business with a
trailing common that suppresses the
trailing newline you can simulate that
by I mean print function will have three
different keyword arguments it will have
end which is the character that is
printed at the is output at the end of
the list of arguments which defaults to
a new line there is sap which is the
thing that gets input in between items
which defaults to a space and there is
file which is the file where it's going
to print we printed to which defaults to
whatever cystal stood out is at the
moment so these three forms of print
syntax all translates to very straight
forward calls to the print function and
there's some functionality that you
can't easily do with the print statement
at the moment that you can do by setting
for example the sep keyword to an empty
string
we can automatically translate this the
only place where that fails is in it
turns out the print statement has a
couple of bits of cleverness where it
works with an attribute on the output
file named soft space which is mostly
hidden but it's actually accessible to
end users if you really want to and the
soft space is attribute is used to delay
the outputting of the space between
items until you actually know that you
have a next item that is pretty murky
semantics and it means that everybody
who implements a file like object at
some point finds that they also have to
support the the soft space feature so I
decided to just get rid of that it does
mean that there are a few corner cases
like if you print a string that ends in
either new line or a tab character and
then comma and another item the current
print is cleverly suppressing the space
between the two items the print function
will intentionally be slightly Dumber
about that so I actually when when
converting the standard library and the
standard unit tests ahead maybe I think
on maybe five cases where I had to fix
this manually in the code and usually
it's very straightforward so dictionary
views this has a star because the
dictionary views currently while
implemented don't quite behave like set
objects yet they can be compared to set
objects but they can't quite implement
they don't quite implement all the
operations that you expect on set
objects like Union and intersection they
do sort of have the basic functionality
you can iterate over them you can do a
membership check and you can compare
them to another set for equality which
is actually a relatively big deal in the
past if you wanted to see if two
dictionaries have the same set of keys
you would have to make a copy of each
dictionaries keys into a list and then
sort the lists or make copies in two
sets if you're sort of using a more
recent version of Python like 23 which
has assets module or 24 which has sets
as a bill set as a built-in time and
then you had to you could compare those
those two sets or there's two sorted
lists problem with that is that if you
have a large dictionary you end up
making a large copy of all the keys what
you can do with the keys view is
actually you can just compare the two
keys and because they act as sets it
will automatically and efficiently
compare whether the two sets have the
same same elements whether one is a
subset of the other and vice versa just
a mathematical definition of set
equality we're doing the same thing for
items items also returns a set view
values of course cannot quite return a
step because you can't have duplicates
and you'd like those duplicates to show
up when you iterate over it we continue
to maintain that the invariant that if
you iterate in parallel over the keys
and the values that you get matching
keys and values at the say at the same
position in the sequence as long as you
don't of course modify the dictionary
while you're iterating over it this of
course is all being borrowed from the
Javas collection framework I'm not
afraid to borrow stuff from other
languages never have been i don't think
i would have gotten anywhere if i try to
invent everything myself so the
important part of the keys are
dictionary views in general I expect
that keys and items are going to be the
most important ones and values are going
to be only rarely used in practice
mostly probably in unit tests that's
where I found most of the uses these
these few objects are very lightweight
because they're basically a structure
containing one pointer which points to
the original dictionary and so it's
rating over a keys view or iterating
over any of those three views was
actually trivially
implemented because I even though I
remove the in Turkey's inner items and
inner values methods I didn't remove
their implementations and their
implementations are still useful as the
it as the iterators over the view
objects so because I actually did some
of the work on this over the weekend
there are two unimplemented parts of it
one is as I mentioned the set semantics
are not complete you cannot check
whether your keys object is a subset of
some others keys object or another set
you can only check compare them for
equality the other thing is that we
currently have about 15 or 20 failing
unit tests still I expect that most of
those unit tests are failing for very
trivial reasons like their assume I mean
what a lot of code does is it assumes
that Keys returns a list and then it
compares that the unit tests especially
often do things like they create a
little dictionary they mess around with
a little bit and then they test that the
list after sort of the keys after
sorting have a certain value when they
usually just compare the keys object
with a list of constants that doesn't
work anymore you could fix that in two
ways you can explicitly cast the view to
a list object and that sort of fixes it
solidly you can also replace your list
constant that you compare it with a set
constant which you haven't mentioned yet
but which is one of the later slides we
have set literals now so much for
dictionary views the default comparison
i already mentioned that in the
highlights equality and not equal of
course compare whether i mean you have a
default comparison and you can overload
your comparison you can implement your
own comparison any way you want it I'm
not touching any of that but the default
comparison that you get when you derive
from object and you don't overload any
comparison operators is changing quite a
bit in Python to even in Python one even
if I thon 0 i think if you compare
two objects of different types for with
an ordering relationship we just compare
the address of each object and say the
one with the lower address comes before
the one with the higher address that
turns out to be mostly a useless
comparison it can give you a sort of a
false sense of security that if you sort
or compare something and you don't know
what the types of the objects are it's
not going to throw a type error but
actually you want to throw a type error
because most of the time if there are
objects of different types that aren't
really comparable that haven't
explicitly programmed how they should be
compared with each other the default
comparison is it's just giving you
random results and maybe in one run this
object always shows up before that
object but another run because you had
slightly different input data their
allocation on the heap is different and
the object that was smaller first is now
certainly larger and you can have all
sorts of bizarre situations where you
have flaky unit tests so in particular
this means that you can no longer
compare or sort integers and strings
just like you can't concatenate them or
do anything else with them before
converting that's pretty much it in
practice I have not found that this this
affects much code I mean I have found
very little code in the standard library
that actually relied on this default
comparison existing except again in unit
tests that were specifically checking
this behavior which always feels good to
rip out code so then we get to the scary
thing and it's scary for me because I
haven't started implementing it yet I
think it's also scary for application
developers because it can potentially
affect application performance
application semantics it's it's going to
be one of the bigger thing
for converting co2 Python 3000 now if
you're not using Unicode today in your
application you're probably pretty safe
but if you're using Unicode today sort
of everything you know about keeping
track of encoding Xand which strings are
Unicode and which strings are not
Unicode will probably have to be changed
somewhat so again we're borrowing
heavily from Java there's going to be
one string type named stir but again its
implementation will be most likely that
of the two point X Unicode
implementation I will have a separate
bytes type which is new brand new
although its implementation resembles
resembles most closely the array module
that has been around since probably
since Python 15 or so you can only ever
go between these using and encoding if
you compare them or concatenate them if
you compare a bytes object to a string
object will just to throw a type error
is this is yet another place where that
the change to the default comparison is
actually helpful because it just points
out that you're doing nonsensical
operations quicker what will completely
disappear and this is actually a big
improvement and the main motivation is
the endless problems you have in current
Python applications that use a mix of
8-bit and Unicode strings and
occasionally encoded Unicode ends up in
a bit string so you have characters with
a high bit set and then suddenly they
will not interoperate happily with
actual Unicode strings the thing is if
you have an 8-bit string that only
contains ASCII characters you can
concatenate it or compare it to Unicode
string just fine and it will have this
sort of the proper semantics but if you
have an 8-bit straining that actually
uses bit number eight of at least one of
the characters in that string you
certainly cannot compare it or
concatenate it to a Unicode object and
unfortunately
this often happens after your
application has been deployed especially
web applications the developers live in
the u.s. they do a lot of testing they
type in their name and there's never an
accented character around then their
first French customer enters their login
name and everything blows up painful so
we hope that by forcing you to sort of
do all the conversion between bites and
Unicode at a much more specified point
slightly earlier in the life of the
strings you won't it mean you basically
if you make a mistake and you do not
explicitly convert your bites to Unicode
typing a name without accented
characters will also not work so you'll
you're much more likely to actually have
effectively tested your application for
all use cases this has caused a lot of
discussion and I think that's still an
understatement there are lots of
different implementation choices my
personal choice would be will go with
the basically the Unicode data type that
we currently have in Python two point
well since Python 2.0 it hasn't changed
a lot it uses an eternal representation
that is either two bytes per character
or four bytes per character when it's
two bytes per character technically it's
utf-16 because you can have surrogates
in there if you care about that but the
surc the surrogates for most practical
purposes look like characters to the
application unless you really go to dive
deep into Unicode that is one possible
implementation another possible
implementation would be to keep a
similar thing but actually have three
internal representations one that is a
single character why single byte white
one is two bytes wide and one that's
four bytes wide this means it's less
easy to use some of the C standard
library that might exist or extensions
of
the standard library that might exist
for working with unicode characters on a
particular platform on the other hand it
means that you would never have to worry
about surrogates because the surrogates
would always be converted into four byte
characters it means that if you have a
string that contains one character that
doesn't fit in two bites the entire
string is four bytes per character
that's a compromise currently you can
compile Python in such a way that all
unicode characters are f 4 bytes wide
it's sort of a cultural choice whether
whether you it's worth to you having the
bytes I haven't having a character be
wider and not having to worry about
surrogates the capi issues frankly our
mess I'm not going to spend much time
describing that here generally my my
approach to Python 3000 is first I want
to get the sort of the Python
programmers API is cleaned up and well
it's too bad if extension writers will
temporarily have to deal with a sort of
a slightly messy set of api's I mean
it's C code you're used to things being
messy there is a different faction in
the Python developer community or at
least in the people who are quite vocal
in the Python 3000 place which is not
necessarily the same who would like to
see things like well the most extreme
view is actually support variable length
and coatings as the internal
representation for example if you have a
large file containing Unicode data you
might want to read that into something
that calls itself a string object but
actually still contains unicode utf-8
bites internally the problem with that
is now I have 10 megabytes of utf-8 and
I have a program that sort of tries to
walk through that code from the end or
just random randomly excesses bite 7
million
there's no way to find out where sorry
character 7 million there's no way to
find out where character 7 million is
without parsing the first seven million
characters you can try to optimize that
keep a cache of a couple of pointers
myth it gets Messier and Messier and
more and more complicated I'm not sure
that that's that's at all a viable idea
maybe someone can prove me wrong by
actually coming up with an
implementation that I'm skeptical a
slightly less ambitious but still very
controversial idea is to optimize things
like slicing operations and potentially
also concatenations so that if you for
example if you have a slice you have a
string of ten megabytes and you take a
slice of four megabytes out of that
string currently Python always copies
you could say well let's just share a
that array that already contains those
bytes I mean after all they're immutable
objects they can't change once once
they've been read into memory there they
are there the objects not going to move
unfortunately it's most of most of the
implementations of that idea are very
easily lured into a worst-case behavior
where you do something like you read
repeatedly you read a megabyte string in
and you slice 30 bites out of it or
something and so now you have a 30 bite
object as a 30 character string object
that references a slice of a megabyte
long string object and you can't
deallocate that megabyte until you
deallocate the 30 by 30 character string
and you can try to work around that with
your wrist Ock's the slice is really
small you you copy anyway or if it's
small or small relative to the size of
the original or you can try to use weak
references to sort of dynamically copy
and the only effect of that is that you
have
more and more code that could go wrong
and less and less actual performance
benefit so I think that in the end the
approach of very straightforward simple
algorithms that always copy is still
going to be a winner but I'm I'm trying
to keep an open mind about this so the
bytes type the best way to think of it
is a mutable sequence of small integers
so it behaves a little bit like a list
but the values you can you can store
into it are limited to being integers
they have to be positive and they have
to fit in a bite it also behaves a
little bit like a string there is a
bunch of string methods that make total
sense for byte arrays like find on the
other hand certain string methods that
are locale dependent or character
encoding dependent will definitely not
be allowed like you will not be able to
lowercase or uppercase a byte string a
byte array to go from a bytes array to
string use its encode method to go from
string back to byte array you could use
its decode method and those always
require an encoding parameter if you
want some kind of default encoding
you're going to have to dig it out of
the environment yourself bytes type has
actually been implemented some of the
string behavior probably still needs to
be added but in general pretty happy
with it you can actually already use it
for IO in limited situations so that's
nice segue to the new i/o library which
is yet another idea inspired by Java and
you could also say it's it's part a
little bit by pearl which also has
stackable components in its newer i/o
library so at the very low level you can
read bytes from a well from a file
descriptor a file handle on unix is
going to be a tiny object that wraps a
UNIX file descriptor on windows it's
going to be a tiny object that wraps a
windows file handle
it provides read/write close seek and
tell methods there's no buffering going
on and it always talks in terms of bytes
doesn't do any carriage return line feed
conversion either if you start on a
brand new platform that is not at all
like UNIX or Linux or Windows or Mac
you're going to have to provide your own
low level by taio implementation most
likely there's actually a unique
simulation library that you can probably
use as long as it you can turn off any
character translation features it might
have I mean that that's that's a
possibility for windows too but on
Windows that are actually slightly lower
level things that are more efficient and
more flexible but that's the only thing
you have to do for a platform I mean
buffering unicode encoding decoding
caricature and line feed translation all
those things can then be built on top of
that without any platform specific stuff
using this I expected in most
applications unless you're doing very
messy stuff where you're sort of not
sure whether you're reading it reading
binary data or text which of course
happens you will not have to change your
program you the open function will
continue to return a file object you can
tell it to open a binary file or a text
file for reading or for writing all
those things will still work however if
you open a text file read and write will
use strings if you open it as binary
region right we'll use byte arrays so
that's probably if you if you're doing
binary I oh you're more likely to have
to change your code than if you're doing
textio now how does it decide only
encoding when you're doing textio and
you don't specify the encoding in 20 as
an extra open parameter
one will have a keyword parameter that
will let you specify it and then coding
but if you don't specify that it's going
to pick a default and I can imagine a
number of different ways of picking a
default you can say well we'll pick a
ski or we'll pick utf-8 or will sniff
the file and actually see whether it
looks like utf-8 or utf-16 little and
you know big endian encoding you may try
to see what the users environment says
about file formats there are a couple of
different ways I mean if you're if
you're dealing with the tty device in
windows environment I think the tty
device actually knows what encoding to
use so that would be another way to get
your encoding by default I expect that
when you're using an opening a file for
binary I oh you will not be able to use
the red lines or red line methods unless
it turns out that a lot of code breaks I
mean I don't actually honestly know if
there is much code around that has a
legitimate reason for calling red line
on binary files but there might be so
we'll see an interesting thing is also
how you're going to tie these things two
sockets but I think all the socket has
to do is provide little wrapper that
implements the same read write
operations that their lowest level of
binary i/o object does and you have to
somehow decide on what your encoding is
Latin one or a ski or something else and
then you will be able to read and write
from sockets by the way we're completely
weaning ourselves off the c standard i/o
library for a number of reasons mostly
having to do with the c standard i/o
library not actually always providing a
functionality that we need like it
provides buffering
but it doesn't provide an API to see how
many bytes have been buffered if there's
anything buffered it doesn't have a way
of peeking in the buffer we need those
things there's also this thing that the
sea standard i/o library says that
basically you could expect a segfault or
World War 3 when you read and then
suddenly you start writing the same file
descriptor even if the thing is suitable
for reading and writing you still have
to seek when you switch from reading to
writing since the standard i/o like the
sea standard i/o library doesn't promise
you get a need error message when you
forget to seek in between that's really
unpleasant thing for Python to have so
python has to keep track of are you
reading or are you writing so we end up
sort of redoing too much of the C
standard i/o library functionality
anyway so we'll just throw it out and
hopefully have a bigger and better
implementation so int and long
unification is a really simple thing
currently python has small ends named
int and large in names long the large
insar actually arbitrary-precision so
you can represent numbers as long as
they fit in memory the small integers
are actually mapped to see long so there
are 32 or 64 bits depending on what kind
of platform you have that was really a
mistake and I made that mistake sort of
very very very early on in pythons
design and over the years we've made
more and more compromises where you can
use in then it will actually behave as
if it were long if it doesn't fit like
in older versions of Python if you kept
multiplying numbers together and the
results got bigger and bigger at some
point you'd get an overflow error in
modern pythons I think it had started in
Python 2 3 or so certainly in Python 24
when the result doesn't fit in 32 bits
or in 64 bits on some platforms you'll
just get a long integer
and more and more places if it doesn't
fit in a small integer will just give
you a long integer even to the point
where if you if you if you call the int
function and somehow the int function
can do a couple of things you can
convert a float or a string or another
integer to an int in most of those cases
is through if the result is a valid
integer but doesn't fit in 32 bits so
it's a valid mathematical integer
nowadays int will just return a long
object and so the only place where
you're still aware of the difference
between ins and lungs is if you're
explicitly checking the type of your
your objects if you say if is instance X
comma int then do this otherwise do that
the new code won't work when someone
passes you along even if it's long
containing a very small value so that
long thing becomes less and less useful
and in Python 3000 were just throwing
the type out we looked at the number of
different implementations what we chose
was actually taking the long
implementation and renaming it to int at
least at the Python level in the sea
level the distinction between long and
int is still very much visible we did
have to optimize it a little bit because
the interrupt the inter implementation
was traditionally very optimized like it
has a cash office of small integers and
a couple of other allocation tricks the
long type was completely unauthorized I
think the new long int type is somewhat
optimized at least it has a cash for
small values we're probably going to try
to get that performance back up to speed
comparable to the best performance in
Python 2 point X during the year after
the 3 point 0 alpha 1 release you know
how close will get but I'm hopeful that
some smart people will be able to do
magic there and it makes life for the
program are much easier because you know
you can actually write if instance X
come on end and it will do the right
thing unfortunately I have no idea what
time it is I'm worried that that might
actually 15 minutes Oh excellent usually
run over except the tape runs out
doesn't matter i'll try to be done in 15
minutes i think that's okay so we have
integer division and again that was
there was a very early mistake where I
sort of mindlessly borrowed behavior
from see if you divide three by four it
gives you zero turns out that certain
algorithms really sort of find that the
booby trap waiting to explode when you
least expect it so we're going to make
3/4 return three quarters in some kind
of float representation and you can use
double slash if you really wanted that
zero now the double slash operation has
been in python 2.2 X probably since 2.1
again 2.2 okay I believe you so you've
had plenty of warning and there's also
an option you can pass to python 2.2 X
that will tell you when you're using the
single slash operator and it is used on
integer operands so changes to
exceptions we're getting rid of string
exceptions we're also enforcing that all
exceptions derived from a single root
exception type which is called base
exception in practice you should derive
all your exceptions from exception which
is slightly lower in hierarchy than base
exception but you can if you know what
you're doing derived from base exception
also we're going to move the trace back
into the exception objects again I
should mention this is an area where
Java has been leading we're cleaning up
the race statement there are two
different ways of raising an exception
with arguments you can say raise e
parenthesis arguments close parenthesis
or you can say raise a comma argument or
arguments in parenthesis even that
second syntax was only necessary back in
the day of string exceptions so we're
getting rid of that if you want to pass
a traceback you call a method on the
exception object that you already
created it sort of adds a traceback
object
which also changing the except clause
when you're catching exceptions there's
a pretty common mistake where you want
it to catch two exceptions but you've
got to put parentheses around them and
now you're catching the first exception
and when you catch one a local variable
is created with the name of the second
exception in order to prevent that we're
going to you instead of a comma between
the exception and the variable we're
going to use the keyword s also new is
and this has to do with the exceptions
now sort of containing that the
traceback as an attribute we're going to
delete that variable if it still exists
at least at the end of the except block
I'm basically going to put a tri finally
in that block that you don't you won't
see but will be there that leads that
value if it exists which means that if
you want that value if you want that
exception value to survive beyond the
accept lock you have to just assign it
to a different local variable so we're
not going to do optional type checking
but we are going to add some syntax that
will allow other people to implement
frameworks that do something like type
checking or whatever they would like to
do basically currently every parameter
of a function has a default value well
it says that it can have a default value
we can now also associate an annotation
with every parameter the annotation is
introduced by colon the default value is
of course introduced by an equal sign
you can combine those the coulomb
annotation equal signs expression
notation you can also annotate the
function return value with an arrow all
those things are evaluated when the
function is defined so at the same time
the function object is created both the
default and the annotation which are
just generic expressions i have no
constraints on that but they must if the
you if they reference variables those
herbals must exist at that point in time
and then you can pull those annotations
out of the function object by asking for
the funk annotations attribute of the
function and that's just the dictionary
indexed with variable names and the
keyword return if you want to do with
something with this you have to do it
yourself I can imagine all sorts of
decorators or meta classes that make
good use of this to enforce all sorts of
things from from actual type checking to
automatic adaptation and a number of
other interesting things I'm not going
to put anything like that in the
language at least not in 3.0 another
small change to function signatures
completely independent from the previous
one both of these have been implemented
by the way sometimes it's really helpful
to have a parameter that is required to
be used as a keyword in your column in
your call syntax if you really want to
enforce that in Python to you can use
stars to our keywords and sort of pull
it out of the stars to our keywords
dictionary but it's kind of messy and
you have to sort of check for each of
the keywords that you might expect and
check that there isn't anything else in
there in order to be sort of robust and
and user friendly now you can just use
this strange notation where there is a
star without I mean the star of course
normally means start you can use it
already a star arcs which means we have
a variable number of positional
arguments here that gets returned as a
tuple now if you leave the name out from
that syntax you just have a star without
the star arcs and then you cannot
specify arbitrary positional arguments
but you can after that specify more
arguments that will then be required to
be key words and they don't even have to
be have to have default so after that
star you can have seized 42 so that's an
optional keyword parameter but D doesn't
have a default value so that's
required keyword parameter so every call
to foo in that case must specify a value
for D and it must specify it using the
keyword notation sep literals very
simple you put a number of expressions
in curly braces and it creates a set
object except if there are is nothing
between the curly braces it still
creates a dictionary at some point I try
to propose to unify the dictionary and
the set object that didn't get a lot of
support from the developer community if
you really want frozen sets it turns out
frozen sets are only very very rarely
used you have to cast that thing
explicitly to a frozen set or of course
you can use frozen set with a list
argument we're also going to implement
set comprehensions those are not yet in
the codebase it works the same way as a
list comprehension except it returns a
set absolute import you can already do
that in Python 25 from under future
import absolute underscore import that
means that if you import a module using
import food or something like that
inside the package normally in Python
2.4 and before it first sees if there
tries to find that fool in the package
if it's not in the package it looks in
cysto path in three point 0 or in 2.5 if
you have that future statement in your
module it's not going to look in the
package that solves a particular
ambiguity where you might have a module
in your package that has the same name
as a module in the standard library a
top-level module in the standard library
currently without this future import
there's no way to reach out and actually
import that the standard library module
because the one in your current package
will always be seen for first well you
could dig it out of system modules but
only if it's already been important by
someone else if you want to say I
definitely want
the food that's in my package rather
than potentially the one in cysto path
you can say from dot import food that's
also already in 2.5 the only difference
really is that in 3.0 you always have
that future statement automatically
implied in your code exec very early
Python versions is actually was a
function it takes an object which is
either a string or a code object or a
file and then optionally Global's and
locals at some point I thought that the
compiler make could make good use of the
fact that you are using exact somewhere
in a function and I decided that in
order for the compiler to know about it
it would have to be a statement well
compiler technology has advanced little
bit and you can actually tell fairly
reliably whether you're using a function
like this so there's no need for it to
be a statement and it's actually easier
to have it as a statement that story has
a function so it's back to being a
function the interesting thing is this
is very easy to to do in Python to point
because since it once was a function
that same syntax with a tuple of three
value of up to three values is also
still supported in two point acts so
range just like we have keys and a
Turkey's we have range in X range
because range was their first the rage
creates a list of many integers
potentially many xrange produces only
the integers that you asked for so we're
going to change that so that there's
only going to be a function in range but
it will behave most like mostly like X
range the difference is the current ex
range is optimized so that it actually
only works for integers that are less
than system accent and Neil Norris has a
patch to fix that but I'm still waiting
for him to upload the patch or something
zip this is actually a pretty minor
issue zip is something that would
be a very good candidate for returning
an iterator in Python to when it was
except it was introduced before
iterators existed so there's an
introduced that I zip thing that does
return an iterator but makes much more
sense for zip to be in a Prius Raider in
the language so string formatting has a
couple of problems and there is a pep
which I hope will be implemented I'm
certainly in favor of the proposal to
give strings a dot format method and to
use curly braces instead of percent
something as the indicator for
replacement in the format string in here
quickly are a couple of examples you can
specify format arguments by position 0
and 1 or by name foo if you want to
include literal curly braces you can
double them you can even access
attributes or use get item dictionary
notation in simple cases on the
formatting object you can also specify
parameters after a colon I think that is
actually borrowed from dotnet although
I'm not sure that we're taking exactly
the same notation read the pap if you're
interested so this is something that's
actually probably not going to make it
but I'm mentioning it anyway because it
is potentially an interesting feature
it's just there there are a couple of
difficult decisions to be made I mean
it's very easy to come up with a decent
switch switch style syntax you can say
switch expression case expression blah
blah blah question is when you evaluate
the case expressions in order to
actually benefit from a potential speed
of your like you could do with this
patch based on a dictionary you would
like to pre compile those case
expressions for example compile them at
the time the function is defined rather
than each time the function is invoked
but that limits you to actually
constants it's not a concept we
currently have anywhere else in the
language which makes it somewhat
problematic sort of conceptually which
is why we have an inkling
did it yet and it's marked with both
stars and question marks another thing
that is likely to be to make it in even
though it's slightly ugly if you have a
function an inner function that
references a variable defined in an
outer function you can use it but
currently you cannot assign to it you
can modify it if it's immutable object
like if you have a list object in the
outer function you can append to that
list or even index it and change a
element of that list which you cannot
replace it with a different list object
using plain assignment turns out that
there are enough places where people
would like to have that functionality
and we had a long discussion we're
capping ye did a brilliant job of
summarizing the discussion and sort of
guiding it towards perhaps not final
completion but at least closure so that
everybody could agree with what was
written down in the pep we're pretty
much settled on the syntax and on the
semantics the only thing is there are
different flavors of keyword that sort
of each have their own advantage and
disadvantage non-local is the current
favorite it's sort of ugly because it's
a long word and it is has sort of a
negative meaning unfortunately the only
real contenders were global and outer
where the problem with global is the
global for most people's minds has
fairly set semantics which really
doesn't mean just go search outward
scope by scope by scope but really go
all the way to the outermost scope the
global scope so that's why I mean even
though global was my favorite nobody
else seemed to like it very much and I
have to respect my users outer was a
nice candidate until we found how often
that that word is already used as a
keyword as a variable name or a function
name and that made it much less
attractive so it's probably going to be
non-local which is not something people
to use law those variable names so
another very speculative thing is
abstract based classes we had long
discussions about interfaces generic
functions abstract base classes actually
if the more I think about it the more
attractive they look from the
perspective of a somewhat voluntary
declaration of I implement a particular
protocol where protocol is a very
informal concept we've had constant the
concept like protocol in Python for a
long time we've been talking about
sequences and mappings as sort of
implementing implementing certain
operations and not others the problem is
if you have an actual object and you
don't know whether it's a sequence of a
mapping there's not really a good way to
decide which one it is you can check
whether it has a keys method but there
are actually some cases where you have
something that really behaves like a
mapping but it maps an infinite number
of keys and you really don't want to
have implemented Keys method that tries
to enumerate all of them so if there was
a abstract based base type that didn't
provide any semantics or implementation
but just serves as a marker class I am
implementing the sequence protocol or I
am implementing the mapping protocol or
I am implementing the file protocol and
it's probably going to be a couple of
but there is there is going to be more
fine-grained distinctions like you have
readable files and writable files and
readable and writable files and you
probably have mutable sequences and
immutable sequences and very basic
mappings that only implement the the map
operation and sort of very complete
mappings that implement lots of other
functionality like update and keys but
if i get time between now and april i'll
write pap about this and then
implementing it is going to be simple
please this is this is something that
just adds some stuff it's going to be
easy to make all the standard types
declare what stuff they implement and
then it's just up to user code to to
voluntarily follow this mean we won't
stop you from implementing sequence
protocol methods without declaring that
you're a sequence but the sort of the
carrot in this case is that if you want
to interface with a large framework like
soap or twist it or something like that
it might be that eventually future
versions of those frameworks that work
under Python 3 point X will actually
instead of sniffing which methods are
implemented actually just look at the
base classes that's the hope anyway so
I'm going to skip the miscellaneous
changes you can get the slides from the
web eventually this is mostly clean up
very small stuff library reform is not
my own idea of fun I like to focus on
the language language is big enough that
other people are interested in reforming
the library there is currently not a lot
of activity going on it's certainly
something that I think is a fine project
to do after we've released the alpha 1
release of the language so again the
capi i'm currently not too worried I
mean just randomly changing the capi as
object types change of course if you're
writing a third party extension that's
not already part of the Python source
tree you would like to know what's going
to happen at this point the only thing I
can promise is I'm not going to change
functions to have a different signature
but the same name or different semantics
even with the same signature i'm going
to add api's I'm going to delete api's
that are no longer relevant or
impossible to income
and I'm not going to change API is in an
incompatible way that would break your
code i am going to require everyone to
recompile their code that's the meaning
minimum i can expect so if your
compilation passes you're somewhat
likely to actually have a working
extension best case scenario if you're
using AP is that no longer exists you'll
get a clear compile time error about
something that doesn't exist or maybe a
link time error so now you have a bunch
of python 2.6 code and you want to turn
it in Python 3.0 code well you could
just try to run it with 3.0 and fix all
the syntax errors and then fix all the
the runtime errors hopefully you have
unit tests that's going to be pretty
tedious because they're even though the
general flavor of the language doesn't
change much there are clearly a lot of
small changes that really add up classic
classes except as and different race in
tax no comparisons keys the dictionary
views is going to affect a lot of people
print statements of force is going to
affect a lot of people unicode is going
to be a major deal for at least some
people so there is a conversion tool now
we cannot do a perfect conversion
because in in some cases it's inevitable
that you'd sort of have to do with
symbolic execution of the application in
order to find out what the types of a
particular variable are before you know
how to convert a particular column and
if I say X dog keys there's no guarantee
that X is actually a built-in dictionary
it could be a completely unrelated
object that has a keys method however
there's a good chance that it is a
dictionary if you have something that
has an it Serkis method there is an even
bigger chance that it's a dictionary
so what we're doing is we have a a tool
that parses your code and looks purely
at the parse tree and it's able to
transform that parse tree in place and
then right back out and we annotate this
parse tree with exactly where the white
space is and where your comments are so
in theory certainly if that i know i
have tested that if you don't make any
transformations it's always the output
is exactly the same as the input every
single white space character it's that's
that conversion is perfect now if you
make transformations sometimes it's
possible that you would lose a comment
if that comment sort of is in the middle
of an expression that gets completely
discombobulated and transformed into
something completely different that's
not very likely to happen because how
often do you have significant comments
between the parameters of a function oh
but right after a binary operator not so
common so if you if you're interested in
looking at this code currently you have
to go to s viendo python org and find
the sandbox code and go to the two to
three subdirectory it's relatively easy
to add new conversions I mean if I've
had a couple of Python developers who
started contributing conversions
actually that that's that's been really
great the idea is you write a pattern
that decides I want to match certain
nodes in the parse tree that look like
like like that match the pattern and the
pattern completely ignores what the
comments say it purely looks at what
what the parts are actually see so there
are really two section two parts to the
parse tree there's the annotation for
white space and comments and there is
the syntactic tokenization and parse so
the matching is purely concerned with
matching
nodes and leaves in the tree and the pad
i'll show the patterns in the syntax in
a minute so you write your pattern and
then you write a transformation function
that sort of pics the the node you find
apart and put it back together in a
different order and returns that that
new node and with some caveats then
there's a framework that does all the
all the rest of the work like traversing
the entire tree looking for all the
nodes that match the pattern and calling
your transformation on each of those
sort of a separate strategy that is also
going to help is python 2.6 by default
it will just be python 2.6 but it will
have an option where it will warn about
things that will go out of style in
Python 3000 it will probably also
backport certain Python 3000 features so
you can start using those I don't want
to give examples because not much of
that has actually been implemented maybe
Thomas can talk about that next week so
here a couple of things that the
transformer is really good at it can't
it and you can take in a call to apply
and turn it into the more modern
notation using star arcs and starts to
our keywords and as long as you don't
have a local variable name to apply this
is going to do the right thing and it
will put put extra parentheses around
the function or the arguments if
necessary to make sure that it doesn't
sort of get affected by nearby operators
slightly less perfect but still pretty
close it turns everything that says a
Turkey's into keys and it ER items into
items it can also do a really good job
with exact it could do a really good job
with print can we go do a really good
job with accept clauses it also
recognizes heskey assuming that you
don't have again a user object that
happens to implement has key well I
found one
example in this in the standard library
whether the bsd rapper library actually
has a two argument has key where the
sacrament second argument I think passes
in transaction state so not quite sure
what to do with that so we just don't
convert that one but otherwise
journaling indeed of heskey k into k &amp;amp; d
again making sure to parenthesis sub
expressions or the whole thing as
necessary based on the context so it
doesn't add parentheses unless they are
necessary to disambiguate stuff on the
other one hand if you have redundant
parentheses in your input you will have
the same redundant parentheses in the
output it's very simple to turn the less
than equal then sorry less than greater
notation for unequal into ! equal sign I
could turn back ticks I can even turn
into too long I found that actually
these things were not quite enough to
get most of the unit tests we'd work to
pass the problem is that a very popular
testing framework in Python is called
doc test and it works by having
documentation strings so they're just
string literals to the parser containing
fragments of Python sessions interactive
Python sessions that you in theory you
can just cut and paste them out of your
shell window into your Python source
code and then there's a framework that
automatically test it's sort of a
regression framework that checks that
those examples still have the same
output as they had when you paste them
in since all this stuff is inside
strength string literals it's not so
easy to to see how we could convert
those because we can't we can't just go
scan all the string literals and assume
that they contain Python code and turn
everything that looks like a print
statement into a print function call
however what you can do is
it turns out that at least for the doc
test stuff doc tests are pretty
recognizable because they have to start
with a Python prompt 3 greater than
signs and if there are continuation
lines they have to start with three dots
and they all have to be sort of indented
the same way so with very great
reliability I parse the dark tests out
of the source file you have to actually
run the tool a second time maybe
eventually all combine that currently
have to run the tool a second time and
it'll just scan the source code looking
for dark tests and this was a great
relief I mean at some point I was a
little panicky because I realized how
much unit testing code it would have two
men you convert manually and then I
realized I just have to do this the only
place where it broke down tremendously
was that the doc tests for the doc test
module itself which applies this trick
recursively there I just ran the tests
and and sort of fix the things manually
until it worked nothing else I could do
now there are also a whole bunch of
things that this conversion
unfortunately cannot do if it sees d
dolt in Turkey's it has no way of
knowing whether d is actually dictionary
if it's CDC's deed of keys it has no way
of knowing whether you're going to
expect that thing to be a list or not it
has no way if it sees x / y whether you
meant that to be whether when you
execute that code x and y are integers
or not so it's not able to sort of turn
that single slash into a double slash it
can't find code that somehow depends on
being able to order objects of different
types it certainly doesn't clean up your
code or remove redundant definitions if
you write your own code that emulates a
dictionary we implement the mapping
protocol it's not going to touch that is
also not going to fix your string
exceptions
basically all it can do is match on a
parse tree stuff that you can reliably
or mostly reliably fix by looking at the
parse tree only as good candidate for
this tool I don't know if that's going
to be enough maybe at some point we'll
have to add understanding of variable
scope and things like that so it can
actually tell whether a particular
occurrence of a variable named apply is
in fact the built-in function apply or
not i'm currently hoping that we won't
need to do that otherwise we would
somehow probably have to merge this tool
with pie checker which would be quite a
refactoring so if you're interested in
actually probably going to skip this
this is what the matching notation looks
you basically you use the names that are
also used in the grammar file python has
its own grammar file here's a couple of
examples power it's a token a power is
an atom followed by zero or more
trailers and then optionally followed by
a double star and something called a
factor and there's a couple of
alternatives for what Anatomy is in the
definition of what a trailer is and
there is like several hundred lines like
this that make up the entire Python
syntax so our conversion tool actually
reads that file with a Python syntax at
the start of a run and builds a parser
customized to death syntax so it's very
easy actually to change the syntax that
the conversion tool uses but you just
have to edit one text file the trick I
use in the patterns is I use the same
notation as in the grammar I actually
use regular expression notation you can
so you can you can match here up at the
pattern power and then the angular
brackets are actually sort of they
specify and inside this node name
labeled power I must match the
following things so this is it we want
to match your power that starts with
well one or more nodes of any type but
they must be exactly at that level and
then a node of type trailer with a
particular sub structure namely the
trailer alternative that has a dot
followed by a name in the name in this
case must be thur items and then it can
have more trailers that's an example of
a matching rule that's close to actually
the rule i use for fixing either items
so if you have that expression a square
record 0 square bracket dot other items
/ and / end the parser sees that as an
atom containing a and then a node that's
a trailer it's the square brackets
another know that's a trailer the dot a
terrariums and another node that's a
trailer that's the parentheses and that
happens to match this this pattern as
follows the first two together actually
match to any plus then follows the
trailer which happens to match a trailer
with that particular sub structure and
then the final trailer it matches the
trailer star and you can you can nest
these things as much as you want and
it's relatively efficient in just
traversing three and finding matches
what your transformation function gets
is it gets the node that match it match
the top level of the pattern it also
gets a dictionary containing elements
sort of subnodes of that node and what I
didn't show when not showing here is you
can add names to any particular section
of the pattern you can say oh this sub
pattern call that foo or call this utter
sub pattern bar and then you can sort of
pool all those up the thing the sub
nodes that match those name subsections
out and you can rearrange those in a
different order that's for example how
you do to apply thing
so here's the slide that you you're all
waiting for what can you do today well
my first recommendation is don't worry
about the changes that the
transformation tool can actually take
care of I mean my first version of this
slide actually started out with okay so
use Starks instead of apply and use
raise exception parenthesis parenthesis
instead of a raise exception comma value
and then I realized no you shouldn't
have to worry about all the stuff that
we can transform for you syntactically I
mean it's unlikely that you'll be able
to write code that is both valid python
2.6 source code and valid Python 3.0
source code so you go to have to run the
transformation tool anyway what you can
do is make things easier so that after
you've run the transformation you
actually end up with working code python
using python 2.6 means that you can use
python 2.6 as warnings to find certain
things that the transformation tool
cannot handle it's always a good idea to
have unit tests so you can sort of see
if the semantics of your your new codes
is still what you expected to be and
then there is a couple of things that
the transformation tool does not handle
like if you extract the keys from a
dictionary and then you sort the
resulting list transformation tool is
not smart enough to correlate that the
variable you assigned on line one is
being sorted in line 27 or online to
even but you can write today you can use
the built-in sorted function which is
available in Python 2.4 and up and then
you have code that can be easily
transformed correctly similarly if if
you really have good reason to want to
reach the return value of keys as a list
call list and pass it the inner keys
function the air keys will be
transformed by the transformation tool
and so it will still be a list
we'll be just as efficient in 2.6 as in
three-point Oh another thing you could
very easily do is make sure that all
your exceptions are actually using
classes derived from exception you can
also make all your class your classes
that aren't exceptions that don't have a
base class and derived them from objects
of their new style there are certain
semantic differences between classic
classes and new-style classes by
converting them to new-style classes now
you catch those semantics while you're
sort of thinking about it and then with
print don't worry about the print syntax
and i recommend recommend that you just
use the print statement and reliable
transformer to turn them into function
calls when the time comes but be aware
of the two cases where the
transformation tool doesn't do the right
thing which has to do I think I show
that on the slide about print if you
have a string ending ending in a new
line or a tab and don't another thing
you can do now is make sure that your
code uses a double slash where you
expect an integer division so now we
have one theory we have five more
minutes for questions if anybody has the
energy yes
I am curious might think that order one
granted access I well yeah so the
question is why do I not want strings to
use internal you GF 8 or 16
representation and why do I think that
order when other one indexing of strings
is important I think because it's a
tradition in Python unlike some other
languages that we actually write a lot
of code that sort of traverses a string
and keeps track of a particular index
there's just lots of code that that
index is a string I mean it's very
common to say that if if s told ends
with dy return s sliced from zero
through a land s minus three that's all
I can say it's it's sort of common
idioms in Python code are are using
slicing which uses numerical indices
quite a bit and pattern matching is used
much less
okay so the question is can the
transformation to potentially be abused
for other purposes I think it definitely
can there's nothing that says you have
to use it to transform into two part you
don't have to use it to transform python
2.2 x 2 3 point xcode you mean you can
you can make the input syntax whatever
you want it and you can slightly alter
the driver so that instead of
transformations you just get error
messages if you match certain patterns
that's that's an excellent idea actually
his does
there are the big sis
status means
this is not me
I didn't get the last few words but your
question is did I consider some other
string abstraction that would not make
it necessary to to rely on indexing so
much Oh as an IC your question is
specifically could we have an additional
string class that has sort of different
a different model that's a reasonable
question I hadn't really considered that
I see it as a library issue i think i
would encourage people to sort of to
write custom strain classes that might
be more efficient for certain situations
and you can you can probably write them
by the moog you can implement them in
Python by using a byte array and a thin
layer on top of that or if you're really
interested in super performance you can
of course do it all and see but mean
that's the beauty of an extensible
language it doesn't all have to be in
the standard library in the back
so
sorry could you speak up it's been it's
getting noisy
okay so yeah so the question is there's
going to be a long period where library
developers third-party library
developers especially will sort of be
required to maintain a 2.6 and a
three-point o version of the same
library or maybe even going back to
earlier versions than 2.6 is the
expectation that they limit themselves
to code that can can be automatically
transformed to 3.0 expectation is a
strong word i would i would recommend
that because i expect that that is the
sort of least painful way for library
developers to go of course if you have
an existing library that has backward
compatibility requirements going back to
python 22 or sometimes even before it
becomes gradually harder to to sort of
maintain your source that code in the
form that can still be transformed i
mean if you're in a lucky situation that
you can actually say 2.6 is the oldest
version of python i support then at
least you can use some of the three
point 0 features that will be back party
to 2.6 but i think the syntactic
conversion approach will work i mean
there's no reason that the transformer
couldn't convert python 2.2 co 2 3.0 it
would just sort of the subset of python
2.2 that actually is validly
transformable into three point 0 is
slightly smaller but that's i would
recommend that i mean that the bigger
nightmare is for developers who have
extension modules because the capi is
going to be it's going to be a rougher
ride unfortunately
well if you all aren't exhausted I
certainly am so I thank you for staying
all the way until the end</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>