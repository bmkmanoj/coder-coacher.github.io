<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>R tutorial: Cross-validation | Coder Coacher - Coaching Coders</title><meta content="R tutorial: Cross-validation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/DataCamp/">DataCamp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>R tutorial: Cross-validation</b></h2><h5 class="post__date">2016-11-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OwPQHmiJURI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in the last video we manually split our
data into a single test set and
evaluated out-of-sample error once
however this process is a little fragile
the presence or absence of a single
outlier can vastly change our
out-of-sample our MSE a better approach
then a simple train test split is using
multiple test sets and averaging
out-of-sample error which gives us a
more precise estimate of the true
out-of-sample error one of the most
common approaches for multiple test sets
is known as cross-validation in which we
split our data into ten folds or train
test splits we create these folds in
such a way that each point in our data
set occurs in exactly one test set this
gives us ten test sets and better yet
means that every single point in our
data set occurs exactly once in other
words we get a test set that is the same
size as our training set but is composed
of out-of-sample predictions we assign
each row to its test set randomly to
avoid any kinds of systematic biases in
our data this is one of the best ways to
estimate out-of-sample error for
predictive models one important note
after doing cross-validation you throw
away all the resampled models and start
over
cross-validation is only used to
estimate the out-of-sample error for
your model once you know this you refit
your model on the full training data set
so as to fully exploit the information
in that data set this by definition
makes cross-validation very expensive it
inherently takes 11 times as long as
fitting a single model 10
cross-validation models plus the final
model the Train function in carat does a
different kind of resampling known as
bootstrap validation but is also capable
of doing cross validation and the two
methods in practice yield similar
results let's fit a cross validated
model to the MT cars data set first we
set the random seed since
cross-validation randomly assigns rows
to each fold and we want to be able to
reproduce our model exactly the Train
function has a
formula interface which is identical to
the formula interface for the LM
function in base R however it supports
fitting hundreds of different models
which are easily specified with the
method argument in this case we fit a
linear regression model but we could
just as easily specify method equals RF
and fit a random forest model without
changing any of our code this is the
second most useful feature of the carrot
package behind the cross-validation of
models it provides a common interface to
hundreds of different predictive models
the TR control argument controls the
parameters carrot uses for
cross-validation in this course we will
mostly use 10-fold cross-validation
but this flexible function supports many
other cross-validation schemes
additionally we provide the verbose
itter equals true argument which gives
us a progress log as the model is being
fit and lets us know if we have time to
get coffee while the models run let's
practice cross validating some models</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>