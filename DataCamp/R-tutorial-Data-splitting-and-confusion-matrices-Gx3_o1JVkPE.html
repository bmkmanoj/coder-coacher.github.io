<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>R tutorial: Data splitting and confusion matrices | Coder Coacher - Coaching Coders</title><meta content="R tutorial: Data splitting and confusion matrices - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/DataCamp/">DataCamp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>R tutorial: Data splitting and confusion matrices</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Gx3_o1JVkPE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have seen several techniques for
pre-processing the data when the data is
fully pre-processed you can go ahead and
start your analysis you can run the
model on the entire data set and use the
same data set for evaluating the result
but this will most likely lead to a
result that is too optimistic one
alternative is to split the data into
two pieces the first part of the data
the so-called training set can be used
for building the model and the second
part of the data the test set can be
used to test the results one common way
of doing this is to use two thirds of
the data for a training set and one
third of the data for the test set of
course there can be a lot of variation
and the performance estimates depending
which 2/3 of the data you select for the
training set one way to reduce this
variation is by using cross-validation
for the 2/3 training set and a 1 first
test set example a cross-validation
variant would look like this the data
would be split in 3 equal parts and each
time two of these parts would act as a
training set and one part would act as a
dust set of course we could use as many
parts as we want but we would have to
run the model many times if using many
parts this may become computationally
heavy in this course who will just use
one training set and one test set
containing 2/3 versus 1/3 of the data
imagine we have just run a model and now
we apply the model to are tested to see
how good the results are evaluating the
model for credit risk means comparing
the observed outcomes of default versus
non default stored in the loan status
variable of the test set with the
predicted outcomes according to the
model if we are dealing with a large
number of predictions a popular method
for summarizing the results uses
something called a confusion matrix here
we use just 14 values to demonstrate the
concept a confusion matrix is a
contingency table of correct and
incorrect classify
occations correct classifications are on
the diagonal of the confusion matrix we
see for example that eight non
defaulters were correctly classified as
non default and three defaulters were
correctly classified as defaulters
however we see that two non defaulters
were wrongly classified as defaulters
and one defaulter was wrongly classified
as a non defaulter the items and the
diagonals are also called the true
positives and true negatives the off
diagonals are called the false positives
versus false negatives several measures
can be derived from the confusion matrix
we will discuss the classification
accuracy sensitivity and the specificity
the classification accuracy is a
percentage of correctly classified
incentives which is equal to 78 point 57
in this example the sensitivity is a
percentage of bad customers that are
classified correctly or 75 percent in
this example the specificity is the
percentage of good customers that are
classified correctly or 80 percent in
this example let's practice splitting
the data and constructing confusion
matrices</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>