<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>R tutorial: Introducing out-of-sample error measures | Coder Coacher - Coaching Coders</title><meta content="R tutorial: Introducing out-of-sample error measures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/DataCamp/">DataCamp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>R tutorial: Introducing out-of-sample error measures</b></h2><h5 class="post__date">2016-11-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uswmGsGmDZc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Zack Dean mayor and I'm one of
the co-authors of the carrot package I
have a passion for data science and
spend most of my time working on and
thinking about problems in machine
learning this course focuses on
predictive rather than explanatory
modeling we want models that do not over
fit the training data and generalize
well in other words our primary concern
when modeling is do the models perform
well on new data the best way to answer
this question is to test the models on
new data this simulates real-world
experience in which you fit on one data
set and then predict on new data where
you do not actually know the outcome
simulating this experience with a train
test split helps you make an honest
assessment of yourself as a modeler this
is one of the key insights of machine
learning error metrics should be
computed on new data because in sample
validation or predicting on your
training data essentially guarantees
overfitting out-of-sample validation
helps you choose models that will
continue to perform well in the future
this is the primary goal of the carrot
package in general in this course
specifically don't over fit pic models
that perform well on new data let's walk
through a simple example of
out-of-sample validation we start with a
linear regression model fit on the first
20 rows of the M T cars data set next we
make predictions with this model on a
new data set the last 12 observations of
the M T cars data set the twelve cars in
this test set will not be used to
determine the coefficients of the linear
regression model and are therefore a
good test of how well we can predict on
new data in practice rather than
manually splitting the data set will
actually use the create resamples
function or create folds function and
carat but the manual split simplifies
this example finally we calculate root
mean squared error or our MSE on the
test set by comparing the predictions
from our model to the actual
MGP values for the test set are MSE is a
measure of the models average error it
has the same units as the test set so
this means our models off by five or six
miles per gallon on average compared to
in sample our MSC from a model fit on
the full data set our model is
significantly worse if we had used in
sample error we would have fooled
ourselves into thinking our model is
much better than it actually is in
reality it's hard to make predictions on
new data as this example shows
out-of-sample error helps account for
this fact so we can focus on models that
predict things we don't already know
let's practice this concept on some
example data</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>