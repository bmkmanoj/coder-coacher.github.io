<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Tutorial: Statistical Thinking in Python II (Part 1) | Coder Coacher - Coaching Coders</title><meta content="Python Tutorial: Statistical Thinking in Python II (Part 1) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/DataCamp/">DataCamp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Tutorial: Statistical Thinking in Python II (Part 1)</b></h2><h5 class="post__date">2017-02-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bgp9RyU_ysk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">after completing the prequel to this
course you are now beginning to think
probabilistically outcomes of
measurements follow probability
distributions defined by the story of
how the data came to be when we looked
at Michelson speed of light and air
measurements we assumed that the results
were normally distributed we verified
that both by looking at the PDF and the
CDF which was more effective because
there is no binning bias to compute and
plot the CDF we needed our old friends
numpy and matplotlib dot pi plot so the
first step was to import them with their
traditional aliases to compute the
theoretical CDF by sampling we passed
two parameters into NPI random dot
normal the mean and standard deviation
the values we chose for these parameters
were in fact the mean and standard
deviation we calculated directly from
the data the result was that the
theoretical CDF overlaid beautifully
with the empirical CDF how did we know
that the mean and standard deviation
calculated from the data were the
appropriate values for the normal
parameters we could have chosen others
what if the standard deviation differs
by 50% the CDF's no longer match or if
the mean varies by just 0.01% so if we
believe that the process that generates
our data gives normally distributed
results the set of parameters that
brings the model in this case the normal
distribution and closest agreement with
the data uses the mean and standard
deviation computed directly from the
data these are the optimal parameters
remember though the parameters are only
optimal for the model you choose for
your data when your model is wrong the
optimal parameters are really not
meaningful finding the optimal
parameters is not always as easy as just
computing the mean and standard
deviation from the data we will
encounter this later in this chapter
when we do linear regressions and we
rely on built-in numpy fun
to find the optimal parameters for us I
pause here to note that there are great
tools in the Python ecosystem for doing
statistical inference including by
optimization psy PI dot stats and stats
models being two good examples in this
course however we focus on Hacker
statistics because the technique is like
a Swiss Army knife the same simple
principle is applicable to a wide
variety of statistical problems now it's
time for you to do some exercises to
demonstrate how choosing optimal
parameters results in best agreement
between the theoretical model
distribution and your data</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>