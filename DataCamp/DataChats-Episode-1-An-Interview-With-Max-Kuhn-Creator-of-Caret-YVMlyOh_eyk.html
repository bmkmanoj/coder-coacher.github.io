<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DataChats | Episode 1 | An Interview With Max Kuhn, Creator of Caret | Coder Coacher - Coaching Coders</title><meta content="DataChats | Episode 1 | An Interview With Max Kuhn, Creator of Caret - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/DataCamp/">DataCamp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DataChats | Episode 1 | An Interview With Max Kuhn, Creator of Caret</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YVMlyOh_eyk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hi everyone I am here with the
legendary max Kuhn author of the carrot
package and lover of all things data
statistics and machine learning just
want to ask max a few questions today
about how he got started in in in data
science and what advice he has for for
you guys and a few other things that you
might find interesting
so first off it would be great to get
your backstory so how did you get an
origin story your origin story how did
you get into data science machine
learning statistics whatever you want to
call it and actually as a as an
additional question which label do you
identify most with a statistician okay
yeah easily
or actually non-clinical statistician
because in the world of like Pharma
that's that if you if you say you're in
pharmacist attestation everybody thinks
clinical and non-clinical is really kind
of a rare breed so we are a very
distinct subset of statisticians in
pharma and make the distinction for
people yeah so it's kind of basically
what it sounds like so clinical
statisticians almost exclusively work on
usually late stage clinical trials the
clinical trials the larger clinical
trials that people do to show that a
drug works or puts marketing trials
where after drugs approved we do a lot
of studies to look for different end
points and things like that you know how
well does it control blood pressure and
what have you and that's let's say at
least 90% of the statisticians that are
in pharma the other 10% range between
manufacturing or drug safety and I work
in research and development so so non
clinical statistician sometimes they
call pre clinical statisticians are
basically everything but those
large-scale clinical trials in in the
whole pipeline of drugs I work in the
very very very early part where you know
people say like what drug did you work
on Mike well you know I kind of worked
on them before they were actually
identified is the final drug so really
early it's just chemical compounds at
that point yeah they're all like numbers
to us you know that we just give them
in numbers and and even when we work on
later stage things we know them by their
name like you know TOFA is tofacitinib
is one that we worked on but it's
xeljanz which is like the funniest name
ever for a drug
so even when I say the drug name that
people don't know what I'm talking about
because it's the internal name that we
use sure so yeah so I'm a non clinical
statistician we tend to be a little
closer to the data science people
because you know one of the things that
delineates us is you know we do all of
our own programming we do all of our own
data management we are the ones that go
out for the most part and get the data
so in large databases were the ones that
are kind of like prospectively and
proactively going out and finding you
know where the assay data that you know
I heard somebody talk about something in
a meeting and they said oh I wonder if
this ain't good so we're the people who
would just not really wait for it to
come to us or necessarily query somebody
to say oh send me the data set when it's
all ready we are a little bit closer
data science we tend to go out and get
it ourselves and kind of do what our
ourselves because we don't really have
the infrastructure because in my group
there's three of us and we support about
600 scientists so we don't really have
the infrastructure or the the you know
all the programming groups and all that
stuff and this let's say like clinical
statistics would have sure you got to do
it yourself exactly so that's what we're
kind of used to so we don't really get
intimidated usually don't get
intimidated by things like that okay so
I have a follow-up question which it has
to do with the breakdown of time like
what what time you spend on different
components yeah but but before we go
there let's let's rewind the tape like
yeah like 15 20 years like how did you
get into the field in the first place
what drew you to working with data
working with numbers yeah I this is kind
of my friends don't believe this to be
the case but I start off in journalism
that was what I was going to major in
whatever undergraduate and as part of
that yeah that's before you could take
any real journalism classes you had to
take a bunch of other sort of ancillary
classes and one one of which was a
statistics class and at a great
professor this guy's name was Bob
Boudreau and he kind of fit the mold of
what we think of his academic
statisticians he was very soft-spoken
very intelligent guy we had like the
Thoreau beer down here and
and but he was he was an excellent
teacher he got me really interested in
it and it was at this weird time where
the the journalism school was very
political and who got to take what
classes and things like that and so I
was dealing with all that and I took
this awesome statistics class on time
you know always kind of like math which
is hard to say when you're you know high
school student or whatever but I thought
well you know I wouldn't mind taking
another class on this so you know I
talked to dr. Boudreau and I said you
know what would the next class be he
just sort of said all you take this
class and so you know the next semester
while I was sort of waiting to get into
the journalism classes that I was
supposed to take I took this next class
and what I didn't know was it was one of
the first level graduate classes in the
biostatistics Department on the medical
campus and and that was kind of like
getting pushed in the deep end where it
was a second semester so I didn't have
any regression training so they were
doing like ANOVA and they were all using
SAS at the time because there was no R
and so on so that was kind of like a
kind of a big challenge and like that so
I liked it so much I said well you know
Dec were generalism and just went like
whole-hog and the statistics and you
know it's kind of like naive in the
respect it like when I take my first
computer science class I thought I can't
type so I must not I'll never be a good
programmer right it's so laughable if
you think about now
but you know just the fact that I
thought well geez I don't do enough
typing to really be good at this I
thought that would preclude me from
being good at any of this stuff so so
anyway so I ended up changing my major
took me next year to graduate as an
undergraduate but I liked it and the the
last some are the last two semesters of
my undergraduate I basically had to
fulfill a lot of coursework for math
because Allah the journalism stuff
wouldn't transfer in it wouldn't fulfill
the degree requirement and so I went to
the the medical campus and my university
had a very very good graduate program so
I kind of took the whole first year of
there or most of the first year of their
graduate department in my last year was
an undergraduate and that I went pretty
well and so when I I applied to that
program and got into it and that'll
worked out really well and so then from
there just started working left there
and worked at a diagnostic company for
about six years and that's kind of where
I started on the the modeling part you
know the part of you know at the time
like neural networks and generalized
additive models were the big thing you
know trees like random forests wasn't a
thing but bagging was just starting to
be developed and so you know I had some
coursework in that but not much but then
you get thrown into a an RD group where
you have to take we we would sell
instrumented diagnostics which are these
big you know boxes where you know a lab
person would put samples in like you
know let's say like 100 samples and the
Machine would analyze them and take
optical readings and other stuff and
they needed people who could take those
readings and produce an actual patient
output so you get some some signal
curved you got to say oh this person has
tuberculosis and this person doesn't and
so that quote-unquote algorithm work was
really what you know let me sort of
build those skills that I got training
some training for in graduate school but
that's where I started to gravitate more
into that type of area and then I left
there and went into farm and drug
discovery
interesting yeah so what specifically
I'm gonna grab my coffee this is gonna
be good so what what it would
specifically drew you to the field
though so I understand the sequence of
events that occurred that land you here
but it's not by chance like there's
something about working with data and
and and and these models that you're
building that really appeals to you and
that got your attention and what is it
about that that you think is so
satisfying I don't know I mean the
things the things that kind of Drew me
in you know the first time I had to take
a real computer science course it sounds
kind of silly but I was so attracted the
idea that you tell something to do it
and it does it right I mean that sounds
stupid but like you know even with if
you tell to do something you don't
intended to do it's going to do it and
so there was like this nut power trip
but kind of this idea that you know if
you deal with people you never get that
right and so and so you can actually
actually do what exactly you can
things that do what you actually yeah
it's become more relevant as I become a
parent so so that was kind of appealing
it was also appealing where you can look
at what's underlying all this stuff and
sort of get a good sense of what's
really driving things a friend of mine
once told me and I don't think he meant
it as a compliment per se but he said
that I liked bringing order into chaos
like you know you you I think what he
was trying to say is you like being in
chaotic situations and then you want to
impose order on it and and a lot of
that's what modeling is such a ton of
data in something it might be crapped
and some might be good and you want to
find the signal and all that noise yes
and to me that's really appealing and
just you know it's a good problem to
have when you have for solving problems
having that skill set
you know people bring us problems all
the time they don't really say do
clustering on this they say you know we
need to predict what's going to be toxic
and what's not and you have this sort of
playground where you can you know attack
in many different ways but you're kind
of the person that is you know along
with the scientists and getting their
input and all their scientific
background but you're the you're the
person that's really going to take
something and make information out of
just data mm-hm and that's a really
appealing thing yeah thanks that's
pretty interesting so so back to the
question that I let onto earlier you
mentioned that that you guys are really
close to the source of the data you're
often proactively going and fetching the
data that you want to work with to
answer some question that's in your mind
sure that's the type that can be a
time-consuming process right and then
often the data is not probably in the
format that you'd like it to be and
there's some time spent cleaning it up
most of the attention in data science
ends up being on the model and
employment or sexy stuff right but but
in reality you always hear that people
end up spending a lot of time on these
other aspects of the problem yes what
would you say the breakdown is roughly
for the work that you do well I think I
kind of luck out because our systems you
know not necessarily spectacular data
systems but at least they're well
understood at this point and so it's
easy for me in
in the environment I work in to get the
data that I need and get in very quickly
usually into a format in which I need it
but that's because of the situation I'm
working in you know we have other we
have other things where especially we
can work with collaborators outside of
you know industry and let's say in
academics you know they'll just dump
stuff on us and we kind of figure it out
so you know the average problem it's
maybe like 10 or 20 percent but once you
VR outside of our data systems that's
probably at least 50 percent of the time
interesting so you you are the exception
of the rule you actually get to spend
80% of your time building models doing
the fun stuff yeah mostly so yeah so we
we are pretty paranoid and so you know
we'll spend let's say 20% of time
getting the data we'll spend maybe
another 20% just making sure it's what
we think it is or what it's supposed to
be or if there's any issues with it you
know we'll spend maybe another I don't
know 30 or 40 percent doing the analysis
maybe less than that because lately what
we've been doing is we just been
concentrating on how to make we've been
doing this for quite a while how to make
the results more consumable by our
customers right so you know in shiny and
knitter and a lot of stuff like that is
really what we concentrate on now but
just to give an example we used to do a
lot of computational biology where we
had these you know like maybe it's 20 or
30 experiments a year where we have very
large-scale RNA expression profiling
analysis and so and we do these design
experiments maybe there'd be like seven
or eight contrasts in the design but for
every contrast you generate about 54,000
p-values and full changes and so you
know we really quickly realize that our
scientists were very overwhelmed because
you drop like a 30 megabyte excel file
of just p-values and fold changes and
genes
yeah and then you know it's not helpful
for any money so you know a lot of times
now we spend more time not necessarily
on the analysis but just on how we're
gonna allow people to easily consume
these right right the you you get some
results but your job is not done you
have to communicate those in a way that
helps inform someone to take action
based on that ideally and to to be able
to solve some tangible problem either in
biz
certain medicine or whatever is
communicating the results is not
necessarily just giving them well here's
my memo or here's my excel file or
results right yeah right it's taking it
the extra step yeah that's interesting I
would say like here's a but the the one
part I want to emphasize though of all
that percentages I gave you is the
paranoia part because you know I don't
know if this is original quote but I had
a professor that said basically the only
way to be comfortable through data is to
not look at it and that's been our
experience I was talking to somebody the
other day where you know we got some
data from a from a well-known University
in this town and and they were pretty
much the experts at this type of assay
and screening information and was based
on gene so you know they would do they
would give us a results back for a
specific gene and it's just you know
they weren't data people they were
really good at the biology and so you
know we get a ton of data from these
people
and you know and so looking through we
start trying to merge these results in
with their teen names and and it turns
out lungs hopefully is long story short
there are these things called Hugo IDs
which are identifiers like you know
il-18 for interleukin 18 and so on but
it turns out there are some genes whose
hugo ideas like SE po5 or something like
that which you know these people who put
the data in excel and they'd save it in
excel says oh that's September 5th you
know and then they take it and they you
know Excel translates that to a date
which they stores our reference date
from some you know relative time versus
some reference date right but then they
copy and they piece it into a CSV file
which then stores it as a number and so
I think there are some Hugo IDs that are
actually numbers but it seemed kind of
odd yeah and you know it took us about
four days to actually back figure I we
could figure out the ones that that were
missing and figure out they were ma r2
and SCP five and and so you know
basically you get data and you do this
you know that we call it forensic
statistics now to figure out what's
going on in this data yeah there's a lot
of that we do yeah an internal data
external data special so at the end of
the day you can build the best the best
models in the world but if you don't
check the integrity of the data sure
if you don't have confidence in the data
then your results or little value to
anybody and people remark on how far we
take that sometimes yeah we have reasons
yeah yeah that's that's really
interesting a little bit of healthy
paranoia goes a long way yeah hopefully
he's helping you yeah okay cool let's
switch a little bit and talk about so a
lot of the people that come to data camp
are total beginners in the world of data
science so maybe they're just learning a
program for the first time or they're
new to statistics and data analysis what
what advice do you have for people who
are just getting into the space
obviously there's there's so much
promise data is pervasive in every
industry yeah but for that reason it's
it's both motivating and intimidating
right because it's hard to know which
direction to go there's so many
possibilities so what did what advice do
you have for those people well you know
I'd say um I'd say work with data that
you understand at first because you know
you know the thing is I have a ton of
great datasets but most of them are very
scientific and they are not really your
teaching datasets because and last year
let's say a computational chemist or
something like that you know it's knock
and these things aren't gonna make they
barely make sense to me sometimes right
and so you know the thing I would
suggest is you find data whether it's
data in the public domain or data you
can even collect or so on something that
you know well and that will help you be
better at data science because it's it's
more real to you so you know when I
teach I use data for like housing prices
because we can all relate to you know a
house and a square footage and you know
longitude and latitude and things like
that so you know there's tons and tons
of great datasets out there but a lot of
times we don't necessarily have any
intimate connection to them and and it's
not that you need to have that intimate
connection but at least you know what to
look for him what not to look for
something seems odd you know I was
giving a presentation you may have been
here where I was talking about this one
model that was predicting as the number
of bedrooms or bathrooms increases in
houses the price of the house was going
down it seemed kind of wrong
to me right but you know once you look
at it a little bit you realize that
those were four houses that had very low
square footage of like let's say you
know 700 800 you know a thousand square
feet and if you have a house with five
bedrooms and in under square feet the
house was all bedrooms yeah nobody wants
to buy right so so you know if that were
some you know crazy high dimensional
biology experiment I'd you know would
never really look at that and say oh
there's something weird that what is it
about that so you know work with things
that you understand well even if it's
data you want to scrape off the web
things like that just just have
something that you know you can
well validate and understand and that
are really I think energized people some
of the best things I've seen there was a
great analysis a few years ago of the of
the t system here in Boston and the
visualization and analysis was
unbelievably fantastic and I think they
wrote something on the website that
basically said you know you sit on the
train long enough you start to wonder
about these things you know so I think
they're better for that yeah that's
really interesting I I see that the
value in two ways the first is its
intrinsically motivating if the data set
is relatable and interesting to you then
you're you're motivated to do the
analysis and to answer whatever the
question is about that data set that you
have but the other thing is like you say
back to the conversation about ensuring
data integrity it's it's hard to know
what's weird about a data set if you
don't have any context yes so if you
have context about the subject matter
then you can look at that data and say
yeah this this doesn't fit like this
part of the data it doesn't fit with my
mental model of what what what that data
should look like yeah yeah I have to
have a good meter another I'll
say thanks for dropping that yeah so
let's see what what else do I want to
talk about so there's so much
interesting stuff happening in the space
right now I want to hear from you what
you think some of the most exciting
trends in the data science machine
learning communities are right now
there's a lot of hot hot topics for
example deep learning is is one of those
everybody's talking about deep learning
what are you most excited about and
where do you see the biggest the biggest
gains coming from in the field over the
next ten years yeah I mean we talk about
deep learning in big data all the time
and everything is about context so where
I work I probably have very few examples
of what most people these days would
consider big data our data is very
expensive a lot of times to come by it
and so you know we don't necessarily
have you know we have data sets that are
upwards of you know half a million
records and things like that but those
are data sets that are accrued over many
years we don't really have much data
that you know is immediately available
like you know click-through rate and
things like that so I may not be the
best person in this so you talk about
that that the thing I would say is that
so I get this question when I teach a
lot of times and people ask me about Big
Data and I've heard other people who
have independently said this I know I'm
not the only person who thinks this but
you know big data is not initially
better data and there's a cost of big
data so what's funny is whenever you
hear anything about big data it's almost
immediately followed by rate perhaps
even started with the idea of our using
Hadoop or using this or that and so I
think what's happening is people aren't
really focusing on what the problem
they're trying to solve is they're
trying to figure out the mechanics to
solve a problem and I think that's a
really bad approach that if you're gonna
you know form should follow function and
and so if you don't have a problem that
is begging to be solved with something
like Big Data then I would say don't
think about it don't let that get in
your way because what we find is the
constraints of having that much data
limits you to the types of models you
can do or you know for example I TransAm
Buhl's are fantastic but people say well
you know why can't I fit a random forest
with a couple petabytes of day and then
it's like oh my god you know random
forest fits unpruned trees that's a
massive massive tree and you do that a
couple thousand times you know a more
compact model would probably do much
better and take less time to solve that
problem and so there are certain things
there's just certain costs along with
having that much data you know I see
people do like logistic regressions on
that much data and all it's doing is
it's making your stain
there is infinitely close to zero right
so it's not much gain into it but I see
plenty of cost to it so I would say
let's do that if it's if it's adding
something if it's just capturing more of
what we're already getting then are you
doing is adding cost to the problem not
any resolution sure okay so maybe maybe
a little bit of media hype around some
of these ideas yeah it's media hype
around everything yeah so where is the
gold then where do you where do you see
the real value coming from over the next
10 years honestly it's so
subject-specific I would say that it
seems like it's obvious but you know we
think about what we cannot honestly get
quickly and more of like you know how
many how many more samples can I get is
you don't see people concentrating a lot
on what better data can I get right so
for example if you're if you're modeling
something like public transportation
trends right if you're trying to pick
ridership on the tea right one of the
things that would go into that well
there's a lot of lot of factors you can
think about when you go to model that
data but you know you would you would
look and see about okay what date or
don't I have and it might be hard to get
so just adding more variables that you
would never considered before is almost
always better than just adding more data
so it's a better data collection
practices yeah and that's not fun or
sexy or anything like that it's just
that you know I think what we do is
modelers is you know I think really
modeling is not that big of a you know
not not to be where our focus is it's
not whether it's rain of forests or
rotation forests or boosting I think a
lot of times what we don't do is we
don't think about well how's the data
coming into the lawn or what data don't
I have I don't think people are thinking
about that and it's not something that's
taught a lot because it's not it's not a
publishable thing and it's not a big you
know it's not a big tagline and so that
that's to me what I see a lot of times
is um just people taking whatever they
happen to have and just dumping in a
terrain of forest yeah it's not that the
models no matters just that the models
can only do so much with a substrate
they have in getting better predictors
and I and I live in a world where we
have a lot of error in what we measure
right if you're if you're doing image
analysis and other things you don't
necessarily deal with the same problems
so again from the context of where I
stand
you know just having higher precision or
better data would solve most of our
problems not necessarily having more
data I mean you mentioned deep learning
I like those models a lot but I don't
really they don't really help me solve
the problems that I have I think they're
really good at cases where you have a
lot of data and even more unlabeled data
so I think there I want to call them a
niche sort of model but I think that
they're you know what the chemists would
call their applicability domain is is
thought to be much wider than it
actually is you know we have people who
we had a person come in for a job
interview and he was a deep learning guy
and he was kind of offended that he
would be working on data sets with less
than a thousand data points and so it's
my fingers off and so so you know the
the context of what I'm working on on a
daily basis is not necessarily
meaningful to be learning even though
that's a incredibly important field and
they're making really really great
contributions and you know I read that
later trying to stay up with it but it's
not somewhere that you know in a job
like mine were you dealing with you know
maybe hundreds of thousands of variables
in less than that and in terms of data
that's not necessarily where you
probably would want to go with deep
learning yeah that's really interesting
and so the challenge there is in many
fields like the people collecting the
data or making decisions about what data
to collect are not the same people who
are actually doing the analysis or
building the models and so there's a
disconnect there often and the modeler
is often just in a situation where they
they have to make the best of the data
that they're provided so you could make
the argument that because I agree with
you I think data quality is super
important and you can make the argument
that like better just better data
literacy like as you if we can if we can
increase data literacy and in our
society that that hopefully lifts
everyone up makes people more aware
around basic basic statistical ideas
like the notion of confounding right oh
you mentioned
the idea of like bedrooms bedrooms
increasing the number of bedrooms
actually decreasing the price well you
have to know something about the square
footage of the house to understand why
that relationship exists and that that's
everywhere yeah and more pervasive than
that to me is making sure that you're
asking the right question and so you
know a lot of times we have people who
come to us and say hey I'm clustering
this data and it's not working the way
we wanted to and they'll talk a lot
about that and we have to sort of pull
back and say okay what are you trying to
do like that's not really what your
question was you don't wake up this
morning and say gosh my cluster these
data you're trying to solve some sort of
problem and that's the tool that you're
using and so like a good example this is
we had an effort within my company to
predict got a bunch of people from
different disciplines together and we
were trying to predict a very specific
type of toxicity in drugs and that
seemed like a prediction problem to me
where we're trying to predict you know
here are the qualities of my drug can
you predict whether it's toxic or not
and we spent a few days talking about it
but when we when we're talking to the
toxicologist they were thinking about it
much differently than the stated goals
so they were really interested in making
associations between risk of toxicity
and in like a laboratory test that they
were running so the compound didn't it
wasn't the compound wasn't involved but
the compound wasn't the thing that they
were trying to make an inference or a
prediction on and so you know when you
talk to him for a bit you know everybody
was all you know deep learning or you
know statistical prediction and things
like that but really at the end of the
day they didn't want necessarily
predictions they wanted to come up with
Association so they could know well what
should we study more to be able to you
know maybe you're from not predict this
toxicity but it wasn't really the
question they were looking for and so I
think what happens is there's something
wrong with that necessarily but I think
people get really overwhelmed by the
what they read in in their reading about
you know things like Google's
self-driving cars and stuff like that
and they think well how can we do that
in my job they're not another thing
about well here's the question is that
the right approach to answering that
question right so we do spend most of
our time trying to make sure that the
the stated question is really you know
that'll tell us the question as we work
with them we understand the question
more and sometimes it's not necessarily
with
thought it wasn't me yeah so it's a good
point so there's probably two two things
that we could do a better job at to
improve the quality of our data number
one improve data literacy for everybody
as much as possible and number two
involve the people who are doing the
analysis and the modeling
earlier in the process yeah right get
them involved in data collection or at
least talking to the people who are
involved in data collection so that
we're taking into consideration all the
right things because that has so many
downstream benefits yeah that's that's
really interesting okay cool I think
that's all the time we've got
okay this is super interesting thanks
for having us always appreciate coming
here yeah cool right thanks nice thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>