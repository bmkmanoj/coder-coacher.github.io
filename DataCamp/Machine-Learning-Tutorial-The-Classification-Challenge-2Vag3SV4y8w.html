<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Tutorial: The Classification Challenge | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Tutorial: The Classification Challenge - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/DataCamp/">DataCamp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Tutorial: The Classification Challenge</b></h2><h5 class="post__date">2017-03-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2Vag3SV4y8w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we have a set of labeled data and
we want to build a classifier that takes
unlabeled data as input and outputs a
label so how do we construct this
classifier we first need to choose a
type of classifier and it needs to learn
from the already labeled data for this
reason we call the already labeled data
the training data so now let's build our
first classifier we'll choose a simple
algorithm called T nearest neighbors the
basic idea of T nearest neighbors or K
and n is to predict the label of any
data point by looking at the K for
example three closest labeled data
points and getting them to vote on what
label the unlabeled points should have
in this image there is an example of KNN
in two dimensions how do you classify
the data point in the middle here
well if K equals three you will classify
it as red and if K equals five as green
to get a bit of intuition for K n let's
check out a scatter plot of two
dimensions of the IRS data set paddle
length and paddle width now the
following holds for higher dimensions
however will show the 2d case for
illustrative purposes what the knn
algorithm essentially does is create a
set of decision boundaries and we
visualize the 2d case here any new data
point here will be predicted
so Tozer any new data point here will be
predicted virginica and any new data
point here will be predicted versicolor
all machine learning models in
scikit-learn are implemented as Python
classes these classes serve two purposes
they implement the algorithms for
learning a model and predicting while
also storing all the information that is
learned from the data training a model
on the data is also called fitting the
model to the data in scikit-learn we use
the fit method to do this similarly the
predict method is what we use to predict
the label of a new unlabeled data point
now we're going to fit our very first
classifier using site
learn to do this we first need to import
it to this end we import ke neighbor's
classifier from SK learn neighbors we
then instantiate our ke neighbors
classifier set the number of neighbors
equal to 6 and assign it to the variable
KN n then we can fit this classifier to
our training set the labeled data to do
so we apply the method fit to the
classifier and pass it two arguments the
features as an umpire array and the
labels more target as an umpire array
the scikit-learn
API requires firstly that you have the
data as an umpire array or panda's data
frame it also requires that the features
take on continuous values such as the
price of a house as opposed to
categories such as male or female it
also requires that there are no missing
values in the data all data sets that
we'll work with now satisfy these final
two properties later in the course
you'll see how to deal with categorical
features and missing data in particular
the psychic learn API requires that the
features are in an array where each
column is a feature and each row a
different observation or data point
looking at the shape of IRS data we see
that there are a hundred and fifty
observations of four features similarly
the target needs to be a single column
with the same number of observations as
the feature data we see in this case
there are indeed also 150 labels also
check out what is returned when we fit
the classifier it returns the classifier
itself and modifies it to fit it to the
data now that we've fit our classifier
let's use it to predict on some
unlabeled data here we have a set of
observations X nu we use the predict
method on the classifier and pass it the
data once again the API requires that we
pass up the data as an umpire array with
features in columns and observations in
rows checking the shape of Xu we see
that it has three rows
and four columns that is three
observations and four features then we
would expect calling KNN predictive xu
to return a three by one array with a
prediction for each observation or wrote
in x new and indeed it does it predicts
one which corresponds to versicolor for
the first two observations and 0 which
corresponds to sat OSA for the third</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>