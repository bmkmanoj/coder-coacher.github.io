<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: “A lock-free concurrency toolkit for deferred reclamation and optimistic speculation&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: “A lock-free concurrency toolkit for deferred reclamation and optimistic speculation&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: “A lock-free concurrency toolkit for deferred reclamation and optimistic speculation&quot;</b></h2><h5 class="post__date">2016-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uhgrD_B1RhQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today's talks going to be about a
concurrency toolkit that we're proposing
for the C++ standard it follows on as a
natural follow ordering beep after that
you have share pointers and then of
course atomic share pointers from C++ 17
well not exactly I guess it's it's it's
in the concurrency tier so it's not
really it's not really in there yet but
it is in the concurrency t-80s
and the follow-on after that we think
naturally is something called hazard
pointers and recopy update I'll see you
I'm gonna skip past most of these things
I'm essentially giving you a kind of a
high-level motivation and then we're
going to be followed by the actual
authors of hazard pointers and I'll see
you coming up to explain what these
these interesting objects are but the
agenda today is essentially we're gonna
start with something of a play on words
Ernie Irvan Schrodinger's ooh and Werner
Heisenberg's advice the increase that
increase uncertainty increase
uncertainty can get you performance and
scalability so as a result get given
that that knowledge procrastinate away
and one way we're going to do that is by
using structured deferral and we're
going to take a look at a table that
we've been working for over two years
comparing the various features and
properties of share pointers atomic
share pointers versus hazard pointers
and recopy update and then finally
maggots gonna come up he's the author
the inventor of hazard pointers and paul
mckinney it's gonna come up and he's the
author and inventor of recopy update and
explain to find details for that and
we're gonna finish that with a
conclusion that this is appropriate with
this along with atomic share point
integer pointer forms the beginning of
an appropriate concurrency toolkit for
C++ so no installing glues would like to
construct an in-memory database to keep
track of the animals in his zoo this is
a thought experiment just like many
things in physics I have a background in
astrophysics so I like these kinds of
thought experiments birds would of
course result in insertion into this
database but deaths would result in
deletions the database is also query by
those interested in the hell
and welfare of shorteners animal
Schrodinger's has numerous short lived
animals like mice resulting in high
update rate high update rates in
addition there is a surprising level of
interest in the health of Schrodinger's
cat so much so that Schrodinger's
sometimes wonder whether his mice are
responsible for these many many queries
is the cat still alive is it similar is
it still alive so regardless of the
source the database has to handle a
large volume of cat related queries
without suffering from excessive levels
of contention both accesses and updates
are typically quite short involving
accessing or mutating an in-memory data
structure and therefore synchronization
overhead cannot be ignored so shorting
also understands however that it is
impossible to determine exactly when a
given animal is born or do or die for
example suppose that his cat's passing
is to be detected by the heartbeat when
a heartbeat stops he's it's dead now set
now we use instruments to do that and of
course seconds or even minutes would be
required to determine the poor cat's
state of health so the shorter the
measurement of obviously the less
certainty the measurement so that a pair
of veterinarians my examiner cat and
would disagree on the exact moment of
the cat's death for example one might
declare the death thirty seconds after
the last heartbeat well another might
insist on waiting a full minute in which
case the veterinarians would disagree on
the stay that he had during the second
half the animal a second half the minute
after the last heartbeat fortunately his
good friend Werner Heisenberg has taught
Schrodinger how to cope with these kinds
of uncertainty the delay in detecting
the cat's passing actually permits use
of synchronization via procrastination
we all like to procrastinate so after
all given that the two veterinarians
pronounced no death were separated by at
least a full 30 seconds a few additional
milliseconds of software procrastination
is perfectly acceptable in this case so
in fact Schrodinger's design goes
further by actually explicitly it's
actually exploiting the ambiguity and
uncertainty he does this by
refusing to require that queries and
insertions and deletions be fully
ordered as we will see this design
choice is going to enable extremely
high-speed queries so now if we step
back from read to real life this
situation is not limited Schrodinger's
Xue xue's database any situation in
which data within a computer is a
function of events and entities outside
the computer for instance similar
uncertainty will be enforced by the
speed of light delays by the time a
given change has been committed to the
systems of memory it might well have
been superseded by some other change it
might not even be possible to determine
the time order of several external
events at the Sun for instance explode
this very minute we would not find out
about it for another eight minutes okay
worst yet these are situations were
protest situations where protocols
introduced additional delays for example
such delays are commonplace common
practice in Internet routing protocols
these routing protocols delays are
absolutely necessary because we have to
use it to preserve Internet stability
however they do introduce long periods
and this is the key during which a given
system connected to the Internet might
be uncertain of the Internet's topology
the applicability of an order lookup is
now quite broad if you follow this
analogy so suppose an algorithm uses the
common idiom that makes a decision while
holding a lock then relies on that
decision after releasing the log this
algorithm is in fact relying on obsolete
information because some other CPU might
have acquired a lock and change the data
on which the decision was based while
the first CPU is still relying on its
now obsolete position in short
synchronization via procrastination is
especially useful when interacting with
external states so given that knowledge
we've actually looked at a number of
algorithms and we've already has
something called there's it turns out
that there are four key approaches there
might be more but right now I just only
want to talk about four that can resolve
this problem one is actually a fairly
common idiom using reference counting
and indeed that's what share pointer
does the other one is something that we
added to the concurrency tiers called
atomic share pointer
using big using double compare-and-swap
reference counts now this is this was
quite interesting this took our group
quite some time to discover until I
talked to Anthony Williams and found out
he's pretty much the only ones who was
able to implement atomic SharePoint or
end-to-end using truly atomic increments
at reference counting as well as
reclamation the boost one claims it does
said the booster Thomas your point of
claims it does it but it doesn't really
do reclamation folio we talked to P the
deema about that as well too so we can
see how challenging this is that this is
I'm gonna talk a little bit more about
that but the third and fourth part is
these really are the really interesting
part we now have these but these two
toolkits additional toolkits hazard
pointers and recopy update which we're
going to offer up to the C++ standard
for Standardization for inclusion into
the concurrent CTS which means that it
could land in c++ 20 so in short what
I'm saying is that synchronization which
is the convoy procrastination which is
the commonality of all these powders is
especially useful whenever you're
dealing with external state okay so to
see how this edit so this is a common
idioms I've talked about the basic
problem ultimately it's just about the
finite speed of light and of course the
fact that atoms and electrons that carry
our information can really move any fast
even though they've been moving at the
same speed every time now we're saying
god damnit you gotta get to do it faster
so just a little bit recap we you know
about the C++ 11 smart pointers they're
error-prone there was Beth this was one
of the primary reasons why were you add
it to them for atomic share pointers now
some people think that it was just
syntactic sugar that we added at that
atomic share pointers and it's it was it
wasn't with with the original share
pointer it's somewhat inefficient
because the atomic compare extrange
strong is a free function taking a
regular share pointer and we don't want
extra synchronization in SharePoint to
itself so this slide I actually took
from Anthony Williams blog which I
thought was really nice a simple
solution for showed as showing there's
cats showing a zoo it's a place a
reference counter of course in each
animal's data element within a hash
table with
collisions handled by chaining so the
reader is atomically increment in
reference these are the mice before
accessing animals data elements and
atomically decremented afterwards so
this provides synchronization only
between readers and updaters updaters
has to synchronize among themselves
using other alternatives using other
mechanisms such as locking non-blocking
synchronization or even transactional
memory something I'm fairly familiar
with so here in this particular example
the stood she pointed works great if
there's an instance of it in every
thread provided so that changes to the
reference count are synchronized
everything just works if your share data
is correctly synchronized you would need
to ensure that it's safe to call in this
particular example according to Anthony
Williams and we agree that it's safe to
call my class do stuff and do stuff with
from multiple threads concurrently on
the same instance but the reference
counts are handled okay
however the minute you move to some of
the words are moved over that's cool
that's close enough so here here we I
want to go but I want to look at what
this really means so when you have
deferred reclamation via reference
counting what you're doing is you're
combining waiter waiting for your
combines waiting for readers and
multiple versions so the four-state
process of removing the data element
corresponding the Schrodinger's pork
head is shown here the initial state
here shows one chain of the hash table
representing Schrodinger's boa cat and
canoe as indicated by the red color of
each box any number readers might be
referencing these data elements so
therefore updates has to be carried out
carefully to avoid disturbing these
readers to transition to stay to the
updated stores a pointer to the good
news data element in the next pointer of
the Boas data element so this store has
to be atomic in the sense that any
concurrent reader has to see either the
old value or the new value not some
mashup of the two so such a store might
be carried out using a C++ 11 at relaxed
atomic variable or an Oda compiler some
sort of volatile caste know that the
cat's next pointer continues referencing
the canoes data element to accommodate
readers still referencing the cat so
from this point on onward there's no
path to the cats data element
indicated by the yellow color okay at
which point we're waiting for reference
to drain so now readers cannot gain
access to it once the catch reference
counter reaches zero transitioning to
state three all the readers are had a
reference to their caste they have to
release their references indicating by
the green color of the cat's box and
because there's still no path to the
data telecast a donor element new
readers still cannot gain a reference to
that okay so going back to why we needed
Atomics share pointers it turns out that
if you're going to access this from two
threads the ship that there's the
sharepoint or from two threads you have
really two choices one you could wrap
the whole object with a mutex so only
one thread is accessing the list at a
time a big hammer indeed this is what
you have to do with the standard library
if it's not thread safe so we could also
try and allow concurrent access but
there are problems these problems I'm
not going to go into them because it's
quite broad such things like removing
from the front of the list the race the
race condition on the head or multiple
threats calling pop front although into
interestingly it doesn't suffer from the
APA problem but with atomic share
pointer it turns out that it can
guarantee atomic access it can be
implemented more or less efficiently and
this as we had a conversation with
Anthea Williams I'm glad he's here to
confirm what I'm saying the
implementation may use a mutex to
provide a synchronization in atomic
shell pointer as we discussed but you
can manage it using making a lot free
and you can query it with the is lock
free function that's common in all
atomic variants but with a long but a
lot for your Tomic share pointer
what he's doing is he's using a split
reference count for atomic share pointer
this is what is commonly used known as a
double word compare and swap so the
share point of control block basically
holds a count of external counters in
addition to the normal reference count
and then each atomic share pointer
instance that holds a reference has a
local count of threads accessing it
concurrently what this really means is
now we have a su of choices between all
these excellent mechanisms and what I'm
here to do and what these guys are here
to do is show you which technique to
choose when so this is a table that we
worked on for over two years
referencing many talking to many people
and it's about beyond performance you
also need to choose from other
properties of lock-free programming so
across the top you have reference
counting which is essentially what share
pointer does you have reference counting
with double compare swab which is
essentially what atomic share point to
it does and then we have aa Cu recopy
update that Paul McKinney is gonna talk
about and has a pointer that maggot
Michael is gonna talk about I've written
red lines across because even though
these many of these properties are
intertwine you can kind of separate two
properties that a little bit of a flip
point side of the other each other so
you can kind of kind of see so here
across the top we have things like
unreclaimed objects what do we do in
this case I'll see you with is it's
unbounded it's a notice of what I need
to mention what the colors mean the
yellow color means it's a good property
the white means it's not so good okay
the green one means it's really good
okay so when you're making choices you
need to think about whether the with
whether the unreclaimed objects here are
bounded or unbounded in this particular
case obviously OCU stands out as being
one of the one of the less preferable
choices because in Aussie you the
unreclaimed objects can be very large
but this is what turns around it gives
it forward progress which is what the
next column about the next column about
non-blocking Traverso is about forward
progress on traversals the traversal
forward progress that's really well if
the trade-off here for our c4 RCU 4 has
a pointer for 4 for the next 2 we're
going to talk about we're talking about
what it means to will have forward
progress on the reclamation as well as
the traversal speed the for progress on
welcome information is as identified as
really good for hazard hazard pointers
because it has a fixed number of hazard
pointers and you just basically go walk
through it but IndyCars okay it is an
atomic operations that can potentially
fail as I understand it could starve
okay
I'll see you is blocking is the less
less capable when you is because
one reader can get stuck and so you can
reclaim so you can't reclaim anything
until that reader gets unstuck okay and
this is what gives it the Ford
properties car goes guarantee property
the traversal speed for RC you can just
be single thread loads it doesn't even
have to be parallel for hazard pointer
it has an overhead Putra traversal
pointer which might have changed you
have to store in the hazard pointer that
you would need to check the pointer it
hasn't changed and make sure everyone
else also see the store hence the slow
load fence in that particular case the
next area talks about reference
acquisition contention among readers and
atomic automatic reclamation with
regards to reference acquisition it has
to do with the chain of pointers that
you would have to unzip if you actually
finish removing one of them it turns out
that reference counting wood decals and
also you do really well on these things
okay you just do two atomic operations
this is essentially it's essentially
done with D casts you might have to
there is a there's I think there's
potentially six atomic operations one to
increment a straight cut on a pointer
they'll go to the ink and then it would
go in and increment the reference
counter and you would have to go back
and decrement the rough count okay so
essentially with the other two with you
can tell that both RCU and hazard
pointers have very behaved very well
with regards to high contention among
readers if you have many readers this is
where I'll see you and hazard pointers
shines and by the flip side of the coin
you see that automatic reclamation is
better because with reference counting
and reference counting with decals they
don't it's all automatic whereas you
have to manually ask to reclaim objects
in RC you and hazard pointers so you can
see that all these are essentially
different sides of the same coin
there's one last report there that isn't
really a big performance consideration
the domain there are different meanings
for domains for RC u and hazard pointers
they don't apply in the reference
counting case so with that I want to
conclude and ask that the as the hazard
point to invent a magnet Michael to come
up and then
you'll be followed by Paul McKinney so
I'm gonna see okay well so I'll continue
the talk and talk about hazard pointers
and describe what the problems are souls
and how does so and what we're you know
what we're proposing for Standardization
and so we'll start with a running
example a white comparing set imagine
that you have like a very large memory
you know memory block that you want to
apply compare and set on and it doesn't
fit in any standard like atomic
operations like on hardware or supported
by language so one common solution is to
instead of like having the the object
embedded in places just you could
represent it with a pointer and then
copy and copy and write whenever you
change it so every new value is just a
allocate a new block and you replace the
old one so this is kind of a naive
implementation it's incorrect but a
class for white compare and set so you
have like whatever the type is you have
the value and you have a point ahead a
pointer to a node and when you do
compare and set you you load the old
value and you try you dare you compare
is it you know does it contain a value
that that's equal to what you expect and
if it does you allocate a new node with
a new value and you try to do a compare
exchange and you know if you succeed you
delete the old one and you return true
this is incorrect because of multiple
problems when you access the value you
actually don't this is unsafe access
because you don't know if this block is
actually still there and still
accessible you're doing compare exchange
with like using a value that that you
assume that it's actually like if it's
still there that means it hasn't the
the original pointer hasn't changed but
actually it could have changed and the
meaning of that pointer has changed so
that's the ABA problem and of course if
we just do delete P that's you know
we're we're reclaiming without really
knowing if there are other threads that
have reference to that block so that's
unsafe reclamation so I'll start by like
just going through that okay going
through an example of the AV how they'd
be a problem would occur in a scenario
running this kind of code so is the same
the code as was in the previous slide
but just like condensed and lines in red
these are the responsible lines for
causing the problem so we start by
having a thread I reading the value a
from the pointer P this here you know
the point that represents the the object
and that's right I will read the value
you from from that block a so that's
fine it looks like we're gonna it says
we're gonna proceed to try to replace
that block in the meantime thread J
changes the value and actually locates a
new Block B that has some different
value W or or whatever you know some
value W and and also thread J reuses
block a to hold the value Z which is
different from you that it used to hold
before now thread I yeah right so thread
J sets up P to a again so now it
succeeds in doing that now thread I
comes back and allocates a new Block C
that holds the value V because it's what
you know okay now it's we are going to
try to do compare exchange on the
pointer so we're preparing the block
that holds a new value and we do the
compare exchange yes it's like you know
the pointer was holding was pointing to
block a
again and we mistakenly assume that
actually that meant that means that the
value is still you whereas actually we
did a compare comparing set where the
old value is Z different from you so I
should so this is like incorrect outcome
and the problem is that we couldn't tell
if the point are changed or not another
problem that we'll we'll solve using
Heather pointers is unsafe neural
reclamation and these two lines in red
are the you know the offending lines so
we start without having a thread I
reading at the pointer a from from I
mean the value a from pointer P and it
now it's about to dereference that
pointer and access the contents of the
note in the meantime another Fred J sets
P to point to a block B and returns a to
the operating system so now block ay is
unmapped thread I comes back and yeah
has access violation there is just not
it had no chance to check anything to
see if it you know if it is safe to
dereference that pointer or not so
that's kind of one case I mean all this
case ABA and memory reclamation issues
you I mean you could get like all these
kind of incorrect result corrupting your
data structure corazon SSD structure or
access violation so I'm gonna switch to
describing has a pointer and it is
simplest form it's really very very
simple it just a hazard pointer is a
single writer multi reader pointer just
like you know I need any pointers size
location and each has a pointer has one
owner at any time that can write to it
but then
Red's can read from it and basically
what what a thread is doing by setting a
hazard pointer to the address or an
object is telling other threads or or
itself even I mean also is telling all
threads that if any any thread remove
that object from being reachable to
create new references now it what if you
make that object unreachable after the
last time the this thread set the hazard
pointer to point to it
you must guarantee that you don't
reclaim it until I change the value of
that hazard pointer so that's kind of
the the message that it's sending and
that's from the user side the user that
the one that is trying to protect
objects using hazard pointer from the
reclamation side the threads that are
actually removing objects and retiring
them and want to reclaim them but they
cannot reclaim it right away what they
do is like in a simplest form they would
just like go and check all the hazard
pointers if it doesn't match any values
it's okay to free but if that's that's
inefficient so you would just like read
all the hazard pointers try to collect
them in some efficient structure that
has like you know constant expected time
lookup time and now you just have a
private list that has so you know
sizable enough that you're guaranteed to
that to reclaim something because the
you you will not fail to reclaim more
than more objects than the number of
hazard pointers they're only like you
know there are only that many choices of
values that will prevent reclamation so
this kind of algorithms simplest form
going back to that example this
incorrect would try to make it correct
and this is pseudocode this is not C++
what I insert it there is that like okay
I wear the hazards the header are
actually the unsafe access like here and
the ABA problem over here so before we
get to
you know the hazards we actually set the
hazard pointer to point to the block
that were we want to protect and then
that's where the store load fence comes
to play
so in general we wouldn't make this lock
free in a different context we might not
not care who might have a lock around
and we don't need friends or stuff like
that but in this case we want to load
again the the source of that reference
to check that it's actually it is in it
is reachable and by the way this is not
susceptible to the ABA problem it's okay
between these two lock steps that the
object is like you know retired and
reclaimed and reinsert it that's okay
because what matters is at this point
when we're checking that it is reachable
we know that the hazard pointer is
already pointing to it so it is
protected from this point okay and that
by doing that then we were guaranteed
that through and until we clear the
header pointer we have protection that
will not be reclaimed and because it's
not being claimed and it will not be
reinserted so that's worse for the
access and for a be a problem so that's
this is kind of more like the template
interface that we have right now and
that would look like like the the node
would inherit from some type that you
know add some capabilities and we have a
hazard pointer owner that automatically
acquires a header pointer so this kind
of on our AI interface and a protect
function that protects the value if it
succeeds we can proceed without worrying
about the object being reclaimed
prematurely when we end the end of the
scope of the has a pointer owner it
automatically clears and releases the
own header pointer from the so this from
the user side the user of the hazard
pointer this is for the removers point
of view when you have an object you
don't you don't delete it you retire it
basically you're kind of handing it over
to the has a pointer library to be
responsible for reclaiming it when it is
safe okay so I will have yeah some time
here so the the point of looking at the
system the health pointer system is that
you have objects in data structures or
represent resources and they can be
protected by users threads that use
header pointers that's right that own
and can write to hazard pointers to
protect these objects to have access to
them and to protect from a be it to have
a be a safe comparison
so the removers they have to read the
hazard pointers and to decide when when
it is safe to move this like to reclaim
these removed objects to make them free
for reuse and reallocation or well or
return to the operating system
you know reclamation resolves so it's
kind of a system view of of hazard
pointers the lifecycle of an object is
that you know it's allocated its
reachable so we can either like have a
shared way of reaching it or threads
hanging over references to each other
basically you can create new references
you can start you can have has a pointer
start to protect objects once some
thread makes the object unreachable that
no new reference can be created then has
a friends can continue to protect the
object but you cannot really start a new
protection and once it's retired then
yes it is it can you can still continue
to protect it from being reclaimed but
also there might be attempts to reclaim
it and they will succeed or fail
depending on are there references to the
object or not once you reclaim it you
can reallocate it if you want I wanted
to mention domain
in the context of other pointers a
domain contains a set of header pointers
and they that this admin may protect
protect a set of objects so I had a
point that would belong to one domain
because it doesn't make sense in that
case to you know to belong to multiple
domains we would have a default domain
because we don't want to bother the user
to define one if they if they're just
using the default domain but it's it's
good to allow multiple domains because
you can think of like you know two
threads communicating together in their
structure why would they bother by
checking the header pointers of like a
thousand other threads that are doing
something else
using hazard pointers that doesn't apply
to their like you know the special
communication between between them so it
really it is like savings in space and
time to allow multiple domains a thread
might operate in multiple domains
depends on what objects they are using
at that time I'm not going to get into
too much detail of this but this is it's
a really multi-dimensional challenge to
design like one to have one
specification of hazard pointers which
just look there are so many I mean there
will be users that want different
features
so I implemented this like many many
times and but it's always kind of
customized to certain use and but so
it's much simpler to do that so here's
like you know you have domains you have
to wait to have a default domain
multiple domains objects reclamation
won't be able to customize that metadata
do you want it to be embedded with the
object or separate hazard pointers I
mean that's like there are they fixed
number at compile time at runtime or are
they dynamic and they have in a link
structure they can do you have one them
pre-allocated because you don't want
exceptions or you know so lots of
options here that probably different
users will want different support
alligators
for hazard pointers for metadata
progress guarantee it's probably more
efficient to have what blocking by
default but if a user wants to have an
end-to-end lock free path they should be
able to do that thread-local that would
be you know better for D by default but
they can be problem for certain programs
so we should allow them to should allow
users to heaven you know TLS free path
and exceptions also we should allowed
users to be able to have no exceptions
and to end so I'll skip this and yeah
the the template interface that there is
a current snapshot of the interface
that's public in on github under fully
experimental and if the user doesn't do
any customization they really don't need
to know about the whole interface they
just need to know about like the
representation of the object that
protected by hazard pointer and the
owner type that actually kind of does
REI
an interface to the actual hazard
pointers so I'll just go through the
owner template as I represented so
there's a lot of change and in the
interface I should in what couple of
minutes so yeah the owner by default use
the default domain and you know and
doesn't do thread caching it we don't we
don't copy or move the owner because it
just like it is actually good to have
guarantee that the owner owns a header
pointer at any time so acquisition it
construction it acquires one and
destruction it releases it okay oh okay
then these are kind of the functions
that are that are on behalf of the
has a pointer to protect a reference to
set the value and appear the value and
then there is a free function to swap to
owners this is good for hand-over-hand
operations yeah this is like if we want
to customize there's a way to customize
the example I showed you for the
comparing set white compare and set I'm
gonna end with showing an example of
hand over hand traversal so here we you
know we have a reader that is like
looking for a value in a in an order
singly linked list and to do hand over
hand traversal it acquires two hazard
pointers so one kind of like for the
previous node and one for the current
node and hand over hand so they it
starts by you know have it has a
reference to previous and current and
now it's like assuming that the previous
one is already on solid ground and now
it wants to set the current has a
pointer to the current node so it's kind
of it's always doing one in every
iteration it's actually protecting one
more node and releasing protection of a
previous one so at the end you can see
here like I skipped some parts of the
loop the code is available in like you
know publicly so there are examples and
test ok sorry there's a typo here that
we it's just swap so you see here that
we we set the we kind of advanced that
prayer and and curved pointers we also
do swapping the owners so by swapping
the the ownership is actually we're just
swapping the the the header pointers
that are owned by the owners but we are
not changing the values of the head of
pointers themselves because come where
we swap them but they they continue to
protect the same node that's been
protecting and at the end there we what
used to protect the previous value it
becomes kind of free now that we can
reuse it in the next iteration but it is
called under like you know the current
Orange
that's kind of that can be as simple for
a user to use as if they don't need any
customization I'll leave it okay I'll
I'll hand over to Paul but I see you
alright we're here so this is just the
RCU part and this is just a quick
rundown of kind of our skews philosophy
the idea is we have a way of publishing
new data into a data structure and
there's an RS you assign pointer thing
that does that and there's things build
on top of that for lists and other
things and the reader can subscribe to a
given version of a given pointer with
our 2d reference and again there's ways
of handling lists other things like that
and the big deal this is very similar to
hazard pointers you have to have a way
of waiting for all the people that might
be interfered with by an update by a
destructive portion of the update and
here what we do is we we define a reader
with hazard pointers the readers are
associated with each object and that's
why they have a small number of memory
because they identify each and every
object same with reference counting here
what we do is we identify a range of
code and say anything that thing reads
it might read and that gives us the
speed and the weight freedom but it
means that we could have a lot of
objects hanging out so we identify with
our see read lock beginning and ending
with a matching our she would unlock
they can be nested and anything in there
is a reader of course you might want say
where's the RAI eye and we'll get to
that with a later slide but this is the
this is where we started from and an
update is gonna have to wait for all the
readers that are already in their read
side critical section that have already
executed there are C read lock to get to
that matching are C unlock before they
can proceed and that duration of time is
a grace period the easiest way is you
say synchronize our see you hit blocks
and then whenever it is done you've
resumed again unfortunately there's a
lot of algorithms that don't like
waiting and so we have an asynchronous
form call RCU where you pass it a
function and a pointer to the object
sort of and that one is we'll see you
later causes a little bit of trouble in
C++ but we'll get there fortunately I've
had some help from a number of key
so with even more help I might get there
this is insertion and it's just how we
get things inserted this is a very
simple data structure it's this is the
four states like Michael Wong had in his
presentation and we allocate something
that's the second state we initialize
that that's the third state and if your
memory allocator is working nobody else
can have reference to that so you can do
whatever you want that's why it's
colored green as soon as you use RC you
assigned pointer to make that pointer
point to your new object bang also on
the readers might be there for any
length of time at any time without
telling you about it and so from then on
you have to be careful so we have to
remove and this looks very similar to
what Michael Wong showed you for
reference counting and you get just the
operations are different but it's the
same process and the same sort of a
thing going on we remove the cat we use
C++ Atomics or we can to make sure that
the readers either see the pointer to
the cat or the point at the canoe but
either way they see a valid list they
don't see some moosh of the two pointers
that you might get if you fail to use
Atomics then you synchronize RC you do a
a synchronous wait for all the readers
once all the readers are done well the
old readers might have reference to the
cat any new ones there's no way for them
to get to it so once all the old readers
are done nobody's looking at the Caddy
cup the updater and it can do whatever
it wants with it including free it but
as Michael pointed out with reference
count same thing with RCU we can have
two simultaneous things Michael talked
about a veterinarian and there they are
if we have a cat that's heart beat
happened ways ago the two veterinarians
might disagree about whether the cats
are alive or dead and this again is
anything you're representing inside the
computer that's outside the computer
there's always uncertainty about what
happened in what order and what is
really finished for example if you
suddenly can't access the memory stick
the memory stick break or are things
just slow it's the same decision is
really generally applicable across
anything where you're interacting with
an outside device all right that's just
putting up there saying you can make it
really
simple thing that acts like RCU this is
what a reader looks like in C so we have
that our C read lock at the top this
starts the critical section we could
pick up a pointer and the nice thing is
that whatever player we picked up it's
guaranteed to still be around sometime
might remove from his data structure but
it's not gonna be freed until we get to
that matching our shoe would unlock down
there at the bottom we can do whatever
we want with that pointer with that
object it'll still be in memory it's not
gonna be reused there's no ABA it's
there and of course that's why we have
so much storage outstanding because it
has to leave everything that might be
reached there there as well but as soon
as we hit that our serial unlock that
thing might be freed immediately okay
but the nice thing is those primitives
all very lightweight and the updaters do
something very similar they probably if
you have multiple operator updaters they
might use a lot to exclude each other
they might pick up the old pointer make
the new put the new pointer in there
which they presumably initialize up
earlier and then once they release the
lock they can wait for the grace period
once synchronized RC returns they know
for sure there can't be any reader
referencing that old thing that they
just yanked out there therefore they can
just free it ok so the same structure
again as the reference counts the
difference is as noted earlier is we
have to tell RC you with the
synchronized RC hey we're interested in
freeing this thing with reference count
would happen automatically
ok this is a view of what a grace period
kind of looks like sort of schematically
we change something we wait for grace
reading yellow there and the grace
period has to extend long enough that
any reader that was there before the
grace period started so you can see
there's two of them that really did that
grace for as gatina those readers get
done once we get to the green portion
any reader that might see the state
beforehand they're completed ok so
that's just kind of a diagram of what
how this looks you could draw a similar
diagram for reference counters or
Heather pointers if you wanted to now
you can go longer than if the grace
Creek runs long no big deal there's some
memory that hasn't been freed ok if you
start the race period late that's still
ok because any reader that was there
at the early gray spirits either there
are gone by time you to the late grace
period you may wait a little longer than
you need to but things will work nothing
will break and this turns out to be
really important because if you delay
the start of the grace period you can
make the grace period cover multiple
changes so those two if we just started
right when that first change was done
we'd have to have two grace periods do
the work twice because we waited we were
lazy we were rewarded we had to do less
work it covered both guys and that means
our / update overhead is less and life
is good and we do that a lot
in fact we do it pretty viciously we
started later we get we amortize out and
it's not hard to do something in the
Linux kernel like make a big tar ball
and then remove it and have literally
thousands of updates using the same
grace period so it's a very good labor
saving device however that means if you
increase the length of the grace period
you're gonna have longer grace raise you
wait longer and you also increase the
memory usage which of course is one of
our so use potential disadvantages if
you have a small memory machine so this
is a design trade-off but that's the
design trade-off RCU is chosen okay now
let's get to the kind of the fly in the
ointment from c++ standpoint we have
this call RCU thing and what it's doing
you pass it in RC you head which is
normally embedded in your data structure
and that allows RC you to track all the
stuff that's waiting for a grace period
just links them all together and you
also pass it a function and that
function is gonna be called at the end
of grace period so diagrammatically it
looks like this so we had a change at
the right when the change is done we do
call our see you we hand it the pointer
to that RC you head structure we've
handed a function when the race-free is
over that function gets called and has
passed that RC head pointer works fairly
well we've been doing it for a very long
time
in fact in Dynex PTX in the 90s we had
this but we didn't have the synchronous
wait okay the problem is what you'd like
and it happens a fair amount of the time
but not always
what you'd like as you like the
shape of that function that thing to be
known at compile time and you'd like it
at least be known a construction time
all right but know a fair amount of time
is not known until you actually do the
call RC you because what you want to do
may depend on the state of the system
for example you might have in the lane
screw you might have a file it might
start off not having attributes and
you're using and using it and somebody
slams attributes onto it and then when
you want to get rid of it you may have
to do something different as a result
and you certainly didn't know that at
construction time let alone compile time
what it was going to look like because
at that point it was out on a distant
where a might not even existed yet okay
so sometimes we can do it you know have
just have any arbitrary thing lambda
capture and all that stuff potentially
but there are cases where that and I
found it with lots of error message
where it does not work okay when F is
not knowing it is only knowing it call
RC you time you have to have a fixed
chunk of storage to put that function
pointer in there okay well C++ does a
lot of other things you could use but
there's another complication and this is
the problem with this stuff is it's I've
been doing this for 20 years so it's not
here it's kind of down here somewhere
and so we had a Korean barbecue that
helped bring it up to my brain instead
of my fingers a couple nights ago and as
the title says any type any time any
translation unit anywhere I mean are she
just gets this stuff dumped on it and it
has to deal with it and so we've got the
six boxes they're representing six
different translation units each
translation has four instances and the
little a1 b1 so on those are the types
so for example translation unit C in the
middle on the top there has three
instances of C one type and one instance
of C to type all right D in the middle
of the bottom just has forces of the
same type and these have been handed to
RC you to clean up in some order and the
gray lines show the order it has them on
this list that's keeping track but you
can see this list just was wandering
through the translation you just kind of
randomly and it's got all different
sorts of types and if it's off somewhere
it does have no idea what these types
are the only way it can
interpret the type so there's two ways
one is you get inheritance from an
abstract type and have a virtual
function pointer which may not be what
you want if you have
performance-critical code the other ways
you have to get the execution into the
translation unit somehow okay so one way
or another you have to get some kind of
hook to get something back into the
translation in the nose what the heck
this stupid thing is and right now we're
kind of at a point where we have to have
some kind of a function pointer to do
that there may be something come of a
better way but that's where we're at
right now okay so this is the C language
API for RCU I'm only going to worry
about the top four are sure we'd lock
our series and locks and kind of our
sivan call our c we've gone through
those there that's just their templates
or their prototypes I should say and you
can define an RC domain with the current
proposal and they're kind of down in the
middle I apologize for that being see
that there's a read lock read unlock
synchronize and call and those are just
member functions which allows you to
have a domain kind of like kind of like
the hazard pointer domains but usually
used for different purpose that allows
you to have a single translation you
deal with different types of RCU or
different domains of RC who wants to we
talked about wanting scoped readers this
is a API for doing that so that you can
say this block is reading and then is
how weary exited it'll stop reading when
you leave I did something for call RC
you and I presented on Wednesday and the
less said about that the better
isabella provided a version this one
requires you to know the shape of the
function kind of at that construction
time and it seems to work she gave me
some code I've integrated in at passes
test and that's good and our third wire
dude another another one that allows you
to pass a thing in at call time so you
pass the destructor in subject he says
the call time and it maybe we want both
of these because you know if you can do
it at compile time you can use valley
capture lambdas for example which you
certainly can't unless you want to
allocate on the free path which I
definitely do not that's a good way to
deadlock your system because it needs to
allocate memory in order
it it's not got any memory that's where
you trying to feed memory the first
place and I don't like that okay and if
you like it you can derive your own sub
class and do it yourself
alright this is just performance
comparison unfortunately I don't have
material install so I wasn't able to
grab Anthony's thing to put it in there
but we'll get there okay I was trying
really hard to make it pull a mercurial
archive and wasn't sure why it was the
yelling at me and I should send me email
Isis look you need use material okay
fine anyway this is just this is a
specialized technique you use it where
it makes sense the Linux kernel
community tends to be a little bit too
contrary so I told them don't use it
down there and they silk are so far
they've come up with two cases where you
actually should use it down there maybe
they'll come up with more but for the
most part you want it to be in the case
where the veterinarian's might disagree
and that's okay all right and if some
agreement is needed you can impose it in
various ways okay anyway this is kind of
our future things I'd like to invite
maggot and Michael Wong come up here and
we got a coldness for questions as
Michael said we want to get these things
into the concurrency TS and thence
perhaps in the C++ standard maybe even
in C++ 20 we have some working drafts
hazard pointers you can see there we've
got the end old version of memory
consumed and I've got two papers which I
have written up with things that I need
to send to John Spicer
thank you John Spicer to get numbers for
and those should be available for a
perusal a little bit questions comments
thoughts tomatoes Wow okay I think I saw
you first then you then you so please
come up okay we have implementations in
userspace that work in all sorts of
environments go ahead
right they do for most of them they do
for all of them actually so what happens
for the there's some user space RC use
that require that require heavier oh I'm
sorry the question was hey we've got a
we've got a bunch of different user
space RCU things they have different
properties do they all really fit that
table that Michael long presented at the
end of his presentation and the answer
is yes there are some slight variations
for exam when you traverse to a new
object it's the same you just use the
normal load instruction on the machine
there's no memory birds no nothing
however to your point our she read lock
might have some overhead for example if
you use the QSB are the questions see
you can bend and blame the us need of
toronto guys for that one they named it
quiescent state base reclamation there
we are q SVR if you use that are she
read lock and archery lock are just free
just as they are a non pre-emptive Linux
kernel there's another one that has
explicit memory barriers or stole or
load memory barriers but rather than
being on each traversal as you'd have
with hazard pointers they're only on the
arcs you read lock in the area unlock of
course if you only if you have less than
if you have one traversal you're better
off with hazard players anyway I'm sorry
about the rest of the questions I just
got the session is over sign but will be
around for at least a little bit longer
and there's a break coming up at some
point and we'll go from there
thank you all very much for your time
and attention and have a great rest of
the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>