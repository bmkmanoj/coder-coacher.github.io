<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Phil Nash “The Holy Grail! A Hash Array Mapped Trie for C++” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Phil Nash “The Holy Grail! A Hash Array Mapped Trie for C++” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Phil Nash “The Holy Grail! A Hash Array Mapped Trie for C++”</b></h2><h5 class="post__date">2017-10-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/imrSQ82dYns" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I the Holy Grail a persistent hash array
map tree for C++ and in the course of
this talk I would explain what each of
those words mean so if you're not quite
sure yet hopefully we'll be by at least
the middle of this talk actually
starting with the first part the Holy
Grail why am i using that term better I
said the Holy Grail exclamation mark
maybe that should be the Holy Grail
question mark because what I want to
examine is whether we can actually
achieve a container that has properties
that have the minimum number of
trade-offs given what we actually want
to get out of it so we'll have a look at
what those properties might be as we go
through and we'll see whether we
actually achieve it so we will discover
this together and I want to try want to
start with that last word there try
because I've been pronouncing it tree
and you can see some trees and
background to try and emphasize that
it's also a good excuse for me to get
one of my holiday pictures in comes from
the word retrieve and that's why it's
pronounced a tree but you know some
people may pronounce it try it that's
fine as well
in fact when we look at the Wikipedia
article on on trees it says they were
first described in 1959 so it's not a
new idea many of you probably already
familiar with these sorts of trees
usually some sort of string dictionary
we're going to use them slightly
differently today says the term was
coined two years later by edward fredkin
who pronounced it as treats we said
because the middle syllable of retrieval
however other authors pronounce it as
try as we said in an attempt to
distinguish it verbally from tree I'm
going to stick with further the tree
pronunciation partly because I just
prefer historical accuracy and partly
because I'm already quite well known for
catch I don't have been known for try as
well
so with that what we'll come back to how
this fits in in a moment but for a bit
of background we talked about set and
actually more generally we want to look
at the all the associative containers
currently under standard you'll see why
shortly hopefully there's eight of them
currently if I counted right and you can
sort of group them quite nicely like
this and you can see that there's some
quite obvious pairing is there they'll
divide into the single and multi
versions sure you know what the
differences are about basically if
you've got multiple identities or the
same identity for different values only
the first one will be entered into a a
single map or set multi maps or sets can
contain multiple values of the same
identity but the underlying data
structure it's going to be usually
identical in both cases so for our
purposes we don't need to consider both
pairs so we're going to drop the multi
versions for now it's cutely simple so
now we can see the top and bottom my
seat divides between maps and sets got
set some maps and again the underlying
data structures are usually the same so
you could implement a map in terms of a
set just by having a set of key value
pairs where the the comparison is on the
key so again pretty straightforward
stuff so for our purposes we don't
really need to consider both pairs so
let's drop the maps as well and that's
how we ended up just talking about sets
so now we will come to this indivisible
set of properties between standard set
and standard an ordered set so before we
look at what those properties are just
an honorary mention to one other
associate to container be the sorted
vector so you load up a vector and you
sort it according to your identity and
then you could just do a binary search
to look up that's still a great choice
for for many applications so what I'm
mentioning here but it's not strictly
speaking a an associative container we
may still come
a little bit later but between sets an
unordered set we first of all observed
that the underlined data structure well
for set is usually it's not required to
be usually a red black tree probably
some sort of balanced binary tree and
Sarai's I know all of the standard
library implementations user a red black
tree we will have a look at little bit
more red back trees later so I won't
talk about them more now but just to
contrast that to an ordered set which is
based on a hash table then we have
requirements on the types and this may
dictate what you can actually put in
these things so send a set we had the
strict week ordering which implies that
we either have a less m comparison
operator or you can provide your own
predicate for that but we must have this
ordering for it to work and an ordered
set well notably doesn't have that
requirement which as we'll see later
does become relevant but it does require
the type to be austere hashable either
with specialization of standard hash or
your own predicate and equatable so
comparing equals again provide your own
predicate so actually quite different
requirements on the type most types may
supply both them then the bit we're
normally interested in the complexity
guarantees red-black trees or the
equivalent have log n complexity this is
on lookup primarily talking about look
up here whereas unordered set is
constant so on average we usually forget
that bit actually it can tend towards
linear in the worst case we don't often
get to that case but that can become
relevant we will see an example of that
later so it's not perfect but in general
this does translate to an order set
being generally significantly better
performing than a red-black tree or a
standard set so where that matters will
usually reach for something like
standard set the standard sorry standard
on all the set or unordered map for our
associative container needs so
we have these available to us but
there's one property that I want to talk
about that these don't have and that's
what's going to lead us on to the main
topic of our discussion so look at that
next so this word is persistence and I
put this picture up here this is me at
the end of the London Marathon last year
as an example of what we're not going to
be talking about not this type of
persistence but also it's not the type
of persistence that involves file
systems databases any sort of
sterilization we're not talking about
that either that's often completed here
what we're actually talking about is
probably best explained with an example
so just for the moment we're going to
talk about lists so we have standard
list but lists in general particularly a
functional programming languages have an
interesting property we'll just briefly
look at so again I'm sure you're all
familiar with how a singly linked list
works but we're just going to run for it
very quickly for for background purposes
so we'll have a number of notes each
with a relevant value and a pointer to
the next node or previous depending on
how you look at it and then the first
one we refer to is the head and then the
rest or the first node in the rest the
tale straightforward enough there's the
pointers they'll be less interesting is
when you want to introduce a new node at
the head so bring this new one in here
all we need to do is write the pointer
to the previous head into the new node
so the original list doesn't need to
know anything about this new node so if
we assume that we're going to be
wrapping this up in some other type so
let's introduce those in here we can now
add two lists in memory at the same time
sharing all of their common States so
this is what we mean by persistence the
old value persists the old container
it's still persisting while we use in
the new one so we could do this the
other way as well we've remove items we
just point to an earlier head again the
previous instances don't need to know
anything about the new one now this is
not the way standard list works but it
is the way lists tend to work in
functional programming languages which
is how you can treat them YouTube
and yet still produced new values
obviously you can stack up all sorts of
operations that way and when you are
finished if you only what the the latest
one you can just clap that down as if
you'd written it that way in the first
place now obviously in C++ we don't tend
to use linked lists very often they're
not particularly efficient usually
something like standard Becht is gonna
service better but if we want this
property of persistence that could be a
really useful data structure but this
was just to introduce you gently to to
the idea obviously that the problem here
is we've only got one pointer so if you
do want to do any sort of modifications
further down the list then you actually
start to get into some expensive copying
but if we we put two pointers in there
then we could have a tree structure we
just get a simple binary tree which has
some other problems we'll talk about in
a moment but it is particularly good for
persistence so if we introduced a new
value now this for here I think it was
right we can we can put in a couple of
places we can hang it off the the free
node or the five node about the five
node because it's a leaf node already
that's much easier we don't need to
change anything to put it down there so
we could add that in as a new leaf node
there and if we're just using a mutable
container then we'd be done here nice
and easy if we want to make this
persistent though then also we can't
write a new pointer into the to the
previous five node so we're effectively
invalidating that so what we do is we
take a copy of that write on your
pointer into the copy but now we're
going to do five node obviously we need
to write into its parents node and so on
all the way up to the root so what we
tend to to get is we get rewrite that
whole path up to the root every time we
we're making mutations so there is some
copying but particularly with it with a
large tree it's actually quite a small
amount of the total number of nodes that
we need to copy and we can then share
significant amount of the common state
so that's quite a nice property now
said there's problems with simple binary
trees in that if you load them up this
way they will tend to get very
unbalanced with very long branches on
one side and not so much in the other
side competely breaks that log n
complexity guarantee so we have
strategies for balancing them so we
mentioned red black trees earlier that's
one such strategy I'm not going to look
in detail the red black trees because
that's not the topic of this talk about
these are the rules if you're interested
you can very easily create your own so
just wanna apply those rules to to this
tree just for the sake of this example
because there's another side effect of
this that we want to examine so same
example but now each node is colored red
or black black can usually be done with
no overhead it's just a convenient thing
we introduce a new node at the bottom
that breaks one of those rules we just
talked about the the red node in variant
red nodes cannot have red children so we
need to start applying these rebalancing
rules which usually apply to the parent
grandparent and uncle nodes see who did
it fairly locally in this case it just
means recoloring those immediately those
nodes in the immediate vicinity and
having done that we've restored our
invariant but now we've invalidating the
next level up with no another violation
of the red node invariants so we have to
keep applying that again all the way up
to root will they get to root the root
node must be black so we just recolor
that as well so fairly straightforward
that gives us his nice property of being
one average fairly balanced that's what
we wanted that's how a red black trees
work but the reason I've gone through
all of this is because on the way up if
we're making this persistent then we
would have touched these additional
nodes as well so as well as all the
nodes up to root it's a little bit of
collateral damage
so we're making these data structures
persistent then there's a little bit
more overhead to the copying as well as
the nodes up to route but it's so useful
in fact again if you go to Wikipedia the
source of all knowledge
look at the page where black tree says
this pear
at the top says red black trees are also
particularly valuable in functional
programming whether one of the most
common persistent data structures used
to construct associative arrays and sets
that can retain previous versions after
mutations that's what we've been talking
about so they are very useful and I've
written a couple of these in the past
their work pretty well but rewinding a
bit earlier we were talking about
standard set compared to unordered set
and the performance scan complexity
guarantees generally we're going to
reach for the at the unordered versions
of hash tables for performance purposes
so here what needs to be performing then
we're actually adding overhead to what
we already had but it's very difficult
it's not impossible to make a hash table
persistent you end up either having to
copy the whole thing or large parts of
it and you can make some changes to to
make it more suitable for the
persistence we're not going to go there
but you tend to end up where we're going
to be going anyway with something more
like the the hash tree we're going to
present but I'm going to talk about it
from the perspective of red-black trees
and work our way from there so what was
the problem with red-black trees the
problem all comes from the fact that we
only have two pointers in each node
because of that then as our overall tree
grows and the depth of the tree is going
to get bigger quite a quite a fast pace
so for example they a tree of 15 million
values will have a tree depth of about
24 nodes so for any given lookup this a
lot of pointer hops
you've got to go through to get down to
the value that you want and that's where
the the log n complexity will will get
you and all comes from the fact that
we've got only took up to two pointers
in each node so that's the problem
solution must be add more pointers and
it is in fact you know we could stop
there but that raises a couple more
questions
and in answering those questions that's
going to lead us to our our hash tree
and that's gonna actually have a you
know if your interesting side effects as
well so the first question is we've got
all these pointers but how do we know
which one to follow as we're traversing
down the tree with the the binary tree
simple because you just do the lesson
caparison takes you left or right but
what do we do here so the answer is well
let's imagine we have a node with an
array of pointers so we're going to
we're gonna use 32 64 is another common
number but it's usually one of those two
in fact so like you've got an array of
pointers and some of them may contain
pointers to other branch nodes some of
them to to leaf nodes how do we know
which one to follow what we do we take
the hash or the value although they're
the key rather and then we peel off the
first in this case five bits from the
hash gives us a value 27 in this example
that's our index into the array so we'll
take out that that pointer follow it
down we get a branch node in this case
there we take the next five bits from
the hash killed off gives us another
value sixth in this case that's our next
index into the next array and that makes
taker still have a branch code in this
case we're at a leaf so we're we're done
but now because we're using hashes we'll
also you have hash collisions so those
leaf nodes will be containers of values
much like with the buckets in in hash
tables we may need to do some linear
search or some other type of search at
the end to get our value but usually
once we got to the leaf what we're
almost done so that's sort of answers
the first question and that's where the
hash part of our title comes in the
other question is well how do we make
this memory efficient we've got 32 or
even 64 pointers in each node
and that's going to get you know very
big very quickly so what we note here is
that most of those those slots are empty
most of the time we've got a few
pointers actually in there
not using them all so using some sort of
sparse array is the answer so what we're
going to do is just have a physical
array of only the number of pointers
that we need free in this case and then
an additional integer that we're gonna
use is a bitmap to tell us what that
actually maps to in our virtual array so
all of the set bit in our bitmap
correspond to the actual positions in
the array and the unset bits to 20
pointers then we just need a way of
mapping those and all we need to do is
trying to give it index will go to that
bit and then we'll count the number of
bits to the to the right they'll give us
our index so but look about a bit right
on the the right left-hand side there's
two bits to the right so index of 2 into
our sparse array gives us the result we
wanted it's simple but then how do we
count the bits we want to do that
efficiently as well that's whatever
interesting question we'll come back to
that a little while as well but where
we've ended up now is something like
this so it's a tree structure in a sense
of TR EE but it's also a tree TR ie
much like you may have seen with a
string 3 string dictionary tree where
you peel off each character of a string
to give you the index to the next level
in this case we're using a hash as we
just talked about and that will give us
our path down through the tree to to the
leaf node and then hopefully a value so
what are the properties of this data
structure
so we looked at the example in the red
black tree a moment ago with 15 million
nodes had a tree depth of 24 well the
maximum depth of one of these trees it's
a 32-bit hash is only going to be six or
seven deep maximum I'll say six or seven
because you've got those last two bits
at the end you may not want to actually
follow that so at c6 and the average
depth for those 15 million values in the
example is only 5 a lot better than 24
we said the leaves hold a raise of
values we'll look at that a little bit
more in a moment they're actually more
space efficient than hash tables because
of this compacting that we have to do as
part of the the storage the complexity
is technically blog 32 n which sounds a
bit awkward harder to reason about in
practice obviously there is the
logarithmic aspect to it but will tend
to be much closer to that sort of
constant time every shook up of a hash
table than the the log in of red black
tree but we will look at the practical
aspects in a moment and this additional
thing no need to rebalance there's a
caveat we do need to have a good hash
distribution because the distribution of
the hash will actually impact the
distribution of the tree itself but
given that the tree should be fairly
well balanced to start with so consider
what that means terms of persistence
still got a tree structure so still have
to do a tree writing to route but now
we've got much less nodes to rewrite and
we don't have any collateral Dallin and
collateral damage due to the rebalancing
so this is much better than where we
were with the red black tree already ok
can you read the the abstract on shade
comm know that I put up well by the way
probably get to that the photo in the
background there anyone recognize that
sir yep see the waterfall from Twin
Peaks turns out to be about half an hour
down the road from here
so every one of my my photos now in the
in the abstract I made this claim I said
that talking of hash trees they're close
but not quite as fast as pure hash
tables that's the claim we're sort of
looked at the background of why that
might be the case but how does that
actually weigh up in practice let's look
at some figures so some performance
metrics I did some benchmarks my
methodology I used a micro benchmarking
framework called non yes the IO and the
way that works is you'll first sort of
run a few tests against the system clock
to gauge the precision so knows how many
iterations to run of your your benchmark
and it will then run that enough times
for X number of samples before it's 200
I've done most of these at a thousand to
give us plenty of figures and they'll
then do statistical analysis on those
samples to look at the B average and the
variance exclude any outliers so it's
doing a lot of the work for us so that's
pretty good
using lip C++ we've clank for this's the
only ones I've mentioned so far that is
relevant particularly because we're
going to be comparing they're going
sound a set standard and all the sets so
it's that particular implementation
other implementations my variables and
at least for our first test what I've
done is I've loaded up each of those
containers with a hundred thousand
integers just monotonically increasing
and then we will compare them so that's
our hash tree or hampt I've abbreviated
it here standard set the standard on all
good set let's look at some figures so
here's one of the graphs so this is the
inserting hundred thousand integers on
the right hand side in green we got bear
hash tree left hand side is sand asset
and in the middle that sort of orange
bar at the bottom is unordered set that
the y axis is time so that means that
the the bigger the bar the worse we're
performing so we're not doing
particularly well on this one
in fact we're about four times slower
than an ordered set twenty times
sorry four times slower than standard
set twenty times slower than an ordered
set nowhere near my claim so what's
going on here well this is insertion my
claim was primarily about lookup but it
certian is important when I read these
metrics there's basically no
optimization on the beam set we're
paying the full cost of resistance
copying every single time we we had
something and there's a copy-on-write
optimization that we can apply because
we didn't invoke need to reference count
the notes and i've done this with red
black trees in the past it's very
effective if you're loading up at an
initial tree the you're pretty much aliy
the whole thing mutiply because we
haven't actually started using it yet I
haven't done that here yet so we are
seeing the full cost of persistence so
yeah it can be expensive so let's set
that aside for the moment just work in
progress and have a look at or before we
get to to lookup plan should also point
it out but we look at the the underlying
data that Nani has given us you can see
like in there there we go that's the
hash tree at the top on the the right
hand side there's a lot of extra
variants over there something is
obviously going on on my machine at that
point it kicked in so no Nia says it's
go to those outliers but there's this
whole second half is it's really gone
out so to be fair we should probably
just consider the first half of that so
what I've done for the next charts is to
just take that data directly and produce
my own graphs
so sober look at look up then so the
same hundred thousand integers
this time we refined now we've got right
the top the orange is standard set the
green below that let's just confusing
the green just below that is ashtray and
at the bottom is an ordered set so we're
beating standard set now this is this is
much better
we're starting to get there but we are
still closer to standard set than
unaltered set so we're not meeting my
claim yet what we get in there this is
promising so what's going on here
remember st of what we're doing is we're
just loading up with a hundred thousand
integers monotonically increasing and
we're using lip C++ integers in Lib C++
these standard hash specialization is
just the identity function so our hash
is also a monotonically increasing
series of integers remember I said that
if the the hash distribution is is not
very good then we're going to be very
unbalanced and that's what we're playing
the cost of here there are ways to
address that but they haven't been
applied yet so we are susceptible to
that so let's move on and have a look
before we do that that was 100,000 I'd
actually did all of the orders of
magnitude between a hundred and a
million just to see where that graph
look like so again the orders are the
same here but now we can see like that
the profile of the different data
structures so the orange line for
standard set is a slightly diverging
from our hash tree and the first
fielder's of magnitude and then sort of
free converges towards the top but it's
it's actually tracking hatcheries
tracking an auto set fairly closely
they've got a very similar scalability
profile that's interesting to observe
so what did it next was I loaded up all
the same orders of magnitude but this
time using strings so now we're hashing
strings but getting a much better hash
distribution so we're getting closer to
to what we all want to end up with now
let's have a look at what we've got so
the orange one at the top is standard
set again the blue one unordered set and
at the bottom is hash tree in this one
we're actually beating unordered set so
we're definitely in the right ballpark
now and said this is definitely a
quality of implementation issue another
implementation I would expect to be hash
tree that's fine but I think this
demonstrates that we're in the ballpark
of my claim with some caveats hopefully
they could be addressed the assertion
time hash distribution but this is what
we want to aim for I'll still a bit
uncomfortable about the integers so I
did one more test for this
I took the hashes from these strings and
then I use those as a integer in another
set of tests so this is another set of
integers using those string hashes but
what's interesting is everything is
actually much closer together in this
one I should point out by the way that
the y-axis in in these examples is
exponential so where things are actually
tracking each other they're actually
diverging in practice but we start off
with hash tree winning again but then
they sort of converge in the middle and
towards the top actually this one goes
to ten mil 100 million or ten million
sorry but towards the top from around
the million mark hash tree starts to
become the worst performing and standard
set is actually winning actually beating
on alder set what's going on there
well wherever we said earlier that
because of hash collisions we would
beget to a leaf node and in a hash table
and the same in the the hannspree we're
gonna have to do
perhaps a linear search so at this point
we're starting to get significant enough
hash collisions that that final linear
search is starting to not dominate that
become a significant factor and what's
interesting is that's because of the
requirement on the types we don't have
the strict weak ordering requirement on
unordered set so when we get to that
last lidia search it has to be linear
really that's the only way you can do it
we're not constrained by that
requirement then potentially we could do
a sorted array at the end in our hash
tree and then do a binary search instead
if we do that I would expect our hash
tree implementation to be winning at the
end as well again it's another
opportunity for optimization that
haven't explored yet but we're looking
pretty good
so let's say we're meeting the claim in
some areas maybe not in others hope to
explore those opportunities chopped for
optimization but it shows the potential
so I gave this talk in in Aspen at C++
now earlier this year the longer talk
and went into a lot more depth on the
the actual implementation so I've still
got these slides here because they
didn't know exactly how long it's gonna
have left so we're not gonna go through
the whole thing I wanted to pull out a
little bit that was interesting so first
of all we remember this one how we map
our sparse array with a bitmap and I
said that it was an interesting problem
how we count the set bit to solve that
here's one possible implementation this
has taken from but there's a link
there's a stack overflow answer it takes
you to a page online somewhere that has
you know dozens of possible ways that
you can do bit shifting to to solve this
problem
I don't follow this but sir I know that
it works
there's no branching in here at all it's
just pure bit twiddling its pretty
performant but we can do better as the
comment in the middle serves actually
most modern architectures will have a
single instruction that can do this
you should call pop camp so if you're
using GCC or clang there's a built-in
built-in pop camp will do this
you do need to compile with a certain
flag to actually get that to map on to
the instruction but when you do that it
performs much better so all the examples
I gave all with this if you use the
previous example that the bit shifting
then doesn't perform as well everything
sort of shifts up slightly but the basic
ordering of remains the same so it's not
significantly different than in use what
I'm doing is using the count set bits to
create or to map the bitmap to a compact
index so because I've all these indices
flowing around compact index sparse
index and so on
I'm using a strong type def internally
to keep track of that and there's some
convenience functions on there we just
need to create a mask of the bits to the
right of the one that we're interested
in so that's what I'm doing at the top
there I don't that will give us this
this method to compact which is just a
method on our sparse index strong type
def so I'm not going to go into all of
this in depth but again visually in
practice here's how we we use it with a
bitmap correct sparse index and then we
can get the compact index from that I
like that turned out to be really useful
because then through overloading if we
already have a compact index we can go
straight there if we have a sparse index
that gets mapped along the way what up
there pretty nicely
then we have the thing that pill is off
the the bits from the from the hash say
five at a time in this case we'll
collect too much detail and load
structure going to going to skip most of
that we get the entity to the really
nasty low-level stuff there's actually
an issue with with alignment here that I
need to fix but it is nasty because
first of all we need to create a note
big enough to contain all of the
pointers or values as well we're at an
additional allocation and then the
replacement new that into into our
allocation but do that where if this is
going to be a copy of an existing node
we'll need to copy those in as well so
so we're not going to go into too much
detail there but you get an idea of what
that codes like branch nodes similar
okay and then again actually if you go
to the C++ now videos you can watch that
talk there if you're interested in the
detail walk through the code so I just
want to get to okay right here so the
example we looked at earlier this is our
tree structure so these are nodes with a
root node and then in addition to the
root node itself we'll just need a site
so we can do in constant time lookup the
the number of elements that really is
the entirety of our hash tree all of the
hard works going on in the notes so
because we just have those two values we
can just click those together in its own
data structure that we use in our hash
tree but we can use it elsewhere as well
we can this is actually small enough to
to treat atomically on most
architectures there's a
implication to that so the hash tree
itself will we'll have a hash free data
top there and then just sort of
basically provides an interface onto
that again we won't go into the details
there right here we go and then we can
have another type I had a really
discussed this bite yet so that's why we
wanted to get into this like it's just
that the test case that we'll look at
the code in a second but this is shared
hash tree and the whole purpose of
shared hash tree is just to hold
atomically that's hash free data and
because we can treat that atomically
then if we want to read or mutate the
undying data what we'll do is take a
copy of it into a hash tree but still
atomic copy or an atomic exchange and
then when we're working with our hash
tree we're completely shielded from any
other mutations going on to the hash
tree so it's just a shared location that
we're going to get the hash tree from
that we can then work with and if we
mutate it we can then write it back
again single atomic exchange and we get
very effective concurrency with no
locking on the condition that we are
mostly sort of bound on reads rather
than writes we're doing lots of reads
then we'll both keep looping around and
redoing our work so it's only
appropriate for some conditions but
they're very common conditions so in
this test I'm not really obviously using
the concurrency I'm just demonstrating
how you can use this transaction to do
that in practice there's a helper update
with that will do all the you know
atomically fetching the hash tree passed
passing that to you you can work on it
and then when it goes back it will
automatically commit it and do this in a
loop so if anyone else is mutated at the
meantime you'll redo the work
so here's the shared hash tree itself
we're just asserting that hatchery data
is trivially copyable it's a requirement
for it to be atomic and then we've got
these atomic operations don't pay
attention to the memory order stuff that
needs to be fixed it's more of a
placeholder and there we go that memory
would have stuff again it's all wrong
right yeah so this is the loop that it's
doing for the update with it so it's
pretty simple stuff you do have to get
it right this is not necessarily right
but we've got it all in one place
as far as atomic data structures go it's
one of the simplest out there and that's
reassuring I think so I'm going to stop
with that
now just to sum up and then we'll go to
questions the claim was that we kind of
approach the performance of hashmaps
without too many trade-offs I think I
demonstrated that we can achieve that
but there's some work to be done to sort
of come some summer Ridge phases they're
more if it's pace efficient the hash
tables so that's good copies are cheap
and memory efficient and we didn't
really go into this too much touched on
the concurrency implications but think
about having a large container
associative container but you can copy
with just the cost of two integers and
share large amounts of common State you
can hold him memory at once
when I was first working on this it was
in the context of financial data and
what you could do is load up one of
these things with you know an object
graph of market data and then you could
make small changes and have entire
scenarios sort of matrices of what these
object graphs in memory at once very
very efficiently and very very
performant as well
so there's many implications like that
and as we mentioned can be made to work
very nicely with concurrency so that's
the summary there's a number of
references including some papers that
this is all derived from a put all the
references up on my website level of
indirection comm slash storage slash
hampt Refs
HTML if you can't remember level of
indirection com
I also have extra level of indirection
calm that redirect say or you can reach
me on Twitter bill Nash
now you can find me later during the day
so thank you very much for listening and
so the question was because we are very
sensitive to the hash distribution have
I considered running it through very
sort of rehashing or very mixing
algorithms and yes that's almost
certainly what would need to do to solve
the problem and I did start looking
about and it was actually making things
worse rather than better so it to
explore that a bit better but though
they're definitely approaches it can be
taken there thank you
so to summarize the question I got it
right you just wanted to clarify how
we're doing the mapping with this
pasture right I go back to the too much
code here right this this one so yeah so
the top is is an integer so that's all
the bits of the integer and each set bit
in the integer represents the position
in your virtual array where you have a
set pointer value and then all the unset
bits are where you would represent a
null pointer in the actual array at the
bottom we're only storing the pointers
themselves so the number of set bits we
have in the top is the number of actual
items in the bottom so the top array is
actually just an integer so it's a
bitmap yeah yeah so you can do this with
32 pointers or 64 pointers if you have a
64-bit integer one of the references
that I showed at the end was a games
developer who implemented one of these
and he experimented with both 32-bit and
64-bit arrays and in his case he found
that the first two bit arrays perform
better he wasn't 100% sure why you can
speculate he did suggest that maybe it
was a quality of implementation issue
and you can get a better trade-off with
64 bits but there's a lot in it so
that's why I've stuck with 32 bits were
you clear on how that map's now or yeah
thank you so yes in the middle
okay so question was how do you yeah how
do you handle knows that have been
deleted going out to scope so in this
implementation I'm using ref counting
it's actually intrusive ref counting so
I'm not using shared pointer if you look
at other languages that implement this
particularly Scala and closure they have
garbage collection and that's usually
considered the way you have to do this
but I've got it working with ref
counting performance enough I think and
in fact the reference counting also is
what allows us to do the copy-on-write
we should say I'm not doing this
implementation but do you see it very
effectively before because I think go
down if you traverse down the tree upon
your insertion point you you checked the
shared count at each level soon as it
got a share cannot more than one then
the rest of that branch is shared if you
get to the leaf and your unshared you
can do all the mutations in place and I
can save a lot on copying so we get the
benefit from from shared counting as
well of being able to that copy and
write should not sure be so easy with
garbage collection but that's how we're
doing just this reference counting yeah
so the question if I got that right is
we have a job what term D D is again I
hash array Maps multi trillion right
multiple values for the same key so the
the difference between the multi and the
single version is really just to
constraint on the on the insertion
possibly that could have implications to
how you storing it as well I haven't
really considered that yeah that would
only really affect the search at the end
though I I think I just would have hash
collisions either way so you still want
to deal with that so I'm not sure that
that would give any any benefit but
maybe I'm misunderstanding your quest
he constructed merge operation right not
still not quite Shawn fully following
him it maybe we should talk about it
afterwards but yeah thank you yes sir it
was back so I struggling to you let me
come a bit closer
our delete operations for leaves
supported with sequestering the keys
delete operations are working so hooking
up with a key and then deleting in this
in the my reference implementation I
haven't done delete yet my experience
from doing this for red black trees is
that delete is by far the hardest part
I'm expecting it to that to be the case
here as well I hope you do not quite as
much because everything's weirder to
worry about the rebalancing so I think
it's I think it just boils down to the
case of where you just create the
complication so but certainly yeah right
so to summarize how to handle memory
allocation in a world where we don't
have a compacting GC so their summary
language is like scholar and closure
when you're allocating everything is
generally much closer to government
memory so you may have to worry too much
about it
so far all I've done is just
straightforward new and delete but
there's definitely one of the
opportunities for optimisation
alternative allocators and being able to
reuse blocks of memory because as you're
building these things up and you're
creating and discarding copies so you're
leaving slots open that you could you
could reuse and keep things much more
closely located which kind of
implications to cache witness haven't
done that yet and that's definitely
something that could be we don't sounds
like you have a follow-up
that's a fair point so because I'm using
malloc under the hood
technically the implementation is not
lock free because there's a lock in
malloc but that's at the point of
insertion the replacing a new instance
of the hash tree where we distinct of
the atomic swap on the root node and the
size that operation is lock free it's
that exchange between threads that are
communicating on it on the single
incidence but there will be locks
involved during the insertion at the
moment that could be mitigated by using
an alternative allocator
does that answer your question thank you
yeah so the question is the elements are
unordered as a statement to verify yes
they are they're effectively ordered by
by the hash but the same as same
principle is done all to set unordered
map yeah okay well if you have any more
questions especially ensel
cache me afterwards thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>