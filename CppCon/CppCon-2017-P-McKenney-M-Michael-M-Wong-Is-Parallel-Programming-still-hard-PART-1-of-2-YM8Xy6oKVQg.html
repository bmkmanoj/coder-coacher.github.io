<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: P. McKenney, M. Michael &amp; M. Wong “Is Parallel Programming still hard? PART 1 of 2” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: P. McKenney, M. Michael &amp; M. Wong “Is Parallel Programming still hard? PART 1 of 2” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: P. McKenney, M. Michael &amp; M. Wong “Is Parallel Programming still hard? PART 1 of 2”</b></h2><h5 class="post__date">2017-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YM8Xy6oKVQg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Hi everybody.
Thank you very much for
coming back after lunch.
My name is Michael Wong.
I'm with Codeplay.
We're presenting this talk in concursion
with a number of people
who have been involved
in parallel programming
for a number of years
and we definitely have the gray hair
and the lack of hair to show for it.
My colleagues are Paul McKenney from IBM
and Maged Michael from Facebook
so you're definitely
getting a cross spectrum
of probably I would say
at least half a century
of experience I would expect.
The topic of discussion
is going to be about,
it's somewhat controversial.
You've always heard that
parallel programming is hard
so what should you do about it?
We are turning that a little bit
and we're saying is it still hard
because if it is, we have
to do something about it.
Again so as with any talks,
it's supported by a number
of people in the background,
various standard committees,
various discussions
that we've been having.
With anything I would
say that there's always
going to be some amount of errors
and anything that remains is
all mine, not anyone else's.
I would like to say it's all theirs,
I would like to think
that all of the good stuff
is all mine but that's unfortunately
been proven too often not to be the case.
Finally there's the
usual legal disclaimer,
I won't bore you with it,
it's a standard template and of course
the thing that my company
always makes me put on,
not a bad thing really.
It tells us that our company does a lot
with heterogenous computing and C++,
which a language called SYCL,
which we're trying to put
into the C++ standard.
So today what are you gonna hear about?
We actually have three speakers.
Hey Anthony.
Don't you hate it when you walk in
and people call your name.
(laughing)
It's okay, we're good friends.
The business is such that you
kinda know everybody by now.
You know all the parallel programmers
who is a practitioner who's
in it around all these years
so our topic is to separate
out into three groups.
I'm gonna bring in the
preamble about asking you
what is parallel programming
fundamentally still hard
and then I'm gonna show you what is,
because we've been involved in designing
parallel programming
models for almost 20 years,
we've had some experience and background
in understanding where the fuzzy edges are
so what does it mean to be designing
a parallel programming model.
Paul McKenney is gonna come in
and look at where the hardware side,
look at it from the hardware
side, can it actually help?
Can it make parallel programming easier?
And then Maged Michael is gonna come in
and give you a really detailed example
just so that you don't say well,
we didn't see any code.
Well there is code but it's coming okay,
and it's not gonna be just any code,
it's not just be any little blob
that you wouldn't fit into a single slide,
it's gonna be about a detailed breakdown
about single producer,
single consumer buffer
and it's gonna show you all the things
we're gonna talk about from
the previous two groups,
talking about false sharing, parallelism,
about locality, atomic operations
and cache line bouncing.
So we figured this talk is aiming
at a mid-level of parallel programming,
not necessarily a total beginner level
and not necessarily at the
more advanced, high level,
advanced level that we usually talk about
so as a result we're not gonna talk
about lock-free programming,
I usually come up and talk
about transactional memory,
you're not gonna hear any of that.
We're not gonna talk about
hazard pointers or RCU.
We're not even gonna talk
about computer architecture
although many of the things on this screen
are kind of required reading, you know
but this is one of those things
where the person on stage is asking you
just to go do your homework to some extent
because it would be boring for us
to actually go through all these things.
In fact, we actually have
slides for all these things.
We weren't quite exactly
sure where to draw the line
so we actually did all the
slides, over 200 of them
and then we threw 'em
all away this morning.
(chuckling)
Have you ever had that happen?
That morning, you're like ahhh,
we don't have for the,
no, no, this is all gone.
It's really good stuff
though I promise you.
You would have been really impressed
when you look at the stuff that we have,
talking about Flynn's classification,
talking about dependencies,
talking about parallel patterns.
These are all exciting topics
but they're not for this talk,
they're for other talks,
maybe other peoples' talks.
We have actually talked about
them in some of our talks
like transactional memory, RCU,
hazard pointers from
previous years before.
Alright.
So when you talk about
parallel programming,
a number of parallels come to mind.
People talk about them all the time
and we definitely have the gray hair
or the lack of hair to show for it
because we've been doing
it for over 20 years
in some form or another.
I, myself, started over about,
because of my involvement in
IBM and as an IBM scientist,
we were doing high performance computing.
I was the CEO of OpenMP so we work hard
to develop the OpenMP which
is a parallel compiler
on top of C, C++ and Fortran.
So we met all these things
at some point in time
whether they were data races
or mutual exclusions or locks,
what happens if you have contention
that would lead to poor scaling?
What happens with false sharing?
That was probably the first
thing you run into, okay.
And then there's also you know,
lack of locality references,
lack of load balancing issues.
They all lead to some
consequence or another
and certainly with communication overhead,
that just causes
everything else to go bad.
As with any talk I just wanna step back
very briefly to talk a little bit about,
not a whole lot of time because I think
many of you guys may or may know anyway
but certainly you can
see that by this chart,
the growth of transistors is pretty linear
and consistent over the
last, many, many years.
However the frequency
clock has leveled off
and the only way to keep that speed,
that speed promise that the vendors have
which is what keeps you hooked
to buying their computer systems
is to continually raise that core, okay.
The problem is that
even that has a problem
because it's very soon, you
run into this heat death
of your system and
you've seen this before,
this is essentially the free lunch
where the free lunch used to be,
you know you just keep adding,
you know you just keep buying new hardware
and you get the free improvement.
Now the reliance has to
move into your software
because the hardware has
stopped getting faster
so your software now has to get faster.
This leads to exactly the same graph
except you'll notice that I've added
the yellow line there on power.
What has essentially happened is that
a hardware problem is now
just became a software problem
like a lot of things in the world,
bottlenecks just move from
one place to another, okay
and this is what's driving
the parallel computing architecture today.
That's it.
Now.
I'm sure all you guys have a car
or know what a car does
but have you ever driven,
have you ever been in one of
the really old Model T Fords.
Okay, that's what it looks like inside.
It would not be impossible
for you to drive it,
you could probably figure it out
but the controls aren't
exactly what you're used to.
For instance the left side levers
sets the rear wheel parking brake
and then puts the
transmission into neutral.
The right lever is
actually the throttle, okay
and then the lever on the
left of the steering column
is for ignition timing
and then the left pedal
changes the two Ford gears
while the right pedal,
the center pedal controls your views
and then the right pedal is the brake.
Now at one time, the
ability to drive a car
was actually only for a very few people
but now it's fairly commonplace.
This drastic change come about
for two very basic reasons.
Okay, the cars become cheaper
and they are more readily available
and the cars simply became
a lot easier to operate
because they have a lot of
assist for you right now,
a lot of automatic transmissions,
automatic chokes, lot
of automatic starters
and they have greatly improved
reliability as a result.
And today, I've bought this
car about two years ago
and it's now basically
the classic bloatware,
I actually have stuff in there
that I still don't know what it does.
This is the manual that
tells me what it does.
I think it can steer
itself through the lanes
and all those things.
Never actually use it.
Half of this stuff is bloatware,
I can still drive the car fundamentally
without any of this stuff but basically
this is a little bit of what's going on
in terms of technological domain
and it happens to computers just the same.
It's no longer necessary for instance
to use the key punch.
I actually started by using a key punch
in order to program.
Spreadsheets now allow programmers,
non programmers to get
results from the computers
that would have required a team
of specialists from before.
Okay.
One of the most compelling example today
about this easy, ease
of use the computers,
of course web surfing
and content creation.
Believe it or not, content creation
would have required a team
of scientists years ago.
Today, we just hack together these slides,
Paul lives in Oregon actually.
I'm in Toronto but if
I'm actually in Toronto.
Maged is in New York okay,
and we've just been working at this
for the last couple of months
slowly putting it together.
It's amazing that the stuff
is actually coming together.
And it's fairly taken for granted now.
Years ago this would have
been a research project
so we're saying that the same thing
is happening with parallel programming.
If you've argued that parallel
programming is difficult,
it's mostly because it's currently
being perceived by many
as being difficult.
Let me explain what
that means in some ways
because one of the thing that you see,
computers have always
been pretty much parallel
but what's going on for
the last 20 or 25 years
are the languages that's been given to us,
we've been forced by those
languages to think serially.
When you create a loop for instance,
even though you know innately
what happens, it loops around.
There's actually a lot of force ordering
that is actually implied in there
beyond what it is actually doing.
A lot of those things are necessary.
In fact they're harmful
to parallel programming
but the compiler can not know those things
and this is indeed one of the reasons why
that in order to think parallel,
you do have to go back to
the reality of it, okay.
All of you guys can drive
and do a lot of things
in parallel, all of you
guys can work your car
while texting on your cellphone.
I know it's not legal but you all do it
and I do it myself, okay.
That's parallel processing right there.
Yet with computers, somehow we're forced
into this perception and
not your fault I would say
of years of this guidance
that we should try
to think things in terms of
steps of serial operations.
When it really is not necessary
and this is actually what's
causing the perception
that parallel programming is difficult.
It's not parallel
programming that's difficult,
programming itself is
difficult, period, okay.
Whether it's serial or parallel
but you've been all trained
to make serial programming
a lot easier, okay.
So if you look back at many of
the historical difficulties,
you'll see that historically,
that many of these difficulties,
these problems that are well on their way
to becoming overcome.
First over the past few decades,
the cost of parallel systems
have decreased tremendously
from many multiples out of a house
to a fraction of that of a bicycle.
In fact I dare you to find
something that is not used,
that does not have parallel
processors today, okay.
So the second thing is that the advent,
the advent of low cost
and readily available
multi-core systems means
that the once rare,
rarefied experience of
programming, parallel systems,
is pretty much available to everyone.
This is pretty much the same process
as an automobile in a way.
It's well on its way of being common
for everyone to be able to
do these kinds of things.
Third, in the 20th Century,
they launched systems
of highly parallel software
that was pretty much
closely guarded, proprietary things.
Today on any GitHub, you
can easily be participating
in a parallel programming project.
The fourth thing.
Even though the large scale
parallel programming projects
in the 80s and the 90s were
almost all proprietary projects.
These projects have essentially
seeded the community
with a massive group of developers
who understand the engineering discipline
that would be required
so that actually helps to
take us to the fifth part.
Unfortunately the fifth part is the one
that is still a problem.
The high cost of
communication relative to that
of processing is still largely in place
and this difficulty, it has
existentially been increasing,
every year actually.
So the onus I would claim
is that if you believe
parallel programming is difficult,
the onus is on you to come
up as to the reason why.
If you really believe that
it's exceedingly hard,
then you should have already
some kind of an answer
as to why is parallel programming hard.
And indeed you could list
any number of reasons
ranging from the things that
I've shown in previous screens
like dead-locks or race conditions.
The real answer, I've already mentioned
is that it's really
actually not all that hard.
If it were all that hard,
then how is it possible
that we have all these
large open source projects
on Apache with MySQL.
On Linux kernels for instance and people
have been able to do that, okay.
So a better question might be why is
parallel programming
perceived to be so difficult?
So how could parallel
programming ever be as easy
as sequential programming, you might ask.
Well it really all depends on
the programming environment.
If you've heard of the
programming language,
SQL for databases.
It's a really under
appreciated success story
because it really allowed programmers
who pretty much know
nothing about parallelism
to keep a large parallel
system productively active.
With a fairly easy command
that is very English,
that is very much English like.
MATLAB actually does
something very similar
and then on Linux systems, take a look
at the command here that I've written
using a get_input with
a grep and then a sort.
In this case the shell
essentially runs a pipeline,
the shell pipeline essentially
runs the get_input grep
and then sort the process in parallel.
Now this is a parallel system right there.
It wasn't that hard so in short
I would say that parallel
programming is just as easy
as sequential programming at
least at these environments
that hide the parallelism very well.
So after years of doing
designs, doing language designs,
some of the things that the
three of us have considered,
the design points that makes
for a good parallel system
so the goal parallel programming,
over and above sequential programming
are these three in general.
You have performance,
you have productivity,
you have generality, okay.
Actually that's a mistake,
portability should really go
with productivity but what it does form
is this well known iron triangle
that if you are team leader of a project,
you will probably have a lot
of experience about balancing
your teams and the tasks
that needs to be done
in terms of resources, in terms of time,
in terms of personnel so when
you're a designing a language,
any language, you have
these things to consider.
Now you may not be actually
thinking about them
in fact most designers probably are not.
When you're examining a
parallel programming language,
you could usually use something like this
to characterize it and see
where the design points are,
whether it's giving more
favor to performance,
whether it's giving more favor
to productivity or generality.
So what about, some people
might ask immediately,
what about correctness, maintainability,
robustness and so on?
Now these are important
goals and they just
as important or important
for sequential programs
as they are for parallel
program and because of that,
we claim that important as they are,
they don't really belong on the list
that we're looking at
for parallel programming.
And if correctness, maintainability
and robustness don't make the list,
why do productivity and generality?
Well given that parallel
programming is perceived
to be much harder than
sequential programming,
productivity essentially is tantamount
and therefore, cannot be omitted.
High productivity, parallel
programming environments
like SQL, they serve a
really special purpose
so generality has to be
also be added to the list.
One of the reason that
they are high productivity
is because they are very specific
and very good at what they specifically do
and that gives you a clue
about language design.
The more general something is,
it's gonna be much more difficult
for it to serve a multiple
large audience of needs
but there's good reason to
have general purpose languages.
You wouldn't be here if
you didn't know about that
because C++ is one of the best
general purpose language there is.
Now given that parallel
programs are much harder
to prove correct than sequential program,
you might still say that correctness
really should be on the list.
Now from an engineering point of view,
the difficulty in proving correctness
either formally or
informally would be important
so far as it impacts the
primary goal of productivity
so in that case correctness
proves unimportant
and they're part of what
productivity would be.
And finally I know that a lot
of you guys are general hobbyists,
C++ is something you
love, you're interested in
but it might not necessarily
be a day-to-day job
so having fun, so what about
just having fun with C++
or any particular language?
Well having fun is important
but unless you are hobbyists,
you would not normally be a primary goal
but if you're hobbyists, go nuts on it.
Let's take a look at the
first criteria, performance.
So the focus of performance has shifted
from hardware to parallel software.
This change
is basically due to Moore's Law
because it continues to deliver
increases in transistor density.
It's basically seeks to
prove the traditional,
single threaded performance
so as you can see
in this picture here on the side
which shows that writing
single threaded code
and simply waiting a year or
two for the CPU to catch up
is no longer an option,
then given the recent trend
on the part of all the
major manufacturers,
parallelism is the only way transferring
the hardware problem into
the software problem.
Even so the first goal is performance,
rather than scalability.
Given that the easiest way
to attain linear scalability
is to reduce the performance of each CPU
given a four CPU system.
Let me give you an example.
Which would you prefer?
A program that gives you 100 transactions
per second on a single CPU or
a program that gives you 10 transactions
per second on a single
CPU but scales perfectly.
The first programs definitely
makes a better choice
and even though the answer might change
if you happen to have a 32 CPU system.
What I'm trying to say
here is trying to bring you
back down to the base case.
People are often too eager to start
using parallel programming right away
because you know, we're all involved in it
and our head's in it but just
because you have multiple CPU
doesn't necessarily mean
that in and out of itself
is a reason to use them all.
Especially given the recent decrease
in price of these multiple CPU systems.
The key point that you
want to take away from here
is that it's just one
potential optimization of many.
If your program is fast enough to optimize
either by parallelizing
it or applying any number
of potential sequential optimizations,
you should definitely go with
the sequential optimizations first.
Let's look at productivity
which is the next part.
Now productivity is becoming
increasingly important
in recent decades.
To see this, think about
the price of early computers
that were tens of millions of dollars
at a time when essentially
engineering salaries
back then were a few thousand dollars.
If dedicating a team of 10 engineers
to this kind of machine
would improve its performance
by even 5% or 10% then the salaries
would be repaid many,
many, many times over.
One such machine, I'm
not gonna talk about,
was this SSRAC, one of
the oldest stored-program
computers there but this computer
had lots of transistors,
was built with thousands
of vacuum tubes, okay.
Requires an army to maintain.
So today it would be quite difficult
to indeed purchase a machine
with so little computing power.
Maybe you could buy some
8-bit embedded microprocessors
like a Z80 but even back
then when we were buying them
as a kid, you could
probably buy them for two,
I don't remember, like
they were two dollars
for a couple of thousand
units or something like that
so it really wasn't all that hard.
Now today with the advent
of multi-core CPUs,
this increase has been allowed to be
continued basically unabated
despite the clock frequency
that we have been encountered in 2003.
So perhaps the take away from this
is that maybe at one time the sole purpose
of parallel software was performance
but now I argue that
productivity is much more
important than just performance.
So one way to justify the cost
of developing parallel software
is basically to strive
for maximum generality.
All else being equal, the cost
of a more general software
artifact can be spread over
more users than that of a
less general one.
Unfortunately, generality
often comes at the cost
of something else and in this case,
one of the iron triangles.
Either the performance or
the productivity or both.
Now to see this, think about
a few popular languages.
If you look at C, C++,
locking end threads,
something we created for C++ 11, okay
which essentially is
based on POSIX thread.
Windows thread or some kernel threads
or something like that.
They essentially give you
pretty good performance
and very good generality
or in this case portability
but it's still not very productive
to program in that manner.
We've all pretty much been told
that you should not
program in raw threads,
you should use some higher
abstractions instead.
However I argue that with C++ 14 and 17,
these days we hope that
we're designing something
that can give you a choice
in that iron triangle.
If you start using higher
level abstractions of task,
higher levels of parallelism TS
using the parallel algorithms
or higher levels of the concurrency TS
which we hope will be coming for C++ 20.
What happens then is that
you can now start tuning,
give you a little more tuning of the knobs
as to whether you want more performance
or whether you want more
generality or more productivity,
more portability, that's the real key
that these new C++
parallelism capabilities
are really giving you.
Now you can all get into the
mud and talk about the fact
that the parallelism
TS allows you do vector
and parallel and vectorize
or just, or parallelize.
At the end of the day it's about the fact
that you can essentially
tune the knobs much better
than before and this is unique
among programming languages.
That's why these guys in red.
You heard Bjarne this
morning talking about Python.
It's probably clear that
Python is a high productivity
language with great
generality and in fact,
in this particular
diagram on the right here,
the further down you
get into the hardware,
hardware level, the productivity
generally decreases,
it's getting more specific okay
but you still, that is the point
where you can start generally get better
and better performance and
better and better generality
so as a result, something like Java
gives you great
productivity and generality.
This is a general purpose language,
inherent multi threaded capability
with a great garbage collector
and a rich set of class libraries
but the performance generally has been
acknowledged as being poor.
MPI, the Message Passing Interface
that the scientific community uses
for node to node parallelism
is something that has been
around for a long time
and it generally powers some of
the biggest computer clusters out there.
It gives you unparalleled
performance and offers scalability
so in purpose, it's pretty general purpose
but it's essentially mainly used
for the scientific and
technical computing.
The productivity is by
many, believed by many
is lowered in C and C++.
OpenMP gives you a set
of compiled directors
using pragmas or comments
and you can paste that
over C, C++ and Fortran in the same way
which give you this
commonality so especially
if you're doing mixed programming
which many of the people
at national labs have,
they might have some code in C.
In fact most of the weather code
that is running, predicting
your hurricane system,
everything around it actually C++ code
but the kernel is all still Fortran code
and nobody can change that
because they've been debugged
over 20 years and if you're
trying to change that,
something will happen
that is not gonna be good
so as a result, this is
the kind of mixed world
where something like
OpenMP really works well
and in this case,
it is quite specific to the
task that has been set forward.
Now this specificity has
a cause because it limits
the performance in some ways
but it is definitely
easier to use than MPI.
Then we come to a language that supports
heterogeneous computing like OpenCL.
It's run by Khronos and they host
a number of graphics
initiative like OpenGL
and now Vulkan and as
well as the language SYCL,
following on that.
It's one of the first
language that allows you
to dispatch to any number of
GPU devices that's not NVIDIA.
If it's NVIDIA, then you
have to use things like CUDA.
It is also believed,
one of its major issue,
not to say its downfall
but one of its major issue
is that it's too low level, okay
and because it's too low level,
it is very low level, its
productivity is not high
so as a result, definitely
it falls in the generality
and performance domain.
Now SYCL is actually the C++ language
that is based on OpenCL
that basically gives you
the same ability, it allows you dispatch
to any OpenCL devices that's not NVIDIA
although that could happen
actually very easily
so it also gives you great
performance and generality
but because it's based on modern C++,
it has the same capability
allowing you to choose
each point of that iron triangle.
You could tune it for higher productivity
if that is what you wish.
If you believe C++ can give
you that higher productivity.
Okay, and finally with SQL,
the Structured Query Language,
it is probably one of the
top language out there
for relational databases.
It's well measured by the
Transaction Processing
Performance Council using TPC benchmarks
so productivity is excellent.
In fact this parallel programming language
essentially enables a lot of people
to make good use of a
large parallel system
without having any particularly
fine detail knowledge
about parallel program.
These people do not know
anything about false sharing,
they definitely don't know
anything about lock-free
programming or hazard pointers
or transactional memory
and yet they're able to manipulate
and get amazing results
based on what they have
so it's important to note that a trade-off
between productivity and generality
has essentially existed for
centuries in many fields
for example in this particular case,
a nail gun is more
productive than a hammer
because a hammer can be
used for many things.
If you have a hammer,
everything looks like a nail
as they say so it should
be no surprise to see
that similar trade-off appear
in the field of parallel programming.
Here in the case, users
one, two, three and four
have specific jobs that
they need the computer
to help them with and the
most productive possible
language environment for a given user
is one that's simply does that user's job
without actually requiring
any particular programming,
configuration or setup, excuse me.
Unfortunately a system that
does the job as required
by user one is unlikely to do the job
that's required by user two
so in other words the
most productive language
in environments are very domain specific
so by definition they lack generality.
Another option is to tailor
a given programming language
to the environment or the
environment to the hardware system
for example low level
languages like assemblers,
C++ or Java or to some
abstraction like Haskell, Prolog.
So that's this given or
shown in the circular region,
in the center there.
And these languages are
considered to be general
in the sense that they
are equally ill-suited
to the job that's required by
users one, two, three or four.
In other words their generality is bought
at the expense of decreased productivity
in that iron triangle.
When compared to domain specific
languages and environments.
Worse yet, basically a language
that's tailored to a given abstraction
is also likely to suffer from performance
and scalability problems
until somebody figures out
how to efficiently map that abstraction
to actual hardware.
So with these often conflicting
parallel programming goals,
let's take a look at how
to avoid these conflicts
by looking at how to do them.
So what makes it hard?
These are essentially the aspects
that you always have to go through.
As you're thinking about
parallel programming,
you will probably have to
do some work partitioning,
some parallel access control,
some resource repartitioning
or replication
or some interaction with the
low-lying hardware, okay.
So work partitioning
is absolutely required
for parallel programming.
If there's just one glob of work,
then it can be executed
essentially at most
by one CPU which is by definition
a sequential execution.
Now the key point take-away there
is that allowing and the thing is,
permitting threads to execute concurrently
allow you to greatly increase
the program's state space
so this is about breaking up your work
so that it can be partitioned,
it can be balanced across these threads.
The problem is that very act
adds to the amount of
overheads and state space
that you might be maintaining
and this is something
you would have to decide so as a result
this can greatly decrease productivity.
Parallel access control essentially means
given a single thread
is sequential program,
the single thread basically
has the full access
to all the program's resources.
These resources are things like I/O,
memory computational accelerators,
files and things like that.
Now the first parallel
access control issue
is whether the form of the access
to a given resource depends
on that resource's location.
For example in some of the
message passing environment,
local variable access is
access using expressions
and assignments whereas
remote variable access
uses a totally different mechanism,
these things called communicators
so the other parallel access control issue
is how threads essentially
coordinate access
to the resources.
This coordination is
essentially carried out
by a large number of
synchronization mechanism
provided by the various
parallel languages, okay
and these things essentially can cause
a decrease in performance because
these traditional programming concerns
like dead-locks, live-locks, coordination
can essentially, is what
is adding to your trouble
with the performance.
The third one has to do
with resource partitioning
your data, your resource
and replicating across nodes
and this happens whether
you're using a single CPU
or multiple CPUs with GPU.
You have to move that data around,
data would have to spread over
NUMA nodes or cache lines.
Okay, indeed one of the biggest problem
with heterogenous computing,
one of the biggest issue
with heterogeneous computing
is when and how to move that
data efficiently to the GPU
so that it can be
computed very quickly okay
and worthwhile for the
computation of the GPU
which often is in a SIMD manner.
So resource partition
basically is frequently
application dependent but it
generally reduces generality
because what happens
is that in our research
and work at Codeplay, we
found that when we optimize
that movement from one particular GPU,
it doesn't work that great with
someone else's GPU like AMD
or whether you might, and none of that
actually works with an FPGA so you kind of
have to throw your hand
up and start over again.
The last one has to do with
interacting with hardware.
At some level you're
gonna have to know that
and hardware interaction,
while it's normally the domain
of the operating system,
the compilers or the libraries
or other software environment structure,
the developers working with
new hardwares and component
often have to work directly with them
and direct access to
the hardware is needed
because that's where
you're gonna squeeze out
the last drop of performance.
Yes it's great to have all
these great abstractions
but at some level you
still need that hardware
interaction to do that and
direct access to the hardware
is great but it greatly
reduces your productivity,
especially portability is required.
So this gives you an
idea of why these things
are important and why every
time you do parallel program
and we're gonna demonstrate that later on
with the hardware examples
Paul's gonna give you
and the direct example
that Maged's gonna give you
in terms of how he's
gonna partition resources,
how he's gonna work with
parallel access control
so at the end of the
day you've already seen
the main theme I've been saying all along.
The reason it is now
easier to drive the car
is because automation is there to help you
so I would say that with
language and environments,
automation is gonna be the key
for some of the killer applications.
I've talked about this
already at the beginning
so I've given it away so what
makes parallel programming
harder than serial programming?
How much, a lot of this is just simply
adapting a new mindset.
Many of us drive cars and
many of our activities
are in parallel when you play basketball
and yet we've been forced down this path
by programming languages before us
to think in a serial manner,
the tools in front of us
are guiding us into doing
this in a serial manner
so if you look back, we can see that given
the parallel systems have
been in existence for decades
it's basically worth asking why is it
that they are causing so much
fuss over the last few years?
This is pretty much toward the end.
I want to step back given the fact
that some of us have a
perspective on history
and many of you guys do too
if you've lived through
the 80s and the 70s,
this is not the first great
parallel programming crisis.
There was another one before
back in the 1970s and 80s
and eventually it led to a large number
of languages, proliferational language
that was the good, the fad and the ugly.
Now there were a lot of ugly languages
and fad languages and of the
faddist languages that we had,
I would say that some of
them things like Paskell
because essentially it's too bad,
you know they held great, great promises
but they had these weird type systems
that you had to work with.
None of it actually
came to pretty much anything.
They essentially gave you small advantages
in some limited area but essentially
large disadvantages in other areas.
When you look at the good languages,
there were actually quite
a few heroes back then.
If you look back, you have,
some of you guys might recognize VisiCalc,
you might recognize the, a
slide presentation manager,
you might recognize the
PDP minicomputers there.
These were back in the 70s and 80s
and they were the successful ones
that allow you to solve
the programming crisis.
The primary solution there was not based
on high minded tools or languages
but rather the slowly
spreadsheets, processors,
presentation manager that
essentially allow you
to multiply your
productivity hundred-fold.
You didn't have to write code in order
to get any of that stuff done.
You could essentially live
through these useful helpers
so given that and I've been
talking about this for a while.
I like to ask what do you guys think
is the most successful
parallel programming language?
What computer language that's used heavily
in productions for decades,
essentially allow for developers
to know nothing about threads locking
or message passing or anything like that,
to keep a very large parallel system busy.
Anybody have any thoughts there?
Yeah go ahead.
- [Audience Member] Erlang.
- Anala?
- [Audience Member] Erlang.
- Erlang, thank you, no.
Yes, over there.
- [Audience Member] SQL.
- SQL, thank you.
SQL is actually one of the most
amazing languages that we have.
It's essentially one that gives
you the high productivity.
At the end of the day, it's all about
either the language gives you,
enable a large percentage of people
to be able to use it in
a very productive way
or itself enable a tremendous
growth in productivity.
Generally the second part is
much more difficult to achieve
so with that, I would like to
pass this on to Paul McKenney.
(audience applause)
- And we'll see if the adapters work.
So I'm gonna take it down a level a bit.
We're gonna go from programming
models and parallelism
into a little bit of hardware.
Software view of hardware,
this is not a presentation
to help you design your computer.
I'm sorry if you came here for that.
I think there's other conferences
that are into that sort of thing.
Risk-V people and some other people
doing open source hardware.
Of course my employer likes
to talk about their hardware
but let's get on with
this if I can remember
my password now of course.
Woo, success.
Okay now let's see if we can convince
this hardware to be productive
and see if I can find the
right place to plug this in
would be another step
in the right direction.
There we go.
Okay, something happened.
And there we go.
Now let's try moving this over here
and let's try doing that
and it almost looks like
something worked, you know.
That's productivity.
Sort of anyway.
Okay.
So I'll be talking a little about
hardware and its habits again.
My website and a place to
find more information on this.
I'm gonna kind of riff on one
of the things Michael did.
I'm gonna say that premature abstraction
is the root of all evil.
Now when I was growing up,
when I was in my 20s and 30s,
it was premature optimization
and in that case,
we had these really old machines.
A machine I used as a freshman in college
had a memory access time
of 1.6 microseconds.
The clock was something
like what, 600 kilohertz
or something like that, right.
And if you wanted the
machine to do anything
you optimized first
and ask questions later
and usually got in trouble doing that
which is why Don Knuth
came up with that phrase
that we all have heard
but this is quite a bit later.
We have machines with much more power
that instead of being refrigerator
size like that one was
but you know there's one thing we've lost
and I think is something
that is really important.
The machine I was talking
about, the one I used
that had the 1.6 microsecond access time
had a little ferrite core this big.
The question I have for all of you
is why should you trust bits of memory
that you can't see with
your own naked eye, okay.
I mean you really gotta
ask yourself about that,
that's one of the things we've lost
although having multiple gigabytes of it,
16 of them on this machine is
kinda handy, I'll admit that
so you know there's something
we've gained as well
so one of the things about
premature abstraction
is there's some things that
are difficult to abstract away.
Alright.
It's hard to abstract away
the finite speed of light.
There was this thing about
neutrinos a while back
that turned out to be
kind of a false alarm.
As far as we know we can't
make the data go any faster
than the speed of light and we'll see
there's other limitations as well
so you can abstract all you want
but the laws of physics
are a little unforgiving.
Now one of the things
we've done really well,
one of the reasons that old machine I used
as a freshman in college was so
horribly slow was it was big.
I mean if you think that it was bad
that the little main memory,
were little cores about this big around.
The registers were about this big
and they only had 12 bits in them
and so it took some serious time
to get stuff from one end
of that machine to the other
if you know, being like this.
This guy's got a CPU
and it's got 16 cores,
excuse me eight cores.
Maybe the next one I'll get.
16 hardware threads excuse me and the chip
is only about this big and so as a result
this thing can run at gigahertz
where the old thing ran
at hundreds of kilohertz
and that's wonderful,
that's something we've done,
we've shrunk these down
so that the speed of light
is less of a problem.
Unfortunately there's
other laws of physics
that get in the way.
Well that was exciting,
anybody want some water?
And the thing is that atoms
are uncomfortably big,
they're inconveniently large.
12 years ago I saw a
scanning electron microscope
of a transistor and the base,
that's the part in the middle
and the thinner the base,
electrically thinner which
is think of it just as thin
right now is what controls how fast
the transistor can switch.
The thinner that base,
the faster the switching.
Well 12 years ago they had
transistors in production
that were about this many atoms across,
four or five atoms across the base.
They have made research
transistors that look like that
where they have one layer
of atoms for the base.
Now it is possible to split atoms,
we've done that for a long time
but doing that kind of spoils
them for electronics use
and our experience and
for much else besides.
Now there are some other
tricks that people are pulling.
I mean one trick is just not
to have any atoms in the base
and believe it or not,
there actually has been
a research transistor
constructed that does that.
You just have the source and the base
with a gap between them.
It's actually a solid state vacuum tube.
I mean it's really cool, the
atmosphere at those scales
is so thin that it's for all
intents and purposes a vacuum
and so you can make vacuum
tubes out of semiconductors
except the way they did that was they made
a huge pile of them on this
chip and a few of them worked.
I mean it's a really cool
demonstration of capability,
I mean don't get me
wrong but we need to have
billions of them on the chip all working
and you kind of have
to plan place the atoms
and well maybe we'll get there sometime,
we aren't there right now so for right now
we're kinda stuck with this.
And that's kind of a problem.
So here we are.
This is kind of a cartoony picture
of what a system might look like.
We've got sockets with CPUs on them,
they can be multi threaded,
I didn't show that here.
There's some kind of interconnectors,
memory, the memory might be
associated with the sockets
or might be off on the
side, let me show here.
There's a little bit of variety.
The problem is,
at two gigahertz, the speed of light
going out and coming
back, it's about that far.
Okay, well that's not so bad.
I mean you know, look at my laptop,
that's most of the laptop, right
and if you've put all
the stuff in the middle,
shouldn't be a problem except that
we haven't used vacuums for
computers since I was very small
and I'm one of the guys
Michael was talking about
with the gray hair alright
and I do have this honestly.
And those vacuum tube
computers are pretty slow.
And the electrons in these things
are not going through a vacuum, I'm sorry.
They're going through silicon,
they're going through copper,
they're going through aluminum
and if you're going through
a conductor like copper, aluminum,
you might get a third
of the speed of light.
That's still not that
bad, it's about like this,
that's as big as a chip, right
except that if you're inside a transistor
you're about 3% the speed of light
and so you're taking forever
in terms of clock rate
to get across your chip and
that's just the physics.
Once you get past the
physics, you got mathematics.
You gotta have protocols to make sure
the data gets understood correctly
to the other side and responded to
and changes state correctly
and it adds extra layers of overhead.
So you know the hardware guy is you know,
maybe very inconvenient
what they're doing to us
but the laws of physics are
being much more nasty to them
than they're being to us.
So maybe we need to give some sympathy
and maybe see if we can help.
So let's just take a look at
what the effects this has.
This kinda cartoony thing.
You know this is kinda
the marketing message
for CPU clock frequency.
Crank it up and we'll go faster.
You know just run the
race, we'll get there,
the faster CPU wins, bigger clock rate,
everything's wonderful
until one of them did
some mispredicted branch
Problem is you know, you think,
the way you think about it,
well you'd hope it'd work,
you know you've got something running at
two gigahertz or even up to five gigahertz
or liquid nitrogen on these guys
doing the overclocking eat up the light.
They think after seven
gigahertz a while back,
maybe they got higher, I don't know.
Haven't tracked it for a while.
You think you know, that's really cool,
we got instructions going along,
we're getting two gigahertz
with two instructions per nanosecond,
more than that if we're superscalar,
one life is wonderful but that's not
really the way they work.
It's kinda like there's
this ocean or pool of stuff
and the instructions kinda filter in
and sometime later come
out the other side.
It's all very parallel
inside the chip as well
and so the thing is you
got a lot of branches
in your program and these hardware guys
really hate the fact we do that.
We have branches all over the place.
They really like to
have straight line codes
so they can just go charging through
and grab it and throw it in there,
smash it up, and some
group and get it done
but we've got these branches and so
they have to predict them
and branch predictions
are pretty good in general but sometimes
they get it wrong and when they do
you've got a bunch of work you did
assuming you're gonna go one way
and then the software just
whacks you over the head
and goes the other way and you've gotta
throw it all away and start over.
This isn't that bad though really.
Let's go on some other things.
A memory reference.
We've got these CPUs
running eight gigahertz
but it still takes many
tens of nanoseconds
to get out to main memory.
Now it used to be hundreds of nanoseconds
not that long ago, it's gotten better
but it hasn't got better as
fast as the CPU rates have
and they're still kind of catching up
and they aren't catching up very quickly.
So if you have to do a
real memory reference
going to real memory, life is hard.
It's gonna take some time.
Atomic instructions,
and this I want to be very clear,
this is not necessarily
a C++ 11 atomic operation
because those might or might not involve
read-modify-write atomic instruction,
okay where you're looking
at some piece of data,
atomically changing it and
putting the result back.
And there's a lot of
constraints on these things.
There's a bunch of
optimizations I'll cover later
that do not work well
with atomic operations
because they have to be atomic.
It is not nice for other
CPUs to see atomic operation
halfway through.
That would not be atomic and
so there's extra overhead
associated with these
which has by the way,
gotten much better over the decades.
These things used to be really painful.
Pentium 4 was just a nightmare, okay.
But they've gotten better but
still, they cause trouble.
Memory barriers, these are fences
and they're needed to restore ordering.
We'll get into why you need to,
well why would you do anything
out of order in the first place?
Well we've talked a little bit about it
with this sea of instructions
that kinda get executed
in parallel inside of the chip.
That means things will get out of order.
We'll see some other reasons later.
If you put a memory
barrier, you're saying hey,
the stuff before has to happen first
and then the stuff
later, just get it right
and suddenly the CPU has
to do something about that.
Now the hardware designers
have gotten really,
really, really clever
about cheating on this
and we'll talk about that later too,
which is mostly to the good.
The thing is though that these things
aren't created equal and I'm gonna switch
to C++ 11 terminology here so
sequential consistency barrier
or an atomic operation based
on it is gonna be expensive.
Acquire release and
acquire release combination
will be cheaper.
Consume if it was implemented
the way I'd like to.
Maybe we'll get there some day,
would be cheaper yet and relaxed of course
on most platforms almost free.
Now this is kinda cartoony again.
If you're talking x86,
relax, consume and acquire
in theory anyway would be all the,
essentially just the instruction,
almost zero cost over the ordering.
But something like a arm
or a MIPS or power bc would
have lightweight barriers
that can do the acquire,
release and the consume
and the consume except on alpha
is just instructions again
but this gives you kinda a
rough idea across CPU families
of kind of, sorta what to expect.
Okay, now if I hit the right button.
A much worse one, these
other ones were bad.
This one's much worse.
The reason that the CPUs can
move along reasonably well
is that they have caches
and can keep the data close.
As we said earlier if you have to
actually go to memory, that's hard.
If you keep it in the caches that's fine
but if the data you
need isn't in the cache,
it's either in memory in
somebody else's cache,
well we can see the one CPU in front
has the memory in its
cache is doing half in
and is snarking off at
the CPU that's got to pay
the cost of the cache miss.
And if you think that's bad,
if you think that's bad, try I/O.
I mean you know, a
cache miss might consume
a few hundred nanoseconds or maybe
a small number of microseconds
on a really, really big machine.
If you're using rotating rust,
you know the standard spinning discs,
those things are in
milliseconds, you know.
Several orders of magnitude worse.
SSDs, solid state discs make
that a little bit better,
quite a bit better actually but even so
if you're using networking,
if you're talking from
here to Bangalore India,
well I'm sorry but that's gonna cost you
100 milliseconds just to
like speed of light, okay.
Let alone going through
glass and being amplified
and switched and everything else.
So I/O can be painful
and that's why there's
this cottage industry
in things like Memcached
and other things like that
to try to localize
things and avoid the need
for unnecessary I/O.
So I'm gonna do this slide
and then I think it's
gonna be time for break.
Let me just take a look and see if I'm,
yes, we're gonna do that.
So this is where we're gonna stop.
This just kinda goes
through costs of operations
on a fairly old but not that old system
and the operations aren't exactly lined up
with the previous things
because it's difficult
to separate out of...
How much is due to (mutters) addiction,
how much to use that to the other
so I basically looked at going across
threads and a core, across
cores and across sockets
for the most part and I also
looked at CAS versus locking
for the simple cases.
So this particular
system had a clock period
of 400 picoseconds, 0.4 nanoseconds
and so we have the operations on the left,
we have the cost in
nanoseconds down the middle
and the ratio to the clock period.
Now there's the number
of clock periods required
to complete the operations
down on the right
and of course hardware's gonna vary.
You take your hardware,
you get different numbers
and somebody else takes their hardware,
get different numbers, that's fine,
this is more or less representative.
The key point if you're just
doing normal instructions,
you should be able to crank them out
at more than one per clock
if you're doing well,
maybe one every second clock if
you're not doing quite as well.
If you are doing complicated operations
like compare and swap
with a read-modify-write
atomic operation that we
saw the CPU tripping over
back there with an electron
crowd around its foot
or a locking, you're taking more than
an order magnitude hit for that.
That's costing you because the CPU's
having to deal with that.
Now this is a fairly old CPU.
Newer CPUs might get
you a little bit better
but it's not gonna be perfect.
Cache misses.
If you're within a core,
one thread to the other,
it's not too bad still.
You're an order of magnitude
but not two orders of magnitude.
If you're going from one core to another
within the same piece of silicon,
you're almost two orders of magnitude.
So just having the data flow
from one CPU to another,
one core to another within the same chip
within the same socket is
almost two orders of magnitude
more expensive than the clock cycle
and if you're going off-socket,
you're well over two orders of magnitude.
So this really does matter.
It takes, you've gotta be careful.
If you spend a cache
miss to go off-socket,
you better be doing a bunch of other stuff
to make up for that
because just do the multiplication.
If every other instruction
is a cache miss,
you're taking a 200 times slowdown.
That means that your parallel application
needs 200 CPUs to keep up with one CPU.
I don't know about you
guys but the idea I had
when adding more CPUs to
make it go faster, not slower
and so when we come back from break,
I think break starts about
now and it's 15 minutes,
have I got that right?
I'll take a little bit of a look at
what we can do about that and after that,
Maged will take us
through a given algorithm
to show exactly how I did for
that particular algorithm.
Thank you much for your time, attention,
enjoy your break, see you
back here in 15 minutes.
(audience applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>