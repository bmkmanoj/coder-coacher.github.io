<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: Artur Laksberg “Concurrency TS Editor's Report” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: Artur Laksberg “Concurrency TS Editor's Report” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: Artur Laksberg “Concurrency TS Editor's Report”</b></h2><h5 class="post__date">2015-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mPxIegd9J3w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you everybody for coming my name
is Arthur clarksburg I'm a software
engineer Microsoft I'm a regular
participant in the work of that
concurrency study group inside the iso
c++ standardization committee i'm i'm
serving there as the editor of the
technical specification for c++
extension for concurrency i am one of
the contributors to the TS i also
coordinate the work of other
contributors so what's a TS concurrency
TS is one of the papers proposed by the
sg-1 concurrency study group this is how
the concurrency study group tries to
advance the state of the art of
concurrency in the standard C++ a
technical specification is not a part of
the standard however we hope to
ultimately make it part of the standard
it is difficult to predict that this
point which portion of the standard will
it appear in but regardless we hope that
this technical specification remains an
authoritative document that there will
be implementations widely available when
many platforms Microsoft is building one
boost already has one I know there are
some people in this room who have built
partial implementations of the TS so you
should be able to use the tools in the
concurrency TS in your code if not today
then sometime very soon so what's in it
I've IDs has three main components one
is improvements to STD future which
allows us to compose multiple futures in
a way three matter a couple of new
concurrency primitives latches and
barriers and also a new concept that is
very useful for locally programming
which is the atomic shared pointer so
you should not think of this TS as a
single cohesive
feature rather it's a collection of
features that the committee could agree
on it could have been less it could have
been more I'm happy that you have what
you have here also this TS is not the
end of the road of this work this is a
v1 version of the proposal there is
going to be a v2 and v3 we know there
are missing features we're working on
adding more so let's dive in into the
first feature the first proposal is
called the improvements to STD future as
a quick refresher of what is that what
is the future you can think of a future
as a as a proxy for an eventual value
typically a future is used together with
a promise and somebody who creates a
promise can then set the value on it and
then that value will be stored in the
associated state which is typically a
heap allocated although some
implementations exist where it isn't hip
located and then the future typically on
another thread can then retrieve the
value from the shared state or wait for
the value to become available a future
is very useful for representing
long-running operations or any
operations where we need to wait for the
results to become available and it's not
immediately available so a typical use
of a future would look like this now
imagine a case where we are reading a
string from a file right the first one
one of the ways to get the value out of
it is to call the get method and when
the value is not immediately available
in the future that call will block the
current thread until the value becomes
available and then extract the value out
of the future right if the calling
thread is a scarce resource which which
happens the preferred way to consume
that future would be by attaching a
continuation to it and that's that's the
dot then method this is that is this is
part of the proposal again the the gate
part is what we have today a
that does then method is what we are
proposing in the in that proposal the
van method takes a collabo object
typically this would be a lambda
expression in which you take the future
and then call get on it that get is then
guaranteed to be non blocking and then
you can use the value right and the
reason we pass the future in it and not
at not a string is because we also want
to take care of exception if exception
is thrown then a call to get on the
result will resurface that exception
right so the interesting part about that
then is that the function itself returns
a future and then the type of that
future is determined by the return type
of the expression inside that then in
this case we are returning an integer
therefore the return type of the then is
going to be a future of int ok another
useful property of the then method is
called the implicit unwrapping very
often when you work with futures and
continuations you find yourself invoking
an asynchronous operation inside of the
call back of the handler of another
asynchronous operation in which case if
you look at this example we are reading
a string that string represents a URL
and then in the subsequent operation we
are downloading HTML from that URL and
here we are turning a future of a string
right so had we followed the regular
rules for the return types the type of
the the type to torrent of the van
expression would have been a future of a
future of a string that type is not very
useful therefore implicit unwrapping and
wrapping kicks in and that's why this
type becomes a future of a string and
this is what allow us to do what we call
a chaining continuation change
often in code that uses a continuation
center features and continued shins you
you'll see this as a pattern you read
something or you perform an operation
inside the operation you perform another
operation return that and you can
immediately attach another continuation
to it right that's sort of this is how
you can ask string together multiple
asynchronous operations in addition to
the van method the TS also introduces
two operations that act on multiple
futures the join in the choice the join
creates a future that becomes ready when
all parameters passed into it already
and choice creates a future that becomes
ready when at least one of the
parameters israeli both join and choice
come in two forms homogeneous and
heterogeneous this is a homogeneous case
here the joint is implemented by when
old operation which takes a range and
returns a future of a vector of a future
of an end and it looks a little bit
intimidating but the good news is that
most often you don't have to spell out
the type you just say auto right in this
case I spelled it out so that I can
explain it what it is first of all the
return type is always a future because
it is an asynchronous operation when all
doesn't wait right second vector is
there because we take the futures that
are passed into the function through the
range and move them into a new container
which is the vector and finally it's a
vector of a future because instead of
moving the values into the vector we are
moving actually the original futures
into the new vector we never unpack a
value out of the future and move it we
only move the future themselves and of
course because it runs the future you
can attach a continuation to it and
process that continuation like here when
you go through the vector it is
guaranteed by definition that every
future in that vector is
ready this is a heterogeneous example
the return type of when old is a little
bit more complicated now it becomes a
future of a tuple of a future event in
the future of a chart again it's a good
thing that you don't have to spell it
out in Jersey order and then extracting
the value out of that future is very
similar to the previous example choice
homogeneous example works like this we
call when any on the range of futures
the return expression is going to have a
type of a future of when any result
which is just a struct of an index that
represents the future that is ready and
the container itself of a vector of a
future of a net right and when I consume
that future I can index I can use the
index which is a member of that when any
result and the future that I get this
way is guaranteed to be ready right at
this moment more than one future can be
ready but at least one this one is
guaranteed to be in a ready state and
finally this heterogeneous example gets
even more complicated now the return
type is a future of an any result of
tuple of a future of inter future of
chart and consuming this is also
straightforward with the exception of
that you cannot use the index which is a
round time variable to index into the
two pole right that's why I'm using an
if statement here if the index is 0 that
i'm using i'm going to use the get of 0
free function to extract that value out
of the two pole so we have seen how
futures can be used for sequential
composition as well as join and choice
right so futures are very good for
creating graphs or expression tags if
you were right what if your problem is
not is not a duck right what if you have
some cycles in that graph
let's imagine a case where you're eating
something from a file and you have a
trivial or simple file in either class
that has a constructor wooden function
is EF that indicates if the reader has
reached the end of oil and they get next
string function right so let's try and
see how we can use futures and
continuations to work with either like
this a synchronous way of consuming text
when that tree there is straightforward
you're looping until the reader has
reached has not reached the end of file
and then within each iteration you call
result that get that is a blocking call
of course and then you print out the
valium let's try to do it asynchronously
so in this case I'm attaching a
continuation to the result I'm getting
the value and then I have to do
something else here right but this being
an asynchronous lambda this this is
where it becomes more difficult I cannot
jump out of a lambda into the middle of
the loop right especially an
asynchronous lambda so what do we do so
let's write something like this
obviously this isn't going to be a
scalable approach but just just humor me
for a minute sometimes it pays to write
ugly code before we can see patterns and
then we refactor that code and turn it
into something more reasonable what if
I'm simply I'm just going to repeat what
i did in the for reading the first line
and come up with something like this
right and I I can do even more and I can
repeat it again right we we all have
widescreen monitors nowadays and you
know we can go pretty far but joking
aside this is the point where we start
seeing some patterns right there is
something that we do in each iteration
that's the first observation so this
piece of code in there in a brown block
right we read the value out of the
future who painted on the screen
and the second observation is because we
cannot use iteration the next tool that
we usually reach for when we cannot use
iteration is what precaution right so
let's let's do that now this is a
function that reads read from the file
that funk is something is something that
is passed by the user right and it
invokes itself recursively i should say
there's not a recursion it's a pseudo
recursion because typically we invoke
the function itself on a different
thread but but it is still sort of a
kind of recursion and we're done right
we can we can stop right there but of
course where engineers would like to
build with a usable components so let's
try to generalize it even further so one
observation here is that in every
iteration there are three main things
right there is a predicate that we
invoke and that tells us whether we need
to continue iteration right then we can
make it again something that the user
can pass into function it's a parameter
next we have an iterator we call the
iterator that yields us the next value
in each invocation finally there is a
user-provided funk tour so we can write
the function it looks like this make
iterative future you can use it for
reading profile you can use it for
pretty much anything right invoking this
function would look like this again
quite reasonable right now no not not
super simple but reasonable so we
started with this simple innocently
looking a synchronous code and wound up
with this right even if you consider
that this part is going to exist
somewhere in the library this is still
hard right this is probably too too
complex for a regular user and the truth
is this is the point where we are
reaching the point of the limit of how
far you can go with the library
solution but don't despair a better
solution is on its way and it's called
the await two things are happening here
first is read a sinc function needs to
return a future maybe future of a void
or some other type that represents an
operation and eventual operation right
it doesn't have to be a future but you
know if we started with you two let's
continue to use future second we simply
rewrite result that get into a weight
result and at this point we are done
it's easy to write easy to read it's
efficient and at this point you might be
wondering okay so if we have that why
even bother with concurrency guess why
even bother with the then function right
why can't we just wait a little bit
longer and get this into the language
and we'll be done and that's a
reasonable question but of course if you
are using futures today what futures
lack what they lack today is the ability
to store continuations right and the
then function is what adds that ability
to the future the difference between C++
14 what you can do today and what you
will be able to do when a weight is in
the language is that today it's you the
programmer who invokes the then method
in the future it's mostly going to be
the compiler that invokes the invokes of
their method right we human beings have
difficulties reasoning about asynchrony
because it means that we have to
essentially rewrite our code into into
very functional form compilers have no
problem dealing with that so I highly
recommend that you attend Gordon Sean of
stock tomorrow he is the expert and at
this point I'm going to move on to the
next part of the controller cts notches
and barriers so somebody using condition
variable for the first time
might naively do it like this so in this
case I have two threads one is the
detecting thread it's a thread that
detects a condition and then notifies
the other thread the waiting thread that
the condition has occurred ok so again
naively I'm going to use a condition
variable weight the weight requires a
unique look I'm going to give it a lock
the lock requires a mutex I'm going to
give it a Munich's save compile run it
will compile and it will run about fifty
percent of the time depending on the
time right there are there are many big
problems with this code right but
they're the frustrating part is that it
compiles first of all of course the the
big red flag that should pop up in your
head when you see this is what's up with
this mutex what is the shared state that
this musics but it is supposed to
protect right obviously that's
definitely a bad smell the second is
more subtle it so happens that if you
notify the condition variable before it
enters the weight that notification is
ignored the other problem here is that
this simple weight fails to take into
account spurious wake-ups it just so
happens that an operating system can
wake up a condition variable for other
reasons reasons other than the other
thread notifying it so it is it is hard
to get right but this is one way how we
can get it right so we introducing a new
state it's going to be a boolean or
Willian bull flag when we notify the can
before we notify the condition variable
we set it under lock and then we call
notify one in the weighting function
first we check that flag if the flag if
the condition is not met then we enter
the weight and then repeat so first of
all this solves the problem of premature
notification in a way in that we set the
flag and then notify right and then the
waiting side will first
check the flag and if the flag is set
will it will never even go into the
weight right so that takes care of that
timing problem now this while loop there
will take care of spurious wake up if
the condition variable wakes up for
another reason we iterate again until
the condition is true and eventually
will exit that loop when the condition
has been satisfied an alternative syntax
would be to call to put the predicate in
the weight function directly right I I
personally like this in text mode
because it makes it more explicit let we
recheck the predicate first before we
enter the weight this syntax does
exactly the same but if you're not sure
what's going on here you have to you
have to look it up the other thing that
I had to do to make this code more
efficient this is an efficiency issue
another correctness issue is adding
those two currently braces here you
should know that notifying condition not
notifying of the condition variable
doesn't need to happen under look right
it's okay to do that but for beta
scalability you don't want to hold the
book for longer than necessary idiomatic
code like this shouldn't be that hard
and this is why scott Meyers in his book
suggests that a sort of condition
variable just use a future of void right
for the reasons that I just stated okay
and this the advantage of this approach
is of course this code is correct by
construction you write it it compiles
and it will be correct right the
additional upside here is that you can
also traffic exceptions for a future
that p-promise care can accept an
exception you can you can't going to
pass the exception throughout state
there there's a couple of downsides here
one is that the promise is not reusable
so every time you try to use it again
you wind up creating a new shirt state
and that is as I mentioned previously is
typically heap allocated
now if you're writing an app and you
know that this isn't going to be in the
hot bath of your app then you can take
that heat and that's okay right if
you're writing a library or a low level
with a usable component that it's much
more difficult to make that judgment
call right you always want to err on the
side of let's be as efficient as
possible and not take any performances
enter latches and barriers the TS
introduces three different types latch
is a simple thread coordination
mechanism that allows one or more
threads to block until the condition in
operation is completed barrier is
similar but it can also be reused and if
flex barrier takes a callback that gets
notified when the when the latch opens
this is how I would rewrite my previous
example using the latch here I'm
creating a latch of 11 is a counter here
this is a number of times that detecting
thread must call countdown okay and the
waiting thread would call weight and the
latch opens when account when the count
reaches zero okay windows programmers
are familiar with the concept of event
right event is exactly like that it's a
it's a larger count one how you can use
it here is we are waiting for all tasks
to finish I'm starting a bunch of tasks
and i'm using a latch of task count
every task when it finishes decrements
the counter and then in the calling
thread i'm waiting for the counter to
reach zero okay another example I'm
starting a bunch of tasks and then
immediately freezing them waiting for
the latch to open right in the meantime
I'm preparing data for those tasks once
the data is prepared
look at this prepare data I'm unleashing
all the tasks then they start doing the
work so that's all what latches and bear
is all about I'm not going to go a
deeper into the difference between
latches and beers but this gives you the
overall idea and find any part number
three atomic smart pointers most people
are familiar with compare exchange this
is a bread and butter of locally
programming the idea a typical pattern
is like this you read the original value
you compute a new value and then you
replace the old value with a new value
that you've just computed if the old
value hasn't changed and if it has
changed in you repeat right this is what
this while loop is doing and often when
using when writing locally data
structures you'll find yourself using a
shared pointer right to to manage the
lifetime of the notes in this example of
a shirt hopefully list so there is a
couple problems in this approach with
what we doing today first of all it's
it's Oprah you need to rely on
developers discipline to always use an
atomic operation whenever whenever you
try to access the shared pointer right
there is no currently there is no way to
declare an atomic shirt pointer that's
what this that's what this proposal is
composing and the other problem is that
it's it's pretty hard to implement
atomic compared to change efficiently
unless you are willing to penalize all
users of shared pointer because a naive
approach would be a let's just put an a
spin lock or a mutex inside a shared
pointer right but that would impose
overhead when users who don't care about
using shared pointers in an atomic way
so for example a Microsoft
implementation uses a single a global
spin lock and you can imagine how
scalable that is
labe C++ is a little bit more
sophisticated there's a hash of mutexes
there's 16 your exes and then it is more
scalable but there is more upfront cost
in accessing the right metrics right so
it is still a problematic with atomic
shared pointers this problem is solved
first of all the access to the object is
guaranteed to the atomic just through
the nature of the type and second
operations like compare exchange now can
be implemented more efficiently yes you
can implement it by putting a mutex or
spin lock inside an atomic shared
pointer perhaps there are more efficient
implementations but at least that would
be better than what we have today and
this brings me to the end of this talk
for references I recommend first of all
reading the latest concurrency th paper
on this link you can snap pictures or
the slides will be available so you can
you can get that I I find it very useful
to sometimes go back to the original
papers and read the original papers that
introduce the concepts because they have
rationale they also seem a little bit
naive in retrospect but at least they
have the rationale that in later
revisions of those papers tend to get
lost and just replaced with standard
ease so those three papers are the
original papers that proposed the
components of the GS and I also found
this load by Anthony villains very
useful in understanding why do we need
to have a short winter so I have about
10 seconds to spare I think and some
time to answer questions
the question yeah so the question was
yeah the question was I told Mike
SharePoint that sounds like a like a
good idea but it's probably not going to
be low free and the answer is you
probably right it probably will not be
locally there is a function called is
locally that you can call and check
whether it is locally right but what you
really care about not whether it is not
free or not but whether it is efficient
to ski in scalable okay I don't see any
more questions thank you for your
attention that's why I have</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>