<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2014: F. Pikus &quot;...Scaling Visualization in concurrent C++ programs&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2014: F. Pikus &quot;...Scaling Visualization in concurrent C++ programs&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CppCon 2014: F. Pikus &quot;...Scaling Visualization in concurrent C++ programs&quot;</b></h2><h5 class="post__date">2014-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tOxi-cH7n8A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is fedora pikas and i'll be
talking about the profiling of
concurrent program specifically memory
with performance visualization of
concurrent programs speaking of waking
them up if you are still not awake after
herbs wonderful performance remember
that if you don't ask questions I'll
start asking questions that's how I know
if you're paying attention or sleeping
and so I'll talk about how to get the
detailed and useful profile in the form
of timeline of events in a concurrent
program what happened before and after
and during something else was happening
we'll have some hopefully useful
techniques for concurrent programming
and there will be some code that i had
to simplify to fit on the slides there
will be some code that actually is that
simple as it will be on the slides so
there are some kind of general
assumptions specific for this talk they
can be extended probably but that's what
i used for for preparing the system so i
program on linux i also use some other
unix is pretty much everything that i'm
talking about will work on any modern
version of linux or unix I as far as i
know yes okay sorry repeat the question
yet the question was if mac is unix bsd
core as far as i know everything that
i'm talking about is supported on bsd
there may be slight variations in system
function calls but as far as i know
everything is supported on bsd I don't
know if Mac actually disabled any bsd
features but as far as like the standard
bsd should be supported 64-bit address
space if you have to work on 32-bit
address space there are some things you
can do but you have some clever code to
work through and i only tested
x86 hardware and actually I tested on
powerpc hardware a bit so other
platforms may add some wrinkles that I
didn't know about ok as far as me asking
questions how many people have attended
harps talk on Monday on almost everybody
ok so this will be easy for you so why
do we measure performance in the first
place and profile it well as C++ is a
performance oriented language if you're
using C++ odds are pretty good that
you're interested in performance of your
programs now even if you're interested
in performance of your programs
programmers are pretty bad at making
guesses about how the program will
perform a why it's performing the way it
is that's why measurements are important
and for concurrent programs it's even
worse because they depend on more subtle
details and more of details of more of
other things like libraries and hardware
and the day of the week and things like
that the programmers are even worse at
guessing why their concurrent programs
behave the way they do now why okay if
the concurrent programs are so bad why
bother writing concurrent programs you
have seen these slides many times which
is why I made a small copy of it
basically single CPU cores are not
getting much faster anymore the CPUs
themselves are getting bigger and bigger
but where that transistor count goes
mostly in various concurrent hardware
caches cache coherency units some of it
into vector units supporting multiple
cores instruction decoders other stuff
that is basically can only be used if
you're writing concurrent programs again
where C++ people we are interested in
performance that's the way you get
performance from
modern browsers okay how do you profile
a concurrent program well there are
profilers and most of them work on
concurrent programs some some don't but
you know Google profiler vtune they will
work and do their thing and all of them
are useful if you have a good enough
profile many problems in concurrent
programs as well as non-concurrent
become obvious there is a key problem
that you don't have to face in serial
programs in serial programs if you know
where your time is spent you you
basically know everything's the order of
execution may be data dependent of
course but well you can print the log
you can reason from the data you know
what what took so long in concurrent
programs you may be confronted with
weird situations where nothing seems to
be taking particularly long but the
program isn't performing well so here
are three examples I actually observed
on on the application I work on so these
are the these started tests like three
subsets of the program I was working
with the first program on one thread it
runs at hundred percent cpu load 11
second real-time one second CPU time on
two threads well that's pretty good
real-time dropped for the same workload
real-time dropped almost by half CPU
time when a little bit up there is some
overhead on four threads real time is
almost back where we started cpu time is
going up and I go all the way to 64
threads i have actually 64 course so
that there is no contention here a real
time is way higher than it was for a
serial program CPU time is even higher
the second program basically shows that
adding threads doesn't help at all
doesn't hurt but doesn't help either and
the third program shows that adding
threads basically keeps the electric
bills high because power is being
consumed CPUs are getting used but the
real time doesn't go down okay so let's
look at our first program the first
program started to scale then stopped
and actually went into extreme negative
scaling any guesses what was happening
here yes exactly so if we had a timeline
and this is how the timeline would look
like so time goes left to right and this
bar represents our unit of computation
on one thread on two threads two bars
going on at the same time almost half us
just a little over than half short on
four threads we start seeing waiting on
locks on all four threads and on 64
threads this is just one of them we
spent all of our time waiting on logs
basically yes that is the symptom of
extreme lock contention sorry not
necessarily so that's kind of a tricky
question that boils down to what cpu
time if you if you use spinlock yes it's
pure busy waiting it's just spin CP over
an hour if you use mutexes you're
supposed to go to sleep try heavily
contend it on the mutex sometimes it
works sometimes you see all your time
show up as a system time so this was
actually observed on a mutex and yes I
didn't tell you that the CPU time which
I measure as like I just give it to you
as a single CPU time if if I broke it
down between user time and system time
you would see that most of it a system
time
cpu it's still spun up so it's just
doing not your work it's doing whatever
OS is doing when it's trying desperately
trying to figure out which thread do I
wake up now and this happens like 100
million times a second okay so this was
one of the programs this is the kind of
information we want to get so it's kind
of sticking a microscope into your
threads we like abbreviation so what
should we call it thread scope what we
want to know is basically what the
threat did how long did it take what was
happening while that was happening what
what happened before and after if we
wait it on a look for how long who was
holding the lock while we were waiting
how many threads were contending on the
log if you hit practicing lock free
programming how many threads were
contending on Atomics and visualizing it
in the form of timeline just makes it so
much easier to see what the problem is
compared to printing a lot of numbers in
table so let's look at our second
program here basically we are threads
and nothing happens so what's going on
here a one option is single thread and
that would be correct so that's not the
one I'm going to show and that's not the
one that happened but that's a possible
option anything else could be happening
exactly there are probably other ways to
get this but this is the one that as I
said this is from an actual program so
this is this is what actually happened
so when you see a picture like this you
immediately go a complete serialization
for some reason each thread has to wait
on the results of the previous thread to
complete and then it starts doing its
part okay and the third program is a
real time stays the same CPU time
steadily goes up what could be going on
here
and he guesses okay let me show the
timeline then timeline for one threat is
obvious timeline for two threads both
threads are concurrently running there
are no locks the same operation takes
twice as long for threads are
concurrently running no locks the same
amount of useful computation they are in
the same data set yes know that no no no
there is no stupid mistake like that so
the question was whether they're just
redoing the same thing over and over
know they're actually doing so for
example let's say if this was sort one
of them is sorting the first half and
the second asserting their they have
well of course and sort doesn't scale
perfectly you have to sort the whole
thing after you sir half and half but
kind of that's the idea or if you are
looking for prime numbers one of them is
looking for all prime numbers like in
the first half of the array the second
one looking for all prime numbers in the
second half of there is something like
that so they are doing different things
but that's what they're doing no logs
logs would show up as those orange blobs
so what's going on Oh false sharing is
what it's close and it could be
responsible for this in this case it
wasn't they really were working on
different sets of the same data set but
you're getting close so false sharing
almost there sorry not quite close
almost there sorry no no disgaea in this
case there it's just computing yes
memory bandwidth limitation so what's
going on yep it's a memory bound program
now how can it be how can a program be
so memory bound that this is what you
get well let's not guess about
performance let's measure what the
memory bandwidth is here is I'm sure
you've seen this plot many times memory
bandwidth on one do we have a pointer of
some sort no pointer okay
okay so huh all right so okay I'll wave
my hands so on there on the horizontal
axis is the size of the memory the
classic way of measuring the memory
bandwidth will be writing a memory block
intimate over and over now oh thank you
okay so memory size bandwidth for small
memory sizes were sitting in an l1 cache
huge bandwidth then we drop the l2 cache
l3 cache and that's the main memory now
I why does it say here 256-bit well
because it was 256-bit i was using SSE
instructions to go up your memory I
really tried to saturate the memory bus
I wanted to see what the memory
bandwidth is now this is one thread
running on one core this is two and four
threads same thing so while we're in l1
cache for four threads there was a lot
of noise maybe I didn't run it long
enough but basically l1 cache stays a 1
cache yep there is one perk or l2 cache
for two threads there they shared
between two cores so for four threads we
kind of dropped down l3 cache there is
only one this this is where we hit the
main memory this is per core okay this
is per core but we have four core so we
do more work let's multiply the middle
curve by two and the last curve by four
and I i scaled cut down on the plot so
this is the total memory bandwidth on
one thread for for for all cores meaning
one core this is for two threads and the
set basically the same is for four
threads two cores fully saturated the
memory now this is an extreme memory
bound program because I tried to be as
memory bound as possible but apparently
the little computational fragment that I
was showing you before was pretty close
it was strongly memory bound so you
actually don't have
that much spare memory bandwidth and
concurrent programs okay a bonus example
I just out good question sorry no
shouldn't shouldn't you seen a small
difference in execution time between one
third and two threats since there was
some spare man I should have okay so
good yes question 1 cor doesn't quite
saturate the memory bandwidth as I said
this was cut out from a real program so
they say basically this was a task that
was going on on one or two or four
threads and the same task was going on
at the same time on another one or two
or four threads so they were already
contending pretty heavily if you did it
in isolation you would see a small my
previous chart shows that the aggregate
memory bandwidth for two cores is not
not quite 2x of one core but the like
one and a half example something like
that so yes if I did it purely if
nothing else was going on and contending
for memory saturating the memory bus
also i would have seen a slight spike a
slight improvement and then go back down
but this is this is how memory but now
also remember this was done or this is
done on purely like using non atomic
operations if you sprinkle in atomic
operations your contention becomes worse
so now we start hitting full sharing
well it's true sharing in this case but
it has the same effect as the full share
so those of you who attended harps dog
have seen his bonus example remember the
bonus example yep the slides were
produced thanks to herb low log Q was
produced on consumer threats that's from
herb slide so what her pad is basically
lo lo q the so I'm not going to most of
you were there
I'm going to repeat it basically the
point is that these are what are these
these are consumers and this is the
critical section for the consumers and
these are producers they also have their
own critical section and what herb found
out that his initial implementation
which is the one I've shown you yielded
negative scaling to consumer threads
made made it worse than one and produce
these threads to producers were better
but Siri already rewards and after some
experimentation what he discovered was
that the consumer look was the main
problem and the producer look was the
second problem specifically doing a lot
of work inside the critical section was
the problem now I'm sure you've all seen
a lot of work inside the critical
section here for those who didn't let me
highlight it this is consumer this is
the work this is copying the this is
copying the data this is producer this
is the work this is pruning deleted
nodes from the queue again all the work
is inside critical section i haven't had
a time to actually convert the slides to
curb slides to code but this is my best
guess at how the timeline would look
like consumer starts consuming within
the task of consuming we can ask
questions okay what was it this is a
rural big task what was it doing a sub
tasks so it was mostly copying that's
fine this but while it was copying
another consumer thread couldn't do
anything it was waiting on a spin lock
then it had to do its own copying and
while it was doing copying the first
consumer threat had to wait on the lock
we can be pretty confident in that of
that because once were moved the copying
out of the spin lock things went much
faster how did the producer look like
well probably the same not all producers
do the cleanup of a trimmed node but
once somebody does this one hits the
cleanup everybody else who is running at
the same time has to wait on the lock
again
once clean up of the of the remove note
was removed from this critical section
things went much better so the timelines
like this make a lot of problems obvious
now I've shown you for different types
of problems with concurrent programs a
memory contention doing too much work in
critical section lock contention and
serialization who had at least one in
their practical work who had all four
okay not all problems are obvious memory
contention for example is not directly
visible you have to learn to interpret
the timelines once you do it becomes
useful okay so hopefully I convinced you
that it would be nice if we had these
timelines well how do we get them the
there are some profilers that give you
something similar the they don't know
what's interesting for you so they tend
to collect a lot of data or they collect
not enough data now they can be very
useful in a in determining where
approximately or your problem lies so
then you can go in and instrument the
problem to collect just the data you
need what's the problem is collecting
too much data in addition to it takes a
long time it's also takes it's hard to
read it takes a long time for you to
figure out to sift through that data
even if it's in the form of time line
you know if you have a time line with a
clarity of one microsecond and the
program is ten hours well you can't look
at every micro second you will have to
zoom out look at something that would
clue you in where the problem is zoom in
and keep doing it back and forth that's
hard ideally you want the interesting
things to just jump out at you so my
solution is intrusive meaning I have to
instrument the program to collect the
data so interesting events could be
start of a function call end of a
function call acquiring looks sending
message or I oh how long did it take
what was going on while
that was happening now we're going to
collect the timeline which would solve
our problems of performance
unfortunately it may create other
problems along the way well programs are
kind of the quantum things measurement
affects the results so more measurements
especially synchronization disturbs the
program if you introduce additional
locks it will partly serialize your
threads which means your program will
not be performing the same way as it
normally performs so your measurements
will be skewed well if we try to not
introduce additional locks we risk data
races because we're collecting data from
multiple threads at the same time and
even if we are pretty confident about
what the interesting things are still we
may have to collect a lot of data
program runs for a long time you you may
you know if you're collecting even like
once every millisecond for 10 hours well
that's a lot of data all of the data
ultimately goes to disk so I 0 can be a
problem well okay let's see in first
I'll do a preview in general how am I
going to solve the problem there is some
amount of work i have to inject onto
your real computing threads the threads
that do the actual work can't be helped
it has to be minimized both the compute
the extra computing i add on your
computing threads and the extra logging
i add on your computing threads has to
be minimized I may have to do additional
processing because my profiling requires
some amount of work done if I don't want
to put it on your computer as where does
it go I have to run separate threads now
that means that i'll be taking cores
away from the main program that's
actually not bad that's better than
injecting work in to compute threads why
well because let's say I have a 16-core
system for example I can reserve three
cores for the profiler so now I'm
running on a 13 core system okay I
I can limit my program to 13 course it's
still it would give me legitimate
answers about how well my program
performs on 13 course probably about the
same if there is a problem on 13 courses
probably the same on 16 course I want to
reduce the memory footprint of the
profiling that's another thing because
we may be already met with a memory
bound finally I owe has to run on
separate thread so the workflow would
look some sort of like this these are my
work threads I have to inject a little
bit of work into them to collect the
data for the measurement but then I'll
send this data to one or more separate
processing threads and eventually one of
these threads will save it to disk okay
in order to figure out what work I have
to do on the computer eyes I have to
figure out what data I may want to
collect so this is how my timeline would
would look like my first thread was
doing this task called F function f1 was
the argument of one second then it
called the f1 was too at the same time
another thread called f1 was three and
hit a log so this is something like what
I want to see so this real time is on
the horizontal axis which means i have
to collect real time I would usually
want to know how busy the CPU was during
this time so I'm probably want to
collect CPU time as well and I may want
to collect CPU time per thread that's
usually useful how busy the CP was on
this thread if it's some large task oh I
may want to collect CPU time / process
that helps me to quickly zoom in on
relevant part ok i have 10 stages in my
computation I'm running on eight cores
the first one was cpu load was a 7.9
exo-k probably doing good second one was
for X so let's see the third one was cpu
load was 1x for the whole thing for the
entire process that's problem so I may
need to collect cpu load for process see
this one
23 that's user data may need to collect
that for some forms of profiling stacked
rates may be interesting could be other
things oh while it's calling of 10 if
one may be big so how what was exactly
doing inside f1 well it called g one and
this one called g1 and g2 so i may have
these nested measurements in which case
i need to know how deeply I'm nested you
always want to collect only what you
need because it's expensive so we want
to minimize the work we want to collect
efficiently we want to collect only
what's necessary but we also want to
make it simple better nap that it has to
be intrusive so here is the simplest way
we can stick this data collection
objects you will see how my code
actually looks like i have a macro that
does this but basically we will measure
essentially each each of these bars on
the timeline will be the lifetime of one
of these objects so here i have an
object that lives for the entire
duration of the call of f1 which means
the constructor starts the measurement
and the distractor finishes the
measurement and if i want the nested
measurement I call a g1 inside there is
another data collector so that will be a
nested measurement here of course can be
in any scope doesn't have to be a
function so we probably always need real
time we need the task ID that f1 we need
to know what it was doing so some form
of task ID threadid so we can this
allows us to draw the timeline almost
always we want if we have any nesting
move on the nesting depth CPU time per
thread almost always useful in some
cases CPU time / process maybe user data
stack trace so yeah we need more than
one collection data collection class
i'll be showing you only one for
simplicity now where does it all go so
i'm collecting this data where does the
data go where would you put the data
that you collect where
in memory yes has to be stored in memory
did anybody say memory memory has to be
allocated which means overhead and needs
to be synchronized did anyone say store
stored means data races that again in
mid-summer nization how are we going to
deal with that well plus there are all
these other things how to collect
real-time nesting depth and so on cpu
time let's simply click go through the
CPU time and real-time Linux provides us
with these high-resolution timers we can
wrap them into a class a very easy a
pretty high resolution Thank You 5250 90
seconds on modern CPUs okay so we can
measure time no problem memory
allocation what do we want has to be
thread-safe has to be low overhead has
to have minimally disruptive
synchronization what does everybody else
want same thing well use your own heap
sure but again everybody else wants the
same thing so why can we do better than
any other heap out there well we
actually have a pretty special case we
don't need a general location pattern we
our allocation pattern is pretty
specialized when the work thread starts
doing one of these tasks or we should
want to generate a time by bar we need
to allocate some memory when the
measurement is done we will finish using
that memory and we never want to see it
again sorry no but something pretty
similar now furthermore as I said the
word threads never want to see that
memory again which means whoever cleans
it up is has full control of the time of
the cleaner okay it's basically memory
pool which is kind of similar to stack
on lids it's not stacked with similar
idea so what's a memory pool
conceptually contiguous region of memory
with an offset which tells you what's
used what's not used allocation from the
memory pool is very simple you increment
the current top of the pool
and grab the memory between the old
location and the new location of the top
and that memory is yours and you write
stuff into it anybody sees any problems
on this side in the context that we're
going to use it in why wouldn't this
work well let's assume that I i I'll
make increments aligned still doesn't
work I have more than one thread ok so
the comment was updating to atomically
you jumped a little more the reason this
doesn't work as written is because it's
not thread-safe fortunately thread-safe
well you can use one per thread too but
fortunately thread-safe memory pool is
just as simple all we need is some form
of atomic increment atomic increment
well either returns the old value or
returns a new value depends on which I
Tomic increment you're using it doesn't
matter if you get the old value if you
get the new value subtract subtract you
increment again so atomically increment
the top now that section between the old
top and the new top is yours only you
see it only the thread that did that
atomic increment can access that
allocated section over here there is no
there is no racing for writing into it
because other threads cannot get to it
so it's very simple so let's say this is
what we want to store in memory depth
threadid is the tag that gives us that
user identifier start time stop time CPU
time so that's the struck that we want
to show them to memory well here is our
collector class it will have a pointer
to the task and it will have the timer
inside and it will have the depth that
counts what does the depth count well
it's a depth of these above these
timeline bars which basically means how
many collectors we currently have alive
on the current threat because remember
we're measuring the lifetime of the
collector object so we created collector
object in the outer scope it's alive in
the inner scope we created another
collector object
now we have a nested measurement so we
just need to know how many collector
objects are alive on the current thread
well ignoring depth of so forget about
depth for a moment when we construct
this collector object we are going to
allocate from the pool and that's where
atomic increment goes and now we own
this memory as long as we want to
because no other thread can get to it so
with no fear of races we're going to
construct the task object in the memory
we just allocated this is just you
displacement new and in the distractor
with no fear of races again we're going
to write the stop the final real time
and the CPU time and we're going to
count the depth increment the depth and
decrement the depth where does that
what's that depth how do we count the
depth per thread yeah okay the question
was at ALS so everybody knows everything
all right if you had on the one thread
you would have a static and because you
have only one thread you would count the
lifetime of these objects with no fear
of races if you have two threads you
need one static per thread which is TLS
in GCC you can you have this attribute
under under thread in pthread system you
have pthread get specific take your
choice there so if depth is thread
specific then we never fear races for
depth because on one thread we're only
constructing one object at a time on one
thread you can have multiple objects
nesting but only one is being
constructed or distracted so no races
within one thread ok so that's that was
easy and basically we're half done I'm a
little over half time but we're have
done as far as the word thread see the
universe is created at the beginning of
the measurement to allocate some memory
store some stuff at the end of the
measurement with store more stuff and we
never want to see that memory again well
that would be great somebody else has to
see that memory again if for no other
reason
to deallocate and that's the runtime
system the profiler it's the runtime
system we create for the profiler so how
does the profiler see the same memory
pool well your threads called this empty
or threats called this empty allocated
this is where they put their data in and
this was they called it basically done
your profile your profiler has the other
view it sees this is ready this this
doesn't mutate anymore this is where all
your work threads are right writing
something into it so this is in flux and
this hasn't been used yet this is what
the profiler sees profiler has to
release the memory now also I said the
memory pool is conceptually one
contiguous long region of memory notice
that word conceptually where would you
get one contiguous long region of memory
somebody has to provide it would be easy
if we had infinite memory we wouldn't
have to deallocate and we wouldn't have
to grow the pool well we don't have
infinite memory how much memory do we
really have well how much memory do have
on your on your machine four gigabytes
16 gigabytes half a terabyte if you're
like it one or two terabytes on a really
big machine but that's not the final
answer how much memory do you really
have virtual memory 64-bit address space
202 to 64 bytes that's a lot of memory
do you really have that much memory know
how much memory do you really have well
on x86 it's actually 48 bits one bit is
reserved to the colonel it's actually 47
bits ok but it's still how much is
hundred twenty-eight terabytes of
addressable memory that's a lot how it's
close enough for infinite from any
practical purposes if you could only use
it well how do you use it the same way
you do now you already use it you just
can't use all of it at this
time so basically the rest of the talk
will be playing games with infinite
memory for those could anybody here does
not know how our show memory works just
so I know how quickly I can go through
these slides I have a client so any bit
anybody needs an explanation of a
virtual memory right raise your hand and
I'll go through it no okay so let's
let's go a little faster so what your
process has logical memory it somehow
gets mapped onto physical memory along
with everybody else's memory did I say
Matt I did what's that mapping thing
well that's the correspondence between
physical memory and virtual memory so
it's done with a granularity of one page
on Linux typically 4k don't hardcode
this number into your program operating
system has this thing called page table
and it gets some help from the hardware
that we're not going to go into but what
it does it maps the process ID and
logical page to physical page
conceptually it looks like this your
multiple processes have their own pages
each process has conceptually an array
of index biological page which points to
the given physical page that's not how
it's really done note that physical
pages can be in any order can don't have
to be contiguous oh note also that some
entries in the page table don't point to
any physical pages so what can the
entries in physical in the page table
point to physical page including shared
memory swap page another disk page and
nothing now I said it's conceptual
because in reality page table is much
more efficient than that all right most
of the address space actually maps to
nothing nothing is very important you
have hundred twenty-eight terabytes of
address space you probably don't use
most of it which means most of your page
table maps or not
page tables by necessity have to be
particularly good at mapping things to
nothing they are since we're going to be
playing with memory maps how much
control do we have over the memory map
well first thing is how do I map a
logical page to the physical page very
simple you do it every day you do it
every micro second of every day you
touch the page if you write or read into
the page physical page materializes into
so you're right to read into logical
page the first time physical page
materializes it's zero field well if
it's zero field why do we have an
initialize variables because usually you
get your pages from Mallik you didn't
you weren't the one who first ash that
Malik was a Malik can recycle the pages
put some garbage into the pages other
stuff okay how do you reverse that
process well there is that system call
on Linux em advise give you the range
and what do you want to do with it this
is a constant defined and one of the
include files this will cause the
mapping to be erased but true by the way
if ya page size keeps coming up every
time as I said don't hardcode 4k this
this this this gives you the page that
ok don't do this on the page you get
from Malik Malik doesn't expect you to
blow away its its page mappings it may
want to store something in that page
after you released it if you want to map
it not just the physical memory but to
this there is the same map call has a
lot of arguments look up the main page
the interesting ones are length and the
file descriptor and that's how your map
it to that file so what would it do if
you mapped it to that file well here is
what it would do logical pages get
mapped to disk pages and for the ones
that you actually touch there are real
physical pages created for the ones that
you don't touch the runt again if you
typically you create like you have
hundred twenty-eight terabytes of
logical space you probably are
aren't going to use that all even on
disk the result will be what's called
sparse file file whose length is whose
size if you ask for what's the file size
will be hundred when a terabyte what's
the actual use space much less a little
trick the file system support most of
them anybody here use this Andrew file
system now okay you're looking anybody
here from IBM be careful this was it was
sparse file systems yes I know my hat
off to you then okay so the important
part is even if you have virtual address
space reserved it's not real until you
use it if you don't have physical pages
mapped it doesn't count as used memory
if you do top or PS you will see virtual
size of 128 turbines if that's how much
you reserve our resident said whatever
you actually used if your is if you call
that I'm advice call that I've shown you
before to release pages on disk what
will happen is the discontent stays but
the physical page goes away all right so
now here is our simple memory pool with
disk support this is the same allocate
call that we have seen before this is
what the work thread see and there is
some kind of flush that needs to flush
it all to this can release physical
pages this is what our profiler sees
so how did that happen okay what should
profiler do well her father should
figure out what memory is not needed it
should then flash it to disk and then
release it page by page just you know a
few days in close proximity of razor
blades and it should all be done because
the easy way can't possibly work this is
locked reprogramming people it has to be
hard anybody wants to know what the easy
way was the one that can't possibly work
who wants to know what the easy way is
okay easy way what would happen if we
flushed the entire memory pool what
would happen if we flush the entire
memory pool you block that's okay
because this is on the profiler thread
so yeah it starts doing I oh it blocks
that's fine what else sorry yes so some
pages would be dirty they would be read
right back in well so this is basically
all the things that would happen if you
have memory pages that are not haven't
been flushed to disk they will be
flushed to disk sometimes you may need
to crawl em sink for that and that's
what I will actually block em advise may
or may not do it the standard ways to
call em sink spend some time with man
pages to learn all the details physical
mappings will be erased the address
space will remain reserved but it won't
have any physical pages backing if your
program including your work threads
keeps accessing that logical space which
it does because next next time it will
write some allocate a bit more and keep
writing it's real time and cpu time that
page is created and populated with the
content of disk this actually does
exactly what you need it to do the only
question is how much does it cost you
okay who here things that this solution
is practically viable the cost would be
sufficient
who think that there is no way the
solution would be too expensive both of
your own what's the kikuna was the
correct answer I do who else what's the
correct answer what's the correct answer
to any performance question yes this is
the correct answer to every performance
question measure it okay well to measure
we have to write something what do we
write we write a simple flash thread
that once a second outside I'm sorry
this instead instead of discarded should
be flush i renamed the function while
working on the slides this says flush it
spelled discard but it actually reads
flush so once a second and the second is
just AM arbitrary time could and yeah in
a practical program you don't you sleep
you use non asleep non-slip didn't fit
on the slide because it takes long
arguments so once a second we would
erase all physical mapping and flash all
memory that has been changed to disk not
we don't we don't even bother to figure
out which ones we already flashed which
ones we didn't flash we're flashing the
whole thing it and the work threads and
we keep doing that until the word
threads are done we have some atomic
count that atomic flag I'm sorry that
tells us when the work threads are done
we need to finish ok here is my
measurement practice so this is the
actual practical result 32 threads
running on 32 cores without significant
with machines without observable
disruption of them of the program being
profiled the sustained rate of events is
about one event per 100 microseconds on
each thread if there are more than 32
threads we have to throttle it down a
bit but peak rate can spike much higher
as long as it's not for long this
testifies to the fact that the page
table is incredibly efficient because I
am on mapping a whole bunch of nothing
on every call to this range and page
tables does it very well it's some sort
of tree actually if then if the whole it
divides the range
so if a node if a parent node map shows
nothing that means all the children are
all nothing and it doesn't go in so this
simple disc pool is actually sufficient
for most practical purposes well what
happens when it fills out that's the
hard part again it has the easy answer
you die practical result that's how I
did it I ran it for 12 to 24 hours I
haven't had the program terminated due
to that yet partially why not what would
I do with several terabytes worth of
data if it terminated I can't read that
much well the harder way is you either
all over or you freeze the entire system
and you swap in another memory segment
it's a very rare event this one question
I'm not player are you flushing the
entire memory pool or just the part that
the threads have already released I'm
flushing the entire memory pool the page
table looks and says okay this has never
been allocated ignoring that this has
already been flashed never mind that
this is the one that has been dirtied
since the last flash let me flush that
and what impact does that have on the
threads what are they able to a few few
more page faults would be so several
more page faults would be generated
because when the thread hits the page
that I just flashed every time it
triggers a page fold that's the impact
it turned out not to be that much again
it boils down to the rate that you can
sustain one page fall to every 100 micro
second not too bad you probably have
more for other reasons if you had to do
it every nanosecond that would be bad
okay let me just show you basically the
house annotations look like in my code
so i have this thread scope timed
process which take and various
parameters and this cap create
measurement this inside this macro
creates an object again every time I
measure the lifetime of the object and
the same thing is for task process
measures the
/ process time task measures per thread
time task is a little bit more
lightweight lock is the only one where I
can't do it with a lifetime of an object
why because i have this log guard here
or scoped look and if I wrap the scope
around it the scoped lock will go out of
scope right there I actually want this
the to measure how long I waited on the
lock from the time I asked for it to the
time I got it but the scope cannot end
here so this is the only one where I
have to use to macros some things don't
even need a duration I may just want to
have a single tick on the timeline this
is where something happened so I have an
event this is a or a pending event event
means constructor puts the tick pending
event means destructor puts the tick you
can do other things okay i don't i have
no experience in writing your eyes i did
i did it the lazy way out i converted
the data to CSS HTML and chrome was the
UI so this is this is actual screen shot
from the UI time threadid each bar is a
record of something i have nested a
record so while it was doing this big
task it was doing this smaller tasks
inside all the user data shows up as
pop-ups on man onmouseover but the top i
have the heartbeat collecting aggregate
cpu and memory utilization why not i
have another thread runs and background
wakes up every second matters to pu time
here is an interesting bit this this
whole stuff is being flushed to disk
once a second i can read it from disc
while the program is still running I
have to live with the fact that some of
my data will not be finished so i have
to like I may get like 0 a real time
because real time hasn't been written
yet I have to discard that's that
somehow if I figure out how to do that I
can visualize this data while the
program is running for the portion that
already been collected okay we have
of a little bit of time I can I can show
you how to do things if you really want
to do things the hard way and I can
answer questions que solo do you have
questions on what I said before yes
unfortunately now i did so i would have
to pull it out of our code base which is
doable and in order for it to be kind of
a practical use it would need to be you
know tested on more platforms than other
than what we have I don't see I never
developed on Windows I not only I don't
develop now actually never developed
anything on windows so I don't even know
if there is a map there so somebody if
you want to do a Windows port you know
come talk to me we can talk it over as I
said I can pull it out of the of our
code base and isolated make it like
devoid of any proprietary content but I
don't know actually anything about
windows and I don't know em map I'm
advised i don't know if it's there or
not any other questions are there things
you can measure using this
infrastructure that you can't with vtune
for example well that's so well so there
is a trivial answer yes i can for
example collect my actual user data on
these time bar so for example i said
this is the Soviet will say you'll this
is like this line of code you can
profile profile for this line of code or
like this is this function and the
function took from here to here I can
add this is this function with
parameters one two and three and the
next event on the same function is the
same function with five six seven so I
can collect arbitrary user data that's
one the other is kind of more subjective
if you you know if you let the team
collect everything it can it becomes
slower and it's hard to figure out what
you're looking for if you
don't you may miss what what you're
really looking for so that's kind of
subjective between you know instrumented
invasive versus automatic so there are
some things which you cannot do and then
the rest depends on what you consider
more useful and what your habits are
what's easier for you to work on yes
instead of having each record store the
thread ID could you have used
thread-local storage to have one pool
per thread and then avoid having to do
an atomic increment hey you probably can
uh you sorry you you would have multiple
files so son which would all be synced
so somewhere you know it inside the
bubbles of the OS there would be some
synchronizations going on because you
actually don't have that much bandwidth
to disk so that they would have to
throttle each other it would have again
it would have to be measured you would
have a bit less control over who
throttles whom and when it would have to
be measured yes questions okay we have
just few minutes well should we see the
hard way okay let's see the hard way now
we don't do the hard way just because we
want to like to doing things the hard
way well sometimes we don't sometimes we
do just for the heck of it but let's
consider practical example I want to
collect stack traces stack traces
actually require a lot of processing you
have to collect stack trace on the work
thread because they stack the bare
minimum and what's the bare minimum for
the stack trace is just an array of
stack pointers but stack traces for
example let's say I want to collect
stack trace of every memory allocation
that's too much data you can't flush
that much data to disk well even if you
could you would basically run out of
this case that's a lot of data
so what you probably want to do is
aggregate how many allocations I had for
each stack trace so now you have some
sort of hash look up or some other
associative look up did I see the stack
trace before how many allocations I had
already add one something like that
that's expensive you move that to
another thread so your work threads
generate events into a pool that is not
backed by disc because we're this cannot
keep up with that you're have one
processing thread that sweeps the pool
pull the data out does the processing
releases the memory and rides it into
another pool that could be the disc pool
and then you have a flash thread that
releases the memory anonymous memory
pool the same as disc pool on Lewis out
the file and if you release the memory
page it's actually gone okay so this is
our stack trace count and pointers and
the processing thread sweeps the
basically the pool that is the memory
pool is is basically one of these trucks
after another that you just keep adding
them into the into your memory pool the
processing thread keeps taking them off
data races how do you avoid data races
you make this for you you have it you
need to have a flag for example this
count could be atomic if it's zero it's
not ready yet haven't finished
collecting stack trace as soon as it
becomes non zero we're done the
processing thread can take it off so
this is what it looks like work thread
keeps shoving stack traces in here we
have the sweep thread that has a pointer
its moves the pointer forward once
tracked of the other as soon as it hits
the struck that hasn't been finalized
this count is 0 it stops and waits for a
while and if it hits the struct with the
count nonzero it discards the memory
page it writes the accumulated data into
another memory pool this is this is the
disk pool that we have seen before it's
being
wash the disk okay one to do things even
harder way what if one thread cannot
keep up you use two threads how do you
use two threads well one of two ways you
can run actually two sweeper threads at
the same time now you have tricky
synchronization problems the easier way
is to do a pipeline the sweeper does
half of the necessary processing write
it into another memorable second sweeper
sweeps that does they have in case of
stack traces there is a for example a
natural separation one of the one thread
count stack traces the other one adds
symbols to the stack traces that's how I
have done and that's the last slide so
yeah the hard way actually is harder so
if you want details you can talk to me
after the session but what we what have
we seen we have seen hopefully some idea
about how to measure performance of
concurrent program how to rip data out
of multiple threads with very little
contention very little sharing very
little overhead and how to create almost
infinite memory more questions how do
you decide where to put your collector
objects it seems like you have to know
the problem where the problem lies
before because your instrumenting the
code uh well yes so as I said collector
objects instrument interesting events so
what's interesting if you don't know
anything your normal profiler would
actually be extremely useful even not
even well we tune may may have some
timelines even like the simple profile
like Google profiler would tell you this
function spends a lot of time and
doesn't seem to produce much result so
that's what you know you instrument the
function in the guts of it if you often
your programs have some logs I'm
starting the simulation I've done the
simulation I've I ran on 32 threads
one-hour real-time 30 30 minutes CPU
time
probably something is going on wrong
there that's what they're going to
instrument so basically short answer is
any way you can see where it's doing
well that's basically what profilers do
they hit you with p trick that's yeah
you're running s tracer ptrace every of
periodic government DTrace or bit race
but that's basically what the profilers
do they hit you with spee trace every
now and then and and see where you are
other questions well thank you a
question okay do you ship your code with
the collectors in it uh I ship it with
collectors disabled well so I have two
ways of disabling it I can disable it a
compile-time because they're all macros
and that's actually one of the reasons
they're all macros and I can enable some
compute some you so normally the very
production version you know the one that
goes out generically I ship with
collectors disabled if I want to if I if
I have a problem somewhere the customer
side and we write software 40 for sale
so it's running a customer side I can
ship it with macros enabled but the
runtime switch disabled and then support
engineer would would enable it on
specific units and send me back the
trace
well if there are other questions then
just come talk to me after the session
and thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>