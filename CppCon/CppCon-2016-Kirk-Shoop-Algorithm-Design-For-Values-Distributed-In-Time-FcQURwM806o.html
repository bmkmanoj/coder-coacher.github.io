<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Kirk Shoop “Algorithm Design For Values Distributed In Time&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Kirk Shoop “Algorithm Design For Values Distributed In Time&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Kirk Shoop “Algorithm Design For Values Distributed In Time&quot;</b></h2><h5 class="post__date">2016-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FcQURwM806o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome my name is Kirk Shoop and I'm
going to be talking about algorithms for
values distributed in time the slides
for this talk are live they're committed
to github and that's a wet live web site
the slides are interactive so if you
want to follow along pull them up and
play with them yeah so about me I love
code this isn't my natural habitat
standing up and talking to people rather
be with my back to a wall and a bevy of
screens in front of me but I care about
code and I care about things that make
it hard to make errors to check in
errors and so I've done a variety of
utilities over time and this is the one
I've been working on for the last couple
of years my team has a meeting every
week my manager goes and asks us to get
a number between one and ten to describe
how good our week was and the weeks that
I code I have a high number and the
weeks that I don't I have a low number
so I work at Microsoft I was going to
school in Seattle in college and my
roommate at the time was a CS major and
I was not and his homework was fun and
so I went and took a class in Windows
programming and a couple of people from
Microsoft we're also taking the class
and they insisted based on the class
that I submit a resume and I became an
intern and then they renewed the
internship and I was in school and
interning until they couldn't do that
anymore and then I became a contractor
and finally eventually succumbed and
became an employee and that was in 2001
so but my internship started in 93 so I
have been in a couple of other companies
and
between the who spent the majority of my
life at Microsoft so the goal here is to
talk about algorithms that are additive
to what we're already defining in the
standard things like STL and range v3
and parallel STL they all deal with
values distributed in space vectors
lists even generators these are things
that are already available before you
call the algorithm and the distribution
of the values is in space and they do an
incredibly good job at that parallel in
addition has the trade-off of trying to
consume as many resources on the machine
as possible to get the processing done
as quickly as possible whereas the
algorithms I'm going to discuss are
dealing with values distributed in time
you may not have them all yet and its
goal is to be extremely efficient while
it's waiting for the next value to
arrive and then process it with a fairly
low latency and so these are additive
not overlapping to what we have already
another goal is to scan through some
fairly dense slides without trying to
completely understand all the code or
all of the concepts and then at the end
go back and talk about questions and
hopefully get a little more depth and
then finally the actual purpose of this
law but the bulk of the slides is to
demonstrate how particular algorithms
that are useful in this domain impose
certain requirements
just like the the iterator types these
algorithms impose certain requirements
on a abstraction that will allow them to
be written that will allow them to
function
so examples of code using values
distributed in time I assume you've
probably seen Shawn parents talk about
using algorithms over raw for loops I
kind of think the same things going to
eventually apply to raw callbacks or raw
futures you're essentially going to get
to this point where you actually want to
see people using algorithms instead of
reimplemented them every time they add a
callback and then the other interesting
formative talk is eric nibblers for the
range v3 library where you'll hopefully
if you've seen that some of these
expressions will start to look familiar
in the way that they compose algorithms
so this slide deck like I said is fairly
interactive I've compiled my C++ code
with him script into JavaScript and it
runs live in the slides on transitions
so the expression on the left is
actually running on the right and the
output is coming out on the right this
is a fairly simple expression that is
just taking the first three even numbers
from a from integer values distributed
every second arriving every second and
it actually delayed them the delay is
highlighted in the algorithm so this
diagram shows what delay does every time
that the source emitted at one a second
an integer the delay made it actually
wait another second before it actually
arrived before it was actually emitted
this is another example this one is
actually I don't I haven't seen this at
the conference yet there's actually a co
wait syntax for the for statement and
what it imposes is that begin has to
return and away table and it will be Co
awaited of the range on the right hand
side of the : + + + iterator also has to
return unavailable and it will be
awaited
and so what you end up with is the
ability this is actually the modern CPP
library this is a code that has been
running in the modern CPP you run
library where pointer presses from a GUI
library from the GUI library from WinRT
are actually being emitted by the
pointer pressed generator a sync
generator code generator and then you
can actually apply these using the pipe
operator apply a transform that extracts
the position from each pointer pressed
and then a filter that only passes on
the position if there's no shape there
on-screen at that point so it can't
select it it can't find a shape and then
it'll just add one since it can't find
one and then that used copy if or filter
which just takes an input stream and
based on the lambda passed in decides
whether or not the value should actually
be passed to the output this is the same
library it's using fork or weight it's
using pointer pressed it's using pointer
moved and pointer released and this
actually only emits objects that are
selected and it emits the move events
from the pointer while the mouse is down
and so this with zero state in your
class this single function
we'll let you click and move visuals
around on a canvas and it just uses the
same standard algorithms this one
introduces merge where it's merging all
of those events all those streams
together of move events and this allows
inter leading between things that are
distributed in time different sources
distributed in time this is the same
thing but using a different library
that's actually hooked up with the M
scripting and so it will actually count
the number of moves that were emitted
during a drag and that used to take
until that's how the pointer up
terminates the previous stream of Loos
so we only subscribe to the moves wild
of mode and pointers down and the way we
know to stop listening to moves is when
the pointer released occurs and so take
until implements that it waits until the
second stream emits its first value and
then it stops taking additional values
in the output this is using it on an
async source of bytes to to extract
packets and so the packets in this case
are delimited by carriage return and so
this is just an application of a few
algorithms on the async source of bytes
to extract out messages and so you can
see use the same algorithms on any async
source of values be they network or HTTP
requests which I use all the time in
JavaScript the same libraries are
available in a lot of different
languages and so my job has me using
JavaScript these days and a whole bunch
like
hundreds and hundreds of coordinated
HTTP requests from the client browser
and so I just use our X expressions like
this to coordinate all of those so that
they happen
they're orchestrated correctly this is
an example of using an HTTP request in
C++ again it's hooked through to the M
script and library so the browser
actually made an HTTP request for the
readme md4 RX cpp on github and then
displayed the first line and you know
it's fairly quick even in JavaScript so
that used scan which is like reduce but
instead of only emitting one value at
the end
it emits each new value as its produced
so the lambda still runs for there's
still an accumulator variable but that
accumulator is emitted every time it's
updated and this allows a lot of common
patterns around making sure that you can
have a whole bunch of streams going into
this scan and each of them is updating
the accumulator which can actually be
say the model for an application or some
other data source and now you get an up
you get a notification every time as an
output you get a notification every time
that data source is updated and so you
can do things like a rendering off of
that or any other sort of data
processing so of course you know the
common reasons why you want algorithms
as opposed to raw code is because you
can reduce them in a generic way and
document exactly how they behave you can
end up having people maintain them
actively for you know a few people
maintain them actively for a huge number
of people so they end up being stable
and optimized and because of all the
time that's gone into reducing their
to a particular doing one thing well
they tend to be fairly descriptive when
you compose them so that you can
actually kind of read code a little bit
better when you're consuming algorithms
consider assembling algorithms together
than if you're just looking at it bra
code so algorithms operate on sequences
of values and we can deliver those in a
variety of ways in space you can have a
vector you can have an array you can
have a generator every time you call it
it immediately returns you the next
value in the sequence these are all
expressions of data that are distributed
in space you can also have things
distributed in time like mousemove
events and network packets and that's
what this topic
talk is about so reactive extensions is
a set of algorithms that has been
implemented across several languages
it's used in production Netflix and at
Microsoft both in the Netflix they use
rx Java and they use rxjs whether
they're client or server depending so
there's implementations for many
languages and yeah so this is just for
the rx cpp library that i've been
working on for a couple years these are
the algorithms that have been
implemented so much like the STL this
ends up being a fairly extensive set of
algorithms and it's not done yet but
it's actually in pretty good shape it
uses C it's built for CPP 11 it runs it
can be compiled by 2013 Visual Studio
2015 Visual Studio clang and GCC and all
of these are
tested every time they github is updated
so let's talk about algorithms to help
define what the shape of an abstraction
to support them requires you have an
algorithm like copy if it's just filter
it's deciding whether or not to pass
something along that needs to
selectively pass on to the next handler
to the output stream laster default
needs to be able to select when the
needs to know when the source is
complete when it's not going to receive
any more values so it can return the
last value that it had stored take needs
to be able to stop the source even if it
isn't done yet delay needs to be able to
schedule a function that will be called
later rather than blocking the source
immediately return to the source but
schedule a function that will be called
later to omit the actual value resumed
error needs to be able to have the error
delivered to it in such a way that it
can instead return a new stream to whose
values will continue the old stream
instead of stopping it so in the course
of building this talk I ended up
building a proof-of-concept library that
I'm calling rx EPP v3 that's C++ 14 and
has only been tried on clang so it's
significantly different from the more
robust v2 but it's useful for these
slides things look a little better so
your simplest features to support
something like copy if is you've got to
have a source and observable that you
can bind to an observer that can receive
the values as they are
produced you have to be able to lift you
have to be able to take an observer and
in order to build an algorithm you have
to have you implement an algorithm by
implementing lift and so it takes an
observer it adds its own functionality
in front of that observer and returns a
new observer and so you can wrap
observers in order to provide additional
functionality implementations of
algorithms and so you'll see that the
source in makes an observable and just
has a for loop that calls next over and
over again and what it got past was the
was the observer and then you'll see
copy if is an implantation of a lifter
and it makes a new observer that just
calls the old observer if the predicate
is true and auto lam does make these
much easier like c++ 14 make these much
easier to show on a slide than the
equivalent functor objects that i had in
our x EPV v2 yes yeah so this is just
copy off' oh yes the question was the
comment was that you know why couldn't
you just use a straight operator
callable to represent the observer the
observer will need some more methods on
it so this turns out to be pretty
effective you can insert the copy if
after the intz and print the results to
see out and it all runs but we want to
support other algorithms and copy F so
Laster default remember it needed to
allow us to know when it was complete
and store the value until then
so we need to know when it's complete so
now the observer needs a complete method
and when we make the lifter we also make
a subscriber and then inside of that and
observer and when we're getting a value
we store it in some state that we
allocated and when the complete arrives
we forward the stored state and then
forward the complete and so that's the
implementation of last or default so
that was a hundred thousand integer
values going through Laster default and
we only emitted the last one that went
through so that's it for that algorithm
on to the next take the take algorithm
requires you to be able to cancel you
have to stop the source early before
it's done and it also needs to keep
state to figure out to figure out when
it's time to cancel so the types of
features that are needed for
asynchronous lifetime and cancellation
so in order to store state you have to
be able to allocate state that scoped to
the lifetime of the asynchronous
expression not scoped to a block in C
C++ and so you need your destructors to
run when the expression is complete
canceled or errored and not before and
so you end up having to provide
something that allows you to do an
equivalent of make shared that's scoped
to a life to a different of the
asynchronous lifetime and you have to be
able to nest these lifetimes
there are various algorithms that need
to be able to nest them and you have to
be able to register with that to a
function that will get called back on
cancellations so that you can do work in
your algorithm to actually perform the
shutdown and then you
to be able to call on that lifetime the
trigger function that actually shuts
down everything and what you'll find is
that if you've got an expression and
there's something in the middle of that
expression like a take everything above
that expression is going to be canceled
and everything below that expression is
going to be completed and so the
cancellation goes up and stops the
source and then the completions go down
to the the tail of the expression so a
subscription represents lifetime in this
library and you have on the right all of
these functions that allow you to use
the pipe operator to bind all of these
things together so that you can lay bind
your lifetime and you can late bind your
observer and with the adapter you now
have the choice for each algorithm
whether you're going to build a lifter
that goes from subscribe to subscriber
or whether you're going to build an
adapter that goes from observable to
observable there are some algorithms
that benefit that actually may require
going from observable to observable yeah
so this gives you all of the options for
signaling the stop for registering for
when it stopped for nesting at other
lifetimes and for creating state that
scope to this lifetime
so take ends up being an adapter where
you make an observable and then once
you've been passed a context to your
subscriber then you're able to make the
state make your own observe observer
that will early come call complete now
there is underlying in this library
there is contract enforcement so every
time a
leet is called in the library it knows
about the lifetime of that scope and so
it will come forward the complete and
then call the unsubscribe that stop stop
the lifetime and by doing that I don't
have to explicitly stop the lifetime
here I just call complete and it
implicitly stopped with a lifetime as
well so we early call complete and that
will stop the source and complete the
targets the receivers observers so that
worked we only we were going from zero
to nine but we only took three values
and then we stopped so we also have to
support failure and so we had to add an
error method to the observer that will
allow us to pass errors on down the
chain from their point of occurrence so
again errors just like complete will
force cancellation so if you call an
error the error will be passed down the
chain but the cancellation will go up
the chain and stop the source right so
this just used the existing copy if
algorithm but it passed a predicate that
always throws so no values come out take
never really came into the picture at
all and the error went down through to
the print to see out and was per and the
what string was pointed printed out
which was always throw and then the
whole thing shut down so
so we also need to be able to schedule
functions to be called later and that
means that we need something like an
executor or a scheduler or a strand here
it's called strand you can defer at a
time point an observer so the observer
on next will be called the observer next
will be called at time point note for
later that we also have a member
function now not a static function now
the context binds the a it binds some
state to a lifetime for these subscribe
for the subscription for the expression
and scheduler for the expression and so
then you can bind those in using
starters and subscribers bind to those
contexts in so here we can delay using
the make strand scheduler so that what
comes out comes out 1500 milliseconds
after it was emitted by the interval at
the top which was only emitting every
second to begin with so we get this
pattern of produces and consumes and
they're fairly stable they pretty much
always come out the same way well they
always come out the same way it's kind
of important
so the strand has to be a FIFO queue for
time T that means that if you add a
whole bunch of things for that are
supposed to be executed at the same time
they have to be FIFO at that time yeah
so we also want to support testing
because it takes a little while to run
this it's a bit too slow we want to be
able to test these things in time
without actually consuming that much
time so you end up with a virtual clock
that again has a member now but also it
now has
a version of now that will let you tell
reset the clock to a new time so you
actually have virtual time you're
controlling time rather than just
reading it from the underlying system so
and then you have to have a loop
scheduler that can make strands based on
this virtual time and when you use it
you can either step - you can step
through time or you can just run it
through all time through the whole queue
then you need to be able to produce
streams and record them in order to do
the testing and so this just lets you
associate time and a value or a time and
an error or complete and call it a
marble and you can use hot and cold
actually produce observables from a
vector of marbles so we're actually
making marbles values in space and then
producing observables on them and as
values in space they're actually
associated with the time that they
should be emitted and so what actually
ends up happening when you test using
this is that that test loop will just
pick the top off the queue set the
virtual time to match the time on the
top of the queue and then call it so the
result is that that same expression
happens instantaneously instead of
taking so long but it still thinks that
it happened over four and a half seconds
and the way that that expression was
done was that you actually supplied the
list of nexts with the time that it
should happen and the value that it
should produce and then you recorded
them using on record at various points
you can record and then on test
terminates the test and then you just
run the loop in order to do it so you
can actually have multiple of these
expressions
up and then run the loop and they will
all run in order so for instance there
they are side by side the one using the
regular scheduler and the one using the
test scheduler and it ends up being
quite a bit faster for testing so this
is what that looks like on the left you
have the way that you recorded the test
result and then on the right you have
the way that you assert so you can
actually then create a vector of values
of marble values that you expected to be
emitted and you'll see that those are
different than the marble values that
were passed in and then you can just
check whether the marbles that were
emitted are equal to the expected and
then you print either the succeeded
which we got from the previous or you
get failed which we'll get from the next
so I passed in a different value for
take in this one which doesn't match
what the expected sequence expected and
so what we actually get is the failure
and then you actually can see in the
printout that we only got one of the
three values and the completion happened
way too early
and so we failed so the requirements
that fell out out of these algorithms
are that we have to have the sense of
knowing when the next value occurs we
need to know when completion occurs and
we need to know when an error occurs we
need to know the lifetime of an
expression have something that
represents that lifetime that is unbound
to the compiler sense of lifetime and
then we have to be able to allocate
things to that lifetime for state within
the algorithms we need to be able to
cancel an in progress
expression and we need to be able to
schedule functions within it to occur
later and those functions need to be
FIFO for time T for a particular time T
we need virtual time in order to do
testing but it's also useful for things
like video decoders or anything else
where you want to actually scrub through
time in your expressions so I also did
some work this year just and you saw
some of it at the top and of course I
can't these are only running on the
visual studio compiler right now 2015
because they use the code weight
proposal and so I haven't actually been
able to get them running and in script
in yet but clanks coming and then
hopefully they'll run an inscription as
well as the rest but this is what
transform looks like if you have a KO
value generator you just Co await your
source and your selector gets called and
then that result gets yielded and so
this is the whole implementation of that
function of that algorithm and can cat
is similar you can have nested for Co
awaits and those naturally expressed can
get these are the easy ones after a few
false starts I went to do merge and I've
emitted the two hundred lines of code
that wouldn't fit on the slide to show
just the kind of surface that invokes
all of that 200 lines of code merge
turns out to be pretty difficult when
you're dealing with Co await because Co
weight naturally uses code rewriting to
naturally serialize your code your async
code
and so can cats very easy to express and
copy if is very easy to express and
transform and a few others but anything
that's actually taking in multiple
sources and wants to actually await all
of them ends up being fairly complicated
to achieve not impossible and that's
what libraries are for but it's not as
like these are really compelling why
would you use anything else and this is
not so compelling so if you you know it
ends up looking very similar you you end
up using fork oh wait instead of passing
lambdas into the end of an expression
you use the expression you know for Co
await and the body of your expression is
where you do the work with the outputs
it ends up having a code generator that
has begin returning on a waiter that
will resolve to an iterator later and
then end just returns in a iterator
straight the end iterator is simple and
then the iterator on the right the plus
plus operator has to return an Enka
waiter this is different we're not
awaiting a new iterator we're awaiting
the current iterator to be updated
because increment applies to this not
returns a new copy of a you know a
different iterator and so there's two
different away ders involved one for the
begin and one for the plus plus
and the iterator a waiter you know it
looks fairly similar to any other a
waiter it returns the iterator on resume
and the Inca waiter returns a reference
to the current iterator on resume once
it's been updated so I have run through
this with great alacrity to fast to
actually get anything to sink in
precisely so that I can concatenate
async algorithms and expressions of them
whether they be you know transducers
which I haven't shown or rx reactive
extensions like push models or koa wait
which is a little more pull but kind of
ambiguous that way yeah
open floor let's have a talk yes yeah so
the question was that it seems to work
fairly well until you get into more of a
dag directed acyclic graph and I'm
processing how I would respond to that I
believe that it just depends on the
algorithms that you use or the
algorithms that you build the ones that
seem that are popping in my head that
seemed to apply to this are
things that combine different sources
and coordinate them so you can have
different you can have the same source
but with different filters to get
different subsets of it or different
views of it and then you can combine all
those with things like combine latest or
zip or merge to get to combine them back
again to order to reduce them there's
reduce and scan there's a variety of
ways to kind of fork out into a multi
into multiple sequences and then merge
them back into a new sequence so it
doesn't feel like it would be any
different than say using the range v3
library if all the data was in space was
already there - like it should be the
same sort of transforms to you know sets
of algorithms to do apply the same sorts
of transforms in time as well because
you you can each of these algorithms can
have state and so for instance combine
latest is one of those algorithms that
has a single value stored for every
stream that was input and only once
there's a value for every input does it
output essentially a couple of all of
them and you know at that moment and
then every time any one of them changes
from then on it puts out the whole
tupple again and so there are various
ways that you can reduce the types of
combinations that you want into these
simple algorithms and then you can apply
them yes
not strictly from a performance question
or our perspective no I have oh I'm
sorry
have I have I the question was have I
compared this to handwritten code for
performance so in the v2 library that's
more stable I spent a lot of time
getting performance compared to for
loops I don't think this is like that's
attack that asked for the range v3
library to actually you know maximize
the the performance over a set of things
this is more of not necessarily taking a
huge amount of data and reducing it very
quickly it's more of a coordination
thing where you're trying to not have
anything on the stack until there's a
value to be processed and then to kind
of wake up and do everything that you
need to do and then go back to sleep
again so you end up having things like
you know mouse-click events so that on
the message loop tick you're getting
some outputs and then those are running
through the functions so I will talk a
little bit about performance in the
libraries that I've worked on the goal
is to kind of keep the zero abstraction
as much as possible lifetime is the one
that makes it nonzero the lifetime and
the scheduling are actually things that
impose some virtual functions so you can
have queues of scheduled functions those
have to be virtual functions everything
else the actual you know simple
operators like transform and copy if and
filter you know those types of things
those are actually by default no virtual
functions like when this expression tree
is resolved
the
either can see the whole thing and smash
it down into one block of code and so
that usually ends up being pretty simple
but as soon as you start booting in the
interesting stuff like merging which
requires some scheduling delays observe
on which is pretty useful in in these
expressions because it'd be it I don't
know if any of you saw the WinRT Co a
routine talk but you know you can do the
wait resume background to switch threads
in the middle of a function well there's
algorithms for that observe on in these
libraries and so you just put observe on
and the make strand and suddenly the
rest of the expressions on any thread
and then you put another observe on with
a different scheduler and you could be
back on the UI thread or whatever you
wanted to return to and so those types
of things when you introduce them
introduce overhead but when you're just
doing things that don't have any of that
scheduling or life time issues they just
smash right down on the compiler
so the question is whether there's
something fundamental for you to the the
time-based sequence that made me choose
the pipe operator the v-2 library of rx
EPP it primarily relied on a huge
observable class with every single
algorithm in it so that you could do dot
method and have intellisense help and
all that well that was a huge recursive
compiler nightmare this really makes
that go away the pipe syntax and it
happens to match range of e3 which is
definitely working in space so it's
certainly nothing specific to time but
also if you look on this slide at the
three even that's actually an operator
definition like I just composed copy of
take three and delay and now I have the
first three even delayed by one second
and I can apply three even in the middle
of any expression and it will just it's
an operator now so those sound like
interesting operators to build I mean it
sounds like maybe an interpolation
algorithm or something Oh repeat the
question sorry averaging so the
averaging method that's in the library
is a reducer so it actually produces an
average at the end you could certainly
implement average with scan as well so
that every time there was an update to
the average it would be emitted
immediately yeah essentially you know
the the goal certainly of this talk is
to talk about
the right abstractions so that all of
these algorithms are possible and then
they just have to be defined and written
yes right so have I ever dealt with
streams that have the timestamps
embedded in the message so certainly the
mouse events are are things that often
have the timestamps embedded and how do
they complicate merge well merge is
dealing in the domain of the stream the
input streams so it's really dealing it
with the time of arrival not the time of
the timestamp but you can build using
these algorithms you can build things
that will say reorder the incoming
streams but you have to have some sort
of your algorithm has to have some sort
of limit timeout if you will to say you
know I'm not going to remember any more
about I'm gonna remember only six values
and make sure that they come out in the
right order but if something is more
late than that where I'm only going to
remember you know 50 milliseconds of
values as inputs to the algorithm you
set these conditions you know so that
you don't end up just queuing forever
and never knowing when you can actually
emit you would have to basically impose
some sort of delay if you want them to
come out in order and then I don't know
what other things you were thinking
about as far as how you would want to
deal with late values because merge
would just deal with them as a rival
naturally
yes so the question is do I have any
sort of branching and you know does this
apply to you know in cases where the dag
is just extremely huge this is really
designed to coordinate events coming in
over time I don't know if the dag you're
thinking of is actually distributed in
time or if it's actually a fixed entity
I see so you would start with a fairly
synchronous amount of data and then a
sink and it would come in as things
completed so this isn't really trying to
update things in place it's trying to
deal with things arriving in time so
certainly as you were processing
something if you were reducing a whole
bunch of things and had a whole bunch of
reduced results that were arriving and
you wanted to coordinate those this is
where this would kick in it's really
more about coordinating the things once
they become a sync rather than
processing them while they're still sync
or static but we do have there's
multicast you can publish a source and
then you can observe that source as many
times as you want and they will all get
all of the observers will get the same
set of data and then you can apply
filters to each one so that each of them
actually only see is a subset of the
data or you can use the group by
operator and based on a key that you
supply a function to calculate it will
create a new stream for every new key
that it observes and then each of those
will essentially be filtered to only
receiving values that match that key and
then yeah lots and lots of algorithms
for switching there's also switch on
next so if you have a set of streams
where the streams themselves are being
produced over time then you can switch
to only see values from the latest
stream or you can merge them or you can
concatenate them so there turns out to
be a whole lot of different algorithms
that apply somewhat uniquely in this
space you know they don't really have
analogs because of the time domain in
the RX cpp I'm sorry in the range v3
library or STI or yes that would be
spectacular and yes do I lazily
construct the graph or do I do it as the
operators are invoked so if I did it
lazily then I could reorder things and
optimize things even if the user wasn't
making good choices definitely true that
would be wonderful this is not an
expression library in that sense it
doesn't record everything and then
reduce it into the final result that
that probably would be a very useful
expression of this library like a very
useful library to have this one very
much just constructs each operator where
you told it to be and pipes them
together
yes yeah so have I seen compile time
issues with the pipe operator I have
seen compile time issues with this
library with all these libraries that
I've tried they definitely do strain the
compiler because of all the types that
are being generated and chained together
into longer types like I said I was for
performance reasons I was trying to keep
virtual functions out and so everything
is actually just becoming this huge
single type over time that the compiler
can just smash down at the end but it
generated so many types to get there
that it ends up being a bit stressful on
the compiler I've added in I think most
of the libraries the virtual functions
can be introduced at any point just it's
another operator and you just pick the
points where you want to kind of cut
down the compiler work and you're
willing to trade off the runtime cost
and you say ok make that interface make
that an interface yeah I guess I found
just as many problems with the dot
method which I really wanted to use
because of intellisense oh that's sad I
wanted to use the dot method so much and
I did so much work to make that work
across all four compilers in v2 with all
of the recursion problems because the
observable type has to exist before the
algorithms are implemented and it also
has to contain all the algorithms that
are implemented as methods and other
than that I don't know how else you
would the other way is the transducer
way which is a function composition
where you end up looking like Lisp it's
tons and tons and tons of parentheses
any other questions well five minutes
back for everyone thank you very much
I really appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>