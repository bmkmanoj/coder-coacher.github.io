<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Tobias Fuchs “Multidimensional Index Sets for Data Locality in HPC Applications” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Tobias Fuchs “Multidimensional Index Sets for Data Locality in HPC Applications” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Tobias Fuchs “Multidimensional Index Sets for Data Locality in HPC Applications”</b></h2><h5 class="post__date">2017-11-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1HqY9dPccMI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Okay.
Thank you very much for having me.
It's my first time at CppCon.
I'm having a really great time.
I hope my talk will be
worth your while as well.
So I'm Toby Fuchs.
I'm from Munich, Germany from
a small research team called
the MNM team, which is for
Munich Network Management.
But actually we assist
the programmers too.
if it has a cable sticking out or a cord,
then the chances are that someone's
on a chair programming it.
So, today I'm
specifically talking about supercomputers.
So this is from the press
release picture by IBM.
I like it a lot.
This is a photograph when
computing was a gentleman's sport.
Actually, it was a ladies sport
because operators actually were women.
Those were the fine days and nowadays,
they look like this.
So it's not a gentleman's sport anymore.
Actually,
the reason why it is not
is because we're are in the post-core era.
This is no news to you.
So that is to say in the
good old days we could
just wait for single cores
to get more efficient.
Then we had to add more cores
and we had to add sockets,
then we had to add PCI Express to connect
and then we had to get
really, really weird.
But nowadays,
specifically when it's about data access,
so accessing data from one core and any
other memory domain in this in topology,
it's pretty much like
moving IKEA furniture with
4,096 of your best friends.
So why is that?
So why am I picking the IKEA example?
Well, IKEA furniture is
made to be assembled but
not exactly to be disassembled
and reassembled again
and this is also true for
some of our data structures.
So,
for here you will find
the corresponding real
world situation in these
drawings that we have.
Narrow hallways, that is to say well,
you can move one couch
from one room to another.
But if it's like four ward rooms,
you might want to schedule that a bit.
Then you have areas
where you don't want to
say drag your bed through
because there is stuff
you could invalidate like mom's china.
The really far bulky interconnects are
pretty expensive in terms
of latency and throughput
and sometimes have,
say specialists that are
really, really good at exactly
one job but not exactly
easy to communicate with.
So for a real world example,
when it's about decomposing
and recomposing,
it should start playing but it doesn't.
Hello?
There we go.
Yes.
It kind of doesn't want
to but all you in the room
know what a multigrid
simulation looks like,
a multigrid decomposition looks like
and all of you know of
course what a stencil is.
So in this example a
five-point stencil where
for every cell, so every
cell here in the darker
yellow shape has a specific
weight and its neighbor cells
to the north, south, west and east
also have weights in every iteration.
You go through all this data,
apply your stencils of
center cube times 0.6
plus 0.1 north, 0.1 south and so on.
But the problem is of course when you
get into the neighbor regions because
then as you all know,
you have remote access
suddenly because you need the neighbor
field that is located at another process.
And what we do is,
well you copy the whole
range into one chunk.
So we pack it into one
contiguous chunk of memory,
we transport it to the
process that needs this
neighbor cell and from
then on we don't have
remote access and then we
access this neighbor region
because we have it in
the process log memory.
So the interesting part about this,
and this is where we get to
the actual topic of this talk,
is the index spaces we have.
So we already have definitely
two index spaces for each
process, then we have another
one for for the halo cell
and this is just one of those.
So for a multigrid
simulation you would have
an arbitrary amount of
decompositions of ranges
and calculating those
indices by hand is really
cumbersome and also quite error prone.
So,
we can pretty much put the
optimization techniques
we have for data access
into three main categories.
One is to reduce the
number of remote accesses,
that is as we just saw,
the packing of strided data
into contiguous chunks.
We can improve the
surface to volume ratio.
So,
to reduce
the border,
the boundary of the inner blocks
and of course we can reduce
the access distance which is,
well generally from improving locality.
So putting the executing code to
the data it needs to execute
and we have locality on pretty much
every level of our machine hierarchy.
We all know it from from cache
optimization but it's also
true for remote access
across an MPI interconnect for example.
So,
we have a library called DASH which is
a pure C++ implementation of
a partitioned global address space scheme.
That is to say we take
say for example an array,
this 40 is just symbolic.
Just assume that it's a
really, really big number.
So it's a huge array
that couldn't possibly
fit into the memory of
a single compute node
and we distribute it to
a thing called units.
So a unit it could be, so
when you're an MPI user,
then the unit corresponds
to an MPI process.
So a specific unit is
pinned to a specific process
but it can occupy a larger
amount of machine hierarchy,
like for example in this case
we have one unit per socket
and one unit per, those
are mics, so CN five.
And we can distribute an array like that
and the interesting thing is while it's,
if any know PS that's not news to you.
This is a global address space so there's,
every element has a global pointer
in our case addressable by its segment ID,
unit and local offset so every array
instance would have its own segment.
Within this segment every unit
has its partition and local offset.
So it can have,
any unit can access, also any process
can access any element in this array.
You can do it with the
regular subscript operators.
You'd have an implicit
local or remote access.
So in this case unit 2 would access unit
indices 27 and 35.
One would be local implicitly
and the other one would be remote.
This isn't too efficient
because you always
have this indirection
of index calculations.
So it works better to have
an explicit local access,
so that's what we do.
You can have the local equalifier
and this would reduce the
index or iteration space
to the local part and
memory of this process.
As you see to the top left, there is
the second argument to the
constructor of the array
is in this case dash::BLOCKED.
That is pretty much to
partition the scheme.
So BLOCKED,
this vocabulary is ancient
and BLOCKED pretty much means
you have an even partitioning
of memory space into the number of units
and this is all fine and dandy.
But we are in C++ so what would,
so we have a dash array
and it works just nicely
as it is but how would a
dash for each look like?
So it has that for each
equivalent on a global array.
While in this case we
have for example for each
from array begin to end,
so over the whole array
and the implementation is
actually trivial as every
unit executes the std for
each on its local range.
Now, is it actually that trivial?
Because we are already violating
STL semantics here because
the standard for each
requires that the unary
operator is executed
in sequence, so one after
another in every element
as opposed to transform.
So when you cover for
example std: transform,
then there is no guarantee
of order on which
element index the operation
will be performed.
So we already deviate from that.
So that dash for each guarantees
a sequential execution in
local memory space, yes?
- [Man] So in that example
where you used dash::for_each,
is that distributed across all--
- Exactly, it is.
- [Man] And on the right,
that's not distributed?
- It is, that would be the
implementation, how you would,
so an overly simplified implementation
of a dash::for_each would look like this.
So a dash::for_each translates to
a std for each on local memory.
- [Man] I see.
- This is classic SIMD, Single
Instruction Multiple Data.
So every unit executes
exactly the same code.
So this seems pretty trivial.
There is no index calculation
indirection involved whatsoever.
But what if, oh yeah, I forgot about that.
So we have, for the
regular std dash for each
we have one barrier at the end,
so it would only return when
it has completed on every unit.
If you do not want that,
we have essentially no way.
Sp we try to stick to
the STL equivalents here
but this is still a work
in progress but eliminating
the last barriers would
be not a problem here.
There is, one way to do
it is execution policies
or any other kind of policy
just to tell the algorithm
that there shouldn't be a barrier to them,
so you don't have any
communication at all.
So the unit operator is
applied on every local
element and there is
no need to communicate
between processes apart from the barrier.
So it gets a bit tricky when it's not
on the whole array but only on
the sub-range of the global array.
So in this example, units 1 and 0 and 3
wouldn't be affected or involved in this
dash::for_each code at
all and unit 1 and 2,
only for a specific sub-range.
So the problem is that we have two
index frames, so to say.
So if there is a goal we
want, it is from in this case
zero to 40 exclusive and
a local one and it's not
quite clear what it
looks like in this case.
So from just looking
at it you can tell that
the affected local index range of unit 1
would be two to 10 and for unit 2,
it would be zero to eight
and the way we solve this,
to calculate this in a more general way,
we use view expressions
that look like this,
so to the bottom right.
So this case, you know perhaps
the syntax from Eric Niebler's Range-v3.
I can recommend it dearly.
It's pretty much the starting point
of our work in this case.
So we take the array,
the types of operator is, has the same
semantic as you would know
from your public cell.
You would take a sub-range
of the global array
from 12 to 28, so this is
in the global index space
and from this we would
like to have the local part
and well, I already mentioned Range-v3.
So yes, we do not use Range-v3,
we also do not extend it but
not because we do not like it
but just because we have
some specific requirements
and I'm not too sure they
can be expressed in v3 eye.
I would like it to do so.
So if you do not yet
know Range-v3 already,
this is pretty much
where we got our syntax
from and the semantics of our operations.
So I can recommend it very much
and it's also a really good talk,
I think the keynote is even on there.
So,
our domain specific needs.
So we have a mental model of
mapping domains or mapping sets.
That is unsurprising.
We have a domain and the mapping
transports it to an image.
Some elements are mapped and some are not.
This is implicitly already
included in Range-v3 but we have,
we need to extend on that quite a bit.
So to just visualize what is happening
in this array sub-local,
we have the global array.
We call the global sub-range on it,
so from in the global
index space from 12 to 28.
From this we take the local
part, so this is for unit 2
and you can take the
index space from that.
This isn't,
well with this you could just take it as
an enumeration but it's
a bit more like that
because you can transport
it back to the global index
space and then you'll get
the global indices from that.
You don't have to, so there's
no no need to query or
interact with those images
but there are situations
where we really have to.
For example, let's take
the basic std copy,
so we already have the dash::for_each,
now it's called dash::copy.
So we copy the sub-range of
one array to the sub-range
of another array and for the
sake of simplicity we assume
that both arrays target, their destination
and source are both partitioned
and aligned in the same way.
So this makes things easier
but still not trivial.
So in this case, we have a sub-range that
is in array A that's located at unit zero
and one and then array B at one and two.
And of course we do not
want to copy those elements,
both in this range and otherwise.
And this would actually work,
you can call a standard
for each on the dash array
or a standard copy on a dash
array and it would work.
The global point semantics
are absolutely compatible
with STL as well as
all our containers are.
But of course the STL has no,
no model of actual data distribution
because that's the
whole idea of it, right?
So if we tried to reiterate this,
then they should do all of that.
So we want to copy in contiguous
chunks as we saw before
and in this case it means
the first sub-range,
there has to be an
intersection of course from
the arrayable index range
on the source and target
units so you don't copy
the whole local range,
you have to repartition it
to the destination range
and we do that for all of those ranges.
This already is really, really,
I would say the opposite of
fun when you have to calculate
or implement dash::copy in a generic way
so it always has the optimal
distribution and communication strategy
for any kind of distribution in an array
and this is really simple
because most of you know,
you can also block, distribute
the elements block cyclic
and you could use an
irregular distribution scheme.
So it's really, really cumbersome just to
get all the destination
and source indices right.
So what we can do with views
to help that is in this case,
we take the sub-range, we take the local
part of that and we
take the blocks of that.
So a block is in this case,
a block is a contiguous chunk of memory.
But it's just,
in this example every unit
has one block in this array.
It could be several and
you also have this in
the destination range and
then it's way easier to
just have an algebra of those ranges
because building an intersection
is rather trivial when,
as soon as you have
this kind of interface.
So,
the important part to take from
this is well we actually do not,
it's not about accessing
every element in the range.
It is important to have
this right and performant
but
the actual most common
use case is that you
just take those sub-ranges
of contiguous chunks
and you're mostly, most of the time you're
only interested in the
first and last index
in local or global index
space of this range.
This is one difference to the
existing range implementation.
Say, the key feature of our whole approach
is the intermappings between
the single V operations.
So you can always go back
and forth between them.
As you saw before, you
can project an index
to a local range, to a
global range and back again
and so forth and you
never lose this state.
So,
as a motivating example,
to the right you'll see
the original implementation
of one of several
specializations of the
algorithm local sub-range.
So that's what we saw before.
Take a global sub-range and
from this the local part
and this is one implementation
we had before we had use
and now it looks like this.
And this is of course, in
general this works for anything.
So in formal terms, I'll try to put as
few mouthy stuff in this talk as possible,
so in formal terms it
works for any n-dimensional
rectangle and this includes of course
one-dimensional rectangles
which are ranges.
And there is,
I can actually demonstrate
this real quick.
Here you go.
And now I must check for...
Okay, so we have an array here
and in the whole talk
for all demonstrations,
the interpretation of the numbers
on the screen are always the same.
The colors indicate the unit.
Coloring is simple, the first of
the number before the
decimal point is the unit ID
and after the decimal point
there is the local index.
So we have for example 0.0, 0.1,
et cetera and you see this is not block,
this is block cyclic.
So the array is distributed
in a fashion that
there is two units, two
elements assigned to a unit
and then to the next like
distributing a deck of cards.
So we take the global sub-range of that,
we show the blocks of
this local sub-range,
so those are the contiguous
chunks in every space.
Then we take a look apart from that
and this looks different
for, so this is for unit 0,
1 and 2 and then we take
a look at blocks from that
and as you see,
when I have a global sub-range
and take blocks from that,
this restriction in the global sub-range
also applies to the local one.
So the first blocks,
in the last line we have a complete block
from local index 2 to 3
and the first is incomplete
because the local index 0 is missing.
It tells me we sub-spaced
that in the global
and then we can copy that.
So in this case I copy to
the global index range 229,
which is the target index range to,
including MI8, that's unsurprising.
Those are the affected
blocks in the target range
and this is the outcome.
So in the last three
lines you see the result
of the modified array
at every unit and before
you see the affected blocks
and elements within them.
This is pretty much the example
on the slides before,
just a bit more elaborate
and for a more complicated configuration.
And...
How do I go back?
There we go.
Well so this is nice,
this is a nice gadget.
But it's not nearly enough to
convince users to actually use it.
We have really, really, let's say
traditional users that are
really hard to convince
to switch away from Fortran.
I know that there are Fortran programmers
in the room, I can totally
see its advantages.
It's really hard to argue
with Fortran programmers when
it's about modern C++ because
they usually don't know it.
And so the only and best way
reasonably to convince
them is proving benchmarks.
So let's just run the code on the right
against the code on the left.
And
so this is not a lab environment so
I don't expect absolutely
clean, reproducible results.
Let's see what we get.
So this is giga-updates per second,
a measure you probably
know pretty well.
That is to say higher is better,
so UGUPs is the giga-updates
per seconds on a random,
so we pick a sub-range
of the array at random
but we make sure that
they are the same for
both variants for the algorithm variant,
the lengthy one to the
right and for the U-variant,
the short one to the left and
we see we get drastic speedup.
So, it went even better a bit.
We have one run
in our lab and...
Well I'll just minimize it I guess.
Ah, here we go.
And it looks like,
yeah,
it looks like this.
So,
this is with GCC 5.4 and
it's a speedup of 6.2.
So,
for those in the HPC domain,
this is probably not
impressive enough because
well speedup, it's always a comparison
of an absolute performance of one against
an absolute performance of the other.
So perhaps the original
implementation was just really bad.
Now I should want,
so we had ran the previous the algorithm
variant against the
reference implementations,
UPC++, et cetera and we
were pretty much on par,
so it's a reasonable comparison.
So where did we get this six
point-something speedup from?
For the same reason that Blaze
high performance math
library is high performance
and for the same reason
that rate of use and ranges
as proposed by Eric Niebler are performing
because we let the compiler do that.
That is to say when you compose,
so when pipe those view operators
in a chain, what you actually do is you
configure the nearest metric expression,
most of it at compile time.
So the whole thing is
evaluated at compile time,
then what the other compiler sees is
the arithmetic expression within every
single component of the new chain.
Like or example SAP, SAP,
one would mean that every
index put into this range
would be incremented by one
and if you put this in a chain
with what the compiler sees,
it is an arithmetic expression
that can even be constant for that.
So it's lots of constant expressions
and expression templates
and every trick in the book.
But actually,
this wasn't actually the goal overall.
My goal was to have
a portable, convenient
way to express those
domain mappings and
what I got was a drastic
speedup just because I generalized it in
a way that can benefit my compiler.
Now this is one-dimensional.
So it's useful but it's
the two-dimensional case
or higher dimensional case
that's more important.
So,
first this is an overview
of the typical view
operations we have, so some
of those you already saw.
Of course you have begin
and end, size, rank.
Offsets and extents is unsurprisingly
an n-dimensional standard
array of the offsets,
of the regions so to say
that you have in a view.
Domain and origins or domain would map,
it's the view back to its domain,
so back where it came
from and origin is well,
when you have an array
sub, sub, sub-local,
then the origin is the array.
So origin is just the,
where the data actually is coming from
and the actual owner of the data.
Views don't own anything,
they're just, all they
own is the parameters you
passed in the construction.
Alas, one thing that is
not evident is this pattern,
what is the pattern?
Well, the pattern is the
static distribution of data.
So,
when you distribute dash
metrics for example,
you configure the domain decomposition as
a static mapping, that is a
mapping that is configured with
construction and never ever
changes and it has three stages.
So first, you have the
global canonical domain.
Then it's partitioned into blocks,
those blocks are mapped to processors.
So unit one will get this block
and unit two will get this block
and once you have this assignment,
you get the local memory layout.
So in this case unit
1, unit 0, the red one,
it has,
the first two blocks are 0, 1, 8, 9,
6, 7, 13, 15, et cetera.
This is called a tiled distribution.
There's also block distribution where
the index would just go
in the canonical domain.
And what we do is the
pattern concept in Dash
which will be refined a
bit in the next month,
it has
categories, so properties
in exactly those categories,
like you would know from
iterator categories.
So you could at compile
time interrogate a pattern
if it's actually possible to
have an imbalanced pattern,
an imbalanced distribution.
So for example, when we
just take the mapping
between all of those possible index sets,
because of block index, of
course it's an index domain.
You have a local index domain,
you have a partitioning index domain
and this is what a pattern
does but it's static.
It will always exactly do
this mapping for exactly
those extents of these case metrics.
When you just take some properties
in the mapping category,
this is not, this is just for
motivation but it gets really nasty.
So those are already
existing definitions of
partitioning properties when we just
implemented them in Dash
and named them according
to the original paper
that have been proposed.
So with the neighbor partitioning,
which for example say that there is no
adjacent block with the same owner.
Balanced would include that every unit
has exactly the same number of blocks
and diagonal would use the
typical diagonal method.
So of course we have several
pattern types and all
of those are implementing
different semantics.
But it's
absolutely,
it would be overwhelming for users
just to learn our API and setup,
also we add a lot of new features
and new partitioning traits.
so what we provide is
just a helper, a compile
time called make_pattern.
You pass it the properties of
the distribution you would like to have.
Those are easy to communicate,
they aren't too much.
It fits on a nice slide of paper and you
can just learn them and understand them.
You put all the properties in you want,
every property you don't
specify is a degree of freedom
and which is
one of the possible pattern types.
What we also do is we
deliver a visualization tool.
So it's a command-line tool,
you just type the domain
decompositions you have
or you intend to use and it would render
an SEG and you could just see what it does
and this is what it would look like.
Now back to iteration spaces.
There is, well we first saw
that within a single block
you pretty much jump
between rows and columns.
But why don't we just use the
regular canonical iteration?
So this, we already do that,
so when you just use array begin, end
it would exactly iterate in this fashion.
So you could use standard
copy on DashMetrics
even if it's multi-dimensional.
This pretty much all exists
in your head, you can
still traverse all the elements
in their canonical way.
Once you take the actual
distribution into account,
things get a bit more nasty.
So we have actually two iterations orders,
the global one, the canonical
one that is also visible
to use, for example
the standard algorithms
and the local ones.
So this would be on the left
the canonical order that you
get when you use standard for
each, for example on metrics
and you have the local
iteration order on every unit.
So why don't we just throw away the
canonical iteration order?
Because well, who would actually
use a set algorithm on it?
And why don't we just only
propagate the local ones so
we always have a duration order
the same as storage order?
Well that would of course
work but it's not part
of it all because when you
change the distribution,
you would also have to change
your whole algorithm because
it's in our heads a totally
different iteration space.
So,
let's say we have a sub-range,
in this case, rows in the matrix
and the applied solid copy on it.
We would expect it to work and it would.
Now this is exactly what you can do.
What about now?
So you still use then the copy
and you change the
distribution underneath.
It would also,
disregarding the actual
distribution scheme,
it would always have to
return the same result.
It would always look like this
and this is exactly what you can do.
So you can use standard
copy, it would take forever
because it is elementarized copying
and you can use DASH copy
which would also have
exactly the same results
but exploit the interaction
with the pattern types and views
to employ an optimized
blocking communication
scheme instead of non-optimized copying.
So this is what you get every time.
The canonical semantics
are always the same.
Underneath we have,
the algorithms have to
adapt to the local storage.
So this is where the mappings between
those index sets have come into play.
Because that's exactly what algorithms
have to do all the time in our case.
They always have to map the
pre-existing local
iteration order and map it
to a canonical index
set and it would return
exactly the same result in every case.
So all of those would always get
the same result and always be optimal.
Okay.
Now we have sub-ranges in a
one-dimensional distribution.
How about a two-dimensional one?
We already see that index
set gets a bit messy.
So sub, you can also really slice,
sub is just more generic,
sub is for subset or sub-range,
slice wouldn't work for a subset so
that's why we use sub,
it's also a bit shorter.
So the template parameters, the dimension,
the first dimension
is rows.
So we take our rows 2 to 4 exclusive
and columns 2 to 7 and this
is the rectangle you get
and when we translate this
to the local index space,
it would look like this.
So sometimes it is a contiguous
range in local memory,
sometimes it is not.
It is not trivial
to write a formula by hand that would
resolve whether an index
range from an operation
like that would be contiguous
at a specific unit.
Because what we get,
the type of solution for
that without views and it
was similarly complex like
the one-dimensional example,
so it was pretty horrible and not
actual motivation for what we do.
So,
let me show you some matrix views.
So it's the same
kind of visualization that we had before.
Let's say,
matrix sub views, that should
work, that's our matrix.
I will just duplicate it to the top
so we have it always in view.
Oh I forgot to part it, sorry.
There we go.
So that's our matrix.
We take a sub-range of that,
so this is pretty much the
example you saw on the slide
and it starts
here and ends right there.
It's of course not highlighting it here.
Just a second.
Starts here and ends there.
So what's interesting about that,
the white number to the very left,
whoops, there we go.
There you go.
Oh, sorry.
The white number to the very
left is the global index,
so when you take the sub-range of that,
the global index is still preserved.
When you call index, the
pipe index on the sub-range,
you get a multi-dimensional
index set and it
would reflect the original global index
and you can take blocks
from the sub-ranges
and just use a dash as
contiguous dash as local add,
et cetera to determine whether
this specific block from
that sub-range is local to a
specific unit and contiguous.
So why do we need contiguous?
Well if it's not contiguous,
we would have to pad it.
For example, if we already
had exchanged it before.
So let's just
have this up here so you
can compare if it's right.
You see those numbers, so
we know we are at this one.
Still, everything is contiguous.
Oops, now it's sorted.
So 9 and 11, oh this one, where is it?
9 and 11.
Here we go.
This one is not strided
and not contiguous
and it's really hard to
find that out by hand.
So in this case, I can show you the code,
this is the sub-range,
sub-matrix view example.
Of course you can also view it on GitHub,
links will be in the slides later.
So this is what we do,
this is a fully specified
declaration of a matrix.
Users actually don't use
this overly verbose variant.
So I take a sub-matrix,
I take blocks from that and indices
and then I chose to ask the block
whether it is strided or not.
So it's locally at my ID,
that is to say it's a locked
block or it's strided.
If yes, I have to compact it.
If not, I can just
quickly use standard copy
to confer the whole range
and you get that for
every block in this thing.
In the beginning we had
the example of halos.
So, can it actually do halos?
Let's take this example, I take a block.
So I can specify a block
by its block index,
so the first block in every
column and row so this would be this one.
Then, oops, sorry.
Then I expand the block,
expand, it takes relative values.
So it could be renamed to
expand by or stretch by
because otherwise the API
would be a bit inconsistent.
So in the first step I expand this block,
the first dimension by minus one,
that is one row up and
the second dimension
to the left and to the
right expanded by one.
Then I shifted it to the right,
then I take the intersecting blocks off.
So this is an actual example
of really neat application code
and there was a grad student,
a really talented one,
I can show you the chat
transcript after the talk,
you will see why he was lucky
to have this pull request.
So let's see what we can do with halos.
I think I used four, not too sure.
This example also takes parameters,
so command-line parameters
to play around with it.
So this one is called halo views.
Yeah this works, okay.
So, this is our matrix.
Ah, forgot to pipe it again, sorry.
So we take, from this
we take the first block.
So this is offset and extents, offset 0-0,
the first block extents, 2-2.
We expand it by one in every dimension
but it didn't expand to top and left, why?
Because there is no index
space that ends there.
I don't know,
when you ever have to calculate index,
those halo cells by hand,
one of the most common
mistakes is to disregard
that you do not have say
a Torus distribution,
and this would lead to
large noise in fact.
But in this case you
can't do anything wrong.
You just, yep?
- [Man] Have you supported Torus?
- Yes, in this branch not yet.
But it, I used to support it (chuckle)
but and it will be supported again
to avoid the nasty word Bach,
but yes of course we do
and also irregular distributions and
there's a long chapter
on sparse distributions.
Okay, we do like that for
every block and now we shift it
and of course we get, so
we check to the left and it
shows the original matrix
to the top, on top.
So we take,
I will not demonstrate every block but
you will soon see that it works.
Well that's the first
block, it's this one.
We expand it, so we get this range
and now we shift it to the right
and we get this range here.
And as you'll see it, now,
yeah, it still preserves
the global index range.
You still get 1, 2, 3,
7, 8, 9 as it were.
So should you ever need
it, because in the code,
I never used the index set at all, I just
use index set algebra
at work and you can just
ask them for the index set
they just to happened to use.
But you don't, as a user you
usually don't have to use,
to actually read those as
a user, they're just there.
And the next one.
As you can see it works pretty nicely.
And now we have some real ones,
some internal blocks now,
this is an internal block
and as you can see,
this is a pretty complex
view chain, so it's
matrix block 5, expand,
input both dimensions, shift
it, take the first block.
And I don't have the
assembly right here but
I can show you after the
talk it is really compact.
So there is not,
there's not much copying going on,
it's pretty much all moved
away and const-expressed away.
So as you can see,
combined with what I showed you earlier
so that you could actually ask the blocks,
like block is strided, block is local,
block local et cetera,
this makes halo exchange really trivial.
So now the simple task,
so it is simple to draw on
paper, this halo exchange.
It actually,
it's just as simple in code.
I will just show you the
implementation of this halo example
and that will conclude my talk.
Here you go.
Oh yeah, just one disclaimer.
All I do runs on
this branch feat to the very bottom right,
feat 192 in use, it is not
merged into development yet.
I will try to get to it on the weekend.
So here's the code I just demonstrated.
It's pretty much what I,
what I explained before.
So here is the expand.
Well we have the block first
somewhere, where's the block?
Matrix blocks, here we go.
So you can use this style,
you can use dash blocks, matrix
or the pipe style, which is
matrix | dash::blocks
and if you know that you
need a specific block,
you can also say block, ah come on,
0
or block, I don't know,
2.3.
so when you pass it in an array,
then it's considered its coordinates.
In this case we want all of them
and this is a general rule in the API,
when there is an operation that is in
plural like blocks or
chunks, strides if there
also is a singular variant
that just takes the index.
Okay.
What's it complaining? Oh, sorry.
Okay.
So we take the blocks,
from the blocks I want the index
because that's what I print here.
There, I have the block index somewhere.
Yeah here, to the,
come on, to the very left.
I can't see my mouse pointer, here we go.
So this block is, corresponds to block six
in the global index range.
That's where I'm taking the index from.
I want the offsets,
that's where I print them.
I expand it in every dimension.
I want the offsets and
extents of the header block,
shift it
and take the first block off there.
So it's only looking a
bit lengthy because of all
these print statements and
step statements in between
but actually you could just
write it in one line of course.
There we go.
Okay.
Yes, and that concludes my talk
and I hope I get a lot of nasty,
constructive questions, yes?
- [Woman] Do you have a Z-order partition?
- Oh yes we do, of course yeah.
Space-filling gaps, lots
of space filling first.
Those are not in the official development
branch because we have some grad students
working on them on their Masters thesis.
But yes we do and speaking of Nvidia,
we also have a CUDA back-end for that.
- [Woman] Yeah, this is
me asking as somebody
who used to be conventionally involved.
- Yeah, there is a reason why I put
at the very beginning, I called them
the Finno-Ugrian Esperanto speakers,
those are pretty much the GPUs and we
have a back-end, so we have DART.
DART is for DASH Runtime,
it's written in plain C
and it's pretty much just to
decouple from the communication back-end.
There is one in CUDA,
but I didn't,
I didn't
test the view expressions
integrations for CUDA.
So that's,
but yeah, it's on my to-do.
- [Audience Member] What
communication back-end is
DASH going off of?
- Actually, so we have,
the canonical one is MPI,
we use one-sided MPI.
But of course that's good reason
to also support
GASNET and we have a nearly
complete implementation in GASPI.
So those, GASPI, this is for Global
Address Space Programming
Interface and GASNET
is for Global Address Space Network,
which is to say those are
communication back-ends that
are specifically designed
for this one-sided access.
so RDMAs, so to say.
MPI has RDMA's since
3, since MPI-3, right?
And it had its issues so I,
well not according to the standard.
MPI implementations have
issues with progress
communication but it's
not in the standard so
in this respect, the standard
doesn't have the problem.
- [Audience Member] That sort
of leads to my next question.
So you showed one basic CPI which was
in my estimation somewhat
based on variables.
Do have more of a handle-based CPI?
- Of course, we have, every natural
algorithm also has an
async variant and it uses,
on the C++ side, it's
a DASH future and below
it's an MPI handle arc as
whatever you already have.
- [Audience Member] So then, does DASH
spin on its own threads for progress?
- We have several approaches for that
and they can be configured at compile time
in several places, at one time in several
places because it is
just not that trivial.
But that's not exactly our fault,
it's a problem everyone
has that uses MPI-3.
Progress just is a problem.
We have one,
we have an old paper,
it's two years old now where we
employ the Ember Pitch variant.
That is to say you have one
watchdog thread and, yeah.
And what we achieved is we made it more
efficient than the
official Ember Pitch flag,
at least on the machines we tested.
Not because we are
geniuses but just because
the official implementations
of that were sub-optimal.
- [Woman] How does your
performance compare to other tools?
- I didn't, I have to say that
I did not pay you to ask this question.
We are not ashamed.
That is to say we are
measurably better.
So measurably doesn't actually
mean significantly and we
also of course only tested on
I think four supercomputers
and any pull request in the
UPC++ or DASH could change that.
But I would say we are on par
with some backups.
And there is of course papers for that.
so on the last slide,
depending on what you're
most interested in,
there is my NASCAR jacket
and also some links.
So of course, there's dash-project.org,
the official URL of the DFG
research project.
More interesting for you,
github.com/dash-project,
this is where the implementation of DASH
and everything I showed you right now is
and also the heat equation that just
refused to play in the beginning.
And of course you have Slack channel
and in the dash wiki
which is doc.dash-project
we have papers, slides,
wallpapers and yeah,
whatever you can find
with the search function.
But in the papers section there is
also the paper I mentioned.
There is a dated paper,
so it's from last year, on the DASH matrix
and I would like to
say,
just to pin the recent advancements
in views to this application because
the API that we proposed there
does not use free functions.
So you have matrix dot local dot sub
as opposed to SAP off matrix off et cetera
and there is a nice talk
that is taking place
exactly at this moment by Klaus Iglberger
which is called Free Your Functions
and I wholeheartedly
support his statements.
Yep?
- [Man] Are there any
applications you'd like
to highlight that are
using DASH in production?
- Applications that
use DASH in production.
Well it depends on what
you mean by production.
We do not have a reference.
- At scale.
- Large-scale replication,
we have lots of those.
We have of course the
standard reference benchmarks
like U-Dash and SPD, et cetera and we run
those on an whole island of Supermock
and Hazelhand and Cory and whatnot.
So it's well tested to behave reasonably
on a large scale but
we don't have reference
implementation like for example COPOS had.
- [Man] Well I mean, I guess I mean like
applications that are using this that
didn't bring it in as part
of DASH development, right?
I mean do you have, like actual users?
- Oh, like the one in the beginning?
- [Man] Independent of this project.
- Yeah, like the heat dissipation
example in the beginning.
Well, we used to have a corporation but
unfortunately their funding ended.
We have a specific
part in the DFT research team that is
only occupied with replication development
and they are doing molecular dynamics.
But they are affiliated
with the DASH project,
so what you are asking for is pretty much
the freelance.
- Can freelance DASH experts
bring this into their application?
I guess I'm looking for--
- Oh, there is a user base that is
really active also on Slack.
You should join just right, just now.
There is an active user base and also
native support, so that's not the problem
but we don't have any
reference applications like
Eckhart Copples has, for example.
Yes?
- [Audience Member] So if you
rotate a rectangle and like
move it around and it has
like its local indexes,
what I'm interested in is like
if you have the half of the matrix
that is your interest,
like I want to cut it and
only have the top region.
So then I want to
integrate this top region.
- So the question is
what if you do not want
a rectangular sub region of the
matrix but a triangular one?
- [Audience Member] Yeah.
- This is actually easy, once you have
the operations that are already there,
there is no triangular, there's no
built-in operation for
triangular sub regions but it's
pretty easy to implement
because you just need
the sub-expression, then
just subtract one dimension
in every step until you reach zero.
So no, there is no built-in expression for
triangular sub-sections but
rather easy to implement.
Like actually I can show you outside.
Yeah?
- [Man] Have you done any
work with sparse data?
- Yes we do, but it's not there yet.
The problem is, so for sparse data,
we use Cell C Sigma, which is pretty much
a generalization of sparse memory storage.
- What was that called?
- Cell C Sigma.
So it's Cell, like the cells,
C and then Sigma like the Greek letter.
I know, I didn't pick the name.
Speaking of naming,
I'm really sorry for the
temblor case containers.
Not my choice.
The reasoning was well,
there are so many people
these days that would use,
say using namespace dash, using
namespace dut or something
when you have an array or vector because
you stick to STL semantics,
or you wouldn't, you wouldn't
even compile it with Run and
you would use a dash vector
instead of a std vector
and wouldn't even notice.
And so we use temblor
casing for containers
but apparently this isn't
a problem for algorithms
because those are lower case.
So I didn't pick it, you
can download the header
that just contains typed aliases,
that's one I actually use and, yeah.
- [Man] So can you spell that
again, you said Cell C Sigma?
- Cell C Sigma exactly.
I can write it down and, or
I can just give you a paper.
Or I can just hack it into the console.
So it's Cell
C and then
Sigma, like the Greek letter Sigma.
I think it's from Harken Vella.
- [Man] I've heard that
because it seems like
the way some guys do
indexing division as well.
- Of course, yes of course.
That's also--
- [Man] It's physical and then
they go in and do it by the--
- I mentioned it briefly,
under my breath when I explained
the pattern concept,
which is
here.
Here we go.
And you see to the right there is like
a diagonal imbalance and the only reason
you need diagonal or a typical
reason you need diagonal
is because you have sparse
data and for that you
wouldn't use a regular
decomposition but an irregular one.
So you have a K-diagonal and the more,
the higher the K, the
bigger the region gets.
- [Man] A distorted diagonal K.
- That's one way to do it, yes.
So that is on paper and that
is from the Cell C Sigma office,
and that's our approach
but views don't support it just yet.
So as you see, the API still
isn't perfectly stable.
We have things like expand
that expect a relative offset
that should actually be named
expand by and stuff like that
but we are getting there and
we have a meeting in two weeks
where the whole DASH team comes together
and fixes the API.
Okay.
No more questions, then
thank you very much.
(applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>