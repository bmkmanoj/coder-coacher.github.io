<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Rich Geldreich &amp; Stephanie Hurlburt  “The Future of Texture Compression” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Rich Geldreich &amp; Stephanie Hurlburt  “The Future of Texture Compression” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Rich Geldreich &amp; Stephanie Hurlburt  “The Future of Texture Compression”</b></h2><h5 class="post__date">2017-11-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q6tC7IVUuEc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Okay, so we will talk to you all
about texture compression.
It's what we do.
So, who are we?
We are co-founders of Binomial.
It's a small startup. We
used to work in games.
So I worked at Oculus building
virtual reality demos,
and Rich worked at Valve and Microsoft,
and before that I worked at Unity
helping them build their game engine.
We actually after working at
a bunch of game companies,
we founded a consulting company together.
We helped launch Intel's
Project Alloy Headset.
We did a lot of virtual
reality consulting.
But overall, we're kind
of optimization experts,
so we help people with
those kinds of problems.
And eventually, that
road led us into starting
a texture compression company (laughs)
because we found one of
the number one concerns
we always ran into was
textures are too big.
They are causing a lot of the bottlenecks,
especially in the virtual reality space.
And we did all of this in C++,
so that's why we're here.
So the main point of this
talk is to talk about Basis.
That is our texture compression product.
That's what we're working on.
And then, we'll go into
a little bit about C++,
could dive into some technical details,
especially for those unfamiliar with GPUs
or who want to know the
innards of GPU formats
and how all this works.
We'll show some benchmarks to show
that we're not making this up.
And then, we'll talk a little
bit about what the future is.
So in short, Basis makes images
smaller on your computer.
It makes them take up less
space, and that's the core of it.
People always tell me I
need like some sort of video
for our product, but it's
just making an image smaller.
So it's not the easiest thing
to describe to people.
- [Rich] It's hard to describe
in an elevator too.
But this is foundational.
Images are everywhere.
So it's a really important
piece of software
in a lot of applications.
And the most important part of Basis is
it actually is compressed on the GPU.
Believe it or not, it's
the only image format
that has similar size to JPEG
that is compressed on the GPU.
We work really hard at that,
and we think it's so important
that we're actually
donating this file format
to the Khronos Group and
making it an open standard.
So that anyone can target
it, anyone can transcode.
We're working with hardware vendors
to get it in hardware and browser support.
We're hoping that essentially
in five to ten years
this will be the image format you use
in a lot of applications.
So why is it important?
I went into that a little bit.
To give you an idea of
how much texture data
is out there, for instance, in games,
it's easily 70 to 80% of the applications.
So think about reducing that size.
We can cut a lot of games
texture data in half.
We'll take questions at the end, promise.
And for a lot of applications,
Basis is just a necessity.
It's just needed because they
can't run their application.
It's too big without
more texture compression.
So we're really excited about innovation
that will open up too.
So I figured talk a little bit about
how we came to this point.
Why is this only being solved now?
GPUs have been out a very long time.
Why didn't they fix this a long time ago?
So JPEG is actually really
good for compression.
JPEG is great in terms
of CPU side storage.
It does a good job, but
it's uncompressed on GPUs.
It was invented before GPUs came out,
and it didn't consider
a lot of those concerns.
So that's not good.
GPU manufacturers tried
to solve this problem.
They made their own GPU formats
with hardware acceleration
that worked great on GPUs,
but the problem is they
were pretty big on CPUs.
And it was a fragmented ecosystem
as I'm sure you all are
familiar with can happen.
They all released their own,
and it didn't work on
their competitor's GPU.
And it was a little bit of a mess.
So that actually wasn't a good solution.
So we like the size of JPEG.
We wanna maintain a small size CPU size.
We like its multiplatform nature.
A lot of web applications
still rely on it.
But we like the size of the GPU format.
So we have solved these problems.
We have a similar size to JPEG on the CPU.
It can be multiplatform,
and it transcodes quickly
to the GPU format you need.
So it utilizes existing
hardware acceleration.
So there's three parts.
There's the encoder.
If you only care about one GPU format,
which is true of a lot of games,
we can target just that one GPU format,
and you add lossless compression to it,
and it sticks right
into existing pipelines.
But if you are say a web application
and you need it to be multiplatform,
we have a multiplatform solution.
We created our own file format .basis.
You compress to that, and then at runtime,
you transcode to the format you need.
So next, C++.
My name's Rich.
I met Stephanie, oh wow, it must have been
almost two years ago.
I was at Unity at the
time, and she was at Unity.
So we got to talking a lot.
And the history behind this whole thing
is we had an open source
library in Crunch, or I did.
And I just open sourced
it while I was at Valve.
And I was busy on other things,
and I didn't have time to
really do much with it.
I just put it out on
GitHub and just let it go.
And my first big customer
was Google, Google Maps.
And I'm like, &quot;Wow, there's
something important here.&quot;
And then other developers
started using it.
More and more game
developers would contact me,
and a lot wouldn't contact me.
They would sneak the information to me
through backchannels
like, &quot;What's going on?&quot;
So that's why we chose Basis to work on
as our first product.
We were doing contracts last year,
and we really thought we want a product.
We need a product because
contract work is just linear,
like you only create income linearly.
But a product can result
in exponential income.
So this is what we've been working on.
And Crunch was written
in C++ and so is Basis.
I've been using C++ since '96, '95.
All the projects I've
worked on pretty much
in the last year and a half,
all of them have been C++.
They made a wide variety though,
mostly 03, to be honest.
Just a little sprinkling of
11, and a little bit of C++,
I think it's called Ox for Microsoft.
Anyway.
We don't use all features.
Like I come from a
game-development background,
and we're really concerned
about performance in games.
Like maybe paranoid about it.
We are paranoid. What am I saying?
And we don't trust a lot of stuff.
And plus, we want to
ship across a wide range
of platforms without worrying about
the standard library
implementation on each platform.
So just like most game developers,
I usually just re-invent
everything myself for the project,
specifically things like
containers and stuff like that.
I like to know exactly
what everything is doing.
So basically, I use the
compiler. I use OpenMP.
And we use our own containers.
There's a few things in
C++11 that are interesting,
but to be honest, like we're working in it
in a hard problem domain right
now in terms of performance.
And we're slowly transitioning to it.
So reason why we care about performance
is because Crunch was incredibly slow.
It would take a minute
to two minutes per image.
Clients would have to compress
thousands and thousands,
hundreds of thousands in some of them.
So they have to deploy it in a cloud.
And for a product, that just
kind of is unacceptable.
So we're down now to about 20 seconds
for a 2K texture on a laptop.
And that, thankfully, all the compilers
that we target have
great support for OpenMP.
So I'm really a huge fan of that now.
But we also like C++
because it gives us access.
It's basically a sweet spot.
I program in C all the time,
and that felt like a high-level assembly.
And C++ gives us this wide range
of this whole abstraction spectrum.
We can go all the way down to intrinsics,
or we can go all the way up
to these higher level primitives.
And that is incredibly powerful.
And every time I go to
some other language,
and I mess around with it
and play around with it,
I just miss that.
I like that abstraction hierarchy.
Anyway, we kinda covered this.
This is why care so
much about performance.
So basically, in coding, time is money.
So, if a client has to deploy
our encoder in a cloud,
like on Amazon EC2 or something
like that, it costs money.
And so for us, every ounce
of performance we can get
out of our encoder, results in
less money for our customers.
And on the other side,
on the decompression
or transcoding side, we
are competing against
some of the fastest,
lossless decompressors
in the world written by some
amazing programmers in C.
So for us to have a competitive product,
it has to be really fast.
And so what we generally do is,
our transcoder is a header file library.
This is something I first saw some people
at RAD Game Tools do up the road.
And header file libraries
are these neat little things
where it's just typically one header file
that contains all the code you need.
So all the application developer has to do
is include that one file and,
in theory, not worry about it.
And so, that's what we also try to do too.
We put everything in one header file
and just ship people that,
and they can cross-compile
that with Emscripten,
which is another incredible
piece of technology
that can take C++ code and
cross-compile it into JavaScript.
And I'm by no means an
expert on this tech,
but that's how Google
was able to deploy Crunch
in their infrastructure.
So I just wrote something in C++,
and then they just
deployed it in JavaScript
through some kind of
magic, and it was amazing.
Yeah, it is pretty amazing.
And I think our end goal
is to have browser support
and not have to force people
to do the transcoding step in JavaScript.
But believe it or not, JavaScript
is actually fast enough.
And that's because our
transcoder is pretty simple.
So for those unfamiliar, GPUs
are basically why we exist
(laughs) to make texture
compression on GPU.
So it's really important to our algorithms
and to our business.
They were created
because graphics requires
a lot of parallelization, a lot,
and really don't need a lot of memory
for a lot of like the
standard rendering going on.
So graphics processing units were created.
It has more limited memory, but it's okay
for a lot of applications.
And the really exciting part is now a lot
of interest has been spawned in compute
and general purpose GPU
programming, which is awesome.
So it's used in AI and machine learning
and all these different applications
that also like highly-parallel algorithms
that don't use a lot of memory.
It's great, but because
it's developing so fast
and so exciting, it's also not as stable.
So that's a big reason we are
partnering with GPU vendors
to try to get this
transcoder taken care of
by their drivers and their
hardware because they are on top
of the cutting-edge
developments and what GPUs need.
Let me just add one thing to that.
So the big problem on GPUs is bandwidth.
So you have a ton of parallel
threads all executing
these little kernels of code,
and they all wanna
fetch from texture data.
And if you have to store a
32-bit or a 16-bit texture data,
the amount of bandwidth
is incredibly high.
So the GPU vendors back in the late 90s
developed a whole bunch of
algorithms for dealing with this.
We're gonna cover this a little later,
but what it all boils down
to is block compression.
So the image is their textures
divided up into these little blocks,
and then they compress
each block separately.
And then, there's specialized
hardware on the chip
to decode those blocks
right from a texture cache.
So, in other words, like
the stuff we're working on
is inherently hardware decodable
because it's in every GPU around.
The problem is the formats
have not been standardized,
so there's a huge variety of formats
between mobile and desktop.
People have tried very
hard to do JPEG decoding
on the GPU, but it just
takes up too much memory
and it's not multithreaded enough.
It's not made for it.
It's inherently not
random accessible either.
And now, there's GPU hardware decoding,
which if something has hardware
decoding, you should use it.
So we focus on using block-based
compression algorithms
ourselves because at the end of the day,
we're targeting these GPU formats.
And that is the end goal.
So basically, we try to be
very smart in the encoder,
and .basis is a
block-based format as well,
but it's one that kind of
unifies all the GPU formats
and then it's really easy
and quick to transcode.
So our algorithms are block-based as well.
So just to inform everyone
and kind of explore
block-based compression algorithms,
I figured I go through a few
of the GP ones that exist.
They're kinda interesting.
ETC1S covers all mobile
devices except really old IOS.
It's actually a really amazing format
with a simple concept behind it.
You split the image into blocks.
You assign each block only one color.
Just one color per block.
- [Rich] The blocks are four by four.
Four by four.
And each pixel within the block
gets a grayscale value of that color.
Now, when I first learned
about this format,
I was like, &quot;How can
that work in practice?&quot;
But it's because the human brain picks up
on grayscale much more than color.
So in practice, it's actually
an extremely good format.
And we've made normal maps work in this.
It's been surprisingly
flexible and really good.
And it's a topic for another talk,
but the whole idea of perception
of image quality is fascinating.
There's no good algorithms.
Like we use SSIM and PSNR,
but there's no good algorithms
to judge how high quality an image is.
It's kinda fascinating.
A lot of big tech companies,
they just sit people
in front of images, and say,
&quot;Tell us how high quality this image is.&quot;
Because the human brain is really complex.
DXT takes a different approach.
It says, &quot;Okay, forget grayscale.
&quot;We want color to play
a bigger role here.&quot;
So what they do is they take
a line through 3D colorspace.
You can kinda picture it.
Each block gets two colors.
And then you can choose
where on the line you exist.
So you kinda see how that
could potentially work.
It's still limited, but now you can choose
from a little more colors.
It's actually a really hard
format to encode to sometimes
because if you get the wrong endpoints,
it can really mess things up.
This is on desktop and
consoles, not mobile.
And so Basis basically,
there's lots more, by the way.
Those are just two, and
that's part of the problem.
Basis looks at all these
block-based formats
and tries to unify them
and tries to create one
that can kind of transcode
into all of them.
We also are better on the CPU.
So this is an idea of the
mess we're dealing with.
These are all the GPU formats. (laughs)
So there's high quality.
There's lower quality.
There's desktop and console
and older IOS that we wish didn't exist
but we have to support it.
And all these different formats.
And this is why we're working on this.
Our plans are right now,
we have individual
encoders for ETC and DXT.
As I said, a lot of game
companies and triple A studios
like those because they
target one console.
They don't target your
mobile phone as well
and things like that.
- [Rich] And we ship these
encoders to companies.
Netflix was one of our
first customers using it,
integrating it in their
animations and videos.
And right now, we're in
talks with pretty much
so many companies. (laughs)
We're like on the edge.
Since being standardized,
everybody wants to use
this technology.
Hothead Games
just licensed our coding.
We have a .basis that just covers ETC1
in a production ready state.
We have the universal,
but we want to make sure
it's more solid before we release it.
So end of 2017, we will cover all devices
except older IOS devices, pre-A7.
And they're dying off.
Well, unfortunately, like in kids' games
and other regions of the
world, they're still there.
But it's a complicated format,
so it'll take some time.
And immediately after that,
we could support those older IOS devices.
We could also go into
high-quality formats.
Those are really exciting.
They're huge now, and we
can make them a lot smaller.
So I really want to do
high-quality, personally.
The goals are to have a
completely multiplatform
.basis solution that can
essentially be a new standard
for image compression in our industry.
We are actively working
with hardware vendors
and browsers and getting support for this,
so that hopefully in
however many years it takes,
you will all have .basis hardware
acceleration in your GPUs.
And we will sell one encoder for that,
but that will be the
standard in the industry.
So that's our goal, and
that's what we're working on.
I just wanna put this out there is
we're open sourcing the transcoder,
and we are writing spec for it.
Anybody can write an encoder for this.
And so we've proven that we
can do this already last year.
So we're gonna license the encoder,
but anybody else can do their own encoder.
So we're just not locking in on that.
No, no. We are here because
GPU formats are a mess.
We don't wanna create more
messes at a higher abstraction.
Our goal is to unify them.
We wanna have the first
unified GPU transcoder.
You can say, &quot;Oh, is
it harder as a business
&quot;if you're opening up your format?&quot;
And I think it's actually good
because then more people are using it.
We'll provide one encoder
that's ready to go.
Other people are welcome
to make their own.
So, Rich? Standardization?
Sure. We've not practiced
this at all, by the way.
(Stephanie laughing) We're terrible.
Yeah, we've kinda covered this.
So the problem you have right
now as a graphics engineer
shipping a product on multiple platforms
if you're like making your own engine
or even if you're dealing
with an existing engine,
there's this huge fractured ecosystem.
And it's really bad because
if you encode your texture to ETC,
you'll get different artifacts
then if you encode to DXT or ASTC or BC7.
And the quality you'll get with say BC7,
the range of quality
there will be much higher
than what you would get with say ETC.
So it can be pretty
nightmarish for a studio
wanting to ship their content
on all these platforms.
They actually have to go and
tweak, sometimes actually,
tweak the settings
individually per art asset.
So we wanna establish an existence proof
that yes, this whole thing is unifiable.
And part of the reason why
the ecosystem fragmented
like this is because
patents because you have
hardware company X patenting their format
and then wanting to license that
to all the other hardware companies.
So then Y and Z came out
with their own formats.
And it's just a huge mess.
And the open source world
has tried to solve this,
but the reality is, all the encoders
have different bugs too.
So it's just a nightmare.
It's like the dark ages.
And I'm not sure if you're
familiar with GPU coding
and stuff like that, but
this used to be a problem
with shaders back in the day,
the equivalent of programs for GPUs.
And back in those days,
you would actually have to code
in like this assembly-like language
that's specific for each GPU.
And they've fixed that.
They have higher level
standardized languages
like GLSL and HLSL.
And so that's kinda what
we're doing for textures.
So anyway, we're official
members of Khronos.
For a tiny company like us,
it's been actually an
interesting experience.
They've been really great to us.
To have our logo up against,
with all these other companies is amazing.
Another little note in there
is that if you ever use Unity,
our open source product,
Crunch, is in Unity.
And we're working with all these companies
to integrate Basis into their tech.
And it can take a lot of time,
especially at the lower
levels of the ecosystem,
but we are working on this.
And Khronos, for those who don't know,
is a standards body.
They create standards for
the graphics industry.
So if you ever had OpenGL,
that's their thing for example,
amongst many other things.
Yeah, OpenGL, Vulkan.
So benchmarks. (Rich laughing)
So as an example, we put this at
kind of high quality settings
just to see how much reduction you'd get
even when the image is
really high quality,
and it's significant.
So the top one is just
the GPU format by itself.
The second one is trying to compress it
with lossless compression,
which doesn't work very well.
And the third is using Basis,
and it's much better.
This is just one
tiny example, but we have some--
We have some more thorough benchmarks.
So what this is here
is what we call a rate distortion graph.
So at the bottom, we
have the compressed size
in bits per texel.
And up and down we have quality.
The higher, the higher the quality
from .66 all the way to one.
One is perfect quality.
And so like if you ever played
in Photoshop or an image tool
like with the various
JPEG quality settings,
what you can do is you can
try every quality setting
and then measure how many
bytes, or record how many bytes
are actually output for the JPEG,
and you make a little graph.
That's what we're doing here.
And this is one of our older codecs
that we shipped to
Netflix earlier this year.
And so what this is is you can see
all these data points on there,
and you can see how the curve goes up.
And then we have, and it's
kinda hard for me to see,
but there's two dots to the far right.
And so at the very, very, very far right,
at four bits per texel,
you have quality level
of like, I don't know,
.97 or whatever, that's the GPU format.
That's the quality you'll
get if you just code
directly to the GPU format.
And of course, all these GPU formats are,
in the general case, loss-y.
But it's still pretty high quality.
And then if you just take that
and throw that at like Gzip or Zstandard
or Brotli whatever, you'll see that
the bit rate has gone down
but of course the quality hasn't changed
because this is a lossless codec.
And that's the I think the red dot there.
And now, you're gonna see this curve.
And the curve starts at quality level .94,
so this version of the
codec we shipped wasn't able
to achieve the entire
rate distortion curve.
To translate that to JPEG terminology,
it was able to achieve
like quality level 80.
So it went from 80 to 50, for example.
You can see as the
quality levels turn down,
as the quality goes downwards,
the bit rate goes downwards.
And so what we do is we run
our encoder on thousands of images.
We record these rate distortion curves,
and then we optimize our codec.
- [Stephanie] We're
constantly tweaking it.
Like you can see in some of.
So in this one, another thing worth noting
is low-quality is usable.
It's very, very usable.
We have a lot of people that
ship only using low-quality
because it's just good enough.
The way we do artifacts
when we do lower quality
is very similar and
mimics the GPU formats,
and you can get away with it.
So the low-quality in
this case is an eighth
of the size of the GPU format about.
So it's huge reductions.
On the flip side, there
are some customers that,
our newer codecs let you
access almost the entire curve
all the way up to the red dot.
And what they do is they
only dial the quality down
a little tiny bit, and when they do that,
they can still cut the amount of data
they ship to their
customers in about half,
which is a huge amount of value.
So you can see like the curves all differ
depending on the texture.
Unfortunately, we still have
to eye this up manually.
We don't fully trust
any technology to judge
the quality.
As I said, image quality
has no metric that's perfect yet.
So we basically run these graphs
on thousands and thousands of images,
like ones that we are not gonna spend
the time examining individually.
But our job is looking at lots
of pictures and judging quality.
That is the only way to
really do this right.
Yeah, so if we do an encoder
change and we mess up,
then the dots will go down (laughs)
or the dots will go to the right,
meaning higher bits per texel.
So our goal is to push
up this curve as high
as we can and to the left as we can.
But you can see, it's already
a dramatically better compression,
and these are actually benchmarks
from like early this year.
So it's even better now.
So in short, we really hope that
this makes web browsers faster and better.
We really hope that GPU
hardware will advance more.
Like something people don't realize
is that because GPUs have to
support all these formats,
there's a lot that they don't.
Like not a lot of people
target ASTC, for instance,
and it's huge on the chip.
Like they could get rid of that,
or they could replace it, or upgrade it.
If we abstract GPU formats away,
they have the flexibility
to create cutting-edge
GPU formats without worrying about
who's building encoders for these?
Will they be supported?
No, all they need to do is
write a really easy transcoder.
So we're actually really
excited about that.
Storage sizes are crazy huge.
If you think about companies
like Netflix or Google
or these big companies,
they just have warehouses
of images stored.
And if we could help reduce that,
it actually does our little
part to help the environment.
And advances in just streaming, maps,
games, the web, virtual
reality, any application
that uses images can be
better because of this.
That is our goal.
Yet, the reality is
abut 50% of the data shipped
to customers is noise.
If you look at the GPU
texture they actually ship.
We're talking games that ship hundreds.
There's games that have a hundred gigs
of texture data they
download to customers.
This technology should have been out
a long time ago.
The problem was
the video card vendors
deprioritize software development
in general, especially tools development.
So they haven't invested much in tools.
So their goal was just
well let's the quality
as high as we can and
ship it and then move on.
Yeah, exactly. So that's our goal.
Like a lot of our job is getting people
to agree with each other.
And we're gonna create this standard,
and it's good compression too.
So we're open to questions.
You can ask away if you want.
But thank you so much
for coming.
Thanks a lot for coming.
(audience applauding)
(Male Audience Member)
What is your HDR support?
So right now, we're targeting,
we're making a value proposition,
and that is, it's a trade-off.
We need to get a universal format
out there first that
targets RGB and Alpha,
and then we'll look at that, at HDR.
But until we have that,
like we need a baseline,
you know what I mean?
So we're targeting DXT1 and
ETC1S, as the first, and DXT5.
That's what our universal
format will transcode to first.
But we have plans to do that,
and it's something that's
really exciting to me.
And just to note it, HDR
means high-dynamic range.
If you weren't familiar with that.
High-quality formats in general
are really important
to me like BC6H and BC7
because they're just so large now,
and it prevents so many
people from using them,
and that's not right.
We should be better than that. Go for it.
(Male Audience Member) I was gonna ask
about mipmaps and generational mipmaps,
and how does your format work
with the mipmap generation?
I'm so glad you ask because it's one thing
we didn't have in our slides.
We kept this talk very high level
because there's such a wide
range of experience levels.
Texture arrays are the way
you get mipmap support.
Well, mipmaps basically are,
it's like an image pyramid.
So you have like a low, the lowest level,
you have a high-res image.
And then, a level up, you have half-res.
And then, a level up,
you have half of that.
And it goes all the way down to like
four by four by one.
So it basically allows you
like if you're looking
at a tree in a distance,
you don't need a 4K tree there.
You don't need to fetch from a 4K texture.
It'd be very alias and very inefficient.
Instead, you can fetch from
four by four, eight by eight.
This is all done in hardware.
And so what we do is we
just support texture arrays.
We decided we're not gonna mess around
with cubemaps and mipmap
support and all this stuff.
We just support
non-uniform texture arrays.
So you can just give the codec,
if you want, thousands of frames.
They can be any resolution you want,
and then we compress all that in one unit.
And that's what Netflix
actually does with our codec.
And the exciting part of this
is it's not only supports
mipmaps, cubemaps, it also
supports really creative uses.
Like we have people that people that put
all their normal maps in a texture array,
and it shrinks them down
like crazy.
Or pre-process a video
and then feed us
those pre-process frames.
It's the basis
of a video codec, so it's really exciting.
(Male Audience Member) So I'm
not a graphics programmer,
but you mentioned normal maps
and strikes me that compression techniques
for normal maps are different
from compression techniques
for like visible photos, right?
So how do you adjust the quality
of your compression techniques
for things that aren't like (mumbles) ?
Yeah, so that's a great question.
So normal maps, basically,
for every texel,
it's just an encoded directional vector
on like a unit sphere.
And typically what they do
is they have zero, zero, zero
represented as 128, 128, 128,
and then they just encode the unit vector.
That way multiplying
everything by 127 or 128.
And so to us, it's not a photo anymore.
And so the image metrics,
the quality metrics
that the photo people and video people use
like SSIM just don't apply.
So what we do with that,
we have two things.
We have MSE, which honestly
I trust fairly well
because I have so much experience
with it in this domain.
And we were experimenting
with an angular metric.
Honestly, MSE. That's
what we use right now.
And the other thing is,
the real question should be how do you fit
your texture data into a GPU format?
Because once it's in a GPU format,
then it's compressible with our stuff.
That's the real challenge.
Because our encoder actually
is just a GPU texture encoder.
Like it's a different take on it.
We basically, we recognize
similar colors in the texture,
similar shapes, and stuff like that.
I can get in the details about
vector quantization and all this stuff,
but to keep things simple,
that's really the hard part
is getting into a GPU texture.
For normal maps, it's pretty much
a solved problem at this point.
But on the other side of the coin,
like ETC, the format itself, is designed
with the human visual system in mind.
It takes into account the grayscale axis,
and that makes no sense from
a normal map perspective.
So I've made mistakes.
Early on, I was like, &quot;Oh,
I should have different.&quot;
What was I doing? I was doing--
We're actually one of the better codecs
for normal maps for formats like ETC
because basically, it's really important
that this is not just
used for photographs.
It needs to be used for
depth data, for normal maps,
for all these things.
Basically, when you call
the encoder, you can
specify the distance metric
that we use.
But we can go into detail
for like hours.
When I first got
normal map support in there,
I was surprised at the things I had to do
to make ETC work well with normal maps
is what I was saying earlier.
So go for it.
(Male Audience Member) So I
have two kinds of questions.
One is, you said you use
of a lot of inline assembly
for performance reasons.
And I assume that means
that you're not having
to go look for compilers (mumbles).
I'm curious, which
compilers? And about bugs?
It was intrinsics
actually. SSE intrinsics.
So not really inline assembly so much.
Like we still have the
compiler do register allocation
and all that stuff.
So, we're not working
here on bugs or anything.
It's just there's certain key parts
of the encoder that we wanna know,
we wanna make sure it's
vectorized because it counts.
But we're not talking a lot of code.
It's a tiny fraction
in the bigger picture.
(Male Audience Member)
Sure, my other question was,
you said that you like OpenMP,
and you use it very heavily.
I would love to hear more
about your experience there,
and what (mumbles) of OpenMP
have you had experience with?
My own has not been (mumbles),
so I'm curious to hear yours.
Well, first, I use just
parallel fors pretty much.
I have my own threading library
like most game developers do.
So I use that, and I use parallel fors.
I've read a bunch of OpenMP docs
and a lot of it was very confusing,
and I've been doing this stuff
for a long time. (laughs)
And also, I was kinda doubtful
about how it would be implemented,
but we're using Visual
Studio 2015, 2017, and Clang.
Well, GCC actually.
I'm using GCC and Klang
on Linux and Mac.
We like it because
it's so easy for us to drop in,
and yet it works very well.
There were some problems with OpenMP.
And one problem was I
wanted to have an option
that allowed the caller to
disable like all threading,
so they can do their own threading.
And I found that to be
a little problematic.
But overall, it's been
a great, I'll be honest,
it's been a great experience.
Like our encoder consists of these
massive parallelizable, trivially
parallelizable for loops.
Exactly, so we were writing
our custom library for parallelization,
and we discovered OpenMP.
Yeah, so the open source
predecessor of Basis did not use OpenMP,
and I use just custom stuff.
And it was a huge pain
in the ass, to be honest,
because I had to constantly be aware
of what data to give each worker
thread and stuff like that.
We also have used OpenMP
Critical for critical sections.
So those are the two main things.
And I'll tell you what,
the amount of value
those things deliver is huge.
The downside is it's very ugly
the way OpenMP looks in the code.
So if that was standardized
that would be great.
It's not like this is a new idea.
Go for it.
(Male Audience Member) Okay,
I have several questions
regarding the (mumbles).
From what I understood from your slides,
you do not have entropy coding done on GPU
and you rely on CPU, so
there just (mumbles)?
That is correct.
We've avoided the entropy
coding on the GPU,
and I can answer why
if you're curious but--
(Male Audience Member) So basically,
you go to some (mumbles),
which are designed
in a way so that it should be compressible
very well with (mumbles)?
That's right.
We lower the entropy or
the amount of information
in the output blocks.
That's called rate
distortion optimization.
We would call RDO mode.
(Male Audience Member)
Okay, so basically--
That's one possible
way of using it though.
(Male Audience Member) Okay, (mumbles)
and my question is what
is through your optimize
the encoder or the
compression is not (mumbles)?
So Crunch supported this mode
where it would output data
with less information,
but it didn't do any
explicit optimizations
specifically for LZ.
So what it would is it
would just cut the amount
of uniqueness in the data, and
then we would just get lucky.
It just worked.
What Basis does, it
takes it a step further,
and it actually simulates,
I've written a lot of
state-of-the-art LZ codecs.
I basically brought those models
into the backend of our encoder.
So we actually simulate the LZ codec.
We simulate the sliding dictionary.
We don't actually go very far.
We don't have entropy simulation yet.
We just simulate the LZ part.
(Male Audience Member) So that's basically
something like optimal (mumbles)?
Yes, actually, dynamic programming.
Very similar to that actually.
And one thing we've been
thinking about lately
is optimizing for specific codecs.
Like a lot of people, a lot
of game developers especially,
use Oodle and things like that.
So you can send a flag saying
which codec you use--
We let you change
the dictionary size.
In the future.
(Male Audience Member) One other question.
There is some papers out
about doing (mumbles) on GPU.
What's your take on this?
We know Pavel at least very well.
He's a good friend. (laughs)
And the work is really great,
and it's really nice to have
that in an academic paper
because it kinda
validates all of our work.
And do you wanna expand
on GPU transcoding?
Here's the thing, we
are shipping a product,
and so we're very conservative.
We're basically taking the min function
across the whole ecosystem.
And I have a lot of experience
with crappy GPU drivers,
and that to me is a nightmare
scenario for a product.
(Male Audience Member) That's
actually not my question.
I understand why you didn't
do this on this product.
I'm asking what's your take on (mumbles)
the future development--
Ah, the future.
Well, I already can hear one
set of game developers arguing
that the GPU is busy enough.
I can hear that one crowd saying that.
Honestly, there are so many
trade-offs in this space.
For example, with the audio
formats that we support,
a lot of vendors have LZ
in hardware right now.
In the future, it's just
gonna be even more pervasive.
So, RDO actually that's
one reason we do it.
We have one format that you basically,
the customer encodes the .basis.
It's a relatively large file.
And then, they have to LZ
it. Compress it somewhere.
And then, decompress it.
They can decompress it with
their hardware decoder,
and then run a very simply shader on that
to recover the GPU texture data.
And in that sense, it's already
completely hardware decodable.
And so I think Pavel's work is great,
but at the end of the day,
the hardware vendors have put the stuff
in Silicon already, and
we wanna leverage that.
Yeah, I think one thing
worth noting is that
as we're evolving .basis,
we're considering splitting,
this is kind of a technical detail,
but we're considering splitting
transcoding into two parts
that kind of Huffman coding
and then the part that actually turns
the blocks into GPU blocks.
And we're working very closely
with GPU hardware vendors,
and they really like that idea.
And so it'll be interesting
to see how it evolves.
I hope we answered your questions.
Anymore questions?
Well you can feel free to reach us
after the talk too or email us.
That email basically reaches both of us,
so we'll be able to see it.
And thank you so much again.
Thank you.
(audience applauding)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>