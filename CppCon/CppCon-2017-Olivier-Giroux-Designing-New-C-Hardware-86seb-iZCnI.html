<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Olivier Giroux &quot;Designing (New) C++ Hardware” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Olivier Giroux &quot;Designing (New) C++ Hardware” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Olivier Giroux &quot;Designing (New) C++ Hardware”</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/86seb-iZCnI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Hello everybody, to my talk.
My name is Olivier Giroux.
I work for NVIDIA.
I've been at NVIDIA for 15 years now.
I've worked on eight
different GPU architectures,
and more recently four
different SM architectures
within these GPU architectures.
And SM in this case is a compute core.
So, I guess that makes me old enough
that there are some people here
who never used GPUs that
I haven't worked on.
Now, I'm here to tell you
something really exciting,
and that thing is that NVIDIA
actually really cares about C++.
So much so that I can come out and say,
quite clearly that we designed this one,
which is Volta, the one
we announced this year,
specifically with C++ in mind.
And, so my talk, Designing C++ Hardware,
in my view of it, it is
both designing new hardware
for C++ and designing
new C++ for hardware.
And doing all that at the same time.
So, to put me into some
coordinates in space here for you,
I've said that I work at
NVIDIA, I'm a GPU architect.
I'm also a member of WG21, which
the contrast is nothing great,
but I'm also a member of WG21.
I've been on the committee
for several years now.
And I'm a member of an
even smaller community
that deals in memory models,
these obtuse axiomatic definitions
of what it means for memory to be memory.
Okay, so that's my perspective.
And the agenda today is largely not code.
And, in fact, there will be...
This is, this is an accurate depiction
of what we're gonna talk
about from a high level.
When we do talk about coding,
will actually be fairly bad code.
I apologize in advance.
Okay, so now I have a
question to start with.
That question is:
what is it about C++
that makes it portable?
We like to think that
C and C++ are languages
that are portable to a wide
variety of computer systems,
but when you look out today,
is there that much variety
that's in evidence of that?
Okay, so what is it about
C++ that makes it portable?
And, I think that if we ask this question
at the water cooler in
most of your organizations,
we might get a list that
looks a bit like that.
Like there are things like
you don't really get to
know how big char is,
you don't really get to
know how big anything is.
Your integers, they might be
2's complement, they might not.
Your floats, they might be
IEEE, or they might not.
You don't really know about
the endianness of addressing.
You don't get to control the
alignment of certain things,
especially you cannot
undercut the minimum alignment
of a type that the platform chose.
And then there's this thing
called segmented memory
that C++ supports through
various arcane rules.
Okay, so in this view,
C++ has robust support
for all of these weird things
across the storage
abstractions of the language,
across the arithmetic
abstractions of the language,
largely through various
forms of undefined behavior
on the arithmetic of integers for example,
not that they are not
2's complement, okay.
Now, I have some, this
may come as a shock,
this may not come as a shock, but largely
most of this also has you covered
through the 20th century trivial pursuit,
meaning there are essentially no remaining
well alive systems that
care about any of this.
Except for the bottom two,
which may surprise some of you.
So we've got this stuff in C++,
C++ has robust support for these things,
which are really a support for fossils.
You know, not living systems.
Okay, so how useful is that?
In my experience of trying to implement
of trying to create a new
implementation of C++,
I haven't found these
things to be very useful.
And by the way, the odds of making
a genuinely new computer architecture
that supports C++ are
really stacked against you.
The odds are, you're just gonna gravitate
to copying one of the preexisting ones.
Okay, so why is most of this not helpful?
Well, it's because behind these things,
are either a lot of false
choices, or bad choices.
If you're going to create
a new architecture today,
and say you're getting funding
to create a new processor tape-out.
Well, most of these options are
really gonna be dictated to you.
If you're making a new
CPU, pretty much everybody
is gonna go match what
the AARCH64 is doing.
Because AARCH64 went ahead
and updated the LINUX kernel,
with all the support they needed.
Are you gonna do the same?
No probably not, you're gonna
want to like stream behind them.
Okay, so for a new CPU you'd
probably look to matching AARCH64.
If you're making a GPU,
you have to match the host.
If the host is using a
32-bit 2's complement int,
you also need to be using a
32-bit 2's complement int,
because otherwise the GPU won't
be interoperable with the CPU.
In fact, for GPUs what that means
is that we end up matching all the hosts.
It used to be NVIDIA GPU's had a switch
in them to switch them from
little endian to big endian.
Because we wanted to be
attached to big endian systems.
And, that went away.
The big endian systems went away,
and so did the switch.
And then other choices are
just bad at this point.
Oh sure, you could build
a 1's complement machine.
Sure you could build a
non IEEE float machine,
but at this point in 2017, you would get
really negligible area savings
or power savings from that,
but you would pay on the software side.
Your users would suffer from all the,
rediscovering all these glitch, which
we know in the C++ community and
it makes us really cool
at the water cooler
when we talk about that.
But, most programmers don't really worry
about it on a day to day basis.
Okay, so now I'm gonna make
sort of a weird comparison.
I'm going to compare the
manufacturing processes
that NVIDIA tape-outs use
at the time
of various C++ revisions.
And, I think this weird
comparison makes sense in my head,
because in my view C++ implementations
go all the way down to the metal.
By metal, I mean periodic table metal.
Okay, like they don't stop at
a particular arbitrary point
in the middle called an isope.
The C++ implementation is not just the job
of taking a program and
lowering to an isope.
It's taking a program and making
a complete physical manifestation
of that program, all the way down.
Okay, so if you were to
tape-out C++ in those years,
what manufacturing process would you use?
And these are to scale, okay.
So in 1998, NVIDIA was using
350 millimeter lithography.
In C++17, we're using 12
millimeter lithography.
A bunch of design...
You would think such a dramatic change
in the landscape would effect
certain design decisions.
There are design decisions
you make differently
when you have, by now,
giant transistors, or tiny transistors.
At some point in there,
IEEE floating point
became a no-brainer.
As far as we are concerned, we
would not bother taping out,
for our main math units,
for our primary math units,
we would not bother taping
out non IEEE floating points.
It just doesn't really make sense.
The units are small, at around this time.
Like, around between 130 and 40,
it's just a non-issue.
So we're not interested in that
freedom starting on that point.
And, my argument here is, most
of the other things that
are on the previous table,
they all fall to a similar argument
at some point on the timeline.
They just no longer, they're no longer
a freedom you're interested in exercising.
Okay, so I would say
that there are sort of
these apochs of C++ design.
And by here, I'm really talking
about the language design.
In the early days, in
this revisionist history
that I'm doing here.
In the early days,
priority is to make sure
that you can make little systems legal.
Systems that have decided
to throw overboard
some complexity, because you're coming
from a minicomputer to a microcomputer
and you couldn't afford
to have all these units
and have all of them behave the same way.
And so the language
enables lower QOI options.
Lower quality of implementation options.
But, in the future, we
have a different problem.
Our problem is making sure that C++
legalizes really big systems.
Okay, so
for the rest of the talk,
we're gonna talk about sort
of non-classic portability.
And these are these two phases,
that I just talked about.
Then, an era of transistor scarcity,
and now, a completely different
era of transistor abundance.
In 7mm, you can put so many.
Question is, can C++ actually use them?
If I fill a really big
die with functionality,
how will C++ get to it?
Okay.
And I would say, also,
slightly revisionist history.
I would say that in C++17,
we have taken steps in that direction.
Okay, so what is it about the next C++
that makes it more portable?
I would like to talk
to you about topology.
Here is a really simplistic
diagram of a computer.
I mean, you see these
diagrams in textbooks.
I have a line, and I'm
gonna call it &quot;bus&quot;,
and then I hook a CPU rectangle
and a DRAM rectangle onto this.
And I'm gonna call this a
1980's type of topology.
Okay, so there's my 1980s computer.
Oh, and there's other things
in my computer, of course.
My computer's got lots of other features.
All of them are I/O.
Okay, so there's the
CPU and there's the RAM,
and then everything else
in the world is I/O.
And really, the interface here
was probably invented to
service tape drives, but...
It's okay, we'll work with it.
Alright, so then C++ comes in,
and obviously in C++ I'm not programming
a CPU in DRAM directly,
I'm using abstractions.
So, C++ introduces layers of abstractions,
in fact C++ alone brings in at least
three layers of abstraction for each side.
We abstract the CPU using threads,
we virtualize it using threads.
We have this abstract notion
of an execution agent,
that a thread is a concrete example of.
And then I have the thread of execution,
which is a chain of value
computations in your program.
One independent chain of value computation
in your program, is the
thread of execution.
At that point, that's
pretty much your code.
Okay, so those are layers
of abstraction there.
And then on the DRAM side,
I have a similar thing.
I have allocations, which have objects
and then the objects hold values.
Okay, now from that time
to, more or less today,
here's what we've achieved.
We've been able to dice the
box on the left into parts.
We have achieved that.
Now they still pretty much
have to behave as one.
They have to be friends.
They have to be friends
when they access the memory.
They need the order to make sure
that they're all coherent, for example.
Okay, now here's the problem that I have.
My design expands the
topology of the system,
in a way that was not
originally envisioned.
It was not originally envisioned in C++.
There are no abstractions to deal
with another co-processor.
Co-processors went away when the
386 integrated the FPU right?
So there are no coprocessors.
The notion of whether they're equal or not
doesn't compute because there are none.
So there are no abstractions for that.
But even at a fundamental level,
even in this, in my computer
architecture diagram,
it also doesn't work.
Like how is my coprocessor
equivalent to I/O?
And this is actually a
real weird thing today.
Operating systems program
my GPUs using I/O controls.
It is basically receiving these
commands as if it were a disk drive.
Okay.
But it's not, it has a very
large number of cores in it,
effectively it has a very
large number of parts.
Now I put two numbers there.
It sort of depends how you interpret it.
It has, it can execute 5120 different
instructions per clock, every clock.
All different, but...
(audience laughing)
There aren't gonna be, like some
of them are gonna be similar, yeah, yeah.
(audience laughing)
But it has an abstraction on top of that,
also of thread of which there are 163,840.
I don't know how many of you have written
concurrent programs with
163,840 threads in them,
but that's basically been my summer.
All summer, yeah.
Okay, so the future scalability
of C++ goes through showing
some love for topology.
Caring about more interesting topologies
that make bigger and bigger systems.
So, this diagram here is
not even particularly crazy.
This is a system that we're shipping.
There are probably some of you in the room
who probably have one now.
So this is one of the
systems that we're shipping
that has eight GPUs and
two CPUs and a bunch
of network connections, and then
there's all kinds of
connectivity in there.
We could imagine way crazier.
Okay.
Alright, so,
so far we've talked about things
in C++ that make it more portable
and sort of future directions
that we should care about.
Now this talk is called
Designing C++ hardware,
so now we're going to
switch to how did we design,
I made the statement up front that Volta,
our GPU architecture from this year,
Volta was designed for C++.
So, let's dive into that claim.
I'm breaking it up into two parts.
We're gonna talk about how
we're attacking the memory vertical,
right, there were two
verticals on my previous slide.
How are we attacking the memory vertical,
and how are we attacking
the execution vertical?
And we're gonna talk about those two.
So let's start with the memory vertical.
At the diagram level, at the theory level,
as long as I have a diagram
with two boxes and two lines,
it appears so simple.
There's a bus and there's a processor,
and it's attached to the bus, and
the memory is also attached to the bus.
And the processor goes
and accesses the memory,
should just work.
But the reality is a lot more like this.
You know, the memory is really,
it's really attached to the CPU.
It's not really on this bus,
and the CPU has a very
special access to the memory.
And it really, this is really,
you can quote me on that,
it's really my terminology.
I use it regularly.
It tastes like memory.
When the CPU goes to memory,
it tastes like memory.
It has these attributes that you expect.
For example, a series of loads and stores
to the same address, from the same thread,
never become reordered.
That sounds like pretty key, like
if I were polling on a memory location,
and I saw that memory location become one,
I would expect that I don't need
to test it again after the polling.
That it wouldn't just become zero again.
Unless someone else wrote
through it of course.
(audience laughing)
Now, that's really not the life experience
of someone working on the
other side of this line.
I said that everything below this line
was treated as I/O, and
so as we go to memory,
it's really like we're
traversing this complicated maze
which might be a series of different buses
with exchanges and switches and
it's really not that simple.
Somewhere along the way,
one of these switches
just really likes to reorder things.
And so, when I go to memory, it doesn't
taste like memory, it tastes like DMA.
Okay.
We've done a lot of work on this.
Actually, all of this talk
is notionally about Volta.
The next slide is mostly not about Volta.
The next slide is about the work
we've done over a decade long period
of making memory and CUDA C++ live up
to the expectations of C++
programmers more and more.
Okay, so if you think of GPU programming
as being, I use this GPU alloc
and then I do GPU Memcpy,
and I have to do GPU Memcpy in and
I have to do GPU Memcpy out.
You are really thinking of CUDA 1.0.
Which, at this point
is 10 years behind me.
Okay.
Those were the first generations
of GPUs that had compute capability.
I said that I worked
eight GPU architectures,
so before that I worked
on Rankine and Curie,
which did not have any compute capability,
they were graphics only devices.
Okay.
So here we are.
That's the first generation, you know,
CUDA made a very big
splash, and it was imitated
far and wide, so this model of CUDA 1.0,
you can now find it on
a variety of platforms
from a variety of vendors.
They're all more or less copies of that.
Okay, but we didn't stop there.
In Kepler and Maxwell, we
created this different idea
called cudaMallocManaged
and cudaMallocManaged uses
this symmetric heap
approach where basically,
allocations that you make
on the CPU with that,
become mirrored on the GPU
when you launch some work.
And they are mirrored on the same address.
So as long as you don't
have any data races
between your CPU and GPU threads,
and so we disallow your
own custom synchronization
between CPU and GPU and you have to use
our heavyweight synchronization
as long as you follow our rules.
Then actually, already at this point,
memory pretty much feels like memory,
but there is one big downside,
which is that we are
making a symmetric copy
of the heap at every invocation.
From the CPU to the GPU and back.
So we didn't stop there.
In Pascal was really when we introduced
another mode of cudaMallocManaged,
where it can migrate the data on demand.
It's notionally similar to a
copy on write scheme, okay.
And we've basically
continued that in Volta.
And this, I cannot wait for this to land.
There is a patch, in the cube,
of the Linux kernel maintainers that adds,
effectively, that merges this into the
operating system as an
extra level of paging.
And then when you have that patch,
in your kernel build, basically
now everything goes away.
It's just malloc, it's just in malloc,
actually even memory that's not malloc
could be memory on your stack,
could be a global variable aesthetic.
Those would all be fine.
It's basically, at that point,
the memory that's backing
the whole application,
not only is it pageable to this,
but it's also pageable to the device.
Okay.
(audience member asking question)
Yeah, yeah.
The question was if there
was an m-mapped file,
could the GPU use it?
Yes.
Yeah, yeah.
Okay, yeah at that point, when
you have a Pascal and Volta or better,
and you have the, and the Linux HMM patch
is either you have a custom
build or it gets adopted
and you have a kernel
from after that date.
At that point, the whole discussion
of where the memory is, it goes away.
You'll notice that my
table's weirdly shaped,
and it's because there's
another part to it.
Now, we are not, we're not satisfied
with this,
this maze separating us from the memory.
You may have heard that we also
have this thing called NVLink.
And NVLink is the hardware
capability that we've built.
And we've developed it together
with IBM and IBM has implemented it,
in Power8 and Power9.
And, once we have an NVLink connection,
instead of a PCI connection, suddenly
all of memory tastes like memory again.
It does not taste like I/O.
Okay, and so there's this other part here.
Which is that if we are
talking about an ARM system,
in which case it would
be an NVIDIA ARM SoC,
or a Power system, Power8 or Power9,
then there is also this
other option down here.
And, under this option down here,
there might not even be any paging.
It is, the memory is at
that point, fully unified.
In hardware.
Moving on, we've talked about coherency.
Well, coherency is not enough.
You also need consistency.
And you'll recognize,
effectively this table
is just a reproduction of your
average implementation of C++11.
There's some, some of the combinations
of load stores and atomics are not valid.
Some of the combinations are valid,
and then there's consume which everybody
decays to acquire until such time
as we as you want fixed it to.
So there is your consistency for CPUs.
So what should it be, what
should it be for GPUs?
Well the answer is exactly the same.
Absolutely exactly the same.
And that is a new thing in Volta.
Although the coherency we,
became a feature on Pascal,
the consistency model
appears here with Volta.
We have, you know, unfortunately the
slide doesn't really do it justice...
It's a completely new memory model.
It, at 10,000 foot, sure
resembles some other models,
for example it somewhat resembles Power.
It resembles other
models that have scopes,
that have appeared for GPUs.
But, it is fundamentally different
from the other scope models, in that
it does not have undefined
behavior for data races
at the level of the instruction zone.
So it is in the vain of CPUs,
where programs that have data races
have loosely bounded
behavior, but not undefined.
And then it's at the language level
that you have undefined behavior.
And in C++, it's undefined behavior.
Data races are undefined behavior in C++.
But how bad can the machine
get once you have data races
is still something you want to care about.
The fact that your C++
program is undefined,
if you have a data race is bad, okay.
How much of that can be
turned into an exploit
somewhat depends on what
the hardware will do.
And so, it's very important to us that we
adopt sort of the best known art
for defining CPU, processor memory models.
Which had not been done
before for a scope model.
There are a few concepts that didn't exist
in the literature that we had to create.
And so now, I'm very luck because
this actually landed on the
website, I think two days ago.
I can now hand you a URL to our
formal axiomatic memory model.
That was in development
for a very long time.
And so, that gives me an
opportunity to tell everybody that
oh my God, I really love
the C++11 memory model.
It's really fantastic.
Whereas earlier, I listed some things
that people might think help the
portability of C++ but don't really.
Here's an example of a thing you
might not think of very much, but
really really helps
the portability of C++.
You all know that volatile is a bad word,
but it is, although it's
bad for software developers
to use volatile, it is
particularly oppressive
for hardware architects
when you use volatile.
Because a program that has volatile in it,
does not actually have
well defined semantics.
It only has semantics that the
machine you ran it on gave it, right.
So if most people do
their development on x86
then it means, for most people
the semantics of volatile
is whatever x86 did with that.
And that would force us, that would...
And this is true story, we
would end up in conversation
endlessly where people are
like well x86 did this.
You should do this.
And we're like no, we can't.
Like, we don't have the privileged access
to the walled garden that owns the memory.
So it's not even an option,
it's not even an option
for me to really go and match it.
What the C++11 memory model did,
is it really clarified the semantics.
Not only did it clarify the semantics,
it clarified them in a way that made it
completely implementable to me.
So that was wonderful.
That was very wonderful.
And, as it happens, I joined the committee
right after that.
And then at that point, I thought
whew we dodged a bullet there.
We were not really paying attention,
but we dodged a bullet.
Alright, I have two open issues.
Not watching time too closely, but
okay thank you.
Okay, there are open issues
remaining for C++ memory.
Coherency and consistency
unfortunately was not all of it.
There is more.
So now, I'd like you to
consider the issue of footprint.
Or working set, could be a synonym for it.
So here I have some example numbers
for that CPU box and these
DRAM boxes and then Volta.
Alright, so I may have a CPU
with 16 threads and 128 gigs of RAM.
That is a perfectly reasonable system
for somebody to assemble.
And I have, in my PC, haha,
(audience laughing)
a Volta with 163,840 threads
and it has 16 gigs of RAM.
Let's, let's take a second
here and look at these ratios.
These ratios are not very similar.
(audience laughing)
One to eight gig and one to 100 K,
these are not very similar.
So, if you'll recall a few minutes ago,
we were looking at the transistors
and I said you might imagine that
with such a drastic difference
in the amount of transistors you can have,
it might change some design decisions.
I would bring up to you that this
would probably change
your design decisions.
For libraries and even language design.
Okay, so here's a concern that I have.
And this is really an
evolution of C++ thing.
It's a thing SG1 has talked about.
I'm gonna give you a number for a paper,
you can go read it and go pine about it.
I would like that paper to get
a future revision, for instance.
Shall we spend all of
that on thread_local?
That is a really significant risk.
People might use thread_local
from C++11 with relative abandon
when it's one to eight gigabytes,
but when it's one to 100 kilobytes,
that's just not going to work very well.
Okay, I would surmise that whoever
came up with TLS, didn't really think
about systems with 163,000 threads.
It probably did not occur to them.
Okay, so we have a problem, we have...
Something's gotta give.
We need some way to clarify that
thread_local may sometimes
have a different meaning
or something, I'm not sure.
That's a wide open field, we
could write something new.
There's another issue 163,840
stacks is a lot of stacks.
In that 100KB, some of
it goes to your stack.
Hopefully not all of it.
(audience laughter)
Concretely, in 2017, at NVIDIA right now,
we are forced to choose
between having stack accesses
that have high performance or complying
with note 11 in clause 4.7 paragraph one.
What does that note say?
It says, I'm paraphrasing, that automatics
are private to a thread however
you can take their address.
And once you take their address,
you can share them with another thread.
And then other threads
can access the subject,
oh and be aware of data races.
Okay.
As it happens, our current implementation
is exceedingly efficient, but it does not
allow addressing of this
memory by any thread
other than the thread that owns it.
I'm not sure how to reconcile these things
and it's not because I
haven't thought about it.
Okay, so memory progress report.
Coherency, we're pretty much
set from Pascal to Volta.
Consistency, we're
pretty much set on Volta.
At the moment, it's at the assembly level.
I don't mean the private
assembly that you don't have,
I mean the public
assembly that you do have.
And you can use inline
assembly in CUDA C++.
So you can have access
to all the primitives
that you need to get the same semantics
that are compatible with C++.
When atomic T appears in CUDA C++,
that is currently TBD, but some
people have particular implementations.
Okay, that's on our side,
that's on the GPU side.
Oh and I really don't know what to do
about Footprint working
set, this is effectively
a problem that's just
endemic to the scale.
And my hunch is we're gonna need
to have a way for C++ to speak to that.
Okay, the C++ disposition, there are
some known bugs with
the C++11 memory model.
Nobody panic, it's fine,
we're taking care of it.
You can read, P0668 and fun adventure,
some of the bugs there were co-discovered
by Princeton and NVIDIA and a group
in Germany at Max Planck.
It's funny because they
were all discovered
at right about the same time.
And then everybody was like,
I have a thing to tell you,
you'll never guess what it is.
And we're like wait,
the proof of conformance
of this process is invalid?
They're like, yeah.
(audience laughing)
Okay.
So there are recommended fixes.
We had a vote in Toronto to
move ahead with the fixes,
so we just need to revise
the paper and put it in,
and then we'll be, we'll
have a relatively short
defect list on the
memory model after that.
That's the C++ disposition
for coherency and consistency.
I would invite people to look at P0072,
half of which was adopted into C++17,
and the other half was left on the shelf.
I am basically saying, the half
that was left on the shelf,
we should think about it again.
And it's because it helps
with the footprint problem.
Alright let's talk about
Designing Volta for C++ execution.
And this year, this year is, we did
something really huge, really huge here.
So huge, yeah.
So I talked about these
abstraction layers earlier.
C++ has a concept of
a thread-of-execution,
which you have to use all three words,
probably hyphenate them because
it's a term of ARM as
a thread-of-execution.
Which is a chain of
evaluations in your code.
It's sort of your code
representation of a thread.
That runs on an execution agent.
Which is just an abstract notion
for that thing which runs your code.
And prior to C++17, there was only
one instantiation of this concept.
It was a concept with
only one concretization,
which is a thread.
And that's just a particularly
onerous version of that, really.
I mean the thread is,
in C++11, fairly fat.
It has all these, you
know the thread_local
and it's behavior on
the automatics and the,
it has a guaranteed forward progress,
it's extremely strong and all that.
So then once we're going down is
CPU runs anything, that's for sure.
And then the GPU, it's a thing
that doesn't run onerous things.
Alright, so in C++17,
by adopting effectively half of P0072,
we've done a lot of clarification.
We've clarified that
actually, execution agents
might have more than one
kind of instantiation.
And there might be more than
one kind of execution agent.
So we've created the concept
of concurrent execution agents,
parallel execution agents, and
weakly parallel execution agents.
Which basically differ among each other,
by the amount of forward progress
guarantee that they have.
A concurrent execution agent
has all of the capabilities
and all of the forward
progress that a std thread has.
And also the special main thread,
that we can't tell you what its type is,
but it's a main thread.
And so that sets up sort of the
old C++11 vertical, is right there.
With the new types of execution agents,
parallel execution agents do not have
a guarantee for when they will start
until you block on their completion.
But once they've started running,
they have a guarantee of forward progress.
They can execute concurrent algorithms
that are starvation
free, not just lock free.
And that is the most important distinction
with the next class, weakly
parallel execution agents
can only execute algorithms
that are lock free.
That is algorithms,
concurrent algorithms that
are correct, even if threads are
indefinitely victimized by the schedule.
Now one of the things that you
may already have noticed here
is I've staked out this position
that Volta is pretty unique
because Volta's here, and
all other GPUs are here.
Okay.
This, this is not
business as usual at all.
This was a concerted effort by engineers
who wanted to move the state of the art
for vector thread execution.
We literally threw out our entire
implementation of GPU threads.
We took all that RTL code,
that part of our code,
that unit, tossed it out, and
did something completely new
and when we designed it, we
were basically making sure,
every step along the way, that
we were conforming with the C++ standard.
We were, we were quite literally
quoting the C++ standard
at each other while
designing hardware, okay.
Okay, and that actually makes Volta
very much alone of its kind.
The tradition of making GPU like threads
is not all that new.
It's almost as old as I am.
But across that vast span of time,
across that span of
time, really nobody else
managed to write, to make
a system that can execute
starvation free concurrent
algorithms at vector efficiency.
So that is completely new.
Okay.
And that allows us to clarify SIMT C++.
SIMT is single instruction
multiple thread,
that's the name that NVIDIA has given
to this type of architecture, and
it's in comparison with SIMD, alright.
Alright, now I'm going to
show you that bad code.
We're there!
We're at that place where
I have the bad code.
So I'm going to show you some bad code
that doesn't really have atomic&amp;lt;T&amp;gt; in it.
So now we can clarify this.
Here I have a piece of
code, which is largely
short of an annotation or
two, it's basically CUDA 9.
It's physically valid CUDA 9.
Actually, as written
here, it's valid CUDA,
for any revision of CUDA
above 1.1 syntactically,
but semantically, only 9.0,
and only on Volta.
And the reason for this is, when
you're running on a vectorized processor,
whether it's SIMD with a compiler
that inserted predication,
or whether it's SIMT with either
a previous implementation,
which was stack based or
this new implementation
which is, I'm not telling you.
The problem is that the moral scheduler
that runs these threads makes a choice
after one of the thread exits the loop.
Okay, so this is just a spin lock,
I'm spinning on CAS,
someone's going to succeed
in switching the lock
from unlocked to locked.
And then they're gonna exit this loop.
When they exit this loop,
the machine has two paths.
I can either execute the
other threads in the loop,
or I can execute the critical section.
The problem is, any static
compilation to predication,
or our previous architecture would commit
to one of the two choices,
and then it would have
to run that path to completion.
Which in this case here means,
you potentially could
choose the while loop.
The while loop is never
going to succeed again,
because the thread that owns the lock
is attempting to execute
the critical section,
and it's being victimized indefinitely.
So we never make it to the unlock.
If you wrote this code in any revision
of CUDA from 1.1 to, even including 9.0,
as long as you don't have a Volta,
then you could lock your GPU with this.
I mean, you could lock
anybody's GPU with this.
You could lock a compiler to
predicated SIMD with this.
Okay.
Alright, so what Volta
just did is pretty huge.
And although I can't share
the details of how we did it,
it's actually pretty wonderful in use.
You write this code, it's
just things you expect
work in C++ and they
just do, without fanfare.
It's like, you wrote it, compiled, it ran.
You're like, big deal.
Well, big deal!
This never works on a processor
with this kind of efficiency before.
Okay, alright there are still some
open issues with execution as well.
I talked about it, I had
some open issues with memory,
we have some open issues with execution
and this is where we're gonna talk
about degree of conformance
or not that we have.
So basically, here's
the short version of it.
I don't want to give
an extra 30 minute talk
about all of the things that don't work,
but basically the state of heterogeneity
hasn't changed in quite a long time,
and I'm not here to announce anything
new with respect to that.
That is basically, there's an annotation
in CUDA 9 that takes the
form of __host or __device
annotation where you say which device
to compile the function to, or both.
And this, in combination with the lack
of integration between the compilers.
The NVIDIA compiler and GTC in particular,
essentially do their jobs separately.
They're not very deeply integrated.
This is basically what's behind
what's remaining in conformance issues.
So, RTTI because we don't know
the internal implementation of RTTI
of every other compiler that we work with.
So, that's just an example.
But more annoyingly for me, and probably
for many of you in the room,
is a very simple thing
like std vector operator[]
doesn't work and it's because it's
just not annotated with device.
If it were, it would work.
Because there's nothing
complicated in here,
but it's not, so therefore it
doesn't work and it's rejected.
My personal experience,
writing CUDA all summer,
as someone who basically wrote,
essentially no CUDA before this summer.
I pretty much waited
for Volta to write CUDA.
(audience laughing)
All I need to know is that I need
to stick device in functions,
and then everything will work.
And practically everything else does work.
Now, most language features
you can think of just work.
Okay, so here's the
execution progress report.
On the issue of forward progress,
we have greatly clarified
SIMT progress with Volta.
And we simultaneously greatly
clarified progress in C++17.
That's not an accident.
So that allowed us to
clarify the meaning SIMT C++.
That's major progress.
And then on heterogeneity, stay tuned.
Alright, so let me wrap
up, let me leave you
with a wrap up on Volta C++ architecture.
This is really the model that I
want you guys to have in mind.
We are sometimes, justly or unjustly,
people are sometimes upset that we don't
give full specs for our systems.
Other companies give full
specs for their systems.
We reserve the right to be
innovative in certain ways
that are just not compatible
with giving everybody our specs.
But I want everybody to know this.
That our platform is
really a scalar platform.
Like this, earlier I said compiling down
to the metal, meaning element metal.
For this slide, I'm gonna
say if you're compiling C++
to metal, meaning instruction set,
above this line, NVIDIA GPUs
are scalar threaded machines.
They are not SIMD machines.
We do not present the
SIMD abstraction at all.
In fact, our instruction
set, both the abstract
instruction set that you have
access to called PTX, P T X,
and the concrete instruction set,
which I work with on a daily basis,
both of them are
inherently scalar threads.
We don't have FMA times
32 with a predicate.
We have FMA of a scalar,
and each thread executes
its own copy of that FMA of a scalar.
And it's the job of this layer
that we have in the silicon
that I'm labeling thread virtualization.
It's the job of this layer to figure out
how these operations can begin so
that the power saving is realized.
Without having any undue consequence
on how you would build the
software stack above it.
And I'm actually incredibly
proud of this approach.
I think that it supports really clean
software stacks, to map to our machine.
Scalar compiler optimizations are old
and well-trusted and well-verified.
And deliver the highest
performance, really.
We would not want to walk
away from scalar optimizations
because we prematurely
committed to a vector execution.
Okay.
So, to these layers of abstraction,
today most of C++14 is
supported by CUDA 9.
There is, just like with
the GCC and Clang web pages,
there is a table of features and
which papers they took the feature from.
We have the exact same thing.
So you can come on the
NVIDIA developer website
and see what the future support
matrix is for our compiler.
It is both the worst
of any major compiler,
and by far the best of any GPU
or throughput processor architecture.
So, it's somewhere in
this, well in this middle.
Okay.
Alright so we have that, and that sits
on top of an array of scalar threads.
And the GPU is a scalar architecture.
Okay.
At this point, I actually
have a really large number
of backup slides that
dive into various kinds
of details about Volta, specifically.
But I'd like you to drive my going there
with your questions.
Yes sir.
(audience member asking question)
So what do I mean when I
say vector [] doesn't work.
What you'll get is, you'll
get a compiler error.
It'll say, so when you write
your code, you're gonna
some functions, you're
gonna designate some
of them as being GPU functions.
When you write code
inside of that function,
you can legitimately pass in a reference
to a std vector, or a
pointer to a std vector,
that's fine.
But then, when you attempt to invoke
the square bracket
operator, on this vector,
you're gonna end up with a
compile time error that says
operator::std::vector::operator[]
not device qualified
and that effectively
means that the compiler
is not going to generate a
device version of that byte code,
the final executable byte
code for this function,
for the device, and therefore
you can't invoke it.
Now this is an experiment
that I did years ago.
I opened up, I have no
idea if that violates
terms of service, but anyway I opened up
my Visual Studio headers
and I just started
typing down device like
all over the place.
And it worked just fine.
Right, now here there be dragons.
Even if you were to do that,
standard library functions
have no limits on like how much
special stuff they can use internally.
They can use x86 inline
assembly for all they care.
And then at that point me sticking device
on it is not gonna help.
So okay, so that's what I mean.
There's actually a fairly
big pile of problems there.
C++ assumes all the code is going
to be generated to one architecture.
The presence of another architecture
in the abstract machine, at
the moment, messes things up.
It would take fairly significant
compiler studies to figure out new ways
of compiling the code
to avoid this problem.
Peter, and then you.
- [Peter] Yeah, I was just commenting,
so what you are actually saying
is that the NVCC is that the name?
- Yes, NVCC.
- [Peter] It has a lot
of range for improvement
by simplifying its syntax.
Getting rid of the distinctions.
- So, so... sure, it's
difficult to do though.
Yes, yes.
- [Peter] Yeah.
- [Peter] If it were easy,
we would have done it.
- In 10 years, you would think so, yeah.
- [Male Voice] If
transistors are abandoned,
then why only 16 gig of memory
or more realistically is a chance to get
at least let's say at
least 200 kilos per thread
in foreseeable future.
- Is there a chance to
get 200 gigs per thread
in the foreseeable future?
- [Male Voice] 200 kilos per thread.
You said it's 100 kilos per thread.
- Yeah.
- [Male Voice] Is there
a chance for this number
to increase, or it will
be the other way around.
Number of cores increasing
and memory still...
- Okay so is that ratio likely
to increase or decrease?
I think that ratio's likely
to stay about the same.
And it does vary across the product lines.
At the end of the day,
the number of threads
and the amount of memory
is dictated by the fact that the GPU
has a very real day job.
And it needs to be
profitable at that day job.
So these numbers are picked based
on availability of the memories
from various memory manufacturers,
and how much yield we expect to have,
and then there's some performance
that results out of that,
and then there's a very
complicated multi-variable
optimization that decides what the mix is.
You can imagine that that ratio
could swing by two to 4x up,
probably not worse than 2x down.
It's in that region.
It's in that region but
I don't see a near point
in time where it would
be drastically different.
Because this other day
job that the GPU has
likes this balance.
Yes sir.
(audience member speaking quietly)
Could we ship NVCC with a standard library
that has device everywhere.
We could.
It's a pretty monumental
task in its own right.
It also would affect customers who want
to use a particular standard library.
Like you would have to like
my standard library now.
You couldn't use another one.
So, it's a difficult choice.
Yes sir.
(audience member asking question)
Can I speak to the differences
between SIMD and SIMT?
Yeah.
Well, that could be another talk.
So the main, the main difference is
is this D versus T distinction.
Do you imagine that the
operation is just mutating
several pieces of data
owned by the same thread?
Or are you executing multiple
threads as part of this?
Now the history of how GPUs came
to this is long and interesting,
but it basically goes back to the 80s
to work that Pixar was doing.
Pixar developed this shading
language called Renderman.
And in Renderman, you
wrote in a C like language,
the math that each color
channel would apply.
And then your program
would effectively get run
four threads wide,
because you were writing
in the C like language, you
were writing the scalar chain
of evaluation for one color channel,
and then Pixar wanted
to run this in games.
And that sort of set the tone
for what later became pixel shaders.
And then GPUs needed to run that,
they needed to run this execution model
and so our view has never been
that you're talking about multiple pieces
of data in one thread.
Our view has always
been that you're running
multiple threads, and then you
want that vector efficiency.
So you add a little bit more hardware.
There's no question that SIMT takes some
extra hardware consideration that you
would not have when your view is SIMD.
And so that may seem to some group
of people as an undue task.
There's definitely people who
want the absolute
barest, simplest silicon.
But in our view, and I would say
in the view of the Pixar
engineers who designed Renderman,
whether they really formed
this thought back then,
but I'm gonna ascribe it to them anyway.
They saw that there's actually a balance
between what is an
elegant programming model
that programmers will understand
easily and will feel comfortable,
and that hardware minimum cost,
and this slight extra tax on the hardware,
which to be quite honest, it's
like a percent or something,
that extra tax on the
hardware is well worth it
when you consider how much better
you've made programmer's lives.
And so in general, people are afraid of,
were in the past, afraid
of porting to the GPU
because it looked like
it was a lot of work.
But most of the feedback we've received
is that once you get
over that initial fear,
you realize that it's
actually quiet easier
to get higher performance using
that programming abstraction relative
to __ built-ins in your
code, top to bottom.
Okay.
Another question.
Speak up.
- [Audience Member] So you said that
with Volta plus HMM plus NVLink,
memory truly tastes like memory.
- Yes.
- [Audience Member] Okay, and
then you made an argument about TLS and
this very skewed ratio
- Yes.
- [Audience Member] Of 100k per thread.
So does that mean memory, does
it truly taste like memory?
- Right, so, so the question is
if Linux and HMM and NVLink and Volta,
all of that makes memory
taste like memory,
how do we reconcile that
with the TLS question?
You can look at it as an issue between,
there are four storage durations in C++.
There is static, dynamic,
thread, and automatic.
And static and dynamic storage durations
are fully addressed by Linux,
HMM, Volta, and NVLink.
The thread and automatic storage
durations have some other issues.
So at the moment, we don't allocate
any thread storage duration.
We really don't know what to
do so we don't allocate it.
And if you try to access thread_local
from a GPU thread, it would just bomb.
And certainly, my
preference is to find a way
to explain this to you
in an acceptable way
in clause 4.7 maybe.
And then for automatics,
they work really great
on the GPU actually,
right up until the time
where you try to share a pointer
with one of your buddies.
But you can take pointers,
and the pointers do work.
And you can pass by reference
in a thread of execution.
There's no problem with that.
You just can't pass it with someone else.
Okay, so your question is is memory
not really tasting like memory.
I guess, I'm going to narrow the claim
to static and dynamic storage durations
really do taste like memory.
Okay, you were slightly first.
- [Audience Member] Yeah
I have one question.
We talked about 100k thread.
- Yeah.
- [Audience Member] If we
have a single code block
that all the threads share, when we've got
multiple data areas in each thread,
the 100k refers to exactly what?
The amount of data in every thread?
(audience noise covers speaker)
Does the thread take any of that?
Or is that a totally separate block
(door slam covers speaker)
- So the ratio that I showed is really,
just how much memory is on the board,
and how many threads are on
the board, ratio of that.
If your threads really
only needed 20KB each,
then everything else is basically
just shared state, right.
- [Audience Member] How much memory,
how big a program can this
(speaker voice drops in volume)
- Yes, yes it's 100k
data memory per thread.
The program can be arbitrarily large.
It can be arbitrarily large
and run arbitrarily long
and run an arbitrary control flow.
There's no...
Some people have a notion
of GPUs can't do recursion
or they can't do functions or
they can't do function pointers,
but like that hasn't been true
for NVIDIA for like 10 years.
So, you can have an arbitrarily
large program that's arbitrarily gnarly.
It's just the amount of per thread state.
(Audience member asking question)
The program memory will
live in one of the two.
But it doesn't, none of that animates,
it's because it's migrateable.
It could start in one and move
to the other a few
microseconds into the job.
You could connotically think of it
as being in the GPU memory, but
it's relatively small
compared to that 16GB.
I'm not sure how many people
have gigabyte object class.
I know, actually NVIDIA has some,
but they're pretty rare.
Okay, alright.
That's it.
I'll take your questions
offline, at this point.
I'll stick around.
(audience applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>