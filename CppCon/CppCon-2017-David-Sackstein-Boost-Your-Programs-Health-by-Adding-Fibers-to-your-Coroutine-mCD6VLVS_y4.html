<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: David Sackstein “Boost Your Program’s Health by Adding Fibers to your Coroutine” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: David Sackstein “Boost Your Program’s Health by Adding Fibers to your Coroutine” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: David Sackstein “Boost Your Program’s Health by Adding Fibers to your Coroutine”</b></h2><h5 class="post__date">2017-10-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mCD6VLVS_y4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- My name is David Sackstein.
I'm a freelance programmer
and an instructor.
I am personally quite
fascinated with the fact
that in the last 10 years or so,
C++ has gone through a revolution
and has really become a
cutting-edge modern language,
we can do wonderful things
that we could never do before.
Today I'd like to talk to you about
threads, coroutines, and fibers.
Threads are very well known.
Coroutines are all the rage,
there are quite a number
of talks in this conference
about coroutines.
Fibers are less well
known, and I would like
to try to convince you that you can Boost
your program's health by adding
fibers to your coroutine.
So the agenda for today is as follows.
I'd like to start with an
overview of coroutines,
and to discuss two high
level implementations,
the stackless and the stackful.
These two implementations are similar
in the way they are used, but
there are subtle differences.
And we're going to be discussing
the coroutine TS and Boost Coroutine2.
Then we're going to talk
about specific use cases.
One is generators, it's a family
of use cases, and asynchronous APIs.
The asynchronous APIs will
lead us on to talk about
threads, and how threads are
used, or should not be used,
for asynchronous APIs, and
then how fibers can help
with some of the problems
that we have with threads.
And then, we will talk about
the Boost Fiber library itself.
I'll show you a few examples
of how it can be used.
As I mentioned in the abstract,
it is actually very convenient
to use the Boost library,
because the semantics
of many of the objects
that you'll find there, are very similar
to those of the stood thread APIs.
And then we're going to go through
a series of demonstrations, basically
a series of projects in which
I am going to demonstrate
the use of threads for asynchronous API,
followed by the use of
asynchronous I/O without threads.
Followed by implementation of coroutines,
and I'll make mention
of using coroutines from
Boost and coroutines
from the coroutines TS.
And finally to see how we can
actually use fibers in that project.
And in the conclusion I would like
to talk a little bit about
when it is useful to use
coroutines, and when it would
be more useful to use fibers.
Now you can imagine already that
the use of threads will not be advised,
at least when we're
using asynchronous I/O.
I mean threads are not
something not to be used,
but they're to be used in a specific way
in combination with either
coroutines or with fibers.
So we'll talk about the differences
between all of those technologies.
And finally, hopefully there'll
be time for questions and a quick summary.
If there are questions
during the presentation
feel free to ask, I
may ask you to postpone
the question or the answer to the end,
because we are fairly short for time.
Before I begin, I would like to thank
Oliver Kowalke and Nat Goodspeed
who both assisted me in preparing
for this presentation,
answered numerous questions
on email, and they are a
great source of information.
Oliver is the author of the Boost context,
Boost coroutines and the
Blues fiber libraries.
And Nat Goodspeed has been a contributor
to these libraries,
and he's also an author
of a number of proposals to
the CDP standard committee,
standardization committee,
for fibers and for coroutines.
He is also a very enthusiastic
speaker about these technologies,
and he speaks at CPPNow and CPPCon
I'll mention some of the examples
that Nat provides during the presentation.
Okay, so more information you can find...
I would recommend, the best source
of information actually is
the Boost Fiber documentation
and the Boost code TS documentation.
There are lots of examples there,
and the tutorials are excellent.
But in addition you can see
the code behind the slides
at this location that
I've posted on GitHub.
I hope it's available now, and if not,
I'll make sure it is after the
conversation, after the talk;
and you can of course
contact me at this email,
and comments on the presentation,
questions all are welcome.
So let's begin with coroutines.
Everyone knows the
definition of the coroutine.
Coroutines are generalizations
of subroutines.
Unlike subroutines they can be suspended
and they can be resumed;
and when they are resumed,
they are resumed with
the same execution state,
with which they were suspended.
That means they'll be resumed
at the point of suspension,
and any local variables
that were originally present
and alive remain alive
and in the same state
when the coroutine is suspended.
And maybe an important
other feature which is
important to mention is
the fact that the objects
that are created on the
stack in the coroutine are
only disposed of using the RAII pattern
when the final return
from the coroutine occurs.
So in a way, a coroutine looks like
a function, but what's happening behind
the scenes is suspension and resume.
And the use cases of a
coroutine that I want
to talk about here our
generators and parsers.
I will mention briefly pulling
from visitors as one of the examples.
And a second pattern is
the asynchronous API,
so that would be the focus
of most of our discussion
asynchronous API's free of callbacks.
And then of course, there are combinations
of the above; there are such thing
as an asynchronous
generator, which generates
sequences of information
that some of those
or all of those are themselves
awaited upon as they are asynchronous.
What is common to all of the above?
And this helped me understand
what coroutines are all about.
What is common to all of these
patterns different as they may seem is
that they help us invert control.
We are now code that is typically governed
or dominated by callbacks to be combined
with the initiating functions and to look
together as if they are one function,
and that is really what inversion
of control is all about.
If we take, for instance,
the generator pattern,
within the generator
we have the pattern is
basically a producer of information
a sequence information and a consumer.
The problem is that the producer
and the consumer both want
to be in the driving seat.
The producer wants to run a function,
update its state,
generate some information,
and send it out using a function call,
a callback for instance
or a function call.
It doesn't like to be called back
for more information each time,
because if it's being called back
it needs to restore its state.
It needs to store state, restore its state
and continue to where it left off.
That's not a very convenient
programming model for the producer,
but on the other hand, the
consumer has the same interest.
The consumer is doing his own thing,
and then from time to
time it needs to get hold
of some information so it would like to
make a function call to call upon the
producer to provide that information.
So what we have here is
a conflict of interest
one of them wants to pull,
one of them wants to push,
and both of them want to
do it by a function call.
How can a function call behave
both as a push and as a pull?
And the answer is a generator,
which can be implemented using coroutines.
We will see later on that
a similar pattern arises
out of the requirement to
make asynchronous API simpler.
Okay, so we have two
implementations of coroutines
that I'm going to talk about to you.
One is the coroutine TS which is
in the process of being standardized,
and the other is Boost Coroutine2.
Boost Coroutine2, actually
Boost Coroutine has
been upgraded, and now is deprecated.
I recommend you use Boost Coroutine2,
which has a similar API
with some limitations.
The difference between
these two implementations,
I won't go into the details
of the implementation,
but I would like to say it,
because it is important
for the usage patterns
is that the coroutines TS
takes the stackless approach,
and another way of saying that
or an implication of that is
that it suspends by return.
What happens is when
you enter a coroutine,
behind the scenes the
compiler is generating
a representation of the
state of the coroutine,
and passing it back in what
we call a future-like object
to the caller; and the
caller needs to be aware
of the fact that this future-like object
will either represent a result
or a potential result,
which may eventually be
an exception or it could
be an actual result.
And on the other hand...
So this what we call suspend by return,
because when we suspend
we rely on the fact
that the information
of the state is passed
back to the caller, and
there is an interaction
between the caller and the
coroutine using this state.
On the other hand, Boost Coroutine2 uses
a stackful approach;
what actually happens is
we allocate a stack for the use
of the coroutine which
allows the coroutine
to run as it were as a regular function
calling other functions and so on,
and the suspend is by call.
At any point within that coroutine,
either in the function
or in a nested function
that it calls, the coroutine can suspend.
When we use stacks
suspension means we are doing
a context switch, so the processor is
running one stack, suddenly it is
switched away and it
moves to another stack.
The same thread continues to run
on a separate stack,
which is basically the
state of the execution but that can
operate at any level, it can be called
from within a nested function
or a nested nested function.
It doesn't have to be called
by the top level with a special keyword.
And this has implications,
which we will discuss
in just a moment because I
want to focus on the use cases.
Let's talk about generators, and see
how they are implemented, how they're used
in both of these implementations,
both in the Boost and in the coroutine TS.
So here is a typical generator
using Boost Coroutine2.
Now this may not be
similar, this may not be
well known to all of you, because we're
all concentrating on the coroutines TS,
but I think you'll find
that it's very similar.
What you see here is...
First of all in order to
use the Boost coroutines
you need to include the headers.
You can use all dot HPP, which includes
all the relevant headers for convenience,
and I've also added here
two template aliases,
so that I can use pull type and push type.
These are asymmetric coroutines.
So there is a puller
and there is a pusher,
and there is a particular
relationship between them.
They both have roles.
What I'm creating here is the pull type,
the function generates integers
that cause a pull type,
creates a pull type, and the
pull type is our generator.
To create this object we call
a constructor as you can see,
which takes a lambda, a callable object,
and the callable object takes a reference,
an implicitly created sink
object or push type object.
And when I say implicitly created,
you don't have to create it.
It is created for you; you can use it
within your callable
function to yield control
and to provide data outwards
towards the pull type.
So all of that boilerplate
code is done for you.
What you really need to concentrate is
on what goes on inside that lambda.
You have a sink, which you can invoke,
you can push, just like a function call.
And this is where inversion
of control has worked.
From inside the generator the consumer has
its own state within this callable object,
and it uses the sink object to push data,
rather than have to be
called back all the time.
So it feels it's in control.
What actually is happening
is that sink object is
generating down under a
suspension of this stack
and a context switch to another stack,
which basically is the
caller at the consumer side.
Okay, so this is how it's
done with Boost Coroutine2.
With the coroutine TS,
slightly shorter syntax,
in order to use coroutine TS today,
as it's an experimental stage
within Visual Studio 2017
you have to include a
switch, the slash await
in your command line, and then make
the appropriate include, which are
currently under the
experimental namespace.
So I've included here
experimental generator,
which is one of the
patterns within that space.
And here again I've used a template alias
to make use of the generator easier.
The generator is an object
that is provided for you.
All you need to do is
invoke the co-yield keyword.
The coroutine TS defines three keywords,
three new keywords for language,
co-yield, co-return and co-await.
I won't discuss them all here,
I going to mention co-await later on.
For the moment as we're discussing
generators co-yield is sufficient.
So this is a generator;
now I want to point
out something here about the objects
that we are creating
and that we are using.
Which of these objects that we've seen
are actually first class objects?
Can they be passed around
by reference by address?
And the answer is that
for both implementations
the generator is a first class object.
You can pass it around, and for as long
as it's an i.d. you can pass around
references to it fairly easily.
This enables you to chain generators.
So here is a chained generator using
the Boost Coroutine Library; here we have
the new generator called
GenerateSquares using
the same pattern again, whereby it creates
an object that takes a
lambda and that lambda,
as you can see, it takes an
implicitly created sink object,
which is a push type,
but you could also see
that it can take by reference,
it can capture by
reference, a parameter that
was passed in which is our generator.
Generators in Boost
Coroutine are pull types,
so that another source
could be, for instance,
the result of the previous call
that I made to generate integers,
and I can use it freely
inside the generator,
the new generator, I can iterate over it,
and then I can finally
call the sink object,
which is how I communicate with my code.
Similarly we can do the
same with the coroutine TS.
The generator, it's an object,
we can pass it by reference,
and we can chain generators in this way.
Again we have to use the co-yield keyword.
So the generator itself
is a first class object,
but what about that sink object?
The thing that we are pushing,
is that a first class object?
Can that be passed around by parameters,
by reference during the
life of the coroutine?
And here there's a difference
between the two implementations.
When you are using Boost Coroutine2,
you receive that object by reference,
which is the sink
object, you can then pass
that along to a nested
function, as long as you're
still within the scope of
your lambda where it is alive.
You can make further calls passing
in that sink object any nest it's called,
and then take that single object and push
outwards any data it wants to create.
So you can create nested generators
from within generators very easily.
However unfortunately with
the stackless implementation
that we're adopting or adopting
for coroutines in C++,
here is a limitation.
If you want to generate from
within a nested function,
you need to call co-await
in that nested function.
To call a co-await in
a nested function means
that function itself must be a coroutine;
and therefore, your coroutine needs
to communicate like with the coroutine.
Typically what you will find is
that if you have nested generation,
you have to iterate
over a nested generator,
and this pattern is
common for, for instance,
the Python iterators and in C sharp,
when you use yield, you also cannot yield
from an inner function, you have to create
a new enumerable and iterate
in the upper function,
outer function over the enumerable
generated by the inner one.
So this is a small limitation,
and it's important to understand
when you're selecting the
technology that you need.
Okay, so this is what you
can do with Boost Coroutine2.
It's a very simple example,
but it does demonstrate the point.
If you have your coroutine
called GenerateSquares,
and you find that it's quite complex,
and there are lots of loops there,
and you would like to somehow rather
extract functions to make it more simple.
What I've done here is show
that I can extract a method,
which actually does the internal loop,
and all I need to do
to make that happen is
to pass in by reference the sink object,
and then iterate over that internally.
So call that internally
inside the nested loop.
This is something that
you will not be able to do
with resumable functions in the standard
unless this generated,
this extracted method is
itself a coroutine, and then you have
to loop over it in the outer function.
- [Audience Member] Question?
- Yeah.
- [Audience Member] What is the
extracted method returning there?
- It's itself, all right, you're right.
As it so happens it doesn't
need to return that.
This is an important point,
yes, it's actually mistake.
This is actually my point,
and I'm glad you pointed it out.
The extracted method doesn't
need to return anything.
Right, it's sufficient for
it to be in a nested call,
and to make calls into the sink
object; thank you very much.
So let's talk about the implications
of the stackless and the stackful.
Okay, a resumable function,
which is stackless,
as the coroutine TS is posing requires
much less memory on an initial call.
You're not allocating a stack,
you are allocating only enough information
to capture the state of that function
that is the body of the coroutine.
In some cases as Gore has demonstrated
in numerous articles and in his talks,
that heap allocation can be alive even,
it can even be made a stack allocation.
And in some cases when
the compiler can see
all of the code the
generator and producer,
it can actually inline all of the code
and the coroutine disappears.
So you have optimizations
that are possible,
especially with the advanced compilers
that will adopt this feature, but are
not possible or are much more difficult
to implement maybe with
a stackful coroutine.
However the limitation is
that when generators are
nested, nested iterations are required.
That's something we've just seen,
and as they suspend by
return, just as I can go...
All functions that invoke a coroutine must
be aware that the caller may suspend,
and that's another point
that I'd like to make
about the differences
between the two, yes?
- [Gore] You can use
a recursive generator,
which allows you to use other generators.
So you don't have to have
it on necessary iterations.
- Okay I hadn't seen that, and I will
take a look, thank you very much.
So what Gore is pointing out is
that it is possible to
do nested generation
without necessarily
creating a new coroutine,
is that is that what you're saying?
Okay, I will certainly
take a look at that.
The second point that I'd like
to make about the differences
are the fact that when you
use stackful coroutines,
you are actually creating a function
which looks like any
function any other function;
and therefore, when you're integrating
with a third-party library,
where you don't control
the code you have much more options,
or it's much easier to
make that integration.
Now Nat Goodspeed has
two examples for this.
One example he provides
in his C++ Now 2016 talk
called Pulling Visitors,
another example is
on the Boost Coroutine
Library documentation,
which I'll mention here,
it's simpler to explain,
but there are examples
of a similar feature.
So the example is passing XML
with the SAX, the SAX API.
As you all know if you're reading XML,
and you want to stream that XML,
you don't want to read
in the whole XML object
in order to see the first node.
And the way you can achieve
that is by using SAX.
What you do is, typically in all the
implementations you provide callbacks
for each of the different nodes,
if you're interested in them,
then you make an invocation to SAX to pass
the XML from a string or from some string,
and then it calls back into your functions
and then you get call backs
on each of those nodes.
But what happens if you want
to iterate recursively over those nodes?
That's not something that's easy to do
without coroutines, and this is precisely
an inversion of control
that coroutines can give us.
So what we would like to do
is we would like to create
a generator of nodes, which I can iterate
over in an efficient way,
while still using the SAX API.
The SAX API does not
know about coroutines,
so the callback that you provide cannot be
something that looks any
other than the signature
provided by the SAX library, it doesn't
return a future object or anything that
looks like a future object; and therefore,
to integrate with that SAX parser,
you need somehow rather to do something
that doesn't look like a coroutine.
Considering the fact that
the Boost coroutines allow
you to pass around the sink object,
you could actually store that sink object
or the address of that sink
object in your own class
during the lifetime of the lambda
that you provided to your coroutine.
And then at any point
including in the callback,
you can reference that sink object,
and use it to push data out.
You don't need to sign in any way
that function that
callback to be a coroutine.
It's a regular function,
so for that point of view,
it's much more straightforward to take
an existing framework that uses callbacks
and convert that into a
coroutine enabled API.
Okay, if of course you control the code,
no problem at all, and
you can actually make each
of those callbacks generalized
and make them coroutines.
And then you can invoke
them as coroutines.
So there's another example as I said
of pulling passes, sorry pulling visitors
from Boost graph but the idea is the same.
So that is a generator pattern;
we've demonstrated how that
does an inversion of control.
Now I'd like to talk
about asynchronous I/O,
and how asynchronous I/O
can benefit from coroutines.
The next step is to talk
about threads and fibers.
Okay, but we're still on
coroutines at the moment.
So when I mention asynchronous I/O,
maybe it's a figure of speech.
What I really mean is any
operation that requires a wait.
Something that I can't do anything about,
but I have to wait for, and as we know
in multitasking programming it is
not a good idea to block on such a weight,
especially not to spin lock if
it's going to take some time.
What you prefer to do is
to use an asynchronous API,
if that's provided,
which essentially allows
you to invoke initiating
function and provide
the callback, and then the callback is
then invoked when the completion,
when the operation is complete.
The problem with that is that you get
a chain of callbacks, and your callback,
and your logic, your business
logic, is now split up
into disjoint callback functions.
And furthermore error handling
can be fairly difficult.
Depending on how its managed,
you have to think about the fact
that the last callback that is mentioned
in the chain that is invoked in the chain
cannot throw an exception
necessarily or simply,
because the stack in which it lives
is not the same stack
in which the original
initiating function was invoked.
So the initiating
function can no longer see
that exception in the normal way.
There are many ways to go around that
and of course this can be managed if you
have good code reviews and
you do a lot of testing
You can make sure this will work
for you and this is how things have
been done for many years
asynchronous I/O's are
not a new art but they have
been difficult to implement.
We would like to have this
initiation and callback
turn into something that
looks like a function call,
and again we have a similar
situation to a coroutine.
We would like to be able to instantiate
the function as if it
was a synchronous call
and then continue with the callback
not to have to provide the callback that
would be invoked later but to actually
continue in the same stack or what looks
like the same stack so this is again a
problem with inversion
control it's a classic
scenario where we can
benefit from coroutines.
What's interesting about this particular
scenario and it's different
from generators is
that the callback is not necessarily being
called or invoked by the consumer.
In the case of a generator I didn't go
into the pattern itself
but the interaction
between suspend and resume occurs
between the consumer of the generator
and the caller and and the implementer.
In this case the initial call is made by
the caller it generates the call,
but the callback may
actually be implemented
may be called back from another thread
some pull a thread pull object which is
now being handling a completion.
But that doesn't change the fact
that we can use coroutines to invert
the control and make our coroutine look
like a synchronous function.
So in order to demonstrate that
I'm going to make a short detour and talk
a little bit about Boost Asio,
because Boost Asio is
really a state-of-the-art
library for asynchronous I/O in C++.
I think it's even being discussed to be
proposed to be part of that
part of the C++ standard.
Of course it implements
a particular pattern,
the pattern is a proactive
pattern, it may not suit
every application but if you're using
this this pattern Boost
Asio is an excellent choice.
So a few words about Boost Asio
because my examples will use it.
It provides a wrapper for synchronous
and asynchronous use of sockets
and other operating system objects;
and it shines when you want to use
the asynchronous I/O; you
have the central object
that you want to use with Boost Asio is
the I/O service every object that you want
to use every operating
system object is represented
by a wrapper which takes a reference
to an I/O service, it's a central part
of the of the mechanism,
and the I/O service
has inside it an event loop.
So a let's say a queue of work.
The queue of work can be dispatched
by a call to I/O service run
and every time our service run is called
it looks at the dispatch of the event
queue it detects if there is something
to do and if it does it invokes isn't
that what it's invoking it's a call
of an object it invokes it that could be
the output of a callback it could be a
callback that's being invoked usually
it'll be a callback but under the hood
that callback could be doing many things.
Boost Asio is not aware of that,
and when there's no more work to be done
I/O run exits as long as those workers
being posted back to the event queue,
I/O run continues to work.
And it should be said
that asynchronous API's
can have many different overloads.
Chris Kohlhoff, the author
of this library has provided
a generalized way to invoke asynchronous
operations so what may look like
a simple operation like async read
on a socket can actually be overloaded
in many ways it can be
overloaded the simple
overload is one that takes a callback
and Boost Asio you then calls the callback
when the completion is complete,
when the operation is complete.
But there are other ways of invoking
an asynchronous operation you can
provide a tag which tells
you which tells Boost Asio,
please return a future object
representing the result of
this asynchronous operation
that's supported by the library.
And future objects are very important
because future objects allow
us to await to co-await.
So the example I want to show here,
a very simple example
demonstrates how we can
use Boost Asio and coroutines TS.
Coroutines TS will work seamlessly
with Boost Asio for its asynchronous API.
Okay, so here we have
a very simple function,
let's look at the main,
the main function invoke
creates an object of type I/O service,
and then it forms sleep and then I/O run.
This function sleepy returns immediately.
May not be obvious at the moment,
but it returns immediately,
and immediately
the main function goes into I/O run
and starts servicing the loop.
What's happening inside that,
what's happening inside that event queue?
Well if we look inside this
operation, this function,
you will see that there
are a number of co-awaits.
Each of these co-awaits is calling
an asynchronous operation,
this is a timer object.
I want you to ignore for one moment
the first line of that function.
This is a detail of Boost Asio,
which I did want to go into now,
but the second line creates a timer,
a system timer which
is based on Boost Asio.
The timer is associated
with the I/O service,
so it can post callbacks
to the I/O service.
And then we can configure
the timer to trigger
in one second, and then we can co-await
on the async await function.
What's happening here behind the scenes?
Well they're actually
two I would say general
and specializations that are happening.
First of all, when you
specify this particular tag
Boost Asio use future to an
async operation in Boost Asio.
Miraculously the overload
that is being called
within the Boost Asio library is
an overload that returns a future
and the type of the
future is actually deduced
from the wrath of the other parameters,
but for the moment we're talking
about the function that
returns no other parameters.
And therefore what is
returned is a future void.
And the second specialization is
that the coroutine TS, which is a very
low level implementation actually provides
wrappers for some common usage patterns
one of them being functions
that return futures.
So there is an appropriate
specialization of the coroutine
that allows you to co-await upon a
function that returns a future.
So these two are working
together here to make
the the asynchronous call
to the timer callback appear
to be a synchronous
call, because immediately
after the call await it looks
as though we're continuing,
but what actually is happening is we are
returning immediately
back to the main loop.
The main loop is invoking I/O run,
which inside the Bruce Asio library is
checking for work to be done,
well there is work to
be done because we've
just scheduled a timer
to go off in a second.
When that eventually does get called
the callable object will be invoked
the callable object
that will be invoked is
the callable object provided
by the coroutine TS,
and the coroutine TS sets the future.
Eventually returning the
control to the function
that was originally
instantiated, the coroutine.
So then control continues
to the next section
of the code, it makes a print,
and then it does another co-await.
And each of these co-awaits does the same.
It provides work to the Boost Asio
asynchronous works to the
Boost Asio and suspends.
Control is returned to the loop,
the loop the I/O service
run continues to look
for work, when it finds the work,
it executes that work, which resumes
the coroutine at the point of suspension.
So what I want to take away
from this particular example is
simply that through
specialization of Boost Asio
and the specialization
that is provided for
us by the coroutine TS implementation
in Visual Studio; we can use
Boost Asio in a synchronous way,
even though what is actually
occurring is asynchronous.
You may ask where are
all the threads of this?
I think it's important to mention
that in this case, this is a very simple
Asio application where there's
only one main thread, and I/O run is
being run on that thread.
But in a generalized situation you can
have many threads being instantiated
and blocking on calls to I/O run,
and any of those I/O
run invocations may be
the one that picks up the result
of this timer or any other
asynchronous operation
calls the invocation and invokes
the resume of this function.
So you need to be aware of the fact that
the actual thread that is running
the code may not necessarily be the thread
that instantiated it if you're running
an application that instantiates the
many threads that call I/O
service run for each of them.
This is entirely under your control.
You need to be aware of it when
you re-enter the coroutine.
Okay so at this point
what I'd like to summarize
and say is that we've
talked about patterns
for using coroutines, and I've touched
upon the implications of the differences
between the two implementations
of which we are aware.
One which is the coroutine TS,
which is in the process of standardization
and the other is a library,
the Boost Coroutine2, which I've
shown you again for example.
Are there any questions about this?
Okay so let's go forward and talk about
asynchronous I/O threads and fibers.
To demonstrate how these work together,
or how they contrast I prepared
a sample which actually is made out
of four separate projects the names are
unimportant they will become clear
as I move through the examples.
But basically we have
in this sample we have
an echo server and an echo client.
The application actually
is one application
it creates threads for each of the
clients and what they do is they collect
to the server, they send some information,
and the server sends back exactly
the same information that was sent.
Both of these objects, the
client and the server, echo
what they receive from the server
what they receive from the client.
So it's a very simple application.
Clients are instantiated, and they start
a conversation where they ping-pong data
with the client, with the server.
Underneath the hood there
is a message object,
and the message object I
have intentionally made
a little bit complex
it has two parts to it.
It has a header and it has data payload
in order to read the message you need
to make two I/O calls, you first need
to read eight bytes size T giving you
the size of the data, and then you need
to read the amount of data
specified by that size T.
This will become important later on,
it complicates things a little bit
when reading from the string.
So I want to zoom in
on the messenger object
and show you how that messenger object
would look like in each of the different
technologies that we can
discuss for I/O in this case.
So the first one called AsioSyncThreads,
it doesn't really use Boost
Asio for asynchronous API's.
It's just a simple blocking API,
but in order to be
little bit more efficient
and to be more responsive to connections,
the connection runs in its own server,
runs in its own thread,
and every time a connection
comes it listens for a socket to connect,
and when that socket connects,
it creates a connection object and stores
the connection object
in a separate thread.
That separate thread runs with simple
synchronous calls using Boost Asio.
What's good about this,
okay here is the code.
The messenger read and messenger
write would look similar,
it makes two calls one
to a read of the header
of size T, then it creates
a message of that size,
and then it invokes
Boost Asio again to make
another synchronous call
to provide the data.
If you're wondering where
the error handling is here
implicitly in these calls
if you do not provide
a placeholder to get back the error code,
these overloads, that I'm using here,
will actually throw an exception,
and if that's handled elsewhere
we are fine with this limitation.
It's simple asynchronous
code, sorry synchronous code.
What's good about this?
Well the good thing is the main thread can
continue to be responsive
to new connections,
because all of those synchronous calls
are occurring in a thread.
The bad thing is that creating a thread
is expensive context switching is
expensive and expensive not only in
allocation of stacks but also in the
time it takes to perform a context switch.
I will show you later on a quick slide
from the Boost documentation, Boost fiber
documentation that points out that
context switching in fibers can be,
not always, but can be
three orders of magnitude
faster than content
switching among threads.
So we like to...
So I would say that if
you're only having a few
threads that's fine,
but as soon as you have
a large number of threads
and you want to scale up
it can be practically impossible
to manage that amount.
The context will actually
dominate even if you're
able to connect to create the threads,
the context will dominate
the operation of the server.
So it's good that it's bad.
So what we'd prefer to do
is to do asynchronous I/O.
And we have Boost Asio, we're using
Boost Asio, so Boost Asio gives us
this support what we want to do is use
the completion Handler to get a callback
and then leave immediately to do more
important things now it does fit on the
slide in this special case but this code
is very confusing this is a simple a
relatively simple asynchronous call an
invocation of a reading the message
that I need using Boost asynchronous I/O.
Now who can tell me what is
the first actual I/O function that is
being invoked here where is it?
At the bottom the next piece of code
after I've instantiating that function
of initiated the function occurs in the
callback which actually is up at the top
here is the callback at the top it then
decides whether the read was successful
and if it does then it continues to the
next call where is the second piece of
code that gets executed somewhere in the
middle the async read
somewhere in the middle.
And where do we finally end up?
We're right bang in the middle
where it says, just next to title
reading a message there you see is the
final handler which does the completion
and it moves the message out of this
callback into the corner into the handler.
So it's very convoluted now
a good code review and extensive testing
will help you here, but of course it's
difficult to manage what we would like
to do is we would like to
eliminate those callbacks.
Okay so the good is again
we have the main training is
responsive and it scales well by the way
in this application why
do I say it scales well?
Because if we're using Boost Asio,
we're not creating a connection a
thread for every connection in fact all
we're doing is we're sporting coroutines
sorry all we're doing is we're sporting
asynchronous operations each of those
asynchronous operations
then gets posted to
the event queue and what actually is
working on threads is the I/O service run.
The I/O service run which is
called elsewhere is the one
that is running in a thread and is
handling our connections and depending
on how many times you called I/O service
run on your thread pool that is amount
of threads you're actually working with.
So you can actually separate your
decoupling the number of threads that
you use for servicing your queue and the
number of connections you can be doing
any number of connections
without creating more threads.
So asynchronous I/O is efficient in this
way and it scales well; however callback
code is difficult to manage.
Enter coroutines, what
I'm demonstrating here is
a simple example of intubating Boost
coroutine with Boost Asio and a similar
demonstration could be made using the
coroutines here so I just chose to use
this one because it's less well-known
the coroutines has already shown is
compatible with asynchronous operations
that are capable of returning a future
and Boost Asio is such
an asynchronous API.
So whatever I can show here we could
also show using the coroutines
yes with some modifications.
Another important point that needs to be
mentioned when you're using Boost Asio
you're using callbacks when you use
callbacks the object that is being
called back upon must in short be
alive if it's not alive if it's
been destroyed then of course we'll
get an access violation; so in order to
be able to use Boost Asio correctly
you need to be able to use shared pointers
efficiently share pointers extend using
enable from this shared from this you
can extend the lifetime of the object
that is being called back to make sure
that when the callback arrives the
object is still there to it to services
so that's what you see here.
What you see here is
just the general pattern
of creating an instance of myself
from a pointer to myself a shared pointer
to myself from a pointer to myself and
passing that in to my business logic.
Okay, and the second thing but this is
actually new because I'm using coroutines
it's the spawn function the spawn
function is explicit is...
Specifically for use with coroutines
this function reminds us in a way of the
way we created the generator
with Boost coroutines.
If you remember over the generator
we provided a lambda and there was
an implicit push type that was created
for us that we could use for the context
switching that's been
wrapped in Boost Asio
with a contact yield context it represents
that push title and it's
implicitly created for us.
And we can then use it and pass
it into our business logic.
Our business logic is all inside
the start operation which accepts
in this case by value that yield object.
That's the first part the second part
actually looks very much like
the synchronous code
here is an asynchronous
implementation using coroutines very
much like the synchronous code who can
spot the difference between this
and synchronous code some slides back?
There's the yield caller that's exactly...
Well actually there are two differences
one is that I'm actually using async
read as opposed to async so as opposed
to read and the second is that the last
parameter is a yield object.
Now what that means, what that
means is an implementation
detail there is an overload in Boost Asio
that accepts this yield object it
is in fact a wrapper object created
by Boost Asio for this purpose,
and internally there is that push type
that does the context
fishing for our coroutine.
So what this acting lead is actually
doing is suspending the call to async read
internally inside that call and scheduling
a callback based upon that yield object.
When the callback returns it is posted
to the Boost Asio event
cube which eventually
gets serviced by one of the calls
to service runner to run which then wakes
up the coroutine and continues
to implement it continues to run from that
point onwards so we have our own version
of control we have something that looks
like a generator in fact but actually is
an asynchronous call as you can see,
and it's very simple to manage.
I make two calls one after the other
the code is sequential state is
preserved and I could nest this deep
under into my application it would look
like synchronous code, okay?
And as I said I could make
a similar change to use
the coroutine TS for this operation.
It would require of course that all
of my functions with themselves to be
coroutines but this is my
code I can do what I like.
Okay, so what do we have here we have all
the good advantages of a responsive API.
An API that scales well, a
rotation that scales well,
because I'm not generating
a connection for every
object, sorry a thread
for every connection,
and now the code has organized
that synchronous code.
Okay I want to make a
short detour now and talk
about the Boost library, this is in fact
what we came to listen about.
So onto the Boost Fiber library.
And really it's an
overview, I really recommend
you look into the documentation it's
very well documented and easy to read
and also it should sound very similar
because the semantics do
remind us of the threads.
So fiber is a userland thread;
it is a thread of execution that is
scheduled cooperatively that is
a difference in a fiber,
the main difference
in a fiber and a thread.
The library is organized as coroutines
based on Boost context plus
a manager, plus a scheduler.
Each fiber has its own stack
and underneath there is a Boost context
every fiber has a stack created for it
and there is a capability of switching
context switching between stacks.
There is a problem here
which is often reported
to be a big problem the fact that we have
to allocate stacks but I do want
to point out as for the case of coroutines
the booth context does provide an API
to allow you to customize the
stack allocation strategy.
So you can decide on
the size of the stack,
and you can also decide on
other allocation strategies.
So fibers that are created
on a particular thread
generally run on the same thread.
So if you have two fibers created
on the same thread they will not conflict,
they will not need protection against use
of a common resource; of course,
if you're cooperating with fibers
from different threads all bets are off,
you need to make sure that those threads
those fibers on behalf those threads
do not collide in use of a resource.
The library is based upon a manager.
Okay the fiber can decide to yield itself
cooperatively, and then it invokes
the manager it says to the manager
I've got nothing to do or I want to give
someone else the chance to do
something yield to another fiber.
What the manager does is it
consults with the scheduler,
whose job is to look around for any
other ready fiber on this thread
and then decides which one is the one
that needs to be run and then it passes
control switch control to that file.
There aren't two context switches here
there is only one context switch.
The original fiber
calls the fiber manager,
the fiber manager consults
with the scheduler,
which decides which fiber to run,
and then it makes the function
to context switch to that fiber.
The context switching is direct.
The scheduler algorithm
itself has a default.
The default is a round-robin scheduler.
So it generally tries to treat
all the fibers in a thread fairly.
But you can decide on the scheduler
algorithm yourself, you can implement
your own, and you can use some of the
other options that are
provided with the library.
So for instance, if you want to associate
a priority with certain
fibers you can do so,
and this is completely customizable.
What I would like to show
you is how fibers suspend,
because here's an essential difference
between fibers and threads.
Fibers can suspend in
let's say one of two ways.
They can sleep; there is a static function
just like with a stood thread,
you can stood this thread in sleep.
You can do the same with fibers,
and Boost Fibers this fiber sleep,
and you have a sleep and
you have a sleep for.
Very similar to the thread API.
But you could also use fiber
synchronization objects to suspend.
Mutexes, I'll have to show you a list
of those in a moment, mutexes
and conditional variables and so on.
The important thing to
remember about these fiber
synchronization objects is that they do
not necessarily block the thread.
All they do is suspend control,
and they tell the scheduler
look at the manager.
Look, I've got nothing to do,
and I have to wait for something.
So here I am going to go to sleep,
meaning I want to be suspended.
I don't want to be ready,
go ahead and consult
the scheduler for a different
fiber that should run.
The scheduler then look for
all of those that are ready.
Now depending on what
exactly the fiber requested,
the fiber that made the call may
in fact be the one that is selected.
If it just yields then it remains ready,
and it just gives chance for someone else
to get a chance to be called.
If there's only one
thread, only one fiber,
then it will itself be
called again, immediately.
But if it made the suspension call it says
I don't want to be ready; I will be awoken
at some later time, and meanwhile,
give someone else the chance to run.
So these are the synchronization objects
that you have within the fiber library.
You have mutexes that
are all within the right
namespaces you can see, conditional
variables, barriers, future promise
package tasks as you might expect.
And you also have a buffered channel,
an unbuffered channel, which behave
if you like, like a future,
two sides of the future,
and two sides of this are
like a future and a promise,
but instead of generating one value,
they generate a sequence of values.
And again what's
important to understand is
that these do not block the threads.
There is of course one caveat.
If there are no ready fibers,
if all of the fibers have gone to sleep
or all the fibers have yielded to some
synchronization object then
the synchronizer will say
sorry, I've got nothing to give you.
I'll give you a null fiber;
and the manager will say oh nothing
to run we've got the thread.
And that could of course be woken
up as soon as some fiber becomes ready.
As I mentioned before
context switching is fast.
You can look at these
numbers in more detail
on the Boost Fiber documentation,
but you can see here
two numbers I wanted to point out is
that in this particular
run the stood thread
context switching was in the order
of 52 microseconds whereas the fiber
optimized with the work-stealing
algorithm is three orders
of magnitude faster.
So this is the real benefit.
How do you use a fiber?
You invoke the constructor of fiber
just like you would invoke
the constructor of a thread.
The one difference that
you see here is that
there's an additional
parameter, which is optional,
which tells the fiber manager
whether to immediately resume
this particular fiber that
would be a post-launch value,
or to put it on the ready queue,
and then allow a yield for another fiber
to enable this one to run.
This is an example of a use
of an unbuffered channel.
I won't go into it in too much detail,
but what you can see here is
both the push and the
pop suspend the fiber.
And here again and this is
really what I wanted to come to
is how you can use futures and promises.
Here is an example of an asynchronous call
to Boost Asio, which is a blocking call,
it would look like a blocking call.
Here you have a promise of an error code,
and from that promise I create a future,
and then in the callback
to my read function,
I set the value I could
also set the exception
of that promise, and then
what is our function going to?
It goes and calls future dot get.
This is a blocking call;
we're used to thinking
of these as bad ideas, this
would require a context switch.
But in the context of Boost Fiber,
it doesn't block the thread.
What's happening here is that
the fiber becomes suspended
and immediately the scheduler gets
the opportunity to
select a different fiber,
which may be ready and have work to do
to work instead from
the thread point of view
the controller is just moving
from one function to the other.
There is no entry to
the kernel, and there is
no unnecessary blocking within the kernel.
- [Constituent] So for
this to work the idle
of the service function would
have to be fiber as well right?
- Yes that's an important point,
I won't go into that into much detail,
but if you look at the documentation
for fiber documentation, there is
a suggested implementation
of a specific scheduler,
which does what you say, it calls both I/O
service runs and the yield to allow both
fibers to run and I/O service to run.
So in the final part
of this demonstration,
which as I said you can take from the link
that I gave you earlier, here is a
new messenger read a function which
would seem to be a synchronous
function, it blocks.
But it's fiber friendly, it only blocks
the current fiber any other
fiber can continue to run.
So if you like internally what's
happening is this is like a coroutine;
there is a suspension
and then a resumption.
So the good with a fiber
implementation is you get
all the advantages of
the threadless option.
Right, you don't create
threads per connection
you create fibers per connection,
and the only thing you have
to manage was pointed out here.
You have to make sure that fiber yield is
called at some point in your scheduler.
Okay that's a small
point which I recommend
you look at the documentation
to get more information about.
Okay one final comment about fibers
as opposed to coroutines; in the specific
example of Boost Asio, this is really
a continuation of what you're saying...
You'll recall that I mentioned
that the fiber library actually is
coroutines plus a
scheduler plus a manager.
This manager conflicts a
little bit with Boost Asio.
Boost Asio already has a manager.
One is managing the queue
inside the I/O service,
and the other is managing,
the fiber manager is
managing the queue of ready
objects inside the fiber manager.
And therefore, some adaptation is
required to make sure that both of those
are called in the right place.
So the ideal situation to use a fiber,
as opposed to a coroutine is
where you, yourself are in
control of the main loop.
If you, yourself have your own main loop
and you are handling the asynchronous
operations all you need to do is to call
to intersperse your calls with yields
from time to time to make sure,
and you can fine tune that,
to make sure that the fiber library gets
the opportunity to reschedule new fibers.
Okay, questions?
So I'll try to answer that,
you're using fiber mutex
from two different fibers?
- [Inquirer] On different threads.
- On different threads?
- [Inquirer] Yes.
- Okay, this depends on
which scheduler you're using.
The default scheduler is
a round-robin scheduler,
which operates in the
context of one thread.
So the mutex will suspend any other,
this fiber and yield to any
other fiber on the same thread.
It will not have any effect,
it will not do what you expect it to do
for the fibers running
there now I'm afraid.
Okay, no you would have
to protect yourself
with the thread synchronization objects.
So in other words the
answer to this basically is
you're still running in
the context of a thread.
As long as you're right in
the context of the thread
you don't need to use thread mutex.
You only can use the fiber mutex,
but if you're running in separate threads,
then you're back in thread land.
You have to protect
yourself accordingly, yes.
- [Participant] Coming
back to your use case
where you implement fiber for SAX parser.
- Yes.
- [Participant] I mean, from my experience
in general one of the challenges of using
that approach of just
converting random library
into kind of coroutine using
a coroutine with fibers is
that you essentially fibers
break thread-local storage,
and it happens often actually even though
we see runtime library it happens.
So what was your experience?
- So you're saying that for
you experience with fibers,
fibers break in some way the
use of thread-local storage?
- [Participant] What I'm
saying if you call a library...
- Yes.
- [Participant] Which take
dependence on the thread
local storage, let's say it set something,
and then later on it
expects to the value be
there that the fact that
it's actually the context
of fiber and it could
get moved from one thread
to another thread the writer
will change in the middle.
- Okay so I understand your point.
What you're saying is
that because fibers can
migrate from one thread to the other...
- [Participant] Right.
- There might be a problem
with thread-local storage.
So I think the correct answer to that is
that by default fibers do
not migrate from one thread
to the other, the scheduler, which is
an extension point, the
default scheduler is
a round-robin scheduler among
fibers of the same thread.
So when one fiber yields
it yields to another fiber
in the same thread; and therefore,
they can access thread-local
storage without any problem.
There are schedulers that will allow
you to migrate thread
fibers from one scheduler.
They can still work from other threads
when you do that you have to manage,
for instance, thread-local storage will
be a problem, because you have
to make sure that it is protected.
- [Participant] Right...
- Sorry that it's accessible.
- [Participant] Even there
you have to be careful,
because if you let's say
schedule another fiber,
which could change thread-local storage,
and then they restore back
it still might be a surprise.
So essentially you need certain
guarantees from the libraries...
- But I don't think that's a problem,
because all of the coroutine,
all of the fibers are
running in the same thread.
They're just like functions
running in the same string.
It's like saying the two functions
in a thread might modify.
- [Participant] Well what I'm saying
the library today might work.
Once we put it in a fiber environment,
suddenly you will find surprises.
- Okay what I suggest you have a look at,
I mean maybe you've already seen it...
The options to customize
the scheduler algorithm;
as long as you're skilled
at your scheduler algorithm,
it does not migrate fibers
to different threads.
- [Participant] No, still not enough.
I mean not all the cases
it will solve some.
- Okay, well maybe come to me afterwards,
and we'll discuss that a bit further.
- [Gore] Hi, I want to
warn about use future
as you had on your earlier slide.
Coroutine TS provides
just raw language changes.
- Right.
- [Gore] The freebies of being able
to await on the future
are the things we put
together in Visual Studio,
so that people can play
but in no way please do not use
them in production; because if you look
at the implementation,
there is no future to them.
So what it does, it launches a thread,
and then it will block and get.
So do not use it in production over.
In plan it will not work,
because it is strictly doing the TS,
and thus you will not have
a waiting on the future.
But there will be a talk this week
where we will show how to use
Azure and coroutines
very efficiently, so...
- Okay thank you, so may I just rephrase
that to make sure I understood.
What Gore is saying is
that the coroutines here,
the coroutine TS is
basically a low-level API,
which allows library authors
to use for their own purposes.
And one particular use is the one that is
implemented today by Microsoft
for future returning object,
functions that return futures.
And as you've probably
seen in the implementation,
I looked at that implementation.
It does rely on the ability to
attach a then to the future,
so as long as you're using
the experimental API's
that Microsoft have been
using here then that's fine.
But if you don't have the then,
then it won't necessarily
work, it'll block.
- [Gore] Well in this case,
they use stood future.
- Right.
- [Gore] Which is not in
the experimental namespace,
which does not have the then.
- Right.
- [Gore] And it has very
inefficient way of waiting on it.
So if you take this example, and I ran it
performance would be horrifying but...
- So I need to use stood
experimental futures,
is that correct?
- [Gore] Use Boost future.
- Boost future? What Boost future?
- [Gore] Create an equivalent
of use underscore future
that is defined by Boost Asio that does
the function for the Boost future.
- Right thank you.
- [Gore] The performance
will still not be good,
but it'll be better.
- Thank you very much,
any other questions?
- [Questioner] I was
curious how much overhead
was there from context switching
between the stacks on a Boost Coroutine2?
- So as I mentioned on the
slide there is an overhead,
but it should be efficient as
more or less like a function call.
The context switching is very fast.
It doesn't require an
entry into the kernel,
so that I think is the
most important gain.
- [Group Member] Was there study how much
actually effect fibers have on their
committed memory of the process
as the stack of the
fibers getting committed?
And if fibers are long living?
- I'm not aware of those, the question is
about stack allocation algorithms.
I'm personally not aware of those.
I've been using the default, which is the
fixed stack algorithm, but as I'm sure
you are aware there are ways that you
can customize that; and I'm not aware of
the studies that have been done.
You might want to contact Nat or Oliver,
and I can give you more
information about that.
- [Group Member] Okay, thank you.
- Okay one more question
and a quick summary.
- [Interrogator] Is there any alternative
for Python yield from when you can
propagate result of nested coroutine
to the color of your current coroutine?
- Well actually Gore mentioned earlier,
you are referring to the coroutine TS?
- [Interrogator] Yeah.
- Yeah, maybe Gore can answer that for us,
because he did mention
earlier that it's possible.
- [Gore] So first yield is an expression
not a statement, so if
your library so desire.
You can implement the coroutine,
which for some reason,
would be yielding something back,
kind of like a request
reply; second if you want
to yield from nested, there
are several techniques.
One of them used like channels in go,
and you pass the channel
to whomever you want,
and then when he pushes
the value into the channel,
you can consume it in some
very far, far remote coroutine.
Or there are several open source libraries
that support coroutines, and they have
recursive generators where you can yield
a result of another generator in one go.
You don't have to spin the loop
and extract, and it's very efficient.
- Right, so in other
words when you're using
a channel of some sort, like for instance,
the fiber channel, then basically you're
converting the co-yield
expression to an object
that can be passed around,
and then anyone can
then push to it just like they push
to a push type in coroutines.
- [Gore] Right but if you want to suspend
while it is being processed
that has to be a coroutine.
- Right exactly, okay.
Okay, I will do one more question.
- [Colleague] It'll be really
quick, it's a statement.
Even if you're not using MSCC,
there's at least one third-party library
that provides an
implementation of generator T,
so Louis S. Baker on GitHub.
- Okay using using the coroutine TS?
- [Colleague] Yeah, and
supplies another generator
and a bunch of other primitives, so...
- Okay thank you very much.
So to summarize, I do want to point out
what we've done actually,
we've talked about
coroutines and the usages for coroutines.
We talked about generators,
and about parsers,
and about asynchronous I/O;
the general idea here is
that you see an inversion
of control pattern.
Then we talked about
asynchronous operations using
threads and the benefits of that,
but also the problems related to threads,
and what I proposed here
is the ability to use
fibers where you would
otherwise use threads.
So I hope I've convinced
you that it's a good idea
to take a look at the fiber library,
the Boost Fiber Library,
and you will discover
that in many cases the use of fibers will
boost the performance of your programs
if you use them in addition to coroutines.
Thank you very much.
(audience applauding)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>