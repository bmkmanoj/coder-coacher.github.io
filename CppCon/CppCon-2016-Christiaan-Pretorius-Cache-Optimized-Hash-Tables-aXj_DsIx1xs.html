<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Christiaan Pretorius “Cache Optimized Hash Tables” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Christiaan Pretorius “Cache Optimized Hash Tables” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Christiaan Pretorius “Cache Optimized Hash Tables”</b></h2><h5 class="post__date">2016-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aXj_DsIx1xs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hi everyone I'm Christian I'm from
South Africa and I work for company
called Rachel rabbit today we're going
to speak a bit about cash optimized hash
tables basically just the unordered map
in C++ terms so basically we start why
optimize the CPU catch I mean why
basically since the 30 mm the despite
the disparity between the world the
lightning she of accessing mine memory
and the actual amount of instructions
you can retire within that time frame is
has become very large and parted that
you can probably retire about fifty
instructions for a single my memory
access and why the CPU manufacturers has
tried to mitigate this problem is with
cache memory so basically if CPU cache
memory increases well as you can see me
if you're eat right which is basically a
percentage defined as okay it's like it
from this expanding hundred accesses and
96 of those comes from the cache and
four of the the other four comes from
from my mind memory it will spend
roughly about 96 milliseconds on
accessing the cache and in the other
four thing that is missing it was
spinned 200 milliseconds accessing my
memory so if I add just one percent to
that he tried my overall time that it
typed it took to do the hundred accesses
or drop from around about 200-300 to
about 250 busy slide and so on 25% just
because my efficiency has increased by
1% also the CPU doesn't do much
while it's writing for me maybe because
it doesn't know that doesn't have
instructions to retire and of course in
multi-core and concurrent program
programming really sort of straights
axis in the CPU there's only one memory
bus sending one little sentence true so
basically the city is trying to get some
air so this is why we would like to you
know just get that want the same one or
two percent extra so I'm just going to
talk a bit further about our caches look
basically it's written in memory is
called a cache line and then no more no
more terms so you can call it a block or
a page or whatever basically the normal
cycle is CPU ask so some memory it talks
to the cache controller the cache
controller says yes I asked tomatoes or
if it doesn't and it's full it needs to
be some other page cached inside so it's
it is just the least recently used
algorithm we can assess the scene which
jesus least recently used algorithm and
the associate we're just going to see
you also that associativity which is how
well it actually can access the data
blocks or all the addresses that it
needs is associated with the blocks is a
full fully associative cache
we're not gonna worry about a 2x4 yo6
invite associativity or anything like
that so some of the techniques we can
use to actually make sure that title we
want at that point in time actually sits
in cache is to firstly make sure that
when the cache creates a block that most
of that data that retrieves is actually
stuff that we're interested in doesn't
help I in the pickup guys who achieve
the cache line is about 64 bytes so I'm
only interested in 4 bytes so it falls
out the rest of the cache of memory
which I'm never gonna use and it's going
to cause other stuff to be victims which
I don't want to use obviously another
sort of often overlooked thing I can do
is to just reduce memories because the
less memory you have the better the
chances that cache will actually contain
the memory or the title that you're
interested in also there's some mention
I'm not going to get too deeply into the
particular instruction caches but
obviously if the compiler in lines code
with other card that also gets called
frequently the layer that is frequently
called card will actually wipe some of
the functions in instruction cache as
well I mean then there's other you
callate of reference so when I'm
accessing memory in the cache and it
needs to big stuff I need to make sure
that the stuff is fix stuff are not
going to use in the near future so
that's basically if I was fancy word
called temporal locality of reference
but also all things being equal when
you're accessing memory they're likely
to access the mirror around that memory
again very soon all right so hang a
little bit deeper into blocks and pages
this is this little picture the black or
the gray ports or the memory that I'm
interested in reports or memory that I'm
not interested in so obviously if you
have some it's better to put all the
stuff you need in to continue contiguous
box as you can see there the top part
pointer here okay you can see there the
red spots or interleaving with the cry
box and there obviously I'm retrieving a
lot of memory data that I don't need
where is this one is a bit better it's a
lot less memory that I don't need you
know putting stuff into the cache but
it's up to you I mean each workload has
its own what you call it it's a little
profile of memory that it once in memory
doesn't that it doesn't want and
sometimes this is a bit difficult to
actually decide in which which memory
you're going to choose as a algorithm
designer or data structure coder
you know which which memory you gonna
prefer right so obviously reducing
memory use is always better so you can
see there on the first
in the first in the worst axis in audio
there's some memory extra day that
doesn't fit into the cache line and
obviously that's going to cause another
cache at another moment memory access
and where is the one at the bottom here
might not cause another memory access so
in the worst one we actually waiting the
CPUs doing nothing again so I think for
more memory to be loaded from from Ram
alright so this one if it's possible
this in your little cash algorithm or
data structure or algorithm or whatever
memory access pattern if there are parts
of the memory that you need a lot it
would be click to put all that stuff in
in a contiguous block that will ensure
sure that we will make the likely bit
higher that you're going to be able to
access that bits of memory again but it
all depends on how you what kind of
memory profile you have and how you can
access it so you're sort of a
complicated diagram so we're talking
about a concept of specific optimality
and optimization generality now my
specific optimality I mean it's in some
situations you actually just need a
little bit of memory to fully optimize
that algorithm or loop or whatever
whereas in general the more general your
solution is or algorithm
I can probably contrast that with
iteration souveneir a versus random axis
or binary search on an array the binary
search on an array is a more general
algorithm while the iteration through
the array is a very specific and it
requires less code basically so while
you may not be able to optimize every
little thing in general it doesn't stop
you from actually optimizing the
specific cases for a specific memory
also the cache hierarchy on CPUs as we
know them today if we store those
registers type at the fastest I can
basically keep pace with the CPU with
the full instruction retire right to the
CPU the l1 is a bit larger but it's
about four times slower and so on and so
on and when you get to the LV it's about
ten times slower which is still five
times faster than my memory all right so
hash tables you be so if it might need
to do toxic it open addressing and close
the dressing there's some examples of
algorithms that a useless linear probing
robbing attacking quadratic program
probing is one of the other ones
quadratic program probing is the one
that used in Google sports hash and
Google Dane sighs isn't really open
source that use pure linear probing
because of the requirements for a
uniform distribution of the keys is very
strict but I'll show you why later and
in our post addressing which is its
opposite that is typically
actually I stables you using the STL you
can see that with the chaining and lip
lists we need looking up a specific key
or value very likely to actually do do
lookups or at least to address to memory
loads because of the linked lists that's
associate that was each bucket in there
in the hash table all right so one of
the techniques that people use it's
quite a common one is to basically just
like the so okay before without the I'm
just going to be specifically talking
about linear program probably and linear
probing tiebacks tables because they are
afternoon to be much more cash friendly
than data algorithms so we won't really
look at all the other algorithms in the
close decreasing things except that
there's a bucket because we'll be using
a bucket at a bit later on so the first
first thing after that is looking at so
you're looking at linear probing hash
tables you know it's not any a nice
table is a open addressing type scheme
so the first thing that people can do
which which helps with keeping that you
know improving that cache performance or
the cache efficiency is to use it so
separate keys and values during the
normal ADT type operations you're
accessing the key is a lot more than
actually accessing values if you think
about it so ways to take for instance if
something is in stable only going to use
keys
if you check to see
if you can you raise something all that
stuff are you gonna use keys for the
normal goosey a bit lighter normal
linear programming algorithm there is a
sort of scan sometimes when the ice
table becomes full which I mean
accesskey so it's better to separate the
keys from the values cuz then you know
that if you load something or if the
cash as a medicine it likes a cache line
from my memory the stuff that you're
loading is King stuff that you're
actually interested in right so yes like
yeah that's a good question to put it
this the main thing there is to actually
not the mind you see in that's funny you
always I have one cache nice actually
because it's like a sort of idealized
case where the key is all in one side
and the values are no other side and we
sort of assume that the keys are in the
cache already then you gotta miss only
once when you access the value because
the key is already in the cache but the
thing here is that you're not putting
large values in the cache when they're
all going to cause a lot of cache misses
a spy oh sorry
when I kind of caused it cash to fall up
because it's just extra stuff that
you're putting in the cache Stettin not
really gonna use
look.we any less looking for that 1% of
improvement
not looking for trying to get everything
in the cache we're looking for a small
movement here okay I'm just gonna
explain a bit what happens with linear
probing so you can see that the green
keys or the green box or occupied keys
the blue ones are empty and the other
ones are deleted keys so the little keys
as you can see you have to sign some
states do to each the first problem sort
of on superficially is you have to have
something that keeps the Stipe for each
of those keys the common way of doing
that is just actually using your key
space itself to to set the stage for
those key so you would have deleted key
key so let's say you have a lot of
integers you would typically assign
minus 1 as the deleted key and -2 is
intiki
the occupied keys can be enough anything
that isn't empty or deleted this is
basically our golden sash work so it's a
bit irritating you have to actually look
at what you're gonna do with it
and in sign keys according to that which
is a bit of a pain sometimes now looking
at the linear program probing algorithm
itself you can see there we need to lead
keys that actually get removed from the
problems because if there was a
collision let's say at the beginning
they were two beginning let's say that
key had a collision so that means
so collisions means that if I insert a
key let's take stable of links right and
I want to insert the values of for and
trade off so four and twelve will both
map to the same position in this science
table let's swings for just for our
example and say it's that one that means
that we have to put so we've restored
for we have to put twelve somewhere so
the linear programming problem chooses
the next one and if I add I'd so that
will be that's 20 so so in this example
there was a number of keys which mapped
to a position somewhere here and then a
little bit lighter on the deleted thing
again but this leaves a whole block here
which is so anytime I'm looking for a
key that falls in this book is going to
do a lot of iterations here it's very
costly obviously and also those
iterations has to actually iterate
through that memory and it falls up the
cache was even more stuff that you don't
need and or might not need but it keeps
the CPU very busy so we want to prove
that a bit so this is basically just
so you can see from this diagram we need
to have a very strongly randomized hash
function so that you have enough spices
between the keys so the dose iterations
don't carry on for too long
sorry so we're sort of forced to to make
that stable a bit bigger so that we can
add little spices in and then that
obviously causes worse locality of
reference and makes the tile bigger than
it really needs to be there are some
solutions to some of those problems
especially the I call this clustering
this clustering some of them or
quadratic programming aspera quadratic
programming using Google DHS and there's
also something like Robin the dashing
which helps with that but with quadratic
probing this each time you stop robbing
you increasing to increasing the probing
links quadratically so that's also bad
so the cash because now is also making
large leaps in reading stuff I don't
necessarily need right now
so the locality of reference of
quadratic burn probing is a bit worst
and especially if you want to keep the
type of small and you want to have our
load factor right so here we have some
examples of flow and a height a factor
so that you can see load load factor it
is a few keys per empty space
factor a lot of TNT spice and you can
see it quickly starts generating long
runs or broad clusters which is a bad
thing so to improve that we're gonna
change the actual sites that gets
thought per perky the one is to exchange
the deleted key with the thing called
the collided key and that's basically
where we have this word called collision
factor so collision factor just means
the amount of keys of the table size not
the total size I will just type out of
keys per total keys which are actually
inner kaleidos type when sorry in other
words if I if I've inserted something on
that key and then later on greater -
value which maps to the same key and
inserted somewhere else there may ever
there was a spice it will be counted as
a collision so you can see they were the
collided flags so firstly I actually
just point out that this table is
equivalent to that point so we don't
have deleted easy anymore set of spices
cut open
here is the yellow one so we're gonna
say that one was collided and that one
as well so you can see just by changing
the deleted flag into a collided state
that it's tied into a collided State
we've reduced a bit of the clustering
but of course neither you don't have a
delete key which means how do you stop
searching when she
something is collided and seeing elected
to do is you just make sure all runs or
all probes or of equal lengths so if
there is a collided key it will probe
for pset links but into that a bit later
but basically this improves two things
first you've got a lot more space to put
other stuff in so your iced I will grow
slower and secondly because you're
limiting the runs here that you can scan
the clusters will obviously not exceed
the size of that constant run day where
is that crime is really long there's so
much much softer okay so any questions
but that is not enough we want to
actually improve the memory use a bit
more services where the pocket at the
end of the timer comes from so if I
store a key and I forget that the
previous one year if I store Ricky
they're telling it falls of that spot
but if I store another key that map's to
somewhere there it has to go somewhere
so at that point
usually you would just call a rehash but
we can actually do a bit better and we
can put a little bucket at the end
so whenever those probes are exhausted
we just put a key in the bucket and as
it turns out the the probability of
these things getting full it's really
love so I sort of think you sink for
yourself that
if I insert the key into a into the
position where these eight other cons
for chi other consecutive keys that the
probability of that event occurring is
fairly low that's why this bucket
basically works and then the next thing
we do is to actually take all the states
and instead of using the keys themself
to store the the colors of the states
which would be occupied empty and
collided States we use a separate bitmap
to don't store it in so that but map
becomes sticky in cash because it's
quite small it's like for iPod keys it's
like 32 times smaller and the title
Ashta also the likelihood of that thing
being in cash is a lot although it does
incur a couple of extra processing
cycles to actually do a shift and a load
year before you can actually actually
access a key all right so this the
birthday paradox or the birthday attack
is it is a problem with almost all mesh
titles where the probabilities of T is
colliding becomes quickly very very high
very quickly and possibly a wall on the
side here is that yard it's a good
article to read the Google the birthday
attack article on Wikipedia and then we
can basically just model collisions on a
nice type of the same WiDi model
collisions in security algorithms the
probabilities and stuff is almost the
same or not the same but tile
of area and it makes it easier to reason
about from a functional point of view
than to try and go and look at the
probabilities of linear probing and with
dreaded problem - tables okay it is some
benchmarks so I can see their discards
the in size map - map in blue and that's
the Google wine the false Google wine my
red one is the rabbit services for Fitch
random fits times in nanoseconds in the
sports hash map which is the Google app
it's basically a type of hash map that
is a lot a lot few spices between keys
that's what I call it sports because it
doesn't have a lot of spice wasted so it
actually uses less memory and in the
normal an audit map that's part of this
they all distributions so there you can
see I think the time series is that's a
couple of about 20% or so faster than
the forces one-state million keys but
it's a few can you see the same pattern
reoccur with hundred millions doesn't
matter it just obviously fifteen hundred
million all of that the racial status
times obviously just took two actual
times which comes a bit higher because
you've got more cache misses so this is
just for random access format graphs you
can see the job you've got more like a
50 years even on recent improvement in
my purse and that's my only because of
the
little bit map that we use we don't
actually need to compare our baby keys
we need to very resize of the hashmap
and obviously because it's much smaller
you resize much less because of the
higher light factor that it can
withstand and you can also use more gas
friendly distributions so you don't have
to strongly randomize ologies together
cause you don't really need a try
dispersed focus you've got a collision
but which avoids the scanning of the
keys that okay this is way gets much
better actually
so if you look at cream there that's the
size of our timbering keys in the HDL
map the bluest will go to any size map
and then the Google's porous hash map as
well as rapid eases about assignment
amount of memory a rabbit is little less
even but it's basically the same I just
have to sort of sign that they are some
cranks it's because gravity's a variant
on linear programming it doesn't have to
resize - map from time to time that
means you have to make a copy of -
badminton set all the keys from the old
version and then delete the old version
CF drones in spark so it's not quite as
low even though it looks like a year
it's not quite as low as that Google's
pore size map but it's much better than
all the other ones anyway while it's
actually Foster and everything else okay
so that was that for the sort of give me
an overview of the algorithms so now I
would like to share a bit about out of
there
- my other thing actually works with
your soul so you have some profiling
information did a bit earlier and this
is basically the workload is linked page
from Facebook so you can see there is a
lot of stuff happening here if you look
at vegetative functions see what's going
on here so you've got a lot of logging
is a lot of compression and stuff like
that is so dissipate okay but here is so
that's just some card within the Africa
it is basically in the alligator so
sister a cattle that we write for for
doing some improving a concurrent
performance of existing I with
alligators and basic ass it's sort of
like the my local one of those anyways
but be but basically this a lot of code
be that we can see find mister it's
called a lot so obviously random axis
random read axis is quite important
Network lied and again we can see here
it does some strange things
when we look at code here it's basically
all cache misses so even though we try
to optimize the ice table for for the
cache efficiency due to all the other
things happening memory allocation stuff
being encoded from one buffer to another
one cache misses so quite predominant
anyway so that's basically that port
okay I think I'm sort of finished if
there's any questions that any questions
anything that it's unclear to you I'd
like to maybe a little bit just if I can
get back to this oh you just talk about
deletes you can see that the leads and
replacements also gonna be much faster
in this case cause we are sitting but
we're not actually overwriting the whole
key to signal that thing is missing all
the and but it depends on how you use
dice table and so on okay right any
questions yeah it actually does it has
the normal iterators and other like find
add or insert constitute a riders
all that kind of stuff it supports the
only thing is with the key bears like I
think it's called the value type isn't
quite the same because because of that
split here oh yeah because of that split
he's invited haunting the same memory so
the actual value table is a proxy to the
actual keys but it works in the same way
I just think if you do type inference
and that kind of thing it will actually
not retain what you expect so in that in
that case it's it's not quite
conformant but I'm pretty sure if you
use it in a sort of real system and
we're not gonna have that problem a lot
the thing is that it's also much more
robust against non-uniform distribution
of teeth because what I explained
earlier so it's probably yeah
so so I guess that is why it's not
impossible to guess it's impossible -
yes that's right
that's quick so so I just want to beat
for for the audience future audience so
your question is that if what is the
performance of the hash table with
Visual Studio as function as opposed to
as opposed to just using integers
directly so what I do for this thing is
to just not use the standard - for
integers
I've rewritten the but it's all about
with strings we have a high probability
of birthday attacks usefully the
randomization hash function that comes
with the compiler will stay of the
vision Avesta or whichever one you using
so forth okay so ask you the others one
is a bit complicated to explain so these
two files
- the algorithm is the first phase we
even set a key and there's no collision
the next phase is if there is a
collision it must probe for a new for a
new spice so what it does in that case
is to take the size of the hash table
say say did the - value divided by the
size of that hash table and the negative
number for instance if it's ten times
larger than a nice table in 1811 and so
on but before I I don't use that number
directly as the offset for the probe
with a probable scored because you can't
do that but then you still have a lord -
Dre spice which in a potential attacker
can use to actually try and follow his
buckets up so it we square so that port
is actually like a quadratic Esther ball
so that means that the total address
space that the attacker has to actually
cause you a stable to file or expand
beyond available memory is a square root
of the type of spice so it's a bit
thinner inside instead if you're using
64-bit Keys title address by stocker can
exploit all the funny distribution or
whatever is then 32 bits cuz it's almost
of the total address space which is sort
of the same strategy that they using the
team size map as well okay but also two
questions okay so rabbit doesn't have a
fixed load factor and what it does is -
at the end here
if that little box at the end is
exhausted that's the trigger for
reaction so we don't I know in Google
the large factories off because that's
sort of optimal value for linear product
having applied racing tires tables but
for this one it will actually adapt to
your specific distribution okay so in
the library is a it depends again in the
library there's a there's a function
which calculated
calculate the size of the bucket and
that function basically nice tumors it's
got a logarithmic mode and it's got a
constant word so there's a function
called set logarithmic ages say it to
one then you take the logarithm of the
this all ice double sided tape for a
bucket and make that the size of the
bucket and then you can say I said
logarithm one two three four five
whatever to make the ice table
resize the lighter and lighter and
lighter obviously making that bucket
larger and larger as a performance
penalty but with like these states said
we had logarithmic factor say two one so
that's about the fastest and and for
that specific benchmark
inputs it in other words basically I
took a goof actual timing card and it
just added a rabbit to it and then the
keys that generate there the random keys
on really that random it's about 20 bits
of random place so it's not a lot but
with some other taste soft time which I
wanted to show you but I don't know
seems like we're not gonna have time we
use a full 60 64 bit of randomness it
still performs really well
much better than thin sash wall using
also approximately after memory but you
the idea is about the amount of multiply
then it's the bucket size at the end by
the logarithm times two times for
something like that it's not just so
it's a bit larger but it doesn't us
because yeah it's just to to make sure
you don't have that lost fiesh which
something like so much bigger anyway
okay okay sort of okay yeah oh okay oh
that's a fair observation all right so I
just wanna repeat for the audience that
that you did some investigation in -
Maps and you found that deletion isn't a
very popular operational to do an ice
table but for those people where is that
for instance in this in the link page
called we asked about in this koala of
species alligators obviously using the
ice table in alligators the others kind
of a lot of delete especially for on the
path because the why that alligator
works is that it has a mind sort of
trouble allocating which is lot and any
for every thread is another little small
alligator which if it's least recently
used allocation sizes back to the main
the main alligator and obviously when a
victim or deleting we were uh
negotiating from that side that guy sure
it isn't he using deleting but for that
in the short
concurrent algorithms we check the stove
versions per page and that kind of thing
you're just creating like you say
requiring all new eyes function is some
other global ice table that contains the
original versions and that per
transaction tribal just gets discarded
when it's finished doesn't actually
delete anything okay any other questions
okay that's fine okay
I think I'm finished
please</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>