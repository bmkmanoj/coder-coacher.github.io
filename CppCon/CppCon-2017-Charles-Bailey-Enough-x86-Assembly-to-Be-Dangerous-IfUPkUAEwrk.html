<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Charles Bailey “Enough x86 Assembly to Be Dangerous” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Charles Bailey “Enough x86 Assembly to Be Dangerous” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Charles Bailey “Enough x86 Assembly to Be Dangerous”</b></h2><h5 class="post__date">2017-10-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IfUPkUAEwrk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Let's get started.
So, my name's Charles Bailey.
I work for Bloomberg in London
in developer experience.
So if developers have an
experience they don't like
sometimes they come to me.
Today I'm going to talk
about X86 assembler.
My first disclaimer is, I don't
write any as part of my job.
Which is great for pretty much everyone.
So unfortunately I don't
have very much time
and I'm gonna get on to
some code as soon as I can.
So really with this short
time, it's not really about
enough x86 assembler to be dangerous
but enough x86 assembler
to be very dangerous.
(laughter)
Okay, how did I get into assembler?
Well, a while ago,
I had an ARM-based computer.
So I don't know if anybody else
has used an Acorn Archimedes?
No, right, okay.
So the Acorn Archimedes
had four megabytes of RAM,
ARM 2 processor or, at least,
the version that I did, had.
And it had a great, basic
interpreter and it was very fast.
I had a lot of fun as a
kid, developing for it.
And one of the things I
wrote was a maze game.
So you think of 3D graphics these days,
it's really amazing, textures, everything.
None of that.
Absolutely none of that.
It was basically 90 degree
corners, step, you know,
one unit at a time and
render a big corridor.
And what I was finding
is that my basic program
was really cool but I had
to learn about sort of
this cool mode-shifting
technique where you could draw
on a screen that wasn't visible yet
and then switch because it was so slow
it looked a bit funny if were drawing it.
It actually had a please wait sign.
(laughter)
I also had a subscription to
a magazine, which had lots of
interesting stuff and some
of the stuff was how to write
ARM assembly code.
And the basic that came
with the Archimedes
actually had an assembler with it.
So I kind of learned that.
And I didn't rewrite my
whole program in assembler
but I did take that core loop
that was taking a long time
and I translated that or transcoded
or rewrote it in assembler.
And the speed increase
was just phenomenal.
I couldn't believe it.
So I was pretty hooked there.
I hadn't really heard of
C at this time, or C++.
But one of the magazines
came with a review
of Acorn's CC++ development environment.
And it took me about
three reads of the article
before I worked out what
on earth this thing was
and what a different programming
language could actually mean.
But the cost, hundreds of pounds.
It's just like phenomenal.
I mean as a kid, it's
kind of like, you know,
I don't understand why you
but something and then have to
learn a new thing to do
stuff I could already do.
So anyhow, sort of a history
of me in assembly language.
And one thing that's really cool
is ARM assembly is now back in again.
Cool, so, why might assembly
language be dangerous?
Well, even if you don't
think about writing it,
a lot of people kind of look
at Assembler and then kind of
scratch their heads and try and
interpret something from it.
Among one of the things to
note is that assembly language
is actually a bit further
away from object code
than you might think.
So it's quite easy to write some assembler
and assume that the assembler just does
a mechanical translation.
But that's not quite true.
There's quite a lot of
intelligence still in assemblers.
So if you write a MOV, for example.
Does anyone know what MOV does
in most assembly languages?
Yeah?
Sorry--
(quiet speaking)
(conversation in a different language)
Oh, yeah, it doesn't move, it copies.
This is C++ CppCon, right?
And we can't tell the difference
between copy and move, so, yeah,
but it's just a blank
to copying instruction.
There're actually about 14 major opcodes
in Intel x86 depending on the size
of the register you're moving
whether you're moving it from memory,
to memory, between registers.
You've got MMX instructions,
SSE, SSE2, three, three plus,
or what, Fnet 3E, that kind of thing,
but there's an S at
the front, isn't there?
So there's a whole lot
of stuff that goes on and
some of those have different extensions
that mean something, some of them don't,
and just depend on the operant.
The assembler is doing
kind of quite a lot there.
Similarly, if you got a jump instructions,
jumps were always,
sorta, kind of, relative.
If you're jumping a really short distance,
it can be quite a shorter instruction,
just a couple of bits.
If you're jumping a bit further away,
it could be more bits.
And the things is, it's like, the size of
your code is now depending on what
the assembler's chosen for
these different things,
so, if its plan is to choose
some quite short instructions,
then, suddenly, a jump that might have
not quite fit it in the
shortest possible jump,
might, suddenly, now fit.
So, do assemblers need
to take a second pass?
Some of them do.
There's actually kind of...
It's quite a bit of difference between
the assembly language and and object code.
The next myth is that some
chip manufacturers kind
of, like, hard code
their Silicon to execute object code.
That's just not true, so
it's just mind-boggling,
the complexity of, like, a modern,
not even desktop, but even though
there are modern processes in your phone,
they're just mind-boggling complex.
You can tell that this cannot be true
because Intel released microcode updates
which effect the way
that these CPUs execute
object code.
Sometimes they fix performance issues,
sometimes they fix genuine bugs,
they're fantastically complex machines.
Really, when it comes to assembly code,
you can't, kind of like,
look at it and say,
&quot;Oh, I can see, you know,
its two instructions
its gonna be a pretty performing.
There's no substitute
for measuring, still.
Okay.
What about writing assembly code?
It's like, there are so
many pitfalls, it's untrue.
Every time I, kind of, look at some
compile that generated to Simba,
I normally see something that
I don't quite understand.
If I do the research, I
see yet another pitfall,
and these pitfalls can be, you know,
performance pitfalls, you know,
if you do this, it's not as informant
as another way of doing
it, which is less obvious,
or sometimes you actually hit, you know,
revisions of processes that have bugs,
so, you know, if you use...
Some situations, you use a
single bit, kind of, return
and there's some pipeline issues
that mean that, that could be missed.
You can actually get, kind of, bad code,
so you end up having to use, like,
a longer thing than is strictly necessary
and that can seem quite odd.
It's so complex that,
the summary is, don't,
but it is quite fun to do, anyway.
I once was doing a
challenge against as friend
and we were doing a poker hand evaluation.
He does some stuff in C,
and I had to look through
all of the SSE instructions I could find,
and I found some really cool instructions,
you can do parallax and awesome things.
I came up with, sort of, really cool ways
to determine whether you had, you know,
four-of-a-kind or if you had a straight,
because you can go, like,
shift everything across
and, like, aimed it down and see
if you still got some non-zero stuff.
You can actually, kind of, like, design
really cool algorithms, just
based upon some of these,
kind of, quite obscure and cool opcodes.
This sort of thing that's, actually,
kind of difficult to do in C++,
even if you know the intrinsics around.
It's kind of tricky.
If you shouldn't write
this, and you've gotta be
careful with reading it and
it's not substitution for,
you know, actually measuring performance.
Why would you want to
learn assembly language?
Well, the key note this
morning is kind of like,
Campion said it better than I could,
it's kind of like, you should really care
about what happens in the
next level down because,
without some understanding
of the things that
the layer you're working on are built on,
you're not gonna have
as deep understanding
of how things work, what
makes things better.
You can also learn about some of the
trade-offs that happen
under the hood, based on
the code that you write.
If you choose a different
way of doing something,
how does that affect the performance?
You have to be very careful
looking at the assembly code,
but in general, if you write some C++
that causes something, some
code, not to be generated
or not to executed, it's generally faster.
You have to be a little bit careful.
I wrote some assembly, once, and I was
removed a totally redundant indestruction,
and the performance went down.
I was stuck on that for quite some time.
Turned out, what I'd
done, is I'd failed to put
some data, later on in the
program, in a different section.
So, it was actually just
straight in the code section.
In the original version,
it just happened to be
aligned so it was a really fast load.
I removed a couple of bits of instructions
and suddenly my code was
misaligned, my data was misaligned,
and the whole thing performed worse.
That's another pitfall to provide.
Good thing about writing
assembly language,
you almost never have to
worry about line length,
so if you know accompany, which, you know,
80 characters, simply,
is never gonna worry you.
You never need to feel
guilty about using a go-to.
In fact, the go-to is so ubiquitous that
in ARM assembly language, the
opcode is a single character,
it's just the letter B, for
Bloomberg, (stutters), Branch.
But, mostly, it's fun.
I, yeah, I do code, so it's fun.
Even C++, but...
Cool.
Normally the clock on my presented display
hasn't started, but...
Yeah, (stutters) we're gonna
get on to code, pretty soon.
In order to get on code, I'm just gonna do
enough architecture to get by.
What I mean by architecture is just
a model of the computer
as it pertains to the
to your program and to your
assembly program that you're writing.
Sorry, the assembly code
that you're looking at,
we're not writing assembly code.
I recommended against that, haven't I.
Okay, so, I'm just going
to talk about three things:
registers, your virtual
address space, and stack.
Registers are
very fast memory locations and
they're the things that can be, sort of,
directly manipulated by the CPU.
You can't point to a place, on Silicon,
where they are in these
modern CPU architectures.
They do cunning stuff
like register renaming,
so some move instructions
don't actually do anything,
but in the assembly code,
you can think of a
register as a really fast
location to temporarily store stuff.
They're tied to a particular
CPU core, so if you're
in a multi-core environment,
the registers are local.
You can't view another CPU's registers.
They can't be interfered with,
that's kind of part of
what makes them fast
as compared to a memory location.
I'm gonna talk 32-bit assembly
because I'm kind of short
on time and it's simpler.
There are basically eight so-called
general purpose registers on the X86.
On the 64-bit, you get all of these,
but prefix with an R,
and they're twice as wide
and you get another eight, as well,
which are imaginatively named,
R8 through 15.
(inaudible questioned asked)
I believe, yes.
They call general purpose registers,
but that's not really true
because many of them
have special purposes.
The most obvious one is ESP,
which is extrasensory perc...
No.
The stack pointer.
You've gotta keep that as a stack pointer.
You could just assign it another value,
but that's probably a bad idea
because there are some instructions that
implicitly use a stack pointer,
particularly, push and
pop and then various
other variations on push and pop,
if you're going to MMX
registers or other places.
EBP, the base pointer,
it's used in functions
as a record of what the stack pointer
is when you go into a function,
so you can think of the range,
between the base pointer and the
stack pointer as, kind
of, local variables.
Kind of, it's not quite that strict.
EAX is very important.
That's user return value for
short
return values, so in
it's pointers and things,
and then various other ones have gotten,
kind of, special to use, like ECX,
those, kind of, elude instructions
that sometimes use it.
Some calling conventions use
ECX as the &quot;this&quot; pointer.
But, yeah, ESP, EBP, and EAX are
the, kind of, most special purpose.
For simplification, anyway.
Then there are the true
special purpose registers.
There are, kind of, two obvious ones.
There's the instruction pointer,
which its prefixed with
E or R, depending on
whether in 32 or 64-bit mode.
In ARM it's known as
PC, the program counter.
It's all the same thing.
You can't generally write and read
from the program counter, directly.
Instead, you use, kind of
like, different instructions.
If you call a function, you use the call
and that will cause
the instruction pointer
to jump to the location that you're
calling, and execution
will continue from there.
You can, then, actually
examine the stack because
when you call, a return address
will be put on the stack
so you can tell what the
instructing pointer, what value
it had, before the call, as well.
Then there's the flags registers,
and the flags registers
are really about...
they're, kind of, mostly
used for conditional stuff.
Flags tend to get set
on arithmetic and logical operations,
like 'and' and 'subtract' and you can
get things like the carry
flag and the zero flag,
and then you can test them, subsequently.
They are the most popular conditional
instruction of the jump instructions.
There's a whole lot of jump instructions
that can be conditional flags, so you can
test whether some value
is less than another
and jump to a particular
location if it is,
just carry on execution if it isn't.
Virtual address space.
Pretty much, everything
that isn't, can have
registers that you read and write from
is something in your
virtual address space.
These days we have the luxury of
flat memory models and we don't
segment things up a
bit, so all of your code
tends to get in the address space,
somewhere, and then other stuff, too.
So, dynamically...
So...
Your code is mapped in
from your executable,
so you can stack that data, as well.
Then, when you dynamically allocate stuff,
that also goes into an
address space, somewhere.
And then, finally, the stack, also,
lives somewhere in your
virtual address space.
And you can just, sorta, read and write
locations from your virtual
address space as you see fit.
Generally, stuff that's
mapped in from executables
can be, kind of, mapped around
the address space, somewhere.
Stuff that's going to
dynamically allocate.
It tends to get, kind of, like mapped
from low addresses,
kind of, upwards a bit.
And the stack tends to, kind of,
be a, like, a high address thing.
There's also, kind of, headroom
to allow the stack to grow.
I'll talk about the stack, now.
Well, there can be multiple
stacks in a program
because its specific to
a threat of execution.
Oops, I think I double-clicked that.
So, you've probably, kind of, looked
to the core stack in a debugger,
or, at least, watched some poor,
luckless colleague look at a
call stack in their debugger.
It's, kind of, you know, just
a list of function calls.
In X86, there's been many
architecture's &quot;the stack&quot;
is, actually, a, kind of, combination,
so it contains return addresses,
so it contains the call stack, and
it also contains parameters
that get passed into functions,
and it also contains
local data, so if you,
kind of, create some local variables,
they're, also, kind of, on the stack.
The really confusing
thing about the stack in
Intel world, and, actually, to be honest,
quite a lot of, most
architectures, perhaps, even,
is that it grows downwards.
So, you push something onto the stack
and that becomes the top of the stack,
but that will, actually,
have a lower address
than things that were
pushed further ago in time.
Top of the stack, low address.
Bottom of the stack, high address.
It's confusing and irritating.
Okay, cool.
I know what you're saying.
You're all just desperate
to see some assembly code.
I came out pumped...
So how'd you do this?
Actually, I used, sort of,
the Godbolt Compiler Explorer.
- [Male Audience Member] Cool.
- Yeah, cool.
That's no, really, right
way to look at this and,
of course, Matt Godbolt's
here, doing a talk,
I think it's on Friday,
so that should be good.
But, I was on an
airplane, not so long ago,
without any network access, so here are
alternatives as far as doing it,
so I don't mainly do this
sort of stuff on Rex.
Objdump, you can give it
the minus D to disassemble,
and if you give it the minus M intel flag,
it gives you the intel syntax.
There are two kind of
common syntaxes for x86.
I always try and go for the Intel
because it matches Intel's documentation.
You can tell your compiler to generate
assembler, instead of generating objects,
so the minus capital X, and, again,
you can give it a hint not to give you
AT&amp;amp;T syntax, which is the other one.
And, if you're in a debugger, so MGDB,
you can just type 'disassemble'
and it'll disassemble
the current function.
If you're on Windows, so
I was playing around on
my Windows machine, and if you have
Visual Studios, it's even easier.
You just, kind of, open
the disassembly window.
Once that window has focus,
you'll step in and step out,
hack into the assembly instruction
network, Sky, it's really cool.
It's really easy to use.
Here's a simple example.
Here's a function that adds
two numbers to get this.
I gave it a basic G++ invocation to this.
When you generate the assembler,
it tends to be a whole lots of
noise, very important noise,
for handling things like exceptions
and position-independent code,
but, to be honest, on 64-bit stuff,
the position intermittent code,
you get for free, but on 32-bit,
there's some clutter involved with that.
If you push past these short options,
you get into a nice, simpler assembler.
Cool.
That's was, actually,
kind of, pretty readable.
So, the first two instructions, there,
are a standard kind of Prolog
for maintaining your stack.
Generally, what happens
is, every function records
the stack at the beginning and
its function at the base point,
so that's EBP, and, then,
when it calls the next function,
the call will store the return address,
so the address of the instruction
following the call, on the stack.
What these two instructions do,
is set up, what's called,
a new stack frame.
We're saving what the base pointer,
from the previous function,
and then we're recording
the value of the stack
pointer in our base pointer.
Okay.
Time for a diagram.
On end feed to a function,
this is roughly what things look like.
My function is executing,
and the top of the stack
is going to the return address
I'm going to go back into.
The base pointer is the previous
function stack pointer, after it saved
the base pointer of the
function that called it.
So, you can see the little, the loop back.
That's a base pointer, pointing to
a previous function's base pointer.
What those two functions, I
showed on the previous slide,
are gonna do, is form that loop back
so that the stacks are
gonna push the base pointer,
so the stack pointer will move down one,
and then, into that new location,
we're gonna store the value in EBP,
which will, then, kind of, effectively,
point back to where the EBP
pointer is at the moment.
So, you can see that the stack is
containing a different source of pointers,
so you've got the pointer
of the return address,
as it points into an area code,
of executable code, and
then you've got the,
kind of, base pointer chain that points
back towards base pointers
of previous functions.
This is what kind of gives you
the ability to walk a
call stack, normally.
Now, I'd like to say
things are not this simple,
but this is, actually, takes a bit of
getting one's head around, anyway.
It turns out that debug information,
so modern debug information, it's so rich,
that you don't always need
to store a base pointer.
So, you can get, from debug
information, your call stacks.
Some compilers will just,
like, omit this jumping around,
but because I'm using minus o0 null
optimizations, we still got this.
So, first two instructions
are about setting up the frame
pointer, and then, the bottom
two functions are,
essentially, just undoing that.
I've pushed, or saved, the
previous function's base pointer,
and at the end, I pop it
off the stack, back into
the base pointer, and then
I returned to my call E.
And then, the rest of
those three instructions,
are the meat of it, and all it's doing is,
it's so, again, the stack contains data,
one of the things that
happens before I got called,
is the parameters for me to
get pushed onto the stack,
so EBP plus eight and EBP plus
12 are the two parameters.
They get moved into two
registers, EDX and EAX,
so it's a moving-in-front
instruction in Intel syntax.
The destination is the first parameter,
so I'm moving the parameters into EDX
and the next parameter into EAX.
The add instruction is like a plus-equals;
It adds EDX onto EAX, and
EAX, as I mentioned earlier,
is used for the return
value for short things,
so there it is, it's in the right place.
And so, when I return to my caller,
that answer is in EAX, the correct place.
Cool.
How long have I got?
- [Soft Spoken Man]
Just over five minutes.
- Cool, excellent.
So, C++ is a cppcon, so, finally,
I get to do just a little bit of C++.
I've got a demonstration class, here.
I believe that, Nico
Josuttis is doing his,
The Nightmare of Move
Semantics for Trivial Classes,
so go to his talk to find out
why all of this is a bad idea.
But, one of the things that I, kind of,
like, looked at before, is like,
often, we're taught implement operator,
plus, in terms of operator, plus-equals,
because you can reuse code and make things
simpler, but how should we do this?
Here are two possible candidates.
Does anybody want to vote for the top one?
And who wants to vote for the bottom one?
Alright, you're all on the fence, clearly.
Okay, so, it turns out that the
top one, I can fit the
code on a single slide
and I've de-mangled a couple
of things, so you can see that
there's a copy constructor,
that's unsurprising
because if you look, I create a temporary,
I then do a plus-equals and I return it.
So, that's cool.
So, I've still got a copy construction,
I've got an invocation of plus-equals.
Candidate two is interesting
because it does, pretty much,
the same thing, but
afterwards, it has more stuff.
It has another copy
constructor and a destructor.
It creates an extra template.
Does anybody know why that is?
- [Man In Audience] Aim
for part of the other ones.
So,
I mean--
- [Man In Audience] It's
starting to transition.
So, sir, why does that not
happen in the second example?
- [Man In Audience] Why it doesn't?
- Yeah, do you know why it doesn't?
- [Man In Audience] I think
it is because you really...
Sorry, because the main
variable returns there?
- Yeah, so it returns the main
variable, but you can still
have a return value optimization,
even without a name.
Yes.
- [Higher Pitched Man]
There's two temporaries,
so it has to construct
them, then destruct both
of them, after, when you call them?
- Yeah, so, I mean, the key thing is,
is that the operator
plus-equals returns a reference.
Now, by convention, that reference is
to the 'this', that's passed in,
but there's no way for the compiler to
know that, so it has to
create another temporary,
which is kind of annoying because
the bottom one looks neater,
at least, I think.
I could go through this line-by-line,
but that's probably not
going to be helpful.
There's a few minutes for questions.
Yes.
(question asked)
John, that...
Should he use the...
Or I can repeat the question.
- [Nasally-Voiced Male] So, when I, like,
go in Godbolt, or Rex and Coda,
and I see dissembler on the outside,
like, I saw, like, the move,
push and pop and all that,
and, like, dissembler, pass and call,
but it's always like move L, move Q, like,
there's always these little, like,
additions to the end.
What are those?
- Okay,
so, when looking at Godbolt, you're saying
that you're seeing a
lot of suffixes on the
mnemonics.
Gosh, that's a hell of a word.
I've managed to avoid it, all along.
So, the thing like mov is a mnemonic,
which gets turned into,
like, object code opcodes,
so it depends on the
syntax you'd be looking at.
So, if you're looking at AT&amp;amp;T syntax,
the suffix is generally a way
of saying what size of
operand you're looking at.
L is,
it's kind of &quot;long&quot;, anyway,
it's four bits, W would
be just a two bits,
and Q is a quadword, so 64-bit length.
However, you have to be slightly careful
because, if you look at
the Intel reference manual,
the mnemonics have variable length.
So, there's mov, but there is also...
I believe movq is...
Mmmm.
There's a mov that's got an extra letter,
which is an SSE mov, and it
isn't just the suffix one,
and this is another great
reason for avoiding AT&amp;amp;T syntax,
is because telling what's a suffix
and what's part of the mnemonic,
and then looking up the right thing
in the documentation, is kind of hard.
Yes.
- [Guy In Top Left] Just a comment from
(clears throat) slide one, slide two.
I think that, just in general,
the version two...
Oh, no, it was like way
later, like near the end.
- Oh, slide one, slide two.
- [Guy In Top Left] Sorry,
I didn't mean the...
I mean the two version of plus-equal.
- Oh, this on.
- [Guy In Top Left] Yeah.
So, the second version--
- Nope.
Sorry, I've lost it.
That one. (laughs)
- [Guy In Top Left] Yeah,
so, the second version
won't qualify for RBL, so,
in general, you will...
I mean, regardless of what happens,
it will not be optimized
as well as the first one.
Just a comment.
- So, the comment is, the second one's
not available for return
value optimization.
Yeah, and the reason is, is because it's a
reference that's coming
in, so it's an L...
Well, it's affect is an L-value,
so it needs to be copied.
- [Guy In Top Left] Right.
- There's--
- [Guy In Top Left] Well,
even if it was the same one,
like, if the compiler looked inside the
plus-equals, and it still was returning
a reference to the same one, it still
wouldn't be allowed to perform RBL--
- Yeah, that's true,
I think.
- [Guy In Top Left] So, it
would have to, like, do
it under the as-if rule,
which is pretty difficult
for most classes,
so probably never gonna
get it to dissemble this?
- Probably not, although, I don't know,
I'd be quite optimistic
if it's in the same file,
and, yeah, maybe, as-if--
- [Guy In Top Left] It's
a very simple classed.
- Mm-hm, yeah.
I have another
demonstration we didn't have
quite the time for, and
when I was looking at,
if you have a, instead
of a const reference,
you can add an R-value reference
because you've overloaded
because you wanted
to be able to add something
that was temporary on the left.
Did something similar,
here, and, interestingly,
I got a difference is
that one could be copied
and one could be moved in a,
sort of, similar L-ish, I think.
Yeah, it's very complex and Nico Josuttis
is a much better expert than
me at that sort of stuff.
Yes.
- (question asked by man on bottom right)
- So, the observation is, if you take
the first parameter by value,
and your operator plus,
then, there, you've got more optimization.
So, the answer is maybe.
- (man on bottom right comments)
- Yeah, it kind of depends.
If you've got some...
If you're adding something
that's generally an R-value,
then maybe,
but, on the other hand, if you're
adding something...
If you've got something that's an L-value,
anyway, so you've got
local variable, already,
for some other reason,
then you're just moving
a copy from inside to outside,
and if you do move that copy outside,
then you don't get
the NRVO in the top one, so it varies.
Thank you very much.
That's all we've got time
for, but you can kind of,
like, hassle me afterwards
and do, then, see our stand.
It's just downstairs from here.
Thank you.
(applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>