<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Tomasz Kapela “C++ and Persistent Memory Technologies, Like Intel's 3D-XPoint” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Tomasz Kapela “C++ and Persistent Memory Technologies, Like Intel's 3D-XPoint” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Tomasz Kapela “C++ and Persistent Memory Technologies, Like Intel's 3D-XPoint”</b></h2><h5 class="post__date">2017-10-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mn42HgAjDug" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">So hi, my name is Tom.
I'm a software engineer at
Intel, and together with my team
we had the opportunity to work on some
pretty interesting software,
with some pretty interesting hardware.
But it's not really
specific to the hardware.
The software we do is hardware agnostic,
so I'm not really going
to talk about 3D-XPoint
in particular.
If you're interested, and I am not legally
forbidden to answer your questions,
then I probably can answer
them after the talk.
So let's leave all the
3D-XPoint specific questions
until I'm finished.
And since this is a C++ conference,
I will be speaking about C++ only
if I'm talking about implementation.
But before we start
looking at implementation
we have to give you some background,
some basic knowledge
about persistent memory
and how to program it to persistent memory
before you can actually
appreciate all the cool work
we've done, and thank you.
Just because if I wanted
to tell you everything,
I'd need like eight hours or nine hours,
or probably more so.
I've got an hour, so I had to cut back
all the things I could.
During all the talks
that me or my colleagues
or the architects have given,
we notice that different people understand
the word model a bit differently.
And so it depends on
who you're talking to.
If you're talking to a hardware person,
he's going to think oh,
he's going to be talking
about some hardware software interface
that I can understand.
And I'm not going to be talking about
hardware specific stuff.
As I said our library is
totally hardware agnostic,
so no hardware software stuff.
The CPU hardware guys,
they're going to say,
or the criMiNal guys,
they're going to think
well it's probably something
about the instructions
that architecture does
different and that's specific
to the technology and it's probably that.
I will be mentioning some
ISA, because I have to,
to give an overall
picture, but it's not the
topic of this talk.
It's not the third meaning is also not
the topic of this talk, but
I'm going to go a little
bit more in depth about
what the OS exposes
to the applications, to
the user applications,
because that's necessary
to get you starting
using persistent memory in any form.
But really what we'll be
concentrating on is the
programmer experience
because this is what matters.
If you have a library,
and you want the library
to be a success, so how
do you measure that?
By the number of users
of your library, right?
So if your library has
a really difficult API,
a big one that is hard to use,
then it doesn't really matter
how good your code is, and
what kind of magic it does,
nobody is going to use it.
Because it's just not user friendly.
So we want to leverage towards
C++ has all the features
to make persistent
memory programming which,
if you saw it in C, how
it looks, I'm not going
to show that.
Because I also like C as
well as C++ and I don't
want to diminish what C looks like.
It's hard.
It's really tedious,
there's a lot of macros.
You don't want to.
C++ looks really, really better.
So this is the mandatory slide.
This is shown in every deck
about persistent memory
that I have seen.
It depicts the programming
model that was written
by SNIA, which is the
Storage and Networking
Industry Association.
There are a couple of
companies which contributed
to the standard, mainly
(mumbles), Dell, HP,
Intel obviously, and Microsoft.
So you had a bunch of really smart people
in one room from totally
different companies
like DRACA and Microsoft,
and they sat and they didn't
argue with each other,
they came to a consensus,
and produced a workable standard,
which I think in it's own
is enough of an accomplishment,
even without all the
software and anything else.
So that's the cool stuff.
There's actually more
than one way to program
to persistent memory, but
I will be concentrating on
one part of the standard in specific,
the one in the red rectangle, obviously.
What this part of the
model says is if you've got
a user space application up
there, and you have a persistent
memory viewer file system in your OS
and you Nmap a file, or map
view a file from Windows
and right now I have a question for you.
How many of you have actually
ever used the CISCO Nmap
or mapViewer file?
That's a lot more than I expected.
Good job, because that's a really,
not a popular CISCO.
So you know what it does.
It takes the contents
of the file and puts it
in your visual address
piece of the process.
So fine I don't have to explain that.
So if the file system,
the persistent memory OR
then it allows you when you Nmap the file
with a special flag called ODAX,
and DAX stands for direct access.
You don't write to page
cache, so every store,
or load you do, you do
actually from the device,
which is kind of a half truth.
I'll be explaining more about that later.
So if you don't quit page cache,
you don't incur that
penalty of the round trip
to the KERNEL and back.
And that is really important,
because persistent memory
is a dim form factor, it's really fast.
So you don't want to go
and ruin all that speed
by going through the
KERNEL, changing context to
the privileged mode, and back again.
It's not something that you want to do.
Now I said that you can
write directly to the device,
without getting page cache,
that is of course not true
because you're going to
hit a cache the CPU cache.
And the CPU cache is not persistent.
If you have a power failure,
and your data is in the CPU cache,
you've lost it.
So in your application,
you see that you wrote it,
but it's not on the media,
it's not in the DIM,
it's not persistent.
So what you need to do
is you need to flush,
you need to flush the cache
line, which you've modified.
You can do it from user space three ways.
The old school way so
you can do a CL flush,
which is a strong memory
ordered instruction,
which flushes one cache
line and sends it to
the memory controller buffers.
There are two new instructions,
one of them is called
CL flush out, and it is
not strong memory ordered,
so you need a fancy instruction after it.
The benefit of CL flush out
is that you can issue a number
of those CL flush outs for
a range of cache lines,
and the CPU is free to do
them in any order it likes,
to somehow optimize, in
fact it does optimize it,
and it's like 10% improvement
I think, in flushing overall.
So CL flush out is a new instruction.
There's also CLWB, so
cache flush ride back,
which is as far as I know,
and I'm probably right,
is not implemented in
any current processor,
at least as an Intel.
So probably not anywhere.
What it does, it gives the hint to the CPU
that you know what, I'm
probably going to be using
that cache line right after I flush it,
I need to flush it, but I probably will be
fetching it again.
So if you could, please
do not invalidate it.
And that's all it is.
It's like, if you could, if
you could be so nice Mr CPU,
don't invalidate it.
Of course the CPU is free
to do whatever it wants.
If it wants to it can invalidate it.
There's also right back and validate,
which is only available in privileged mode
and it's a really heavy, really,
really heavy instruction.
What it does, it clobbers all your cache.
All of it.
Flushes the dirty cache lines,
clobbers all your cache,
and issues a cycle for the
memory controller buffers
to flush as well.
So it should not be abused
and as far as I know,
the KERNEL guys are not abusing it,
because they are smart guys.
So once you flush the cache
line that you want to evict
from the CPU cache, it hits
the memory controller buffers
and historically the memory
controller buffers were not
in the power fail safe domain,
but all these smart guys
and all the companies decided
that it's going to be too hard
to program if we have to manually flush
the memory control buffers,
so there was an instruction
for it.
It got deprecated before it
got implemented in the hardware
so right now, if you flush something
through the memory
controller you can assume
that it is safe, even if
the power fail occurs,
the ADR which stands for a
synchronized dRAM refresh
it just evicts it from the
memory controller buffers.
So you're safe.
It will reach the DIM.
And we've got to talk
about this Atomicity.
The CPU, the only guarantee that I know of
is that an 8 byte STORM
is not going to be tall.
So if you get an
interruption at 8 byte STORM,
an 8 byte STORM will either
result in the new data,
or the old data.
Nothing different.
But as far as I know, people rarely change
to their consistent state of application
from one to another, with just 8 bytes.
Usually you want to do something more.
You want to do something like
a string copy for example,
and a string copy is typically going
to take more than 8 bytes.
So what happens if you do a string copy,
and you want the (mumbles) persist there.
You think of it as a CL flush.
It's essentially what it does.
Depending on the CPU on the
features CPU advertised.
So, you've string copied it,
you're about to flush it,
and then something happens.
Anything, take your kid to work day right.
You go to your data center
and he yanks the cord
out of the wall socket before
it goes down for the server.
And it's like it can happen right?
So, what do you think,
what will be the result
if you power up the server again?
You load your persistent part of the DIM
into your application
and what is the result
of the string copy?
Anybody have a wild guess?
One two three four five, none, any?
Yeah any of them, I don't know.
There's this thing called cache pressure,
and the CPU is free to
evict any line of cache
it sees necessary for that
moment because it's running out
of a three cache, so it
decides that I'm going to
evict any line.
And because this is another line,
this is a character screen,
it's another line it can
cross cache line boundaries
anything goes actually.
So this is the problem.
This is probably the biggest problem
of persistent memory programming
that's it's low level stuff
that you have to worry about.
And that is what we are here for right?
We the software ,library developers,
we want to make application
developers happy.
We want them to be able to
program to persistent memory
easily naturally, not
worrying about all those
intricacies of hardware.
So we developed a library, and some tools,
which together with the
language run time's features,
provide you with the necessary
abstractions for you to
easily, at least, kind of easily,
program to persistent memory.
What we eventually want
is something like this.
So you've got a std vector,
which everybody knows
and appreciates is probably
the best container there is
in the std library.
And with just substituting the allocator,
you could potentially end
up with a persistent memory
aware and transactional
and really working without
all the manual flushing and everything.
It's something that I think is really neat
considering all the stuff that is going on
behind the scenes.
And actually this is already there.
We have in our GitHub recall,
because I probably didn't mention,
all of this is open source,
it's on GitHub.
You can download it, fork
it, poke it with a stick,
anything you want.
You can correct it if something's wrong,
do a bool request.
Find an issue, find an issue we're open
to any kind of contributions.
So it's available, we
have a modified version
of C++ and as far as I know,
I haven't tested it extensively.
I did some user testing and I think
all of the containers work.
They are slightly modified.
Yes.
- [Man] You might be
coming to this shortly,
but to make sure this
was really simple for
populating anti vector
persistent memory do we have
a slide on how we then load
from persistent memory into--
Yeah, I probably, I think I
don't have a slide for opening
an existing pool, but I
will be talking about it.
I have a slide where we
have a pool handle, so I--
- [Man] You wait until it's a good time--
Yeah, yeah, yeah.
I'll handle it when time's appropriate.
So yeah, it's there.
And it's backward
compatible so it didn't ruin
the ABI changes that we've made,
I'll be talking about that at the end.
So naming rules.
What is persistent
memory sometimes called?
Storage class memory
or non-volatile memory.
You've got to think of it as RAM,
because it kind of is dRAM.
It's really fast.
It's right up there with
dRAM, it's just like
you've got NAND flash here, dRAM is here,
so persistent memory is kind
of, depending on technology,
but it's kind of here.
(mumbles) there's a type
of persistent memory called
(mumbles) which is actually
normal RAM, with a battery and
a NAND flash so it gets
dumped, the contents get dumped
to a NAND flash.
But the problem with that
is that you have to have
a really big battery and a lot
of flash to have a large RAM
DIM/FAC.
They're not really popular
because the capacity's small
and they're pricey.
So it's right there with dRAM speed,
it's just that dRAM is like
Bill Murray, in Groundhog Day.
Each time he wakes up,
it's a new day right.
It's the same, ah, what happened?
It loses it's old value.
And persistent memory doesn't,
which is both a benefit
and a curse really.
And depending on how bad your code is.
And the other word I'll
be using frequently
is a pool.
What I mean by pool is
actually the contiguous blob
of memory you have from Nmapping a file
from persistent memory.
That's like 90% of the use
cases of pool if I say it.
The other 10% varies from a context.
We're going to go through with an example
of a really, really simple trivial queue.
I just want to start with
the volatile implementation,
and then step by step take
you, converting that queue
example into a persistent
memory implementation.
So this is the queue entry.
The whole queue structure actually.
I told you it was trivial.
It's a singly linked
list, with shared pointers
and we have a head and a tail
because we want to push
back and pop front.
This is the only API
that we'll be supporting.
So you've got a push, which is trivial.
You allocate, you update the pointers
and you're done.
The pop is also similar.
Some of you are probably
going to look at it
and say what are you throwing.
You probably shouldn't throw.
Maybe, yeah.
I don't know.
It's just an example.
So deal with it.
You can yell at me afterwards.
There are three most important pillars.
All of them are most important.
I know that it's not correct, but it is.
Many things that constitute the library,
there are three things that
are really, really important.
Transactions are one of them.
So what do you want to do
if you want to make the API
transactional.
As easy as it goes.
You just wrap the whole
thing in a transaction
because there's really nothing
non-transactional in this.
Because you do an allocation.
An allocation has to be
transactional, you can't
leak memory in case of a power failure.
And the only other things we're
doing in this implementation
is changing pointers.
So you're changing the
state, so you have to keep it
under a transaction.
This is in the form of a lander function
because I like lander functions for this
particular case.
I like them generally.
But in this particular
case they're wonderful
because you see what's happening in the
place we define it, so you read the code,
and it's easy to read.
As you probably think it's not
going to be really different.
It's going to be the
same thing, you're just
going to wrap everything in a transaction.
Some of you might say, well
you don't need to check
the head pointers under
a transaction right?
You're just reading there,
you don't have to do it transactionally.
Now imagine that you want to make this
queue, probably not this
queue because it's simple,
but you want to make it multi thread safe
then the check actually has to
be inside of the transaction.
Because of locking, which
I will explain later.
There are a lot of intricacies,
but we'll get through them.
So the transactions, the decision we made
is that we're going to do
undo log based transactions
and that is because of visability.
If we decide to redo log transactions
the changes would be made visible only
if you decided to commit the transaction.
And it's not really the natural way of
writing algorithms right?
Steps depend on one another, usually.
So you want to write a
equals five, b equals 42,
and c equals a plus b and not do some
round about computation
for the re-logging.
They have ACID like properties.
They are atomic so either
everything gets committed
to persistent memory or none of it.
We take care to make our data consistent
so the library data is consistent.
What you do with your data
and your algorithms, I don't care.
It's your data.
You have to care about
your consistency, yourself.
The same goes to with isolation.
We isolate the changes of
the metadata of the library.
We give you an entry
point in the transactions
so that you can isolate
transactions from one another,
but that's it.
If you don't want to, or you have a bug,
then it's just that.
And durability, well I mean
it's called persistent memory
right, if it wasn't durable
then it wouldn't really
make much sense.
Those actions can be nested,
that was also a design decision
that we made.
And since they're under
log based transactions,
unless you come at
everything gets rolled back,
the state is pristine,
as if you never started a transaction.
That includes allocations
and deletions of data.
And locks are held until
the end of the transaction,
which is important, and
I will get back to that.
So we have three types of
transactions in the library.
Two of them are available
since every C++11
complied compiler.
One of them is available
only in C++17 and up.
This is the simplest form of transaction
and I urge you not to use it.
It's there because it
was simple, we wanted a
scope transaction, a manual transaction,
but it has many issues.
I'm going to explain the first line
because I'm going to do it once,
I'm not going to talk about
pool handles any more.
So this is an opaque pool handle.
Obviously you can open more than one pool,
load more than one pool
into your application.
So you have to be able to discern them.
This is the root object
and this partly answers your question.
Because this is an open, so we are opening
a persistent memory pool and mapping it
into the other space.
And there's this notion
of the root object.
And the root object, you have
to think of the root object
as like the root of the file system.
So it's your handle to
all the other objects,
all your directories and files.
So we start from there.
You'll open a pool, it gets
mapped into your memory,
and all the objects are already there.
Everything is there.
And you've got to do your handle,
and then you just traverse
the structure from there
and once you find your
vector, it's going to have
all the data.
And you just use it as any
normal vector you would.
- [Man] I don't see the
part that turns that vector
into being a live object
that the abstract machine
knows about?
Okay--
- [Man] I've got memory layout
that had a vector in it,
but I've not blessed
that to now be a vector
to the abstract machine that knows,
that that memory really
is a live vector object.
You're going to treat it as a
vector, because you're going
to have a pointer to it.
So for all intents and
purposes of your application,
it's a live vector
through pointer arithmetics.
It's going to be a vector.
You're just going to treat it as--
(mumbles)
Okay, this is really a sanity check,
because if you've got
two pools and one of them
has contents foo and the other one
has contents bar, and you try and open
foo thinking that this is
bar, then you shouldn't,
because it's a totally different layout,
so we do a sanity check so that
you don't clobber your data.
I mentioned the synchronization
point and it's here,
it takes an arbitrary number of locks,
different type of locks,
preferably persistent memory
resident locks which we implemented,
which have a special
feature and you just specify
any number and we will keep the lock
until the transaction ends.
It's called manual for a
reason because you have to
manually commit the transaction.
By default the destructor
tables the transaction
for security reasons and sanity reasons.
And the reason it cannot be automatic,
is because you don't know
why you're leaving scope.
You don't know why the
destructor has been called,
at least not until C++17,
where you do know why
you're leaving scope.
So that's why I'm saying that
don't use manual transactions.
Unless you really have to.
The log's up here as I said,
which will be important
in 20 seconds, maybe more because I
have other stuff on this one.
So the basic implementation
of the library is in C.
So all of us developers are C people.
And literally nobody saw this.
If you read it left from right it says,
transaction, get last transaction error.
What else would it get?
If not a transaction error.
It's from the transaction ID
right, it has to be through
a transaction.
If we want to review
everyone is like perfect.
That's the name that we want to go with.
And I'm looking at this
and this is terrible,
but it's only three letters right?
It's only three characters.
It's in the API, we've committed to it,
we could defecate it, put another one,
and ah, I don't think it's worth it,
so it's probably going to
stay and each time I give
this presentation, I'm going
to have to say I'm sorry.
There's a nother reason I don't like this,
is because we cannot
throw from the destructor
of the scope transaction class,
so the moment we leave scope
the transaction is like
(mumbles).
You have no idea.
You don't know did it commit,
did it abort?
Unless you call this function and check,
you have absolutely no idea.
Do not use manual transactions.
I couldn't, I contribute a bit more.
This is going to be on YouTube,
people are going to know
don't use the manual transactions.
I said all of this, and
this is just a recap slide,
I'm not going to go over it.
I have many more slides.
Automatic transactions
they are the cool kids.
Because C++17 fixed the
std uncaught exception
by adding an s, it's now
std uncaught exceptions.
And thanks to that, you
know if you're leaving
scope with an active exception.
You know whether it's (mumbles) or not.
So now, and I think, I'm not 100% sure,
because I did a pool request two days ago.
I was looking in the slide
and we still get like
transaction, transaction,
transaction error, and I was like
do we really need this?
Does the end only benefit from this be
that we don't have to manually commit,
is that it?
Can't we do better?
And I think we can.
I mentioned that logs are up here
which is really important
because in the cache clause
the locks are no longer held.
So if you have some clean
up that you've got to do,
you have to manually
relock and that's potential
raised conditions.
Yes?
- [Man] Sorry I'm just
trying to understand the
get last transaction error is that asking
for the last transaction
on the current thread--
Yes.
So is this still
embarrassing, I don't know.
If my pool request gets merged.
Okay the pool request,
what it does is it actually
allows the throw an
exception from the destructor
of the automatic class.
I know it's from the plan.
I know it's to terminate territory,
but since we now know that
we're leaving the scope
because of our next exception,
we can avoid triggering std terminate.
And I'm like 95% sure.
If someone gives me a reason
why I shouldn't do that,
I'll be happy to close the pool request.
And I don't know, go
into the corner and cry.
Because I'm going to
say don't use automatic
transactions as well.
Thankfully, again recap, no time.
Thankfully, we have something
that I'm using in the
queue example, and I like to
call it lander transactions.
They're actually function
transactions because they
take a function object as
the body of the transaction,
but it's the best of all
the world's because it's not
an object, it's a function
call, so I can throw
anything I want out of it.
So I can notify the user
if something happens
that he has to handle.
But there's one awkward thing.
The synchronization
point that I mentioned,
is implemented as variatic template.
So it has to be the last
parameter of the function.
Because you don't really
have to specify analogs.
So it looks like this.
It's saying like, I'm going
to start this transaction
and the body's going
to be a lander function
which is going to do
some transactional work,
oh and by the way, I'm going
to be locking two blocks.
I'd like to have the information up front
if I'm reading code and not at the end.
So, it is what it is.
Trade offs all the way.
But you don't really have to call this,
unless you want to know
the specific error code
of why the transaction
failed, so why the C backing
transaction failed.
But you don't have to.
You're going to get an exception
and if that's enough for you,
then it's fine.
So it takes an std function object,
it's automatic, available
in C++11 and later,
so it's got all the cool
features except the locking
at the very end.
Now the other most important,
the second most important,
is not going to have nearly
as many slides is that
I like to call this the
resides in persistent memory template
because that's what it means.
This is our entry, and the queue.
This will get allocated,
modified, deleted,
depending on what the ABI call is.
So what you want to do
is snapshot the state
of the object.
The end value has to be somehow remembered
by your transaction in case
you abort the transaction,
you have to revert the state.
You probably see where this is going.
You need to do this.
What p does is it overloads
the assignment operator
and the assignment operator
is a good trigger point
because it probably means
you're gonna write something to it.
So you're going to change the value.
If we're going to change the value
then I inadvertently have to do a snapshot
of the old value.
That's not really the only thing it does.
It also overloads a
bunch of other operators
so that if you have an end
and now you have a p and,
it has to behave the same so
that you don't have to change
all the other implementation
details of your application.
And it was designed in a way that
it's really a language feature
because you don't have the
overall operator dot.
So if you want to store
aggregate types in the p template
there is no convenient and
natural way to access it's
members.
There is a way through the (mumbles) API,
but you know, then you'd
have to change code,
actually.
If this is okay with
you, then fine, you can.
It's just a bit harder.
And now persistent pointer
is the third and last
most important part.
We have our entry once again.
As you can see we've corrected the value,
and now we still have a shared pointer.
The shared pointer does not know anything
about persistent state but
it does need to be snapshot.
The value of the pointer
needs to be snapshot
because that also changes.
It is part of the state
of the queue entry.
You probably know what the fix is.
Because it's quite straight forward.
You just change it to
a persistent pointer.
And since it's a persistent
pointer, then it knows
that it's in persistent
memory and it is pointing
to persistent memory so it
knows it has to be snapshot.
It will snapshot itself if the value
of the pointer changes.
Easy.
So how that works.
Once you Nmap since almost
all of you know what
Nmap does, it doesn't always
map to the same address
so you can reload the same
file into your application
and get a different base
address each and every time.
So the persistent pointer
is actually a base address,
so it's sort of an offset from
the beginning of the pool.
So you have to recalculate
your runtime address.
It is a random x, yes.
- [Man] So previously we
had shared pointers that had
other shared compounds
contacts over the pointed to
object, does the persistent smart pointer
have other shared schematics as well?
So the question is, does
the persistent pointer
claim ownership of the memory?
And the answer is right now, no.
And I'm going to argue
about it, that it should.
It's just that it's not really trivial.
There are problems, but I
don't like that it doesn't
and I think that it should.
And the persistent pointer
it's got all the (mumbles)
necessary for it to be allocator friendly.
(laughs) So yes, the first
bullet on the next slide
does not manage object
lifetime, and I don't like it.
There are reasons for it, mainly because
you have to treat differently
stack persistent pointers
because you can't have persistent
pointers from the stack.
The persistent pointer for
the root object for example
is a perfect candidate for that.
99% of your root object,
pointers are going to be on stack
and the problem is that once
that pointer goes out of scope
it triggers the deletion
of the root object,
which holds all the peristent pointers,
all the references to
all the other objects
and the domino just goes, and
everything gets de allocated
and that's not the point
of persistent memory right.
So we have to treat persistent pointers
on stack differently than
normal persistent pointers
and implement a reference counting,
like (mumbles) does.
And that's a round time penalty.
Probably not a negligeable
one, unfortunately.
Maybe with some optimizations
we could make it
livable.
But I think, I'm going to show
you how that affects the code
and I don't like it.
I really don't like it,
and I think we should be
reference counting for
persistent pointers.
It does manage a snapshotting.
Question, yes.
(mumbles off microphone)
No, no, no, just--
(mumbles off microphone)
Yes but you still have to
do that manually somehow,
or like policy based pointers?
(mumbles off microphone)
Maybe it's going to mess
with the allocation functions
a little bit.
Yeah I think policy based design
in this case might cut it.
That's a good point, yes, thank you.
- [Man] Is there a reason why there's only
one pointer type?
Like you replaced a shared
pointer but you had like a
unique pointer or a lock pointer.
Why don't you have like a
resistant unique pointer,
resistant shared pointer,
resistant (mumbles) pointer
something like that?
You probably could.
I'm not saying you couldn't.
You probably could.
- [Man] Would that solve
the issue that some pointers
you don't want it to delete stuff
and go into some sort of--
No, because it's not really
the type of the pointer
that matters.
It's the place that it
resides in that's the problem.
Because if it is in the
vault of your application
then it's going to be always destroyed.
The destructor s's going to
be, well maybe not always,
if (mumbles) but it's going
to call the destructor
and it's the behavior of
the destructor that has
to change depending on
whether it's a std variable
or if it's in persistent memory.
- [Man] But wouldn't you
have different types of, so
if you have a pointer that's
in RAM then you wouldn't
have that being an only
pointer, it would just be a raw
pointer into the persistent memory.
But if the pointer was in
persistent memory, you could use
a different type.
It doesn't even seem like any one type
needs to be the same--
Okay, so the question
is, if I could manage
on stack pointers as
well as stack pointers
into persistent memory.
Maybe, yes.
I think this would add
additional complexity to
your application actually,
because right now we're just
return your persistent
pointers to the root object.
Because it's fine if
it's a persistent pointer
even if it's on stack.
And then we'd have to
return, depending on,
because you can start a
reference to the root object
in persistent memory.
It's totally fine.
You can.
So we have to return
different types of pointer.
Yeah maybe, it's an option yes.
It's a viable option.
Okay I've got 20 minutes, okay.
Now I'm going to dash
through the rest I'm sorry.
Oh no, this is important.
No polymorphic objects.
The persistent pointer does
not allow polymorphic objects.
Which probably kind of
answers your (mumbles)
behavior question a little bit.
Yeah, we don't allow polymorphic objects.
Because there's no possible way to rebuild
your Vtables because it's part of the ABI
it changes from compiler to compiler,
architecture to architecture.
We just decided we're
banning polymorphic objects
from the persistent portal.
Sorry.
It is what it is.
So this is now our queue.
It's fully persistent, it's got all the
persistent pointers and the
data is also protected by
the p template.
Now the push is going to
be really simple to fix.
The only problem is that
now we're allocating
a shared pointer fix.
We are now allocating an
object using our allocator
and returning you a persistent pointer
to the newly transactionally
allocated object.
The allocation is not
part of the transaction
so if it aborts the
allocation never happened,
it's fine.
I'm giving you the pools
for completeness for that.
We have a complete example
of how this is implemented.
And now the top, and this
is going to be embarrassing
and well, okay, you live only once.
So I removed some code, because
it didn't fit in the slide.
You'll probably saying,
I'm all good seeing this.
Needed a temporary value,
because of what's next to come
so you have to manually delete the object
as it is right now which is a bother,
because now you have to
go through all your code
and analyze where is it
okay to remove your object,
to de allocate your objects.
And, it should be an anti-pattern really.
So that's why you advocated
that it's probably a good
idea to implement some
sort of reference counting
and manage the destruction
the last time all
the objects held at the
persistent pointers.
Now you can update the
head with the new value
and it's fine.
So this is the complete example.
It's not like it's a lot of
change, it's just one step
too many than I would have
wished and I think we can
fix that, and I think we need to fix that
at one point or the other.
So it's now a persistent
memory handling limitation
of the queue where we added
this at the beginning.
The allocation functions
are transactional.
Make_persistent look
exactly like make_shared.
Does it a bit differently.
But the three persistent
calls that we have
is actually not something I'm fond of.
It doesn't have a counterpart
in the standard library.
You don't have a (mumbles).
It would be nasty.
The stress interrelation isn't
really all that interesting
except for the fact that
these are persistent memory
resident.
So we can put these logs
in persistent memory,
and if you re-open the pool,
you've got the old state
of the, it's not zero initialized
or whatever is the
initialization of the lock
in your current OS
architecture or whatnot.
So we have a scheme that
reinitialized it automatically
each time you reopen the pool.
So if you use a lock,
then it gets magically
re-initialized to the correct
values, and it's fine,
you can use it.
That's the only cool thing
about these, but other than that
normal mutex and std text are sent.
So it was quite obvious
that if we have a persistent
pointer we should have an
allocator which is standard
library compliant.
And we do, and there's
really not much more
that's interesting about it.
It's normal standard
library compliant allocator.
Yes.
(mumbles off microphone)
So the question is what are
they, could you actually
repeat it, because I don't think I.
- [Man] Since you mentioned
that the pointer addresses
all stay the same between
the process (mumbles)
and the processor.
Is the size of the pointer just like,
have you determined that
you just like to strip back?
It's actually a little bit
bigger than an integer,
because it has to also have
a reference to the pool.
And so it has to have a pool identifier,
and then an offset within that pool,
so it's actually 16 bytes,
which is 8 bytes more
than we would want.
Because if you do, for
example if you want to do
a data store with it, and
you have to have a lot of
pointers then an additional 8 bytes.
Normally it's 8 bytes but
if you have to manage them,
and they take up more of your
cache than you would actually
want to, so it incurs a
quite significant performance
penalty, unfortunately.
And we do not have a really
good way around that, yet.
So it is bigger.
So the containers and the
question about the (mumbles).
I'd like to hear more about it.
So we have the pointer type
implemented, we have the
allocator implemented.
The std containers are
there for free and they are
maintained outside of
our team, which is like
the best plus ever.
Because we don't have to
back tracks everything.
It would be a sin not to
consider using them right?
We could implement our
own, maybe the algorithms
would work better, would be
faster in persistent memory
because we could fine tune
them to the intricacies
of persistent memory programming.
Maybe, I don't know.
The implementation on the library
is quite cold actually.
So this is a std vector.
I have abbreviated all
the insignificant stuff.
So it's three pointers right?
It's I think is the most
common implementation
I haven't seen any other implementation,
in any standard library
besides this three pointer.
So it's a begin and end
at maximum capacity.
And since these pointers are
actually persistent pointers,
since we can pass our
persistent memory allocator
and it works out of the box, naturally.
It's a different matter
with for example SIG map,
which is actually a red
black tree at least in C++.
The problem with this is
that it has a member data
which is of type bool and
as we've seen in the example
of the queue, bool does
not know anything about
persistent memory.
It doesn't know that it
has to snapshot its value
in a transactional context.
So this will not be snapshot.
And that's the problem.
And it's a problem we need to solve.
But, in this particular implementation,
the node does not know
anything about the allocator.
And the allocator is
actually the logical place
to put information about
what kind of memory you're
allocating from, but
it's not available here.
It's only, you only have the void pointer
and you only know it because of the name.
Because it's a template.
So we decided to inject the
knowledge of the p template
inside of the pointer, which is not ideal,
but it was the simplest
thing we could do exactly.
So we decided to go with it.
There is that proof of
concept kind of thing.
What it does, it defines
in the pointer trace
it defines an optional typedef
that if you don't define,
and it falls back to a default type,
and if you do then it's going to be used.
It's a nifty trick in
the standard library.
The rebind persistency type
which you can see up there,
what it does it takes your
pointer, which at this moment
is at void pointer, so it's a
persistent pointer for void,
rebinds it to a persistent
pointer to bool,
and extracts the persistency type.
So right now, the bool type as we can see
is actually p from
bool, and that's exactly
what you wanted right?
Now the node is aware
that it's going to be used
in persistent memory context
and it has to snapshot
its data.
So we had a bunch of changes
to all the containers
to all the data that was relevant,
I think I got all of them,
some of them are static cost data members
so it doesn't really matter.
I think it's fine.
I didn't notice any ill behavior
and we tested it with a custom tool
that we built, which I can also
talk about off line.
This is the end, I'm going
to take questions now.
I don't know if I'll be
able to answer all of them,
but the known issues.
These are just the ones
that I could think of,
off the top of my head.
Probably more, and I hope
you'll find more of them,
actually.
I think the biggest one is that the,
maybe it's not the biggest
one, it's just a consideration
you have to take into
account while designing your
application.
Because the data section of the BSS data
and BSS sections are not
persistent memory resistant.
It would have to be like a loader change
and an ELF standard change
so you have to bind your
application, your
executable with a pool file
so that the loader knows where
to load the data section.
It's a bother right.
It's way more changes than we want to,
so all the data in the BSS
all your static data members,
or static variables they're
not going to be persistent,
so don't rely on them being
reloaded in the correct value,
if your application crashes.
I'm going to leave the a
versioning it until the end.
I already mentioned the
Vtables so we don't support
polymorphic types.
We have no std string because
the standard explicitly
says that the type of
the character held by the
std basic string has to be a basic type,
so a p template it doesn't really cut it,
it's not a basic type.
So by default a std string cannot be used
with our allocator.
And there are no persistent preferences,
I just put it there because we didn't do
any core language changes,
and we don't want to.
That's a lot of fighting
with the committee
I suppose, so if we don't
have to, we get by without
persistent references.
If we don't have to
change the core language,
it's fine, don't do it.
So back to the layout version.
As far as I know, there is no such thing
as layout versioning in
the standard library.
If I'm mistaken, please tell me.
And this is an issue because
you compile your application
with one version, then you
re-compile it with another,
which could have a
totally different layout.
The implementation might be different,
the layout of the data might be different.
Any change, might inadvertently, yes.
(speaks of microphone)
So yes, you can (mumbles) but do you have
a way to query the version
of the ABI at compile time.
No, at run time actually.
- [Man] At run time I think you can--
If you can, then I'd like
to hear more about it
after the presentation
because that would solve like
a non-existent problem right
now, but maybe in the future
it might be a problem if
this actually gets used
anywhere.
So that was my last slide,
and some handy links.
So thank you for coming.
I hope I didn't bore you too much.
Barely made it.
(applause)
- [Man] So two questions,
one would this library
work with shared memory in
a persistent to applications
but not necessarily
going to be the lifetime
of the machines you could
have transactional memory
between processes?
You mean like DSX,
transactional memory DSX, or--
- [Man] I'm just saying
if our system could
allocate a shared memory partition,
could this library be used
to manage that shared memory
by multiple applications
and the same type of
journaling and unrolling
of journal rights in
a pseudo persistent memory?
So you like to just review
the transactional path
for your purposes?
- [Man] Yeah.
All the algorithms are really
persistent memory specific.
So it's probably, it's fine
tuned for persistent memory
so you might not get the
performance that you want.
And the other thing is we
don't support multi-processing,
but if you hacked into it,
maybe it would probably,
I'm going to say probably no.
It's probably going to be
more work than it's worth.
- [Man] All right.
I think so.
- [Man] And the second question
is since the root object
has to have knowledge
of the beginning of all
your data structures, would
it be possible perhaps only
once they had reflections
develop automatically,
but do garbage collects and
just sweep through everything
root know how to find
and determine what memory
is still alive and which memory isn't?
Maybe.
I really like the reflection
parts, and I see like
a multitude of applications
for the meta classes,
and could be garbage collected.
Garbage collection is
actually a heavy thing.
We have thought about
it, and it's not easy
to implement and I don't really know
if it's worth it, maybe.
This is C++, so we're actually aiming more
at performance probably
than universal applications.
I don't know if garbage
collection, it's worth looking
into, but I don't think we'll go there.
- [Man] All right, thanks.
In the near future.
Thank you.
- [Man] I have a question
about the snapshotting
you were mentioning and
the situation you mentioned
where bool was the wrong
type instead of a p bool.
I don't quite understand
for primitive types,
what needs to be done.
What does the p bool do
that bool doesn't do?
Okay, so what it does, it's
got, the bool has a value
right, true or false.
Once nodes are interested
the the color really matters
if you're doing rebalancing.
So doing rebalancing, the
value's going to change.
And if it changes, and
the application crashes
during the rebalancing
all the other things
are going to be tracked
by the transaction,
except for the color.
So if you have an inconsistent
state of the tree,
the nodes will be back
in their place before
the rebalancing, for
example, but the colors
of the nodes will not
be because the value was
not snapshot in the
transaction and was not
reverted to the initial state.
- [Man] I see so the
transaction has to keep track
of every change that happens within that?
Yes.
If any object is in persistent
memory and the state
changes, then the
transaction has to keep track
of the values, yes, of
literally everything.
And as you can probably
imagine it's (mumbles)
- [Man] So I think the reflection
stuff will help there too.
Especially the kind of stuff
that Herb was talking about
with being able to inject code.
Yes, yes.
That's one of the
application I was thinking.
Yes, exactly.
Thank you.
- [Man] On your initial
image you only handled
the right side of your image.
In the middle you said something like,
you can handle it like memory map file.
Yes.
- [Man] Which I assume that's
exactly the same way of
working like you normally
use a memory map file.
How do you then compare
it regarding performance
with a normal SSD drive,
compared to non-volatile memory.
Also regarding sharability.
A memory map file you can
access with two processors
the same time.
We don't allow, yeah.
- [Man] What that is.
Okay so, we do not
support multiprocessing,
so we don't support, because
of the way our metadata
of the library is organized,
we don't have an easy way
to support multiprocessing.
That was a design decision
we made a long time ago.
We just said that okay,
we're going to support
multi stranding, I hope that's enough.
And that's what we did.
We are looking into making our library
multiprocess safe, it's
just that it's not easy,
it's a big re-write of our
code, and that's one thing.
About the performance compared with SSD.
SSDs have, they're slower
than persistent memory,
the medium is slower.
That's one thing.
The other thing is that you
have block access to an SSD
and here these are persistent
memory is byte addressable.
So the can rate is smaller.
And to write to an SSD
with an unmapped file
you have to unsync a whole page.
You have to send at least a 4K page,
and if we're talking huge pages of 2 meg
or a gig page, which would be disastrous.
And here you don't.
You have a cache line.
So depending on your
algorithm, I'm going to say
90% of the time, persistent
memory is going to be faster.
There's the other 10% when it won't.
Because all the algorithms you have now
they are actually fine
tuned to block storage.
Where you have to actually
redesign everything
for persistent memory.
- [Man] Okay thanks.
Do we have time for more questions?
- [Man] Hi, I was curious
about what CPUs actually
support these instructions today?
You kind of implied (mumbles) doesn't,
so I was just curious if
you could expand on that.
This is actually, you mentioned
cabulaic is a client side
CPU.
Everything I talk here,
this is data center stuff.
It's not like you're going to take it in
and put it in your pc.
It's not going to cut it.
It has to have a server.
So, right now, there is
no CPU which supports,
I mean there's no CPU
that supports 3D XPoint
because 3D XPoint is not out yet.
But as far as other
non-volatile memory like an
(mumbles) you just need CL flush
and CL flush has been there
for ages.
CL flush out, I think it's not
commercially available yet.
So you can't use CL flush
out with off the shelf CPUs.
- [Man] So yeah the snapshotting problem's
is an interesting scenario.
Have you looked into whether
you can get hooks out of
the transactional memory
TS so that you don't need
to create a p template, but
would get language support
to say within this transaction
now I need to handle
these snapshotting requirements?
No I haven't, and I'd like
to hear more about that.
- [Man] It kind of
looks like you're taking
a lot of data searches that
are returned for (mumbles)
and making them work
for normal time memory.
What's the big motivation
behind, because most people who
rely on persistence, they
don't use these data stages.
No-one puts SSD string or
SSD vector on a system.
So what's the big motivation for doing.
Okay, there is true, but as I said before,
normally persistence store
is not byte addressable.
You don't have a small (mumbles).
You have to think at least
blocks of the media supports
and if it's the name of file
then you have to think pages.
So that's the main difference.
Here you can use a vector.
If you use the vector
on an SSD, it wouldn't
make sense.
If you wanted to add
transactionally, add just one element
that meant you had to
flush the whole page.
You had to put all the 4K
page, at least a 4K page
to the SSD, which doesn't make sense.
That's why, with persistent memory,
you can actually use, leverage
the std vector or std list,
because you can.
You don't really have to invent your new,
you can, it would probably
be faster and work better,
but you don't have to.
And that's a big benefit.
- [Man] The question I
had was you had these
cache line flushes, is it
possible at all for the cache line
to be flushed even without you
committing the transaction?
Yes it is.
It would be flush, but that's why
you have the transactions for.
- [Man] All tight, so in
that case you roll back
to the previous values?
Yes, you rewrite the new
state that got flushed
with the old data.
- [Man] Even on a crash?
Yes.
I mean, on the reboot.
Well then not on the crash.
The next time you load
it, the moment you load
you open the pool we go
through all the transactions
that have not been committed,
or have been committed
but not finished, and
we either finish them,
or roll them back.
- [Man] I see.
Other question, a lot of questions.
So is there any possibility
for the hardware
to detect that it's actually
dealing, certain cache lines
are dealing with persistent
memory and do something
different about them,
instead of you guys doing
software style roll backs and commits.
I don't think roll backs and
commits are not going to,
they're going to stay.
One thing you can do right now is mark
a portion of the memory as non-cacheable,
so that you wouldn't have to flush it.
But then the CPU cache
is there for a reason,
because it's fast.
So depending on the--
- [Man] So what happens let's
say, for this cache lines
if I just set something that says that
every line would just get flushed?
Like would it terribly effect--
Yeah that would be marking as
the region as non-cacheable.
It's not going to go to cache.
It's going to go straight
to the memory controller.
But then, yeah it's a penalty right.
You have to go through
the memory controller,
the memory controller has
to then put it onto them
and it's a lot of time.
I'm not saying it's not possible,
and I'm not saying in some
use cases it wouldn't actually
be better, than manual flushing,
it's just that it's not general purpose.
And we want to go general purpose.
- [Man] All right, thanks.
- [Man] So when you create
the root you were giving it
just a file path?
Does the OS, is the OS support necessary,
or you just need to have
the NVMe drive on your box
in order for that to be
able to open and go through,
because you mentioned a
special option that Nmap
needed to have, which none
of the Nmaps I know of
have.
Okay, so first of all, an
NVMe is a block device,
so it's not persistent memory.
That's just like, so that
we have an understanding,
it's not persistent memory.
But the Linux Nmap it has a new flag.
Since kernel 39 something or 4.
I think it's still in the 3 category.
But it has a flag, it's
called ODEX I think,
you have to map it as a shared memory,
because otherwise it doesn't make sense
it goes to RAM.
And what it does, it
tells that that you won't
have to have direct access.
So you don't want to write to page cache.
And as far as I know,
both Microsoft and DFS
has that support and on
Linux, EXD4 and XFS do have,
but it's a funny thing,
because we started off
as a Linux library and the Linux community
decided that it's not really safe to flush
from user space on EXD4
and the XFS because they
don't have a synchronization
for point the metadata.
So it's not safe to flush from user space
on XFS and EXD4, but as a
work around, the kernel guys
implemented something called a device dex,
and it's a character device
which you can Nmap and use
flush from, just normal.
Does that answer your?
- [Man] It answers one part of it.
So the library does
require OS support for this
Nmap option?
There's not a way for it to talk directly
and get this support.
No.
I don't think I understand the question,
because if you don't have
the ODEX, then you're going
to hit page cache.
If you hit page cache,
then you have to unsync.
So, we do support that.
It's just that the human persist
call is going to boil down
to an NSync which will
write 4K or 2 meg or 1 gig,
which is not ideal.
So if you want performance,
you're going to need
to have persistent memory
a verified system with
ODEX extension or a (mumbles).
- [Man] And then with
the direct addressable
with your vector, do I
understand it correctly
that in this case, because
of the characteristics of
the persistent memory that
when you grow a vector
that you can just extend
the address space,
or do you actually have to
go through the copy process
of a memory vector?
You just go normal path copy
and copy all the elements.
There's no really way to grow.
It's a standard allocator.
It's standard compliance, so
we didn't really do any changes
to the vector itself, so if you run out
of space you have to
reallocate a whole buffer
and copy your existing data.
Okay, thank you very much.
(applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>