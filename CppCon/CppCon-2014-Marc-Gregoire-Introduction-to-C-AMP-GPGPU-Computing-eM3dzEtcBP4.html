<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2014: Marc Gregoire &quot;Introduction to C++ AMP (GPGPU Computing)&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2014: Marc Gregoire &quot;Introduction to C++ AMP (GPGPU Computing)&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CppCon 2014: Marc Gregoire &quot;Introduction to C++ AMP (GPGPU Computing)&quot;</b></h2><h5 class="post__date">2014-10-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eM3dzEtcBP4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Marie Claire I i work for
nickel or nikon as you pronounce it in
the States I'm also the author of
professional C++ and the dirt edition
has just been released it includes C++
14 features and I saw it's available in
the bookstore so feel free to take a
look my presentation is about C++ amp
which stands for accelerated massive
parallelism and can I have a show of
hands how many people have heard about
C++ app okay pretty much everyone anyone
using it in production code okay one two
nice i was thinking what is the best way
to start the presentation on gpgpu
computing and I think that's with some
with a demo in fact I will start with
two demos to demonstrate the power of
C++ amp the first one is the n-body
simulation problem what this does is you
have a number of stars with which with
each individual star as a certain mass
as an acceleration and has velocity to
calculate the position the velocity and
the acceleration of any of the bodies
you need to calculate the interaction of
all the other bodies on that one body so
this is an N squared problem so it's
compute computation intensive but it's
highly parallelizable you can calculate
everybody individually so it's perfectly
suitable for something like C++ am poor
gpgpu computing in general so let's see
let's first start by running it on the
CPU so at the top you see the number of
you see the number of gigaflops that we
are getting out of the CPU this is a
quad-core CPU with hyper-threading so
there are eight course but right now
it's using a single core so the first
step to increase performance is to
switch to using all the cords available
in your machine so we switch to a
parallel library like ppl or until TVB
and now you see we have a healthy
performance increase we are getting
around nine-thirty gigaflops out of this
cpu the next step to increase
performance that you can get on a cpu is
to use the vector instructions like as a
Z AVX and so on all modern CPUs have
these so if we do that and we combine
that with the parallel version then we
get another nice increase now we are
getting 40 gear flops out of this out of
this CPU in this laptop but that's
pretty much it now our CPU is running at
one hundred percent of its power and you
won't get any more performance out of
this for this problem so let's see what
we can do with C++ amp so this is a very
straightforward implementation
converting our parallel implementation
to C++ amp and you see we already f 74
gear flops but right now it's using the
same number of bodies as I was using for
the CPU version if we just increase this
then we see that we're getting 65
gigaflops out to the out of the
integrate I'm actually using the Intel
integrated graphics for this one so
that's already not too bad but this is
like I set the straightforward
conversion from
a cpu parallel version to C++ amp if you
use a more advanced mode like the C++ em
tiled mode then you see now we are
already getting 134 gear flops out of
this laptop GPU and yeah see now it's
perfectly parallelizable this laptop
also has an nvidia quadro inside so
normally you should be able to use both
of them at the same time but there is
something wrong with the configuration
and the number of gay flops actually is
dropping by a few so not sure why that
is happening normally you should be able
to use to it rotate genius GPUs in
parallel so here is a screenshot of an
older machine it's more a desktop
machine a bit more powerful this one has
six cpu cores and you see you we get
around 31 gigaflops out of the cpu this
machine has to AMD Radeon GPUs in it and
when we use both of them at the same
time then you see we are getting one
teraflops of single precision
performance out of these two GPUs and
this AMD Radeon HD 500 5000 a ton it's
not the most recent one in fact nvidia
just released their quadro k6000 and
according to their specifications you
get around five point two there are
flops of single precision out of a
single GPU that's of course
theoretically so you have to lower it a
little bit the next demo that I want to
show is a cartoon Iser what it does it
takes an image or even a live stream
video and it makes a cartoon image out
of it
oh this laptop doesn't have any webcam
so I can only show the static image
version first we load our image and here
you can choose how we shoot cartoon eyes
it so there is a CPU single core
implementation but I'm not going to run
it because it will take like a minute or
two minutes so we're going to wait for
that so let's run the multi-core version
I also want to show you
put it on top just
okay so this is a multi-core version and
you see that all my eight course will be
at one hundred percent calculating and
it should take around 12 seconds or so
eight seconds so it took eight point two
seconds to calculate a cartoonist
version of this image if we switch to
c++ amp then you see it takes 300
millisecond 80 milliseconds on a little
bit more powerful machine that means you
can do this in real time with a webcam
and have 30 frames per second you see if
you Carsten eyes it on the GPU the CPU
there is no spike it's not it's not
doing anything there is also something
called warp I will come back to that
later but I just want to demonstrate it
it's actually an emulator of DirectX 11
GPU and if you run it this is actually
running in software now it's not using
any hardware acceleration if we run this
one you see it takes a little less than
two seconds while my own implementation
on in software Duke 12 seconds so this
is highly optimized emulator but I'll
come back to that later so yes can you
please use the microphones for them
question is recorded why did the images
look different but it was yeah
organizing the cognized version or no no
actually the GPU quarter neither is even
doing more than the cpu Carsten eyes
because the car tonight the result of
the CPU carcinogen was not completely
customized either you still saw some
some of the pictures through it so
so that's why we're to demo so let's now
continue small introduction so before
2005 you had something called a free
lunch so every year processor would be
becoming more and more powerful single
thread performance would increase every
year so basically if you add a new
feature to your application and it was a
bit slow you could just wait a year and
then it will be more than fast enough
unfortunately since 2005 that's not the
case anymore and clock speeds actually
have almost stabilized they are not
increasing pretty much what cpu vendors
are doing now or putting more and more
cpu cores on one piece of silicon that
means single thread perform stays the
same so what is the result of this if
you want to have scalable performance
with current compete with currency pews
and future CPUs you have to have
parallel ism either you do it on a cpu
or on the GPU or on both but there is
nothing like wait a year and will be
fast enough so that's finished I have
one slide on cpu parallelism you can do
that with vector instructions like the
SIMD single single instruction multiple
data for example you have as a Z you
have a a VX like I said all modern CPUs
have these and the other thing you can
do is using multi-threading you can use
frameworks like marks of ppl parallel
patterns library or Intel TV be reading
building blocks and in fact if you use
visual studio 2012 then there is an
outer vectorize ER and an out of
parallel wiser built in the auto
vectorize ER is enabled by default out
paralyzer you have to switch on of
course this is a very hard problem for
compiler so there are a lot of
restrictions that you have for your
loops before that the compiler will
factorize it or paralyze it
so this is on the CPU what about the GPU
you can either target nvidia gpus you
using cuda but that software will not
run on AMD cpus GPUs if you want to
optimally target AMD GPUs you you have
to use opencl you can also use direct
compute from Microsoft but that's that's
an H L&amp;amp;SL higher level shader language
it's more like a c language it's not C++
and we want to use C++ and yeah as
allies last bullet set actually all
three of them are more like C so they
are not really c++ there is not that
much type safety and generosity using
templates and so on and another problem
is yes it's its heart if you want to if
you don't know what hardware your
software will run on then you have to
target both Nvidia and AMD so it means
you have to learn two different
technologies so C++ amp is a solution
for this and it's actually next
abstraction layer over your GPUs it's
pure C++ nothing see so you have all the
type safety of all the generosity in the
form of templates it's also not an
extension there is oh it's actually
almost completely library code except
for two extra key words which are tile
static and restrict and I will come back
to them later and C++ amp is available
on all machines where you install they
say reduced so you don't have to install
anything extra the best thing is it's an
open standard so Microsoft designed it
but it has been it's now an open
standard and intel has an experimental
implementation and AMD also as an open
source implementation since a couple of
weeks
so I set its vendor-independent it works
with AMD GPUs it works with intel z GPUs
with NVIDIA GPUs and everything that
supports DirectX 11 it will work on
there is this fall back warp driver
which stands for the windows advanced
rasterizer platform and that one isn't
highly optimized and it uses vector
instruction it uses parallel code it's
an emulator for DirectX 11 GPU and as
you saw in the demonstration it's
depending on on your problem it might be
better to just write C++ app code and
let c plus plus M fallback automatically
to warp that might be most performant
and writing your own see software
implementation of your of your algorithm
and you're in the future it's it's
extensible so in the future who knows
maybe we can accelerate on fpgas or
maybe offside cloud computing course
then you have quite a cost of uploading
it to the cloud perform the calculation
and downloading it again but all these
things could be possible in theory and
they support something called the
heterogeneous mix of accelerators so a
few if you have a machine with an AMD
GPU in it another video GPU then c plus
plus m can use them at the same time and
distribute the load or both of these
accelerators
so what is faster if we are talking
about it two or a 3x performance
increase that's just a little bit faster
you can either you do a little bit less
work you do a little bit more work or
you wait a little bit less for the same
amount of work but you're not going to
rewrite your application or change your
architecture or anything if you're
speaking about five or 10 X and
performance increase that's already
starting to become significant so you
might start considering rewriting parts
of your algorithms or parts of your
applications but yeah that's that's
pretty much it pretty much it's it but
if you're talking about a 100x
performance increase and that's
something that you can reach it with
using C++ amp then that's actually a
game changer then you might consider
changing your complete platform or re
architecture your complete application
and here are a couple of problem domains
that's coming from our video let's give
some idea about the possible performance
increases that you can get so if you're
doing a lot of financial simulation
pointer is not really working then you
get you get an almost 150 x performance
increase and even things like the string
matching in the lower right corner you
can get answers the X performance
increase by running it on GPUs so it
doesn't have to be always graphics
related it can be completely different
just a little bit about the difference
between a cpu and GPU so CPU has
compared to a GPU a much lower bandwidth
the GPU really has extremely high
bandwidth and the CPU you have these
four cores eight cores machine if you go
to aziem say on it tests I think 16
course but that's pretty much it a GPU
you're talking about like thousands of
2,000 computing units so that's that's
highly parallel
the CPU of course it was completely
generic and general good because it has
to run everything as you run your
operating system all your programs and
so on GPU it really likes data parallel
code so if you have a lot of data and
you have to perform something on all
that data of course programming for the
CPU that's mainstream for a very long
time and now with C++ amp it's also made
extreme for for GPUs it's part since
visual c++ 2012 and obviously there is
complete integration into Visual Studio
so you have all the expected debugging
tools you have intelligence if the
profiler and so on and the Microsoft
implementation is built on top of
direct3d but this is not in the standard
this is not a requirement so the AMD
implementation for example targets
opencl it translates c++ amp code into
open open CL so that's for the
introduction let's go now a little bit
more technical bit more interesting I
assume it's the first thing you have to
do everything is in one header file you
just have to include a mph other file
and it's in a concurrency namespace
there are a couple of new classes six
actually you have an airy and a review
that's for representing your data you
have an extent do represent the size of
your of your problem domain you have an
index which is a point inside your
inside your space that you're working on
and then you have an accelerator and
accelerate review which allow you to
query information about your
accelerators there is only one function
peril for each and there are as I said
two new keywords the restrict and the
tile aesthetic I'll come back to all of
these some next slides
the first one is the concurrency array
it's a template it has to template type
parameters it's the type and the
dimensionality so that type is the type
of the data that you want to store in
your area and the dimensionality is is
it visit the one-dimensional array
two-dimensional three-dimensional and so
on it can go up all the way to 128
that's pretty hard to visualize but it
is possible and if you construct an
array you have to explicitly call the
copy method to copy the data to the
accelerator and to copy it back to the
CPU here is an example of a
one-dimensional area of 10 floats so you
specify the type hear you say that it's
one dimension and in the constructor
argument you specify how many elements
you want in in that one-dimensional area
here is one over three dimensional
arrays or your specific doubles you
specify double it's 3 dimension and then
you specify the size of each dimension
the next one is an area of you a review
it has the same template diaper and the
parameters so type and dimensionality
what this one does it's not allocating
any memory for you it's actually
wrapping your memory so that C++ amp can
use it and the nice thing about this is
that it will automatically transfer the
data from your user allocated memory to
the accelerator in the beginning of the
calculation and at the end it will
automatically copy it back to the CPU
it's something else it's the review is
and the area also is danced in the least
significant dimension and that will
become clear on one of the next slides
in a review it wraps user user allocated
array so you can make a reit write a
review that's just like on the previous
slide
but you can also make a read-only buffer
and that's simply by specifying by
specifying const here and by doing that
you're actually optimizing the code
because it's a read-only area so you
will not change anything it that means
that C++ amp all only has to copy it in
the beginning to the GPU or to the
accelerator but there is no point in
copying it back to the CPU at the end
because it's read-only so it cannot
change and the opposite is also possible
you can have a right only buffer this
one you have to define just as you would
define a normal a review but then you
immediately call the discard data method
and again c++ amp can optimize this so
it sends it right only there is no point
in copying some data that will be
written over anyway from the cpu to the
accelerator so this one will only copy
the data at the end of your calculation
from the accelerator back to the cpu
here is a graphical representation of
the extent class so extent is it
represent the size of your of your space
it's an Undine tional space so give
three examples so this one is a one
dimensional extent of size five you have
a two-dimensional one there and the
three-dimensional one on the right and
you see they are dense in the least
significant dimension so the least
significant dimension is a 4 here so you
see the four they are next to each other
so it's dunce in that dimension then we
have that index class which the extent
and the index de belong together they
work together and the index allows you
to define an n-dimensional point inside
your n-dimensional space so in this case
you're just defining this red square and
here also so again this is all so first
you have to specify this dimension and
second the horizontal dimension so the
least significant one is is the dense
one
then I said there was only one function
that you can call for C++ amp and that's
this parallel for each as that's your
entry point in the c++ amp it requires
two parameters just the extent or your
your space in which you're working and
along the function and the extent that
you specify here that's also used to
decide how many threads C++ amp will
launch on your accelerator the lambda
this one has to be marked as being
restrict amp as such and that means that
the compiler will verify that you're not
doing anything inside your longer that
will not work on a JP you when this line
execute it will send the work to the
accelerator and it will immediately
returned so it's not blocking and so the
rest of your code following your peril
for each will will run in parallel while
your GPU or all your accelerators are
doing the work because it's non blocking
your if to remember that your longer
that you've passing you have to capture
everything by value or except for
concurrency array object they have to be
captured by reference here is a very
simple example it adds to two areas
together you have an area amb with a
certain number of elements in it and we
want to calculate the sum of it so
that's pretty straightforward just a
simple loop if we want to convert this
to em first thing you have to do is
include the header file and let's just
add a using namespace here then we
convert our input errors into a review
so that C++ amp can make use of them
since this will calculate a plus B
equals sum so a and B are read-only eros
so we define this as a review of
constants of one diamond of one with
dimensionality of one the sum on the
other end is right only
so we immediately call this card data
then we call our parallel for each and
we specify the size of our space that we
are working on which is simply the
extent of our some in this case ok
that's the restrict amp keywords that I
was talking about and one of the next
slides I will explain what kind of
restrictions there are there are
actually quite a lot inside your lambda
you have an parameter and index into
your space and this one you can use here
to perform the calculations and I said
the eruv you will automatically copy the
data from the cpu to the GPU and the
other way around one the calculations
are finished so what about these
restrictions if you mark a function or
alumni as restrict amp then it can only
call other restrict amp functions it has
to be in line nabol and you can only use
a few data types in unsigned int float
double and bull and structures and areas
of these so that's already quite a lot
of restrictions but that's not all
actually this is the whole list of
restriction so things like recursion
virtual functions pointer to functions
all that stuff is not allowed go to
label statements are not allowed but
they are actually also not allowed on
this on the cpu the exceptions don't
work obviously assembler you cannot use
and there are lots of restrictions here
and this keyword the restrict amp is
actually part of the signature of your
function so that means that you can
overload on it here we have a function
foo that is restrict for the cpu and EMP
at the same time so that means that this
function will come and be executed on
the cpu or on C++ amp but you can make a
distinction so we have function bar that
is
only on the restrict keyword the first
one will be a will be used when you run
it on the cpu the second one will be
used if C++ amp is using the function
now I didn't say anything about error
handling yet so let's let's fix that so
when the parallel for each code is is
gold then it will immediately return and
it will center of the work to the GPU so
at one point the CPU will block to get
the data from from the GPU and that's
called a synchronization point and there
are actually three ways to have a
synchronization point can either do it
manually by calling the area of you
synchronize and that's the recommended
way so because at that point you can
perform your error handling I will show
that in an example code later there is
also an automatic synchronization point
so whenever cpu code is observing your
arivu that you gave then it will block
until your accelerators are finished
with calculating it but if there is no
try catch block at that moment then you
will lose all exception that might be
tried by c plus plus m the third one
that's the worst case that's there is a
synchronization point when your arivu
goes out of scope and that means that
the distractor of your arivu will
actually block until the GPU is finished
or until the accelerator is finished but
a distractor is not allowed to throw any
exception so they will be completely
silently in Orton this is the worst case
so best case is to manually synchronize
and have a try catch block around it
we have these two helper classes
accelerate and accelerate review this
these allow you to query all the
accelerators are installed on your
machine and to ask if they support
single threaded single performance
single precision floating point
operations or double-precision
floating-point operations and so on and
this is a very simple example it just
gets all the accelerators in your system
and just looped over them and write the
description to standards out there is a
sting called tiling this pretty advanced
feature of C++ em so i'm not going
deeper in on this basically you will
split your real work in smaller batches
then so that they fit in you're in
you're in a faster gash on your GPU then
you can get much much greater
performance increase but yeah there can
be some race conditions and you if it's
it's much more complicated so that's for
another presentation let's see how we
can write C++ M version of a Mandelbrot
roundish so this is straight forward
straight forward single-threaded
implementation it just loops over all
the rose in my image and for each row it
loops over all the pixels in it and then
here it calculates a Mandelbrot value
for each of these pixels if you want to
convert this to do a CPU parallel
version using marks of ppl it's actually
very simple not sure if you so what what
changed but it's basically just this
instead of having a normal for loop you
just write parallel for you say that it
has to loop from minus half height to
half height with a step size of one and
then the body of the original for loop
you convert to a llama
and that's basically it um at the end
you have to close your your parallel for
function call and just notice that I'm
capturing everything here by reference
it's actually not not recommended to
capture everything automatically by
reference or x value it's better to
explicitly state what you want to
capture so that you don't have some some
strange problems with that now we now we
have the ppl version now we can convert
this one to C++ em first thing is we
convert our our array I will buffer to
an array view so c plus plus m can use
it this one is just calculating a manual
broad image so it's right only so we
immediately called the discard data in
our review instead of parallel for like
for the ppl version we call parallel for
each we specify the size of our
two-dimensional image and then we
convert and then we have our lambda
lambda expression this one is a
two-dimensional image so we have a two
dimensional index here so the first
thing we do is we convert our two
dimensional index to an x and y that I
can use it in the rest of my
calculations then you cannot use the
normal mathematical functions like
logarithmic and square root you have to
use the C++ version so there are two of
them here i'm using the first log and
fast sqft those are single precision
versions of the of those functions if
you want double precision then you have
to use the ones from the precise melt
library but not all GPUs support double
precision so you will have to check
using the accelerator and accelerate
review classes you have to check if your
accelerator is supporting double
preciate precision or not and provide
for a fallback if you want
so then we calculate our value for for a
certain pixel and we store it in our
area view and the last one like I said
the recommended way is to manually
synchronize and that is the place where
we can put our that's for for our error
handling so we have our synchronized
here and we just put a try-catch around
it and here we can react to two
exceptions thrown by c++ app so if you
don't do this seek the aid synchronize
their then you will lose these these
detailed exceptions so let's see how it
looks like
so now it's as button does this
programmer art so it's not really
beautiful this is the S button for a
single threaded run during um at the top
here you should see I cannot see it yeah
so it takes 500 millisecond it's a very
low resolution this project so it's not
that slow but it's not really
interactive as you can see if i switch
to ppl that's already much more
interactive and I'm getting like 78
milliseconds for each frame probably not
going to get too much performance yeah
still half but that's because the
resolution of the the projector is
pretty low if you render it on a much
higher resolution than the difference
between the single threaded the ppl
version and the C++ lamp version is much
more pronounced but yeah now it's
perfectly interactive thing is this is
the single threaded version so if i zoom
in long enough so that's the single
precision version so you start seeing
artifacts if you don't want this you
need to implement a double precision
version if your GPU supports it
so a couple of words on visual studio
integration of course it's completely
integrated in visual studio since 2 2012
we have all the things that you want you
have cpu GPU break points even
simultaneously but there are a couple of
restrictions there again you know you
can inspect all the GPU threats that are
running you have the parallel stacks
window the parallel watch window and the
concurrency visualizer so GPU break
points are supported but if you're
running Windows 8 or Windows 7 you have
to make a choice you can either put a
breakpoint on cpu code or you can either
put a breakpoint on GPU code and yeah
you have to tell them the blogger what
you want so here in the you specify if
you want native only or GPU on you
cannot have both but both are possible
with restriction so it requires windows
8.1 windows 8 will not work it requires
8.1 it requires at least visual c++ 2013
and you have to use the work driver so
the software emulator lets you have in
your project properties you have to
specify explicitly that you want to work
use the warp software accelerator when
doing that then you can have multiple
breakpoints in CPU code and in GPU code
so here in this example the first
breakpoint is obviously in CPU code it's
on a try dry line the second break point
is inside my peril for each so that's
inside GPU codes when you break at that
point you can inspect all your GPU
threads at the bottom here you see in
this particular case there are 256
threads active
and for each of these threats you can
inspect the local local variables in
this window so you can see what each
thread is is computing there are lots of
threats of course going on here there
are only 256 four of them are blocked
for some reason I'm but on a different
Hardware you can have like a thousand
pets or or two thousand threads so it's
possible to do flag them filter them
group them or to free certain groups of
threads and unfreeze them and so on I
mentioned the concurrency visualizer it
used to be included with visual c++ 2012
but it's not anymore in 2013 but you can
just download it from the visual studio
gallery and install it and I recommend
to give it a try it shows you the
activity not only of the GPU but also of
all your CPU codes of course you can use
it to look at performance bottlenecks
you can use it to investigate how much
time you are wasting copying your data
from the cpu to the GPU and the other
way around here you can see if threads
are or blocking each other and and so on
it's a very interesting tool if you're
doing parallel code here is a simple
screenshot of the mandelbrot program
that was running so first it is
rendering single threaded so you see
there is only one core that is doing any
work and a GPU is completely silent it's
not doing anything and I'll switch to
GPU rendering and moved around a little
bit interactive and you're just
rendering one frame so it barely see it
here it's rendering while while moving
then you see the CPU is not doing
anything in that case
so c++ amp it allows you to make use of
heterogeneous accelerators like in a
video AMD intel and so on and well you
saw the examples it's not that hard if
of course it depends on your problem
domain like the Mandelbrot rambler which
is just calculating independently for
each pixel a value that's very easy to
paralyze of course if your problem that
you're trying to solve is not not not
that parallelizable then you won't see
the 100 to 150 x performance increase
obviously it's a high level abstraction
in c++ not in see em yeah it can use
multiple vendors at the same time and I
said it's an open specification and
effect right after it was an open
specification Intel was working on a
proof of concept called Shevlin park but
i'm not sure what happened to that that
one was based on on the clang and opencl
i haven't heard anything about that
afterwards but last month in august AMD
announced an open-source implementation
together with multicolor where and that
one is converting c++ am too opencl if i
remember and it works on both windows
and linux and maybe also mac OS so it's
not just theoretically anymore it's
really an open specification there is an
open source implementation of it so and
AMD is already proposing a couple of new
features which might get into the style
night or might not so now discussion is
starting about that there are a couple
of other interesting presentations about
parallelism because I set these days all
your cpu's even your smartphone has four
or even eight cores these days so you
need to become parallel aware and have
you need to start writing parallel code
yes there is a question yeah sorry if I
if you went over this the beginning I
would sooner talked it ended early but
how old is this do c++ aunt interact
with C++ threading like Atomics things
like that um so how well does C++ amp
interact with the C++ trailing library
atomix and yeah well it doesn't so the
Atomics and the the C++ start reading
library that's for CPU it doesn't say
anything about GPU and that kind of
stuff you cannot use inside your
restrict amp code do you hear there's
like a there's like a a cost calling
some of you know some of the functions
to set up the set up the extent and
things like that do you have a like a
kind of a rule of thumb of like the
number of elements that you might be
iterating over where it would start to
make sense to take advantage of GPU like
we say okay well if I've got like a
hundred stainles then yeah I should
start considering putting this over to
C++ amp or if that's or if we're talking
a thousand or you know or 5000 gessoed
everyone's like that that you would
recommend oh no there are no rules like
that because it's it's less important
how big your your extent is or well your
your areas but it's it's more important
the calculation that you're doing on it
of course if the best way is to profile
it and to use the concurrency visualizer
and there is not really a rule of thumb
for that no so you can use a concurrency
visualizer to see how much time you're
wasting copying the data to your
accelerator and the other way around but
there is nothing like there for 100
elements you have you can do it less
than one under to you don't do it no
it's unfortunately not
so you mentioned the Intel working with
OpenMP so can you go into any details
about the implementation of this the
Intel version you mentioned Intel was it
yes transfer to OpenMP 20 not open open
open CL yeah so what what are you
transferring to well I don't laugh much
details about it there I just read
somewhere that Intel was working on a
proof of concept I'm not even sure if
it's public they were just trying to do
it what they do is convert c++ amp to
open CL codes and then that one is
running on on the GPU our Microsoft is
converting it to direct direct compute
so it converts it's too I think hlsl I
level shader language kernels yeah so
the Microsoft implementation requires
directx11 GPU but the Intel
implementation or the AMD AMD open
source implementation they don't
necessarily require the directx 11 GPU
so my question is related to that so
that so the code is generated by the
compiler is embedded in the in the
compiled image this non CPU code is
embedded in the compiled image is that
right yes so the CPU em goat is for
Microsoft I think convert to hlsl and
that's embedded inside your your
executable yeah I get a couple of fellow
yeah does this work with just 32-bit
colors this work with sixty four-bit
binary steal it worked with both of them
out with both of them and then if you
let's say it you're let's say you're
building this on a with sixty four-bit
binary and you've allocated a huge chunk
of memory that's re that's maybe more
memory than can fit in the GPU I don't
know much about GPUs but let's say it's
you know it's it's just more than you've
got available on your GPU
is that going to just take parts of the
job and do parts of the job is that just
gonna fail you know that's a good
question so what happens when your
allocates a buffer that's too big to fit
on the on the GPU not sure I would
expect it to throw an exception oh um so
it's not like it would just say all
right I don't have I don't have end
bites but I i have i have half that on
the GPU so i'll just take half the job
and start no it will not do it
automatically but that's what you can
use a tiling mechanism for you so you
can break up your problem into smaller
tiles okay so that's a that's a tiling
job yeah okay thanks and these days some
GPU cars have already upwards of eight
gigabytes of memory so you really need
to allocate a very big buffer to hit
that limits but yeah of course yeah it's
possible is there any special treatment
for sparse matrices let you do it
copying all the data across the GPU and
back is there any special treatment for
sparse data nope so just copied
everything is coping commentary yep so
on Tuesday it's an interesting day for
if you're interested in parallel talks
we have writing data parallel algorithms
on the GPU by eight Miller we have
talked about open mp4 GPU lxl
accelerators and other things by michael
wong and we have decomposing a problem
for parallel execution by Pablo Halpern
so that's what I what I was mentioning
C++ amp is easy to use or ppl Mike's of
ppl or Intel TVB are easy to use if your
problem is is easily split into parallel
parallel workloads if that's not a kiss
them you have to redesign your problem
and try to make it parallelizable
i also want to mention this book there
is a great book written by gate gregory
and eight miller it covers everything of
c++ amp including the advanced piling
stuff so if you're interested in that we
should get a copy of this one I think
it's available in the bookstore thumb it
is okay and if you're interested in
learning more there are lots of
resources and there are on this website
you don't have to write this down I
think the slides will be shared
afterwards and there are like 36 samples
that you can use of all kinds of things
like visualization problems or even non
visualization visualization problem so
and if you have any problems or
suggestions or comments or whatever you
are always free to contact Sharma ramen
which is a program manager of C++ amp so
that's what I had if there are any more
questions do you have any plans or you
know if there any plans to incorporate
something like C++ a app or C++ it up
itself or something like it into c plus
plus the language is that in the cards
or not um some there so the question is
if there is any work on implementing
something like C++ amp and established
they are working on adding parallel
stuff to the to the to establish there
is something they are thinking about
something with policies so for example
if you have the stl algorithms like st
STD sort you can give it a policy and
tell it run this sequentially single
credit or run this multi multi threaded
or and this policy is also vendor the
vendor can change this policy so for
example Microsoft compiler can add a
policy to ellenton C++ amp and things
like that so yeah
yep hi huh I'm Michael Wong I might give
it okay so that okay yeah that that
question on the standard is adding
parallelism in the library purely using
expired but by creating unparallel
rhythms for standard template libraries
that can have different policies
attached to them as you said they're
either vectorize or vectorize am
paralyzed or which is called back paw or
sequential and theoretically it is
possible to add something that says GPU
acceleration depending what the library
came deep in the library if we're
talking about language itself now much
of this obviously is library based if
we're talking about the adding
capabilities the language itself Allah
like C++ am hopes comment was that it's
still too new yet at this point for him
to try to put it into enters into the
standard which is pretty consistent
whatever I think it is I think there's a
lot of different options there and you
talk about some of them I'm going to
talk about how OpenMP does it they all
bring in different perspectives so it's
important for us to gather a lot of the
data in order before we try to solidify
it into the standard so sorry for
interrupting I don't know it might help
what is this interesting it so okay if
there are no more questions and thank
you for coming here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>