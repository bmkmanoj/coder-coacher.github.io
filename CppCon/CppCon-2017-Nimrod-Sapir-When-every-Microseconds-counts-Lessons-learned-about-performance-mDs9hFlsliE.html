<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Nimrod Sapir “When every Microseconds counts: Lessons learned about performance” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Nimrod Sapir “When every Microseconds counts: Lessons learned about performance” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Nimrod Sapir “When every Microseconds counts: Lessons learned about performance”</b></h2><h5 class="post__date">2017-10-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mDs9hFlsliE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Hi, I'm Nimrod from qSpark.
Today I want to talk
about some of the lessons
that I learned in past three years,
working in environment in which
every microsecond actually counts.
So, what makes high-frequency
trading environment unique?
So, in those low-latency environments,
performance is truly a key feature,
since every transaction is a race,
and in many times the second
place simply gets nothing.
This forces the realtime flow to be
extremely optimized in every way possible.
There's no notion of fast enough.
If I can improve the realtime flow
in even a fraction of a
microsecond, it is worth the effort.
I'm going to show a few ground rules
I learned doing my work
in this environment.
So, the first basic and simple rule is,
it is not faster if it wasn't measured.
This sounds obvious,
it was told many times
during those presentations,
but we tend to make a lot of
assumptions and rule of thumb
that under closer inspection may proven
to be somewhat false.
For example, from our environment,
we're using a very
efficient object allocator,
over pre-allocated memory space.
And we always assumed it would be
much more efficient than Malloc.
But some measurements show that Malloc
is sometimes more efficient.
That was because our
implementation was not optimized
to reuse hot memory
segments from the pool.
When we changed that,
the issue was resolved
and the performance boost
was very noticeable.
To get those kind of measurements,
we create a performance
graph that looks like this.
Let's take a closer look.
So you can see, I hope you
can see, that the graph
gives the realtime cost of
every action in the system,
which gives us a performance baseline
that we can use to compare
different environments,
different configuration,
different version.
Of course, creating such a
graph has an overhead by itself.
So we can't always use
this kind of measurement
in actual production environments.
The second rule I want to give is,
branching is sometimes more
expensive than you think.
We, of course, tend to rely
a lot on branch prediction.
But branching is prediction still
a real performance issue for us.
So we are always trying to minimize
the branching in our code.
The most obvious way and
common way of doing that
is to move as many decisions as possible
to the compilation and
initialization time,
and model C++ provides tons
of techniques of doing that.
And I'm sure you heard about
them during this convention.
I want to look at another method,
a design in which we sacrifice, almost,
the safety checks in sake of performance,
allowing us to receive
a more deterministic,
although a risky flow.
So, this is your standard
error handling flow.
In that flow, the code contains
a lot of safety checks,
which translate into a lot of branching,
and sometimes some
branching mis-prediction.
The logic here is simple.
You understand the benefit,
if we have an issue
we try and discover it as
soon as possible and handle it
as soon as possible in
a safe and graceful way.
But sometimes we strive
to a different design
in which we assume that
our input is just okay
and everything good and safe,
until we actually have to use it.
When we do, we either succeed
and everything is great,
or we pay the price.
The price can range from
anything, expensive exception,
to data corruption or
even segmentation fault.
All are very expensive
to handle and to debug.
And stabilizing this kind of environments
tends to be much harder
and very expensive.
When it does become stable,
we get a more predictable
pipeline of events
that allows to eliminate a lot
of the redundant branching.
This is, of course, very dangerous design,
but sometimes the
insurance fee is too high
and you have to waiver it.
The last rule I want to give you
is to watch for the
rare, yet critical, flow.
Let's look at this code.
It checks repeatedly if hell is frozen.
If hell hasn't frozen yet,
it updates some counters.
Otherwise, it buy oil stocks.
Of course, the most critical action here
is to buy oil stocks
when hell freezes over.
But since hell rarely freezes
over, once, maybe twice a day,
all the runtime optimization,
branch prediction,
CPU cache, will all
work to work optimizing
the more likely case, in
which we only update counters.
Fooling those mechanism without triggering
unwanted flow is quite complicated,
and I can't get into it in
this small presentation,
but at least try and map those flows
and their impact on your latency.
Thanks.
You can find me there.
(audience applauding)
- [Man] All right, thank you.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>