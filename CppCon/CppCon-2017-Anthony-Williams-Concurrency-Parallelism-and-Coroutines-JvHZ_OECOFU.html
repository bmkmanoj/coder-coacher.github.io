<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Anthony Williams “Concurrency, Parallelism and Coroutines” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Anthony Williams “Concurrency, Parallelism and Coroutines” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Anthony Williams “Concurrency, Parallelism and Coroutines”</b></h2><h5 class="post__date">2017-10-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JvHZ_OECOFU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Okay well good afternoon everybody,
I hope you've all been
enjoying the conference so far.
(applause)
Excellent. (laughs)
Okay so I'm gonna talk to you about
a few things this afternoon.
First off I'm gonna talk to you about
the support for parallelism that we've
introduced in the C++ 17 standards.
Then I'm going to do a very brief section
on the Coroutines TS and
likewise on the Concurrency TS.
Then a bit about how coroutines
and parallel algorithms can be related,
and finally I'm gonna finish
off with a brief summary
of the grand unified executors proposal.
So that's the order of play.
Lots of things from TS's are all
in the standard experimental namespace.
That is very long-winded for slides,
so we're gonna assume that we've got
this namespace alias in
effect for all the slides.
Stdexp instead.
So, Parallelism in C++ 17.
C++ 17 can provide with a
whole new set of overloads
for just about every single one of the
standard library algorithms.
We have a new first parameter.
This is a magic execution policy.
So for each you have ExecutionPolicy,
and then your begin and end iterators,
and the function you're trying to pass on.
This execution policy tells the library
what you're wanting out of
your parallel execution.
It can be a sequential execution
that you're asking for,
in which case it's just going to
run it on the same thread just as usual
with a few different constraints,
because now it is a parallel execution
even though it's the sequential policy.
And so it has the same
constraints as parallel execution,
and we'll look at those in just a second.
You might have the parallel
policy execution par,
which means that hopefully
your compiler and library
are going to split up the work
that you've given it
across multiple threads.
And it's going to divide up
the work in some random order,
which might vary between runs.
But each individual invocation
of a user supplied function
will run in it's entirety
on a single thread
in some relative order
compared to the other part.
So if you've full reach,
you're going to run your single function,
the function you supplied,
for each element.
And on a given thread it might
run a random set of elements.
It might run five, four, seven,
27, three million, some random order.
And it's an unspecified order,
it might vary between runs.
But on any given thread you can say
it did these elements in this sequence.
This is different from
parallel unsequenced,
the final standard
supposed execution policy,
because that one allows
your functions to overlap.
Now this is sometimes hard
to get your head around.
But if you think of it like vectorization,
if you've got a vectorization instruction,
you're running four each
and what you're doing
is adding five to every element.
If your process has
vectorization instructions,
it can probably--
No if your elements are integers,
then it can probably manage to add five
to quite a few integers
in one instruction.
And so you're hoping therefore that
the compiler and library are gonna
work together to do that for you.
So not only are your functions
running on multiple threads,
but it's overlapping the execution
and running them together simultaneously
on a single thread if possible.
But it's not just those
vectorization instructions.
It's generally unsequenced.
So if you're pushing things onto the GPU,
then the things that the GPU
can do as a vectorization,
there's much more scope there than
what there is just on a plain CPU.
And it also allows the
compiler to look at your--
If you pass it something
that's inline like a lambda
to the parallel algorithm,
then it has a lot more scope
then for looking at that,
the library can unroll a loop,
and the compiler can then reorder things
as an optimization and you know
it gives it much scope for running faster,
but you can't rely on each
individual function having completed.
So yes you say you know thread
one is running elements,
five one 17 three million and 23,
but it's all matched them up together,
so you can't say which
order it did them in,
'cause it did them all together.
It started all of them,
then it did a bit more,
and then it finished them all off.
And this has further consequences.
And then of course your runtime library
might supply you with some
additional implementation
to find policies.
Anyone who went to the talk by
the code play guy this
morning will have seen
they had an implementation-defined policy
for how to ship stuff onto the GPU.
I said most of the algorithm is there,
it really is, there's over 100.
I've put in bold if anyone can read them,
there's a few key ones there.
Copy, count, for_each.
Reduce, transform, transform_reduce.
Sort.
Most of them are there,
so a few things aren't.
Accumulate isn't there,
because the specification for accumulate
is inherently in order.
You have to do each element
having done the thing,
the previous ones.
However the parallel equivalent is reduce.
Reduce is an out of order generalized sum.
And so if you can cope with
things being out of order,
which if you're dealing
with floating point numbers,
then you might get a different answer.
Because the precision will change things.
If you have a lot of small
numbers and then one big one,
then it can depend which
order you add things up.
If you use reduce, you
need to cope with that.
Accumulate has a specified order.
But you know that's for your to decide
if you're trying to parallelize things,
the option is there.
So yeah for the most part it's easy
to parallelize your code,
if it works with a
standard library algorithm.
You just have an execution policy.
Of course you need to ensure
that there is thread safety.
Because the library doesn't
know what your function does.
So it's going to make some assumptions.
So if you're running things
as a sequential policy,
then it's sequential.
There's no additional
thread safety requirements,
because it is all run on the same thread
that you actually called
the algorithm from.
If you specify the
parallel execution policy,
execution::par,
then it's up to you to ensure that
if the library runs your user supply code,
so you know with a four each example,
the function that you've supplied.
If you're doing sort then a comparison.
Or a swap.
Provided that the elements being worked on
in different threads
are different elements,
then those invocations should be okay,
it's up to you to ensure they're okay.
So this is just like you know
if you've got an integer.
Two threads each working
on separate integers, fine.
Two threads working on the same integer?
That's potentially a problem.
It can be a data race,
and undefined behavior.
And so this is just a general rule.
So for the most part,
you don't need to do anything
to deal with this constraint.
It's only if your code does anything fancy
in terms of synchronization
itself that you need to worry.
If you're using the
parallel unsequence policy,
not only do your operations
need to be thread-safe,
but they must not use any
synchronization mechanisms.
So no mu-tecks, no atomics,
nothing that imposes
synchronization overhead.
Because not only might
they be interleaved,
which obviously if you've got two things
and they're both trying
to fire the same mu-tecks,
and they're both on the same thread
and it's trying to interleave them,
you can deadlock your own thread.
But also the whole concept of which thread
they're running on might be
somewhat you know disjointed,
and they might actually look as if
they're migrating across threads.
Particularly if things
have been moved to the GPU,
which doesn't have the same
concept of thread as the host CPU does,
then the mapping might
be somewhat arbitrary,
and you can't therefore rely on
them being on the same thread.
So synchronization is just completely out.
But again, for lots of
things, it doesn't matter.
If you're trying to add
five to a bunch of integers,
then you don't care which
thread it's running on.
And the other thing
that really changes with
these parallel execution policies
is the behavior with exceptions.
If you're doing a normal for each,
and your function for the millionth entry
throws an exception, that's fine,
it will just not process any more entries
and propagate the exception out.
And then you can catch it in your code.
If you're running this parallel
and one of the instances
throws an exception,
it has no means of doing that.
Say you're running on a compute cluster,
you've got 1000 cores,
and the operation that's running on
each of those 1,000 cores
throws an exception.
You can't propagate 1,000 exceptions out.
So the original proposal before C++ 17
had a sort of aggregate exception list
of all the possible exceptions that
got thrown all gathered together.
And the committee decided no no no no no,
we don't like that.
If you're going to make this parallel,
don't throw exceptions.
So if you throw an exception
for a parallel algorithm,
even if you chose the sequential policy,
it will terminate.
Obviously if the implementation provides
and additional policy it's up to them,
because it's only the standard
policies that force this.
So yeah so for lots of code,
you can just add execution::par
as the first parameter.
And it will parallelize your code.
But the thing is this is an optimization.
And like all optimizations,
you need to profile to make
sure that it's worth it.
There is overhead to parallelizing stuff.
It's got to synchronize
between the different threads.
It's got to divide up the
work in the first place.
Again for those people that were at
the code play demonstration this morning,
they actually demonstrated
in real time what that is.
They parallelized stuff on the GPU
with the smallest size they
had in their demonstration.
It was slower.
But by the time they got to the
biggest size in their demonstration,
it was five times faster.
And this was just on the built in GPU,
on chip on the CPU that is in
most of the computers
that you buy these days,
have an on chip CPU.
So there is potential
for optimization there,
but you've got to make sure that
your problem size is big enough.
If you try and sort 10 elements,
there's no point in trying
to make it parallel.
But actually there might not be any point,
even if you've got 100,000 elements.
It's worth checking.
So yeah so, so coroutines.
Fundamentally a coroutine is
an extension of a function.
The function might be
suspended mid execution
and then resumed at a
later state, a later time.
And it will resume directly
where it left off in the middle
with all the local
variables having exactly
the same values that they
were when it got suspended.
And this includes any function primitives
that were passed by value.
The reference still
refers to the same thing,
but the referred to
thing might have changes.
But you know it does mean
that then when you resume,
you know it really does just resume
as if it was sequential coding.
From the point of view
of in the coroutine,
everything just carries on.
Conceptually there are
two types of coroutines.
Stackful coroutines that store
the entire execution stack,
and stackless coroutines.
The TS is about stackless corountines,
so it is only the local function
that is the coroutine
itself that gets saved.
This does have a strong advantage,
it means everything is localized.
It means that there is
minimal memory allocation,
and so you can have millions
of in-flight coroutines.
If you try to have millions
of in-flight coroutines
with stackful coroutines,
that means you've got millions
of full execution stacks,
and unless your execution stack is tiny,
that's gonna require a
heck of a lot of RAM.
And it does mean that the
compiler can look at the function,
and everything that it
needs to worry about
about this coroutine is just there.
It doesn't need to care what
the rest of the stack does,
how it was called, where it comes from.
And so it can then make optimizations,
it can eliminate that memory allocation
if you're not doing anything that
requires you to keep it around.
Gor has done presentations,
he calls this the magic
disappearing coroutines,
where all the overhead disappears.
So you know there is lots of potential
for the compiler to
optimize your coroutines
because we're using stackless ones.
There are disadvantages.
You can only suspend
the current corountine.
You can't suspend the entire stack.
When using co_await to
suspend your coroutine,
it means only the current
function suspends,
and the call is immediately
returned to the caller.
Which can have you know
knock on consequences
about the way that you
structure your code.
So you need to be aware of that.
In some cases it's not what
you would have ideally chosen.
But there are ways around it,
and you have to design your
code with that in mind.
So in C++ what does a coroutine look like?
It's just a normal function.
The crucial is it must
in it's implementation,
in the body of the
function, directly there,
not in some nested some function call,
but directly there in the body function,
use one of the three magic keywords.
They start with co, co
keywords make coroutines.
It can be wait, co_await,
which will conceptually
wait for something.
Co_yield which conceptually returns
an intermediate value to the caller,
but then allows your
coroutine to be resumed.
And co_return which is an I'm done,
if you resume me then
bad things will happen.
Final result coming out of your coroutine.
And then the return type of the coroutine
must be something that is
designed as a coroutine return type,
and you must have a corresponding
coroutine promise for that.
Which is an internal data
structure you specify
by specializing the
compiler's coroutine traits.
So co_return looks like
a return statement.
Co_return some value, that
returns a final value.
Co_await you pass in some awaitable thing,
and you're telling the
compiler and library
that you're going to wait
on that thing to be ready.
And if the awaitable is not ready,
then you're going to suspend
the current coroutine
and return to the caller,
who will then hopefully do something
and then your coroutine
will resume at a later point
back right there after the await call.
Co_yield is similar,
but the intent is that
you yield with a value.
So that value then somehow gets
propagated out to your caller.
Your coroutine is now expected
to resume on the next line,
so this is different from a return,
it's expecting to resume.
And it will then carry on straight away
from the following line.
Now I mentioned coroutine promise types.
A promise type is how you specify
how the coroutine internals
correspond to the return value.
And it specifies what the compiler
and library will do at those co_return,
co_await and co_yield locations
inside your coroutine.
How it deals with
suspension, what happens,
how the return value gets handled.
And then they also have this
concept of an awaitable.
The awaitable is something that
you can wait for with co_await.
And often they will be related types,
so you might have something
that is both an awaitable,
you can wait for it with co_await,
and has a corresponding
coroutine promise type,
so that you can use that type as
a return value for your coroutines.
But they're not necessarily the same.
So as an example of something
that is gonna be the same,
or you might expect to be the same,
is something like a future.
Either from the C++ standard directly,
or the concurrency TS type features.
A future is a placeholder for a value
that's gonna be available at some point.
And so you might have some function
that's going to give you that,
a future, some value.
And so then inside your coroutine,
you can co_await on a function
that's going to generate you a value,
and it will suspend your coroutine
until your value is ready.
But then you could also return
from a coroutine a future,
and so then when you do,
it will you know wake up
anything that's waiting on it.
Now if these are just standard futures
or standard experimental futures,
then you can then use them
with all the rest of your code.
And if it was a coroutine
that returned that future
and the coroutine got suspended
because the future wasn't ready,
then when you wait for the future,
then it will check depending on
how the library's being implemented,
and make sure that the
coroutine gets resumed,
and when the coroutine
is finally finished,
and the data has been returned
with that co_return call.
Then the future will become ready,
and the code that's waiting
for it will finally return.
So this provides a mean
of meshing coroutines
with code that uses future
from like standard acing,
called from a standard promise.
Whether or not your library provides that,
the standard library provides
that integration varies.
I think the visual GR 2017 implementation
does provide that for standard feature,
but the Clang 5 version doesn't
yet, if I remember rightly.
But they might then add it.
You could also write your own future type,
my lib future instead of standard future,
and then you could
implement that yourself,
it's not particularly complicated.
Now stack list coroutines work nicely
if everything down your
stack is a coroutine.
Because actually then you can say,
co_await on something I'm waiting for,
and then it returns up to the caller.
And the caller well immediately it doesn't
care about what result you're returning,
so you've got some form of
future value coming through.
Then you do some more
code and then that point,
it'll say well actually
now I do need the value,
so I'm gonna co_await on
that future that I got back,
and suspend that coroutine,
which obviously you know it's nice
to propagate all the way up.
So if you're doing that and you want
to work with parallel algorithms,
then you could actually,
the implementation could provide
a custom execution policy
for your parallel algorithms
that makes your parallel
algorithm work as a coroutine.
So you could have you
know a standard for each
parallel_as_coroutine from
begin to end do stuff.
And then you can then co_await
on that returned future
that you get from this
invocation from this
invocation of parallel for_each.
So that's the first way that we could
potentially integrate
coroutines and parallelism,
is because by having this
custom execution policy,
that allows us directly to mesh the two.
So let's leave coroutines
aside for a second,
and we'll have a look
at the concurrency TS.
Which is you know extensions to futures.
It's probably the core bit,
so that's what I'm primarily
gonna focus on here.
There's also other things,
there's also latches and barriers,
which are synchronization objects
for events across multiple threads,
waiting for five events to
happen across a load of threads.
You have a latch and
then when all the events
have happened the latch becomes ready.
Barriers are similar but they're reusable.
They are for when you've
got a bunch of threads
that are working together and they all
need to be in lock step in some fashion.
So they all do their first chunk of work,
and then they hit the end,
and they all wait for the barrier,
and then they loop around again,
they do the next chunk of work.
Each thread doing it's own relevant parts,
and then they all wait
at the barrier again.
And so forth.
Now the latches and barriers are actually
now being targeted hopefully for C++ 20.
They've been pulled out
of the coroutines TS,
and the committee has said, yeah,
we like these pretty much as is.
There might be a few minor tweaks.
So they're going ahead with C++ 20.
We've also got atomic smart pointers.
So atomic shared pointer
and atomic weak pointer.
These allow means of you know--
Standard C++ shared pointer
that we all know and love,
it works across multiple
threads in the sense
that each thread can have it's
own shared pointer instance,
and they can all be
referencing the same object,
and the reference count all works nicely.
Now what we get from atomic shared pointer
is that we can have a single
atomic shared pointer instance
which is referenced in multiple threads.
Possibly because it's like a pointer
to a master data structure.
And so everyone has to come and say,
can I have the pointer please?
But then someone might
want to come and update it.
And so then rather than
having to mu-tecks lock ground
for protecting that master pointer,
then you can just make it
an atomic shared pointer.
And then each thread can do that
and it is inherently safe.
And that's also being told for C++ 20 now.
The spelling has changed.
In the TS it was
atomic_shared_pointer of T,
and now it is atomic angled
brackets shared point of T.
But hey,
what's an underscore to an
angle bracket between friends?
Anyways so I'm not gonna
talk any more about
latches and barriers and
atomic smart pointers.
If you want to talk more about them
then come and catch me later.
But for now I'm just going
to focus on the future part,
because that's what is relevant
to parallelism and coroutines.
So a continuation is a new task that
you want to run when the future is ready.
So you've got a future that's some task
that's gonna generate some value for you.
And you say well when that's ready,
then I want to do another
calculation with that value.
And so you can either wait for
the future and then spawn the task,
possibly as a new asynchronous task using
standard async or thread
cool or something.
But alternatively with the concurrence TS,
we now have this new member
function on the future, then.
So you can say when the
value is ready, then do this.
And the library will take care of
automatically scheduling your new task
when that value is ready.
And you can specify all this up front
as soon as you've launched
off the first task.
You can specify what to do next.
Without having to wait for it to be ready.
Now obviously futures, as
in directly standard future,
and therefore standard
experimental future,
which is based on the
same, are one shot things.
So you can only use it once,
you can only get the value out once.
So if you specify continuation,
then you can't get the
value out any other way.
That value can now only be
retrieved by the continuation.
And so the source is no longer valid.
And secondly the continuation itself
will get a future as the parameter
because the future might contain
either a value or an exception.
And so rather than having to have
different continuations
for the different options,
you know well there's one function
that we get called if we have a value,
and another one that gets called
if we've got an exception.
Then we already have
this packaged up thing,
of a value and an exception.
It's the future,
so that future itself gets
passed into the continuation,
when that future is ready.
So a continuation always
received a future,
but always received a future
that is already ready.
If you try and wait it will not
have to spend any time waiting,
because it's guaranteed to be ready
when the continuation is invoked.
And it does consequently
mean of course that
you can only have one
continuation per future,
though of course that continuation itself
will return a new future
so you can chain them on.
Have a whole series,
but only one at a time.
So let's have a look at some code.
We some function that's
gonna find us the answer.
It's going to return us
standard experimental future.
We have another function that's
going to process that returned result,
only that takes a future as a parameter.
So we spawn our function,
we call find_the_answer,
it gives us a future back, alter F.
And then we chain on our continuation
of process result by calling f.then.
So f2 on this slide will
be an experimental future
for a string because the return
of process result is a string.
If the future contains an exception,
then the exception gets
stored in the future
that's passed to the continuation,
and like normal futures if you
then call get it will throw.
So the concurrency TS
has this nice function,
make_exceptional_future,
which directly stores the future in.
So you don't have to try to catch them
and store a value directly,
or use promise.set_exception or something.
You can just have a future that
already contains an exception.
So we call fail,
and that's gonna give us a
future that holds an exception.
We put on the continuation
with a call of then
and passing of our next function.
And that will throw when it
is called by the runtime library.
And so yeah so the call to
get inside next will throw,
which means that the future
that is returned from our continuation,
that F in foo will hold an exception.
So when on the next line
in foo we call for.get,
that will itself throw.
So it is the same,
the runtime error that
we created at the top
is then propagated across the chain,
and then comes out the final get.
Of course next could catch
and handle that exception,
in which case then it
wouldn't be propagated,
it would be caught and handled,
and then the call to get
at the bottom there in foo
would then not be throwing an exception,
because you know it was
handled inside next.
If you just want the
exception to propagate
and you don't want to handle them,
and having a function
for your continuation
that takes a future as the
result, as the parameter,
is undesirable for your circumstance,
possibly because you
already have a function
that you want to use that
doesn't take a future parameter,
then it's easy enough
to wrap it in a lambda.
I've specified out the lambda
function parameter there.
You know it's an experimental future.
But of course you could just like auto,
a lambda that takes auto F and then
returns a call from what
we're actually wanting to.
And that for.get will throw
the exception if there was one,
which will then be
propagated out and caught.
So you don't have to worry about
what happens at the exceptional case.
And if you don't like
writing lambdas there,
then you can just write
this unwrapped function
which contains a lambda
to deal with it for you.
So then you can say
unwrapped process result,
and that will unwrap the value
and propagate the exceptions
if there were any.
And then call the value
that it got from the future.
Call the supply continuation.
So that's there as a helper obviously.
It's not part of the TS, you'll
have to write it yourself.
You can copy it off this slide,
the PDF will be available
at CppCon at some point
so that you can copy that out.
And call it what you like if
you don't like the name unwrapped.
Continuations also work
with shared features.
But they have some changes.
Shared futures allow you
to call get multiple times,
but you get a cont reference back.
And so likewise you can schedule
multiple continuations
with shared futures.
They can't take a normal
future as their parameter,
because then otherwise you know you might
remove the value that was then trying
to be propagated to all the others.
So they take a shared future as the value.
And the source future remains valid,
because hey there's no
reason not to make it.
And so you can chain
multiple continuations
on the same future either directly there
or across all the copies.
So we have some code,
we've got some function
to find the answer.
And two continuations we
want to use this time.
Next1 and next2.
So we call find_the_answer for the future,
we share it to get a shared future,
and then we can chain two continuations.
Each of which will get the
value from find_the_answer.
And of course they will
have different types,
because the continuation
functions return different things,
so f2 will return an
experimental future of void,
and f3 will be an
experimental future of int.
That's just how you might expect.
Alongside continuations we also have this
concept of being able to
wait for multiple futures.
Because future is all very well,
but if you've got a whole bunch of them,
then sometimes you want to say,
when one of them is ready,
and I don't care which one,
then I want to do some stuff.
I want to process the first
result that comes back.
Now that might be because
you're actually using,
you've got a whole
bunch of cores available
and you're not doing anything
meaningful with them.
So you can say well actually,
I don't know what the best way
to calculate this result is,
so I'm gonna launch oFf five threads
that each are gonna calculate
the result in some different fashion.
And then when one of them comes back,
the first one that comes
back with a result,
I'm gonna use that.
And so when anyone of them are ready,
then I'll do some processing.
And there's two ways of doing it.
You can either pass when_any and
pass the futures one at a
time as function parameters,
or you can have some container that
contains a lot of futures and you can
just pass the begin and
end to the container.
And then depending what you do,
you either get back a future
that holds a tuple of the futures,
or it comes back something that holds
a vector containing the futures
that you passed through.
And in any case they're
stored inside the structure,
of a when_any result which
the key thing about that
is it tells you which one of
these set of futures it was
that was the one that woke you up.
So you don't have to go through and say,
are you ready, are you ready,
are you ready, are you ready?
Oh wait it's that one!
If you've got a whole set of 100,
then that's a bit messy.
And so it actually just tells you,
it was the 97th, okay fine.
I can take the 97th and use that.
And obviously you get the futures back,
because if they were standard
experimental future and you got past it,
it's a one shot thing.
So you've just passed
it in to your when_any.
And so therefor you
don't have the anymore,
so you then need to get
it back in the result.
So yeah, it's great
for specular new tasks.
If you've got something it's like okay,
well when the first of these is ready,
I'm gonna do something.
So say you're writing a web browser
and you've got you know downloading
five images in the background.
When the first one comes back,
then I'm actually gonna update
the rendering of the page.
I don't care which one it is,
I'm just gonna do that processing,
and then I'll do something
else and wait for the next one,
or whatever.
So that's great.
So we have two functions
with three different futures.
Foo and bar.
We then move those futures into an any,
this is moved because
it's passed by value,
and experimental future
is a move any type.
And then it just returns
an experimental future,
so you can chain a continuation.
So f3.when.
Instead of just one you
can wait for all of them.
This time it's slightly simpler because
the return value doesn't need to tell you
which value it was that woke you up.
They were all ready!
That's why you woke up.
But again we've got the two overloads.
And so we can spawn three futures
and then we can wait for all of them.
So how does that relate to coroutines?
We've got coroutines from coroutines TS.
We've continuations
from the concurrency TS.
How does that work?
Well in some sense futures are
ideally suite for coroutines.
They hold a value which is gonna
be available at some point.
You can already wait on them in your code.
They can represent asynchronous tasks.
And you can create one that
directly holds a value.
So these map very nicely to coroutines.
And so you might hope that
your implementation of futures
would work nicely with coroutines.
So the compiler needs to
know what to do with it,
and so the library would have to
specialize coroutine traits to say,
yeah we can deal with coroutines,
you just deal with it like this.
If you're trying to wait on
futures, that works fine.
And then operate a co_await,
which is how do we wait on the future
when we're in a coroutine?
But once it's done those two
things, it all just works.
And so then,
there's a nifty feature
in the continuations part,
which is that if your
continuation function
returns a future rather than a value,
then that future just gets subsumed
into the final return
value of the continuation.
If your continuation
function returns an int,
then the result of your then call
is a future holding an int.
If your continuation function
holds a future to an int,
then the return is
still a future of an int
rather than a future
of a future of an int,
cause otherwise that just gets too messy.
And so if you have a coroutine,
then if you're trying
to use it with futures,
your coroutine will return a future.
But that's okay because you
can use it as a continuation
and then that coroutine return of a future
will then just get nicely mapped through.
So you know in this case our continuation
returns a future holding a result.
And then the final result
of our continuation
is again a future holding a result.
And it doesn't matter that
that continuation was a coroutine.
The library doesn't care,
it's just something
that we turn to future.
What coroutines and parallel algorithms?
I mentioned on our last coroutine
as a possible execution policy,
but what else is there?
So if we're writing parallelism,
we're using parallel algorithms,
then one thing we care about specifically
is processor utilization.
Now we've got 1,000 cores and
we're tying to stick a
load of work on them.
And what we don't want is for those cores
to be sat blocking doing
nothing if we can help it.
Because that's just wasting
available processor resources.
So if we have blocking operations,
which you get normally with
futures, then they hurt that.
They complicate the scheduling,
they occupy a thread.
They can force us to do a context switch,
which is expensive as far
as the OS is concerned.
Just generally saps our
overall performance.
So coroutines allow us to
turn blocking operations
into non-blocking ones.
Because we say co_await on something,
and this suspends our current coroutine
and returns it to our caller.
And the coroutine can then be resumed
when that waited for thing is ready.
But in the meantime the current
thread can still do something.
So this works well with
parallel algorithms.
The issue is of course is that
if the suspension is a nested call,
then a stackless coroutine like we have
with the coroutines TS just
moves the blocking up a layer.
So with F which calls G which calls H,
then if H is a coroutine and uses
co_await to wait for something,
then that just resumes execution in G.
But if that then needs the result of H,
then it has to block, unless
it's also a coroutine.
And can use co_await.
And then likewise all the
way up the call stack.
So if we make everything
a coroutine, we're fine.
And we have H which is
a bottom level thing.
The result we get from
co_awaiting on something,
processing it, and then
we can't use co_return,
because we're trying to return
a value from a coroutine.
And then likewise in G and F.
And that means that then we can use that
when we're building a parallel
algorithm as an implementer.
So if you're trying to implement something
with a parallelized coroutine algorithm,
then you provide an overload that then
internally will look something like this.
It uses something to divide up your data
and co_awaits on it so that
can be a parallel task.
And then when it's
ready and it comes back,
then we're going to do recursive calls,
you know this is a recursive
call to this function,
which is itself a coroutine
and is gonna somehow be parallel.
And then we're gonna need
to co_await on those results
in order to combine them somehow,
which again we want to be parallel,
and then we're gonna
wait for that to finish
in order to return our final result.
So then the way that all these sub-tasks
get scheduled is then an
implementation detail of the library.
You don't have to look
at your code and say,
how do I post all the tasks onto my
thread pool that's
running this or whatever.
How do I post it up to the GPU?
This is completely hidden.
The runtime library in
it's coroutine traits
deals with making sure that when co_await
on something inside
your parallel algorithm,
it gets passed to the thread pool.
And so from a users perspective,
it looks like magic.
But all that magic goes in
the operator co_await upload,
and the coroutine trait specializations.
Okay so the final thing.
Executives.
What do we mean by an executor?
The core concept of an executor is
it's something that's
going to run our tasks.
Controls where and how they get executed.
But people have different use cases,
and can lead to quite different approaches
for how you implement
and specify an executor.
What kind of tasks have we got?
Where do we want them to run?
What relationships are
there between tasks?
Are they just completely independent
or are they in some crucial
way dependent on each other?
Can they synchronize
with each other, or not?
Can they truly run concurrently,
or do some of them have to run
sequentially with each other,
but they can run concurrently with
some other batch right over there.
Can they run interleave like
with a parallel unsequenced?
Can we move them around across threads?
Can they in turn spawn new
tasks on the same executor?
Can they wait for each other?
All these are important questions.
And we want to allow as many yes and nos
different answers for all
these questions without
fundamentally compromising our framework.
And then we have other questions.
Can an executor be copyable?
What would that mean?
Are they composable,
can you somethow build
a composite executable
out of individual parts?
If you have a task that
you know is running
on an executable and you have
some sort of handle tight task,
can you get the executor back in order
to run a task on the same one?
Or if you are running a task right now
and you know that this
function that I am in
has been called from an executor,
can I get an executor,
find out which executor that was
in order to run a new task?
As some form of continuation
of the current one.
So these are again important questions.
There have been I think
I counted something like
23 papers on this topic
over the last few years,
from the C++ committee.
Of which the latest is P433R2.
The second revision,
therefore the third
paper with this number,
of a unified executors proposal for C++,
which provides us with
many customization points
in order to allow as many possible
answers to the previous questions.
It has a basic concept of what
we mean by an executor in this case.
It is something that is copy constructable
and quality comparability.
So you can say is this the same
executor that I had over there?
And I can freely copy it around.
It provides a context member function,
which tells you what the
execution context is.
So for example if you're trying
to implement a thread pool,
then your execution context
is the thread pool itself.
And then the executor is something that
tells you to run the
task on that thread pool.
So you can pass around your executor,
'cause it's just a handle.
And you can copy that around willy nilly,
and any thread that has
a copy of that executor
can launch a task on that thread pool,
which is what the
context fundamentally is.
But the details of what a context is
is specific to the executor type.
And then fundamentally you somehow
have an execute function,
whether that's a member
function or a free function
in order to actually launch tasks.
And the way that we get to
all the different properties,
we wanted to know, can
things run concurrently,
can we bulk execute things, whatever.
We have this require member
function which provides overload.
So if you need a specific
property from your executor,
then you say I require this property.
And either it will compile and work
because it can provide you that property,
or it will fail to compile
because it doesn't.
And the framework will allow you
to build everything else from there.
So let's have a look at some code.
We have some executor, my executor,
and we're trying to find a function.
So you can just say execute some function.
That works.
But we don't know quite
what it's gonna do,
in terms of how it's going to schedule
and execute our function.
So you might think okay,
we need something specific,
we need some properties on that.
So I say I'm going to require that
from my executor I need it
to be two-way communication,
so I need to know when my task is done.
But I'm only going to execute
one function at a time.
And then that gives us a combined executor
from our source one that
has those properties,
and if we can't do that,
it will fail to compile.
And then we can call two
way execute of our function,
and that will definitely
then give us our future back.
Which we can then wait on in order to
know when the task has been done.
There's a whole bunch of
properties that you can require.
You can launch you task and say, well,
I don't need to know,
because I've got some other mechanism
of knowing when it's done.
Or I want a future back.
Or this function that I'm passing you now
is a continuation of this other function
that I'm passing you a future for.
So this works with the continuations
from the concurrency TS.
I only want to pass in
one future at a time,
one function at a time.
I want to pass in a whole
bunch of functions at a time!
In order to bulk execute things,
because then you don't have the overhead
of filling up your task key one at a time.
I want to guarantee
that I will never block,
the execute function is never gonna block.
It's always going to run the task
in the background on some other thread,
or add it to some queue for
running later, or something.
Or I want it to always block,
because when I call it I want it
to run the function right now.
Or possibly you want it to do
whichever is most appropriate
for it, possibly block.
Or you want to say well the functions that
I'm passing in are in some
continuations of the current task.
So when the current task is done,
then start these new
functions, and not before.
Or tentatively they're independent things,
so don't bother waiting for me,
you can start them straight
away on some other thread.
And there are more,
there's a whole bunch of
properties that you can query.
And the framework is extensible,
so in theory you could
add extra properties
for the continuations that you
as a library implementer are providing.
So when this finally gets
widespread implementation,
then you might find that
each implementer provides
a few extra properties
that are just for them.
This can then interact with
the parallel algorithms,
because you can provide
a custom execution policy
that says spawn the tasks for
this parallel exeuction on my executor.
It's also natural to want to think
that you can do the same
with the concurrency TS continuations.
So I want to call this continuation
and do it on an executor.
Now that's actually worked quite well,
because the TS I specified
runs the continuation on
an unspecified thread.
You don't know whether it's a new thread,
you don't know whether it's a pool thread.
In theory it could be your GUI thread.
And you don't want anything to run
on the GUI thread unless
you're intending it to.
Or you know any thread in the system.
So with the concurrency TS
which is, this is deliberate.
It's a TS, we're asking for--
It's implemented like this in order to
get maximum feedback and
allow library implementers
maximum scope for seeing
that their users want.
But with an executive framework,
then it makes sense to be able to say
I want this task to run on this executor.
So there's different availability
for all these different parts.
There is not a shipping implementation
that I'm aware of that
supplies all of them.
Visual Studio provides the coroutines TS.
Clang 5 if you use it with libc++ 5
also provides the coroutines TS.
The HPX team have got parallel algorithms
and futures with continuations
which look like the concurrency TS ones.
And they have some executive support,
but it's not quite the
same as this proposal.
My Just Thread Pro library
provides the concurency TS
and integrates with coroutines,
and I'm working on parallel algorithms.
But the whole lot is not
in any one implementation.
So at the moment you're going to
have to choose piecemeal or actually
help work on one of the
open source implementations.
My book covers stuff on the
parallel algorithms
and the concurrency TS,
there's nothing on coroutines in there.
The early access edition is
available for the second edition
and I believe that there's
a 50% discount code
that you should have all received by email
for being at the conference.
If you're actually interested
in Just Thread Pro,
then go check out the website.
It's a commercial library
with an open source.
So we have a few minutes left,
has anyone got any questions?
There's a couple of mics.
How far?
That one. (laughs)
- [Man] So does that
mean that if you're using
coroutines in your main function,
that you have to make your main function
return a coroutine?
- I don't think the coroutines TS
supports main being a coroutine.
- [Man] Okay.
- [Man] A lot of slides
back you had a shared future
where you continue at two
continuations using them twice.
How will that read run,
will that run one and then the other,
or in parallel, or is it unspecified?
- If you schedule multiple
continuations on a shared future
then is there any guarantee
what order they run?
Well, no.
The TS, things run on unspecified threads,
and they're on unspecified times order
so if you schedule multiple,
they might run concurrently
on separate threads,
they might run sequentially
on one thread in some order.
You don't know.
That's a deliberate again decision,
to allow TS implementers maximum scope.
- [Man] And that will change
if the executor comes in?
- Then you can then say what
does the executors require?
What does it specify for that?
- [Man] And the other question was,
you mentioned a when any.
So you can wait for any continuation,
or any of the tasks to be ready.
What if you would want
to wait for the next,
how could that be achieved?
- Okay so you call wait any,
and it tells you that
one of them is ready.
But when you get the
result back from that,
then you've got the full set of futures
that you waited for.
So you can then pass those back in
to a subsequent when any call
if you want to wait for the next one.
- [Man] Oh, you get them all?
- Yes, you get them all back.
- [Man] Thank you.
- Okay, next.
- [Man] You had some sample
code for parallel funk
that called parallel divide and
then parallel combine at the end.
It was sprinkled with
these co_await co_returns.
And I looked at how that would look
if you did it with promises
and futures instead,
and you just chain them
together with then,
and at the end you have a win all,
the parallel will combine.
And it seems like about
the same amount of code,
but the futures promises one
seemed to be more refactorable
in that you didn't have
these syntactic constraints
of having to put the keywords everywhere
and then breaking when you
pulled out a piece of it.
Have you seen good examples of
places where coroutines really give
a big benefit over just you saying
futures with an executor that
would be completely synchronous?
Something for when you
schedule on an executor,
it just runs the thing.
- Yeah there are plenty of examples
when you're looking at real code.
Obviously this is code to go on a slide.
The advantage if you
use something like that
where you're using coroutines
in a parallel algorithm,
then it means that the
whole thing is a coroutine,
and you can disable the
parallelism outside of that
by putting what the context
that it's running on.
And so it pumps that decisions
out of the code that's right there
into the library that's dealing
with the parallelism at the back.
Which could then put it
all on a single thread,
and just interleave all the calls
as various bits become ready.
But in some circumstances just
changing it with promises and
futures is just the way to go.
It isn't a panacea, it's
not an answer to everything.
Sometimes it makes the code clearer,
and other times it doesn't help.
- [Man] So in the coroutine
case, who's deciding?
Which part of the program decides
whether it's multi-thread?
Is it coroutine traits?
- Yeah it's coroutine traits
and operator co_await.
- And those are global like operator new?
- Operator co_await is overloaded.
So you specialize it for,
it picks the right overload
for whatever type it is.
But it works together
with coroutine traits.
It's rather nifty inside so when you--
Your coroutine traits allow
you to have an await transform.
If you co_await for something
then it transforms into something else
before it looks up operator co_await.
It's really quite nifty,
and it allows a lot of scope
for changing things around.
It took me a while to get my head around
the scope of coroutines TS,
and how actually really good
it is and the scope we have.
Completely messing up your code,
but in a good way. (laughs)
- [Man] Thank you.
- [Man] I actually have several questions.
First of all you mentioned GPU.
So please clarify.
That means that if I use a
parallel version of SAL algorithm
it can actually offload
some work on the GPU?
- Yes, the and code play example
this morning did just that.
- [Man] How exactly this happens?
- I think their implementation,
their compiler is a
wrapper for two compilers.
So you pass it your CPP file,
it compiles for the CPU and the GPU,
and then at runtime depending on where,
it ships some of the work over to the GPU.
- [Man] Okay so now developers
of the C runtime compilers
actually are required to
adapt some GPU operations
for this particular thing?
- Yeah, so yes.
Their implementation
requires compiler support,
so they're shipping a
compiler that does it.
- [Man] Okay yeah that's
pretty cool actually.
- Yeah I think so.
- [Man] Well this brings another question.
I was wondering,
this seems like a really hard work end,
and a big amount of work.
How it is actually applicable?
I mean usually when you parallize things,
you separate your work into tasks
and then you make task-based parallelism.
And it seems like a very corner case
when you want to parallelize
actually one algorithm.
Because you have many many algorithms,
and you have a some kind of pipeline,
and maybe you have a dependency graph.
But parallization level
of particular algorithm
looks like just some corner case.
- Sometimes that is what your problem is,
and you have big problem which is,
process this large chunk
of data all the same
and you can use one of the
S tail algorithms to it.
Lots of problems of other varieties
you can rewrite as they
call to transform reduce.
Transform reduce is so general
that you can do a heck of a lot with it.
So parallel transform reduce,
you can do a whole bunch of things.
But sometimes that still doesn't work,
because you have a pipeline.
In which case this doesn't
solve your problem,
you need something else.
There are pipeline proposals
before the C++ committee,
but they're not in any TS's
or in the standards draft yet.
- [Man] So basically the
committee decided that
this particular approach is particular
to many situations in real life?
- Yes, and it really is in practice.
There have been libraries shipping
that do just this for a long time.
And so we finally got
a standard API for it
in the C++ standard rather than
having to use the
vendor-specific implementations,
and their APIs.
- [Man] Okay great thanks.
And the last question is,
you mentioned that when
we throw an exception
from a function that is
executed in parallel.
So for example for each
we terminate, right?
But we could have accumulated exception,
but committee decided not to do that.
Why?
- Because it's messy,
and what would the user
do with 1,000 exceptions?
- [Man] Well we have nested
exception, exception pe-ters.
We have all kind of stuff.
I'm pretty sure we might
come up with some solutions.
So there should be some
fundamental reasons
why we don't want to do that, right?
- It's messy, it's complicated,
it makes the implementation of
the parallelisms really hard.
- [Man] Well it's C++, right?
- But it also adds overhead because
there has to be synchronization.
This grand list has to be
synchronized across the things.
It's something that implementers said
we don't want to do this.
If you're on a GPU
exceptions are not the same,
so it really complicated the
task of shipping stuff to GPU.
If you wish to write
yourself a parallel algorithm
that propagates exceptions, feel free,
no one is going to stop you.
- [Man] You already
did, actually. (laughs)
- Or alternatively badger somebody else
who is already writing
one, an open source one,
and get them to do one.
- [Man] Thanks.
- [Man] Thank you for the presentation.
I have a question.
I feel the concurrent TS is similar
then Microsoft's PPR task I think.
Is Microsoft PPR task
comparing to concurrent TS?
- In some ways I believe that they
have the same sort of concepts,
tasks and continuations and features.
Yeah if you--
I don't use Microsoft PPL,
so if you want to know about that
then you're probably best nagging
one of the Microsoft guys.
There's quite a few
here today, so. (laughs)
- [Man] Okay, thank you.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>