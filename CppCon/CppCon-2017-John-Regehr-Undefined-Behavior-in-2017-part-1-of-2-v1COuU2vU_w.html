<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: John Regehr “Undefined Behavior in 2017 (part 1 of 2)” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: John Regehr “Undefined Behavior in 2017 (part 1 of 2)” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: John Regehr “Undefined Behavior in 2017 (part 1 of 2)”</b></h2><h5 class="post__date">2017-10-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v1COuU2vU_w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Alright I'll get
started in just a minute
and in the meantime if,
I just wanna call your attention
to the URL on the screen,
in case you're one of those people
who likes to look at
your screen more than--
(coughing drowns out speaker)
Okay, is it okay to start?
Great, thank you for attending
and my name's John Regehr,
I'm a professor of computer science
at the University of Utah,
I'm gonna talk about
undefined behavior today.
And I just wanna add
that I would certainly
love to get questions
while I'm talking,
I'd rather kind of be interactive
and address the kinda things
that you'd like to hear about
rather than get through
all my slides, okay?
So what I'm gonna do today is talk about
what undefined behavior is, why it exists,
what are the consequences and sort of
what can we do about it
and if you noticed from your program,
I have this slot and another
slot from 3:15 to 4:15
and so these topics will be
spread across the two slots
just kind of however it comes out
depending on how interactive
you are and how fast I talk,
I'll try not to talk super fast
although I sort of by default do that.
So let's talk about interface design.
Interface design is hard, right?
Interface design is never easy
and designing failure modes of interfaces
is one of the hardest parts.
As you can see here on the screen
I've showed just sort of
a really easy question
or a question that should be really easy
which is what should our
language implementation do
for the square root of -1
and you can see that there
are some, a number of choices
and none of them is
obviously the right answer
independently of everything
else that's going on.
All of these things could
be the right answer,
and the different choices
have a lot of affect on
our programming system,
whether we return an inbound
error or an out of band error,
whether we have a
language with exceptions,
whether we have complex numbers,
these are pretty big decisions
and getting these right is hard,
it's sort of these global design decisions
that make designing programming languages
and programming systems really
this really challenging thing that it is.
So you can see that on the
bottom one of the choices,
one of the possible choices
for this is undefined behavior
and so undefined behavior
is just a design choice.
So it's just a design choice,
it's one of the things we can do
when a program does something
but it's this pretty extreme
design choice, right?
And the reason that it's extreme is that
out of all the choices we can make
of what the compiler's required to do
or what the programming
language implementation
is required to do, when
the program does something,
this is the one that imposes
the fewest requirements,
and you can see that I've regurgitated
a little bit of language
from the standard here.
So undefined behavior
is pretty commonly seen
on hardware platforms
and what it does there is
gives the hardware
manufacturer's a way to say
here's a register,
we're gonna put something
in this register later,
add some functionality later,
we haven't done it yet,
and really, really, really,
don't mess with this register
because then later your
software will be broken
once we do something there.
So that's sort of the kinda the most basic
and most defensible use
of undefined behavior is
to warn you away from doing something
where nobody wants to do error checking
and where nobody wants you to rely on
any particular thing
happening when you do that.
So this is kind of the, like I say,
this sort of baseline
usage of undefined behavior
and it's kind of crept into,
you know it's sort of pushed
into other usages as well.
So now what I do next is make an analogy
and it's a pretty loose analogy
but I'll just ask you to bear
with me for a little bit.
So here's my loose analogy, so
this is a derivation that we,
many of us ran into in grade school
and the derivation ends up,
starts off with something sensible
and through a series of
steps, mostly good steps,
ends up with an
indefensible result, right?
So you can see that we've ended up proving
that two equals one and
each of these steps,
at least to somebody maybe in grade school
or possibly high school looks okay.
And so which is the wrong one?
(audience member speaking off microphone)
Yeah, we had an obligation
before performing this
step of the derivation
to prove that the devisor,
to prove that we weren't
dividing by zero I mean.
And once we do that, things go wrong
and we end up with a
completely nonsensical result
from a series of what seemed
to be pretty normal steps
and the reason I use
this example is because
I think it really kind of
helps convey the flavor
of what's going on in undefined behavior
where nobody tells you what's going wrong,
you sorta have to
observe this for yourself
by knowing all these
little, all these rules
and it doesn't just kind
of necessarily give you
sort of a predictably bad result,
it kind of makes the whole thing garbage
and you can see this entire thing,
our proof here is garbage, right?
It just doesn't make any sense
because we've admitted an
inadmissable proof step.
So anyway it's just an
analogy, yada, yada,
I don't know if you like it or not
but I sometimes use this
and I think it's helpful in,
it's helpful in understanding
undefined behavior,
and let me do another example.
So here is a little program,
and what this program tries to do is
something which is
undefined by the standard
but which a lot of people say
or have said in the past is okay.
Which is an unaligned memory access
and the reason you would
say this is okay is because
you don't know that your compiler
is targeting an architecture
that supports memory
references to any byte address
and so since you know
this, this should be okay.
And this is the kind of argument
that people sort of put forward
for doing undefined things
and it's a very similar argument
to one you kind of hear pretty
often which is I know that,
here we have a two's complement platform,
so of course overflowing in integer
should have two's complement
behavior and people sort of,
it's really hard to kind
of escape from a mindset
where you make these
assumptions about the platform
and you lose track of
the fact that C and C++
defined these abstract machines
that really you have to think of them
as not really being tied to the platform.
And so the analogy I'm gonna use here
is one that I didn't come
up with, it's one from,
one that somebody posted on
the internet a long time ago
which is that saying that
this program should work
because X86 supports
unaligned memory references,
I'll just let you read it, you know,
it's a reasonable analogy
here and it's wrong
and so of course it all sort of hinges on
what do we mean by
playing basketball, right?
Do we mean holding a ball and moving
or do we mean something more,
something more sophisticated like
adhere to a collection of rules
that we all understand
as being basketball.
So is that sorta clear?
And anyway it's a
terrible argument even so
because I mean X86 64 has movdqa anyway
which has, the A is for aligned, right?
So it has a, so the
program I just showed you
doesn't crash on X86
but programs not too much more complicated
compiled by GCC will crash on X86
because you get the
compiler to do a mini movdqa
and then the program
actually does crash, right?
So it's not even true,
the basic reasoning isn't even true here
but even if X86 didn't have an instruction
or if the compiler never generated it,
it still wouldn't be true
because the compiler
would be allowed to assume
that things were aligned
and do something wrong with your program,
do something that you didn't
intend with your program.
Okay so C and C++ have
lots of undefined behavior.
They have undefined behavior
to avoid overhead in the generated code,
they have undefined behavior
to avoid compiler complexity,
the compiler gets to assume things
that would either be difficult
or impossible to prove
from first principles
if a compiler weren't
allowed to just assume them.
And they have undefined behavior
to provide compatibility
across different
implementations of the language.
And if you look at the
history of undefined behavior,
the way I understand it,
and this could be wrong,
I haven't talked to the people responsible
but the way I understand
it is up through KNRC,
the use of undefined kind of seems to mean
you get an unpredictable
result if you do something.
As in it's this kind of bounded thing
which means that if you shift
a 32 bit integer by 44 bits,
you might get something weird
in that particular value
but there's nothing that really implies
that it's gonna sort of tank
your entire computation.
So then it's only an an and CC
where they put in language which said
the compiler is allowed to assume
or the compiler makes no guarantee about
what happens when you
execute undefined behavior
but so that seems to be when
some kind of a line was crossed
and we went from undefined behavior
as being this kind of limited thing
to undefined behavior as
being this very powerful tool
for the compiler to assume
that something doesn't happen
and thereby to end up not
making guarantees about
programs that do the
things that are undefined.
So let's look at this question,
so this question is kind of fun is
does undefined behavior avoid overhead?
And of course the answer is
absolutely yes in many cases
but you have to look at
this on a case by case basis
and so let's look at a
really simple example.
So the question is is something
like the shift operation
in C++ made more efficient
by the fact that shift is undefined
when the shift exponent is too large?
So well what we have to do is
we can observe that the standard
should say something about
what happens when you
shift the 32 bit value
by 44 bit places.
It should say something, right?
It shouldn't just be silent on this issue.
So since it has to say
something, what should it say?
Well hardware platforms do
at least three different
things in this case.
And there's probably more.
So the standard has to say something
but it can say anything it wants
but it wants to be efficient
so of course what it says
is this is undefined,
the compiler just assumes
that this overlie shift doesn't happen
But what if we wanted to standardize
on a particular behavior?
So if the behavior that we choose is
masking off the shift
exponent by the number of bits
by the width of the shifted value,
so of course there's
trade offs here, right?
That may not be the behavior
that we want for an overlong shift
but if it is the one that we
choose, and this is one that,
for example, WebAssembly has used,
then basically this
mandates a mask instruction
and and instruction in
front of every shift.
And with that extra instruction
we would get this kind of
uniform behavior across
all implementations.
And so now we can move on to the question,
is adding an and instruction
in front of every shift
a reasonable thing to
expect a compiler to do?
And some compilers like
WebAssembly say sure you know fine
we'll just take the pain of
these masking instructions.
And C and C++ have sort of
said no that's not right.
And so in this case as we can
see exactly what the cost is,
we add an extra instruction any place
we can't prove that the
shift exponent is in bounds.
You know and so you know is
that a good trade off you know,
this is open to debate,
it's not a super super easy question.
But in this particular example
the actual overhead is very
clear, it's an and instruction.
Does that kinda make sense?
No questions yet,
I'm talking so fast that
everybody's quiet, good.
Okay so yes this undefined
behavior avoids overhead
and that's what it avoids.
So let's talk about undefined behavior
a little more globally.
So appendix J of the C standard,
and here I'm gonna kind
of unfortunately kind of
switch around between C and C++ some
and for most purposes
the two languages aren't
functionally different
for purposes of undefined
behavior, alright?
They share the optimization parts
of the compiler largely, right?
And so then that's where
most of the undefined
behavior magic happens.
So I'm gonna kinda switch between
talking about the
languages, I'm so, you know,
I'm sort of sorry to do that at CppCon
but I'll have to do that.
So there's a list in the
back of the C standard
which lists kind of most
of the undefined behaviors,
it's definitely not complete
but it gets most of them
and it has almost 200 undefined behaviors.
And as far as I know,
this doesn't exist for C++
so nobody's sort of made this list
and this would be very
nice if this list existed.
It's tricky because
some undefined behaviors
are undefined only by sort
of implicitly undefined,
does this kinda make sense?
You'll see sort of wiggly
language in the standard
like the implementation
is allowed to assume that.
So if the implementation
is allowed to assume X,
what that really means is
not X is undefined behavior.
Does that kinda make sense?
So there's some places
where things are undefined
only by implication
but it really would be
nice if this list existed
and if this list were being expanded
for every new version of the standard,
and I have on the slide here
an example of a undefined behavior
that just seemed to have appeared
if something went from
unspecified to undefined,
just in C++ 17.
So this, so having a
complete list would be good.
I'm not sure that it's a really big deal
because the big, the sort of heavy hitters
in undefined behavior we
know and they're well known
but it would be nice
to have a complete list
and I always feel kind
of a little slighted
that the C++ standard
doesn't have this appendix J
which is I think kind of a very
nice part of the C standard.
So let's talk about-- yeah.
- [Man] I just had a comment about that.
You haven't even touched about
what the standard language
considers undefined behavior
and basically they do that
by defining requirements
or conditions that must be satisfied
before you make a call...
(speaker fades out)
- Yeah so Marshall's observing is
if we look at the standard library in C++
there are even more undefined behaviors
and that's absolutely the case.
(audience member speaking off microphone)
Sure, sure.
So let's look at what happens,
what are the things that happen
when your program executes
undefined behavior?
Well one thing that can happen,
and this is very nice is your
program breaks immediately,
it segfaults or whatever.
Another thing that can happen is
the program keeps going
as if nothing happened
and either maybe it kinda
keeps going in a good way,
maybe it kinda keeps going in a bad way,
and something goes wrong eventually.
Another case is
the code works as you kind
of hoped it would've worked
but you have a problem moving forward.
So for this particular
compilation of this program,
there was no problem, but
change of the compiler options,
change of the compiler version,
changing the compiler supplier,
one of these things might
make the undefined behavior
behave badly at a later time
and I like to call these time bombs
they're problems in the
code that are latent,
they're waiting to go off
at some point in the future.
And of course not all undefined behaviors
execute at all normally, right?
Some undefined behaviors only execute
when somebody owns your code
and so out of these four,
I really just wanna kind of
talk about the two middle ones
because case one is sort of
easy and uninteresting, right?
We just debug it.
And case, sort of unnamed case four here
where we don't know how to trigger it,
this is Kastia's deal right
this is why we need fuzzers and stuff.
That's not my job here to talk about that.
So it's really these two cases
where undefined behavior
does something weird
or where it might do something weird,
that's kind of the
focus of the talk today.
(audience member speaking off microphone)
Oh that's, okay so the question is
is a math exception undefined behavior?
The answer is no, the answer is that
the division by zero
is undefined behavior,
the math exception is
what might, you know,
that's the symptom that you
might get on some platforms.
(audience member speaking off microphone)
But the standard doesn't
say anything about
integer divided by zero
producing an exception,
that's pure X86.
(audience member speaking off microphone)
It's the abstract machine
which makes it undefined.
But this is the thing is
if the abstract machine guaranteed
that you would get the exception
then that would be one thing
but it doesn't guarantee this at all.
I mean that code could
be alighted or something.
If the compiler decided to do so,
if a compiler could prove
that the division by zero happened,
it wouldn't need to trigger the exception.
And I strongly bet
that you can construct
cases where it does that.
So let's look at what's been happening
over the last 25 years or so.
So one thing that's been
pretty awesome to watch
and to see happening is that
the undefined behavior detection tools
have been getting good.
So starting maybe, I
don't know what the first
undefined behavior detection tool was,
but something like Purify
maybe in the early 90's
would be a good example.
So this was one of the first
memory overflow detection tool
and you know those of us
who were around in the 90's
remember that that was awesome
when you got Purify I mean everybody,
I don't know how many of you remember
first running Purify on something
and even stuff like Alas
would like have all sorts of problems
and I mean it was incredible.
So Purify was this thing
which rewrote your
binary to insert checks.
Maybe not unlike Valgrant
except that it was not
a just in time compiler
as much as a sort of a load time compiler.
Anyway, so these tools
have been getting awesome
and they're just in the last five years
there've been enormous improvements.
But on the other hand,
compilers have been getting
cleverer and cleverer
at exploiting undefined behavior
to improve code detection.
And even sort of simple things
like improving the in liner
can greatly increase
the scope of the undefined
behavior exploitation
that happens on your program
because it exposes more code
to the compiler's analyses,
or it puts code together
where the compiler analyses
can look at it all at the same time.
So compilers are getting cleverer
and this has been causing
since maybe around 2000
these kind of time bombs to go off.
And one of the interesting ones
that's been kinda fun to
watch is integer overflows,
those sort of, a lot of 90's era C code
assumed that integer overflows
were two's complement.
And since like 2000 GCC especially
has been making those
sort of one after another
kind of not be two's complement behavior
and this has caused a little sort of
you know it's not been this huge issue
but it's been sort of a
niggling source of crack bugs
for a number of applications
for a long time.
(audience member speaking off microphone)
So the question is
what should one shifted
left by 31 do and this is,
isn't this one of those cases
where the different standards
have different things?
Shifting into the sign but
this is one of the ones where
you have to read the
fine print for real about
what version of the language you're using
because the standards committee has--
In C 99 I believe that's undefined,
to shift a one into the sign that all was.
That was perceived as I believe
breaking too much software
and they've backed off on that
so that was a really nice example
of the committees taking feedback
and backing off on something.
Which probably provided pretty
dubious optimization value
in the first place.
Okay so besides these two things,
of course security's become
this paramount concern
in developing software
so the consequences of undefined
behavior in the real world
have gone up dramatically since 2000.
So this is just a tiny
rhetorical question.
What happens when you go
to a compiler developer
and you tell them that your program,
which executes unaligned
memory access is broke
because you changed the optimization level
or updated the compiler?
But the compiler developer informs you
that there are these
things you shouldn't do
and please just don't do them
because it's not their problem, right?
So one of my kind of more fun,
this is sort of small of me perhaps
but one of my more fun moments
was when we started running the tool
which became part of UBSan so
detecting integer overflows,
this is like what happened
when you ran GCC is
it just kind of like you know so GCC,
the GCC developers are a little bit famous
for sort of telling people
just stop with the undefined behavior
it's not our problem it's your problem
but of course GCC was full
of undefined behavior,
was full of these integer
undefined behaviors.
And so it's a little unfair to pick on GCC
because basically all old programs,
this sort of 90's era C and C++ code
contained a lot of undefined behavior
and it's just sort of how
things worked at the time
and pulling that code forward
has been one of our major ongoing problems
because it requires a bunch
of maintenance and fixing
because things that used to work break.
Or maybe even they never worked
but the security problems have
become so much more severe.
So what are we gonna do about that?
What are we gonna do about
all this 90's era code?
Well one thing we can do is
go back and fix all of it,
that would be awesome.
Another thing we could do is
just stop dialing up the optimizer,
that would maybe not be awesome,
but that's a possibility
and of course you're not
gonna do those things, right?
We're mostly gonna kinda
do a mix of these things
but we're stuck with a lot of code
that doesn't wanna get
fixed, at least aggressively,
and we're definitely not
dialing back the optimizers
in any systematic sense.
So now I wanna sort of take a look at
some specific examples
for undefined behavior
and this is gonna be kind
of a mix of C and C++.
And I'm gonna just admit up front
that I cherry pick my
compiler version somewhat
when I make these examples.
So if you type in the code here
and run it with the compiler I said,
it might not give the result that I said
because I might be using
GCC 5.4 instead of,
and you have 5.5 or 6.5 or whatever, okay,
so I just have to sort
of say that up front.
But this particular one that
I'm showing on this slide
is generally pretty robust
across versions of GCC
and Clang for a long time.
So what this program shows
is a function which compares,
the function foo which asks
whether x+1 is greater than X
and so mathematically of
course that's true, right?
X+1 needs to be greater than X.
Two's complement wise,
that's not necessarily true
because X might be int max
but C wise and C++ wise,
it is true again because
either the result is undefined
in which case it doesn't matter,
the result is irrelevant,
or it's true.
Does that sorta make sense?
So the compiler can do
this little case analysis
and that case analysis will let us infer
that the function is always true
but the fun thing about the program here
is in the same compiler at
the same optimization level
in the same program execution,
the compiler asks the same question twice,
says different things.
So you know the thing
being illustrated here
is that there's no requirement
for consistency, there's no,
this isn't like implementation
defined behavior,
this is something where the compiler says
right after one another
that X+1 is greater than X
and is not greater than
X when X is int max
so of course it's an undefined program,
it's an erroneous program,
it shouldn't have done this,
UBSan would've detected this
but if we do this the compiler is free
to sorta mess with us a little bit.
So that's kind of a fakey example, right?
It's not necessarily very
interesting realistically
so here's one that may be more
interesting realistically,
so here's a integer overflow
caused by shift by too far
that was in an old version,
this is maybe 10 years ago
or something of Google Native Client.
And so what happened is
they had a mask operation,
shown in the little code fragment
towards the top of the slide
and that was masking off
some load bits of a number
and they refactored the code
in some sort of a routine refactoring
that none of the code
reviewers noticed to be
the second snippet of code on the slide
and the problem was is
that the shift exponent,
the nap align boundary, ended up being 32
and shifting one left by 32 places,
if one's a 32 bit integer
then this is undefined
and what the compiler did is basically
dropped this mask from Google
Native Client silently,
causing the program, causing the sandbox
to be weaker than it should've been.
And so if you think about it this is like
kind of a little bit of a
worst case scenario, right?
We have something which is a sandbox
that's designed to be
enforcing security properties
on this containerized code and it's not
but it sure looks like it is
and if we don't have a test
case for this specific case
then we're sort of hosed, alright,
so this showed up on a Google
Native Client mailing list
as sort of a scary example of
the kind of thing that can happen
if you aren't explicitly checking
for undefined behavior in a code.
Like I say, that code just went away.
And there's lots of other examples
of code going away silently
and some of the more famous examples,
and I'm not gonna go
through them in detail
but you might've seen these
where people use uninitialized data
as entropy in random number
generators sometimes,
this is sort of poor always, right?
This is never like good at all
but the compiler can make it so much worse
by sort of transitively seeing
that none of the code that
depends on that matters really
because it's undefined code
and the compiler can go sort of clean up
with a fairly broad scope if it wants to
it can kind of clean up that
code and just delete it.
And again we have the flavor
of the thing happening is that
the source code looks one way,
the object code looks a different way
and if somebody really doesn't go dig in,
we're never gonna see it, alright?
This might be my single favorite
undefined behavior example.
So P is a malloc cell
and let's just assume
for purposes here that
none of the mallocs fail
so we don't need to check return values
to make this defined.
So the undefinededness here
doesn't have anything to do
with malloc returning null.
So P is a malloc cell, Q is a
realloc of that malloc cell,
so as soon as Q takes on a valid value,
P has disappeaared, right?
P is a dangling pointer.
Now P of course
still has some sort of a
value in its bits, right?
But we're not allowed
to indirect through it
even though it's probably
gonna point to the same place
that Q now points to,
does that make sense?
I mean we didn't change the
size with the realloc, right?
So realloc would be stupid
to actually do anything here.
But in any case, logically
speaking in terms of
the abstract machine
P now became dangling.
So as soon as we store through P
in the third line of main there,
as soon as we store through P
the program makes no sense, right?
Now it's gone undefined.
Anyway so let's keep going.
So we store through P, we store through Q,
and now we ask if P is equal
to Q, then print something,
and so the compiler goes ahead and decides
that P is equal to Q
and yet it prints two
different things for P and Q.
Like what.
So this is one of those things
where you say to yourself
what is the compiler thinking
but it's really an error
to talk like that, right?
Because the compiler isn't thinking,
it's just this collection of rewrites
and this collection of replacements
that just glom your program
into some sort of binary that emits.
It's not thinking anything and then
so that's the best way to think about
these kind of programs.
(audience member speaking off microphone)
The question is did I look
into disassembling this code
and no and I doubt it's interesting
because the program probably
almost completely went away.
- [Man] I expect what's happening here is
P and Q don't alias because
their lifetimes don't overlap
and star P gets one and
then get one back out,
star Q gets two you get two back out
so those get constant propagated
but the P equals Q doesn't
get constant propagated.
- Actually fires a runtime
probably, that's right.
That's most likely what happened.
So does this program
make sense to everybody?
Did I sort of explain it adequately?
So let's look at another
one, and this one,
this is another one that I like, so...
We have some code now so
if we define a debug flag,
a debug macro, we're gonna print
the string pointed to by P,
and then if P is true we'll call bar on P
and so now let's compile
that without the debug flag,
so with, or sorry, yeah sorry,
compile that without the debug flag.
So when you look at the assembly
code that gets generated,
it looks pretty good.
The assembly code says that
we're gonna test the value
and conditionally call the bar function.
So there's no problem.
Okay so now let's define the debug flag.
So we'll define the debug flag
and now the print F
gets optimized to put F
so don't get distracted by that,
that doesn't mean anything,
but what happens is that notice that
we now didn't conditionally call bar.
We unconditionally called bar.
So why did we do that?
Somebody tell me quickly.
(audience member speaking off microphone)
Yeah, if we had the
reference, if P had been null
then the print F would've
been undefined, right?
Since the print F, so there's
two cases, if P is null,
the print F is undefined,
the program is like
logically irrelevant so the
compiler has no obligation.
So the compiler's allowed
to assume that P isn't null
since there is a print
F so why emit the test?
So I've constructed the
example fairly deliberately
to show that the sucky effect,
so it's almost kind of like
sucky chronomechanics or something, right?
We didn't want looking at P,
we didn't want printing it
to affect the semantics
of the rest of the program
but it did so this is, yeah.
(audience member speaking off microphone)
- Yeah so the comment is
logging can have the same effect
and yeah this stuff can
certainly happen for real,
that's right.
- [Man] I've taken advantage
of this with break statements
like if you put a print F in there
it forces the compiler to generate.
- Actually generate the code.
So here's another function,
so this one calls mem copy
and then if the source
pointer was null, aborts.
And so this, it should
be no surprise, right?
That the jump to mem copy occurs
because if it's null that mem
copy would've crashed anyway
so at first glance this looks
just like the previous example
but it's not, it's not just
like the previous example
because N could've been zero.
So the way this works
is even if N is zero,
even if we're not copying any bytes,
the standard says that you
have to have valid pointers,
P and Q for mem copy
regardless of the value of N.
So the inference is the
compiler is allowed to assume
that it's a null pointer
and what this particular thing does,
this is actually a little
bit harmful in practice.
This particular line burns people
because you'll have a loop
that's doing mem copies in it
and sometimes it's easier
to have a mem copy with zero
at the last iteration of the
loop, a mem copy of zero bytes.
Instead of jumping out of
loop somehow you go ahead
and execute the code
and so this is all fine,
it's a little bit cleaner loop structure
because mem copy's just not gonna fire.
But you can get into trouble
because the compiler can then assume
that the arguments weren't null
even though they could've
been at that point in the code
and this can sort of lead
to unexpected consequences.
That sorta make sense?
So the compiler's getting a data flow fact
that many people wouldn't
have expected it to get
and like I say the standard is clear
that you're not allowed to call mem copy
within a valid pointers
even with a zero argument
so we're all to blame if we write the code
but on the other hand
this is one that's tripped
up people for real.
(audience member speaking off microphone)
(mumbles)
(audience member speaking off microphone)
Exactly, exactly the kind of thing, sure.
So let's look at another one,
so here we have a program,
or it's not a program, a function
which assigns five to the
pointer through the pointer H
and then assigns six through the pointer K
and you can see in the
assembly code that the,
there's a very easy nice correspondence
between the assembly code and the C code.
The store through H turns
into the first move,
the store through K turns
into the second move
and then why didn't we just return five,
why do we actually have a
load following the two stores?
(audience member speaking off microphone)
Yeah, the pointers might've been aliased,
and the pointers might alias each other,
they might point to the same integer
in which case we can't just return five,
we need to return six
when the pointers alias
and five when the pointers don't alias
and the compiler knows this,
the compiler's very smart about
this kind of a thing right so it's all,
the compiler generally does,
compilers generally do very well
with this kind of constraint
even though as people
reading assembly code
often it's like we look at the code
and we're like why is it
so sucky and it's because
the compiler is doing the
right thing most often
and seeing that there's
aliasing relationships
even when we as humans
knew that there weren't.
So if we adapt the code just a little bit
by making K a pointer to a long,
so now H is a pointer to int still,
K is a pointer to a long,
so you can see that the
move of six became a move Q
so you can see that this
is now a 64 bit move.
But also the function just goes ahead
and returns a constant five.
So the aliasing couldn't happen here
when it could in the previous
code and this is because
part of the C and C++ standards
that we often call the
strict aliasing rules
where pointers to different types
are assumed by the compiler not to alias
and this gives us as programmers
an extremely powerful tool
for telling the compiler
that things alias each other
and there's a lot of code
iterating over vectors and things,
there's a lot of code
where things like iterators
would just be loading and
storing from ram constantly
if the compiler didn't do
type based alias analysis.
And the type based alias analysis,
the strict aliasing based optimizations
allow quite a lot better code
to come out in some cases
without sort of heroic
compiler optimization.
Okay so that's all good but
it's sort of nasty though
that even on a 32 bit machine,
so I didn't show you 32 bit code here,
but even on 32 bit machine
the same exact thing happens, right?
So even when size of int and
size of long are the same
the rule still applies.
So now the compiler's kind
of being a little bit, right,
it's being a little bit nitpicky with us
but the optimization is not valid
if H is a pointer to int
and K were a pointer to unsigned int,
does that kinda make sense?
So the compilers use
extremely specific rules about
when different pointers to different types
could alias each other
and these are fairly hard to get right
and this strict aliasing stuff has been,
if you really look at code,
some large fraction of code
is broken with respect to
the strict aliasing stuff.
Because if you think about
it, well typed high level code
just isn't gonna have the
kind of pointer casting
that's required to break
the strict aliasing rules
but on the other hand why are
we using C and C++, right?
Most real programs somewhere at some point
get some type cast in them
and end up falling afoul often
of the strict aliasing rules,
it's quite hard to avoid
breaking these rules.
And consequently, it's not that uncommon,
I'll talk about this more later,
it's not that uncommon
to see real programs
that compile with strict
aliasing stuff turned off
because we just don't know
where stuff is broken often.
So here's one where I had to go back
to a bit of an old version of Clang
to get the compiler to generate
the code that I wanted.
I don't know if this was fixed,
I don't know if the thing
that I'll talk about here
was fixed on purpose or accidentally
but the recent versions
of both GCC and Clang
have been doing something friendlier
than what the code is showing here.
So what the code does is
function foo calls bar
and then does some math
and if you look at the assembly language,
the issue is is that the
integer divide instruction,
the idiv there four
instructions down in foo
precedes the call to bar.
Does that make sense?
So there's no reason at first glance
why the compiler should care,
it can reorder instructions fairly freely
but the issue is is that
what's the thing about the
idiv that's kinda funny?
(audience member speaking off microphone)
Yeah, that's a trapping
instruction, if we divide by zero,
on X86 that's gonna throw a
trap and terminate our program.
And so that trend generates
something which is,
so it's undefined behavior
so the compiler doesn't care
but it's something that we can see happen
and this externally visible behavior
that happens because of the idiv
isn't considered external behavior.
It's undefined behavior
and the compiler's obligations
have sort of ceased
if that makes sense.
So if we take this program
and we generate kind of
the obvious wrapper for it
we're gonna call foo with zero
and we're gonna provide
an implementation of bar
that prints something,
this is what happens.
You call Clang without optimizations
and you run the program
and it prints hello
and then terminates from a divide by zero.
You do it again with
the optimizer turned on
and it doesn't print hello
because the instruction
which terminates the program
preceded the divide.
So this sort of, this is something
that I personally kind of hate
because the whole reason
I put that print F in
there presumably was,
it was like a got here, right?
I got here and we didn't crash
so now I sort have divided,
bisected my program into the part
which could've generated the crash,
and the part that couldn't
generate the crash
and now that bisection has
potentially been invalidated
by some instruction reorder.
(audience member speaking off microphone)
Well okay so the comment
was undefined behavior
can travel back in time
so yeah exactly right so.
(audience laughter)
So undefined behavior
can travel back in time
and so this is and this is, here go ahead.
(audience member speaking off microphone)
So you're saying that
was stopped on purpose
because we didn't know that
bar was a purer function?
- [Man] That is an invalid transformation.
- Yeah okay okay good good
well no that's good to know
that was stopped on purpose
okay good sure yeah.
(audience member speaking off microphone)
The question is if the
compiler had seen the print F
if it might've been able to see that
that didn't throw or didn't call exit
yeah yeah good good anyway
so the kinda cute thing about this one
is that the time travel
aspect of undefined behavior
is actually explicitly spelled
out in the C++ standard.
So I've just grabbed from
C++ 17 here but it says that
the undefined behavior sort
of precedes the instruction
or the statement that caused it
and so this is so it's
sort of explicitly allowed.
Okay?
Still happy, still with me?
Alright now I'm gonna show you one
where this is the most
artificial code fragment ever.
It's explicitly designed
to show you something,
it's not real code and it's not good code
so what it is is code
that returns one of Fermat's last theorem
as a counter example
within A B and C less
than a thousand, alright?
And otherwise doesn't return
and I've explicitly made it
so that it doesn't return,
I've explicitly structured this
in an extremely specific
way to prove a point.
But the point is looking at the code
it shouldn't return one,
it shouldn't return true
unless Fermat's last theorem
can be disproved within this bounded space
of the space of integers,
is that, do we buy that?
(audience member speaking off microphone)
The integers can't
overflow here, that's good.
So the question is yeah so if
we could have integer overflow
in the multiplications
then my whole program would be bogus
and the compiler could
break it in a different way.
But the bound of a thousand is chosen
to put us just below two billion.
Yeah, yeah.
So we're gonna barely
stay within 32 bits here
so there's no integer overflow.
So we take the sort of you know silly code
that's doing some Fermat stuff
and we wrap it in some C++ code
which just calls Fermat
and then Clang five
will just happily print
that Fermat's last theorem was disproved
and of course it didn't of course right
obviously the theorem isn't false
and obviously the compiler
didn't find values
for A B and C that disproved it, right?
What it did was terminated
an infinite loop
because that loop didn't contain
any side effecting instructions
and this is explicitly
allowed in the standard
and I was kind of complaining about this
a number of years ago and
Hans Boehm wrote a nice note
about this issue which explains
exactly why this is a good design choice
and why it's hard to
do something different,
it's actually pretty awesome,
I really recommend reading it,
Hans is an extremely smart
guy and persuasive author
and so like I was all mad about this issue
and then Hans writes this so I was like
alright well whatever
you know he's right fine.
(audience laughter)
Fine whatever.
But the funny thing is though is that
this particular issue irritated enough
embedded systems developers
that in C11 they put a loophole
which is that if you have
while constant expression,
that's an infinite loop,
alright so while one
or something equivalent is
an infinite loop for real
and can't be terminated
by the compiler but other,
even syntactic forms,
it does nothing about
the semantics of the code.
Other syntactic forms
are still vulnerable to
this particular thing in C
and like I say what I
believe happened is is that
enough people got irritated
with the while one loops
in embedded software where
you're trying to hang the process
there at the end of some loop
and just take int erupts.
Enough of those were
terminated in the process
or dropped in the
garbage that people were,
people put some pressure
on the standards committee
to add this loophole, I guess, I don't,
I didn't hear that story but
that's what I believe happened.
Okay so what does this do,
this sort of forces us to put
some sort of a side effect
in a potentially infinite loop
or else the compiler can
just sort of drop us out.
And so if I had gone back to
the code and put in a malloc
or touch a volatile or
anything like that--
Sorry, say it again?
(audience member speaking off microphone)
Yeah, returning A would--
(audience member speaking off microphone)
Yeah, trying to print the
actual counter example
would've generally caused the compiler
to not do the thing that it
did, that's right, that's right.
Okay so here's the deal, so as a developer
you have to obey all of these rules
and you have to obey them all the time,
you have to obey them consistently
and you have to obey them all the time
and by default nobody tells
you when you break a rule
and this recipe has lead us
to sort of serious problems
starting with like maybe the
Morris Center net worm, right?
So something, some, 20 some
years ago, what was that, '88?
So starting with the
Morris Center net worm
or something like that
when buffer overflows
and finger demon and things
were turned into a nasty virus,
we've been sort of getting
burned by this stuff
for a long time and it's not a very,
it hasn't been a very good situation
so it's getting better now
but it's sort of getting
better slowly and in pieces
but anyway I just wanted to kinda
paint the picture of the
situation as I see it.
Okay so let me switch gears some.
So what I would like
to do now is talk about
what we can do about it
and this is not gonna surprise anyone,
the things we can do about it,
none of these things
should surprise anyone.
So let's talk about it though.
So one thing that I
feel very strongly about
is that code reviewers
need to be thinking about
undefined behavior, so the code reviewers
need to understand undefined behavior
and they need to be sort
of on top of this issue
and so it's pretty
reasonable as a code reviewer
to ask the person who wrote
the patch what's the proof,
you don't want a mathematical proof,
but what's the informal reasoning
why this particular line can't
invoke undefined behavior?
I think it's absolutely
reasonable to do that
and if people put up a fuss
about you ask them to proof
just say that I told you
to tell them to do that.
(audience laughter)
So let's just look at an example.
So the function foo here
just executes a shift
and so what's the
precondition of function foo,
what has to be true before
function foo executes
or else our program is undefined
and so while all of us
know the first one probably
which is that the shift exponent has to be
larger than or equal to zero and also
less than the width of the shifted value
so we all know that one
but on the other hand there's another one
which is much more often
violated in real code
which is that the Y high
bits of X have to be clear.
And whether it's Y bits or Y-1 bits
I mean there's a little
bit of ambiguity there
based on the version of the standard
but there have to be some high bits there
and so like I say this
is, I'm very serious
that when somebody has this code
if there's any doubt as to
whether those preconditions are true
you should as a code reviewer
you should put up a fuss and say
what have you done to prove
that those things are true?
And they should put an
assertion if they can't prove it
or there should be something
because it has to be true
and it doesn't make sense to ask somebody
to test the case or it isn't true.
It just can't ever be false
or else the program's broke.
So let's talk about, so--
(audience member speaking off microphone)
No, so the question is does
this mean a barrel shift
is a valid implementation left shift
I'm not sure what to do with that.
(audience member speaking off microphone)
Oh the question is is a rotate
a valid implementation of left shift.
(audience member speaking off microphone)
Yeah I suppose it is, that
would be a fun code gen change
to put into a compiler
and see what happens yeah.
(audience laughter)
So reasoning about
preconditions is hard, right?
I mean this is program
verification stuff and people,
and pushing these things
around the program
to the place where they all need to be
and especially pushing preconditions past,
this is really hard stuff and
we wanna offload this to tools
but on the other hand this is stuff
that as a computer science teacher
this is stuff that I really try to
beat into the heads of the
people that I teach is really
people need to be capable
of this kind of thinking
and so like I say I think
it's absolutely reasonable
when reviewing patches
to make people produce
these little proofs.
If it's an important part of the code.
So that's kind of manual static analysis,
a static analysis in your
head but also there's,
we of course have static analysis tools
and I don't need to say
too much about these.
What I wanna talk about
first really briefly though
is unsound static analysis tools,
these are the ones that
are designed to miss bugs.
And so designing tools that
miss bugs by design is awesome
because these can provide in many cases
a much better value proposition
than tools which are
designed to not miss bugs
and so here of course I'm
talking about things like
compiler warnings, tools
like Coverity and Klocwork,
Clang Static Analyzer
and all of these things
that are allowed to miss
bugs are super awesome,
they're gonna look for stuff
that's definitely wrong with your code
but they're not gonna find everything
that's wrong with your code
because that's just
generally way too hard.
So sound static analysis is a
whole different can of worms
and this is something where,
so a sound static analysis tool
is one which is not allowed to miss bugs
and so a tool that's not
allowed to miss bugs,
if it says that there's no
buffer overflows in your program,
then it's essentially, that's
equivalent to the statement
that I've proved your program
is free of buffer overflows
and these kind of tools I
spend a lot of time with them
and they are sort of hell on earth to use,
they're just horrible
horrible tools to use.
The state of the art for C++
is much worse than it is for C
and that's on the C++
committee basically I mean
that's on C++ design, C++ is just really,
I mean I've talked to people
while they were working on
C++ Static Analysis Tools
and it's really hard to do.
It's not easy at all for C,
static analysis is
fundamentally a hard thing,
but the C++ as it currently exists
has really made things
hard for these people.
But so there are some tools
that can do sound reasoning
about C++, not very many,
and they require a lot of hand holding
basically you sort of have to
become an expert in the tool,
it's not like an unsound tool
where you kind of use
it and get value from it
and if it doesn't provide value
you kind of don't worry about it.
These tools you have to
invest and you have to,
you really really really
sort of need an expert
and to some extent the company's
selling these sound tools
have had sort of a bad
luck selling the tool
and better luck selling the services
of people who know how to run the tool,
does that kinda make sense?
(audience laughter)
because the tools are that hard to use
that they really require somebody
who knows how to baby them along
and so my personal belief is
it's more of this kind
of sound static analysis
will happen in the future
but the gap between where we are now
and applying sound static
analysis to Chrome or something
is really a vast gulf
that may never be bridged.
So I'll tell you what, I'm gonna stop here
and I'm happy to take questions and stuff
and I will switch to the next,
I'll start talking about dynamic tools
in the next part of the talk, okay?
Feel like I've been talking long enough,
my throat's all dry.
(audience applauds)
Thank you.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>