<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Mike Ritchie “Microcontrollers in Micro-increments...” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Mike Ritchie “Microcontrollers in Micro-increments...” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Mike Ritchie “Microcontrollers in Micro-increments...”</b></h2><h5 class="post__date">2017-10-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XuHlDtWYeD8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- [Mike] Okay, hey, everyone.
Sorry about the slight
delay in kicking off there.
I've been MacGyvering some
hardware on the chair here,
but I think we're setup and good to go.
So hi, it's good to see you here.
My name is Mike, and this talk today
is about approaches to development
for small microcontroller class devices.
Heavy emphasis on TDD and
continuous integration.
There's gonna be a real
little mix in here.
There's gonna be some tools,
there's gonna be some build automation
and there's gonna be a
bit of line code even.
What excitement.
I think we're gonna be
kind of tight on time,
so if it's possible for
you to part questions
until the end, that'll be great.
The slide numbers are really
prominent on the slides,
so just note down one of those,
and we can to back to them.
So just to set the scene a little.
I just want to look at
some trends embedded that
I think shape the way that
we're doing development now.
I'm gonna caveat this very heavily,
because I don't have hard data on this.
This is based on
conversations that I've had
with individuals and with teams.
But I think this situation
of a more library-rich
embedded environment
is becoming more and and more common.
Customers, whether they're
industrial, or commercial,
or consumer, they kind of
expect more from devices
and they want it faster.
So it's more and more common for us,
to be reusing established libs.
So I think for every team
that breaks new ground,
using their sort of classic method
of looking at the spec sheet and doing the
classic board bring up stuff,
there are many, many more
teams who are living
in this kind of world,
where they're expected to
make a quick start a project.
So the vendors are investing more and more
time and money.
And this is their response,
to make it faster and easier,
to get started, with a
working configuration.
Some vendors are doing
full blown web ID's,
Arm Mbed is one, I think
MicroChip are doing this as well,
as an alternative to desktop tilling.
So to point and click,
you can figure the pins,
you can figure your clock parameters,
you push the button, and here, presto,
you get a hardware abstraction library
that's customized to the
functions that you've selected.
And it's great in a sense.
It gets you started really quickly.
That's a useful thing.
I find these tools very
useful to start projects.
But, the libraries always have some kind
of testability cost.
Right, they want you to
work in a particular way,
and sometimes that can be a barrier
to doing meaningful development testing,
as the project progresses.
They also, incidentally, generate
two of the most dangerous lines
that you'll find in any
embedded source file,
which is this one.
And people take that sometimes
just a little bit too literally.
It doesn't mean put all of your code here,
it means there are some
initialization here
that you might require.
So what these lines be, they're automated
for two inconvenience.
They're optimized for cogeneration,
and not so much for humans
to read and maintain,
and certainly not for testability.
The vendors aren't that
focused on the use case,
where you are testing
your code off the target.
So designed to work well on the target.
So we have this challenge,
of tills being really useful
for initial config, but then, potentially,
obstructive as you project moves on.
So, yeah, I like for more,
than none for me, as well.
So, this is a problem.
If you've got libraries
that are only really usable
on the target, they you get
drawn towards the target.
More and more of your
testing can only be happening
on the target hardware.
So, that's a problem, with
an ever increasing size
of software stack.
So, why TDD, and how does TDD help us?
Let me just say upfront,
that TDD is not some silver bullet, right.
It's not panacea, it's
not gonna immediately
make your life easier.
It's a difficult thing to practice,
difficult thing to grasp.
And you are gonna have to do
some grand work in your build,
to make this happen.
There's basically a feedback
loop, that tells you
how your code is behaving
with a test passing.
And it also gives you
feedback on the health
of your design, as your design progresses.
It doesn't fix your code
for you, that's your job.
So there are really two
versions of the story, for TDD.
One is the completely generic
one, that you see here.
And you've probably heard that many times.
There are standard arguments
for why TDD is a good thing.
Then there's the embedded story,
the embedded specific story.
So see, the one that probably stands out
from the generic ones,
is the decoupled code.
If you are testing your
code off the target,
on your host, on your development machine,
you're forced to address
hardware dependencies.
You can't depend on
hardware that isn't there.
And that's the discipline.
For TDD for embedded,
starts to get a little more interesting.
This is the embedded
perfect storm, if you like.
The constraints are
well known, that's been
the sort of defining
characteristic of embedded
for many, many years.
But there are other things,
that make our lives complex.
You might be surprised, I
don't know how many people here
work on embedded, or
other technical domains.
But despite all these libs,
and even with a real-time
operation system,
it's a remarkably blank canvas
compared to other applications.
If you look at people
doing server development,
and enterprise systems,
or GUI development,
there are very definite guide
rules these people work along.
That's not so much the case with embedded.
So the kind of industry
response to this is,
well, more process,
we'll do more UML models,
we'll do more design.
I don't really think that's adequate,
I think a diagram is just a hypothesis.
The bigger the model, the
bigger the hypothesis.
It doesn't really reduce risk.
Coding standards and static
analysis tools are great.
I love static analysis tools,
I find them very useful.
But they don't tell you code is right,
they just tell you it's safe,
and is doing what is should be doing.
It doesn't really tell you,
functionally, it's correct.
So I think TDD in small
increments is a rational response,
to these sorts of challenges.
There are many variants
on what a test does.
So TDD tests can be run on the host,
you run it on your developer machine,
using your framework of choice,
I'll be using CATCH and
Trompoloeil For these demos.
You can run tests on a simulator,
you can have tests running
on your eval hardware
but still stubbed, you're
gonna see stubs and mocks.
The tests running on eval hardware
and actually interacting
with the peripherals.
And then full-on target
deployment, complete system tests,
running the fully integrated hardware.
So we're gonna be doing some
examples with the top one,
and kind of with the
bottom two, in this talk.
So, a few patterns that I
think are worth talking about.
I won't be able to get
into this in massive depth.
I really highly recommend this book,
if you've not yet had
a chance to look at it.
Gerard Meszaros wrote a, I guess,
what is the definitive
view of patterns for test,
and patterns for substitutability,
because that's really
what testability is about,
is the ability to isolate
your code for testing,
by different types of substitution.
I'd also recommend Michael
Feathers' book as well.
I think that's a good
background for this stuff.
But we're gonna be looking at mocks,
we're gonna be looking at stubs,
but there's more out there.
And a question, rather than a statement,
is we're thinking up-front about
what are the different
concerns within your system?
I mean, most of us have
some kind of domain logic,
most of us have, obviously,
a hardware interactions.
But, what else is there?
Because these things are gonna help
to frame the kind of high-level
software architecture of our system
So classic wo.
Let's talk about a dependency
inversion principle.
So this is, I guess, can
I say 1990's style wo.
The principle is that high-level modules
don't depend on low-level modules,
both depend on a shared
set of abstractions.
This is a mechanism for teasing apart
these dependencies within your code.
Abstractions don't depend on details,
and details depend on abstractions.
All right, the best thing
about dependency inversion,
is that it always works.
The worst thing about
dependency inversion,
is that it always works.
And people overuse it,
and use it unwisely,
use to excess, and it's
very easy to end up
polluting your type system with a bunch
of very fine grained, tiny interfaces,
that makes your code hard to understand.
I did smile up Bianer's
reference in the opening keynote.
He talked about C++ being hijacked by
sort of inappropriate idioms.
I think I about I this
when I heard him say that.
So I did find that it works.
I mean, it's usable, but I think it helps
when these interfaces kinda
coalesce module boundaries
rather than having each and
every type in your system.
So really simple example.
Here's our abstractions.
We got a couple of types,
we've got our washing machine.
We've got our indicator LED.
So pure virtuals, interfaces.
So both sides of what was their dependency
only know about these abstractions.
Our implementation of the washing machine
is expressed in terms of the,
we abstract indicator LED
not the concrete type.
And so that's easily
substitutable for testing.
So a typical test with
catch and Trompoloeil.
We create our mock object.
We inject that into the washing machine.
We run some code and we make an assertion.
And the fact in the case of Trompoloeil,
we have a requirement up front.
We expect that mock object
to be interactive in a particular way.
So this is seductively simple.
And this is why I think this kit of idiom
tends to kind of spread in codebases.
It's quite easy into the framework,
so posted very well, but
that doesn't mean it's always
the right thing to do.
A word on mocks before we move on.
I find these misused an awful lot.
That the point of the mocks is
there's a way of discovering
the requirements, needs that
your code has from other types,
so basically a design process.
So what I'm implementing here, my,
what is that?
Rinse cycle, my washing machine.
And so the washing machine
references will become clear
a little bit later on.
What I'm really doing is I'm
discovering the requirements
I have from other types.
And if you aren't doing that,
if you're talking to
some third-party library
or the hardware abstraction library,
mocks don't really doing anything for you.
You're not designing those things.
They're fit complete.
They're there already.
So only mock when it's your design.
Don't mock otherwise.
So testing the right
interactions of the hardware.
If you are implementing your
own hardware abstraction types,
like GPI or ports, analog to
digital converter, whatever.
That's actually refreshingly simple.
These kinda state-based unit
tests are really fun to write.
You do a thing and then
you verify the effects
of the thing that you've done.
So in this case, I've got a GPIO port
and an STM32, the GPIO has lower half word
and upper half word and
one of the registers
for setting or resetting bits.
So when you set it low,
then that goes to the
lower half word.
If you set high, there's
a shift and it gets set
into the upper half word.
So these are really simple tests.
As long as you can
construct the memory layout
of that peripheral register in your test,
set the pre-conditions and then
make verifications about it,
it's really simple.
So simple construction of the memory
that fakes out the memory
that would otherwise
be in hardware register
and then placement new
at that location.
Really simple with your test.
And I would strong recommend
that if you can get
away with writing these
kind of state-based test,
do that in favor of mocks.
So you make your types
easy to use, easy to place.
And then you will write more tests.
And when you write more
tests, that will help you
improve the usability of your re-types.
So something like GPIO is never gonna have
a virtual interface in front of it.
There's never gonna be
an abstract interface.
The closer you get to the
hardware, the less effect
of virtuals become as
a kind of design idiom.
So template seams can
be quite effective for
if I have a dependency on GPIO,
then I can use a template to use
with some favor (mumbles)
compile type duck typing.
But I can use is it in
combination with mock objects.
It's important to note
that with mock objects,
you're not absolutely compelled
to be doing the kind of
classic virtuals abstract
interface idiom.
Works perfectly well for
concrete types as well.
So in this case, our door status indicator
for our washing machine design,
those are the couple
of methods, on and off.
It requires the services of GPIO,
which is actually quite a large type,
but we only need to mock the substitute,
the things that we're using.
Okay, so we're only using the
right function in this case,
so a mock object could be very small.
We can define that within our test case,
construct our object
parametrized with the mock,
rather than the GPIO type.
And we're good to go.
Okay, so there we go.
So mock GPIO, there's a test case.
Construct the mock, construct
the door status indicator,
parametrize based on the mock
that we're using this time,
rather than real deal GPIO.
Expect a call to write.
And then exercise the indicator.
Okay, couple code words with real time op.
Who uses real-time operating system here?
Is that fairly common?
Some, bear mat lonely.
Yeah, I really, I'm gonna air
my colors to the mast here.
I really like RTOS immortalized.
We find that can do better
design when I'm using
a real-time operating system.
But there are some challenges.
So we tend to kind of
complexed in by those tasks.
And real-time operating systems
tend to kind of create a
lot of responsibilities.
And that's something are
saying that the design
has not been truly worked out.
So there are things that we can
do to improve the situation.
We have the problem of
while(1), of course,
which kind of a showstopper
from a testing point of view.
They never exit, so
we're gonna have trouble
doing anything with it.
So I often find as well, when I'm using
a real-time operating
system, I wanna transit
from task land into object land, right?
Have a more, kind of
object-oriented design.
And I need to find out if
we're doing that effecively
so with free RTOS, for
example, we have our
taskCreate function.
And the one I want you to
focus on there is PD parameter.
It's like there's a single parameter
we can pass through this point void.
And what do we do with that?
Well, we can construct
something, an object
passed in as a parameter.
But what are their
resources we need to access?
We need to access semiphores
and queues and new texts
and other typical kind of RTOS constructs.
So you can create a struct,
a kind of a parameter pack,
but that's not ideal either.
So what I tend to do is a very kind of
very lightweight use of templates,
just to bridge that gap from I have my
RTOS createTask that I wanna invoke,
but I also wanna get to root that task run
to an object instance that I have.
So couple of very simple templates.
A wraparound createTask template function,
which basically replaces what
would previously have been
our RTOS task.
And then when the scheduler runs,
it's the function template instantiation
of that RTOS task
function that gets called.
So they'll have the
same task being created
for the same type.
Let's say you have three buttons
and have three different tasks for these.
But that's okay, so same
types, so same function,
template instantiation, so same address.
But all it does is casts that
one parameter to the object.
And then passes onto that.
That still leaves us
with the 401 challenge.
So a bit of very light touch
policy based design works here.
So we can compile time
strategize loop that effect.
So we have strategies forever,
which is the default case
in the RTOS.
And for testability we can
have a specific number of times
through a loop.
And we'll end up with our testable blinky,
which looks a lot like that.
So default template parameter is forever,
as I said, that is the normal case.
And then we can override
for test purposes.
So we have a blinky task.
We parametrize it to see
what list of one single time.
Now we run it and then we
make an assumption about it.
So what was previously quite
a difficult thing to test,
we've adapted to fit
the model that we want
and to adapt to the way we want to test.
Okay, there's brief third-party
libraries, Stub, just Stub.
They're the least work that you can do
in order to get your code
running to get it compiled,
get it to link.
And only do something more sophisticated
if there's an absolute
requirement to do so.
So if there's a return value
from a third-party library
that's gonna effect the
way that your code flows,
then, okay, you may have
to do something there.
It is absolutely essential that you know
the right parameters
that are passed through
this third-party API.
Then, okay, you might have
to record those parameters
in your stub.
But the default approach is just stub,
hardcore return values, unless
it's a significant impact
on the way that your code behaves.
So test spies are another
pretty good approach.
These basically just record.
The parameters are passed
to the function call.
But only stub as well when you're impeded.
You don't go and create
proactively a whole set
of stubs for an entire framework,
unless you absolutely
need them in your code.
But most people, you'll
probably use fraction
of the library's capabilities.
And do, do, please resist the temptation
to use mock objects for these.
That is almost never the right thing.
That's just not the right tool.
You're not doing design of that library.
So you don't really need a mock for it.
There's a nice framework called fff,
which I'd recommend you look at
if you need a little
bit more sophistication
in your stubs and spies.
Okay, so stub when you need to.
Couple words about testing on the target.
There are a lot of reasons to do this.
You definitely want to know
you're co-compilers, obviously,
you're gonna be compiling
every time you have a commit
for the target.
Compiles are different quirks,
you have different bugs,
standard supports in different places
for multiple compilers.
Really identical.
But I found over time that the TDD test
that I want to run on the host
are not really the same test
that I want to run on the target.
I rate these tests on target
tests for different reasons.
They're really kind of verification tests
on integration tests.
So I know some people try and get
all of the TDD tests
running on the target.
I found that to be pretty painful,
I think you'll run into size issues.
And you end up wanting to chunk
your tests into many suites.
I find it difficult.
And I also find that it sort
of drains the energy from
your TDD utility.
So over time I've come to use
different frameworks for these.
If we're gonna test on
target, or use unity,
if I'm doing TDD, I use a modern test job
in CSS framework like CATCH.
Okay I use SEGGER tools for this.
One of the challenges is
just getting the results
back from the target.
I use SEGGER_RTT framework,
which is kind of telemetry
and data transfer
and library, that let's you in, that data
over your JTAG debugger
capture using console logger
and get that to a file.
So Unity is super simple,
really, really low footprint.
If your code runs on the device,
you can probably get some
tests running by Unity
on the device.
So the mechanics of this are that
I have to implement
some putchar that works
over that mechanism or RTT.
I have to let Unity know
that I want you to use that,
That's how we're gonna putchar.
And then when I'm capturing a log,
I have to use a Segger tool to do that.
Now you can do this with ARM semihosting
you could do it with a UR interface.
It's entirely up to you how you do this.
But I tend to have the Segger
tools within my build anyway
including the RTT framework
so that works pretty well for me.
Whatever you do, you're gonna have to make
significant changes to your build
if you don't have any tests
in there in the moment.
It takes a little bit
more internal modularity
to get this working effectively.
You're you gonna have to
substitute implementation
or link time in order to get
that sort of substitution
of stubs and spies.
So you will find that you have to look at
how your build is chunked.
And also use that as a way
of I guess constraining
dependencies within your build.
It's good to kinda reinforce your software
or alcatites you're using
the build if you can.
And talking about build, CI.
So TDD is a very designing testable code.
It's about designing, good
code, getting it right.
But what happens with
those little increments?
How do you actually get to the next step?
So the way I think about CI build
is kind of as a bridge.
It takes me from walking on the host
to testing on the target.
I very, very rarely build manually now
for the target platform.
It's virtually always automated,
it's virtually always
through the CI build.
Not never, just rarely.
Some things you're gonna
have mysteries to solve
and you still have to do it.
So all this is possible
with open-source tools.
We're gonna be using
Jenkins here as an example.
But we'll look into all
the different elements
that are involved in the build,
but I'll give you a
walkthrough, first of all.
So build on every change.
Yep, so I use Git commit hooks to do that.
The way that Jenkins works, it will index
an entire repository.
So any commit on any branch
will result in a build.
So you build and run the
unit test on the host,
right you're running on
a EMD 64 architecture.
Usually the next box for me.
And any failure you stop the line.
And by stop the line I mean two things.
One is you stop the build pipeline.
And the other thing is you
stop the production line.
You stop coding.
You don't keep adding on
top of that failed test.
So you diagnose and fix the
problem and you move on.
We compile for coverage.
I use Clang and LVM and LLVM cov.
I find it's a really nice tool.
I archive the coverage
reports in the server as well
in case I want to go back and look at 'em.
Re-compiler tests and
re-run using the sanitizers,
so address sanitizer memory
and undefined behavior
as a minimum.
Threat sanitizer wouldn't make sense,
it's an ARM cortex, ARM device
so that's not really gonna
be applicable.
That's the slow part of
the build as we'll see.
Stack analysis, clang-tidy,
clang stack analyzer.
We're kinda getting into slightly
more subjective territory
or you've got this kinda difficult call
about what you do with clang-tidy issues.
It can be a bit sometimes,
so sometimes you have to dial
back the severity of that
or say, &quot;'m gonna treat
that as an advisory
&quot;and I'm gonna let the build move on.&quot;
Other people take a stricter view and say,
&quot;Well, if you're taking it seriously,
&quot;why is it in the build?&quot;
You know, fill the build.
So, yeah, it's a judgment
call on your part.
Build and deploy the
test-on-target tests, always.
So using Unity and using
the Segger RTT framework
as we looked at.
This is a little bit convoluted
so you have to deploy
and run the tests.
You have to have the logger running.
And you have to deal with timeout as well,
because if your code ends
up in a hard fault handler,
then you're gonna be waiting
a long time, you know.
You'll have the longest
cup of coffee ever.
So you need to kill the logger process
and see didn't work, in
the event of some timeout.
So you're scanning a log
for a success indicator
and usually it's pretty clear with that.
Start a line, okay, and
the absence of that,
all bets are off, stop the build again.
Building the real deal
binary here in multiple.
You're building the L and the X files
in multiple configurations.
So debug, release, releasement of size
and release of debug info.
Archive the whole lot.
And then finally deploy
that to the target.
Okay, really, there's a
lot of value in there,
but the start and end of that, to me,
are the most kind of liberating part.
Just the fact that I know
that every single commit
is gonna get built and
every successful build
is gonna end up in the target.
I've wasted too many days, weeks, months
thinking are you chasing
phantoms on the target hardware,
in debugging hardware sessions?
I'd just not been really
clear what's on there.
You know, &quot;Did I make that
change before I flash that?
&quot;Or do I have to do it.
&quot;Okay, I know, what version is this?
&quot;What branch am I on?&quot;
It's really clear with CI
because the last time this was all green,
that's what's on the board,
that's what's on your eval hardware.
One thing I would add to that is fuzzers,
don't have the present.
So this is what's involved.
We'll use, I don't know
how many of you are using
Docker as a kind of CI
containerization mechanism.
Really useful tool.
So Docker plus Jenkins and a JenkinsFile,
CMake Build as well,
and obviously Git for commit hooks
and your source, at least.
So there's kind of symbiotic relationship
between these things.
Jenkins is gonna run assuming
that there are certain
tools and tools chains
on the host environment.
That's Docker's job to provision those.
The JenkinsFile is mostly
gonna be running targets
that are on your CMake build.
So they have to be kind of co-versioned.
So you have to move these on.
There's a little bit of elasticity there,
but you can end up with
incompatibility issue.
So you're changing the
structure of your build,
you're probably gonna have to change
your Jenkins ability as well.
Okay, couple of very smart (chuckles) guys
once wrote this statement.
And so this is, the stuff
that we're talking about,
the TDD, and the continuous integration,
this is, for me, is a
way of getting to that
idea of correct.
I know what correct is,
because my tests are passing.
The sanitizers aren't firing.
Stack analysis is all green,
clang-tidy is all green,
et cetera, et cetera,
it's running on the board.
And then it gives me a test
harness on a safety net
for getting to the next step,
where the next step is fast,
the next step is small.
It's all good.
Okay, hardware.
Let me introduce you to
our hardware de jour,
this is the washing machine.
I know you can't all see that.
It's kind of awkward.
I've got it up on the screen here as well.
This is a design that I've created
to help you with the
practice these things,
to help practice test
run development and CI.
Understandably if you're
working on, let's say,
safety critical systems,
you're gonna be a little bit nervous
about just randomly experimenting
with new design approaches,
new test approaches.
So it's good to have
a safe sandbox to that
and that's what this is for.
So it's a platform for
deliberate practices,
basically a code cutter,
but it's a code cutter
backed up with hardware as well.
So as functional spec,
there's a hardware design
and the idea is to use that as a vehicle
to explore TDD, continuous
integration and other techniques.
Okay, so let me tell you
about the design of it.
I'd think about some of
the challenges embedded,
what makes embedded difficult.
So there's different
programming models in there.
You have to work with timers,
you have to work with interrupts,
there's pooling as well.
There's DME, if you wanna use that
for the analog to visual conversion.
There are finite state machines
within finite state machines
within finite state machines in this.
It's actually a reasonably
tricky control problem.
And lots of different
stimuli, lots of events,
there's mix of sensors,
there's analog sensors,
there's button presses,
there's user actions,
there's timers, et cetera.
Okay, so we're gonna put this to work.
Now bear with me just a second.
We're a little bit weak in the setup.
So I just need to verify a couple things.
I actually have a connection to...
I have one here, (chuckles)
you can't see it,
it's on the seat.
So just for clarity, USB connection,
I have Segger there, honest.
I have the hardware there.
I have a webcam,
which is very badly set
up and badly focused,
but when it comes to actually
getting this running,
we'll see if we can make it work.
So I am going to start my CI build.
And I am gonna try and
unmirror the screen.
(mumbles)
'Kay, that might work.
I think it did.
Okay, so let me just flip
back the slides for a moment,
just to kind of explain the...
So this is the kind of top
level state machine design
for the washing machine, an
actual state machine design,
like I'm some kind of
professional or something.
So the top level is you're looking at it
from a selecting state, and
it's an operating state,
so depressing button just
to slide the program.
You close a door.
It's a very significant event.
You close the door and then
you're ready to run the program.
So what we're gonna be
doing is precisely that.
We're just gonna do some
TDD or going from that
kind of open to ready state.
And just to make it interesting,
to make this a non-generic TDD example,
we're gonna do that using
our real-time operating system task
and see how we can TDD that.
Okay and (laughs) yeah.
I've done these before.
It gets kind of really confusing
when people have conversations as well.
So if you don't mind,
park questions there,
and I'll be very happy to take them.
But let's make it start.
So our problem is a task, right?
Our RTOS task.
I'm gonna just jut down some tests here,
kind of a to-do list.
So what we do is when
we read our high signal
on the slide switch, that
means the door is opened.
When we read a low signal,
that means the door is closed, right?
So we're gonna go through
our washing machine.
Our washing machine already
has a logical narrative,
switch on the LED,
so we're not gonna be worrying about that,
so just the task implementation.
So when high signal is read,
let me go into presentation mode for this.
It'll be easier.
Can you see that okay?
Is that legible? Cool.
So when we read a high signal,
sets door status to opened.
Nobody wants to see me typing
so I'm gonna copy and paste some of this.
When a low signal is read,
sets door status to closed.
And then we got the looping logic as well.
So gonna say when run multiple times
or run in a loop, sts door
status multiple times.
Oops.
So, in defense, my finger flops (mumbles)
until about two months ago.
I'm really not used to
IDs trying to help me,
so you'll have to put up with those.
Okay, so implicit in that last one
is that there's some
commonality in the first one.
So I'm just gonna capture that.
Say, when run once, this.
'Kay?
So Ken Baker recommends
writing a test list
for what we're about to do.
So we have a mini roadmap,
a sort of a do list.
I find increasingly we're using CATCH
I'm just writing that, writing the code.
I don't implement all the tests,
just one at a time, but all the same.
So this'll what we'll do.
So what we're gonna need here,
well, we're gonna use a mock object,
or washer that we're gonna use to
set the state of the ID into.
We need some memory.
Memory_layout, call it fake_memory.
And the only register in
here that we're interested in
is the input data.
The rest of it is not gonna
be used in this scenario.
So I just made that explicit
and initialize it there.
And, finally, we want
to construct the GPIO
using placement new.
So gpio_port calls standard
two, port, place that.
Okay, so these are two
dependencies our task needs.
The task needs GPIO.
It's gonna read from that in
order to find out the state
of the door switch.
It needs to know the model to talk to,
that's our washing machine.
So the final missing link
here is we need our task,
door handling task.
And our hypothesis here
is we're gonna inject
the washer and the GPIO and that's all
just gonna work splendidly.
So this is our system under test, right?
That's the thing that we're testing.
Everything else is kind of a collaborator.
Okay, excess elements,
instruct initializer.
So compiler speak for, please implement
our constructor.
So let's do that.
So what do I need here?
I need the washing machine.
And I need the GPIO port.
What happens when I run the tests, okay.
So I'm back to a passing
state, not actually
doing anything with them, yet,
but we'll compile 'em.
So we can go out and do something
a bit more real in there.
So I'm gonna set my expectation
that the washing machine gets called,
REQUIRE_CALL, mock_washer, door_opened.
Okay, that's an IO state.
What do I have to do to make that happen?
Well, I have to make that pin go high
for the door switch,
fake_memory.input-data.
Now let's DOOR_SWITCH_Pin, there we go.
Finally, we're gonna run the task.
So door_handling_task.run.
It doesn't exist.
Classic TDD, you're coding against API
that doesn't even exist yet.
So I have to create that.
Woops, not quite that, I don't. (chuckles)
See a line trying to help me again.
Create new function, there we go.
So what's the bare minimum I have to do
to make those tests pass?
What's the smallest thing
that can possibly work here?
Well, I'm gonna need the--
- [Audience Member] Always call.
I'm sorry?
- [Audience Member] You always call the--
Just call the washing machine, right?
Just absolutely better call, yep.
So there's my washing machine
I'm gonna need at the field, obviously.
Oops.
I'll just change a new one.
I have to initialize it.
Here, washing...
There we go.
Yeah, typo.
And then invoke it here,
washing_machine.door_opened.
All right, that's the simplest,
crudest thing I can do
to make the, oh, not quite.
Assertions, five, one failed,
what's going on?
(mumbles) Reference.
Oh, yes--
- [Audience Member] (drowned
by distance) Temporary.
Sorry, why am I doing that?
(audience member speaks from a distance)
Ah, yes, good catch.
There we go.
There we go.
So that was the wrong case.
What's the next one?
So another require.
REQUIRE_CALL, mock_washer,
door_closed this time.
Don't need to do anything
to set the memory for this,
because it's already set
low in the test setup.
So I'll just hit to run the task.
Run.
And two failures.
So it fails twice because
it's being called,
that we didn't expected,
and it's now being called
now we did expected, so
we get two assertions
with appraiser one, like some
strange supermarket offer.
So back to my task, what we're doing here.
So the kind of obvious next
step is I need to obviously
get the field for GPIO
port reference, gpio_port.
I need to initialize that.
GPIO.
And then I need to conditionally call only
if the pin is set.
So if the DOOR_SWITCH_Pin is high set,
then we call the print.
And I'll just run that
again, see what happens.
Okay, so we're going from
two field assertions to one.
So with the alternate case we'll know,
door_closed.
Okay, passed.
Cool.
So let me just flip back to the test.
Finally, we have the loop logic, right,
so I'm gonna be lazy here and
just do a little copy again.
So I'm gonna say that with
our code, multiple times,
and I'm leaving the...
I'm assuming it says door closed,
they're gonna just go
to the simplest case.
So that's not gonna work.
There's all looping logic in there.
And also we're not instructing our task
to loop a number of times.
So I'm now gonna have to fix that.
So what I do is I'm
gonna make this explicit
and say, well, your policy is
you're gonna do this once.
Your policy here is you're
gonna do this twice.
Okay.
And I have to make the
change, the task itself.
So that should compile.
Still failing, right?
Still expected to be call twice
when it's only been called once.
So my next step is just to do
something with that policy.
Okay.
Cool.
Not finished, though.
I wanna throttle this task a little bit.
Even though it's a low priority attack,
I don't want it running that often.
So I'm gonna put an explicit delay in here
and say you're gonna
delay for 50 system tics.
Okay, ah, linker error.
So I'm using some RTOS
API, but that's not linked.
So in order to make this compile,
I just have to define the stub for that.
Now we can make some assertion about that,
but what's the point?
It's 50. (chuckles)
I'm gonna serve it 50 is 50, you know?
It's just a literal value.
So it doesn't really serve
any purpose for me to then
do anything sophisticated
with my use of that RTOS API there.
I just need it to work.
I just need it to compile.
'Cause it doesn't affect
the flow of my code at all.
If I was working on some
safety critical system,
and it was absolutely vital
the right value is passed,
can double-entry bookkeepings of approach,
then maybe I'd have to
do something to record
the values that were
passed in as arguments.
But, for now, it works.
Okay.
So, just gonna go back to my test.
And say I think that looks all right.
So I've got a final load
of bookkeeping to do here.
And that's in my builder.
I just have to wire this up
so that the task is actually initialized
when my system starts.
So what do I need?
Yeah, not that, that's for sure.
Static platform, door_handling_task.
And then I'm gonna inject the washer
to act as the real-deal
washer in this case,
and the real deal GPIO.
And so in case you're
wondering why it's static,
the real RTOS schedule works at the point
where the scheduler starts.
That resets as the main stack pointer,
so if you have anything that's
automatic storage duration,
it's gone.
Well, it's not gone, but
it's not safe to use.
It's gonna be used pretty quickly.
And so I need to, what
else do I need to do?
Create the task, get the
address of my new task object,
door_handling_task, give it
some very imaginative name.
And guess that's stack size.
I'm plucking numbers from the sky here,
time honored tradition.
Okay, use of task, yeah, template worked.
Seeing that's actually a template.
There we go.
Too few, all right, did I?
Ah, yeah, okay, I meant
to do this, that's why.
Okay, so it runs forever
unless I say otherwise.
Okay, so
I'm gonna get logged into my CI server,
bear with me a second.
A little rushed in to
print, so couple of steps
I was hoping I had done.
There we go, in the middle
of my pipeline here.
And see what happens.
Okay, right, gonna commit.
(audience member speaks from a distance)
I'll have to share my
state of my mind, then,
(audience laughing)
commit messages.
Okay, so I'm gonna flip
back to the gold here.
So you can see because
I've got a git commit hook
that started immediately.
Now we'll see if I can do
something clever with VLC.
Okay.
Okay, there should be a
small flash on that board
as the target test build gets deployed
and it starts logging on it.
Hope we could see it.
There we go.
The light's kinda dim.
When the (chuckles)
sanitized builds run it,
it's pretty intense.
I can get quite a fast build.
I've got a Z and wash station
at home and it's pretty zippy.
Not so good on a laptop,
but what am I gonna do.
'Kay.
So very little flash there.
So now we're using the logs,
checking that we're good to go.
- [Audience Member] Thought you were using
the exact same test?
Sorry?
- [Audience Member] You weren't
using the exact same test?
So the test that was running on there
was the, there was kind of
a naughty example I did,
which is just reading from GPIO.
Actually it reads from the Start button.
So it was like one sample test.
So it was just reading from the GPIO
by the assumption that it's configured
in a particular way.
So the buttons there have
pull up resist or enum,
so you can see with confidence
that it's gonna be high
unless somebody's
standing there, (chuckles)
with a finger on the button,
which we may try in a second.
And there's our test completed.
So, end-to-end, and what was that?
One minute 28 seconds, that's not bad.
So the moment of truth.
That's my LED, we expect didn't go on.
That's the switch.
Please let this work.
Ah! Failure!
What have I done?
(laughs) Spectacular.
- [Audience Member] Debugging.
Debugging, yeah, yeah.
I don't know if we have
time for debugging,
but that would most
certainly be my next step.
- [Audience Member] The
delay's out of control.
Sorry?
- [Audience Member] The
delay is not in control.
Oh, true, but it should still work with
the switch on and off.
Let me just look at the builder.
Create task, GPIO, I see, yeah.
Okay, looks feasible.
So I'm gonna invoke the standard mantra
of the embedded systems
developer here and say
it must be a hardware problem.
(audience laughing)
I'm just gonna flip to system view
and we'll have a look at this on here,
see if we can see what's happening.
It might shed some light on this.
Okay, that task is
definitely not registered.
Okay, which leads me
neatly onto my next point
which is that just
because you're doing TDD
doesn't mean the debugger
and hardware goes away.
You're still gonna have to do that.
Unfortunately, I think
we're gonna be out of time,
to really diagnose this.
But, hey, there you go.
So there are...
I'd make a point about the
deploy and test on target,
which is that these kind of tools,
these kind of diagnostic tools
are immensely useful as well.
So you gotta look at kind
of the overall economics
of your build and
debugging test scenarios.
You can invest an awful lot of time
in writing tests and running the target,
when you might be able to do
a quick visual eyeball of this
by just pressing a couple of buttons,
capturing some telemetry.
And you've got a system that can be tested
in 30 seconds or 60 seconds,
then that's a pretty low investment
to get some insight into
whether your code's behaving.
So, sorry for our (chuckles)
failed demo there.
I'll have a look at that
in a bit more detail.
There's gotta be something
locking somewhere else
in the build.
So I'm just gonna refer back to slide,
while we wrap up a couple
of closing comments here.
Two. Okay.
So the points that we'd make is that
small increments make
for very small mistakes.
They make for sort of
containable mistakes,
like that one there, for example.
I'm only a few moves further
from the last commit.
Be careful about targeting
your testing target.
It can be a massive
investment of time and effort,
so pick the right test.
Pick the ones that give you most value.
And like everybody, RTOS create task there
adapt to fit your model, but
don't try and boil the ocean.
Don't try and build an all framework
for something that's just intrinsically
never gonna be all...
I see a few attempts of
that and doesn't really
look like a good policy.
And it's impeding you,
stop worrying about it.
Don't even stub it.
Unless it's failing to link,
just put it out your head.
And then we see that the
HAL is gonna be probably
your biggest barrier.
The thing that you use most frequently
is a hardware abstraction library.
So you might wanna think about
selectively refactoring
to something better.
Because we can get started very quickly
with vendor HALs.
It can become a real burden over time.
So refactor the things that
are causing the most pain.
And the CI build, bring
that right to the doorstep
of your device.
Be hands-off with this stuff.
And trust yourself to build the pipeline,
but don't trust yourself
to second guess it.
Like make call on whether that code
is good enough to put in the board.
And optimize for development
test on the host.
That's where you spend most of your time
is writing code, running
tests, writing tests,
so make that fast.
And sometimes that means that the vendors
really want to do this at the moment
and bring IDs and platforms
closer and closer together,
there's actually a case for teasing them
a little bit apart.
And where you have embedded
tools that can allow you
to test on the platform and
diagnose on the platform,
do that.
So play to the strengths
of the different tools.
There are different use cases.
Great books.
Really recommend James Greening's book,
fantastic read for your
look into see your C++.
One of the best books ever written
about test-drive
development, in my opinion.
Jeff Langer's book is an
excellent introduction
for TDD for C++.
And Michael Feathers seminal book
on Working Effectively with Legacy Code
is a fantastic read.
Should be on everyone's bookshelf.
That's it.
That you very much.
(audience applauding)
Questions?
- [Audience Member] Oh, hi.
Hi.
- [Audience Member] First
of all, thank you very much.
I think it's one talk this week
that I don't feel like an imposter--
(Mike laughs)
In these C++ conferences.
Looks like what I do every day.
So quick question.
So clearly having that whole setup
for your CI integration is fantastic.
How hard it is to set up all of those?
Mainly so you need to have a machine.
Yeah.
- [Audience Member] Hardware dedicated
hooked on the server that
gets flashed with the test
and then gets flashed with the--
Yeah.
So the best way to do this is to...
I run this locally all the time.
I use a Docker container
and I just have the Docker container
running on the workstation
that I'm working on.
So the nice thing about
Docker is portability.
So the same build that you
have running on the cloud,
pretty much, the cloud unfortunately
does not have a USB port,
so you can't plug in your JTAG debugger.
But the ability to do that extra step,
running locally on a workstation
I think is fantastic.
There is an investment.
It's probably not as big as you think.
If you looked at the kind of
first phase of that build,
virtually everything
I'm doing is recompiling
and rerunning the same tests.
So I'll compile and run the tests
in different configurations,
compile and run the sanitizers.
So you get a lot of value
back with just a little
tweak to tool change.
But start small.
I just start with build,
test, green light,
and then start to kind of
embellish on top of that.
- [Audience Member] Right,
so the actual TDD loop
inside of the, in the development machine,
that's fantastic and we
do that all the time.
Feels like...
That extra step, we
actually started doing this
and we started building
for running the test
on the platform and the target.
And that was a nightmare.
(Mike chuckles)
And was way too long, right?
Because your feedback needs to be quick.
You need to write, get that
green, write a new test,
refactor, whatever.
So that needs to be quick.
And if you have to flash
the target every time
it doesn't scale at all.
It does.
It drains the energy out, TDD.
It's not a good way to--
Exactly!
It's no longer a TDD.
It's a verification step.
And that's why I say that it's better
to think about what the tests are
that you want run in an
hardware and make it a subset
and make it targeted
and make it the things
that are gonna give you the biggest value
from running on the hardware
rather than on the host.
- [Audience Member] Last question
and I think it's still related.
Works fine to have all those step,
to commit it in a half,
but it's a tiny program.
Doesn't scale, it seemed to
me, when compiling takes,
just one compilation, takes
two minutes (mumbles).
Yeah, and the bigger the system,
the more you might wanna push
the majority of your build in the cloud.
The only things you'll really need
is some agent running locally
that at the right point in your pipeline
is gonna say, &quot;Oh, I
have some work to do now.
&quot;And I'm gonna deploy to
the target eval board.&quot;
So that's something you do as
a divide and conquer thing.
So you're gonna have the
majority of your build
running on a really high performance kit
to get speed for the sanitizers
and then some local build agents
that are just gonna do the
things that they need to do
with sort of physical hardware.
Takes a bit of orchestration,
usually networking
and security issues,
(laughs) everything else,
the usual gamut of problems.
But if you really do find
your builds are slow,
then you can sort of divide
and conquer that way.
- [Audience Member] All
right, thank you so much.
I guess I would like to continue the talk.
This is fascinating--
Yeah, absolutely,
come and have a chat.
- [Audience Member] All right, thanks.
Any other questions?
Yeah?
- [Audience Member] Yeah, I had a comment
on the code in the live coding session.
You were passing in references
and then saving them as private members.
And the references were
actually referencing things
that were of, you know,
global storage duration.
You could've been using
non-type template parameters
and save some space in the object.
Right, sure, yeah, yeah.
- [Audience Member] And I'd
also like to comment that
there aren't very many embedded talks
where I don't want to heckle.
But this was a good one.
(audience laughing)
Okay, good to know.
Thanks for that.
I'll take that as an endorsement.
Any other questions?
Okay. Cool.
Come and give me a shout if you want to.
Thank you.
(audience applauding)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>