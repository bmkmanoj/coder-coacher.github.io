<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2014: Pablo Halpern &quot;Decomposing a Problem for Parallel Execution&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2014: Pablo Halpern &quot;Decomposing a Problem for Parallel Execution&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CppCon 2014: Pablo Halpern &quot;Decomposing a Problem for Parallel Execution&quot;</b></h2><h5 class="post__date">2014-10-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ej97699t-G0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello my name is Pablo Halpern I am a
parallel architect at Intel which means
I kind of coordinate the development of
parallel languages many groups at Intel
are going to be talking today about
decomposing a problem for parallel
execution so how do you break things
down so you can run multiple parts in
parallel like that remember this thing
doesn't doesn't click our goal is to
sort of learn how to break down a
problem so that you can run the bit
different parts in parallel I'm going to
be moving pretty quickly so if you have
a question ideally get to the mic but at
least make some signal line me to pause
okay but ask the questions as we go
along it's fine okay so we're going to
be looking at two problems and kind of
diving into the actual code of how you
paralyze these problems the first one is
a pretty simple one that's counting the
number of stars in an image it's it's
pretty easy but it does expose certain
issues they're best described in an easy
situation and then we go into a little
bit more involved problems and bodies
problems and show that you can get only
so far with certain techniques and then
you have to like rethink some things and
maybe restructure the certain parts you
recovered so the last bit is is very
cool the restructured stuff so make sure
you stay to the end all right some
principles when we're parallelizing code
the main challenge is to find the tasks
that can execute without dependencies on
each other or with minimal interaction
with each other the the process should
be fun it's kind of this new dimension
on programming where you get to apply
your creativity to you know get get the
problem solved
there's a lot of measuring there's a lot
of testing iterating over to prop the
process let me talk a little bit about
what parallelism is how its
how it's expressed mathematically this
is a graph of the dependencies or the
flow really among different tasks in a
program but what each of these bubbles
represents is a task and if two bubbles
are connected by an arrow it means the
first one must precede the second one so
a precedes B and mathematically we write
it this way this thing's not a less than
symbol this is a special symbol that
means proceeds and if those of you how
many people worked in my earlier talk
yesterday a lot of you so this wouldn't
be new to you but I'll just very quickly
go through it for for everybody else
the this graph also tells us that F and
B are in parallel with each other that's
what this symbol means and that K is in
parallel with H and and K is also
imperil with B and C and so on you can
see that from the graph that there's no
forward arrows connecting a I'm sorry
connecting B decay and therefore they're
in parallel I'm going to be using silk
plus as the language for teaching this
so plus is a particularly easy notation
to to learn and it is it's available in
the GCC compiler in a branch of clang
it's available in the Intel compiler
it's not the only language but it's it's
the one I'm using for fusion here just
to understand what's going on here what
we have here the recursive Fibonacci
calculation the worst way possible to
compute Fibonacci and the spawn says you
can continue to execute the thing that
follows the spawn without waiting for
the spawn to this spawn function to
complete all right so this creates a
past and it says these two tasks can
execute in parallel the sync says we're
done with parallelism there anything
spawned within the same function must be
done before we continue here simple
fork/join there's an implicit sync at
the end of the function for what for
the pair of the Silk 4 is another
fork/join construct it says everything
inside the floor is done simultaneously
sorry in parallel I never make that
mistake in parallel does not mean
simultaneous necessarily so the
iterations of the four are all in
parallel with each other which means
they are least the programmer is
asserting that they are independent of
each other and can be executed in any
order and simultaneously if there's
resources to do it simultaneously
another site highlighted allowed in when
you're writing parallel programs what
you're trying to do is annotate where
are things logically parallel it has
been up to the runtime environment to
figure out what can actually happen
simultaneously if you've got a thousand
CPUs a lot more will happen
simultaneously than if you've got two
the idea is you do not create a thousand
threads on a two CPU system you create
you don't create any thread you let the
runtime system create as many threads as
make sense which on the two CPU system
might be too like beef more than two
might be two might be one and execute
things as resources allow so just
because things are notated as being in
parallel does not mean they're going to
run simultaneously it's a load balancing
thing in the runtime knows best okay so
let's get to our first example we've
gotten through the preliminaries there's
star counting problem we're going to
talk about the what the problem is we're
going to look at how you would implement
it serially and then we're gonna we're
gonna implement it in parallel fix a
race condition that we've introduced and
then we're gonna find a better way to
express the loop this is the problem
count the number of stars in this image
this is a real Hubble telescope image of
stars how would you go about this well
the most straightforward way is to write
a loop that says let's go over every
pixel and if that pixel represents the
middle of a star
then if it's Center of star then we
increment the count now this how it can
be neatly left out the details of how we
figure out if it's in the middle of a
star that's not my problem okay
not my Larry of expertise suffice to say
that it would require seeing not only
the pixel in question X&amp;amp;Y but probably
all the pixels around it at least nine
pixels and probably more every time we
find one pixel that is the center of a
star we say all right will increase our
increase our count at the end we return
the total count all right so just note
the disclaimer here I haven't actually
written this code hey I would have had
went if I had more time they've had more
time i probably would've written a
bigger talk yeah okay
so now let's take a look at this
algorithm and see where is the
parallelism well what you want to find
is what can be happen independently and
when you see a loop you immediately
think can the iterations be executed in
parallel well finding the center of the
star is almost certainly a completely
independent operation because you're
reading the pixels you're not even
modifying them so you should be able to
check all the pixels at the same time if
you have enough compute resources right
increasing the count while counting is
independent you can you can count on
multiple CPUs at once but the problem is
updating the single variable so when we
tried parallel eyes this in the most
obvious way which is to use these silk
four loops to make these loops in
parallel it almost works but we end up
with the problem of parallel updates to
a single value okay and that's a race
condition so a race condition is defined
as multiple parallel or concurrent
strands accessing the same memory
location where at least one of them is a
right and in this case they're all
rights in fact they're all
read-modify-write set so that increment
is all right so we have a correctness
problem okay data races and then
determine the sea races in general which
is a gender
of the idea of data race is one of those
real correctness bugs one of the few
real correctness bugs in parallel
programs there's a lot of things that
will interfere with good getting good
performance data races interfere with
getting the right answers so you want to
fix those pronto
so our first solution which some of you
may have already thought of is how about
we use atomic variables
Tomic variables can be incremented in
more than one thread at the same time so
having them executed in parallel would
seem to work and it does it this will
actually do the job the problem with
with these variables is that they you
have a contingent problem on the shared
variable the contingent problem is not a
if you have a big black area with you
know a few dozen stars you won't notice
the contention if you have the picture
that we just showed you'll probably
notice the contention and the more CPUs
you have the more contention you'll
notice why what's going on keep clicking
this thinking is going to advance this
is what's happening when you access an
atomic variable from more than one
thread and especially if you have if
it's happening frequently okay
what we're doing here is each thread is
on separate pixels computing is at the
center of the star if it is the center
the increments the count then it goes
back and says is it the center of the
star and if it is in crunch the count
okay it's doing this over and over again
another CPU is doing the same set of
operations okay I'm different pixels the
problem is that when this guy wants to
increment the count he has to say wait
this is mine there is an invariant that
says a mod a location can be in one l1
cache in modifiable state at a time so
you've got two CPUs here
it cannot be modifiable count cannot be
modified not modifiable in both CPUs at
sitting
hi so what happens is every time one of
them modifies it it has to send a
message by a couple of wires on the on
the integrated circuit if you're lucky
or it goes to the actual circuit board
if you're on separate sockets to
communicate hey if you have this in your
cache you have to invalidate it so every
time one of these guys increments count
it invalidates a cache line in the other
guy and every time he wants to get it
again he's got to read it back from main
memory or from whatever the most common
shared cache is and and wait until it's
there okay there's a latency involve
reading it again from memory you've got
two two forms of slow down here one is
the actual inter CPU communication and
then the other is the latency on there
on the rereads okay so the question is
and in the future people please queue up
on the microphones if you want so the
question is what happens in Hardware if
if they both want to do it at exactly
the same time well that's a hardware
problem we're in software group right
here but I know that's a glib answer so
I don't know at the very very detail
level I could say that nothing ever
hacked exactly happens at the same time
in the hardware the hardware really has
these it has specific circuitry to
randomly pick a winner when there's
actual two signals arrive effectively
simultaneously it's not complicated
circuitry I don't think but it's there
something going on there one of them
okay yeah right right that's well I like
the answer don't worry about it okay so
that's yeah okay so that this is why
Atomics are not a panacea why they don't
solve all your problems there's there's
two things that's going on here one is
that even if you are the only CPU
accessing the atomic you're still
sending out these invalidate signals
just in case some other CPU has the
cache line and then there's these
latency when there really is contention
under variable and the contention is
we're really cozy if you have a lot of
contention
okay so Atomics are not always the
winners all right a better solution is
to use a reduction operation this is the
syntax that we use in silk plus would
create a variable that is called the
reducer type variable and it has a
pointer like syntax it holds it
effectively holds a value it looks like
it holds the value anyway and you access
it using this pointer like interface and
so what we're incrementing is the thing
that we get from the referencing count R
which is not a shared variable the thing
you get from incrementing the reducer is
not a shared variable I'll get to what's
exactly happening in a second so the
important thing is there's no cash
ping-pong going on here each separate
thread is accessing its own memory and
at the end everything's combined
together into a single count so to get a
graphical view of what I just said we
have the our same calculation here but
now what instead of incrementing the
account work count we're incrementing a
view of the count in one thread and the
separate view of the count in the other
thread this is all done through the
reducer abstraction which says we don't
care from threads we're writing parallel
programs threads are something that
someone writing producer-consumer or UI
stuff worries about in parallelism you
say
the parallelism I don't know how many
threads I'm actually gonna get but the
runtime says these are separate threads
so the runtime figures out how many
copies of count I need to make okay so
because they're incrementing their own
there's not this invalidation thing it's
not an atomic operation and the count
never moves from its cache line it just
stays in whatever CP you happen to have
it last at the very end we compute the
count here all right what is this why
did I bother writing it out this way all
right it's pretty clear that the answer
is correct right we've implemented this
thing five times and this adds up to
five why did I write it this way well
what is the importance of zero in this
case can anybody tell me John it's the
it's the base case the initialization
and y0 it's it's some neutral values
it's the identity for addition okay
if the operation that we were reducing
on were say multiplication we would
start with a 1 but we're not reducing on
multiplication we're reducing on
addition and so 0 is a special number
for addition 1 is a special number for
it
for multiplication all one's is a
special value for bitwise and and a 0 is
a special case for what azores you get
the idea there the collection of an
identity and the operation is called a a
monoid in mathematics and I won't get
into it but the important thing is an
associative operation so here you see
the friends here and it has an identity
and they start out each counter starts
out with the identity value and then we
perform an operation a single operation
repeatedly I say single operation the
operation here is addition it's
represented as increment but increment
as a form of addition ok so this just
shows because they can group this
different ways then
number of times that one thread does
this operation versus a different thread
is irrelevant you'll still get the same
answer so the load balancing allows you
to move things around and you'll still
get the same answer what you don't get
is a correct answer half way through
this thing does not give us a total
count as if it were a serial program so
we're taking advantage of the fact that
we don't need the intermediate result
and then the final result is going to be
correct okay take a breath here that was
the easy problem let's look at now and a
little bit more involved problem with
what we'd already look at the end body
problem we're gonna again we're gonna
introduce the problem we're gonna
paralyze it with loops I actually think
this is important I wanted to ask you it
looks the way we just described the
problem that given that we've paralyzed
it we're willing to do something maybe
much less efficient per separate
parallel path like we're gonna go over
each pixel and we're gonna see a blob of
pixels and for each pixel we're going to
do some calculation like 5 is at the
center and we might be that calculation
an enormous number of times but because
we have let's say a very large number of
different processors we don't care that
we being someone inefficient as long as
we can parallelize it is there a
trade-off but you see there is sometimes
a trade-off that's not what I'm trying
to illustrate here and in fact I will
illustrate something kind of like that
later I the reason I chose this
particular algorithm is because it's
really dead simple to understand ok but
it is a it is a fair point that if
you're looking if each pixel calculation
is looking at all the pixels around it
then it wouldn't be more efficient to
sort of go left to right top to bottom
or go through the diagonal or something
like that and it may very well be and
later on we'll show it's a situation
where you could actually take advantage
of that Thanks ok
so the end body problem we're gonna
describe the problem parallelize it and
then we're gonna see we're going to sort
of go through a bunch of strawman
attempts to make the parallel program
work right and work efficiently and then
we're going to so
try a different algorithm that gives you
the very elegant way to get around the
number of the problems we'll run into so
what is the N value problem the
unbounded problem is anytime you have
some number of objects and objects and a
computation needs to be done between
each object and each of the other
objects in that set okay and by so the
classic and body problem is planetary
body or you know celestial bodies where
each one of them exerts a gravitational
force on each of the others and you
compute the gravity between them with
this first Formula which should be
familiar to anybody who's taken high
school physics and who remembers any of
it
so this is our gravitation law it's
really it's proportional to the two
masses and inversely proportional to the
distance to the square of the distance
between them okay so this is the force
between any two bodies the force on any
one body is the sum of the forces
between it and every other body that's
our end bodies okay our new velocity if
we if we know the velocity and object
and we know the force on it we can
calculate the new velocity at Delta P
later using this formula here okay which
is this F over m is our acceleration
times the change interval time interval
that were computing on and the change in
their distance the new distance from the
old distance the average velocity in
that interval times the interval okay if
you didn't understand all that math and
don't worry about it
the important thing is it's it's correct
okay what we're going to be doing is
we're going to be computing this
computation performing this computation
for every pair I and J where I am J our
numbers are elements of our set of
bodies okay and then we're going to
repeat it for multiple time steps so
that we can get kind of a you know
progression of movements okay so here's
our illustration that what's going on
every body has some force on it that's
the capital F which is the sum of the
forces applied by all of the other
bodies and here we have
four bodies but the idea is to be able
to calculate for much many more bodies
it has a velocity and there's a change
in the velocity based on that force so
it's gonna want to go this m2 is gonna
want to go off to the left of it all
right ventually you know it wants to
orbit m1 kind of okay what we have here
is a little video of what happened we
compute four thousand time steps and
each time step every forty time steps we
create a new frame here so this is just
100 frames total it's not a real time
calculation this is not intended to
after all if this were real time it
would take years right because these are
supposedly planets and they take years
to go around their son okay or refract
at least a good fraction of a year but
the idea is anyway that you know we
generated a bunch of bodies and put them
in semi at random give them all kind of
a initial motion roughly clockwise and
let's go ahead this is a particularly
dumbed down version of the n-body
problem because a it's 2d in real life
in reality if it's gravitation it would
be at least three dimensions right we're
not taking into account collisions the
bodies just kind of passed through each
other convenient right and which also is
what makes this a fun problem to work
with because once you've gotten it and
you can start playing with things like
what happens if I put some inelastic or
partially elastic collisions in there
and so on okay so let's look at the
general framework here's our our body
structure you don't have to memorize it
but what's important here is that we
have the position and velocity and force
and mass of each object the density and
Colour are really there just for display
purposes notice that there's an x and y
for each of them that's because of
course position is a two dimensional
position and velocity and force are two
dimensional vectors vectors in the
mathematical and physics sense of the
word not in terms of STL vectors so
everything is compute has to be computed
on both x and y
our main loop is going to go through and
say well how many frames we want to
generate and then for each frame how
many steps per frame like in the
previous animation there were 40 steps
before we generate a new frame if we
didn't do those steps there would be so
little motion within a frame that you
you wouldn't be barely perceptible you
know some a lot of bodies wouldn't even
move a single pixel and then so for each
step we calculate the forces and then we
update the positions based on those
forces okay we have to do these two as
two separate steps one after the other
because we have to we if we started
moving the bodies before we calculated
the forces with the other bodies it
would smudge the overall calculation so
we need to calculate all the forces at
once and then apply those forces as a
separate step okay the central
computations here calculate force this
is basically the C++ code that does this
formula should be pretty straightforward
this is our G we got our masses and so
on I don't think there's anything
special anything I point out that we
compute the distance squared very easily
and we hold on to it before we get the
square root because over here we use the
distance squared so it'd be silly to
take the square root and square it again
otherwise it's a straightforward
calculation addforce is even more
straightforward takes the force vector
and a and adds it to the current force
the current force vector okay so every
time we call add force we're
contributing to this so giving them time
okay okay so we're going to start with
the easier of the two parts remember
there were two steps the two steps two
parallel eyes first once we know the
forces how to update the positions we do
this set of calculation and if X is
shaded here you know corresponds to this
formula updating each of the velocities
based on the force that we calculated in
the previous step which we have yet to
get
- and the this part shaded here
corresponds to updating the positions
once the velocity has been computed all
right so this this here is our average
velocity thing take the initial velocity
and the next ement and the new velocity
and average them okay so how do we
parallelize this well we change that
loop from a serial loop to a parallel
loop and we're done okay
that was easy right just so they'd show
that I'm not chauvinistic about it this
is how you do the same thing in TBB and
in open and P okay simple parallel loop
all right now the fun part is
calculating the forces in parallel again
we'll start with a naive fort now I
should mention something about this
slide there's a error in this slide it's
a grammatical error John you've got to
you've got to find it it's got to be
here right English doesn't have that
symbol Thank You autocorrect from
Microsoft Office yes
it decides that naive comes from a
different language and we're gonna spell
it that way so Who am I to argue okay so
to calculate the forces we simply have a
pair of nested loops saying let's go
through each of the bodies and then for
each of the bodies we go through each of
the other bodies all right we go through
each of the bodies sort of twice right
or it's a two-dimensional traversal of a
one dimensional array is what we're
doing here and if they're the same that
we skip that so we skip the Bagnall
because a body doesn't interact with
itself and then we calculate we just
call calculate force and add the force
to our running sum for that body
okay
all right I'm glad you're on the wall
okay hmm
so we're good but there's a there's
something kind of silly about this if
you know the physics you know that you
know the action and reaction for every
action there's an equal and opposite
reaction thing right we calculate the
force that that body is sorry the body J
is exerting on body I and adding it here
but you know that same force is being
exerted from by body I on body J so why
are we computing it for I here and then
sometime later we're going to compute it
for the opposite one so here's a better
implementation we're still cereal here
it says well once we computed the force
in one direction we'll just take the
opposite force and apply it in the other
direction
very good what will you do that simply
by saying we're just going to start
start iterating from I plus 1 so we
don't cover the ones that we've already
computed and what we're doing is if you
want to look at graphically is here's I
and J we're computing this upper left
triangle of the total iteration space
okay and back from our middle school
geometry the dotted line means it's not
included ok so just the shaded part good
so we've cut the effective work in half
almost in half why because the thing
that we're there calculation that we're
saving is the most expensive calculation
in the entire program which is the
square root okay and the there's a
couple of multiplications in there as
well so that's where all the computation
is done it's not a lot of computation
but it's basically all there is so now
what happens when we go parallel
well we stains the outer loop to a
parallel loop now we could have changed
them both to parallel loops and that
would at
increase the amount of parallelism but
probably beyond the resources we
actually have to take advantage of it
and it would make the problem about to
point out worse so given that we're
about to find a problem
let's not aggravate it in advance okay
so we made the outer loop parallel so
we're calculating calculate force which
is our expensive
calculation and add force in parallel
what's happening if we look at this as a
directed acyclic graph we start before
we enter the loop the outer loop is
shown as these parallel strands okay so
we're doing the calculate force for I
equals 0 in parallel with the 1 for I
equals 1 and then in cereal cereal e
within each iteration of the parallel
loop we are calculating for J is I plus
1 and J is equal I plus 2 and so on
anybody see a problem with this yes
cache coherency it gets worse than that
the inner loop goes smaller and smaller
you know what that's not actually a
problem because the load balancing is
going to be automatic so what's going to
happen is the the one CPU will simply
execute will get done faster and then
it'll start on other iterations of the
loop so that's not really a problem
anybody else
there's a race thank you very much
notice that we're adding to the element
one here and here and here these two do
not race with each other but they both
race with this one they're taking place
in two parallel strands they could
happen concurrently so that's a race
there's another race here just showing
on this slide which is B between this
one and that one okay but really there
are there are close to n races here lots
of races all right so how do we get rid
of the race well let's try the first the
same solutions that we tried for the the
stars problem
but first let's try a mutex right
anybody stays a race anybody from
current current from the concurrency
world knows that if you see a race the
first thing you do is go to the shelf
and you grab a mutex well we could do
that we could embed a mutex here and it
better be pretty small because we have
thousands of bodies potentially right so
we'll do this thing that's called a
small mutex and you know we're doing
very little work in a critical section
so we'll just use like spin lock which
is efficient enough for our purposes
generally and and then within the actual
computation we'll grab a lock on an eye
and then we'll do our add force and then
grab it on J into our add force this
this will work this will fix the
correctness problem that the race bus
but you should know that mutexes are not
cheap even a spin lock has that same
problem of the bouncing cache line as
the as we did with the race in the case
of another race and sorry the atomic in
the case of the
of the start counting okay so spin lock
is not a panacea it's got the same
problem and it's got the same area where
it works which is to say when the
contention is low enough it's fine
okay we mentioned the comics why not use
Atomics well here's a straightforward
attempt to try and I'm sorry today I'm
sorry I left out a step here the
previous example showed a mutex per body
and I said that could get kind of
expensive even with the Muta even with
the small mutex here what if we wanted
to reduce the total number of mutexes
because of memory pressure so we could
actually do this and this is a technique
that is used more in concurrent programs
and in parallel program to have some
limited number of youth of mutexes for a
very large data set and that is that we
create here just 64 mutexes and that's a
static so there's just one array of 64
mutexes and then each each bodies mutex
is effectively a hash into that array
now does that mean now each body doesn't
have its own mutex is that a problem a
little bit when more than one body
shares the mutex you get more contention
than you would otherwise but it but it's
what it is there it is correct you'll
get correct behavior from the program
you'll just be locking mutexes more
often than you want to or hitting a lock
even when there's not a real collision
okay and so this is the same program
here basically but using the mutex that
we get as a member from the member
function interesting technique useful
not great in this situation Atomics so
our first attempt at Atomics don't work
doesn't work because then we replaced
our force vector with Atomics we can't
add to it atomic Li because there's no
such thing as plus equals four atomic
float and the reason there isn't such a
thing is because most hardware I don't
fact I don't know of any hardware
supports it right they supporting atomic
addition but not a comma condition on
integers but not atomic increment of
floating-point values
so that's not gonna work so you end up
by everything to write something
significantly more complicated and
believe me I took more time than I
should have trying to get this right
okay and I and I don't have proof that I
got it right because I did not actually
compile in testing so this is comparing
exchange week but if I got it right it's
not that bad but there's a loop involved
okay so you can imagine potential if
there's a lot of contention there's a
performance hit to be taken there and
you still have that ping ponging cache
line problem we were talking about
before now here's a counterintuitive
solution how about we undo that
optimization we made earlier and compute
only the add add that do the add force
only to the I dimension and do the full
traversal here's a full rectangular
traversal or square traversal of the
traversal space instead of that
triangular thing we did before this will
solve the race because for each parallel
iteration of along the I dimension only
one body is being updated okay no - no -
iterations are updating the same body
that's that's actually correct way to
say what I'm trying to say mmm good so
it tells the race but what about
doubling the work well what's doubling
the work when you're saving cash ping
pong the answer is I don't know you
actually have to measure it this may
actually be faster than using a comics
or mutexes even though we doubled work
now obviously if you have fewer than two
cores yeah yeah I mean if you would in
two cores and if you have only one core
which is another way of saying fewer
than two obviously this is going to be
worse than the serial program if you
have two cores it's still gonna be worse
because you're doing twice the work but
you're paying some overhead for the four
just for notating it is parallel not a
lot of overhead but some at three cores
you're probably doing better than break
even and if it's the best way to write
the thing then maybe if you've got a
thousand cores
this is the
do it remember their thousand cores not
only are can you absorb that you know
that factor of two loss to some degree
because they're still getting a 500x
speed-up but cash ping-pong is much
worse with a thousand cores than it is
with two so it may depend on how big
your machine is whether this is the more
the the better or worse approach okay
but is this the best we can do well at
this point let's take a breath and
remember why we're here now okay and
let's look a different way of solving
this problem of solving the race problem
and let's see if we can even get our
factor of two speed-up back okay let's
take a let's take a little break from
the actual program at hand and look at a
general problem of cache locality here
what we have is a pair of nested loops
it's part of the n-body traversal and
where we're doing we're doing the two
dimensional traversal is it's all before
so when you have a specific I we're
going through the array for Jing okay so
the first J we see is zero until we read
body zero by zero gets moved from main
memory into the cache for the l1 cache
for the CPU okay it just so happens that
the body structure is just about one
full cache line back the way actually
defined it's even bigger than the cache
line which is pretty horrible but so so
it's gonna let's say it's one cache line
so this takes up one cache line and then
when J is 2 is 1
it reads 1 for the cache line then 2
then 3 and so on we get up to a thousand
and now our cache is full
so what happens when we get to a
thousand one is we get to evict
something from the cache and there goes
element zero and then so we put 101
where zero used to be one thousand sorry
one thousand I think I'm off by one I
know a thousand here and a thousand one
right that hasn't one where a zero to be
and then when we want to read the
thousand two in we have to evict
something else and it's more or less the
least recently used so there goes
element number one okay and now this is
our cache at the end of the first
iteration but now we're immediately
going to start at the beginning again
with I equals one right with the next
version with the next value for I and
we're gonna start reading jay-z and
again wait a second
zero wasn't our cache just not a second
ago but now it's gone and we have to
read it from main memory again and
that's really slow like takes
nanoseconds so there again we're we're
we're evicting something that we're
about to read which is really fun right
that's great
and read in something that in pilla a
short period of time was available to us
so what's happening is the cache is
really it's it's like not doing any good
at all right and the pattern here that
you should learn to recognize is that we
are running through memory we are going
sequentially through a large enough
chunk of memory that by the time we get
to the end the stuff at the beginning is
a long from forgotten from the cache why
is this a specialist issue for
parallelism well it's a problem for
serial programs for sure because the
cpus are faster than memory but two CPUs
are even that much more faster than
memory you still have this kind of
little narrow straw between the two CPUs
and our memory so the more CPUs you got
especially on a single socket the more
this cache thrashing
hurts you let's go back to our stars
program for a moment where we go back to
our end bodies and say well the cash
problem affects the Stars - can't
remember that we're seeing all the
pixels around so if we saw one pixel and
then after going through all the row we
come back to a pixel that we already saw
it would be nice and we're still in cash
right so the way to solve this problem
is we'll divide this into tiles and you
know what a pile is simply is just you
know a chunk of our traversal space it
could be 2-dimensional it could be
3-dimensional that or it could be n
dimensional really depend on what your
traversal space looks like we're all all
of the memory that you are reading in
for that tile will fit in cache that's
your ideal tile size if you can't figure
out your ideal pile size go for a small
tile because that your short will fit in
cache that's the idea so now if we were
to traverse this what we would do is we
would traverse a tile at a time so we're
incrementing my tile size here and then
within the tile we do the same old
serial algorithm we were doing before
okay well we're traversing in parallel
the tiles so we're saying we're gonna do
this tile in parallel with that pile
over there
but within the tiles we don't need to go
parallel because we have enough
parallelism just doing all the tiles we
could do Terrell parallel within the
file the problem with doing good
parallel within the tile is now you've
got different CPUs which means different
caches and it's not clear you'll get any
win at all okay so that's tiling in the
simple case can we do something with the
n-body problem
okay can we do it and can we get our
triangular traversal back that would be
nice too right
the answer is yeah we can but not quite
in the same way as we did with the start
we can divide this into piles but the
problem is that the tiles aren't
independent at least not in this simple
grid
these two have the same range of AI
values and we know that when the I
values overlap you get racist so we need
a different way how about we tile it in
this kind of recursive way what we have
here is our triangular traversal and
we've divided it into three big tiles in
order for two tiles to be independent to
begin parallel with each other this is
the condition that has to hold the I
range of one has to not overlap the I or
J range of the other okay because we're
updating both the I and the J elements
so I can't overlap either of the ranges
and that also means that this by trip I
if you transitively apply this it means
the J Iranian one can't overlap the I or
J range of the other as well okay
doesn't even have to be stated because
it's automatically implied by this
statement okay so in this case the
triangles are in parallel because they
have this joint I and J ranges but they
are not in parallel with the square or
the rectangular he's here because they
have overlapping I and J Ranger okay
given the time constraints you may have
to you know I it took me sitting you
know at least two minutes to completely
absorb that but trust me these these
don't overlap and these do overlap
now let's recursively apply the process
of tiling at the next level down so we
take each triangle and we divide it into
the same three tiles that we did before
but now each of the tiles is smaller
right and again these two are in powell
and they're not in parallel with the
square and but now we've also subdivided
our square into four squares or are it's
really a rectangle it's almost square
but there's a divide by two round off
that happens so that one may be one
larger than the other
even when dimension or both dimensions
so in this case the formulation is still
the same they have to have non
overlapping I and J ranges and it just
so happens that for anything in this
quadrant
the eye range will not overlap the J
range so you can forget about having
that problem and now you only have to
worry about the eye range overlapping
the eye range or the J range with the
day range and as a result you discover
that these two diagonals are in parallel
they don't overlap and these two
diagonals are in parallel they don't
overlap so you can do two in parallel
sync and do the other two in parallel
okay same thing with the triangle you do
two in parallel sync and then do this
one and because it's recursive remember
that when you do this one it would even
look like one big square but it's going
to be recursively subdivided so this is
what the algorithm looks like I'm going
to explain cache-oblivious in a second
what that means
but the algorithm looks like this this
will start with a triangle so the
outermost thing calculate forces is
simply calculate the forces for this
triangle so it's just a single call to
our little triangle which is a recursive
routine for triangles and what's
happening here is we're dividing the
range in half and spawning a triangle to
do one half and the triangle to the
other so these two run in parallel the
two triangles then we sink we have to
sink so that neither of these triangles
are still being computed when we get
down to competing in a rectangle I'm
really clear on that it's really
important that that sink point be there
so that you don't have parallelism where
it doesn't belong the square is kind of
the same thing it looks like a lot more
code but it's actually you know it's the
same concept concept which is we
calculate we divide their ranges in half
and we calculate the two a a squares and
do them in parallel then we sink and
then we do the to be squares in parallel
again because the a and the B cannot be
in parallel with each other we have to
sync now we also have a base case at
some point we get down to a single a
single body right and at that point we
calculate
yeah a single now a single body a single
I and J and at that point we compute the
forces between body I and body J and
store it and this is not gonna happen in
parallel with anything else it'll happen
in parallel with updates on different
bodies but it will not happen on in
parallel with updates the same body
because of the way we've recursed down
the other thing so what does it mean
that this is cache-oblivious well we've
we're doing recursion and in each level
of recursion we have a smaller and
smaller data set until our data set ends
up as being a single point in our two
dimensional space that's gonna fit in
cache okay in addition the next n levels
up of recursion will also fit in cache
and so once you have crossed that
threshold into being small enough to fit
in cache it all goes very fast and but
notice that nowhere in this code did we
actually specify the cache size it's
oblivious to the cache size and that's
what the term cache oblivious mean cache
oblivious algorithms are useful in both
serial and parallel programming but as I
mentioned cache effects are magnified
when you have multiple cores so you want
to use it when you can question good
point
all right so the question was when
you're below the cache when you're
within a single cache line or a cat I'm
sorry not a single cache line with when
you're small in this data set that's
small enough to fit in cache then all of
this spawning and sinking and recursion
is wasted extra extra work right you
could just you could it's not it's not
completely clear because depending on
how many CPUs you have you may still
have resources left to exploit so you
may want to continue recursing down and
sinking but at a certain point you will
you will have so many so many greens
going on the same time that you you have
more parallelism than you f CPUs
and you need more parallelism they have
CPUs now I put a load balancer to do its
job but once you get more than left out
about 10 times as much parallelism as
you have CPUs then any additional
parallelism is unnecessary and I don't
know if I mentioned it earlier but
spawns spawns are cheap the idea is you
want to be able to put spawns in your
program as often as possible
so that if you go from your lowly to
Core circa you know 2010 laptop into you
know a 15 core server somewhere it'll
get that speed up automatically right so
you want to put more parallelism is in
then you can exploit but it's not free
even though it's cheap they're not free
so there is something to be said we're
trying to cut back on the number of
spawns to through degree once you know
that you saturate into parallelism and
similarly recursion is a bit more
expensive than iterating and so even if
it were serial recursion you would like
to avoid it and switch so that was a
great question because my next slide
addresses it all right there was a
question back here though okay great so
and that is what we've done here is
we've done a process called coarsening
where at a certain point and then we've
specified our threshold here at 16 but
the threshold is best determined by
experimentation that once we have once
we have gotten below our threshold and
then the size of the rectangle or
triangle that we are traversing is below
our threshold we switch to serial not
only have we switched the serial but
we've switched to non recursive serial
we switched to a loop okay so this is
this is our old serial program rewritten
down but only for the smallest tile in
our set question
no they apply today to the sink ah good
right so here's the question that's a
good question right so what is the scope
of the spawn of a sink I'm sorry the
sink applies to any spawns in flight
within this function so if there are
bigger spawns ahead of here like this
entire routine is probably has been
spawned okay all but the first iteration
up yeah all all but the first all but
the first invocation of this routine is
spawn a that whatever has happening in
parallel with this continues to be in
parallel with this the sink does not
stop that it's completely modular that's
one of the design goals of so plus and
of many of the other parallel systems of
the god
it's in the case of silk it's the whole
function that that's correct that's
correct it's not important to this it's
not important to this example look we're
staying within the same scope but it is
worth noting that that if I had put
spawn up here this sink would have
synced with it really there's a couple
of places where it's in silk where a new
task pass block is created there's a
term for the pass block so there's
there's three task blocks in silk plus a
function body is a passed block a try
block is a pass block and the body of a
silk floor is a task block so within
each of those the sinks are strictly
limited to the things that happen within
that pass block okay the function is the
one that we normally think about and
that's important because you need to be
able to do a spawn in a conditional
without it having automatically sinking
at the end of the if write and things
like that
okay good questions about silk but the
point here is about the parallelism in
general so we have here is coarsening
which is the final optimization that
we're doing today on this code alright
and this is quite fast it is very
parallel it'll saturate your CPUs and
use them very efficiently can we do
better better can we do more well you
can always do more right I mean pretty
much one really important optimization
we've we did not attempt was to
restructure our data structure the data
structure of a structure containing a
bunch of doubles and in a rate an array
of those is very easy to understand
structure but it's terrible for
vectorization what the vectorization
would like to see is a struct containing
a bunch of arrays instead of an array
containing a bunch of structs okay
there's an add there's acronyms for that
too SOA and AOS
array of structure structure of array
okay and vectorization which is this
ability to execute one instruction on
multiple pieces of data at once likes to
see all the data contiguous in memory so
if your exit if you're operating on FX
within a single body you would like all
the FX's to be you know the multiple
body I'm sorry multiple values you want
to all be contiguous in memory rather
than in the middle of a struct in the
middle of the next struct and then we
live the next track okay vectorization
can do that but it's much slower
that's called us that's a gather
operation as opposed to a contiguous
load so that's one big optimization we
left out here and we could have gone
into it but I don't have time to either
write the slides or present them to
really figure out what you need to do to
make a parallel program run best you got
to measure it you got to measure measure
measure ya figure out where is the parts
where the program is slow in the serial
case there's no point in parallelizing
part that's not taking up much time
where are the bottlenecks in the
parallel case you need to use some time
sophisticated tools like Intel has be
tuned amplifier which actually can read
the special registers on the CPU that
measure how often did you get cache miss
you know and other kinds of things like
that that are really hard to judge
sometimes your program is like it's
running full tilt the CPUs or a pin to
the 100% and you're getting lousy
performance what's going on it could be
a memory bandwidth problem and you can
sometimes figure it out by reasoning
through it the tool will get you there
much faster just like you can use printf
or you can use a debugger and that
bugger will usually get you there faster
the other thing about measuring is you
can do some guesswork you can even be
pretty darn sure that you've got the
fastest thing and you could be wrong
okay so the whole thing about using the
Atomics or should we just double the
amount of work we're doing so we can
avoid the atomic crash entirely I don't
know which is faster and in fact the
answer may depend on what hardware I'm
running on and how big my data set is
things like that
so parallelisms about decomposing a
problem it requires some creativity so
it's kind of this really is a mixture of
art and science there are techniques
that's the science part but it's a
programming problem it's an art like
programming it's even if you have a
correct implementation that gets the
correct answer it doesn't necessarily
mean that you are getting that you can
have practical effects like cash effect
intention and then the last thing is
measure and do this as an iterative
process so I that's it for this talk but
so I can stay for a few more questions
how do I handle how do we handle
exceptions vilette's depends on the
framework in the case of silk plus it
remembers the exception that would have
been the first one if this had been a
serial program and it throws it at the
at the appropriate join point a small
point so people are confused the CPU
cache doesn't use an LRU algorithm it
does not use an LRU okay thank you so
the cache it's more complicated than
that for sure but there's certainly cash
eviction effects yeah so you describe
the the algorithm as cache oblivious but
you're setting an arbitrary threshold
I've actually implemented this and I set
the threshold and it's essentially a
function of the size of your cache yeah
you you basically set it so that you're
working on a block that exactly fits in
inside your your cache
that is correct so so you could
certainly take a cache oblivious
algorithm and you can make it cache
aware by doing enough experimentation or
taking advantage of some knowledge of
just how big the cache is on a machine
you're using generally we could do when
you when you set the threshold if you
set it small enough that you're
guaranteed to be in cache of any
reasonable architecture modern
architecture that you might be running
on but big enough so that it actually
has some effect
and is the diminishing returns you can
as you make the threshold bigger and
bigger the advantage you get every time
you double the size of the threshold is
it gets less important so so the first
16 as we showed makes a big difference
the next 16 the threshold going from 16
to 32 that might make enough of a
difference to make it worth it from 32
to 64 my experience is around 64 is
about where it tops out and you stopped
noticing the effect very much even
though that may be smaller than your
cache so if it's not it's not a hundred
percent accurate to call it cache
oblivious but it's pretty close and if
it's if it's within the size of even the
smallest machine you may be running on
it'll work well on bigger machines that
have big passion thank you maybe a
two-part question can you give us some
intuition on the cost of silks pawn and
also how expensive is the runtime itself
so silks pawn is between five and ten
times the cost of a normal function call
in the Intel implementation there the
original MIT implementation was slightly
faster but it involved the inability to
call back and forth between normalcy and
silk and we paid the penalty instead to
make that compatible still looking at
ways to make it even cheaper the steel
is where the expense is in a way I
haven't talked about work-stealing which
is the scheduler the runtime scheduler
the if you do a lot of thrashing if
you're spawning off a lot of really
small bits of work that steel overhead
think it could be six significant I'm
not exactly sure how to quantify it but
that is where the expensive part of the
scheduler is and that is another
advantage of recursive algorithms is
that they they spawn off big chunks and
then when you get down to the leaves or
not even to leave but halfway through
the recursion there's no more all the
CPUs are saturated there are no more
steals so you're not paying the overhead
of Steel
okay whereas if you if you were to like
spawn in a serial loop you're constantly
stealing stealing because you're
spawning something short it's done and
now that CPU becomes idle again and it
has to steal again so there's all sorts
of techniques there that we didn't get
into how avoiding spawning off two to
small chunks of work the one other thing
is to mention about the overhead last
night when I lost my train of thought so
sorry thanks well I know people want to
leave so I'm gonna just try to be the
last person I want to thank Pablo very
much because I learned a lot in this
session but Pablo I am troubled about
something you mentioned that races a
number of times in this session right
yeah and that having lots of races is
not a good thing right doesn't that my
definition make you a racist yeah yes we
have a tool we have a set of tools that
we call race detectors so it'll tell you
what your what your your your genetics
are pointing at you the race detector
will do it thank you very much for
coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>