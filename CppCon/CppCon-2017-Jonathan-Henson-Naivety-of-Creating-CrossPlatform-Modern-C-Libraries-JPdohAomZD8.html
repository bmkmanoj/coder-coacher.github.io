<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Jonathan Henson “Naivety of Creating Cross-Platform, Modern C++ Libraries...” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Jonathan Henson “Naivety of Creating Cross-Platform, Modern C++ Libraries...” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Jonathan Henson “Naivety of Creating Cross-Platform, Modern C++ Libraries...”</b></h2><h5 class="post__date">2017-10-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JPdohAomZD8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Thank you for coming.
There are several interesting
talks in this slot
so it's a great privilege
that you've chosen to come to this one.
My name is Jonathan Henson.
I'm a senior software engineer
at Amazon Web Services
and we're gonna talk
today about the design
of the AWS SDK for C++
and some of the things
that were challenging
and some of the things
that were successful
and some of the things that
are also even a bit naive
as I allude to in the title.
When we first began noticing
that C++ 11 standard
was making its way into
commonly deployed compilers
and with the release
of Visual Studio 2013,
I thought to myself, I
have to find a good excuse
to use this without any
legacy cruft to slow me down.
Just start with modern C++,
cross-platform should be a bit easier
and let's see what happens.
Fortunately for me, AWS was receiving
a great deal of requests for C++ support.
So we began talking to our customers
and what we realized very quickly
is that we were talking
about multiple domains
and many of them with,
seemingly incompatible design constraints
from video games to systems to mobile
to embedded in devices
and we had to find a way
to meet these design requirements
and still provide an
idiomatic C++ interface
and we knew that we
were gonna have to spend
a great deal of time
upfront, thinking through
how we were gonna address
these design requirements
and have a flexible library for later on.
Our agenda today for C++ engineers
is we're gonna go over each
of these design requirements
or many of them.
For each one of those,
we're gonna talk about the
design decisions we made,
the results, so we're gonna
look at some consequences,
largely what was successful,
what was challenging about that
and then we're gonna have
some recommendations.
And some notes on these recommendations,
I know we're all a little bit rebellious
being C++ engineers.
These are not mandates
or even necessarily
always the best practice,
they're questions you should ask yourself
in the design phase and if
you come to different answers
to those questions, that's great.
Just ask the questions.
Before we go too far,
let's dive into exactly
what is the AWS SDK for C++.
It's a, open source library
that does pretty much
everything you need to do
to connect to Amazon Web Services,
each of our 70-plus services
however many there are
at the moment, there's a lot.
It's cross-platform, it's modularized,
we have a core runtime that we hand code,
we have high-level libraries
that we build to do,
very common tasks such
as uploading directories
to Amazon simple storage service
or uploading large files
and doing it in parallel.
And then the middle part,
are the generated SDKs.
Amazon has an interface
definition language
that we on-the-fly generate the part
that you would most likely interact with.
So, the S3 SDK or the DynamoDB SDK
or the SQS SDK.
Those are all generated on the fly.
Our first and foremost design requirement
is that it was cross-platform
and this meant that we were
going to need to support Linux
both RHEL and Debian
variants as well as some,
how do we say it, vintage
Linux distributions
as well as the more modern ones.
We're gonna support Windows
desktop and Windows Mobile,
Apple so iOS, OSX, Apple TV, Apple watch
and so on.
We want to build for Android
as easily as possible.
This one turns out to
be quite challenging.
And then we want to have at least hooks
to handle embedded systems
where we cannot predict
what the operating system is
or what types of libraries
are gonna be there
for cryptography and things of that nature
and lastly we knew that we
needed to support video games
and therefore video game consoles.
The first design decision here
and this is more of a meta decision,
is to start and commit.
We don't want to do a prototype
and have it running on Linux and then say,
&quot;Oh wow, this was really useful,
this is really successful,
&quot;now we need to get this
running on Windows.&quot;
Because that's a recipe for disaster.
We decided we wanted to
think about it up front,
every decision we made,
every dependency we took,
every interface gets scrutinized
for how is this gonna scale,
when we realize we missed a platform
or when Linux changes out from under us
or anything of that nature,
how are we gonna handle that?
The first thing that you
usually need to decide
when you're writing an application,
is how are we gonna build it?
How are we gonna expose
this to our customers?
Another thing to take into account
when you're writing a library,
is every decision you make
that is in an interface,
that a customer touches,
is a decision you've
made for the application.
And it may be a decision they don't want.
So we don't want to make a decision
for what your build system should be
and we also want it to,
run across multiple platforms.
The easy decision here
for us was to use CMake.
We didn't have to maintain
multiple visual studio solution files
for multiple visual studio versions,
we didn't have to maintain auto make files
or Xcode projects or any of that.
You code up what you need in CMake,
you let it generate the solution
or the build script and you just run Make,
you just run MSBuild, whatever it is.
So that's the idea we were gonna go with.
And this makes it really
simple to not compile things
that you know are of no
use on that platform.
We weren't gonna compile
libcurl or OpenSSL on Windows
or OpenSSL, we weren't going to try
and compile that on Mac.
We can compartmentalize
out the parts of the build
that are irrelevant to
that platform in CMake.
That's the choice we made.
How did that turn out?
Most build systems just
worked out of the box
and that is exactly
what we were going for.
We never needed to touch
a visual studio solution,
we never needed to touch a make-file.
Even the ones we didn't
test with such as Xcode
or Android studio or Qt's toolkits,
just worked.
And if they didn't work,
it was easy enough to shim together.
Custom platforms via toolchains,
we have platforms we can't
publish in open source
and we can't write the
builds for lots of reasons
and all we had to have
was a separate repository,
a toolchain file and the build just worked
for those platforms.
Target exports helped
tremendously with CFLAGS
and library linker flags.
And it helped a lot when
exposing these to the customer.
Now, where this was challenging,
is that the deployed
versions varied widely
especially on Linux.
And going to our customers and saying,
&quot;I'm sorry, you have RedHat 5
&quot;and it has an ancient
version of CMake on it.
&quot;You can't use our product.&quot;
Is not a great option for our customers.
So that was a challenge we had to address
or live with, however
you wanna look at it.
Because of that, modern semantics
were not always available
so we didn't get the nice
and tidy target exports
that we've come to expect in CMake.
Another thing that we thought about a bit
but we realized how important
it was after the fact,
is to think hard about the
&quot;but I don't use CMake experience&quot;.
What if your user is using Visual Studio,
what if they're using directly,
what if they're using VC package
or what if they're just writing make files
with package config?
How do we integrate with that?
So that's another challenge.
Here's one of the consequences,
is you can make a really
nice CMake experience
for your users.
And in this case, we simply
call out define package
for the AWS SDK.
It defines a bunch of macros for us,
it defines a bunch of variables.
We even could write our own custom macros,
where you could say,
&quot;Hey, I want to use the
S3 encryption client.&quot;
And it would figure out
that you need S3 encryption
or figure out that you need
S3 KMS, the core runtime,
figured all that out for you
and just passed it to your
target link libraries call
and most of this is designed for you.
There's been some excellent
talks this week on CMake
so I will actually move on past this
'cause there are better experts
on this experience than me.
Our recommendation here
is to just use CMake
if at all possible.
It's less work for you, it's
a better customer experience,
we have not had any of our
customers complain about CMake
as a design decision.
The target exports,
there is the best practices
on CMake talk yesterday,
I encourage you to look it up.
Use them they're fantastic.
But think really hard
about how is it gonna work
for someone that doesn't use CMake?
How are they going to get your binaries,
how are they going to
look up your CMake files?
If they're using CMake,
how are they going to
specify the linker flags
and include paths?
Figure all of that out and
have a good story for it?
The next decision we made
that I've already alluded to,
is to baseline on C++ 11.
We're not including this
into a legacy code base
so we had free reign on it.
And the main goal here,
was that we can take advantage of things
such as standard thread and standard mutex
and standard atomic that are
painful to implement manually
and this saves us time.
This makes it far less likely
that we're going to screw it up.
The memory model was
also really attractive
with shared pointer and
unique pointer and our values.
We don't need to spend near as much time
thinking about the ownership
model of our classes.
We don't have to scratch our heads
and draw it out on a whiteboard,
how the handoff is going
to happen from two modules,
a lot of that is taken care of for you.
And clocks are painful.
I don't know if you've ever tried
to implement a clock manually
but it can get really tricky
especially when you go cross platform.
Standard chrono in the entire name space
was very attractive for us.
We're going to look at something here
that probably every
one of you have written
at some time in your life.
I'm not claiming it as
a perfect implementation
but try and imagine here,
how you would do this pre C++ 11.
Here we have a thread task
that lives inside a thread pool
and it simply runs in a thread,
grabbing as much work as
it can off of the pool
and executing it.
Immediately here in the
header file we're using atomic
and we're using standard thread.
Two things we didn't have to implement.
And then in our implementation,
we've stripped out a bit
to try and make this more slide readable
but you loop over, while continue,
check the executor, see if it has tasks,
read everything you
can, as fast as you can,
execute the function.
Hit a semaphore which is sync points
and standard conditional.
We have a unique lock here.
All of these are C++ 11 features,
none of which we had to implement.
It takes 15 minutes to write this
and we never had to go through
the pain that we used to
or would have had to have done.
But, there's a dark side here.
This is a function that we added to verify
that a DNS label was valid.
Can anyone tell me what's
gonna happen there at runtime
on GCC 4.8 or older?
(audience laughs)
- [Male Audience Member]
Nothing whatsoever.
- Worse.
Something worse, much worse.
- [Male Audience Member]
Exception will trigger--
- Yes or it will segfault
or it will crash badly.
And you don't get an assertion,
you don't get anything built on,
all you get is a complete failure
when the user runs this code.
You could argue that, &quot;Well,
that's the cost of innovation
and live at the head,&quot;
but we can't make our problems
the customer's problem.
We have to find a way to
work around these things.
One of the design consequences here
is you have to be really
careful on some of the features
on older, almost C++ 11 compilers
that customers are going to use.
This worked well and we already talked,
we had far less to implement
on different operating systems.
Microsoft Visual C support was consistent.
The only drawback was
on Visual Studio 2013
and auto-generated move
constructors and operators
which you find out usually
when a build fails.
Clang support, we had
no problems with clang.
Modern features cleaned up the interfaces,
we didn't have to have clunky ways
of exchanging shared features
and moving them around
and the defined memory model just made
most decisions really easy.
Where this was hard.
Linux GCC is really,
it's a special case.
And sometimes you can't even
upgrade the compiler safely
where you can update the
compiler safely, it's not,
but you can't upgrade it safely
unless you rebuild the entire thing.
So that's a bad customer story.
Integrating with legacy applications.
What if somebody wants to
use Amazon Web Services
from their application and
they've got a crufty old system
that was built with GCC 4.1.2 on RedHat 5,
what are you gonna do?
I'll skip over this, I'll go to the last.
Many of our users hated our
exposure of standard iostream
and I kinda like it, but I
understand why users hate it
and so this was another
thing that proved difficult.
Some recommendations here.
Do you need to support
older Linux distributions?
If your answer is no, you're
done, don't worry about it.
If your answer is yes, we are
trying to provide a feature
that enables an entire
business to have a capability,
then you need to think really hard
about how you're gonna do that.
What about the embedded story?
Sometimes you don't get a
full C++ compiler even there.
The standard Lib.
We had great success
using the standard Lib.
Most of the previously hard
things are done for you,
it's less to code, less to think about.
My one thing I would throw out there is,
talk to your users about standard iostream
before you expose it.
Because once it's in your
interface, it can't come out
without a breaking change.
It's not wrong to use it
but talk to your users.
And take advantage of the memory model.
We'll come back to that in a bit.
This is the decision that I
would argue has worked out
probably the best for us and
if you look at just that slide
you'll see it seems somewhat inconsistent
but it's not inconsistent
based on our philosophy
so I'll tell you what we mean.
The idea here for no dependencies
is to use the operating
system distribution
and to abide by what I
call the first boot rule.
On UNIX this means Libcurl for HTTP,
OpenSSL's libcrypto for all of our hashing
and symmetric crypto,
things of that nature.
Use POSIX for operating
system specific features
such as file access, directories,
a lot of that goes away
with the latest technical
specifications on file systems.
On Apple, it's similar,
we just used common crypto
instead of OpenSSL and on Windows,
we had far more options, we used WinHttp
on most builds.
For universal Windows platform
we could use IXMLHttpRequest2.
It's a COM interface.
Bcrypt is the cryptography
API that you get on Windows
and then the Win32 API for
most file system functions.
And then we decided to
manually implement utilities
such as DateTime on top of
standard chrono of course,
Logging, String split.
There's been a lot of talk
about String splits this week,
Base64 encoding, hex encoding,
most of these things
take 30 minutes to write
and write a unit test for
it and verify it's correct.
So we just went ahead and bit
the bullet and did it there
and decided the dependency
was not worth it.
How did this work out?
This is our Build pipeline
and it's really complex
and it's really long
and this is for only one of our consumers.
But this is an AWS code pipeline
and we consume our source.
First thing we do is we kick
off our mobile platforms.
We have Android and Apple,
various permutations of that,
when those finish we go
to the desktop platforms,
so multiple versions of visual C,
multiple versions of Linux,
multiple versions of Clang on OSX
and we build all of these
and aggregate them through
and what's beautiful about it,
is we don't change anything
in the CMake script.
It is just CMake, make or CMake MSBuild,
it all just orchestrates
and it worked out really well for us.
Builds for custom platforms
also ended up working well
because we don't compile things
that we know will not work
on that platform, we don't
have any build breakages,
the user is just responsible
for injecting their interfaces
at runtime and we'll get to
how we do that in a little bit.
There is no surprise behavior
that wasn't a bug in our code.
How many times have you gotten
almost to the finish line,
you're a day before release and oh,
the XML parser and .net
makes some weird assumption
or the runtime library
in X, Y and Z does something weird,
it breaks our entire system,
we didn't know that was how it'd work
and you're two weeks late shipping
or you make some really
ugly hack in your code.
We didn't have any of that.
Legal and licensing was dead easy
'cause we didn't have
any legal to worry about.
And where this was hard, is
not because of no dependencies,
it was because we didn't
take it far enough.
Libcurl causes us a
great deal of complexity
on both iOS, Apple TV,
that sort of thing and Android.
And there's behaviors in
libcurl that we don't want,
we'll come to those shortly as well.
And because of that, Android
and iOS still have needed work.
And that's the only place where I think
this didn't work out well for us.
So recommendations.
I wanna make a distinction
between a library
and an application.
In an application, you know,
usually you know the target,
you know the host, you often
know the operating system,
you know the memory constraints,
you know the threading constraints.
You can make these decisions
and you have the knowledge necessary
to make a good decision
about a dependency.
In libraries, every decision you make,
you make for the application.
And if you take a dependency,
every decision that dependency makes,
is made for your application.
And given the various use cases we had
that was just not an option for us.
You need to think about
that, what are the decisions
that you don't want to make for the user?
This involves threading,
memory, what about exceptions.
Now the first boot rule,
what's the first boot rule?
I think it's pretty easy to explain.
If it's on the machine the
first time you boot it up,
you can consider it for a
dependency and that's the rule.
If libcurl is on there
and OpenSSL is on there,
have at it, consider using it.
But if it has to be pulled
down from a package manager
or something of that nature,
unless you are internally in your company,
have a way to solve that problem
in non NP-hard ways
then, first boot rule.
Just a quick reminder, I
know we all know this but
dependencies don't always save you time.
As I mentioned hidden features
and then there's security and patching.
Avoid deep branching dependency trees.
Your dependencies have dependencies
and that's unmanageable.
Libcurl is an excellent example
of a beautifully written library.
It's been wildly successful
but on different platforms,
it links to GnuTLS.
On different platforms
it links to OpenSSL,
on different platforms
it links to Darwin SSL.
These things get out
of hand really quickly
and it makes assumptions
about your I/O model
and it gives you ways to plug in
but it's really complicated.
So deep dependency trees,
can be very bad for your library.
How is this dependency that
you're considering, built?
Can you embed it, can you
just put it as a header file
inside your project and name space it
and then use it that way?
That could be fine.
Does your company have a build
solution for native code?
If they do, it may be a
lot less to worry about.
We'll just skip on down
to how do you patch?
What happens when zero-day
vulnerability comes out
in whatever you're using?
How are you going to get that fixed
on the user's operating system?
What's the thing you're gonna trust?
On windows usually that gets
updated by the Microsoft team,
on your Linux distribution usually
that gets updated by RedHat or Ubuntu
or whoever is doing it.
But who patches it, how is it deployed?
These become more complex
when you have a dependency
that is not part
of the operating system
distribution itself.
And just to remember as a general rule,
every barrier a user has
to building your library,
is a barrier to people using your library.
Another portion of the NO
dependencies design was,
just calling me OS API's directly
instead of taking some
other dependency to do it.
And this worked out well
for most common platforms.
The build was simple, bugs
are easy to fix and ship
but there is one thing that makes it hard.
This code here, is part
of a directory traversal
and you'll see that we
call readdir on a string,
right here
and then we evaluate the enum d_type
to figure out if it's a
directory symlink or a file
and that's all the way down here.
Any of our file systems
experts know why this is wrong.
- [Male Audience Member]
It's not portable.
- It's not portable?
- [Male Audience Member] Yeah,
it doesn't work for Windows.
- Right, well this would
never get compiled on Windows
but yes very true.
Neither did we, because
you would have to read
very deep down into the Linux man pages
to find out that not
every file system module
that's part of the kernel,
will give you the enum
on directory type.
So we wound up with some
very hard to diagnose bugs.
We had to introduce lots of logging
to figure out what was going on,
on this one users unit test,
on this one variant of CentOS.
You actually have to call LS or Lstat
before you can do any of this.
And it's technically in the man pages
but it's really easy to miss.
The advice here is,
Linux man pages are your friend.
If you go down there you really need
to almost memorize those man pages,
have very strong code
reviews by a Linux expert.
Where this was hard.
Mobile persistence layers
and this was a complete miss
that we had to go back
and fix a good deal of,
most of the time on a mobile application,
you don't hit the file
system directly, ever.
Unless you have a very special use case,
the security is complicated
and usually there's a persistent system
such as some fancy hash map
or some sequel lite mechanism
where you go to,
to persist your data for the application.
As I've mentioned,
variants on POSIX file
system implementations
is challenging, windows in
Unicode, I could go on forever
on Windows and Unicode and
I don't think we have time.
So we could talk about it afterwards.
But you need to make a decision
about this ahead of time.
As I said, think carefully about mobile,
think carefully and read
the man pages in Linux
and decide on Unicode
before you start coding.
If you're going to
touch the windows API's,
are you going to expose WCHAR?
We decided not to.
I'm not saying that's the
right wrong or the wrong,
the right answer or the wrong answer
but if you do that then you got to say,
&quot;What do we do with
the Unicode only API's?
&quot;How do we handle conversion for the user?
&quot;What about the ifdef
Unicode defines on GetObject
&quot;when you have S3 functions
&quot;that are publicly
documented as GetObject?&quot;
How are you gonna handle this?
Have a story for it.
Allocation control came directly
from our systems developers
that were in highly threaded environments
as well as our video games programmers,
where it was not an option
to touch new or delete
and it was not an option to
override new and delete globally
and that's a really complicated problem.
I'd love to talk to you
about this afterwards
if you have questions
but we couldn't do it.
Also, there's potentially,
more performant heap allocations
out there that people want to use
such as dlmalloc, the
list could go on forever
or you may wanna do
pre-allocation of pools
and have stack pointers moving around
based on the various types.
The different types maybe could
go to different allocators.
So global new and global delete
or malloc and free were not an option.
So how do we handle this
and still provide an
idiomatic C++ experience?
Our decision was to
write a memory interface
which we'll look at in
a moment and internally
we would always go through
this memory interface,
anytime we needed to allocate memory.
We would default to malloc
and free and in-place new
and call the destructors manually
and everything that goes with that.
We would use the STL allocator hooks
where they were available
and we would allow the user
to turn the feature off.
So if the user did not need
custom memory allocators
then AWS vector is just a standard vector
and AWS MAP is just a standard map
and you could go on about your happy way
using the standard C++ standard Lib types
and not worry about it.
We also knew that since
we were working on allocators anyways,
we should check our
memory in the unit tests
and just be done with it.
We don't need extra valgrind runs,
we don't need any of that,
since we already have an allocator hook,
we can track all of the
memory and verify it
as part of the unit test run.
Here's how the code
looks for the allocators.
It's really simple.
We simply call into
malloc which is a symbol,
that we define,
the user can install in a
factory in a different spot.
We allocate the memory,
we do in-place new here
and then we return the constructed memory.
Deletes, very similar if
we just call the destructor
and then call free and
that's all there is to it.
It's a bit more complicated for Arrays,
happy to show you that
code a little later.
And then to use this,
if custom memory manager
is not being used,
we define an allocator as mapping directly
to standard allocator, we're done.
If custom memory management is on,
we define an allocator that
hooks into all of these things
that we just described.
And then here's a couple of
the type defs or usings here
where we define AWS MAP as a standard map
using our allocator.
And we can do similar
things for shared pointer.
Where this worked well for us.
The applications that needed it loved it.
It was super simple they
wrote a single interface,
injected that into the SDK
initialization and it worked
and they had complete
control over their memory,
most of the time.
Unit tests verified memory usage.
Some we would have, say we
have an engineer make a commit
to the repo, push it up,
we run our unit tests,
immediately they fail,
we know that we've got a memory problem.
STL integration worked
well most of the time.
There's some complications
with standard function.
But this is gonna get better with C++ 17
so we'll look at that a little
bit later how we can do that.
This had minimal impact on
customers not using the feature.
If they didn't need it, they
just used standard C++ types.
Worked very well with
standard shared pointer
since it stores the deleter
and we had a expected,
unexpected benefit
and this helped
tremendously with DLL Safety
because we essentially forced everything
through the same allocator.
So we didn't have a
problem with multiple heaps
on Windows and DLLs.
And this was a side effect
that I did not even think about
ahead of time, until we noticed it.
Where this was hard.
It changes the ABI.
This one feature will essentially change
what a standard map, what an AWS map,
whether it's a standard
map or something different.
The initialization scheme
installed global state
which is bad and there's
more reasons why this is bad,
we'll come to shortly.
OpenSSL and COM don't
provide simple memory hooks.
So there's memory that's just lost
that the customer has no control over.
STL type confusion, we
would have customers
passing a standard string
and then in GitHub,
sending us a linker
error that could not tell
that the linker error was
because the types didn't match,
so this was painful for
our customers sometimes.
The Init and shutdown sequence,
I'm gonna show this on a later slide.
Another unfortunate side-effect
is we could never accept
or expose a standard unique
pointer to a customer
because the allocator gets lost.
Yup.
So recommendations here.
If you're a library,
consider providing a
flexible memory model.
If you allocate any memory
and you have applications using you
that you cannot predict,
then I would argue, it is your sacred duty
to provide them control over
how the memory is handled.
And if you choose not to provide it,
just check with your applications.
Ask your users how they're using memory.
What are their constraints?
Are global new and delete sufficient?
Standard unique pointer becomes available,
it's a simpler way to do
it if that's sufficient.
Is C++ 17 an option for you to baseline?
Then the memory resource
is a great option here.
For sake of time I'm gonna skip on down.
Do you need to allocate memory at all?
This is a perfectly valid question.
Some of the best API's I've ever seen,
will tell you how much memory they need
and then you allocate it and
then you pass it to them.
That's also a very valid option.
Part of the allocation
strategy ties in here
to the customizability.
Since we cannot predict
all of our platforms
and since we cannot predict all of our,
build types and all of
the customers needs,
everything that did anything substantial,
needed to be customizable.
And then with Sane Defaults.
For platform specifics,
we would use the OS API's
but usually provide a hook
for you to do something different
if that was not available.
We had used the principle
of not being surprising.
So if an interface has
a name called executor,
it's a thread executor,
do what people expect
thread executors to do.
Allow the user to control the behavior.
And the benefit here is simply that,
if you make an assumption and it's wrong,
it is easy to undo it,
if you put it into the right
customizable framework.
The first decision here was
to use runtime polymorphism
for this.
I'm not gonna bore you on Gang of four,
style, data structures and all of that.
I'm assuming,
you all know how object-oriented
programming works.
So I will fly through this.
We would use polymorphism
and abstract factories.
All ifdefs get hidden
inside a default factory
where we kind of join everything together
and figure out what got compiled
but they would not be
scattered all over the code.
And then we would expose the
factory interface to the user
so they could pass us a factory,
if they don't want to use the default one.
For example we need to do
SHA-256 HMACs all over the place.
Here's an HMAC interface.
It calculates a hash
result based on a secret
and the actual thing
that you want to sign.
Here's the factory which
returns an instance
of SHA-256 HMAC,
oh this is the abstract Factory.
It returns an instance of HMAC.
You want to implement a SHA-256 bcrypt,
you would just implement this.
And then here's how the
default Factory would work.
We have the default SHA-256 HMAC Factory,
if we're using Windows,
return the bcrypt one,
if we're using OpenSSL,
return the OpenSSL version,
common crypto, so on.
And that's all there is to it.
Then to allow the user to
override this mechanism,
if we got it wrong for some reason,
you don't need to do anything fancy.
Just give them a standard
function they can set
and call it at a later time.
So you pass this in to the SDK options,
whenever we get ready to
start computing SHA-256s,
we'll call your function,
get a factory back,
use that for the rest of the application.
The consequences here.
Cryptography and HTTP 1:1 worked great.
Where our users hated curl
or didn't wanna use it,
they wrote their own.
We actually had a case where,
a user was writing networking hardware
it had hardware based crypto,
where they would go directly to the board
to do these things.
They did not even tell
us they were doing it,
they didn't ask us how to do it,
they just did it and then
in a meeting one day,
they told me they had done it
and that was really exciting.
And they were able to extend this simply
especially adding new platforms.
Right after we launched the
SDK for Developer Preview,
we started getting build failures on Mac
because OpenSSL had been deprecated.
So about six hours later,
we had common crypto plugged
in and it was finished.
It was really simple to fix our mistakes.
Where this is hard.
The factories due to
some other constraints,
were getting installed
into the global SDK initialization state
and this has special problems
whenever load library comes into play
or whenever you do not
have complete control
in your library and the
application cannot know
that it is using the SDK.
Global state gets really
hard, we couldn't help it
because OpenSSL forces
you to use global State
and it all went into global state.
We'll have a recommendation
on this in a moment.
Not all C API's map well into C++ classes.
So Bcrypt is a prime example
if you've ever looked at
the AES CBC implementation in Bcrypt,
it is really hard to
make that work in a C++.
INIT update finalized
sort of idiom for crypto.
We have one if you wanna know about it,
we'll talk about it later.
And it forces more heap allocations
and you have the overhead of the dispatch
that comes with runtime polymorphism.
This is definitely something to consider
before you make that decision.
So our recommendations are,
if it's possible, take
advantage of inheritance
and polymorphism.
It's there, it works,
it's usually performant
and the heap allocations aren't a big deal
if you initialize them at the
right time and don't hit it,
all the time.
Think carefully about
your C API INIT calls,
where is that state going.
Ideally, you could encapsulate
that into a library handle
or some sort of state that is
per instance of the library.
That's not always
possible but think through
if you can do that.
Don't get fancy with your factories,
they don't need to have,
complicated abstract factory hierarchies
and multiple inheritance
or whatever weird things
people do with factories.
You can just take a
function pointer and use it.
For the things that were
not platform specific
such as threading, so
an executor interface.
The design decision was just,
default the standard thread
with an optional pool.
That's what I would
expect if I was the user
and I saw that title.
Memory allocators.
We already talked about
using default malloc and free
also what I would suspect you
would want most of the time.
And then all interfaces, all custom things
that do any task within the SDK,
get injected.
Either via constructors
or client configurations
or in the global state for the SDK Init.
So it's simple for users
to override this behavior.
For the client scoped changes,
we have a retry strategy
which figures out when to retry an error,
if there was a network timeout,
whether the service had the problem
or the client had the service.
Does exponential back-off,
that sort of thing.
A thread executor, a rate
limiter which allows say,
a video game to say &quot;Hey,
I'm using all of my bandwidth
&quot;for this multiplayer game,
&quot;I really need you to
stop uploading that data
&quot;to simple storage service
&quot;or I need you to do it a lot more slowly
&quot;'cause I need the bandwidth.&quot;
So that provides them
a mechanism to do that.
And then for the more global
options that should be SDK wide
so not scoped to a single client,
there would be logging
options, memory options,
HTTP, crypto, so on.
And here's an example of one
of them with memory options
is we just take a pointer
to a memory manager, done.
Where this worked out well.
Customers rarely ever used it.
Which means, I think the
defaults were good defaults.
Singleton support.
This one's kind of weird but suppose,
you're in a constrained
threading environment
and you don't want 10 threads
across multiple clients running,
you're in a video game engine
and you have two threads available for I/O
and that's all it should ever be used.
This allows you to pass
the same thread executor
to all of your clients
and be done with it.
Custom platforms, once
again they're manageable.
If we didn't write the
right implementation
for that platform, it's
easy to fix that mistake.
Testing was really easy
'cause we just stub out the
interface and pass an instance
to the constructor and we
can verify the behavior
goes through the interface properly.
And because of this,
this results in cleaner
interface separation,
smaller classes.
Where this got really hard is ownership
and scoping complications.
We'll go ahead and cover this now.
We take an instance of a
shared pointer for executor
to, if possibly the same instance
to an S3 client, a dynamodb client
an SQS client, go down the list,
what's gonna happen here, when say,
one of those clients goes
out of scope or gets deleted
and they're passing
member functions to it?
If one of the clients goes out of scope
and it scheduled a task on that executor,
then the executor picks
it up and starts running
but now the object is gone
from the history of the universe
and you will immediately segfault.
You can work around this obviously
and you can tell your
users to do smarter things
to handle the situation but it's something
that is a side effect of
the customizability approach we took.
Every virtual function you put anywhere
is now an expanded API surface
and it's expanded blast radius
and it's often easy to forget this.
Some recommendations here.
Default to the least surprising thing.
What does the user expect
an interface to do?
And do that.
Consider constructor injection
rather even than using a giant struck
to pass a bunch of pointers to it.
It will make the bloat obvious,
it's idiomatic to resource
allocation as initialization,
it can make bad code
look bad really quickly.
Think about ownership ahead of time.
This was a mistake we made
at least in not documenting
that that's how that would work.
Who owns it, am I using it that way,
am I encouraging users to use it that way?
And then--
Go ahead.
- [Male Audience Member]
Would you be able to elaborate
a bit more exactly what you
mean by constructor injection?
- Yes.
As a constructor argument
take a shared pointer
to an interface.
- [Male Audience Member] To
something that that thing needs.
- Yes.
- [Male Audience Member]
Why (mumbles) work?
- Yes.
- Okay.
- Virtual functions are API's.
Anywhere you put virtual, a
user is going to override it
and implement that.
If you ever add virtual equals zero
and then push up to GitHub,
then you will have those
customers breaking immediately.
So remember you're
increasing the blast radius
every time you do this.
This is our most contentious
design requirement
and...
Yeah.
We couldn't use exceptions
because video game engines,
most of them forbid it entirely,
legacy applications that
aren't exception safe,
forbid their use,
those two go together.
What was our decision to do this?
How are we gonna handle it?
Well, we still outcome from
Erlang essentially and use that.
And it's just a union of
return type and error,
this actually gets a bit easier in C++ 17
where you don't need this at all.
So I would recommend
actually looking into that.
We decided to log very heavily.
Constructors, don't do too much
and force allocations up to
the user where at all possible.
And that's the best we could do.
Put a Boolean operator on it and say that,
initialization failed
and that's what we got.
So here's a brief snapshot
of the little wrapper
around the exception union
or the outcome Union.
What were the consequences
here, where did this work well?
Well, we don't make that
decision for the user.
So if the user doesn't want
exceptions, they don't get them.
User wants to throw exceptions,
the SDK is exception safe,
sort of.
They'll be fine and this
supports more customers.
Outcome worked out nicely.
It made for nice and tidy
code most of the time.
We never had to have the
religious zealot argument
about exceptions because we simply said,
our customers not all
of them can support it,
it's not an option, close
the book, it's finished.
And more recently,
it makes the move to
async/event-driven I/O, far simpler.
If you just wean yourself
off of exceptions now,
when you go to an
event-driven architecture,
you have, I don't know,
two function calls in
the stack trace anyways
so how is it gonna help you very much?
But there was one challenging part.
This is a curl callback to read the body
that curl calls when
it's ready for some data
and so we read from the iostream
the amount of data they requested.
What could iostream do?
Anyone, what could it do?
Was something it has the
option to do right here?
It can throw.
And iostream can be overridden
with your own stream buffer
or if you're really brave,
your own iostream subclass.
And if you throw here,
what's gonna happen?
This is five or six functions
down inside libcurl.
What's gonna happen to all
the memory curl allocated?
It's gone.
Well who knows... it's not gone,
but you don't have any
pointers to it anymore.
It's done.
So while our C++ code was exception safe,
the callbacks from the C
functions were not exception safe.
Where this turned out hard.
Outcome bloated binary size.
That ship sank in the harbor years ago
on binary size anyway so I don't know,
that's the end of the world.
But you never can turn
them off on some platforms
so say on Windows, you
can't make it not throw
as far as I know.
How are you gonna handle
out of memory errors
without exceptions?
If you're using the STL
types of standard vector
and you run out of memory,
what's gonna happen?
If you're not getting the out of memory
or the bad alloc
exception, how do you know,
how can you do something
reasonable about it?
This is more of a complaint but RAI,
becomes technically unsafe and you end up
with assert all over the place
and you end up with Boolean
operators sitting around
to say that, &quot;Yes I allocated
all the things I needed
&quot;in this constructor.
&quot;You can go ahead with your...
Yeah it's fairly standard
idiom in C++ anyways.
Polymorphism based API's
as I just showed you,
used in C callbacks, that
one will definitely bite you.
Recommendations on exceptions.
These are questions you need to answer
as you're thinking through this.
What about out of memory errors?
if the program can just
crash when it's done,
fine who cares?
Don't worry about it.
If you're somewhere where
you have your memory,
very well segmented and you're
using the allocation strategy
we showed before, then out of memory
is usually a perfectly handleable thing.
Drop some textures or load
the lower res textures
or, there's a whole, you know,
change how you're communicating,
how frequently you're calling
out to another service.
Can they be trapped
behind a compatible ABI?
Can you put a C wrapper around it?
What about LoadLibrary?
If it can be detected at debug
time, assert's an option.
Do you need to throw an
exception here anyways?
Be extra careful when
you touch the C API's
as I just showed you.
And consider providing and
committing to providing,
object validity checks via bool operators,
that seems to be the
common way of doing it.
Our last design decision here
that we're gonna talk about
is the threading model.
This is one that I think,
you really need to think very deeply about
if you have the default,
the default idea that I had.
We chose to base the Asynchronous APIs
on the synchronous API.
We did this for a couple of reasons.
One, it was simple to reason about.
You just submit it to a thread executor,
let the thread executor
run it, asynchronously,
return to the user when you're done,
call a callback, give them a
future whatever you wanna do.
And most of our core code
was already stateless
so thread safety was gonna
be a bit easier to pull off.
Here's how this ends up looking.
This is in the Amazon simple
storage service client.
We have put object Async,
which is what we expose to the user
that just submits a lambda to the executor
which calls a private function,
the put object Async helper
which you will see here.
Calls PutObject and then calls
the handler when it's done.
And arguably this is really
simple to reason about.
I just showed you this problem.
Where this worked well.
User can control the thread count.
It's a simple model for
people to reason about,
that worked out well
but there's a lot where it's challenging
and where it's perhaps a bad
idea for your application.
Shared memory is theoretically impossible
to prove correctness between threads.
Unit tests are difficult.
You end up with a bunch of
conditionals and mutexes
and counts all over the
place to synchronize things.
Mutex on some platforms
is a substantial penalty.
On some platforms it will flush
you all the way down to RAM.
So where these things matter,
you are, basically the moment
you make this decision,
you are tying your customer
in to this decision.
Blocking I/O is an efficient use
of the networking interface.
It consumes substantially more CPU
when you start doing sleeps to handle it.
Changing to non-blocking I/O,
would break your interface.
So if you ever say, &quot;Hey, C++ 17,
&quot;has a new technical
specification for networking.
&quot;I'd like to use this.&quot;
Ee!
You can't because you've
made decisions here
that will,
at the best-case-scenario,
is going to decrease the efficiency
of your code substantially to adapt to it.
And some applications only
have a few threads available.
Let's go back to game engines,
what if there's only one thread?
You've made it non blocking
from the users perspective
but you may queue up a
hundred web requests,
all of which have to wait
for the other one to finish
before they can execute.
So my recommendation here.
Start with your I/O
model and work backwards
and your threading model
may just work itself out.
Instead of thinking procedurally
and about your algorithm,
if you're going to be I/O bound,
start with the I/O.
Start with epoll, start with IOCP, KQ,
whatever it is or start with
the new C++ 17 spec on this
and work backwards.
And then, while you're
doing this you may realize,
there's no need for the
shared memory to begin with.
If you go to an event-driven framework,
where you go to a callback C style thing
or co-routines, there may be no need
for you to ever share
memory between threads.
You might need to hand
them off between threads,
you might need to do
some trickery with pipes
but at the end of the day,
you may never need to even
share the memory to begin with.
And if you forbid blocking,
then this becomes super easy to do.
And then build your synchronous API
on top of the asynchronous one.
It's perfectly legitimate,
that customers want a synchronous API.
They want to think
procedurally, that's fine.
Let then make that decision though
and they could use co-routines,
they could use standard
condition variable.
Condition variable is
simple, easy to test with.
And then as I said before,
what happens in a
single-threaded environment.
As always, thank you
for coming to the talk.
Please send us pull
requests at our GitHub,
if you see any problems
and thank you for your time.
(audience claps)
Think we have some time for maybe
some C++ specific design
questions, if anyone has them.
Otherwise, I'll be available around
to talk afterwards.
- [Male Audience Member] Oh sorry.
I had a couple of question actually.
First, I didn't recatch the reason why,
you use shared pointer everywhere
but not unique pointer.
- Yes.
With shared pointer,
the allocator is stored,
I'm not very good at templates but,
the allocator is stored
with the shared pointer
that you specify.
And it does not change the type
so that you can pass a
standard shared pointer
back to the user and it
will keep the allocator
that you specified.
On standard unique pointer,
it goes through the standard delete.
If we just pulled up the
reference it would show it
but if you specify the allocator,
it changes the actual type.
You can use unique pointer,
but you can't pass the
standard unique pointer
without the extra
template arguments around.
- [Male Audience Member] Okay.
I think there are some
options around that but okay.
- If there are, I'll be happy to use them.
- [Male Audience Member] You
can use those two functions
as your shared pointer (mumbles).
- I'm sorry, I can't hear you.
Could you go to the mic possibly.
- [Male Audience Member] You
can use these two functions
as a share pointer deleter.
- Okay.
- [Male Audience Member]
Yeah you'd raise that high.
- Yes.
- [Male Audience Member]
My second question
was about all the factories
and pointers everywhere.
Well, I don't want to gain bingo points
for (mumbles) like Java but,
was that really necessary
to have dynamic polymorphism
for something that depends
on your architecture
that's determined when you compile?
- We don't always know those things
at compilation time.
If we can't compile it,
there has to be some mechanism
for the user to inject it.
I'm certain that there
are some templates way
to do things of that nature.
But nothing's ever necessary
to design pattern to consider.
- [Male Audience Member] Yeah but,
you're making everybody
pay for virtual calls
like when you said 1% is going to use it.
- You mean 1% is going
to use the customization?
- [Male Audience Member] Yeah and then
99% pay for what we don't need.
- That's true and that could be something
to consider ways to provide
a zero cost attraction
on that in the future.
- [Male Audience Member] Great, thank you.
- Anyone else?
Great, I'll be sticking around for a while
so if you wanna talk let's talk.
Thank you again.
(audience claps)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>