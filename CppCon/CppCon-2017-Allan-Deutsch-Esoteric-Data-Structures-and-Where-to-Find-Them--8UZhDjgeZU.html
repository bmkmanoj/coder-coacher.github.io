<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Allan Deutsch “Esoteric Data Structures and Where to Find Them” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Allan Deutsch “Esoteric Data Structures and Where to Find Them” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Allan Deutsch “Esoteric Data Structures and Where to Find Them”</b></h2><h5 class="post__date">2017-10-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-8UZhDjgeZU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- All right, it looks
like for the most part
people have made it into the room.
So I'm gonna go ahead and get started.
I'm Allan Deutsch
and I'm here to tell you guys
about esoteric data structures
and where to find them.
I hope you enjoy.
So you might be wondering who I am.
I'm a student at DigiPen
Institute of Technology.
It's a small school in Redmond.
We make video games.
I'm studying for a computer science degree
and I've interned as a software engineer
at Microsoft Studios
Global Publishing Team
and as a program manager within Xbox
and the Advanced Technology Group.
And I'll be returning once I graduate
as a full-time employee there.
So the first data structure
that we're gonna talk about is slot map.
Slot map is an unordered container,
which means that your data,
regardless of the order
you put it in, may not be
in the same order later on.
And it's associative.
So when you insert something,
you're gonna get a key to find it later.
It stores all of the
elements that you've inserted
into it in continuous memory.
So there's no gaps,
which gives it great
performance for iteration.
And all of it's operations
are very consistent.
And I'm currently proposing it
for standardization through SG 14
and it's in Library
Evolution Working Group now.
So the red asterisk is because
the committee may change the name.
So if you're watching this
video in like five years
and you're thinking wow there's
something just like this
in the standard but with
a totally different name,
that's probably why.
So the problems being
solved by a slot map are
that you have some items
that need unique identifiers
and you have to have
consistent performance.
So this is specifically from games.
We have a very tight time schedule
and it needs to be consistent
for a pleasant user experience.
And so making sure that we
don't have amortized costs
that could spike
unexpectedly is desirable.
And we also get to reuse
memory so that we don't have
to do lots of allocations
and deallocations.
Another nice feature of slot maps is
that they avoid ABA-like problems,
which is where you have
some data that you've put in
and you have a key for it.
And then later on
somewhere else in the code
that element gets deleted,
something else gets put
in with the same key,
and then you have the key
from before trying to access it.
Slot map uses a generation
counter mechanic to avoid
that problem so that new
keys that get handed out
will not incorrectly get old data.
So it can be found in game engines.
It's also very similar
to a pool allocator.
So if at some point you're a
bit confused about what it is,
just think of it as a pool allocator
with something resembling a map interface.
And then in the future, once
it's standardized, I expect
to see it in browsers, compilers, and more
as those are some of the
interest groups that have shown
that they're interested in having it.
So the guarantees of a
slot map are that it has
true constant time lookup,
erase, and insert operations.
So rather than being amortized,
these will always be a fixed cost.
There's no hashing or anything
going on that could change it
with the one exception
that insert will be O(n)
when it needs to reallocate and grow.
So insert calls will return
a unique key to the user
which they can then use
to access their data in constant time.
And it's a fixed set of operations.
There's no hashing or anything.
So, again, very consistent performance.
And the keys to erased values won't work
until an overflow occurs.
That's on the generation counter.
So if it wraps around from
whatever your max value is back to zero,
then you could have ABA problems.
So one of common alternatives
to this is using a hash map,
but the lookup costs are
inconsistent in those
due to both hashing and probing.
And the hashing has a high overhead,
which is also undesirable.
And in many cases you might not
care what the unique key is,
just that it's unique.
Whereas in a hash map, you
have to create your key.
A slot map you don't, it gives it to you.
This hash map is also
prone to ABA problems
and at least in the STL
for us C++ users here,
it's not contiguous.
So a lot of undesirable characteristics
for the types of problems
that a slot map solves.
Another alternative would be
using an array or a vector.
You can use the indices
as kind of like keys,
and since it's all packed
together it's gonna use
a bit less memory than a slot map
because it doesn't have
some of the overhead
to make the guarantees.
But the downside is that when
you want to remove something
from it, that's gonna
either invalidate everything
after it, or at least the last element
that might need to be
shifted into that spot.
And arrays are also
prone to the ABA problem.
So the way that a slot map works is
you have an array of data.
Right, so that's pretty straightforward.
You're gonna keep all of
your elements stored there
and, as you can see, they're packed
towards the front in contiguous memory.
And where the real magic
happens is in a slot's array.
So that's this guy.
It'll have these two values in each slot.
And one of them is an index,
and that index will go
into the data array.
And the other value is
a generation counter.
And that's how we solve the ABA problem.
So, as you can see, all of
these indices correspond
to some data that's being stored.
It also uses a free list mechanism
in order to have O1 inserts.
That way, it's able to find
a free slot in constant time.
And you'll note that the free list head
has an index head in it.
That index is set to the next
element in the free list,
so it's able to store it all in place.
So the way the insert works
is we have some element
that we want to insert that'll go
in the next empty slot available.
And so we're gonna put our data there
and then moving forward
we have to pop something
off the free list head.
That's gonna be the slot that corresponds
to our newly-inserted data.
And we generate a key
that points to that slot.
We have to match the
generation to make sure that
when we look later we'll
not have those ABA issues.
And then we update it
to point to our data.
And we can then return
our key to the user.
And the key will point to the
slot, not the data itself.
So there is that layer
of indirection there.
To erase something, you
give a key to the slot map
and it'll have an index and a generation.
So the first thing we're gonna
do is check the generation.
In our case here, the generations match.
And so we can go ahead and proceed.
So we're gonna increment
the generation to make sure
that we don't have any
issues with ABA going forward
right off the bat.
Then we can find the element
that it's pointing to
and go ahead and destruct it.
And then once that's destructed
we're gonna move the last
element into its slot.
Then we need to find the
entry in the slot's table
that corresponds to that
and update it to point
at the new address that
the moved element is in.
Then we need to go to our free list tail.
And what we're gonna do is add
the newly-vacated slot entry
and set the index of both of them
to point at the vacated slot.
So what this does is it
adds it to the free list
and the last element
will point to itself so
that it can detect that it's at the end.
Finally, we set the
free list tail to point
to that newly-freed slot.
And we're done.
So some of the trade-offs
of a slot map are
that the dynamic sizing can
cause memory usage spikes.
Especially in games, this can be a problem
where you have space
for a thousand elements
and then you want to go and grow it.
And typically that's gonna
be a 1.5 x or two x growth.
And so you not only need
the memory from before
but also the new memory,
which is a big jump.
Whereas something like a
hash map that's node-based,
you wouldn't have this sort of issue
since you can allocate
just the node you need.
So at those times it can
spike in memory usage,
but it's otherwise quite consistent.
Also, due to the dense
storage, the addresses
of the elements are unstable.
So the lookups are bit slower
than just using a raw pointer.
It also uses additional memory
to make some of its
performance guarantees,
which may be undesirable
in some circumstances.
So one of the common variations is
not using the index table.
You're able to save a
bit of memory by that
and have stable indices and pointers.
The way that it works is
you just store the slot data
right next to the data itself.
And so by doing that, you
don't have the indirection
in memory either.
So it trades faster lookup times
for worse iteration performance.
And also it has the downside
that the elements aren't densely stored,
which is what causes the
worse iteration performance.
Another variation is
using a fixed size array.
So this one will have constant
time insert in all cases
since it doesn't ever grow
and caching is constant time.
But the generations increase
roughly uniformly as well,
since they're all allocated from the start
and there's no memory usage spikes.
Unfortunately, dynamic
sizing is typically something
that we want to have,
and so this variation isn't
usually what we'd want.
And some of the edge cases
can cause the generation
to overflow quickly.
Another one is using block
allocations similar to a deck.
So that one also has the O1 insertion
and the allocation memory
usage spikes are a bit smaller
than using a vector-like backing
and the iteration performance can be tuned
to get pretty close to
just having a fixed,
single block of memory.
But the elements aren't
stored fully contiguously
because of the gaps in memory.
And the cache misses scale
inversely with the block size.
So the smaller your jumps in memory usage,
the worse the cache's performance is.
And the lookup has an an
additional indirection
since it also has to
find the correct block.
So the way that it works is
the keys are some sort of
decomposable pair of integers,
at least in the current
state of the proposal.
So the first is the index and
the second is the generation.
And the index from the index array is used
when the generation counters
have been detected to match
and then the keys can be
tuned a bit by the user
as a specified template parameter.
So the next data structure that
we're gonna talk about today
is a bloom filter.
Bloom filters are a really interesting
hashing base structure.
They use up a fixed amount of memory
and rather than hashing the element
and storing both the
element and some sort of key
that's used to find it using the hash,
it's a probablistic data structure,
which means that it doesn't
always guarantee correctness,
but it uses a lot less
memory and gets pretty close.
And you can tune your error
percentage with these.
So the probabilistic part
is that sometimes it'll say
an element is in the set
when it really isn't.
But it doesn't actually
contain any of the elements,
it's just a bunch of bits.
So it only supports find
and insert operations,
you can't remove things from it.
And the only options that
you can get from a find are
that it's definitely not there,
or if it says that it is there,
it could be incorrect and
the data's actually not.
And so that's where the
probabilistic part comes in.
So the problem being solved
is typically when you want
to check a set for the
membership of an element.
So you have some data and you want
to see if it's in your set.
And bloom filters are very
efficient with memory.
They use far less than any
sort of set representation
you would have that actually
stores the elements.
It's like several orders
of magnitude better.
And the trade-off here is that
you have to be okay with it
probably being right and not
always being able to guarantee
that it's correct.
So the guarantees that it does make are
that it'll have constant
time for both insert
and lookup operations and
its memory usage is constant
and none of the actual
elements are stored.
And that's kind of an
interesting property,
that the elements aren't stored,
because it means they
can't be reconstructed.
And it's privacy-friendly,
so if you want to keep track
of some user data without
actually having the user data,
this is a cool way to do it.
So the most common alternative
would be something like a
hash set where there's
only one hash per lookup,
which is typically gonna be
better than a bloom filter.
But it's gonna use more memory
since it has to store all
of the elements.
And they're generally not contiguous
since they're typically
implemented as a red-black tree.
And it also contains the elements
themselves in a hash set,
which in some cases
could be quite desirable.
In others it might not,
such as the privacy cases.
So the places you can find a
bloom filter are in things like
website safety, email contacts
lists, newsfeed pruning type
of head search for looking
for your friends on like
Facebook and even as an early
out for expensive queries.
So the way that some of
these work is that say,
for example with the early out example,
you would have a bloom filter
representing your files
that you have on disk perhaps.
And before doing the
expensive file OI operation
to read from disk, what you can
do is query the bloom filter
to see if the element is there,
and if it says no you can be 100% certain
that the element really isn't on disk.
And if it says yes, there's
a pretty good chance
that it is there and then you can go ahead
and take that more
expensive file IO operation.
So the way that a bloom
filter works under the hood,
let's start with this storage.
It's actually a bit array
and as I've said before
it doesn't have the actual elements.
So that looks something like this.
We have our bits,
zero through 23 for a total of 24 bits.
And these are gonna get set
by various hash functions
whenever you insert an element.
And the way that hashing works is
that you'll have K hash functions,
and K is some user-specified amount based
on the margin of error you want.
And then each hash will
set one bit based on
whatever element you're inserting.
And so for these, you
prefer speed over security
for your hash functions.
Since you're not storing
your data anyway, it's not
that important that they be
cryptographically secure.
So the hashing, we have
our two nice little
hash functions here too.
I chose to give them cute
names like FNV-1a and Murmur3.
So to insert something, say auto, it goes
through those hash functions
and you get some sort of value out.
And then you find the bits that
correspond to those values,
so at those indexes, and you
go ahead and set them to one.
And so I have this little
set on the right side here,
it's not a real set, but
we're just gonna keep it there
to keep track of the elements
we've added for later.
So let's add a couple more elements.
We can put in a struct
as a string, template,
class, and constexpr.
And so the way that lookups work is
that they're similar to insert.
You also take it and run it
through the hash functions.
But rather than setting bits,
you check that the bits are set.
So let's say we want to look up constexpr,
which is one that's in there.
We get our bits and we check them.
And both of them are set to one.
So they're found.
And that means that it's
probably in the bloom filter.
It was probably inserted at some time,
but again were not 100% sure.
But in this case it happens to be true.
Another one would be var.
So we can look that up and
we get a couple of bits.
And we check them and one of them's set,
but the other is set to zero.
So we know for sure that
that element's not in there
because if it had been inserted,
both of those bits would
have been set to one.
So another example is hello.
And so this one will
point to a couple of bits
that were both set to
one, but as you can see
in our little example set on the right,
it was never actually inserted.
So this is the case where
you have to watch out for
and be okay with if you're
gonna use a bloom filter,
because it can be incorrect at times.
So that's where the
probabilistic part comes in.
The false positives can happen,
but false negatives don't.
And the amount of these can be tuned.
So let's look at how that works.
If we have a number of hash functions, K,
then we can set that based
on the negative log two of P,
where P is the desired
false probability rate.
The other thing that
you might want to tune
is the number of bits or
otherwise the memory footprint
of your bloom filter.
So, in this case, M will
represent the number of bits.
N is the number of elements inserted
and P is, again, your desired
false probability rate.
So that's the equation for figuring out
how many bits you want.
As a small disclosure,
I'm not really a math guy
and I just kind of took
the math from Wikipedia,
but it's probably right.
(laughs)
So the takeaways from
a bloom filter are that
if you're willing to accept some error,
you can save a lot of memory.
It's also able to store data
without actually storing the data,
which is a kind of cool way
to think of a data structure
and not something I've
seen in many other places.
So the next adventure we want
to go on is navigation meshes.
And it's a bit of an adventure
because they represent
how you find things.
They work well with
A-star search algorithms
as a representation of the search space.
So a navigation mesh is a graph
and it works for 2D or 3D.
And, in general, it
will produce fewer nodes
than other representations of
a two or three-dimensional,
traversable search space.
So the problem being solved is
that you have some search
space you want to represent
and you you need to know which
areas of it can be traversed
and which can't and probably
want to pathfind around it,
typically in like a game or something.
So some of the other
representations we might have
for this are a grid.
Right, we can look at a grid
with some example map here
and you can see that
there's a lot of nodes
and also the edges are quite jagged.
So it doesn't really
smoothly fit the walls
and so if we want to pathfind
from that little nook
in the bottom-left on
the right side of the map
to the nook in the bottom-right
corner on the left side
of the map, like so,
the path takes 17 nodes
and there's 370 total nodes
required to represent this map.
So that's not great
and we can probably do
a little bit better.
So if we look at a hex
grid representation,
there's also a lot of nodes
and it doesn't handle the
walls and corners very well.
But at least this one,
it has equidistant nodes.
So even if you're moving
diagonally or whatever,
the cost for moving from one
node to the next is equal.
But it also has some issues with
not perfectly representing
the search space.
So if we run that same search again,
we can find that nine nodes are used,
which is a lot better than before.
And 85 total nodes is
also quite a bit better.
But I still think that we can do better.
So let's look at what a nav mesh can do.
This is a triangle-based nav mesh,
so all of the nodes in
this graph are triangles.
And, as you can see, it
uses a lot less nodes
than any of the previous representations.
It smoothly handles the walls and corners
and it's a pretty tight
representation of the search space.
But there are some oddities like those
little skewed triangles
that are really thin
and don't do a lot.
So our path this time is seven nodes,
which is only a couple less than before.
But that's also one of the worst cases
for this particular map's
triangle representation.
And the 16 total nodes is a lot better.
So in your worst case
search, you're gonna come out
a lot further ahead than
you would have otherwise.
Finally, we can look at a triangle
and quad-based nav mesh representation.
So in this there's very few nodes.
The walls and corners
are all smoothly handled.
It's got a pretty clean layout.
It tightly represents the search space
and, as an added bonus, all of
that expensive triangle math
is mostly avoided with just one triangle
on this representation.
So our search example
here, we only have to cross
three nodes to get from start to finish.
That's a huge improvement from our start.
As well as the nine total nodes.
So for this particular
example, it was one order
of magnitude better for the search
from the original grid representation
and two orders of magnitude better
for the total search space representation.
So that's a big improvement.
So the big takeaways from
the navigation mesh are
that it represents the data in the way
that it exists better.
And so that's something
that we can keep in mind
as we build solutions to our own problems.
Just because there might
be a common representation
for similar problems
doesn't necessarily mean
it's the best way to do it.
And in this case, we're able
to make each node store a lot more data.
Rather than having to have
each individual grid cell
connected with all those edges, we're able
to make much larger ones that
cover a big swath of area
that's easily traversable
with no obstacles between it.
I'd also like to talk about hash pointers.
These are another really
cool data structure.
So they're from the name you
might have guessed, a pointer,
but what they also do is store a hash
of what they're pointing at.
And so, because of this,
you're able to verify the data
is the same state that it was
in when you first put it there
or got your pointer to it.
And it's pretty tamper-resistant.
The problem being solved
by a hash pointer is
that you need to locate
your data and you want
to verify that it hasn't been changed
since you got your pointer to it.
So this is a really useful tool
and it's found in a few
places, mainly in Block Chain
and Merkle Trees, which
are the two data structures
that are derived from it.
And both of those are
common in cryptocurrency,
which has become quite popular these days.
The way that it works is
you'll have the pointer
to the data and the hash of the data.
As you can see in this example
here, pretty straightforward.
So for a block chain, one of
the first derivatives, we have
the hash pointer as well as some data
that makes up that block.
And each of these blocks
has a hash pointer
to the previous block.
And so using this technique,
you can make something
like a secure log or cryptocurrency,
which logs the transactions.
So the way that tamper
detection works is, say you have
some case where a black hat wants money
and they want to go alter the block chain
to give themself more
money in a transaction
that never actually happened.
And so ideally they would
just go in and change history
to add that transaction.
And all of a sudden, they're really rich.
Sounds great, right?
Not for everybody else.
So the way that we can figure
out how this is solved is
actually just diving and
seeing what it looks like
if someone were to try to
modify a value in a block chain.
So the first thing that
we're gonna do is assume
that this data has been
modified in some way.
So we're gonna represent that with red.
So, seems straightforward.
In any other data structure
this would probably
be a problem, but not with
block chains and hash pointers.
The hash for the next block
has now been invalidated
since the data's changed.
And so that means that we can detect
that the data was tampered with.
So that works and it only
took one block to do so.
But let's say we have a really, really
persistent hacker here.
And they want to tamper with the hash too.
So let's see what happens
when they do that.
They go in and they tamper with the hash
and now are hash pointer is valid again.
But oh wait, it's all better, right?
They've got their money.
But not so fast.
There's the hash of the next block.
And so that one has now
been invalidated because
those hash pointers are stored
within the blocks themselves
and so the hash for the next
item down the chain includes
the hash for the previous one.
So in order to fix this,
they'd have to change
the next hash as well.
So let's say they do that and they run
into the same problem
again with the newest item
on the block chain.
Again, the hash has been
invalid and it will no longer be
an accurate representation
of the previous block.
So with block chains,
tampering is really hard
and every downstream
block must be modified
in order to have a successful attack.
The downside of this is
that detecting these attacks
is an O(N) operation.
You have to go through all of
the entries in the block chain
after it in order to detect it correctly.
So there's another approach,
which is a Merkle tree.
This is a tree-based data
structure made by Ralph Merkle
that also is based on hash pointers.
And in this one, only
the leaves hold data.
All of the nodes up the tree
simply contain hash pointers.
So this is roughly what a
Merkle tree would look like.
At the top you have a
couple of hash pointers
for the left and right side
and then each of those will
also have hash pointers.
And then down at the bottom
you have just the data.
And so to detect tampers in a Merkle tree,
you have to traverse to
the element from the start
of the tree, which is
O(log(N)), since it's a tree.
So that's a bit better as
far as tamper detection goes
and it's easier to detect that
your data's been modified.
But if you're a hacker
and you want to go in
and modify the data and not get caught,
you only have to modify
the things up the tree
from what you're attacking
rather than having
to modify every single block in the chain.
So the takeaways from hash pointer are
that tamper detection gets
really easy if you use them
because now a hacker has to do
all of this work changing all
of the data all over this structure,
whereas you just have to
follow it and see that all
of the hashes work still.
It also means that the hashes can be used
for more than just finding data,
but you can actually use them
to make sure your data
is unchanged as well.
And so this can have
practical applications
in all sorts of other domains as well.
So that concludes the
containers that I'm here
to talk to you about.
So if you have questions or comments,
please step up to the mics.
Yes.
(applause)
Thank you.
- [Audience Member] I have a question
about the navigation math.
- Um-hmm.
Can you step up to a mic so
they get it on the video?
Thanks.
- [Audience Member] Hi.
So about the navigation mesh.
Can you please explain a bit
more about when it's useful
and how it works because I--
- Yeah, so it's a graph representation
just like a grid would
be or any other graph
that you might be used
to with nodes and edges
and it's used typically in
games for two-dimensional
and three-dimensional
search space representation
for pathfinding problems.
So you have a character that
wants to go from point A
to point B and there's only
certain areas of the map
that they can walk on
and those areas are
connected in some ways.
The navigation mesh represents those
and you're gonna typically
use an A-star search
to get from point A to point B.
- [Audience Member] Okay.
Okay.
I don't really understand the
actual implementation though,
like the other, the other
structures you mentioned.
So it's--
- What specifically do you mean?
- [Audience Member] Nevermind.
I'll talk to you later.
- Okay.
Yeah, you're welcome.
We can talk after.
- [Audience Member] So
for the bloom filters,
is there a concept of delete
and, if so, how does that work?
- No, bloom filters don't support delete,
but there are some other
derived-types of data structures
that use a similar technique,
and some of them do.
- [Audience Member] Okay.
Cool.
And do they just keep another structure
on top of that that--
- Yeah, typically they're
gonna use more memory
and that's how they're able
to do it like, for example,
a Count-Min sketch will
have multiple arrays
of the bits rather than just a single one
and each hash gets its own array.
- [Audience Member] Okay.
Cool, thank you.
- Yup.
- [Audience Member] Is
there a well-known algorithm
for the navigation mesh for decomposing
the unblocked polygon into
rectangles and triangles
that produces that graph?
- Yeah, so the most
commonly used approach is
through a software package called Recast.
It's open-source available on GitHub.
It's used by both Unity
and on real engines.
And so typically anyone going
with a navigation mesh would use that.
I don't know all of the details
of how the meshes constructed by it,
but I know one approach is
something like a flood-fill.
- [Audience Member] Thank you.
- You're welcome.
- [Audience Member] This is just a comment
about hash pointers.
Possibly a more commonly-known
example is git revisions.
- Sorry, what?
- [Audience Member] Git revisions.
- Oh, git revisions.
- [Audience Member] Git
revisions are in themselves,
each git revision is a
hash of its entire log
all the way back to the
very initial comment
that created the repository.
So it's pretty much on the same idea.
- Yeah.
- [Audience Member] So on
a navigation map it seems
that your area just gets bigger and bigger
so you don't really have
much detail on the path.
I mean I don't understand
in terms of the trade-offs
on knowing what kind of path,
how much path do you need to know?
- So the way that the
navigation mesh works is
each node will only represent
traversable terrain,
and so you don't actually
have to represent terrain
that can't be traversed
because there simply
won't be a node for it.
- [Audience Member] Okay, thank you.
So it's all a matter of the application,
how much area in each node?
- Correct, yeah.
And then you just go between
the edges of the nodes
to find where you can go
from your current position.
- [Audience Member] Okay, thanks.
- Yup.
Looks like there's no more questions.
Thank you all for coming.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>