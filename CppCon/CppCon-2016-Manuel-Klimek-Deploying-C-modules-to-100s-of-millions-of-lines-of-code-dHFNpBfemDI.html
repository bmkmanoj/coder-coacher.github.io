<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Manuel Klimek “Deploying C++ modules to 100s of millions of lines of code&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Manuel Klimek “Deploying C++ modules to 100s of millions of lines of code&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Manuel Klimek “Deploying C++ modules to 100s of millions of lines of code&quot;</b></h2><h5 class="post__date">2016-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dHFNpBfemDI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello good morning I hope you all awake
my name is Manu Klimek and I work for
Google on C++ tools and generally on
making C++ easier to develop and today
I'll talk about how we rolled out C++
modules to our hundreds of millions of
lines of code code base overall this
will be the story of how we went from
this graph to this graph and how that
made us both both very happy and very
sad but before I tell you that story
you'll need to learn a bit about how we
develop code at Google and Google we
have a very large continuously
integrated code base most of our
development is done in that code base
for C++ we have a couple of hundred
million lines of code in there that's
handwritten and we have roughly the same
amount of generated code it's
continuously integrated that means that
if you submit a change everybody in the
whole company will immediately see that
change and to be able to develop that
way of course we need to run all the
builds and all the tests of all the
transitive rippers dependencies of your
code and to do that you can obviously
not do that on your single machine every
time you submit something so in our data
centers we have a lot of machines that
do nothing but compile code and run
tests they also catch the results so if
you compile again you basically get the
cached results back we've published a
couple of papers on this so if you if
you search for google build system you
will find more details on the internets
and if you look at the amount of
generated code right we have roughly the
same amount of generated code as we have
hand written code that might seem like a
lot of code but it turns out that this
is mostly one kind of generated code and
that's protocol buffers who here has
heard of protocol buffers okay so
protocol buffers are how our servers
come
with each other and so they're really
important in general it's just a simple
data description language right in this
example you have a message that has one
field bar and the protocol compiler
compiles that into a C++ header file
that gives you getters and setters and
we use protocol compilers at the
protocol buffers all over all right they
are the main dependencies and now let's
look in a more low-level how we build
stuff so our build system has evolved
out of a make fire based system so
basically originally we had a single Meg
file at the root of this large
continuously integrated tree and it
turns out that doesn't scale right so at
some point we thought well we'll build
our own build system that is now open
source it's called basil you can use it
and I'll explain a bit about how the
syntax works how we describe our builds
so it's very Python II all right you can
see here and you see an example of a
couple of libraries right you have a
proto library named P it has a source P
dot proto right that what we generate
the c++ source code from and then we
have two other libraries a library a we
see that we specify headers that's kind
of interesting because many C++ build
systems don't specify their headers we
do that because it has turned out that
letting everybody just include Harris
from all over the codebase is a bad idea
right did people then basically use the
implementation details of other people's
libraries and they cannot change it
anymore
and we of course specify the sources
that those are the translation units
that are compiled right and then next we
specify the dependencies on a high level
here CCC library P has dependencies on
the library a and the program of a
library P and if you look at how this
then gets compiled let's just take a
translation unit out of that
so here we have P dot CCE and let's say
P dot CC includes a one dot H P dot H
other dot H and P dot p bh and now when
we want to compile this we need to ship
the source code to our distributed
builder so the question is what source
close to we need to ship well the first
idea is we just ship everything that's
specified in the headers of the
transitive dependencies and the source
code itself while in this case we have p
dot c c metered age a1 dot h a2 dot H
and P dot P B - now the first thing
you'll notice is that a - dot H is
actually not used so that that seems
wasteful we want to get rid of that and
so we included a local optimization
that's called include scale so the build
system actually follows the includes
through all the files right it has a
preprocessor implementation and figures
out what headers a source file actually
needs so it figures out a 2 dot H is
actually not needed so it's not an input
to this action so we don't need to send
it to the remote execute additionally
you see that P dot CC includes other
knowledge so the interesting part is why
we would like that all headers and the
code base are fully specified they're
currently not we are working on that
it's a multi-year effort as long as
that's not done we need to also detect
additional headers we have to set so
this is basically the core of how our
build system works now we want to roll
out modules so when we say modules here
what do we mean with that this was
multiple years ago right what we wanted
to do was we wanted to take clanks
module implementation at the time and we
knew that at some point modules would be
standardized but we had no idea what
that would look like so we said we do
not want to make any changes to syntax
in our code that means right we want to
still spell out the hash includes hash
includes and but we what we would want
to allow is semantics
because we had we had people previously
trying to do precompiled headers and
most of the time they ran into problems
when they wanted to keep the semantics
of C++ with precompiled headers and that
basically took away all the upsides so
we allow changes and semantics we do not
allow changes in syntax and now we have
a bit of background of how we build code
how our build system works and what kind
of modules we actually want to address
but the question stands why do we did we
want to do that so why not just wait
until modules is standardized right and
then implement it that seems lower risk
but we know that for a code base of
multiple 100 million lines of code what
you actually need to do is you need to
start early because it takes many many
years to make a large change to the code
base so we wanted to start early we also
wanted to start early because we want to
gain implementation in size right we
have people working in our C++ team who
are also parts of the sentence committee
and they wanted to be able to contribute
that inside from our implementation from
our experiments back to the
standardization process so we make sure
that when modules are standardized it's
not standardized in a way that precludes
us from getting the benefits that we
need for a large C++ code base and more
importantly why we wanted to start was
that we had a really bad problem and to
get an look at just how bad it got you
can look at this graph though this graph
started back in 2013 when we started to
look into why our build second a clause
so we had this this intuition something
is getting slower what's actually
happening here and I did some log
shuffling and a foul that some people
actually change a test file and then
they run their compiled and then it
takes one minute for the compiler to get
back to them with value forget a
semicolon here and at that point I was
like why are they not coming this
pitchforks is at us that seems like
really bad for activity and the problem
is it's a bit of a boiling frog problem
right
it just gets slowly worse over time and
the big problem that we see here is this
graph goes up to the right that this is
super Linea we really like those graphs
when they mean money earned we don't
like them when it's cost right and if
you explain to people who provision for
the machines you need for building
things that well our our requirements
for building our code base actually
super linear increase they're like well
that's that's not something we can
continue to do so we wanted to address
this problem to address this problem we
first had to understand what's actually
going on here
so we looked into why those things
happened the first idea we had was well
perhaps optimizations getting slow has
nothing do with optimization
perhaps the compiler circuit is law in
fact no the compilers had been getting
faster over time per line of code what
really has happened is that the
transitive closure of includes
that we expand into the pre process file
grew super linearly so we had overall we
had translation units that turned out to
be more than 10 million lines of code
after preprocessor so where does this
come from right we already knew that we
have lots of generated code we have all
these problems and now these protocol
buffers have some interesting properties
first the example I showed you
previously was a simplification right if
you have a very simple message with a
single field the protocol buffer
actually expands into roughly what you
can see here right
we have many overloads for the setters
we have many more like site methods we
have actually what's missing here's all
the reflection parts that also get
generated and basically you see two
parts of
the protocol before five five right you
see the declarations of all the methods
we need for the fields and then we see
the inline definitions and we have the
inline definitions because because we
needed to be fast right it's what
connects our servers we need this path
to be super fast this is actually the
hot path in our code additionally we
have the problem that if a protocol
buffer depends on a different protocol
buffers type it would just directly hash
include' that header so this actually
just transitively expands all
dependencies of a protocol buffer every
time you use it
so the first idea we had was well this
is easy to solve right we seems like in
in up mode we need those inline
functions but in debug mode you probably
don't need them so we just left them out
the the preprocessor is actually really
good at just jumping over stuff you
don't need and we implement that and our
graph changed so it gave us some relief
but that only lasted for a very short
time because we did not actually address
the underlying problem right after that
change again the super linear growth
just persisted so how do we address the
root cause here there are two possible
ways to address the root cause the first
is well this is a protocol buffer
problem let's address in the compiler
right we can hack up the proto compiler
to work around the fact that it needs
this those transitive includes the
problem with that approach is obviously
that will only help us for protocol for
the protocol buffers we also have a lot
of code and a increasing number of lines
of code for the rest of the code base so
we thought instead of having the proto
compiler and actually figuring out how
to make it not need the types of its
direct dependencies right you can do
template magic and stuff what if we just
change how we compile code on a more
fundamental level right and they are the
idea of Moore has come
or busy it store the ast the idea to
make that actually scale is met you
lazily load the symbols from the module
file if you want more details on all the
technical parts you will have to go to
Richard's talk which will also talk more
about that how this all applies to the
TF in the afternoon and in this talk I
will continue to talk about how we
implement that in the code base so the
first thing because before we wanted to
roll that out is we wanted to make sure
that we actually have an understanding
of what would happen right we didn't
want to just go and do it because we
knew it's a lot of effort so let's
predict what happens to predict what
happens we have to look at how we how we
think modules will change the
fundamental compilation and let's look
at an example right you have a couple of
translation unit t1 to t4 at CC and they
all have the include the same chain of
headers right C dot H includes P dotage
includes edit H now in the current model
in the old model what happens is that
you distribute all of them to build
workers and each of the build workers
will in parallel just take one of the
source files parse a dot H parse P dot H
Bar C dot H and the translation unit and
compile the output so the nice property
of this is that you can do all of this
in parallel and the problem you see is
that we do lot of work right every green
box here means that something needs to
be parsed so we we actually spend a lot
of time reap arcing the same headers
over and over and over again with
modules this will look different right
with modules we will take a header in
this example a dot H and we compile what
we call a header module that's the PCM
file in clang landfill right so a dot H
will be compiled into a dot PCM and then
a dot PCM and B dot H will be the input
for the compilation of P dot H and
repeater PCM
and then we compile Peachy PCM and all
of these then are the input to the
compilation of the translation units and
those again can have min Pro so if you
know about distributed systems what we
what we did was basically we introduced
a larger serial step right in theory
this shouldn't actually make a
difference right because we actually do
the same amount of work in just serially
before we do the parallel steps but in
practice in a distributed system you
always have communication overhead right
you always have some latency when you
when for example a dot PCM is compiled
on a different machine than the t4 dot
CC so we expect some overhead here but
we don't expect that overhead to be like
significant on the other hand for the
single translation unit recompile case
so let's take the example where somebody
just edits a test so for example 200 cc
and they just make write they just add
some new lines of code in the old system
what would happen is let people to
recompile a B and C reparse a B and C to
compile tierod CC and with modules now
all we need to do is we need to reparse
g1 not CC and take the modules as input
so here we expect some serious speed so
to conclude with modules we expect some
longer critical path we don't exactly
know how that will turn out we expect it
to be not too bad we expect a lot of
speed up for incremental compiles right
that was what our original problem also
was and also we expect less CPU use of
all because if you remember right the
parallel compilations all the headers
get recompile all the time in the old
model so we don't do that we save some
CPU hopefully so now we want to roll
that out over all of our code the
question is how do we start if you want
to make such a large change you want to
find something small where you get
for with very little effort get a lot of
benefit right because your team needs to
actually pay for itself over time you
cannot just for ten years develop
something that hopefully has impact at
the end so you need some incremental
impact so turns out that protocol
buffers as you probably expected by now
are the perfect example again right
it's the largest problem we found and
it's also a very very small problem
comparable because if we want to change
our protocol buffers right if there are
semantic changes that require us to
change protocol buffers we just change
the generator we don't have to actually
go all over the codebase and change
called code all over the codebase
because that's actually a lot harder so
cool let's start with protocol buffers
how do we how do we implement that with
clang so the trick is we don't want to
make any changes to the syntax clang has
this idea of a module map that you can
give it in addition to the source code
you give it and that basic describes
what the C++ module will look like if
you have an example of a proto library
right with a source P dot proto that
will generator had a p dot p b dot h the
module looks roughly like this right you
have it says header header means clang
this will be a header that i want in
that module the dependencies are
specified by saying use here right we
use a and everything uses the STL of
course and then we have the interesting
part that's the export star so when we
roll out modules we want to make as few
changes to the code base as we possibly
can
so we tried to have the semantic model
match as closely to the old like include
model and if you include something you
can use everything in it so we do expert
star which tests clang well everything
in that header is exported so somebody
who uses that module can also use all
symbols that are in that header
now one interesting part is that the
protocol buffer headers also include
other headers right they obviously
include some stuff from the STL but they
also include their own utility libraries
for example every protocol buffer header
has a every protocol buffer has a base
class so every protocol buffer header
includes the header of that base class
and we thought at first that well people
don't want to go all over the codebase
and have to modernize everything so we
introduced the idea of textual headers
and we introduced that in the clang
module map so that you can basically
declare a module message so that's not
really a module because it will not
compile any PCM file out of it because
the header in it is texture and that
means if clang C is in a when you
compile a module it sees an include of
that header it will just include that
textually now the interesting part is
that because of the export star all
symbols from that header will also need
to be exported so overall let's look at
when we made those changes do we have to
make any changes because we expect the
semantics to be different so generally
with modules the core problem is that
every header must stand on its own right
it cannot be different depending on
where it's used from we will also we
will also detect some more audio
violations and we thought well over the
code paste do we have audio violations
in protocol buffers perhaps a few right
probably not too many we just fix them
as we go so it seems like we all set up
right the protocol buffer headers are a
very simple world they stand on their
own so let's roll that out so we started
implementing that in our build system
and we started
building code in our Google cope with
that as you expect not all things went
as we expected so the first thing that
happened was that module compiles for
twice as low as non molecule PI's that's
unexpected and it turned out that the
problem here is that when we developed
modules we always tried it on small kind
of examples and then when we went in and
started to use it at Google scale it
turns out that we find some algorithmic
problems that we didn't predict so we
have many translation units that depend
transitively on thousands of protocol
buffer files here we have an example
right we have a t1 dot C C and that just
depends on a lot of protocol buffer
modules and all those program modules or
the the headers in those modules include
the base class header right they include
the message hood age which provides the
class message so as we said earlier
because of the export star every one of
those header modules actually contains
that class now when we compile t1 and CC
and we load a protocol buffer we need to
actually go through all the modules and
figure out that this exported symbol is
the same in every header module and this
is a thousand modules right so you have
Mexican Oh n in there that you have to
do for every such symbol that you import
now we don't only have the message to at
age right the protocol buffer headers
also includes the STL for example so in
the early days where we hadn't
modernized VHDL what actually happened
was that we put the STL into every one
of those modules and when we started
merging them clang ran over and in 32
for the number of pre-processed tokens
that we found and crashed
well but that seems like an easy problem
to fix
we just modernize the dependencies that
we use from every protocol buffer and
then we have the symbol in one place and
everything's fine the problem is with
this that this also means that we need
to go now and actually modernize larger
parts of the copis the protocol buffers
actually have a few quite a few
dependencies and those have dependencies
and overall you basically need the first
layer that every protocol buffer
includes to be modular to work against
that pessimism okay so what we did was
we went over the code base and we
started actually trying to compile
normal code non-programmable code as
modules and there you run into the
problems you expect with the semantic
changes you would get from modules all
right the simplest example that actually
happens a lot is somebody has a header
that had a declare some function
something else some other header uses
fat and does not include the first
header this of course works fine with
the traditional C++ module where
everything if you just happen to include
the files in the right order everything
works fine right but here P dot H does
not actually compile on its own so when
we try to compile it as a module we get
a compile error so we went and started
fixing them basically all over the
codebase what else happens we have we
have some people who really dislike
having any implementation details in
their head so what they came up with was
the idea that well we also want really
performant code so we we want our inline
methods to be in the header but we don't
want them in the header that a user of
my library would look at so they define
a class and then they at the end of the
header
they just hash include' an impelled or H
and that will give you all the or the
inline methods the problem with that
approach right again the input
- does actually not compile on its own
if you if you happen to follow the
Google style guide actually a while ago
we change the style guide to discourage
at least those implement sets and to say
that every header should stand on its
own another thing that's interesting
that breaks is C code so the problem is
that we have a lot of third-party code
and some third-party code is C only code
where the authors don't really care that
their code might be used from C++ so
they don't want to sprinkle if the C++
extra and C all over the headers right
so what people do is they put the extern
see around the hash include' now this is
a hash include' that is hard to put to
make into a modular use automatically so
we have some workarounds but within our
own build system we don't have a really
good solution yet mainly because our the
names for c and c++ headers are
basically the same right both have the
dot h any and so as long as our build
system doesn't allow us to specify but
this RC harris and these are c++ headers
we don't actually know which are which
without like because they are compatible
most then we come into the more akane
things that break when you do that so
clang thought that using under and a
module under under wild compiles a
module is a good idea and apparently the
arm C++ compiler Forks thought they use
that for something else so we have some
third-party code in the code base that
does weird things when it encounters
that define and that leads to random
crashes another interesting story what
happened is that we pass all the module
maps that describe to clang how we want
to compile the modules on the command
line and it turns out that these are so
many more Maps again this is like just
the huge scale of the Google codebase
we actually broke the expectation of our
build system on what the maximum command
line length is what we did to work
around that again is that we then ran
some graph algorithm on our dependency
graph to only pass the top modules on
the command line but running those graph
algorithm to take an expensive so that
actually makes your build slower so we
were basically fighting a bit with the
build system yet and finally one of the
core things if you implement modules in
your codebase is that you have to think
about your configuration and your
configuration management what do I mean
with that everybody basically uses a
couple of macros to switch between
different configurations for their
project write an open source project
that's often a conflict on edge that
gets generated and with modules you
cannot mix these configurations anymore
right one of the core things is debug
versus opt and for example the assert
library behaves differently whether you
have the an assert and debug macro
defined or not so code that actually has
a header that uses this library right
for example with an assert behaves
differently when compiled with modules
or without modules when people try to
actually manually define that macro so
this code actually will give you
currently with clang it will give you
different results but the a dot H comes
from a module that was compiled without
the define or whether it's included
naturally overall the problem here is
that if you think about it you need a
different header module for every
possible combination of configuration
flags you have that is mostly not
actually as much as you would fear it
might be right most code bases have a
handful or if they are very large code
bases they have might tens of different
configurations so the overhead
not that bad for now we actually decided
that when you built you select one main
configurations and that's what we built
the header modules for and if any of the
build files define some different
combination or configurations we will
just not build the header modules we
will just continue to do textual
inclusion so now we have implemented
modules for the protocol buffer headers
and we went over the code base and fixed
all the problems our distributed build
system now nicely supports modules we
know that how we handle all the
configurations so let's let's flip the
switch and see what happens first let's
look back at the graph where we were at
when we started with this whole thing
and this is what it looked like the
problem is that this was now a couple of
years ago and things had happened in the
meantime
specifically the graph now looks like
this things had improved why did they
improve well the problem was so bad that
a different set of people actually went
and fixed the problem in the protocol
compiler like what I told you in the
beginning one possible solution was to
try to make the protocol compiler
generated files not need all the
includes and still have the performance
of all the inline functions so we used a
couple of template tricks to actually
allow this and then migrated the whole
code base to that model and that got us
a lot of benefit so now let's see what
happens is in this situation we switch
on loads right at this point we were
like well for we didn't expect that much
let's see what happens well we were
pretty much right right boss some
all increase with a 10% further decrease
of the average compile times nothing
major but overall that's actually pretty
nice because that 10% decrease in
average compile times comes from a huge
improvement in the 99th percentile and
the 99th percentile is what's what was
really bad right that's the I weighed
one minute to get back that my semicolon
is missing so that's pretty cool but one
of the things we really had hoped for
was that the load of our distributed
system would decrease and we actually
told the maintenance of our distributed
build system with virtual modules will
make your life easier and then this
happened we actually doubled the load so
what has happened we looked into that
and turns out that we were just
compiling more so on average we had a
10% improvement and that translated to
more than twice the number of
translation units that we compile on
average day so we have twice the amount
of load where does this come from so to
understand that we have to look into
some more detail in like some
idiosyncrasies of how our bit looks so
we look at a more complex example here
I'll explain it quickly right we have
three libraries the LEP Lipsey Lipsy
depends on the P the B depends on the
Bay
each of them has to had us write a1 a2
b1 b2 c1 0 and it has very sparse
dependencies actually right so we have a
translation unit C dot c c that depends
on c2 and the header c2 dot h just
depends on like includes they had a bit
would be do dot H and not neither b1 or
b2 dot h include anything from the lip a
and the delay dependency from the b2 lip
a exists because there is a translation
unit in Lippe that actually includes a
header from Lib a and that's very
typical of our corpus
now if you remember in the traditional
model when we compile this what would
happen is that you to include scanning
we would figure out that C dot C C only
includes 0 C 2 dot H and P 2 dot H we
would only send those to the to the
worker only those are the inputs but now
with our naive modules implementation
what happens is that the number of
inputs actually increases a lot so now
we have all the header modules and the
header files as input and that means two
things first the time it takes to stage
or the inputs to the remote machine
actually increases a lot so we had this
10% improvement in average compile time
we had actually a much higher
improvement in average compile time but
that got eaten up basically nearly fully
by the time it takes to just stage all
the additional inputs like the the
header modules to the machines but more
importantly we now have more inputs and
if an input changes you need to
recompile your translation unit so
previously right only if C 2 or B 2
change we need to recompile seed or CC
but now also if a1 dot H changes we need
to recompile cc and that's really where
this huge amount in new recompile
culture right because we have a lot of
paste libraries that very frequently
change and now we just recompile the
world over and over again the solution
to that is basically translate the idea
of include scanning 2 modules and the
ideas that we do include scanning so we
follow locally follow the set of
includes that we find and every time we
find an a header that is part of a
module we mark that module as an input
and all other modules are not inputs so
in this case we can actually see that
lip a module a dot PCM do not feel use
we don't need to ship it it's not an
input this is something we currently are
in the process of rolling out we haven't
rolled it out yet but
results indicator that will actually get
most of the problem of the increased
load out of the way now if I if I do a
very simple modification to this we
still cannot solve that that they
include skinny and that is we have an
additional include from b1 dot H to a
one dot H now the problem is that due to
the way modules works the a naive
include scanning would not find that but
when we include scan from C dot C C 2 C
2 dot H 2 balloon of H we now have to
also include scan or the different
headers in that module right because the
compiler actually needs to know about
these headers so remember in the old
model we would just send Sidra - and B 2
to the H but now we would again send
everything what's the solution yet the
only solution here we can think of is
actually to make people split up
libraries because if you look at it the
headers in Libby are actually not
tightly coupled so other headers in
Lippitt right we have a complete
different dependency graph so we can
split up the libraries now we have three
more libraries each with their own
header and now we can we compile that as
module we actually have exactly the same
order of magnitude as input of inputs as
in the traditional model now spring of
libraries can sometimes be hard our goal
is to give people tools to do that so to
wrap it up the results we got were right
we had 50% better compile times up to
50% in the 99th percentile I think it
was like 30% on average in the 99th
visitor we had an average improvement of
10%
we had a large increase in the overall
load of the system but on the upside we
have unlocked
a lot of optimization potential why is
that right we have taken a lot of work
that the compiler has to do and that it
has no chance of not doing right it has
to recompile all the files it sees we
parse all the files it sees and we put
that into the build system right and now
in the build system we actually can pull
a lot of tricks to make that faster
that's something we were very good and
we have a lot of ideas how to like make
that significantly faster and also we
gain some insight into how modules
behaves what the corner cases are how to
actually migrate a codebase and in the
process we've actually got our code base
a lot closer to what we think it will be
once need to be once modules launches
once modules is in the standard so that
we can then migrate our code base to a
like standardized modules module very
quickly we have a couple of things that
we learned that worked very well and we
have a couple of things that didn't go
as well let's look at the stuff that
worked well right one very good thing
about our code base is that we have a
very strict style guide and people
actually mostly adhere to it and that
means that we had to fix things right
the semantic problems but the order of
magnitude of the fixes was tractable
right it was a couple of ten to ten
thousand fixes that's actually something
like that takes a couple of days of work
we have we have a lot of tools that help
us migrate our code base at large that
made it very easy and because of
completely different reasons our code
base already specified headers in the
build files and that's also interesting
because that helped us because
specifying all the headers in the build
files on its own is actually a very
large project and we had started to do
that for for different reasons because
we actually want to control better what
people can include a couple of years
before that so that was also very nice
we got lucky that
couple of things that didn't go as
expected right the performance
improvement our predictions were not
totally off but a couple of things
obviously when differently from how we
expected them turns out some broken code
was hard to fix and mostly that was not
the scale but for example when we
modernized the STL and STL mode an STL
implementation that's not written with
modules in mind seems to be quite hard
to modularize you have to talk with
Richard or Chandler about that they will
give you war stories modules makes the
code harder to distribute so we actually
had to put a lot of effort into making
that work with our distributed build
system right you saw most of the
problems we had were due to interactions
with the distributed businesses and also
we lived on the bleeding edge of clang
so we naturally ran into quite a bit of
clang bugs Richard also always nicely
quickly fix them but the Diagnostics if
you if you get Diagnostics for bucks in
your compiler that is often very
challenging and with that questions
so I may have missed it but I kept
waiting for you to show a big picture of
an elephant and say something about
templates you will need to come to
Richard's talk in the afternoon I think
ok like yes Richard implemented the
template merging and clang and that is a
story on its own can you speak a little
bit about what happened to link x ok can
I speak about link x so we had some
initial problems with link x mainly
because of static initializers that
showed up now in every module but we
fixed that and I think link times were
after that after the fixes link times
were pretty much unaffected so you
mentioned that you encourage your
developers to break up libraries into
smaller chunks aren't you concerned that
the number of libraries will explode
because of this and also that the the
break lines would become slightly
arbitrary along dependencies rather than
a long functionality of course these too
often align well but not always so the
question is if I understand it correctly
the idea to encourage developers to
split up libraries more might lead to
too many libraries or libraries that are
not well cohesive so generally we found
that developers tend to bundle too much
into a library and the promise that that
is also an incentive for you to write
bad code because within the library we
allow you to have cyclic dependencies
and that means you split out less like
you modernize your code less because you
just have those include cycles within
your library so you actually you
actually start creating larger and
larger blocks of things that are really
hard to split apart but often you
actually have things in in the
that you would want to split apart that
have completely different sets of
dependencies right and that's really
problematic because the set of
dependencies means that if you if you
have to if you have a library that has
too many dependencies will be recompiled
way too often and that's the case with
modules and without modules though so I
think for incentivizing people I
actually think smaller libraries make
for more modular command regarding the
number of libraries we've not seen that
our build system has any problems with
that
on the contrary because we have more
precise dependency chains it leads to
just less recompilation and less work
you have to do in the build system thank
you oh you've mentioned that this change
enables you to have loads of
optimization potential and what do we
actually have in mind
potential what so the question is I said
we have optimization potential so you
saw that for the overall load of the of
our build system it went up but if you
if you remember the the graph in the
distributed build system you in the non
modules case you reparse the the all the
same headers over and over and over
again so obviously there's potential to
get rid of like we've got rid of some of
that but we put that basically into the
recompilation of the same translation
unit to to dependency to to new
dependencies and if we can get rid of
those dependencies we reap both the
benefits of not recompiling the same
header as part of every translation unit
as well as then not recompiling the same
translation unit with modules because of
increased dependencies so that's where
we expect the the large decrease in
overall load and also another decrease
in compile time to come from also as you
have seen with this merging example the
more of your code base is motorized the
more benefits you get out of it a few
questions that I've got but maybe I'll
just stick it to
USU's unity builds at all or anything
like that like see files including other
C files so the question is whether we
use unity builds the answer is no we
generally don't believe in young cubits
the other question then would be do you
use shared libraries or dll's that might
make it difficult to split your
libraries like now the question is
whether be you shared lobbies we
actually do use currently few shared
libraries when we build tests and we
don't use shared libraries when we build
stuff to deploy I don't understand so
you mentioned that you think that that
might be harder to split up libraries I
don't understand why that would be so
the core idea about our build system is
that we always rebuild everything from
scratch so we build shared libraries but
we don't deploy them ever does that make
sense so splitting it up will just split
up the library into two shared libraries
I'm thinking the problems around exports
and imports and getting them correctly
around circular dependencies would be my
thoughts if you break the library at an
arbitrary point it might be difficult to
solve those so the problem is if you
have circular dependencies splitting up
libraries as how yeah that is true that
is true in general our build system does
not allow circular dependencies between
libraries like and it never did so
that's where why if you can split it in
our build system you actually solve that
problem Linux also allows circular
dependencies between shared libraries I
think so but anyway yes um do you have
any insights into removing inline and
using link time optimizations to offset
the performance cost in order to get
faster compile times so do we want to
remove in line and use link time
optimization so I'm not the expert on
that you have to ask Chandler for the
details but I think the high level is we
have no plan to remove in line but we do
actually develop link time optimizations
because they have a lot you mentioned
that there's analysis to understand
dependencies that are declared but not
actually used
feedback into some kind of tooling so
that you actually restructure your code
and don't declare you don't actually
have the include file for the module so
we have dependencies that are not really
used and the question is does that feel
fit in the tooling actually we are
currently building up tools to get that
dependency better under control because
partly because what we learned from the
modules experiment and we learned that
we really need to get our dependencies
in order so we are currently building up
tools to help you also split up
libraries and which dependencies you
want to delete that give you the most
benefit and things like that so like
clang include what you use is
insufficient for this task to acclaim
include what use is insufficient for
this task yes hey to expect more build
time improvements when modules become
standard or you think you already take
all the benefits do we expect more build
time improvements when modules becomes
standardized I think that's orthogonal I
expect more build time improvements in
the future all right
richard has put some effort into making
modules faster obviously but I think if
we prioritize making clanks clanks
module implementations faster that we
can get additional time improvements but
that is like what we know - how its
standardized I think well depends on
some of the standardization but that's
all open so I don't know yet right
thank you can you comment on other
potential ideas for build improvements
that were considered at Google let's say
you know can we figure out what can be
for our declared rather than you know
including a header file or how about
splitting header files into individual
concepts so that header file would and
have more than one class in it so that
then we can benefit from incremental
builds a little bit more because if
there are changes to the hair file it
would affect fewer CPP files that
dependence on those things that are in
the header file things like that
have you guys done any like big changes
like that across entire code base to
simplifies those things so the question
is how we split up libraries and the
question is what you described is
something that isn't applicable to many
companies because modules it is a
preparation for modules are there any
other efforts like the protobuf that you
described across entire google to speed
up the build times let's say you know
using for word declares is one you mean
but there are efforts that were not like
now we rolled out modules where the
efforts before that to speed up build
times without using modules right or in
parallel okay well in parallel as you
saw right we have actually got a lot of
benefit out of changing the protocol
compiler and we had some people
investigating for example forward
éclairs
the interesting part was that from the
start the forward declarations actually
have different problems of because the
problem is that they prevent you as a
library maintainer from changing types
without changing all the code so we
actually never went very far into trying
to force forward declares all over the
codebase we have them obviously because
of performance but we hope that we can
get rid of them more in the modules
world because they don't help well then
they do still help some with the D
coupling but it's also not really a
decoupling so we hope we can live in a
world where we don't need for vertically
eration that's right this is just an
example how about pimple pattern or
something like that so the question is
how about the implementers I'm not a big
fan of the input pattern but just makes
code more complex to work around your
build system I think modules is the
right solution that we have a
fundamentally different compilation
model that enables you to not need
workarounds in your code and
your code more complex just so that the
compile doesn't like isn't completely
crazy right got it
Thanks am I correct and understanding
that you have a one-to-one
correspondence between libraries and
modules there's one module per library
one library promotional the question is
whether we have a one-to-one
correspondence between libraries and
modules where modules are enabled
currently yes so we also use a an
implementation detail of clang that's
that that sub modules but going into
that would probably go too far but
generally there's one library one module
okay
so within a single library do you get
any benefit for modules then do we get
benefits from modules within a single
library I don't think so okay I come
from the windows world and so I'm not up
debated all the clang technologies
however one of the things I kept
expecting you to mention or at least
would have some some talk about because
I believe it is this is the concept of
precompiled headers it was is that just
not allowed in the distributed build
system or can you talk about that so I I
didn't mention it very shortly I think
we have people look into precompiled
headers and no precompiled headers in
the windows world but the problem is the
way you use precompiled headers there i
think is that you very early on you
decide you have a precompiled header
this will be my precompiled header and i
just put all the others headers into
that and everybody includes that one
header in a large to distribute code
base that is problematic because you get
this the single point of dependency
where everything depends on and again
you get this dependency like the recode
like if any of the headers needs to be
recompiled the precompiled header that
precompiled header needs to be
recompiled
and everything that depends on it needs
to be pre-compiled compiled and that's
something we want to try to get rid of
and also in a code base that was not
written that way you don't naturally
have those points so what people
actually do is write you include one
header from this library one header from
library and somebody else includes the
other header from this and the other
handle from that library and we actually
had people go in and try to use
precompiled headers but because they
needed to keep the code compiling
because we also didn't have a probe like
there was no idea that modules would
come and actually require everybody to
rethink how they think about semantics
of C++ anyway that completely failed and
another unrelated question you would
mention that you're trying hard to get
your dependencies together our company
is doing that as well do you have any
interesting or useful visualization
tools to see the dependency graph
between headers and and libraries and
whatnot or have you developed those or
do you find those useful so the question
is do we have visualization tools for
our dependencies so we have people who
have hacked together tools like websites
where you can watch the dependencies and
it's not helpful if you want to see
spaghetti yes you see spaghetti we have
actually created tools that help you
guide which dependencies are actually
important and you need to look at as a
human it's just so much code and like
right it's much of a hundred million
lines of C++ code if you look at that
depends if you have you seen nothing but
for a tool it can actually figure out so
if you if you if you remove these five
dependencies now suddenly you actually
have two components that are completely
split apart and you will say that many
test runs from running than anything in
the transitive closure changes and those
things those things are the things we're
focusing on and less on visitor
visualization thank you all right just a
simple question why did you choose to
distinguish between modular and non
modular headers and describe those
modules explicitly in your build files
instead of just generating a PCM file
out of every header you have to speed up
the build process so why did we not just
create modules like switch on modules
everywhere basically it doesn't work
because most of the code like you have
to change code in order to in order to
make it work with Moya's
and I think we are over time sorry</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>