<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Anthony Williams “The Continuing Future of C++ Concurrency&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Anthony Williams “The Continuing Future of C++ Concurrency&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Anthony Williams “The Continuing Future of C++ Concurrency&quot;</b></h2><h5 class="post__date">2016-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FaHJOkOrfNo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">laughing folks I hope you've been
joining conference so far I got to talk
to you about the continuing future of
C++ concurrency I'm gonna start with
what's new in C++ 14 since the C++ 11
standard obviously we add we add the
first standard that I can knowledge the
existence of concurrency receive bus
plus 11 so C++ 14 the first time we
actually added new stuff to that and
we've got new stuff coming in C++ 17 as
well and then of course a bunch of
technical specifications on concurrency
parallelism and transactional memory so
I'm going to give a brief description of
some of the contents of those as well so
yeah there is a continuing future for
concurrency in C++ and so that's what
we're gonna talk about in C++ 14 we
actually only added one new concurrency
feature and that was a shared timed
mutex and then the corresponding Rai
lock type shared lock so those of you
coming from a POSIX background might
know about the pthread read wider
mutexes or on Windows the slim reader
rider locks and that's essentially what
this this wraps you multiple threads can
hold a shared lock alternatively one
thread may hold an exclusive lock so if
you've got a it's typically an
optimization for a data structure that's
read most often and then updated rarely
so you can now have multiple threads
that are reading concurrently and it's
all safe but then when you want to
update everything then you say hang on a
minute all the reading none of you can
read whilst they just change this and
then you go again
it's got timed in the title because some
of the functions have timeout so you can
say try and acquire a lock for a certain
period of time no this is in the in C++
7 we've got standard time to meet X and
standard recursive time to mute X so
this is not a new thing the concept of
it being a time mutex so yeah those and
those functions with the timeouts are
therefore both the exclusive lock
fraction functions and then for the
shared lock functions so you can try to
say try and acquire a shared lock for a
certain period of time you use it
okay libraries as an example so we've
got some some some data structure that
we that we're trying to deal with you
know we've got a you know in this case a
table which a map of strings to strings
we've got a we have our shared time
mutex at the top and with implementing
some function to find an entry so with
this is a read operation with finding so
we're gonna take your shared lock and
we're going to use that nice new Rai
type shared lock on our shared time
mutex construct a card we're going to
find the entry in the table we can just
throw if we didn't find what we're after
and if we did find it then we're gonna
return the sink maybe I mean that's just
the standard map poker but the whole
point no the the the bit that we the
specific to this is we've got the shared
lock on the shared time mutex on the
flip side then we know if you're adding
an entry to this table then again we're
going to want to know protect it this
time we want the exclusive lock so we've
got our lock guard know the same lock
guard that we would use it with a
standard mutex and now it works with a
shared time mutex because it implements
is the lockable content so it just works
he's just sticking in there and this
this then acquires the exclusive lock
once the exclusive Rockies acquired none
of those none of the readers will know
it will wait until any pet
readers have released their shared locks
before you proceed and then no news no
new readers can acquire shared locks
whilst this the exclusive lock is held
so our adding of an entry to the table
is perfectly safe we don't have to worry
about know the concurrent readers on on
my table like I said there's a timed
part so the shared lock Aria has allows
you to put that spot of the constructor
just like unique lock does so you can
say I'm going to try and acquire my
shared lock in this case for one second
if I'd if you don't get it in one second
then the share dog just doesn't own the
lock and so then you if you use it if
you use this tri tri lock and
constructor you then get after query it
did I get the lock does my share lock
own no L got owns lock and if I didn't
then I can't do whatever it was I was
trying to do and so you have to give up
and and do something else and again and
typically you want to use this where
you've got some sort of well I have to
I've got a certain amount of time in
which I can do this bit processing
before I need to do something else with
its update the screen respond to an
event or no do something but the if I
can't get my lock within that period of
time um typically it's not going to be
seconds it's gonna be nine milliseconds
but anyway so if you can't get them up
within a certain period of time then I'm
going to give up and I'm gonna do
something else and typically that's
where I need to do this eventually but
if I if I can't do it right now then
that's not that's okay that's not a big
problem I'll do it next time because I'm
no I'm going to call it unction in the
loop and every time we're going to try
and acquire the lock and if I succeed
then I can do it and if I don't know
nevermind and I mean that's just the
general principle whenever that's why we
have these time functions in the
standard for this standard mutex anyway
no for obviously for time to me text
rather than just plain centimeters so
that's it's the same applies with shared
time to text so it's not not a new
concept there
there is a question already yes so okay
so the question is are we going to use
shared lock for reading purposes and
lock guard for writing purposes and yes
that's in general true because for for
writing purposes you want an exclusive
lock and so we'll lock garden and unique
locker the other types that give us that
and the reading purposes you want the
shared lock and so the standard shared
lock guard is what gives us they gives
us that with the better name of being
read lock and write lock
who knows that's water under the bridge
this is what we've got so on this side
say profile promo profile
it's all about timings now fundamentally
the reason why you would want to use a
shared mutex is because you're trying to
optimize for the case where you've got
lots of readers and an occasional update
and it's you're trying to do that this
isn't trying to be an optimization but
in order to keep track of all your
readers every time you require a read
lock you've actually got to update the
mutex itself in some fashion to say I've
got another reader and then when I'm
done you've got to say that read has
done now so any exclusive lot of people
who are waiting can now take the lock so
there's actually still the contention on
the lock itself so you need to profile
to make sure that you are getting the
optimization and the performance benefit
you hoped you would know like any
optimization profile beforehand profile
afterwards you might find in your case
that's actually better just to use a
plain standard mutex and this will
probably vary between platforms so and
possibly even with standard library
implementations on the same platform so
you need to if you're porting across
multiple platforms you need to profile
on all the platforms as well
so C++ 14
that was all there was to it concurrency
in C++ 17 we've got a little bit more so
we've got shared mutexes non-shared time
mutex is non timed Brummer standard shed
mutex we've got a new very Adak version
of standard lock guard and then a nifty
c++ 17 feature which it's not a
concurrency feature but it works nicely
and that is that you can deduce your
template arguments for class templates
from their initializer so standard lock
guard doesn't require you to specify the
mutex type anymore and know that a
standard unique lock or standard shared
lock parameter on top of that we've got
parallelism the parallelism TS version 1
has been merged into the c++ 17 working
draft so there are now parallel versions
of most of the STL algorithms and also
say for anyone who's been following the
standardization process and was hoping
for joining thread you're not gonna get
it unless somebody resubmit to the
proposal and the committee changed their
mind
no it it got voted down at the last
meeting so sorry we're not gonna doesn't
look like we're gonna have standard
joining thread after all so we got
shared time mutex why do we need shared
mutex and the answer is of course
performance on some platforms notably
windows I think then if you don't have
to support they'll try lock with timeout
then you can actually make the
implementation cheaper overall and so
you can get better performance if you're
never doing timed operations and given
that that's probably most of the uses
never use the timed operations then
that's a potential and and yeah and
you're using this for performance in the
first place so having your know making
sure that it is as fast as it can be for
your use case is probably quite
important and the reason that we didn't
get it in the first place is just a
matter of known history and how the
committee works so as far as baryonic
what guard goes this is great if you if
you're trying to do an operation that is
going to be needed to acquire multiple
mutexes then if you specify specific ly
lock them one at a time then your own
that's a classic deadlock no deadlock
case is no you you try and lock commute
XA and then you lock mutex B and in one
thread and somehow another thread locks
B and then a and all know everything
goes wrong your program grinds to a halt
so in C++ 11 we had this standard lock
function so you could give it a couple
of function no to mutex III mutexes biog
mutexes and it would work out a way of
locking them all in a deadlock free
fashion but in order to use that with
your RA how I guards like standard lock
garden standard unique lock then you
then had to either do the lock first and
then adopt everything afterwards no you
split over multiple lines and it's it's
easy to get wrong and making mistake so
with the new very avec la guard you can
do that all in one line standard lock
guard you take the type 1 mutex type 2
and then your parameters and this uses
the same underlying acquire all the
locks without deadlock mechanism the
standard lock does but know is it's all
already owned work by the by the lock
guard so you don't then need to worry
about making sure that they things
release when you get to the end of the
function
and like I said we also C++ 17 also
gives us this implicit deduction so you
can specify still standard lock guard
off standard mutex no and it just works
but that's one but you can also emit
that now the commands template type
deduction means you can just say
standard lock guard no template
parameters and that that is great if
you're no it simplifies typing it means
if you change the type of the mutex for
from no standard mutex to standard time
mutex because somewhere in your code you
realize that you need a timeout
then that's now just going to work and
all the uses of lock guys you don't need
to go through and update them or see and
that obviously applies I mean either
it's a it's a general feature from C++
17 so it applies to all class templates
that can be deducing the initializer so
it applies to standard unique lock and
standard shared lock as well okay so
that C plus plus 17 either way so let's
move on to some of these TSS concurrency
TS version one now this has actually
been released as version one it is a
published iso/ts I give us a few things
gives us continuations on our futures so
now you can say here's my future and
when it is ready then I want to do some
other asynchronous tasks you can say
I've got a bunch of futures and when
they're all ready then I want to do
something I'm going to wait for all of
them to be ready or for the first one
out of a bunch to be ready so this
function is to give you that no there's
sort of no set joining for so
for gathering futures together which is
incredibly powerful - and actually
combined with the continuations that is
a great combination because you can say
when everything is ready then schedule
this new a synchronous task and that
task will only start when all the
previous one when all the features are
ready and then we've got latches and
barriers for know providing scheduling
points within our code so when a whole
bunch of threads have reached a given
point in the code then we can move on
and now we the latches and barriers give
us that those increments and then
finally we've got atomic shot smart
pointers in the form of atomic shared
pointer and atomic weak pointer so where
as ace in the C++ 11 a single shared
pointer instance no cannot you cannot
have two threads accessing map because
otherwise it's a database unless you
obviously wrap it with a mutex or some
other synchronization in that if you use
the concurrency TS then you can have
atomic shared pointer so that you can
then have multiple threads access that
without external synchronization but
there's also a whole bunch of things
coming up for the concurrency TS version
- now these are all proposals under
consideration the working draft for
concurrency but TS version two is
somewhat scant at the moment we've got
the big big thing really from my
perspective is the executives and
schedulers now this is the the the
fundamental building blocks which are
going to get us thread pools thread
pools and and things like that are
things that we have been wanting as a
concurrency group for concurrency
support in standard TS since we were
working on C++ 11 but we ran out of time
to specify them properly and so all we
got the standard async
and as a group people have been working
on ever since there have been many many
proposals and changes to the way the API
works around executives and schedulers
but we're really really keen to get them
and so hope we're gonna get them
somewhat sometimes see actually in an
API that we can all agree on
there's also proposals for distributed
counters so obviously if you've got
hundreds of threads and they were all
trying to update a single count then
that's an epic point of contention and
that's going to cause everything to slow
down so you don't want to do that so if
you don't need everything to really
really really be up-to-date and you can
cope with batching things then you can
have your distributed counter and you
say well here's this counter that we
eventually going to update and this this
thread is got is going to it updates it
to the local count and then the
propagation to the central count then
happens automatically in some fashion
and obviously there's ways that you can
say update it now please because I
really care but it means that it
provides you the eventual consistency on
the count without the direct contention
that you might otherwise get also we've
got proposals for concurrent unordered
containers so I mean in the standard at
the moment we've got no unordered map
and unordered set and there are
proposals to provide concurrent versions
of those so that you can do your table
lookups so the example that I had with
the shared mutex then it used a standard
map so it's not quite know but if you if
you use the unordered version and the
the new the proposed concurrent on order
version you wouldn't need to put mutex
lock in because that would deal with
internally in the container it is
potentially possible to for some
implementations for that to be locked
free so it depends on quite on the API
and whether whether the the
implementations then provide those but
there is potential for that
similarly we've got concurrent queues
now seeking know so can come on an order
containers and concurrent queues give us
basic buildings structures for lots of
cross straight communication so again
it's all about you don't not needing to
use a mutex yourself because the
containers and data structures take care
of it for you and queue is a great
before you can use them for message
queues so you can have no communicating
state machines they horse communicating
sequential processes so you gives you
can have each each thread essentially
communicates only with the outside world
through through a queue of some
description so once you've got that then
you don't then need to worry about lots
of synchronization problems and simply
so and then we've got no safe concurrent
stream access is coming we hope and
there's certainly proposals for it so
that you can say here's this bunch of I
know I'm running on a thread it I'm
writing stuff out to a stream I know
that another thread is also going to be
biting stuff happens to a stream here is
a bunch of stuff that I want to make
sure it appears together but I've one I
can't do it as a single insert I'm like
and I want and so we're providing them
some mechanism for ensuring that a write
is then atomic from the point of view of
the outside world resumable functions
and care routines there's a whole bunch
of talks on that today guys I spoke to
yesterday how they put up a slide and
said that it was no like-a a mini cooker
routine calm so yeah there's a whole
bunch of stuff on that that that allows
you to simplify code again with
asynchronous stuff if you Hartmut
haven't had a slide up yesterday about
the making a synchronous parallel
algorithms and how him no sticking : you
got two two parallel algorithms that
you've got with futures that you're
waiting on so that you could then and
that
the outer function then to also return
the future without even having to worry
about making sure they were both ready
in that particular case he could have
used when all and-and-and continuations
as well but it's it co-routines one
alternative in some sense they provide
you concurrency on a single thread
because you can no switch between your
different functions
now when you haven't got work to do on
this function you're waiting for
something then you can run something
else and then pipe lines so we've got
sequence of data no data coming in and
we've got a sequence of operations we
need to do to it so we'd be able to
pipeline and we say they do this and
then do that and then pass a result of
this other thing and then split it out
and pass the results of these two other
functions that running parallel and then
combine the results both of you do
command line processing in UNIX the UNIX
pipes no but but actually in your code
in your C++ program solver see it's
great if you've got a whole bunch of
data coming in and you do the same
sequence of operations on every bit of
data that comes through okay so I'm
gonna have some actual more concrete
examples of some of these things
everything in the TS is is in the
standard experimental namespace that
doesn't mean that it's poor code and
that you shouldn't use it it means that
it's an experimental interface the C++
standards committee aren't committing
that this is what it's going to be like
when it gets into the final standard so
I don't let that put you off I know that
some workplaces have an experimental
main space for their own bits of code
where don't use this in production may
say well just because it's in standard
experimental from the point of view of
the standard doesn't mean that obviously
you hope that if you use whichever
implementer you've got your
implementation from you hope that it's
as good quality as the rest of the stuff
they provide you in the slides I'm going
to use HDD exp instead because standard
experimental is a bit long for on slide
examples
okay so continuations continuations give
us a new task to run when a future
becomes ready and conceptually you say
when the future is ready you then do
this and so sure enough the function is
called then if you've got if you start
with a standard future well if you start
with a standard experimental future
because they only work on that then your
function then must take a standard
experimental future as the only
parameter and the source of futures are
one-shot things so if you normally if
you get the value from a future then
that future is no longer valid so
similarly if you change a continuation
on a future the source feature is no
longer valid because it is going to be
passed into the continuation and instead
you've got a new future to hold the
result from the continuation and all
this means that on any given future and
you want continuation can be added but
of course you can chain them so here's
some code we're going to try and find
the answer we all know it's going to be
42 but we're going to assume that we
haven't got there yet and when we do get
the answer we're going to process the
result so we use standard experimental I
think to find the answer and that gives
us back a standard experimental future
we can then say then process the result
and that gives us a new future f2 which
processing the result is returning a
string so f2 will be a standard
experimental future for a string and at
this point F is no longer valid because
the know it's it's been swallowed into
the continuation
when the original async task returns
then it will pass the the resulting
future holding the result as that
parameter to process result so we can
see it takes span to the exponent of the
future of an int as it's one and only
parameter so that's where know that is
going to be that is conceptually where
our where a future F goes it becomes the
parameter to that function
yes okay so the question is can you add
parallel veins to a single future so you
can say F dot then to get process result
and then also F dot then something else
and the answer is no you can't because
after your first F dot then your F has
no has no future in it is it's invalid
so you can't then do that you can chain
on to the result of F two but you can't
have multiple parallel ends on a stand
an experimental future if you want to do
that you can with shared futures but
we'll look at that in just a second
continuations also allow you to process
exceptions so if your first first
function throws then that gets stored in
the future like it normally would and
this is it's stored in the future that
is passed to the continuation so we have
stand experimental async or fail fail is
going to throw a runtime error we then
chain on the continuation next next
takes the future that that will be a
future that is holding the exception of
the standard runtime error that was
thrown and so when you call get F gets
hit inside next then that's going to
throw because F got get throws if the
future holds an exception and then
obviously in food then the final future
is the the one returned codes a result
from next next through because it
because and so you actually end up the
exception propagates all the way out so
the final get info will also throw an
exception obviously if you catch the
exception in your continuation then you
can do whatever processing you like with
it which is why we pass in the future
rather than passing in the values so
that these exception gets propagated to
and you can then process it or all allow
allow it to propagate out and pass on to
the next continuation in the chain
if you've got some function that doesn't
take a future so your protests result
wants a warrant well you can just wrap
it wrap it in the lambda know so your
lambda takes a future if you use the new
generic lambda so that could say square
brackets Auto F so to take the future
and then obviously you can process the
result a unpack call F get it's a little
bit of overhead but it's not too tricky
okay so continuations work we shared
features as well someone said can you
have multiple parallel veins on the same
future well with shared future you can
obviously if you started with a shared
future then the continuation must also
take a shared future and shared futures
are reusable so and calling get on the
shared future it's still usable same
applies do you have a continuation may
remains valid so you can have multiple
so same example here we've got a sink to
find the answer we're going to call dot
share so that Fi is a span of the
experimental shared future of an int and
then we are training to continuations
these will run in parallel so as soon as
the original a sink has finished then
the tasks for next one and next two
we'll both be scheduled I will say at
this point that the scheduling is
unspecified
the Tia says that continuation tasks are
gone on unspecified threat the idea is
that by the time we've got as far as
integrating into the C++ standard then
we will have sorted our problems with
executives and thread pools and so we
will add to then a means of specifying
where you're going to run your run your
continuation but for the meantime it
will run on some threads somewhere in
your system yes there is a question in
the middle what happens if one of them
throws well it's just a continuation
which is amazing tasks wrapped in the
future so what the each continuation
that throat if it throws then the future
associated with that continuation will
swallow the it will capture the
exception so in this case we've got two
continuations which has stored futures F
2 and F 3 although actually on the slide
I've called them F 2 and F 2 so that's
just confusion but each of those will
know if next one throws an exception
then the next two tasks is still going
to run and but the future from that
associated with the next one
continuation will hold the hold the
exception ready to throw when you call
get and obviously if you've been chained
continuations on on those then the
exceptions propagate down the one path
where where that exception went and not
down the other one okay so what if you
wanted to wait for your futures you can
wait for any of them to be ready you've
got a whole bunch of futures and you
want to wait for just one well that's
right that's we've got to overload when
any I've got two overloads these are
great big long signatures but actually
it's really quite straightforward the
top top overload is a very addict it pot
when any and then you pass it any number
of futures and it returns you a result
now the result is a B is a package that
holds a tuple with all the futures that
you gave it and then a flag to say which
one of those it was that was ready when
it checked obviously by the time you get
around to processing it they might all
be ready or some no three out of four or
whatever but there will be one that
triggered the the when any result the
the future that's returned from when any
to be ready and the when any result has
a frank to say it was this one the third
one in the list and by the way here is
the list so it gives you the list back
so that you can not only get to the one
that you've triggered it but also to get
all the others and you can then process
them no wait for them in turn if they're
not ready yet
or do some others form and processing
the bottom overload also has a great big
long return type but actually it's it
just when any on an iterator range the
so you pass in a range any iterator
range and it will iterate through and
build your set the result is always a
vector that holds the futures that were
passed in and regardless of where your
Ritter ages came from have a question at
the front yes if a top so the question
is if a task associated with a future
throws an exception does it make it
ready and yes with futures if they if
they capture an exception they are then
ready and so then the when any will
indeed trigger and so if you've got
three tasks and one of them throws and
then and that throws before the others
have got the results when any if you can
you pass those to win any the when any
result will be ready and it will say
that one that through is the one that's
ready the one that triggered so this is
great if you've got speculative tasks if
you want to say well I've got plenty of
course to spare and I've got two
possible ways of calculating this value
three possible ways maybe and I don't
know which one's going to be fastest
today so let's launch all of them and
whichever one gets back quickest I'll
take the result and then process it
so that's good alternatively if you've
got a whole bunch of stuff that you want
to do and when the first things ready
then I want to do some more processing
and then when the other bit has become
ready to but I want to process which I
don't know which of my three things is
gonna be ready first and so I want to
process the one that comes back first
and I want to do that processing first
so I could chain a continuation on each
one individually but actually I only
what I want to only process one of the
results at a time so I would say when
any the first one comes back I do the
processing and then wait for the second
and third in turn and it allows you to
not fuss about which order so we
launched two things with async f1 and f2
give us our tasks and then when any
these are standard futures or
experimental futures and so we have to
move them we move them into our when any
call and then we get back a future that
holds the potential that when any result
we can have got no it's an experimental
future so it can of course train it with
continuations so we can say F 3 dot then
some some do some processing ok when all
is the counterpart you wait for all your
futures to be ready and again it's got
two overloads first one just takes a
bunch of futures as parameters and the
second one takes an iterative range and
again the result holds a vector this
time because the results are all deaf
definitely all going to be ready you
don't have to there's not this when any
result complicatedness the first one is
just a future holding a tuple and the
second one is a future holding a vector
or futures and this time it's great if
you've got a bunch of subtasks you want
to launch the off and then when they're
all ready then you've got some more
processing to do so you do exactly that
now here's a three task subtask 1 2 3
with async and then when all now you get
the result
or chain your continuation or whatever
okay it's not the only changes to this
so the futures interface as a whole
there's no a collection of little
changes we've got first up are two
functions to create futures that are
already ready standard no make ready
future that gives us a future that holds
a value so rather than actually spawning
a task you say well I already know the
results gonna be so here we are and then
the counter parts for that which is
making exceptional future which is no
I'm going to holding a creative future
that's holding an exceptional ready to
throw so if anyone dares call get on it
then it will throw up in their face and
both and again these cases where as a
general thing you're built you've got an
API and the library structure that you
use these futures for communication and
in this particular code branch you
realize you know the answers already you
don't need to actually spawn any tasks
to do it so rather than having to do a
little dance of creating a promise in
getting a future and setting the value
on the promise or selling the exception
on the promise then you can just call
these functions instead and then we've
also got this nice little nifty function
is ready we can ask a future are you
ready we can pull it rather than happen
to wait for it to be ready or do a nifty
little dance with saying wait for no
seconds to see whether you're ready then
we can just worry are you ready
okay next on the list is laches and
barriers laches a single-use countdown
you say well I've got this bunch of
events that I need to happen and then
when all the events have happened I can
do some more processing and so know it
counts down and they count and as each
as each straight does that does the
things they it says count down the hatch
count down the latch and then you and
then you can have another thread or some
number of threads waiting on this latch
and then when as soon as the counting is
zero all the waiters are notified and
everyone can proceed its it latches it's
permanently that so it's a one-shot
thing whereas barriers on the other hand
are a reusable mechanism and this time
it's where you've got a bunch of threads
that are waiting for each other in some
sense so if you if you've got a
multi-stage process and all the threads
need to have completed stage a before
they can all move and do stage P then
you have create a barrier and no at the
end of it so you say here's a barrier
for five threads here are my five
threads they each of them will arrive at
the barrier when all of them arrived
then they all can proceed and then they
and if often if this is a cycling
process because of the way you're
processing your data so then they've all
proceeded and they do the next no so
you've got packet of data coming in so
five threads all do their parts on on
packet a when they've all process pakad
a they all arrive at the barrier and
then they could all then move on to
process packet B and then again when
they've all done through packet B then
they will arrive at the barrier and then
again they're all released to move on
final thing in the concurrency TS is
atomic smart pointers now shared pointer
and weak pointer are not bitwise
copyable because they've got to deal
with
France counts so you're not allowed to
say standard atomic of standards shared
pointer or standard atomic of standards
weak pointer but it would be really nice
if you could and so in the TS we've got
standard atomic well standard
experimental atomic underscore shared
under supporter and atomic underscore
weak underscore pointer and these are
special types that provide the same
interface as the standard atomic
interface but they actually work on
share pointers whereas the standard
atomic interface can't because it has to
deal with the reference counting and
this you would use where you've got a
single shared pointer object which you
want to have multiple threads obviously
each thread can quite happily have
shared point or objects of their own and
they can even refer to the same thing
and the reference count will be dealt
with accordingly but if you've got a
single object light in a linked list if
no is that the next point is in a linked
list that's being traversed by multiple
threads then if you try and update those
concurrently from multiple threads and
that's not going to work if you've got
another thread traversing and you need
to use external synchronization like a
mutex if you use the atomic shared
pointer the synchronization is moved
internally and again depending on your
implementation does it may use a mutex
or it may manage to be locked free
okay so what's coming up proposals under
consideration so yeah all these things
that I've just done the examples for are
in concurrent CTS version one so that's
actually an official ISO document these
things some of them there's I've been
put in the working draft but most of
them haven't they're just proposals for
discussion so everything could change
executives and schedulers these are the
building blocks our thread pools an
executor schedules tasks for execution
and you might have different properties
you might have three pools might have
serial executives that run things on a
particular thread one in after another
partner took yesterday about the
potential for GPU executives so you can
say no his bunch of stuff go run it on
the GPU or numerous acute errs so here's
a bunch of stuff go run it over on that
particular node that has best access to
this chunk of memory so we're working on
an interface that's going to work all of
these I think there's a talk running at
the moment that's talking more about
that so go check if you're actually
interested go and watch the video
afterwards okay I said what about
distributed counters we can attempt to
improve performance by reducing
contention on a global counter by
buffering the change changes locally and
then only to provide know providing
eventual consistency to make sure that
the global count is eventually correct
but doesn't have to be correct right now
and all those maps I mean proposal is
concurrent unordered value map which is
a bit of a mouthful but it does tell you
essentially the essential properties its
concurrent for concurrent accesses it's
on orders so now like the unordered
containers it's Pro uses a hash
internally its doors values rather
because all the standard containers that
we have at the moment then if you if you
you can get iterators that and you can
get references to the contained values
with the concurrent or normal value map
you can't get those references and
everything in all the functions that
would return a reference instead return
a value and some of them return an
optional value because the container
might have changed under no so the value
that could have come back comm isn't
there and it all does simplify things
because they don't they're not going to
work with the span of algorithms then
they could contain some algorithms
themselves so reduce and for each for
example as member functions the same way
that standard list provides sort because
it doesn't work with the standard sort
as I said queues are a vital means of
Internet communication a building block
for doing communicate Tings sequential
processes as well as all sorts of other
just general no inter thread
communication they may or may not be
knocked free they might be fixed size
they might be unbounded now there's a
whole bunch of properties the proposals
give you lots of ways of tweaking that
so you can have lots of different cue
types with all sorts of different
properties one nifty feature is the
ability to close queues because
obviously knows you've got a bunch of
work coming through and you can say okay
I'm done now done sending so then the
processing thread at the other end could
say yeah okay if I got more work to do
no no more work
yeah okay I can I can finish now and it
helps you be cleaning up
the proposal allows offer other than
that we have at the moment allows you to
get a handle to just one end of a queue
so you can have a push handle where you
can ship stick more stuff and a pop
handle for taking stuff that's ready
so rather than passing the cue round
then you can pass around these and that
sort of a means of abstracting it so it
doesn't matter what the concrete type of
the queue is you can you can pass round
a a pop handle and you don't have to
worry about whether that was a pop
handle to a bound EQ or an unbound EQ or
a lock for EQ or a lot bass queue it
doesn't matter from the point of the
code that's doing the processing and
then they work with these iterators as
well so that's there's quite a lot of
flexibility in that proposal safe
concurrent stream access at the moment
if you use the standard output streams
then it is guaranteed that each
individual insert is atomic from the
point of view of the final output but if
you're doing multiple inserts no insert
ten twenty thirty years three separate
inserts and another thread does 40 50 60
is three separate inserts then the two
threads can now be interleaved so you
could get no 10 40 50 20 60 30 and
obviously the the the 10 is before the
20 is before 30 and the 40s before the
50s before the 60 but they've all
interleaved across the across the two
threads so it would be nice if we could
avoid that and those proposals around
that one of the proposals has basic go
stream buffer the name of this type and
the way it's specified keeps changing
through the proposals so quite how what
it's going to finally look like by the
time it gets as far as RTS I'm not sure
but the whole but the principle is that
you create some some buffer for that for
what you're you're saying I want to
gather the output I'm going to output it
to standard standard C out and now I can
do a bunch of inserts and it's open and
the whole thing is then atomic from the
point of view of the final output
it's to be used in conjunction with is
that does it replace basic Oh stream but
no of course it doesn't is to be used it
it will wrap another basic Oh stream so
so then yeah it's exclusively for this
purpose rather than a general
replacement co-routines co-routines
great they expose a pull interface for
call back style implementations and and
reusable function resumable functions is
a way of generating async calls from
codes no weak things like the KOA weight
interface keyword so you stick koa
weight in and it says this and this
function is it's going to resume when
it's when this other task is done and
there were alternative ways of
structuring code that does asynchronous
operations like I said this whole bunch
of talks on that it has the potential
for greatly simplifying things if you do
it wrong it's also got the potential for
greatly confusing people so make sure
you don't confuse your your users but
yeah it it has there's a great great
potential there and can consider FICO
quite a lot the boilerplate if you look
at the boilerplate that's equivalent to
lots of the the know like the using the
KOA weight keyword then you can then you
could say I'm glad I didn't have to
write all that myself and the compilers
doing it for me but you could in most
cases then things are equivalent to no
span of async tasks and things okay so
pipelines again like like unix pipelines
and on the command line and things so
you have some inputs from a source you
have a sink where you're sending stuff
and then you create a pipeline which
take theta from a source and then does a
series of processing then dumps it out
to the final sink
again listen no these are all proposals
so the precise syntax and function names
might change but that's the general idea
another thing that that's coming up and
I really really hope he's actually going
to make it into into the next es is
hazard pointers and they fundamentally
help us provide an answer to the
question when is it safe to delete this
object if you've got a data structure
which as is accessed from multiple
threads and you want know you've got a a
map or a list or something and you've
got some some node that you want to
delete some part of the data structure
that you want to want to remove then if
you're running things in a lock free
fashion then how do you know that
somebody else isn't accessing it so it's
safe to delete and has a pointers help
give us an answer to that question
because you declare I got a hazard
pointer pointing to this object before
you try and use it and then if you want
to delete it you say has anybody got any
hazard pointers pointing to this object
and if the answer is yes then you stick
it on a deferred reclamation list and if
the answer is no then you know you can
delete it right now it's an alternative
to garbage collection and reference
counting so rather than using atomic
shared pointer you can use hazard
pointers at the point of use it's
potentially a little bit more
complicated but the potential for
performance is much greater so it's a
trade-off like most things in life say
yes and this is how it works you've got
on the using thread you obtain a pointer
to an object you mark that you're
accessing ax Xing accessing that object
X with a hazard pointer
you check that it that at that point
you've still got is still valid before
you actually dereference it you then use
the point in some fashion you
dereference it you pass it to a function
you do whatever then when you're done
you release the has a pointer and if you
actually will and
you if X is marked as marked for
reclamation and you're on the
reclamation processing list then you
delete that object if it's been marked
so and there are no other has a pointers
if a thread is actually trying to do the
deleting deliberately trying to remove
it from from a data structure then again
you start by obtaining the pointer you
mark that it is now to be deleted and
then you check the hazard pointers and
if somebody holds a has a pointer then
you have to put it for reclamation later
otherwise you can delete it right now
there are many more proposals not coming
here if you actually if you want to have
a look look on the committee website or
go to the ISO CPP foundation because
lots of the papers getting published on
the links get published on there as they
get released and you can look through
the archives to - the final papers so
there's also per a specification for
parallelism
we've got ts1 which is now merged into
c++ 17 got parallel algorithms including
MapReduce alright well or something
similar it's called transform reduce
it's not quite the same spec as no
people might expect from MapReduce but
it does essentially the same thing we
have like way execution agents and the
potential for Cindy and vector
algorithms for parallelism TS version -
the only thing that's concretely added
to the working paper at the moment is
task blocks but there's no more work
being done the c++ 17 library
incorporates the version 1 TS and that
essentially provides us with a whole new
overrides for all this but for most
let's times YB albums they all have this
execution policy as the first
first parameter that execution policy
can be one of three things at the moment
sequential which is just the same as a
normal algorithm standard path which is
and that was it to be parallel long from
multiple threads and then par Veck which
says we'll also take make use of
vectorize a
if you can and I said the vast majority
it's a great big long list I've
highlighted in bold a couple of ones
that are relevant to people and for each
is good general one but also merge for
urging things we'll find and copy and
counts transform reduce down at the
bottom now sort most of the algorithms
of that task blocks from Powell ISM to
you t2 are about managing hierarchies of
tasks so you can nest task box ND and
nested task box can wrap in parallel
it's sort of fork/join parallelism so
because all the nested task blocks
within task region are complete when it
exits so that's no so you you cradle
your nested task box and then when you
exit the block then all those parallel
tasks are joined we have that fourth
line fork/join parallelism which is
great for something so if a recursive
divide and conquer through four out
through us through a system then then
that's great there's also the
transactional memory TS two basic types
of transaction blocks synchronized
blocks and atomic clocks and the
synchronized blocks are sort of
equivalent to a know as if there was a
mutex around it but it sort of makes it
work so there isn't atomic blocks are
lower-level potentially and then you
have to deal with Possible's of
exceptions and whether they
automatically commit or automatically
cancel so you introduced them you just
put the key appropriate keyword before a
block and mady's now part of the
transaction it behaves as if it's a
global if there's a global mutex on the
synchronized block atomic box executes
atomically and not concurrently with
synchronized blocks again so you stick
your neck your keyword on and declare
the block is very very straightforward
to use
but atomic box a lower level so they can
potentially execute concurrently with
each other if they don't share this I
use the same variables and they differ
in their behavior with exceptions so
obviously no except you're saying there
won't be any exceptions so it's
undefined behavior and then you can
either commit or cancel when the
exception it escapes by the even on
those cases if you're trying to if the
exception is get is allowed to escape
for a cancel then it must be a special
type of exception that transaction safe
because otherwise the cancel transaction
will roll back the creation of the
exception that was going to be thrown so
you don't want that okay so I reckon
there's about two minutes for questions
anybody got any yes
okay so so the question is in obviously
the operating systems provide there's a
whole bunch of things that we could
potentially wait for so we could wait
for things coming from a socket we can
wait for things coming from from a file
and all sorts of stuff and so is there
any means of in standard C++ of
capturing that and I think I have to say
no I think you look at the various TS is
so like if you're looking at waiting for
the network then you look at the
networking TS and use their facilities
which will wrap the OS and likewise if
you'll know but there isn't a general
general framework for capturing OS wait
of all things would you be able to use
networking with the futures mmm
probably at the moment no but but but I
I imagine that that sort of integration
when things all get everything gets
moved into the C++ standard then that
would be something that we were looking
as the integration between the different
parts but not a moment they're different
yeses so they don't talk to each other
yes
first I would like to point out that
make exceptional future is probably the
coolest name in STL so please keep it
and my question is regarding shared time
mutex okay I'm not sure I get it
correctly but so when you request a
write lock you still don't get it
because there are some read locks on it
but you're requesting it so you're
waiting for for the read locks to to
finish does this this mere fact prevent
newer read locks to be created already
or or can it be a starvation that you
know you just keep waiting and waiting
that depends on the implementation and
obviously avoiding starvation is a key
feature that implementers are going to
want to do there are techniques to do to
avoid exactly what you you said so that
if a waiter is waiting then further
readers cannot acquire the lock and they
have to wait too so you can implement it
like that but
sometimes there are reasons why you
might not want to you might want your
readers to starve or your waiters to
starve because of the way that your
system is structured and you know that
eventually it will get to the point and
no so there is no wings measure to do
specify there is no there is there's no
no way to specify you have to look at
what you're implemented us and if you've
got a choice of implementations you can
use choose the one that has the
properties you want otherwise you have
to hope its quality of implementation
and not specified Thanks
so when an exception gets propagated
from threat to threat when you're using
futures is it exactly the same exception
with the same type or is it some sort of
a wrapping object okay it will
definitely have the same type I will say
that and on some platforms it will be
really really the same object that is
wreath Rohn you know I think GCC does
that on Linux it references counts the
exceptions so you catch an exception you
stick it in the future you then call get
and it wreath rose the same object on
other platforms like say Visual Studio
on Windows then it will use internal
stuff to copy the exception so when you
so when the exception is wreath thrown
it is a copy of the same type but it's
it will always have the same type though
when we're when any return the thread
that triggered it or does it just return
a thread that's ready at the time that
after it was triggered okay so yeah so
when any it will yes the the result will
have a flag that says which of the
futures it was that triggered it and
then it returns all of them to you so
you you know which one was the one that
triggered but then more than that might
be ready but by the time you look at the
result stop aren't you a question yes
have a really quick question about the
when any also is there any expectation
of fairness in terms of like if there's
multiple things waiting is there some
expectation that these things will be
kind of generally fair not over and
above anything that your RS provides so
it's probably going to Europe use the
underlying OS weighting facilities so no
on Windows for example your problem
might end up using the equivalent of
wait for multiple objects and so it
doesn't provide you any more fairness
than that would okay thanks yes when you
called get and an exception is wreath
rowned
do you still have the original back
trace from the original throw point or
is it wreath rone from get I would I
would say that probably depends entirely
on your implementation with GCC it's
possible because it's the same object it
might still have the same back trace
available I haven't checked but if it's
a if it's a fresh object because it's a
copy but then it definitely won't have
unless you did something to capture it
in your exception yourself manually very
simple question how come nobody
implemented or proposed a semaphore
because they didn't yeah and the the the
C++ c11 library was based on boost which
didn't have a semaphore and nobody's
bothered write proposal for to have one
since so that's the fundamental reason
there's no proposal for it okay so it
looks like we have no more questions
yeah
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>