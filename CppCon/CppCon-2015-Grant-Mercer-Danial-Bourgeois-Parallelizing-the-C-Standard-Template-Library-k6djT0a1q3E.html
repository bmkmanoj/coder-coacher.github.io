<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: Grant Mercer &amp; Danial Bourgeois “Parallelizing the C++ Standard Template Library” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: Grant Mercer &amp; Danial Bourgeois “Parallelizing the C++ Standard Template Library” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: Grant Mercer &amp; Danial Bourgeois “Parallelizing the C++ Standard Template Library”</b></h2><h5 class="post__date">2015-10-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/k6djT0a1q3E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hear me all good all right so we can go
ahead and start the presentation this is
daniel a nice presentation on paralyzing
the c++ 10 standard template library a
little bit about myself first my name is
grant mercer i'm currently a third year
student at the University of Nevada Las
Vegas I'm pursuing a bachelor's degree
currently over the summer of 2014 I did
some really cool work with the stellar
research group and that was mainly
implementing a standards proposal which
is a n 4505 and this is a technical
specification for extensions for
parallelism and this is going to be one
of the two main proposals we're going to
talk about during this talk I worked on
implementing this inside of our runtime
system hpx which we'll get to in a
second and my name is Daniel bajwa I
started on the parallel algorithms after
Grant had left so we have just met each
other at this conference will we work
together at afford to make this
PowerPoint for y'all um hi so I'm
currently working for the Stella
research group at LSU and I worked on
the parallel algorithms and in addition
in 4406 which is parallel algorithms
meet executors and so first grants going
to take it off and then we're going to
talk a little bit about executors and
then I'll hand it back to grant okay so
first some background information on
what the stellar research group is
stellar is all about shaping a scalable
future with the new approach to parallel
computation one of our notes most
notable ongoing projects here is h PX +
h PX is a general-purpose c++ runtime
system for distributed applications of
any scale so what hpx does is it enables
programmers to write fully asynchronous
code using hundreds of millions of
threads it's the first open source
implementation of the parallel X
execution model and this execution model
serves to overcome the four main
barriers of parallelism which is
starvation latin sees overhead and
waiting some focus points we want to get
to it during this talk are some reasons
why we should actually paralyzed the stl
and why this may be a good idea some of
the features these algorithms should
offer our experience
implementing these algorithms at hpx
some benchmarking results we want to
make sure that these algorithms actually
work as their intended to and some
future work on what we want to do for
these algorithms so why exactly should
we paralyze the standard template
library for one thing multiple cores are
here to stay the old way of amping up
your frequency and gain additional power
has long passed back in the older days
people thought that you needed to get a
higher frequency to have a better
computer and eventually they hit a mark
where you start running into memory leg
RC delay power consumption and number of
other reasons so instead of getting more
frequency they decided to make more
cores instead and this was the birth of
parallelism so now we're running into
this new era of really great parallel
computation and we need to start making
taking advantage of parallelism another
reason why we should paralyze the
standard template library is some
scalable performance gains so having a
parallel SEL gives your user a ton of
flexibility because now they don't need
to reinvent the wheel when they have
these parallel algorithms at their
disposal so if you have a large data you
would like to run some algorithms on you
can simply say ok I'd like to run this
in parallel now instead of sequential so
we're still giving you a sequential
option but now we're offering some new
options for you and we want to build
widespread existing practice for
parallelism in the c++ standard template
library and this is because we're seeing
new languages like rust go and Swift all
putting parallel ISM inside of their
core library features and I think it'd
be really cool if C++ could keep up with
these other languages and also add some
existing practice for parallelism here's
a small graphic so everyone knows
Moore's law the number of transistors in
integrated circuit should double every
year and so far it is but Moore's law
will eventually come to an end there is
some work saying that our transistors
can only get so small soon we're going
to start hitting the quantum level and
that's when electrons and everything
starts getting really weird so instead
we're starting to see a new Moore's Law
and this is for the number of cores
inside of a computer as i mentioned
we're seeing a frequency level out now
so we're not trying to push up our
frequency anymore because this gives us
more power consumption so now our power
is also leveling out so this is great
for us our transistors are still trying
to double which will eventually slow
down but our course our course are
getting more and more inside of our
computers which is going to give us more
power for parallelism and this is where
the status proposal and 4505 comes in
it's a technical specification for C++
extensions for parallelism which pretty
much means its implementation details
for how to get a parallel stl when
you're writing this it's important to
realize that not all these algorithms
can be parallelized we have stained
accumulate for example where one value
will depend on the previous so this
cannot be parallelized theoretically so
it's defined a list of algorithms which
should now become paralyzed there's a
huge range of them anywhere from copy to
find to sort to transform so all these
algorithms could now be offered to users
in a parallel fashion and speed up a ton
of new code it's aimed for acceptance
into c plus plus 17 our implementation
that hpx already takes advantage of c++
11 and i believe some 14 the components
of the specification right now are will
lie in this standard experimental or
standard parallel experimental v1 which
is a mouthful but one standardized that
we expect to be placed in standard just
as the current stl is our implementation
in HP X uses the HP X parallel namespace
now all these algorithms inside this new
proposed document will conform to their
predecessors so any iterator
requirements and return types of the
current stl will remain the same we're
not going to impose any new restrictions
on these algorithms this is the simple
search algorithms from cpp reference
nothing will change inside of this new
proposal minus one new argument and this
new argument is an execution policy so
an execution policy indicates the kinds
of parallelism allowed
expresses the consequent requirements on
the element access functions so it's
pretty much saying you can't run this in
parallel you can go ahead and run this
in sequential or some other options we
officially support sequential parallel
and parallel vectorize perla vectorize
is to disambiguate on the tag so it'll
specifically say this can be vectorized
here's an example of how this new stl
would work so you have your normal
standard vector with view let's see if
this pointer works a little bit and
you'll have nope that doesn't work so
we're going to go back to the
presentation whoops jump right back up
there okay so um the first two lines are
the standard how your sort works
currently and after that is when we're
using the namespace hpx parallel these
are our new algorithms were writing you
have a standard sort and now you're
explicitly saying sort this sequentially
by passing in the seq tag after that and
the line after that you're using
parallel execution and parallel
vectorized execution and one of the
coolest things about this new proposal
is the bottom part which will allow you
to do dynamically selective execution so
if you have some continuously increasing
data size you can set a threshold and
you can check and if the data size is
over a certain threshold you can start
parallelizing it so you don't need to
know the type of parallel execution you
want to do at compile time this can be a
completely run time dependent operation
and then down below I don't know if you
guys can see that or I believe you can
we're simply passing in this blank
execution policy which has been set to
either sequential or parallel
some things to note about this new
parallel execution policy inside these
algorithms is that it's the caller's
responsibility to ensure correctness so
we're not doing any work to make sure
this is thread-safe anything you do with
your own code is your responsibility
will simply try and parallel why's it
for you so if you're throwing in a data
race or a deadlock it's going to be just
as you tried to write your own algorithm
we're not going to try and fix anything
for you so here's a quick example of
what not to do if you're trying to push
back multiple elements into a vector at
the same time it's going to give you
some undefined behavior a little bit
more about these parallel execution
policies is that what we're doing is
we're permitting to run in parallel
we're not forcing it to run in parallel
so what this means is that as I
mentioned before we're not going to post
any new requirements on these algorithms
this is in the case of an input iterator
input iterators cannot be parallelized
because one value you can't go forward
or backward inside the input iterator
what this means is that since we're
still going to let you use all the
iterator requirements if you pass
parallel we're simply going to force
back into the sequential execution so
this is a case where you might call
parallel on a data some iterators and
these might be input so we'll go ahead
and force them back to sequential now
for the exception reporting behavior if
temporary resources are acquired but you
don't have them it'll throw a standard
bad alloc that's pretty normal and now
if the invocation of the ailment access
function so what actually happens inside
the parallelization terminates with an
uncaught exception for par and for
sequential all the uncaught exceptions
will be contained in exception list that
you can use after now what we did
different hpx is since we're doing a lot
of asynchronous programming we found a
new policy would be really useful we
haven't proposed this to the proposal
yet but we found this really useful in
our own runtime system and this is the
task execution policy so what this does
is it gives users an additional choice
of when they want to join back in with
the main thread it'll return a future of
the result as opposed to just the result
and this will allow you to continue
we're doing work and then choosing when
you
we want to wait for this work to finish
so it's useful in that if you have some
big amount of work in you to do you'd
also like to do some work while that's
happening you can go ahead and send it
off get a future for it do some of your
extra work and when you need that value
you can go ahead and wait for it then so
let's meet Daniel so we've defined a way
to interact with the algorithms via
these execution policies but if we look
at the code example the bottom that was
in grand slide we're choosing
dynamically at runtime whether or not we
should do something sequentially or in
parallel but there's more that we can
give to the user or more that the user
might want to provide and so this is
what my section of the talk is going to
be about and it starts with the proposal
about executors to extend where work is
going to be done and then we also want
to be able to say how much work is going
to get sent to each of our processors in
other words control the grain size and
in doing so we hope to develop
abstractions that are useful for the
parallel algorithm writers but also
useful for other people who want to make
use of these facilities so n 4406 came
after the proposal about parallel
algorithms and it's parallel algorithms
need executors so the idea is that we
want to be able to control where
parallel or we want to be able to
control where work is done on executors
and we would like a way to do that so
they proposed the dot on syntax so the
dot on syntax attaches an executor to
the policy so we're creating this
interface to the user to gene interact
with the parallel algorithms while still
maintaining the same same extensibility
as the original stl algorithms so we've
got three examples on how this could be
done and this is all defined it in 4406
so we look at the first one we have for
each call and we create a parallel
policy but this time we attach an
executor and in this case it's just the
default parallel executor so in reality
instead of doing paired on parallel
executor you could just do pair because
n rear and what you would expect of pair
would be that it defaults to parallel
executor nevertheless as an example I
put it there um and you would expect the
work to be done in parallel however we
could also attach a sequential executor
so maybe this is an execute that has one
thread or require only does work in a
way that maintains that order of what it
when it was called then we are going to
have sequential execution but we can
still send it to a parallel execution
policy because the policy only restricts
that it can be done in parallel or
sequentially but it doesn't say it has
to be done in parallel so we want to
allow that to happen but we don't want
to allow a sequential policy to have a
parallel executor so that does not make
sense and so we're going to need a way
for the executors to be able to
advertise what sort of work it's done
there what sort of work it implements or
execute so the requirements to be met we
want to be able to accept an executor
advertise the restrictions and we'd also
like at least for implementing these
parallel algorithms or using executors
in general an API that's easy to use and
this is where executor traits for and
4406 comes in there's four function
calls and in other words two functions
and they are a sink execute and execute
a sink execute takes a function and it
calls of that function with the past
executor and it returns the future of a
result of that function but there's also
the bulk version of facing execute which
can do which would just return a future
void representing the function for each
of a set of inputs where input supports
beginning or supports an index range for
and 4406 and execute
is this analogous sequential or
synchronous version so it can be it
would just return a void for execute
bulk but for the normal execute just the
return of the function passed to it so
here's an example of what we what we
envisioned out of this we have some
executor type we'll call it exec some in
some shape type inputs and in this case
will say that input supports it began in
an end so we can do for auto and for
auto v and inputs and so on we're going
to have two functions f1 f2 and say they
return a type the second one will have
one argument and type def for traits to
be called traits and so here are the
four function calls just to make it
clear and if we look at the second one
that's that will call f2 once for each
of the inputs and it will use the
executor now what I personally find neat
about this is that if our executor
doesn't implement an async execute with
this book overload it'll actually call
um it'll actually synthesize the async
execute bulk overload based off of the
basic execute provided by the executor
so what's neat about that is that it
makes it easier to define the an
executor all it has to do is implement a
sink execute and same for execute and
the bulk version of execute they don't
need to define those functions because
it can just be synthesized by a sink
execute however in some cases it makes
sense to define the eight in at bulk
version of a sink execute because for an
executor it might be easier to implement
I create all of these function calls at
once instead of just doing a simple for
loop over the inputs in which case the
executor traits would default to the
executors function so once
hpx not only as we implement the
parallel algorithms but we afterwards
and at the same time we started doing
this proposal and we really like that we
can extend our policies with the dot on
syntax so that the user has more control
it's a convenient uniform launch
mechanism so all the way I'll only have
to call create a parallel executor and
executors with an async XE call um and
we can do the work in book on quantities
but we didn't quite like one feature and
that feature was not maintaining the
results for the book async execute so
let's pretend that one of our function
RF two calls to one of the inputs takes
a really long time but all of the other
calls doesn't take very much time and
let's also suppose that if half of our
calls are complete we can continue with
something else so inside of using this
version if we were to attach a
continuation to this feature then we
effectively insert a sequential portion
of code because we have to wait for that
one really long call potentially and
that my tape calls a sequential point in
the code so inside of hpx we decided to
change it a little bit and we added the
way to get the future results of our
call and we do that with a vector of
features and a bigot to REM adult and
all off graph the idea is we want to
give the user all the opportunity that
they can to avoid sequential work and
the idea is that if you have even a tiny
amount of sequential work inside of your
code then you're going to have a maximum
theoretical speed up and that's what
this graph shows so here's executor
traits inside of HT PX just to give some
more clarity about the things that does
it exposes in X the executor type the
execution category of the executor so is
it sequential or is it parallel
big ones also the future doesn't have to
be a standard feature or even in HPA
axis case the HP X future it can be
defined by the executor but by default
it would refer over to one of the
defined features and we also implemented
our async execute and execute and in
addition there's apply execute so we
just do that in so that it calls uses
the executor to call an apply function
which just sends off work and it doesn't
require you to know what the return of
that is and so you save any overhead you
might have had from having a future
however we want to know more information
about these executors because they can
do different types of things so we might
have an executor that has three threads
on it and it so it has three threads but
we don't know that if we're calling them
with the parallel algorithms so we also
added an information traits where you
attach an executor and by default it
calls the so if by default looks for the
executors function for get OS threads or
get threads and it'll use that number to
decide how to break up that work also
those other traits just to show it's
it's modular and there's different
things you can expose to the user so
there's time use act as executor traits
um so do something on this executor at
this time or after this length of time
and here's our parallel executor I want
to talk about the top part but first I'm
going to talk about the bottom part to
get there and this is just the
implementation all it does is implement
a sink execute and the rest is done by
executor traits and that's at the bottom
the template stuff at the out the
template of the return type and all that
says just the return type of the future
of the function and then
hpx a sink with a launch and that launch
can be sent over in the constructor and
so by default it does launch a sink and
la chasing creates another thread for
the future to be for the function to be
run on but because hpx sort of manages
the threads under the hood that we also
expose something that the standard
launch policies doesn't and we called
that fork and so fork lets the main
thread of execution take the function
call F and then it lets other threads
steal than the what was originally
called on the main so that's
continuation stealing as opposed to
parents dealing and that that was made
clear to me by n 3872 which is a primer
on work stealing it was very useful for
me because I was googling child stealing
and not getting any results that helped
so when you call it a sink you you're
going to create another third of
execution and so in in 3872 they have
this example so you call E and then you
spawn f and if you spawn f with in HPA
or and a sink with a async launch policy
then f will be up for grabs by other
processes and then the main third of
execution will go on to call it G and
then when sink is called oh we're going
to make sure that F is finished and then
we can continue with H so if there's no
other processors available to order of
execution would be EG fh however if we
were to use fork we could change that
order of execution potentially so it
would go e the main thread of execution
would then execute f when we spawn it
with a fork launch policy but then G
would be up for grabs by other
processors and if we didn't have any
other processors the order of execution
would have to go e f g h and i make this
point um not just because it was a good
primer on work stealing but because at
the bottom and this is they explain why
this is important if we have no other
processes available and we're using
a sink to launch our in F calls then
that a sink launch policy will call F n
times so o n operations of that function
call so we're wasting a lot of memory
and then once sink happens it has to go
through and call f0 up to FN so we can
avoid that and we can limit the maximum
number of function calls to be the
number of processors available if we use
the fork and so with fork the main third
of execution if there's no other
processors just calls f0 all the way up
to FN and then it sinks and so here are
some of the types of executors that we
have so far um obviously this thread
executor does what you think it does it
creates a thread and it sends its work
to that thread we have the parallel
sequential executors of course that are
defaulted so if you use a sequel seek
parallel asik policy then it uses a
sequential executor by default and ditto
for parallel policy then it uses the
parallel executor by default they're
also thread pool executors serves
executors and I found this interesting
because hbx does locality stuff so it
does stuff on other nodes of computers
um there is a notion of a distribution
policy which existed before this idea of
X of policies for the parallel
algorithms but now we can have an
executor that we create this would
create this executor by passing and
distribution policy and the distribution
policy defines how work does is done on
other localities and now we've just
created an executor that does work on
other localities and it was fairly
straightforward to do and we can treat
it like any other executor so taking a
step back we've are through in 4406
defined a way for launching work through
the information traits we can have a
flexible decision making um but we still
haven't defined a general mechanism for
great size control when we break up that
work too
down in parallel and this was done with
the execution parameters so the idea is
that it just has a function that defines
how much work should be done given the
number of processors available as
defined by an executor so it's similar
to the openmp dynamic static and guided
but we've also used in auto chunk size
and what that does is it measures an
amount of work and then based off of
that amount of work it create it decides
what the rest of the work item should be
done so here's how the user interacts
with the pedal algorithms and it's via
these sorts of policies um the idea is
that it's easy to make your own type of
executor or chunk size or executor
parameters armed to the first one we
have pair with and auto chunk size that
is also equivalent to pair because
that's the default chunk size um we can
have our own so we can have pair with my
executor and then I'll pair with my
executor on my chunk size is that I
think I got that backwards pair on an
executor with a chunk size my bad um but
at the same time the policies support
tasks so we can also get future results
of our parallel algorithms and we can
use the task X cast task policy and it's
just the same way so in doing this we
sort of taken some concepts and created
those concepts and so this is just a
summarized there's restrictions so can
this work be done in parallel can it be
sequential and that's defined by the
execution policy as it was originally
intended but to make sure that the
different things that can be done are
separated then we have the other
concepts so the executor defines where
work is done and it also says hey I do
my work in parallel or I do my work
sequentially and then of course there's
the grain size of work and if we want to
do all everything on one note or
one grant block of work or if we want to
do on many blocks of work we can define
that with the grain size and of course
there's no cost of not using any of
these and just using the pair and
sequential all right so um we had Daniel
talk about this great interface for
these algorithms and how we now have all
this flexibility for them but we need to
write these algorithms so the thing with
these parallel algorithms as defined is
they will all take a range and they will
all need to be partitioned and some
action will be applied on this range of
the important thing to note is that
since you can do this you can create
partition errs to solve this general
problem with parallel algorithms and in
hpx we wrote three different partition
errs to implement all these algorithms
our first partitioner is the for each
partition ER and this is the simplest of
the mall what it will simply do is it
will take a range it will chunk up that
range into equal partitions as defined
by the amount of course you have in your
computer or if you specify it a certain
chunk size and it will apply some
function f on each chunk so it's used in
a lot of our really simple algorithms so
implementing for each for example of
course say films well it'll work by say
you have a range of 12 elements and
let's say you have 3 course so what the
partitioner will do is it will
distribute that range into three equal
chunks containing four elements each and
you've passed in a function f and this
function f will be applied to each
separate chunk and what that does is it
allows us to execute on these algorithms
in parallel and solve this problem so
inside of for each n um the simple stuff
is the 10 points we're defining at the
top some just some extra stuff and where
the reason we call it parallel is
because for each n also has a sequential
version because we also want to give the
user a chance to just sequentially
execute for each n and then in the
middle is the important part when we're
actually calling the partition
wera forwarding any of the arguments we
needed to and then this lambda function
is what will be applied to each chunk so
inside this lambda function we're
looping through that chunk of data and
we're applying that function f you
passed into forage n on each separate
element of the chunk at the very end
we're returning in algorithm result of
it so in the case that you asked for
task will give you a future back that
result and if not we'll simply give you
the iterator back next up we have a
partition ur so this is a little more
complex than a forage partitioner it's
got one extra step and what it'll do is
that after we've partitioned this range
into equal elements will apply some
function onto each chunk and will store
the result of that function on each
chunk inside of a vector and then that
vector will be passed to a second
function so this is what you have to
step algorithms like finder search give
another example of 12 elements three
cores it will evenly divide your data up
first and what it'll do next is what's
different here is that you have some
vector which is set to the result of
each function applied on these chunks
and at the very end you have an
additional function past in which will
be applied to the vector V so um this is
useful in an algorithm like reduce for
example because what we're doing is will
reduce is similar to accumulate but it
can be parallelized you have the first
lambda function like we had last time
with the for each partitioner and what
this will do is inside each chunk in
parallel it'll grab that initial value
and accumulate the rest of the chunk so
now what we're trying to do is we have
each chunk accumulated and we need to
accumulate the chunks together and
that's what this second lambda function
does so this is the function that will
be applied to the results of the vector
an interesting thing that we had with
reduce actually was that we had a
coworker come in when we finished some
of these algorithms and she needed to
write a benchmark that computed the
vector dot product in parallel and was
so interesting
is that since she heard about us she
said okay we can go ahead and use reduce
for example because if you were to do
this sequentially let's say you have an
array of X values and array of Y values
to accumulate a dot product of this it
would be really easy you could zip those
to erase together you can set the
initial value of the accumulation 20 and
then inside each you could loop through
it find the dot product that should
actually be multiplied on the right hand
side inside that lambda function give
multiply the 2 x and y values together
of their location and then add it on to
the result but we found with reduce
there's some requirements I'd that
function that actually limit us and we
can't do this so as you see here in the
first lambda function we're setting the
initial value that initial value t val
is stuck being the type of the iterator
that we passed in so when you have
something like this and you have a zip
iterator the type of the iterator will
be a tuple in dereferenced so what this
forced us to do when we wanted to write
a dot product is we ended up having to
make the initial value a tuple because
when you dereference this zip iterator
you're going to get a tuple of the x and
y values respectively and then during
the actual lambda function this also
forced us to do tuple multiplication
because now we had the result on the
left side and then we had our x and y
values it was a horribly hacky solution
and we didn't like how we finished this
this was the actual solution we didn't
use it but we finished it and this is
what led us to write a proposal on
introducing transform reduce so this
algorithm wasn't initially in the
proposal but while we were doing this
vector dot product problem we realized
that this proposal you would need to
transform reduce in cases like this and
we proposed it in 4167 and it was
implemented later on and it's now in the
proposal but what transform reduced does
is it'll add in an additional function
you can pass and this function will be
used as a convert function so now inside
each chunk your respectively getting the
initial value but it's going through a
convert function first so in the case
that your initial value is going to be
of a tuple type you can pass it through
convert
function and get a double out of it and
now this will allow you to accomplish
anything you wanted to do as what you
did with accumulate and then the
accumulation function after we're
grabbing the initial value we also have
to pass in that convert as well to
convert each subsequent element and we
came up with a much more simplified dot
product in this case so using transform
reduce we were able to zip the to erase
together set the initial value to zero
this time didn't have to use a tuple our
addition function was simply a standard
plus so just add two doubles together
and our convert functions so it'll grab
that tuple of x and y values and it'll
just find the dot product and return
that so it's simply adding up a dot
product which is found through convert
function and we really like this
solution so this was a good addition
into the proposal and now onto the last
one and the most complex the last one in
two steps and now this one has three
steps the scan partitioner will first
partition the data and invoke a function
on each equal partition as we do with
the other ones now what's different in
the second step is that will invoke a
second function once the current and
leftmost partition are ready so this
will allow us to kind of overlap our
second invocation and then on the third
step we're going to voc a third function
on the resultant vector of step 2 so
this is really specific but it's for
cases like copy if or inclusive
exclusive scan and this allowed us to
solve these problems so we'll do another
example so you have your 12 elements
again you'll partition that into equal
chunks now let's say this is your
current chunk so you just computed this
current chunk once your leftmost one is
also ready this will be stored in a
vector V sub 0 and now let's say your
move down the chain so now the rightmost
one is your current chunk and that'll be
V sub 1 will be equal to the leftmost
end the current and then you can put
those vectors together and apply some
function on to that and that will be
stored into R which is then finally
applied to the function H so this is a
three-step process a little more complex
but it allowed us to solve a problem
like copy if now the interesting thing
when we were writing copy if at HP X is
rina straight thought it was really
simple I wrote it like the first week in
half of starting and I left it and then
maybe your month and a half later I
realized how wrong I was when someone
was trying to use it I'd initially only
copied everything that matched the
predicate but I didn't realize that when
you're doing this in parallel you need
to start squashing the array after and
this became a problem because using the
other partition errs I would have had to
do two separate parallel calls that
would have had a copy everything over in
parallel and then squash everything in
parallel and this was really inefficient
because you don't want to construct
resources execute something in parallel
deconstruct the resources and then do
that one more time so that's why we
created the scan partitioner and I just
so happened that this was really useful
for stuff like inclusive and exclusive
scan as well so inside the actual
revision of copy if I couldn't fit the
whole function on the screen but this is
just the partitioner call so we're
grabbing the scam partitioner and inside
this first iteration which we apply it
to each equal chunk we're simply
flagging any of the elements to be
copied in the second iteration which
will be the current and leftmost chunk
which will be applied to we want to
determine the distance to advance the
destination iterator for each partition
and then our last which will be applied
to the resultant of the vectors of step
2 we're going to copy those elements
into destination in parallel so this
allowed us to tackle problems like copy
if where you don't have a really
intuitive solution to begin with as well
designing some of these parallel
algorithms we realize that we wanted to
start simple and work up the grapevine
so ways algorithms like for each it
applies a really good base for building
upon other algorithms and some of these
future ones really easy to write
actually so you have the case of like
fill in for example we wrote that in
technically one line so a lot of these
algorithms are real
related to each other in the sense that
for Phil n all we need to do was call a
parallel for regen and apply a custom
lambda function to be applied to each
element and all this would do is copy
over the elements so what that for each
end is actually doing with that boost
MPL false stuff it's just we have some
internal mechanism so this will call
parallel without having to do any
checking so we know we're executing
parallel because we're calling the
parallel fill n and then we're grabbing
that parallel for each end and executing
that as well as of today here are all
the algorithms we've completed we're
getting close to finishing up we still
have some of the harder ones to do such
as sort um and maybe a couple others but
we aren't working on those currently
alpha waiting at the end of the year or
in the students future will have these
all finished up and we'll be able to do
some complete testing on all these to
make sure what we're doing makes sense
and then next up we wanted to measure
these so after we finished about half
our algorithms we wanted to make sure
that what we're writing actually made
sense and this is a quick example of one
of the benchmarks we did to measure the
performance this was a performance
measure for for each we grabbed some
parameters for the command line and
we're looping through in applying some
for each function on some data set which
you specify and then we're averaging out
the executions of it so when we were
benchmarking we compared our sequential
parallel and task execution policies
it's important to mention that task we
could do something special with them
because how I mentioned that you can
begin choosing when you want to join up
with the threads with task we could
overlap our executions so you have with
a parallel execution policy if you want
to call one parallel algorithm right
after the other you would execute that
parallel algorithm wait for that thread
to finish join back up and then execute
the next parallel algorithm but with
futures and tasks what you can instead
do is send both off at the same time and
choose to wait for them after once
you've sent both off so now you can do
multiple pieces of work and you don't
have to wait for that first one to
finish until you wish to so in order to
get out of the most out of performance
we wanted to see the typical strong
scaling graph what this means is that on
the x-axis you have a grain size and on
the y-axis you have a time scale the
grain size means the amount of work
you're putting on each thread if you
have a really small grain size that
means you're creating a lot of threads
and not putting enough work on it and
this is going to give you really bad
performance because you'll get too much
overhead of parallelism without enough
actual work being executed to make up
for this parallelism on the right if
you're putting too much work on one
thread you're not going to have any
parallelism you're going to put all of
your work on one thread and that's going
to be sequential so there's a sweet spot
for every algorithm in that you get the
perfect amount of work at the perfect
size and you'll get the most speed up
now this isn't always known so we tried
to just do an auto partition so we would
grab the amount of cores you had we
would split that up evenly with the
amount of course to try and make use of
all the threads but uh some executions
if you don't know how much work actually
occurs inside your for loops or anything
it can be kind of hard to tweak so
that's one of the things we are hoping
to work on in the future we used a
Marvin note on our harmony cluster so
the important thing note is that it's
got 16 cores so in a perfect world we
wanted to see 16 times as fast parallel
algorithms and while we didn't get 16
times as fast we saw some decent results
with some smaller data sizes so inside
each for loop iteration if we had 500
nano seconds of delay in a vector size
of 10,000 we saw about 10 to 10 and a
half times fast as a sequential
algorithm which is still great it's not
16 it's not perfect but it's still some
great scaling that on the left you have
the typical strong scaling graph where
too much work to less work now we wanted
to see if we could get even closer to 16
times as fast so we went ahead and amped
up the amount of work and what we did is
we doubled the amount of delay per
iteration and we also increase the
vectors
by 10 or 10 times the vector size so we
had a vector size of almost or a hundred
thousand and a thousand in a second
delay and here we saw a lot better
results so now that we have more work to
do it makes more sense to paralyze it
and now we're seeing about 14 kind of
near 15 times as fast and this was
really good for us because in a perfect
world will see 16 and getting almost 15
this pretty good sign that what we're
doing is currently right and we can move
forward with the rest of our algorithms
and trying to finish this up now the
cool thing with that task I mentioned is
that now that you can overlap your
executions you've almost got pseudo
perfect scaling while it isn't actually
16 times as fast what it means is that
now that you're overlapping executions
you no longer have to wait for previous
executions to finish before you can
launch more because i believe for this
one we overlap two maybe three or four
executions so we would launch three or
four at a time until we waited for the
first one and this gave us some really
good scaling results and it made us
happy because it actually makes sense to
use a task in some scenarios when you
have a lot of work and you may want to
do some extra work before you actually
wait for it one of my co-workers Martin
stump also had a really cool
implementation he had an open CL backend
for HP X and he actually used one of our
4-h algorithms inside of his work and
this was for grouping work items into
work packets and this was the actual
algorithm he used it was a simple for
each and he iterated over a range and
did some function lambda function on
each range and he saw some really good
scaling as well so this was some of his
research in that the speed-up verse
sequential you have a good grain size
and a good amount of work then you can
see that you're getting really really
good scaling with our algorithms then
this could come in handy in hpx since
we're all about large data asynchronous
programming being able to suddenly take
advantage of parallel algorithms without
having to reinvent the wheel could be a
big side bonus
so we have all these good prose about
C++ algorithms but we also had a little
bit of downside and we have some future
work that we want to do so one thing to
note about these algorithms is that
there's a fundamental design issue in
that resources will not be shared with
multiple parallel algorithms so it's
important to note that if you're doing a
lot of these algorithm calls it's going
to construct these resources for a
parallel algorithm go ahead and execute
on those resources and then deconstruct
those resources so if you're doing
multiple calls of that it's going to be
difficult to see some of the better
performance if you're not sharing
resources and this isn't necessarily a
problem with how the algorithms are
written it's just a design
implementation to take into account that
it's not as a not as efficient if you're
calling many of these algorithms at the
same time we also haven't finished all
the algorithms so of course we want to
make sure that we finish these and we
also want to perform some additional
benchmarking on these algorithms so we
know where for each is doing great and a
couple of other ones are doing great but
we want to get into the more difficult
ones like sort and we want to see our if
our implementations are worthwhile and
if we're still getting great speed up
according to all the other algorithms as
well grain size is important so right
now we're simply just splitting up the
amount of work into the amount of course
available but sometimes if you have say
a lot of work per iteration it might not
be the smartest thing to just auto
partition the grain size so we're doing
some experiments of work into seeing if
we can somewhat guess the grain size
better and see if that will improve your
performance as well and there's a lot of
experimentation that we can do with
custom policies so I've example that is
it we don't have but if we have a GPU
then do some work on the GPU otherwise
do it on a NUMA domain with this chunk
size um but also we can have make
policies that connect with each other
and use introspection tools that is
inside of hpx already so
there's performance counters that
measure your power consumption we could
also measure how long some function call
takes inside of the async execute and
based off of that we can have an
executor that minimizes these things in
some way when called with the function
repeatedly or at least called with it
some grain size inside of a parameters
repeatedly and but those are just ideas
at the moment um we can open the floor
to any questions yes well we do have
inner product I believe but we found
that you have cases like transform
reduce so the question was why would you
not use inner product instead of
transform reduce right or whoa yeah well
we thought that transform reduce would
be a necessary algorithm and that inner
product could possibly solve that
solution as well but um it's good
designed to have some algorithm that
could transform as you reduce over some
range and we felt that would be useful
to include in the algorithm list because
I'm sure there are some other uses that
transform reduce could have over the
inner product any other questions
mmm so the question was why would you
not put the execution policy at the end
instead at the beginning and I think
that was just a design implementation at
the proposal writer wanted because you
wanted the how would execute to be the
biggest priority in the algorithm so
whether you want to execute in
sequential parallel that should be your
first stop before you put in the rest of
your arguments Marshall and so the
common was that there's different sizes
in the argument so you want to
disambiguate whether it's sequential
parallel by placing it in the beginning
the difficulties with sort are just the
actual designing how the sort algorithm
would work so I never actually got to
the point where I would start tackling
sort we apparently have one person at
stellar currently attempting to write
the sort algorithm but it's mostly
communicating between these chunks
because you can't simply sort each chunk
because some data might be in reference
to the first chunk and maybe the second
one so we're likely gonna have to use
something similar to the scam
partitioner where you have these
overlapping chunks that you can start
communicating in between well I I was
i'm communicating with the person who
started doing that and i think how they
did it was or how they're doing it is
breaking it up into
separate parts and then combining them
back but I don't actually know um but it
does lead into a problem with the
executor parameters because then it's
harder to define it does it could use
recursion just in some sense so in that
case we might be able to have the user
control um how deep that recursion can
get but at the same time that's not
really the same as chunking into
different partitions that the algorithms
currently have and so that becomes a
little bit tricky of how effective the
executor parameters are or if we're just
going to scrap them for some algorithms
and their question was why is sort hard
any other questions yeah so the question
was why don't we just give a compiler
error if we're trying to paralyze input
iterators and that's because we don't
want to place any new requirements on
these algorithms so anything you could
have done before simply by adding the
part tag it should permit parallel
execution so the design thing is we
don't want to force the algorithm to
parallel we're permitting a parallel
execution so in cases like that you
might not know the types of your
iterators but you can say i want to
permit parallel execution in this case
and you know it won't give you a
compiler error because the algorithm
requirements are the same as before so
that's why we decided that so the
question was how is you need to term
that the algorithm is run
deterministically and
I can't give you a solid answer for that
you should see me after all right well
oh so the question was could you see
this tongue into daniks such as
mapreduce and such um I can't say for
sure because I'm not as experienced in
this stuff I'm also new to parallel ism
I'm simply just a student worker that
really likes this stuff but uh um I
could see having applications just due
to its really broad and what we're doing
and it could be applied to a lot of
things and especially the runtime system
the runtime system that we're doing this
inside is applicable to applications of
any scale so i could see the work being
moved over a little bit and to speak to
that there is some work at the stellar
group about in parallelizing data and
file systems so yes thank you guys for
coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>