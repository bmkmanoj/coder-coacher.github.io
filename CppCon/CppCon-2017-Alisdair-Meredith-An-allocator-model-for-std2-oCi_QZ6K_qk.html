<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Alisdair Meredith “An allocator model for std2” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Alisdair Meredith “An allocator model for std2” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Alisdair Meredith “An allocator model for std2”</b></h2><h5 class="post__date">2017-10-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oCi_QZ6K_qk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Okay quick introduction for those of you
who don't know me.
I'm Alisdair Meredith,
I work at Bloomberg,
my thing I've been playing with recently
I've now got a Twitter handle
AlisdairMered at Twitter but...
Generally I tweet just around
about the ISO standard meetings
to give a quick update
as to what's been happening that week.
So my other sin is I like to
attend ISO standard meetings
which is where this
presentation's going to go,
I'm going to suggest
how we, speculatively--
there is no endorsement
from either Bloomberg
or the ISO presses of this yet--
but this is something I hope to be cooking
over the next 12 months, the
idea of a new allocator model
for the new standard library.
So we'll start with the simple question of
what is an allocator.
Let's be sure we at least know
what we think I'm talking about.
An allocator, I can look down
here and see more clearly,
is a service that grants exclusive
use of a region of memory
to the clients that are
going to need to use memory.
That's my basic notion of
what an allocator has to do.
It's got a second side that
if we've got nice, well-behaved clients,
they might want to tell the allocator
when they're finished using that memory
so it can reuse that
region now somewhere else.
So that's my basic model of
what an allocator is and does.
Why do we want allocators?
And the fundamental question here is,
is the new operator not good enough,
can we get by just making
the new operator better?
And don't get me wrong, we
all want better new operators,
that's not a reason to stop investing
in research and making this better.
So the fundamental reason
I believe for allocators is performance.
Shortly followed by performance,
and then more performance.
This, from talking to folks
who've actually tried
working in this domain,
seems to be the key driving factor.
It's actually not what I tend to
use them for so much at work
and I tend to use my fourth
option here, instrumentation.
I find being able to
enrich the information
you have with an allocated query,
its usage, has been
very beneficial for us.
But I'm an outlier,
performance is the main reason
this whole talk was
created in the first place.
And the final note on
things people sometimes
want to use allocators for
is special memory access.
I've got a slide on that
just coming up shortly.
So when it comes to performance,
I'm not going to say
too much about that now
'cause hopefully tomorrow
at 2:00 in room 403
John Lakos is giving a talk all about
the measurements we've done, benchmarking,
to try to determine the benefits
we can get from different
allocator strategies
how useful they are,
whether we can measure it,
and we think we have some
good results to show there.
But my quick takeaway from that
with sifting through
reams and reams of data is
when we benchmark the
performance of the allocator
just doing allocation type behavior,
when we have a well-chosen allocator,
we can typically get a three
to five factor increase
on throughput on the allocator.
And if you start finding
the extreme outliers
we can certainly get orders of magnitude.
The interesting thing that
came out of this research
though was that, again if
you were in the previous talk
you'll have heard some about this,
is that better memory locality
that you get from a well-chosen allocator
buys you significantly more performance
after the allocation.
So this is the real performance
that we're trying to achieve.
By allocating memory locally
so it's friendly for cache
lines in future access
and use throughout the system,
you're going to get
generally a faster program,
certainly on your hot path
and the performance-intensive
parts of your code.
This is where the notion
of memory pools comes in.
While different structures
will allocate from different pools,
so each pool will keep the data local
for a specific container, graph,
overwhelming data structure,
whatever you've got,
having a separate pool
for each data structure,
at least the ones you care about
for your performance critical code,
can make a real substantial improvement
on the runtime of your application.
Another observation when you've
got long running processes,
memory fragmentation over
the whole address space
can become a concern if you're
trying to run this process
for months, weeks, years, whatever.
And if you can allocate memory locally
off the current stack frame,
if you've got small allocations,
this can substantially,
again, reduce the eventual
drag on the system as a whole.
So that's another place
where we can gain performance
and indeed gain reliability
over the long running evolution
of long running server processes.
I hate to hop on this but again
I just sat in a talk where we spent
an awful lot of time talking about
the benefits of getting
good cache locality
so I feel redundant having just
a line or two about it here.
But the more you can take
advantage of well aligned memory
with the right pages to shoot
through your L1 L2 caches,
the more performance you're
going to get out of your process
and to do that you really want to be able
to give the allocation
system some hint to say
this memory I want to
keep collected over here
and this memory over here so
they're in distinct regions.
And you can't get that
from the simple hints to operator new.
There's two main strategies
we've been using,
referencing on those previous slides
and John will be talking about tomorrow.
When we try and allocate off
the thread stack, that keeps,
that's as local as memory can get
because that's where your
instruction processes,
everything is going through,
it's where all your local data is,
and you're not having to
do additional management,
the stack just takes care
of itself essentially
with the compiler.
The other thing that is very beneficial is
as I said managing pools.
When we, I should probably have
an extra slide in here about
the actual allocators we use.
The notion about stack allocation is
we're going to provide a pre
sized buffer on the stack,
and at some point you
might exceed that buffer
in which case we'll go and grab a bit more
from dynamic memory.
We've got a buffer that we treat
as one long region of memory,
that is very key to allocate from,
all we're going to do is move
the stack pointer through it,
and we never give that memory back.
This is very good for certain
kinds of data structures
and this as I say
the stack based notion of
some code works very well.
The problem is if you
start churning memory,
you're handing it back to
the allocator and reusing it,
that quickly will exhaust what it can do.
So the other notion is you
have a pool that will--
that will manage its allocations in there
so that when you hand the
memory back to the pool
and you request a similar map,
so I'm inserting and erasing frequently
from a single container,
that memory gets reused
much more frequently.
And then again this is where
you want separate pools
for the separate data structures.
And you can layer one of
these on top of the other
which then means my problem I was saying
with the very efficient stack allocation
where we just move a stack
pointer all the way through
and we never reclaim the memory,
if you put a pool
allocation in front of that,
that will mitigate the fact
that you've now got something
that is managing that bit
of memory you've grabbed.
My bad, I should put an
extra slide or two in there.
But the fundamental question
is why do we want allocators
and is the new operator not good enough?
And the observation is most of the time
the new operator actually is pretty good.
It's good enough.
But when you get to
the essential critical
parts of the system,
the parts where performance
really matters and you,
as the application developer
have much more information
about the context
and the way you're using the memory,
you can make a real difference here.
And when you want to plug something in,
operator new doesn't give
you the knobs and whistles
that you need in order
to control that facility.
So that's where we want to use allocators.
As I said, an extra facility we have,
once you have the ability
to plug in allocators
is you can do extra
utilitarian things with them.
You can have them to support debugging,
you can log your memory allocations,
which on the way towards
being a profiling system,
and the key one I use
them is for test drivers
so I can confirm that when I'm
testing my code in unit tests
that the memory allocation behavior
is actually allocating the way I expect
and I don't have any expected
leaks, double queries,
I'm suddenly allocating
more memory than I expected,
why did that happen?
And having an allocator that's
instrumented for test drivers
is very helpful in maintaining
the quality of your code
and getting the behavior
you believe you're getting.
The final column we have the
notion of special memory.
Shared memory is probably
a common example of this
where we might have some
memory we're trying to share
between processes so we need
to access it in a special way.
Some hardware systems have
memory with special access routines
that you have to access in different ways,
newer systems maybe.
I'm not sure if getting access
to the VRAM on video cards these days.
This isn't my area of expertise.
But I'm assured that these
systems certainly exist
and hopefully some of you folks out here
have more experience with them than I do.
But that's another aspect
that people look to for allocators.
So with that...
I'm dying of thirst here.
It's been a long day already.
I'm going to take a quick
look at the brief history
of allocators in the C++ standard.
So let's go back to C++98.
Why did we add allocators
to the original standard?
And the answer was simple:
it was near and far pointers,
back to those funny accesses again.
And that was essentially the only reason
we had to add allocators to C++98.
But why did we do it in
the mechanism we did?
It was a speculative notion that
if we have to customize it for this now
we're going to want to customize it
for more things in the future,
so let's put in the facility
with a customization point
that we can then experiment with
and start doing more
interesting, useful things with.
So the basic notion...
I'm probably teaching my
grandmother to suck eggs here.
Who's actually played
around with allocators
in the C++ standard
containers and things before?
So I'm seeing about half the audience.
Basic model is if you
have a standard container,
the template parameters have, at the end,
the idea that you can say
this container works with a
different kind of allocator
than the standard allocator,
that I am going to supply.
The model that we came up with
is that allocators are
allocating memory for objects.
So the allocator that you
plug in to your container
is also going to be a template
that's parameterized on the kind of object
that it's going to allocate for.
So for a vector allocates int--
A vector of int and a vector of float,
they're going to have
two different allocators
because one knows how to allocate an int
and one knows how to allocate for floats.
Even if they happen to be the same size
returning exactly the
same amount of memory,
they're different allocator
types doing different things.
Which means that each
allocator and each container
with these different allocators
being plugged into them now
is a new type in your type vocabulary.
And vocabulary is an important issue
when it comes to API design,
because if I want to
have easy interoperation
of my code across a system
I need to be passing common,
well-understood types.
And if every time I want to do something
subtly different with
my allocation strategy
I create a new type,
is that new type a reasonable
type to have in the interface,
do I need to do translation
now between my type,
so you might be copying
the data around again.
So just for the sake of vocabulary,
having allocators as template
parameters on our containers
could be a bit of a problem.
But this also poses a bunch
of other advantages as well
so let's not throw the baby
out with the bath water
straight away, we'll do it
in about another 20 slides.
The other thing we got with C++98 was,
because the whole notion of an allocator
was a bit speculative and new,
we gave implementations freedom to ignore
a couple of properties that you would get
from plugging in an allocator
when you create your objects.
And first thing we did in C++11,
is we started trying to tackle the notion
of how well can allocators
be used in practice
where there's removal of weasel words.
So the notion that back in C++98
you might have heard that
allocators are useless
because you can't do anything with them
certainly seems to be true in C++11
because little bits of
wording that told vendors
I can make simplifying assumptions
but ignore the vast majority
of what your allocator is going to do,
those have gone away.
And in order to make life a little easier
for the container vendors,
the people who implement data structures,
we provided the allocator_traits facility
which does all the clever work
to try to figure out sensible
defaults and routine behavior
if you don't have an allocator
that provides everything
so that allocator developers focus on
just what their allocator does,
container folks say I'm
going to access the services
of the allocator now through
this allocator_traits,
rather than going down
directly to the allocator.
And we threw in an extra allocator
in addition to standard allocator,
standard scoped_allocator_adapter,
which basically is a neat package
for supporting nesting
of different allocators
at different levels of your system
so that if I just have
one allocator in my scoped application
it would mean that my
elements will use the same
(inaudible speech) as
the container itself.
And I'm doing something
funny with my microphone.
I'm jumping in and out here.
So, just a quick recap
on the weasel words that we struck,
going way back to C++98.
There was an assumption that all instances
of a given allocator object
would always be the same, they
were always interoperable.
And the translation there is
that your allocator objects prior to C++11
could not reliably have state.
If your allocator object
doesn't have state,
it's really hard to do a lot of
the interesting things people want to do
to make allocation more
useful and powerful.
And in particular, you're not
allowed to have a pointer to
another mechanism that's looking after
a lot of that allocation for you.
The other part of the weasel words
was we got a bunch of
typedefs that you have
in an allocator that say, this
is the kind of pointer I use,
this is how you measure
the size of allocations
and so forth,
and the implementation was
allowed to simply assume
that these are always raw pointers.
And that means that you
can't have allocators
returning fancy pointers, C++
smart pointer type things,
that allow you to encode those
special rules for allocating
at a special memory region,
and making sure I've got
pointers that do all
the appropriate wrapping
to get into that special memory region
that I am trying to do clever things with.
So by striking those bits of weasel words,
C++11 allocators now
can support the models
to do all the things that
we were talking about
and want to do.
A quick walkthrough of what
allocator_traits looks like.
It's a class template
parameterized that says
on the allocator that you supply to it,
and your allocator has to
have a nested value type
because this is still
modeled on the C++98 model,
allocators allocate for objects.
So the allocator that I
plug into allocator_traits
is going to be a class template
and it's going to say
I know how to allocate
for this kind of object.
And then we have a bunch of other typedefs
that are really useful when
you're dealing with allocations
to say what kind of
pointer, concept pointer
to the allocated object I'm going to get.
Pointers to void being addressed for
general access into that memory buffer.
And if the allocator provides these,
allocator_traits picks them up,
and otherwise it will compute
a good default for you
so that people don't have to worry about
my new CI for their allocator
that they're not dealing with.
Likewise we need to be able
to rebind an allocator.
Because allocators have
to be class templates,
we can rebind from, I've
got an allocator for int
and I want to store it in a list
so I need to have a
node that holds an int,
so I need to allocate something different
than the allocator the user gave me.
I can rebind it, have an allocator for
the kind of thing that I
want to allocate for there.
From the same family, all
through the allocator_traits.
And finally things got
interesting where-- Oh, sorry.
When you go to actually
perform you allocation now
you're going to use these
static member functions
of the allocator_traits template
rather than access the
allocator directly itself.
So it's used as a utility class
where all the member functions are static,
there no intention that
you'll ever instantiate
an object, or create
an object of this type.
And the first argument to
every one of these functions,
you will notice, is the allocator
that you're trying to use.
But this allows the
traits class to synthesize
the operations that the
allocator might be lacking.
Interesting thing I'll
just call out in passing,
as I always do.
The allocate and deallocate functions
work in whatever kind of
pointer type your allocator has,
whereas the construct and destroy methods,
we've got customization points to say,
&quot;Now I'm constructing something
&quot;that might want to know
about the allocator I used.&quot;
Those actually just traffic in the address
of the object I'm creating,
and for convenience they're templates
so I don't need to keep rebinding these
as I'm trying to construct
three or four different things
with the same allocator.
So that's just a little
gotcha to be aware of
that doesn't catch people out
until they start using smart
pointers for the first time,
and then some will discover these APIs
are subtly different than
they thought they were.
And then we come to the joys
of allocator propagation,
which I've got a few
slides coming up shortly,
so I'll jump over to those,
but those are another aspect
that you'll find in the traits
that turn out to have a big impact
on how we use these things.
So progress in C++98, we now support
a wide variety of allocator models.
Ah, back.
You'll spot we've got four different
traits here for propagation.
Most of those are boolean
so that's a binary choice,
so that's at least 16
different allocator models
depending on how we plug things in there
regardless of what we're doing with those
because these kinds of allocators
will all behave very
differently, as we'll see.
So we're actually supporting a wide--
Much more than just the
new allocator model,
we've supported a facility that supports--
I'll not say every imaginable allocator--
more than I care to think about.
But the interoperability
and vocabulary types
remains an issue because the allocator
is still tied to the type
of object it's allocating.
The data structures that
consume these allocators
are still templated on the allocator type
that I'm plugging into the
container, the data structure.
Because we couldn't change that.
This is just an enhancement of
the existing standard library
and if we make a change of this model
we're instantly giving up
compatibility with that library,
and we can't do that while
maintaining that same library.
Skipping over C++14,
I don't remember much
happening with allocators in 14
apart from maybe some
cleanup of the wording.
How do we improve allocators in 17,
just to bring us up to date
with where we are today.
Sorry I'm spending so long
on laboring the history
but it's good to know what
we're trying to change,
and what problems we have,
before we start trying to solve them.
We've got a new trait, is_always_equal,
to say that these two allocators--
any two objects of this allocator type
are always interoperable,
so I don't need to actually take a look
at the state of those allocators
to see can we interoperate,
one allocate and one deallocate
the same region of memory.
And this is very useful when it comes to
providing no exception specifications
because if I know that the two allocations
are always the same,
I can start making better
no throwing assumptions
about move semantics
in a few other places.
So this new trait ends up appearing
in quite a few of the
exception specifications
related to swaps and moves
throughout the standard library.
We also added another facility
called polymorphic memory resources,
which again will be--
Actually I don't think this is
too much of John's talk tomorrow.
This is the inspiration for
John's talk and this talk.
But the notion of a
polymorphic memory resource
is we provide the new model for allocators
that rather than allocating
objects, just allocate memory.
We then wrap that up in
a C++98-style allocator
that knows how to allocate objects
using this memory resource.
But the notion now is
we have memory resources
that are just about managing memory
and we wrap them in a
C++98-style allocator.
And in addition to that
we have pmr containers,
which is pmr, short for
polymorphic memory resource,
just trips off the tongue,
is a namespace within std,
and within there we have an alias template
for every standard container
that just says rather than
using standard allocator,
I'll use the allocator that
wraps the pmr resources instead.
Because we don't need
to do any other work,
it's just plugging in a
different allocator policy.
All the work for doing
allocations correctly
is already implemented in
the standard containers.
So this is not a whole new
set of separately implemented
containers doing lots of
different, interesting things,
it's just an alias template that says,
&quot;Yeah, I'm plugging in
a different allocator.&quot;
So the pmr containers have
one fewer template parameters
because the allocators are
already picked for you.
And the final tweak we did for C++17,
not that relevant for this talk
but I'm trying to be complete,
is we now require that when you use
those fancy pointers for
special memory regions,
the pointers they return have
to be contiguous iterators.
We used to have the notion that,
perhaps somehow for a secure system
you might want to actually
have a security cookie
or something between each allocation,
and therefore when you
implement the pointer semantics
your operator ++ would have to know how to
skip over those little cookies,
but people doing address
arithmetic on the allocated objects
are now going to get the wrong answers.
You have to do everything
through the pointer type.
By requiring those to be--
fancy pointers pointers to
always be a contiguous iterator
you can now do pointer arithmetic safely
with the objects in your containers
where you've got contiguous guarantees.
How how does the polymorphic
memory resource system work?
We have a basic abstract base class,
std::pmr::memory_resource,
that provides a basic interface
for allocating and deallocating memory.
I've got it on the slide to come.
Clients are going to store
pointers to that base class,
and we're going to use a traditional
vtable-style C++ programming.
We've got a pointer to this thing
is the service that's providing you memory
and now clients can plug in
their implementation of that interface,
and at different times I can
plug in different allocators
without changing the type of
the object I'm dealing with.
I'm just picking the
strategy at runtime now.
And going back to when I was
talking about those propagation traits,
the key issue is that the resource pointer
never propagates.
And for folks who haven't
dealt with stateful allocators,
I'm gonna spend a couple of slides now
talking about why the
propagation property matters
and can be surprising to people.
The final issue that we have
with the pmr model is that--
When I have a container using
polymorphic memory resources,
so I've got a vector.
If I have a vector of strings,
I want to guarantee that the strings
are all using the same
allocator as the vector.
We don't support the idea that
I've got a vector of strings
and each string is going to
have a different allocator.
The whole notion of being able
to rely on memory being in a pool
is that all the elements in the container
have to have their memory
coming out of that same pool.
The work is, the allocator is going to say
when I construct elements
in my container--
this is what the construct method
is there for in allocator_traits--
I'm going to make sure I propagate--
Propagate is what I want to use,
but it means something in the standard.
I'm going to pass that allocator
down to the element when I construct it.
So this iIs the basic
memory_resource interface
which looks a little small
on this slide I'm afraid.
Basically you see it's
got a virtual destructor.
It's got public member
functions that are not virtual,
allocate and deallocate,
and they take a number of
bytes that you want to allocate
and an alignment.
And the alignment will
default to max_align
if you don't supply it.
And then we have protected
pure virtual functions
that are, again, the
do_allocate and do_deallocate
that the memory resource
is going to implement.
And the idea here is, with
the public member function
I can have a defaulted argument
so I've effectively got two
functions for the price of one,
but when I'm implementing an allocator
I don't want to be implementing
two different virtual functions,
so the function we call always takes--
that we dispatch to internally--
will always take the alignment,
even if I choose to ignore it.
These methods are virtual.
The allocate and deallocate
are the key member functions
that clients of the
allocator are going to use.
The do_allocate, do_deallocate,
using this idiom, as I said,
so we can not worry about funkiness
with default arguments
on virtual function calls
of what the memory resource implementers
are going to implement.
And our choice of vocabulary type
for how do I refer to memory is a void*.
We have the object, it could be void*,
it could have been char*.
It came into 17, it could
even have been byte*
if byte hadn't landed so late.
But void*, on balance, turned
out to be the right answer
as we were developing this proposal.
I'm going to skip the
is_equal part for now.
So idiom and usage of pmr.
The notion of my memory resource
is it's going to be an
object like any other
but it owns and manages
a region of memory,
and I'm going to create
it locally in my function
on the stack just like any other variable.
And now once I've got that object,
the lifetime of objects below it
can happily use that memory,
because as long as they're destroyed
by the usuals of the C++ language
before the memory resource
object itself is destroyed,
I know I'm not going to have any funny
pointers and references
to memory I don't own.
So the basic model is,
in a function you're going to create,
on the stack, your memory resource object.
That slide I was missing earlier
about on the stack or in the managed pool,
those are the kinds of things
we're talking about here.
And then I will allocate out of those.
I will pass those memory resources
to the containers I want to be managing.
I tried going through this too quickly.
I don't have enough informative
slides unfortunately.
The first time I'm giving
this talk and I'm realizing
I'm afraid in front of
you what I'm missing.
We've also got three other notions
of special kinds of allocators,
or terms we use for the allocators
or these memory resources.
I keep going on the term allocator
because that's what we've been calling it
in Bloomberg for the last decade or so.
The default allocator is we
have a system-wide default
to say if I don't request
a specific allocator for this object,
use the same default that the rest
of the system is going to use,
and by default that's
going to be operator new,
but we have the ability to customize that
globally across the system as a whole.
The notion of the object allocator
matters in the context of I now have
an object that's managing memory.
I'm owning and managing a data structure.
The object allocator is the
allocator I'm going to use
to allocate nodes, allocate buffers,
things that are going to persist
beyond a member function call.
And the idiom is if someone
invokes a member function
and I have to ask the question,
&quot;Which allocator should I use,&quot;
I've got the default allocator
or I've got the object allocator,
because I know which
allocator my object is using.
And I'm just creating a string object
that's going to be used within
this function and go away.
Always use the default allocator for those
temporary object allocations
that do not exceed the function call
because those are not part
of the persistent storage
of your container,
and they're not expected to be
part of that memory resource
when people compute the
expected size of that container,
the amount of memory it's going to assume.
They're not looking for these
funny transient allocations
that occur within member functions.
Whereas if I'm going to allocate something
that's part of that data structure,
it's going to persist
beyond the function call,
clearly that should come out of,
or be allocated using
the object allocator.
And then you get the
fun things like, okay,
I'm going to do my work,
I'm going to create
this object on the stack
but then I'm going to transfer it
into the data structure when I'm done.
In that case the intent is that
the object is for persistent allocation
so you'd use the object allocator,
even if we end up throwing in--
that memory then gets reclaimed.
And final notion is we have
something we call the global allocator.
This is not part of the
standard pmr process,
but the notion is still good.
If I have an object of
static storage duration,
or thread lock of storage duration,
its destructor might happen
after main has concluded,
and if I've installed a
system like default allocator,
that's probably been
installed somewhere in main,
so I want to have allocations for global--
static storage duration objects
that might persist beyond
the registration of the default allocator.
Each would then have
its own local allocator
just for that object, or
we have a global system
that again, runs for the
duration of the process.
The other key thing though that
comes out of the pmr system
is we no longer have
support for fancy pointers.
If I jump back. Where's
our virtual functions?
We've got virtual functions
here, and they return void*,
and they work with void*s.
Because they're a virtual function,
that's a signature I'm bound to.
I cannot plug in a different fancy pointer
and try and override this member function,
because it's the wrong signature.
So it's part of choosing this ability to
plugin our allocators at runtime
rather than compile time,
we give up the flexibility
of the fancy pointer.
That's our tradeoff.
To an extent, we do get, for instance
we still want to work with
shared memory at Bloomberg
and we do this by simply ensuring that
memory segments always
load into the same address
in the address space of all the
different running processes.
It's a bit--
There's tricks that you
can do to make this work
so you can ameliorate some of this.
But for instance,
I think Herb Sutter
had an interesting talk
about using fancy pointers for
garbage collection last year,
and that might run into an issue here.
This is still something that
I don't have the syllable
that solves all the problems
of all the array of allocator models
that I was mentioning we could build,
but it's a very specific solution for
trying to ensure we can
have the maximum flexibility
that we can support at runtime.
The key thing is it solves
a vocabulary problem.
So these things are very
good for the kinds of objects
you want to be passing through APIs.
You don't have an interoperability problem
when different objects are
using different allocators.
And so we've given up the
support for the fancy pointers.
There's an implied cost here, though,
that every object now that manages memory
is storing an extra
pointer to the allocator.
This is not a cost that was paid
with the traditional standard allocator
that just dropped everything
down to standard new
or any allocator model where
the allocator type itself
was just an empty type.
John will talk about
this somewhat tomorrow.
Again, the other cost is now
that our memory allocations
are a dynamic dispatch rather
than a straight function call.
John will have measurements
tomorrow that show
typically these costs
turn out to be negligible.
But there are concern that some people
when they're down measuring
at the nanosecond level
might have some of these concerns.
The goal is that we gain enough elsewhere
that it swamps any potential
cost of dynamic dispatch
when you're not using the
facility deliberately.
I say Bloomberg have
been using this facility
for over a decade now.
Not specifically the pmr
that's in the standard,
but essentially the project
that then became that.
It's most of the same without
the support for alignment,
that's a little bit more subtle.
Our experience is that the pmr allocators
have been a big win on performance,
especially in performance-critical code
that people are invested in
making the effort to use the facility.
I've found, and certainly
other folks around the company,
but especially me, have found
the instrumentation helpful
in getting our test
drivers much more reliable.
It's fantastic, the ability
to instrument and profile
and do interesting things
with the instrumentation.
But performance is the real
key everyone is using this for.
And despite all these great things
that we're saying
allocators are wonderful,
the users still bridle at the complexity
that comes with trying to plug allocators
into the C++98 model.
It does seem right on time,
I've got 20 minutes to go.
What's my slide number?
Yeah, this will be behind.
The first time I'm giving these
I'm never very good at pacing myself
and then race through all the
interesting stuff at the end.
Complexity of C++17
allocators is a problem.
Why does complexity matter?
Complex facilities frequently
are simply not used.
They're intimidating,
they're more work than
people want to run into.
I'll give you an example.
C++98, we have a very
powerful iostreams facility,
and I know very few people
that do anything other than
use the straight iostreams
that come out of the ISO standard.
In fact they often don't
use them as templates,
they just use the typedefs
of standard stream
and overload a few insertion
operators and that's it.
Again the slow adoption of use
of allocators from the standard library.
The facility has been
there for a long time,
and it's only in the
last four or five years
we're starting to see users experimenting
and using this facility a bit more.
Certainly allocator_traits in C++11
made it a lot more interesting
to experiment in this space.
But even if we've got this facility
that we're not using that complexity,
we're still paying the price for it.
It's an intimidating thing to learn,
implementers have to pay the cost
of implementing these things,
and the complex facility, it's
more expensive to compile,
are you paying a runtime cost?
Most of that complexity is there
actually to eliminate runtime costs.
But the net complexity
is itself a problem.
One of the biggest
concerns that I had hit--
Or perceived notions of complexity
when we're dealing with the
98-- C++11's allocators,
let's get the right standard version--
is the notion of allocator propagation.
Because this is where the pmr model
and the pre-seeding model in the standard
where you have a stateless allocator
start showing interesting
divergences in people's intuition.
Certainly you're going
to provide an allocator
at the time you construct an object.
I've now said this object is allocating
using this strategy.
And I now assign another--
So I've got a string, I
assign another string to it,
and the string I assign has
a different allocator in it.
Do I replace the allocator
my string said it was using
and say, &quot;This is how I manage my memory,&quot;
because I'm now being assigned
the state of this other string
that's saying, &quot;No, this
is how I allocate memory.&quot;
Which allocator should be used
for the purpose of the assignment?
And this is where we have those
multiple propagation traits
in the C++11 allocator_traits
template saying,
&quot;Well, different people have
come up with different answers,
&quot;they want to do different things.&quot;
In fact we gave them multiple dimensions
to do different things.
We've got this dizzying complexity
of the ways that we can do these things.
I'm looking for a point I
thought I'd got on that slide.
I need to walk through
my slides more frequently
before I give them, I think.
Not getting a bold, confident
portrayal today, I'm afraid.
So in terms of the pmr model,
and the model I'm going to propose,
you do not rebind
allocators on assignment,
on swap, on any other operation.
The notion is when I construct an object
I'm going to say I know how this object,
within its lifetime,
expects to manage memory.
So I'm giving it an allocator,
it's fixed for life.
And the reason this matters is
because I'm also going
to make the assumption
that all the elements that I allocate
are going to use this same allocator.
So if I have a vector of strings,
I expect all my strings
to be using the same pool
that I was given, if that's
what the allocator does.
Everything is using the same allocator.
Somebody assigns to one of
those strings as my data member
and the allocator swaps out underneath me,
I've broken an invariant that I assumed
when I designed the whole data structure.
So that's one fundamental
reason why we do not want
allocators to be reassigned.
Pablo.
- [Pablo] I'll just point out
that I have a talk on Friday
about some of the issues
you're talking about here.
So if it helps you to move on
to the things you actually
wanna propose, (mumbling).
- Pablo points out he's
going to go into more detail
about the problems I'm
talking about Friday,
and specifically about the propagation.
- [Pablo] (unclear speech)
- Lots to do with the complexity that pmr,
you might hit it with
implementing it, it helps solve.
Very quick, John?
- [John] (unclear speech)
mention that pmr simplifies,
not complicates, the use
of the allocator model.
(unclear speech)
- John wants to point out
that the use of pmr simplifies
rather than complicates
the use of allocators
and yes, that's the
point I want to get to.
But when people hit allocator
propagation for the first time
it's a road block and it stumbles people
because when you have
a stateless allocator
like standard allocator,
does it propagate or not?
It doesn't matter because
you're going to get
the same execution regardless,
because there's no state in the allocator.
And as a consequence--
Some consequences follow from
allocators not propagating
that I'll defer to Pablo's talk on Friday.
The traits give us control
over the propagation strategies
and the default in the
C++11 allocator_traits
is at least the allocators
don't propagate,
which is the default we're
going to want for the pmrs.
The other reason that propagation matters
is when I imbue my container
with an allocator--
Remember the model I
gave you at the start.
I'm going to have a memory resource,
and then I'm going to have a
container using that resource,
and the elements of that
container use that resource.
The reason that I know that
I'm not going to have to worry about
memory being used outside
the lifetime of that resource
is we've got the guarantee
that that whole data structure
lives in that resource.
If someone starts being
able to move that memory,
those elements in and out of that memory
by changing the allocators,
it becomes very hard to reason about
the lifetime of the allocated memory--
of the managed memory that's
managed by the resource.
So we really don't want
people swapping out allocators
once objects have been
created underneath us.
So the big problem we've
got with the complexity
of the propagation model that's in 11,
is it's too versatile and
it leads to strange things.
What does it mean if
an allocator propagates
on move assignment but
not swap, or vice versa?
And writing test drivers for
these things is horrible.
Implementing the thing that
passes the test driver is worse.
The syntactic overhead of
dealing with a lot of this
is relatively high.
If you're now using C++11 allocators
using the allocator_traits facility,
all your management of memory
now has to go through allocator_traits,
and we've now got this very
expert-friendly facility.
We want to move away from expert-friendly,
that's what I'm trying to get to.
Because we don't want people
to worry about allocators
unless they're using them,
you end up with a default that
we've a constructor if I
don't give you an allocator
that just says use the system default,
and then one that does use an
allocator that I can supply.
So we're potentially doubling the number
of our constructors now
because every constructor
has to have an allocator-aware version,
which we might be able to handle
with the use of a default argument,
but if we do that we now need to
have a default argument at
the end of every constructor
so that if I've got a constructor that has
four or five default arguments,
such as in unordered_map,
I now need variants with an allocator
at the end of each list,
which produces all the
constructor spam that we saw in...
unordered_map, for example,
in C++11, it has eight constructors.
In C++17, it has 15 constructors,
all to solve the same problem.
We're just dealing with
those default arguments
so now we've got a few constructors
that delegate to other constructors.
And part of the way we solved this
is we just want a different
way of passing constructors--
allocators, sorry.
So we've now got a convention where
we pass allocator arg T
and the allocator as
the first two arguments.
That's great for all
new code going forward.
They can have any many
of the default arguments
after this as I like.
But unfortunately we're baked into
an interface in the standard where
already we had allocator
passing as the last,
not this encoded first argument,
so now we've got two ways
of passing allocators
that code has to be aware
of, which is not great.
Then we end up with generic wrapper types
like pair and tuple,
because they don't know the
element you're constructing
and might need to pass allocators
through to the elements of the pair,
even though the pair
itself doesn't allocate.
So now we have allocator support
in things like pair and tuple
because we have to be able
to pass the allocators
down through the library.
And even that's not good enough,
because unfortunately we hit things--
Through the next slide.
I'm gonna skip that one.
We hit other problem cases
like std::array and other aggregates
where because they don't have constructors
I don't have the ability
to give them an allocator
even if I want to.
The same is true for a native array,
so an array of strings.
And that problem goes down levels.
I've got a vector of array of strings.
I can pass my vector its allocator.
That can't pass down to the array,
so the array can't pass it down
to the strings in the vector.
So there's these holes that
we really want to try to fill.
And again there was a problem
with type erasing allocators
and type erasing objects
like function and any.
I've run too slow, I'm gonna run past.
I'm really running out of time.
I've not got to my talk yet,
I've got 10 minutes to go.
Let's get to my proposal.
Sorry, planned to be here 10 minutes ago.
It was heavy on the introductions
so you knew the problem
we were trying to solve,
but I got half the time
I wanted to solve it.
Again, this is my
personal vision for std2.
The way we solve a lot of
this complexity problem
is we agree on a single allocator model
so we no longer have allocator_traits
to try to solve all of this mess.
We have a single,
well-defined way to do this
and that promotes all our vocabulary
for the interoperation of APIs.
The anti of going with a
single model, of course,
is you're going to have less
customizability and flexibility,
so we pick the most
flexible model we know,
which is the pmr system.
And the key thing is we keep allocators
out of the type system which
greatly simplifies things
and gives us a vocabulary notion.
We want to keep allocator
spam out of the interface,
none of those constructors everywhere
all taking allocators.
We need to find a different way
to get the allocators into the objects.
We're going to bake in the notion that
I've got a data structure,
the whole data structure will
always use the same allocator.
That's going to be built in to this model,
it's a built-in assumption.
If a type does manage memory,
it will always do so using
this allocator system
so I know it's plugging it
in at one end of the system,
it will be used all the way through.
And we're going to introduce
the notion of allocator-aware types,
that are (mumbling) a special
way to the type system.
In the proposal I'm going to come up with
a type is explicitly
allocator-aware if we mark it
in a way that says this
type is allocator-aware.
And that's going to mean something special
that we're going to talk about quickly.
And a type is implicitly allocator-aware
if it derives from an
allocator-aware class.
Kind of like I've got a vtable.
if my base class is
polymorphic, I'm polymorphic.
But unlike virtual functions,
also if I have a data member
that's allocator-aware
I'm going to pick up this
funny allocator-aware tag.
So it's viral on members as well as bases.
Why is it important that
we have this property
for members as well as bases?
Well for generic code
I don't want to have to
conditionally derive from something
if I try implementing it
through some kind of base type,
base class type mechanism.
But fundamentally, for aggregates,
we know we can't solve this problem
if I have to introduce a facility
that means my type is
no longer in aggregate.
I can't solve native
arrays at all this way.
So the notion is we're
going to store this pmr
pointer somewhere,
somehow, and it's going to
implicitly find its
way through the system.
So an allocator-aware class will use
this pmr allocator I supply
or some variation of pmr,
whatever works for the model,
to manage the memory.
There will be a consistent
API across a system
that says if I'm given
an object I can query it
and say, &quot;What allocator do you have,&quot;
or, &quot;Do you use allocators at all?&quot;
And this allocator is
assumed never to change
through the lifetime of the object,
the non-propagation trait.
Querying matters because when I come to
things like move and swap,
knowing if I've got the
same allocator at both sides
turns out to be a big deal.
There's a few other things,
but I want to jump through.
The notion of implementing
allocator awareness.
This slide is slightly out of order
I thought I was going through them in.
Some implementation details
before I've quite described
everything I wanted to get.
We're going to stash away
an allocator pointer--
or a memory resource pointer,
kind of like a vtable.
And it's going to be consistent
all the way through our hierarchy.
As we start allocating
and creating new objects
as I pass down to my members,
my members will all have, again,
pick up that same allocator.
We're going to customize this API to say
implicitly this system knows
how to store this pointer away
but if I'm implementing
something like standard optional
I know that when the
optional is holding an object
that object has an allocator,
so I don't need to store a copy.
But when the optional object is empty,
I need to retain a copy of that pointer,
but I know that if the
object I was storing was
allocator-aware, it's at
least as big as that pointer
so I can reuse that
same region of storage.
So I'm going to want to take
control of this mechanism
and say where that pointer is stored,
and it might even be changing
where it's stored at runtime,
and where I find it,
for some of these use cases.
So that's the ability we need to have
within the facility itself.
And if allocator awareness is implicit,
well that means that something,
either my bases or my members,
already has allocator awareness built in,
so I'm not going to store an
extra copy of that pointer.
The implicit system knows.
I just keep going down the
chain until I find that pointer,
so we can at least push
all the allocator pointers
out to the leaves of the data structure.
So we don't get to lose
all the duplicate pointers
but we can implicitly
reduce some portion of them.
So the notion of implicit awareness is
if I have an allocator-aware type,
or if I have a type built
out of allocator-aware types,
it itself will always be allocator-aware.
The key issue was allocator injection,
which unfortunately I've got a very small
point size on this.
The idea is when I now construct
my allocator-aware object
I want to be able to supply its allocator
not as a parameter to the constructor,
but I'm going to have a
separate mechanism to say
as I run the constructor,
here's the allocator to use.
So for allocator-aware types,
all constructors have a hidden
additional allocator argument
that the system just knows
how to pass down through.
So at this point I have taken allocator
out of all my constructors,
they no longer appear
in the interface of the code
that implements these things.
The system knows how to
pass them down through,
so I have no code to write
to pass the allocator
in my memory (mumbling)
through the rest of the system.
So ultimately all the coding complexity
perceived in the system goes away,
because the syntax is there is no syntax,
other than those few places
where I actually have
to directly interact with
the allocator itself.
So this is kind of my mental model.
I'm not saying this is what
the facility should look like
but this is the notion of how
we might be able to use this.
So we're in C++20, we
already have concepts, great.
I've got template Movable Type pass vector
that says I inject allocators here.
So I'm explicitly stating in this case
that vector is allocator-aware.
Then we'll have an alias called
not_aware for tuple of int.
Well tuple doesn't need allocators,
int is certainly not allocator-aware,
so tuple of int is not allocator-aware.
Whereas a tuple of int and vector of int,
vector is now allocator-aware,
so I'm associated with the
tuple with a vector in it
that this type will be allocator-aware,
and now its constructor have
this extra allocator pointer
hidden behind the system.
For the implementer of tuple,
it's just going to happen by magic.
They don't have to interact with this
because this implicitly
falls out of the system.
So I now have my
LocalAllocator from_stack.
Same basically, I'm going find allocator
off some stack resources.
Vector of int, data{1, 2, 3}.
I've not given it an allocator,
it just uses the default allocator
which will typically be operator new.
We're going to have another syntax to say
when I construct this object--
I'm again using the attribute syntax.
Hopefully we can find something neater,
but it seems to work--
I can inject from_stack.
And from_stack is an allocator
so we've said we inject
at the allocator facility,
so it says, &quot;Okay, whenever
I call a constructor
&quot;I'm now passing in the
from_stack allocator.&quot;
I've set that's what it will be,
so the implicit extra
argument of these constructors
now knows to pick up
the from_stack pointer
and pass it down through its (mumbling)
through all its bases
and members all the way.
And obviously we want to be able to do
allocations with new and delete.
So when I do a new, you
can see the bottom example.
new inject from stack.
I put it on the new call,
so that performs the allocation
using that memory resource
and passes that memory
resource through the
implicit system that I was talking about
as the additional constructor argument
all the way down through the chain.
So a lot of this implicit argument passing
means the complexity is
all hidden from the users,
there really is not much
more to it syntactically
when you're implementing
these containers in this.
So your containers use
the allocators internally.
So when your container
has to do an allocation
it's going to have to go retrieve this,
perform the allocation,
but the vast majority of your types,
especially in modern C++,
we learn not to write new and delete
because these things are
all wrapped by objects
that are already managing
map memory for us.
And that gets pushed way down the system,
most of it comes out the standard library.
I'm trying to figure out
which of these to dump through quickly.
Where are my slide numbers?
I think I'm about over actually.
So, sorry. Paced myself badly today.
(applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>