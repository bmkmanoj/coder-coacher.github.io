<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2014: Nicolas Fleury &quot;C++ in Huge AAA Games&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2014: Nicolas Fleury &quot;C++ in Huge AAA Games&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CppCon 2014: Nicolas Fleury &quot;C++ in Huge AAA Games&quot;</b></h2><h5 class="post__date">2014-10-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qYN6eduU06s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone my name is Nicolas Karim a
Technical Architect at Ubisoft Montreal
it's an honor to be with you today
um Ubisoft Montreal is known for its
huge triple a game open-world games like
Assassin's Creed watchdogs Far Cry port
and games like Rainbow six printer cell
and subversion I've been at Ubisoft
Montreal for eight years now 11 years on
the game industry before was working in
other industries but still in C++ but
coming in the became insular change the
way I see C++ and I want to share part
of that experience with you today
um this is a talk where I will cover a
bunch of different things ABI will
repeat a few points from the Mike Mike
Acton talked earlier but my way the
person will discuss the situation at
Ubisoft Montreal then I will talk about
what we do to improve completion time in
iteration time and C++ then I will
discuss a few performance issues and
finally we will conclude with some
debugging tricks and tips and please
keep your questions for the end of the
talk
so first is situation Ubisoft Montreal
it is as far as I know the biggest video
game studio in the world slowly reaching
3,000 employees and working on big games
up to a thousand employees worldwide at
the end of a project with up to 400 in
Montreal and the technology mainly in
Montreal as 300 developers approximately
and its
much windows centric development
environment then if we're doing
multi-platform development
enjoy game console they come with visual
studio integration maybe a few points in
the fluid slides might be window
specific but not much and working on big
games big codebase biggest one Sassin
screed unity should be released soon I
could count over 6 million c-sharp a C++
lines of code and the code base made by
different sass and three teams over the
years that isn't the code base actually
9 million more C++ size would go that
are from insider projects they can be
technology group code that can be code
from external partners and I could count
five million c-sharp lines of code as
well if you look at Rainbow six siege a
game I am working on if I count only the
code that we are compiling and linking
together in the same executable I can
count in half million C++ lines of code
made by the rainbow 16 in 4.5 million
C++ line of code from the technology and
even the technology group code typically
we compile it on developer machine
because we want to be able to tweak that
code and debug it and I think overall we
still manage to have a good compilation
time compiling and linking all that
together it takes three minutes five
minutes depending on the target I will
discuss more about that soon the code
structure the engine the game itself I
will describe it as an inverted pyramid
we have multiple layers and they
introduce new external libraries at the
bottom we have the core of the engine so
mathematics object model it will simple
stuff that is used across all the errors
then we have a graphic layer which was
possible rendering where performance is
extremely important and then over that
what why we call the engine engine
services or physics sound animation
services are that are used then by the
last a year the game player which
contains code that is specific the
gameplay beginning and we have singer
programmers working in all of these
layers but junior programmers they tend
to often end up
in the gameplay here it's really
important for people working on lower
layers to provide an API that is as less
error-prone as possible the irony here
and not only not every problem when it
comes to stability but also when it
comes to performance and sometimes you
have to choose between the two minimum I
choose performance container might take
the most performant approach by default
but if you want something else you need
to specify it explicitly and we don't
consider C++ to be the most productive
language in the world we use it because
we want performance when we don't care
about it
we tend to and said you'd see sharp sole
editor that is used to make the content
that the game is made in c-sharp and it
is embedding windows that are from the
engine in C++ and it is communicating to
with the other process I chew with two
tcp/ip and of course when we need
performance sometimes we need great code
I'm a shark to the engine side in C++
that stuff we don't use from C++ we
don't use on time type information we
have an own reflection system on object
model we even ship the game with our
reflection system so we have data driven
systems like epic systems that can edit
properties of objects too complex for
bridge paths and you want to control the
exact memory overhead of this system we
don't want to ship with the game a game
with class names or property names that
would be a waste of memory and that will
help actors uselessly we don't use
exception and link in the pass it add a
runtime class on some platforms as I
have 10%
that was clearly a no-go but even today
it's still as a compile time class and
anyway there's no return on investment
for us to make our code exception safe
we have utilities to reduce the call
stacks where there's stuff to fix
and we even use a static analysis tool
like every beauty that can tell you you
know this function 8 times of 10 you're
looking at the return code but even here
you're not be making a mistake
we don't use the STL containers in the
engine but we do use the SL Egger rims
with other containers
affecting Estela interface we don't use
a CD vector because our hourglass has
more features especially debug features
and for the other containers there are
alternatives with better performance
often these alternatives they will come
with some restrictions for example a map
container could say only supporting
achaeans values that are value types
that oughta have constructors in our
case not only we're fine with these
restrictions but we see em as features I
don't want my programmers to make a map
with ease of values without with
constructors and we never ship a game
with a map with a string as a key this
is just not the way we work and the last
one might sound controversial but we
have no boost included in the engine
sometimes they they appear in the past
word remove camp or compilation time
reasons we have much better completion
now than back then so maybe it will come
back in the future I don't know
iteration time so as I said I think we
have a good completion time considering
the huge code size we have the first
tool I want to mention is a tool called
fast bill
fast bill is not bringing that in
knitting that much new to the table to
us but it is containing a bunch of
utilities and tools that were using
before as solutions for completion time
what is fast bill for us is replacing
game is built for C++ so it's really
make file to like Boojum or G make it's
open source and it's made by Fanta with
working at Ubisoft Montreal today so
compared to a mess Belfast it means
smarter a dll dependencies since we can
compile it provides a dll without
waiting for the dependencies of the DLL
itself better CPUs it according to your
numbers and it has support for
distribution and caching so what we do
is
we is like incredible basically so what
we do is that we run fast build workers
on every developer machine and every
developer is contributing the
compilation of others with as much as
half of his course if it's machine is
idle and whence the teresting is that
before I said you know three to five
minutes depending on targets five
minutes actually without distribution on
Rainbow six an assassin's creed building
without distribution is a bit slower but
the serious thing is that with
distribution which seems to compile
pretty much every project at Ubisoft
Montreal in three minutes there's a kind
of a critical path that we have that is
order to make its order to go slower
look pastor than that and fast bill is
also support for caching so the way fast
build is distributing compilation is
like by pre-processing locally the file
to remove dependencies of all the
includes and then send it to our machine
and so it can make an md5 at that
pre-process file and look on a cache in
the network
the result was done already by somebody
else so what we do is that build
machines on the bill farm they will
update the cache because there's
overhead so we only do it on dual
machines and programmers they typically
don't sink the last the add of the code
always the instead go through a tool
that will think the last changes that
was successfully compiled in machines so
it means that the cache is up-to-date so
the first time a programmer will will
sync the compilation will be extremely
fast because typically primers their
work on very high layers so bank bunch
of a big part of the code is well they
will get a recompile for most free and
lastly fast bill supports unity builds
built in we've been using unity builds
and QB soft Montreal for over ten years
we call them the blobs that Ubisoft much
wrong so if you don't know the concept
as the concept of having let special CPP
file it will basically include a big
number of UCP files and come compile
that unity CPP file instead and overall
it will compile faster is what us will
link faster my 10 line a few more
functions as well as
consequences it means your high scope do
you need to have unique name I need if
you make defines and simplifies you must
not forget to under them because I can
affect another CP pile and the same
unity but most importantly means that
you need to maintain still a compilation
without unity builds because you want
every CPP file to add all the included
needs and people also complain when
using unity builds that when I modify is
CP file then I need to recompile the
entire unity in ok sit as when built
being true but only the first time we
modify a file so what we had before is a
special unit here work unity and will
edit a file it will remove us from that
unity and put it in the work unity so
that way you will iterate over your work
indeed I have never done it but you
could even do that in parallel as you
first edit the file in vyasa you
directly launch the completion of the
unity - change um but since fast build
is creating the unity builds on the fly
as it is compiling it doesn't even need
that step can just remove from unity
builds ICP file you are working your
onion just you just iterate over them
directly there's other stuff we do as
well use pre compile errors like a lot
of people some people they stink they
are competing with unity builds before
my numbers are completing each other
there where we don't use pre compilers
is in some distribution patterns
depending on platform sometimes they are
not worth it on other machines to use
but luckily it for us it's like a big
big game oops sorry there's one
Microsoft compiler option that I think
is really resting for debug builds the
option is called ob1 not to confuse with
the Star Wars character what the obi-wan
is doing is minimally lining so it will
inline simple functions from 805 so it
mine signs we'd at first we have a debug
target where some functions are in line
but I've been working with that for over
five years now and I don't want to go
back um the reason is that my debug
target is not much faster and since the
video game it's it's
it's appreciable and also it will link
faster because there's a big bunch of
symbol that don't need to be object five
anymore and this one is really important
when people have bad completion time is
often because of template's so we try to
make template classes derived from non
template based classes the idea is that
template code should end up in line by
your compiler otherwise maybe you should
write your template code differently if
I take an example an assassin shoot one
at first was not done so we add an RA
class the entire code was there then in
every translation unit we compile aways
the exact same re types even if we use
unity bills we still have maybe 300 of
them in our code base but we did instead
is that we change the class to move
inside a non template based class as
much code as possible and some cases 7
is CPP 5 to make sure it's not in line
yeah and then in the template code we
keep simple function that will end up in
line we also use a lot of generate code
we have our own interface description
language for our object model
so basically next - most header file and
CPP file in the engine we have another
file that is the interface description
file and there we declare classes
structures denims and there and some of
their properties and function and this
valley is responsible of code regions
inside the corresponding header file and
CPP files so for example in our header
file you will have a class and vary as
the bracket is opening you have a code
region and you can still the primary and
seller right is all members and all
functions are outside the cold region
but current record region itself is
completely and all by a tool that we run
very quickly at the beginning of every
compilation and this tool is allowing us
to the interesting we can avoid a lot of
meta programming in the CP files example
the skills a shell function they know
exactly all the properties were declared
inside the interface description file a
narrow file we can sort members to
minimize padding between
save memory we can generate IDs as CRC's
of a name all of that directly inside
the tool so basically we have created
another file with another timestamp that
will affect the generated code so that
way we can then speed up the c++
compilation alone and we also have our
own programming language as it actually
it looks very much like c++ it's
actually generating c++ under it and the
reason we did that language because we
wanted to have edit and continue working
on 64-bit platforms or gameplay
programmers we event went further by
making sure that editing cut you is
working event in multiplayer session
propagating code to all to all players
and we even wanted to be able to remove
and add virtual functions so when it
comes to the virtual functions
implementation is not using the c++
virtual function we have our own table
so that we can patch function pointers
at runtime more easily and since
generating c++ we benefit from aligning
all the performance from c++ and we used
no preprocessor tricks like dash line
all of that to specify the original file
line so we support the C++ debugger
completely naturally since for the
debugger it's like it's it was passing
and we have developed some tools for the
years to help up speed up improve our
code one tool to mention is an objection
eliezer so the idea is to give you the
size of a symbol but by making the total
of its size across all the translation
units so you might see functions that
are an arrow files but should not be
there because they are just too big but
more likely what you will see are
templates I the area class I've
mentioned earlier about on Assassin's
Creed um before doing that change or
splitting the class into pieces an
assassin's creed one that place that
class before that change was taking
almost 80% of object files of all the
imagine you have a million multiple
millions lines of code engine and you
have a single template class taking
almost 80% of object file still today
with the changes who did in debug we
have some functions that don't end up in
line because of debug check that we are
them and still the error class is taking
15% of objects files and debug target
we also made our tool to remove useless
include and Ave and add forward
declarations I want about talk about it
much because realized recently that
Google's include what you use is
actually a better tool these Ness
configuration which is the same thing so
just use that thing instead when it
comes to performance it's well known
that we care about performance in the
game industry some performance very
important for us the last console
generation lasted for eight years you
can imagine that we need to be able to
always make more with the same there's a
90/10 principle arguing pretty much 90%
of the time wasted and 10% of the code
so people working on that 10% pretty
much I have a different mindset people
working on the rest of the code um that
mindset is pretty much similar to what
we heard from Mike Acton earlier today
and the way an engine is made is it has
a frame rate so it's a big part of the
code is being stressed confusing so it's
not like a desktop application that is
event-driven and it's not running that
much code um so yeah typically a game
will run at 30 frames per second so
30fps gain of games like Rainbow six
siege a seven 60 FPS what happens is
that when shipping again may be months
before shipping a game especially
single-player games your game might be
during at 20 FPS or 22 FPS but you want
to ship a game at 30 FPS and you don't
want to cut stuff that people have put a
lot of effort in so what
that a lot of people a lot of talented
programmers they are dedicated to just
optimize the last months of the game
have been extremely impressed in the
last eight years seeing people
optimizing code that was technically
have been already optimized by multiple
other talented people and they succeed
when way of doing that is by
understanding the art where so I want to
give a quick example very simple I have
a structure
it's basically an area of integers which
are a bunch of different values they
make a make a you Jerry and the exact
size it matter it's not least it's one
Meg of objects from that type and I have
two different piece of code doing exact
same thing are just calculating the
total of all the value don't reach a
spec is to join the same speed which one
you expect one on the same speed I've
seen a lot of stacks this week that
assume that you know the answer this um
so if it's not your case
let me explain it preferment this on my
pc the second example is running eight
times faster so why is that programmers
they understand there's a huge
difference between the rhyme the our
drive not event access to have them
anyway the same way i drive is bigger
it's slower but the cpu and it's
registers is not accessing your eight
gigs or 3d quick gigs 32 gigs of ram
directly it is going to on the processor
to multiple layers of cache the exact
number of cache is mean it depends on
the hardware but there are common stuff
the l1 cache will be specific to the
core while the last level cache and that
case the l3 cache will be shared across
all the court so suppose i imagine
here's some fictional hardware made of
six cores i the l1 cache is specific the
core and then i decide that the l2 cache
will be shared across pairs of cord and
finally I'll have the l3 that is shared
across all the cores and then the memory
so when a core is accessing memory
address it first look if under old it
will look if the if it's in the
l1 cache the caches are caching cache
lines so for example the l1 cache could
be 64 K big and with cache lines of 64
bytes so it means I had thousands pieces
from the memory 64 bytes big align at 64
bytes these pieces can be from all
underpriced
in the main memory so it will load with
the address the memory to accessing if
it's part of the cache if not you have
l1 cache miss l2 check it's not there L
2 cache miss if not then you have l3
cache miss and the main memory would be
access for the l3 cache miss in that
case the worst case because then you
need create a price the speed of the
main memory since if you have the layers
the same rules are playing they are
further you get from the chord the
bigger they are but the slower they are
there's a lot of things here to talk
about the resting stuff how much all of
this can be done in parallel other
processor is making sure all the l1 are
current but I won't I think that's a C++
forum what you need to understand is
that less you access a memory address
the more likely you are to less like it
more like you are to pay I cuss if you
work with huge data you don't work
properly you are so likely to you have a
big performance it also typically the
way the l1 cache would be endl if it
requires writing to a with l1 cache it
will be it will have exclusivity to the
cache line that it is writing to which
means that if 2 core are writing to the
exact same cache line if they are not
writing the same bytes you will have a
performance it one example earlier this
week and an episode or stock where you
have a remote treated container and
aligning the node on the cache line size
was improving performance greatly
because that way you could not have to
note in the same cache line if you come
back to the example that was running
faster it is accessing the arena
completely sequential way this is the
best because event modern processors
today they can detect such patterns and
even prefetch the cash in advance to me
- mrs. but if we switch these two for
loops then we're accessing memory in a
completely different way or first
getting the first element of every
object and then we start back in yet the
second lemon are very object so if I'm
on this fictional I'll where I have
cache line of 64 bytes the size of
feature is 4 divides 64 by 4 it makes 16
I could expect this to run 16 times
lower another example very simple I have
a structure with an integer member and I
have a function taking a bunch of values
it will just update the member with the
total the value I have two simple
implementations the first one is very
intuitive and of situating the value is
incrementing the member and the second
one is using a variable on the stack you
expect these two to run on the same
speed Pamuk the tests on my pc with the
Microsoft compiler and ode to
optimizations and the second one is
running 12 times faster there are two
things happening here this is dependent
on odd where and this is dependent on
the compiler itself as well so what
happens is that in the first example the
or compare is very likely that decided
what you doing inside the follow is read
from memory make a calculation and then
write back to memory you quickly in the
next iteration you're reading back in
the same memory so there's a read after
write opening on the processor
processors will make sure that we'd have
the right size as fast as possible but
the point is that you cannot expect this
kind of operation to be as fast as just
working with the registers that are
exactly that are on the same core and
this is what the second example is doing
as far as I know compilers could decide
to transform the first example into the
second example but obviously they are
not and F says it with clang as well
when it comes to Singleton's i don't
know if you have Singleton's inside your
different code bases continue that with
a huge code base yeah at Ubisoft we have
a lot of singleton a lot of classes with
name ending with manager because we lack
imagination especially when it comes to
singleton names we don't want to create
them on the fly I want them to construct
them and deterministic order want to
destroy them as well dynastic after like
release so what we do is that we use a
property of C++ very simple that
constructors of members are called in
the under declaration and destructors in
the reverse order so every library has a
special structure that we call singleton
story where we declare this our
singleton in their order of dependency
so Auto implement such a singleton one
way of doing it is just have a static
pointer and set it in the constructor
but what happens if this singleton is
access thousands of times every frame
and very different context what happens
is that you get something that is
actually less performant than just using
global object and I couldn't measure the
difference on xbox 360 the reason is
that the global object you will access
directly the memory address of the
object itself while here the global
object is actually the static pointer we
have an indirection the problem is not
you know CPU instructions to just
dereference the problem is the cache
miss that could occur so one solution is
just to use global objects instead and
call maybe initialize and shutdown
functions inside this an open sorry
another solution would propose we that
is available in origin is our class
class called global singleton
so what is doing is that it has a static
buffer that as the same size and the
same alignment as the type you are
passing and the scope nested class under
the hood is constructor will mate please
meet you inside the buffer and the
destructor will do the opposite
call the destructor so then getting the
instance is just casting the address of
the buffer as a pointer to distinguish
in pipe so in the end you
yet Singleton that has the same
performance as a global object but with
control construction and destruction I
have talked about the lack cache miss
but there's coast cache miss as well
maybe when you started programming a
long time ago you were writing you have
a big bunch of shapes of different types
you were writing an ugly switch case
like on the right then you realize oh no
this is not the way it should work I
should make a shape abstract base class
and have an abstract draw function in it
and then if you shape type will
implement the draw function then you
iterate over a new array of shapes and
then you call the draw function on every
shape what's happening under the hood
the object pointer will be different
probably to get the first eight bytes of
your object to get the vtable pointer
you might add a cache miss but maybe
such a big nut should big deal because
probably the draw function will access
members in the same cache line then it
needs to access the table itself if you
have a lot of shape types good change
you have a data cache miss here then
you're only interested in a single
function inside that table you get the
function pointer and then you go to the
where the code it is again our to
predict that you were needing that good
chance you have a code cache miss as
well because these two are multiplied
and under F types you have a quick fix
to at least sort your shapes by type
that way you will get these cache misses
only as you meet a new type but what I'm
saying is not to not use virtual
functions just as people working in that
10 percent of code they are will avoid
the pattern at left it will typically
instead aim for that are driven
programming the idea is that is to
remove abstraction don't iterate over
pointers of shape their iterate over
shapes and put the shapes of the same
type together so iterate over the single
shape and then you are so avoid code
cache miss because you're running code
exactly for that type and since you
iterate over shape themself you trigger
they rake over them in the order they
are in memory which will make the
best performance possible the processor
can help you to prefetch more easily and
to be honest when we on video game
consoles if we need to prefetch manually
to help the compiler we do it don't do
that on PC that can slow down things but
but on a fixed side wall at consoles we
event prefetch remember yourself I don't
want to come back to an example I've
shown earlier with the RA class when we
divide the class inside in two classes
so what we did is take template code
that was not being end up in line and
put it instead in a base class and make
that call a bit more generic so
basically with that code without
slightly more instructions you know we
will pass probably a size of type in the
another mind of type so does it run
slower I think it might even run faster
and I want to at least talk about that
in the case on the left if there's a
constructor or something and that type
and I'm instantiating my re class with
2,000 types
means I have 2,000 function in memory if
there's no constructor or something
that's so type specific probably yeah
maybe with the different size of pipes
or ABI I will have maybe 30 functions
and memory but you can understand that
you increase the chances of code cache
miss with the example on the left my
point is just that templates especially
templates that don't end up in line by
the compiler they might give you the
illusion that you get something at
runtime but you might not another thing
too we do too
performance is trying to avoid the heap
the heap is a heavy I always found it
funny when I see people writing
containers trying to make sure
everything is in line while the way to
use allocations is taking more time than
the rest of the container combined and
the reap is global so you can allocate
in the thread the allocation another
shred so when you pay a price for that
and in video games where we allocate so
much every frame we have can have issues
of fragmentation permutation is very
simple you still have space inside your
locator but will still go out of memory
because you're asking a buffer that is
bigger than any of these small spaces
something that was done on Assassin's
Creed 1 and we're still doing today then
when we have an area on the stack like
this no case is like our SD vector we
look at what is the typical size what is
what size is big enough for 95% of the
time then we believe place that by a
class with all in place area so it's not
like your STD are a lot more like Annie
bread between STD IRA STD vector so it
will use the static buffer if the size
is in that case 8 or smaller so it will
use the stack so you also like save a
lot of CPU everybody's happy and out and
if it's bigger than it will use they eat
so unprincipled to do the same changes
as on Assassin's Creed and they were
giving good results but when it came to
memory fragmentation I was still once in
a while in that case allocating and was
removing their way to do that more
thematic and then we found patterns on
different platforms to know if your
pointers on the stack on xbox 360 and
playstation treat was as simple as a
mask
it was undocumented but for us that was
good enough so what we did is that we
change the area locator to look if the
disk pointers on the stack and if it is
we use what we call a frame a locator
and so it's just a different alligator
that is asserted the empty at the end of
every frame since it's empty at the end
of every frame there's absolutely no
fermentation with every
going to that alligator it's really cool
because this is even working for arrays
that are members inside objects on the
stack in doing that change at that time
on prints our first year it remove all
the remaining issues and we had a
fragmentation that played a game for 40
hours and no such problem when it comes
to debugging we have our own challenges
as well we have a huge mullet codebase
you can imagine same codebase with over
sometimes 100 programmers working at the
same time in it as a consequence we have
bugs that only reproducible in optimized
targets since you know different tax our
tasks are running at so much different
speeds want to avoid recompiling for
debug options want to debug issues are
do as they are occurring and the debug
targets might be fast to be usable
because a game running at 2 FPS it's no
more game this is unplayable I'm a
consequence for the debug targets
there's a bunch of stuff we disable this
is all Microsoft stuff on this slide
debug iterators from STL I said we're
not using STL but we're using third
parties using them and using it is using
our tools as well the visual studio
debugger heap we disabled we disable
default windows fault around heap as
well to the registry the last one is
something Windows is the way when the
application is crashing too often we'll
just assign a different EEP to it but
prefer to fix the crashes then have
something run ten times slower and no
programmers they need to be able to
debug release code we have a one day
course that is very popular in Ubisoft
Montreal where programmers they
typically learn to get back the
disappoint through from the code the
assembly and the registers here we can
see a screenshot of a visual to your
extension we have that is adding
information so that people understand
code the assembly better as well
so one give the course but I can at
least share one tip with resistors so
on your PC you end up with a call stack
like this that completely empty if you
don't know what you do you're pretty
much screwed because you have no no idea
what just happened what happened is that
your striction pointer register is now
corrupted and what you can do is just
look at the different registers and
typically there's one with a value good
enough to debug and just try them very
quickly and as you press ENTER you will
see your call stack resurrect from the
dead when it comes to memory allocations
I said we try to avoid them at your
relative that we're doing a lot of them
so much that we don't debug them and
Julie so what we do instead is that we
use something we call memory tagging so
every memory allocation we do we pass a
string that will be used to tag that
allocation we even pass a parent pointer
but the parent pointer is not used for
ownership Allah this is bathing like any
C++ knew the parent pointer is used to
make a path with the tags so like Pat's
on a file system so the reason we work
that way is that it has a very low
memory footprint it's not an issue and
on all gen consoles because we just need
to keep an adam account for every path
and that's it so for example on Prince
of Persia I added a tester playing the
game for over 20 hours on multiple days
and and then you will come back at the
central hub and you will make regularly
a dump
a memory dump but then I was really easy
for me I will just make a difference
between these dump and see which memory
tags were leaking then I will break in
the debugger on these tags and just fix
the code accordingly the way will break
inside the code is by using something we
call the breaks it's very simple it's
just that a bunch of our singleton and
managers they have a global object which
just is just a structure with a bunch of
values that we can set to trigger
breakpoints and debugger so they are
like conditional breakpoints from the
debugger but with two differences
to adventages they are much faster
conscionable a point from the debugger
if you're running through the code where
you have put it that will be extremely
slow but here they are compiled inside
the code and you can set them you can
post a label game the debugger and set
them at write time where you can set
them at compile time and also they are
permanent so if a new developer arrives
on the project we have a master class
with a memory for all of these breaks
classes and so can browse to all the
breaks available inside the engine on
this screenshot we can even see that we
have another master class you can even
actually browse through all the
singletons for example all the singleton
story when it comes to memory corruption
as I said before you can have as much as
100 programmers working the same
codebase a lot of people using the same
eat so you can understand that
another programmer can make your code
look bad um so what happened is that
when we have a memory address where we
suspect there's been a memory corruption
we post the bugger and we have a
function called debug memory we can pass
that address but what it will do is tell
you which allocation was done in the
past close to that address because often
what happens that someone is writing to
an object that is actually now deleted
so this is finding things after the fact
but often this is good enough well I
like to do in the future is not even run
code inside the process but instead add
visuals to do add-in that is getting a
memory and then you cast it in c-sharp
and make the exact same thing so we have
another approach as well we have an
alligator that when you allocate it will
allocate a complete page or allocation
and put the allocation at the end of the
page the next page is allocated as
read-only so that way if you write pass
an allocation it will break that within
the lab error it's a class you are
trying to write inside the read-only
page and when we do
the subject will keep the page as we
only for some time
so again someone is writing to an object
that it's been deleted it would break in
the debugger directly will be a crash so
this is cool because now we are finding
the memory corruption exactly as they
are with the guilty code file oh sorry
yeah yeah of course we have asserts
inside the area class but suppose I'm
just have a pointer to an object I could
cast it another wrong type and then
access pass the address oh yeah okay
that will work as well yeah yeah so and
but the problem with this is actually
that's taking a lot of CPU time to
allocate all these pages so we need to
use it with specific specific allocation
and maybe specific size depending on
what the problem while I was waiting for
questions at the end but anyway that's
on the next slide inertia
not the entire game not the entire game
no but I think I've done it on an adjust
dance project but yeah someone tried to
do it on Assassin's Creed there's no no
way but if it all was on here a memory
problem we would not care we say okay we
have a machine with whatever Ram is
necessary but now it's more CPU problem
yeah
so that's it before the conditioned I
talk I want to mention the first
reference link it's a talk from game
developer conference in 2003 from mr.
Rickson from Sony if you interested in
memorization its really resting so
that's it yes time for some more
questions
hi can you tell us briefly a little bit
about what kind of concurrency
primitives you guys use is it kind of a
set number of threads thread pools
actually the next talk my friend just
pressing is about bit of that I want to
spoil him I don't yeah now he's really
answering that question in one-hour talk
yeah so you will have your answer in
like half an hour
Thanks do you use any other debugger is
like windbg
for some of those problematic crash
cases or do you just use I will use the
trace the Sony offer and then it's just
you and that's it
I wanted to ask if you guys have used if
use any other memory debugging tools
such as like address sanitizer or
anything like this or if this is
completely infeasible with video games
uh by me I have not I don't know people
have try yeah Jen do you have a lot of
custom solutions all interesting that's
familiar stuff so to see our curiosity
feature selection you know we're talking
about game beside games this size teens
this size games different types of games
do you have anything systems for feature
selection between them anything that's
actually managed or is it pretty
monolithic sorry can you repeat the
question do you have any do you have any
systems Ubisoft having system for
feature selection between the games like
are you cutting out parts oh yeah okay
yeah yeah so so let me show you man yeah
sorry so we have a few have
possibilities of choosing features for a
game at Ubisoft Montreal is extremely
organic every project is branching from
whatever project so it's pure chaos ah
but it works and that's actually I think
we are able to be 2000 because we are
extruding bottom-up we work the same way
as you know 300 studio in the Sun some
way but yeah so the IV project is
branching and then it will just remove
stuff like crazy that they don't want
and the sequel needs again you have an
OC end of something both need to put it
back so so yeah this so they were
definitely not for not working in a
marginal way to remove stuff and add
stuff it's maybe we maybe it's possible
but the real life situation is that
we're not doing that at all it's pure
branching and very organic natural
section good ideas that will finally
come from project what you've seen over
the years that having people moving from
project to other projects as newly gets
more
other and bigger on this ship it's
really helping to propagate good IDs
people with a was using that on watchdog
and so yeah that's that will say that
it's very organic that means how much of
a performance boost you get out of
having a unity build versus relying on
link time code generation so yeah how
much performance we get at runtime from
unity build I don't know maybe they
assess the screen one day will have a
good idea at that time they were
maintaining both deals on Rainbow six we
we only compiled without unity bills
just to make sure they having food we
don't even link that that target anymore
so I really don't know but I know it's
in lying a few more functions since you
often you have function that are good
thing candidates with foreign lining in
CTD files but yeah edited more questions
thanks for it up sorry your codebase
predates se plus ax plus 11 standard can
you talk about what any sequel has
eleven features that you've added since
the standard came out yeah by add some
slides
yeah box so the question is about c plus
positive in usage I had some slides at
first regarding some features we use
from two plus plus eleven I removed them
because they were not fitting in any of
the three sections really far C++ is
pretty much what the different compilers
are offering with the new generation of
consoles it's much better than in the
past since we Sony made the great move
up going with clang and then we have the
Microsoft compiler so the yeah so what
both are supporting is something we can
support so then I will look at it and
then I will sign an email to my team
saying okay we can use this on this and
this in these conditions so for example
I made we're not using the debugger ders
from Microsoft by I made my own actually
in our container and so what I did is I
made a container integrator that is only
movable so that way I could use the last
alive one and use it
hope to make made the can tailor read
only while the iterator is existing by
just incrementing an atomic count inside
the container um so that's an example I
have between my mind but yeah otherwise
yeah I cannot think of other features
but yeah we're using a few features C++
11 cool thank you for coming
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>