<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Michael Park “MPark.Patterns: Pattern Matching in C++” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Michael Park “MPark.Patterns: Pattern Matching in C++” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Michael Park “MPark.Patterns: Pattern Matching in C++”</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HaZ1UQXnuC8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Alright thank you everyone for coming.
This Mpark.Patterns.
Pattern matching in C++.
This is gonna be a talk about a library
that I wrote in C++ 17
to try to support pattern matching.
We'll go through a lot of
the intricacies of it today.
So my name is Michael Park,
perhaps better known as
M Park in the community.
I'm currently a software engineer
at a company called Mesosphere,
where I'm building out the visions
to see what it would look like
to have an operating
system in the data center.
I'm also a member of the
C++ Standards Committee,
and I'm also the author of the
Standard Variant Implementation for C++.
So to set the stage for the talk,
I'm going to cover a brief history
of how pattern matching has
gone through history over time
in different programing languages.
So pattern matching started back in 1960s,
originally with SNOBOL.
And it provided a language level support
for pattern matching on strings.
Now SNOBOL was a text based,
text and symbol based
programming language,
and so this made a lot of sense for them.
In 1987,
Pearl came along and evolved that idea,
and they basically introduced
by far the most popular
and well known,
well understood form of
pattern matching today.
And we all know that technique
as regular expression.
So ML took that idea of
pattern matching on strings
and generalized it to
more complex stenotypes
like tuples and variants.
In the 1990s,
Haskell and OCaml we're introduced
with pattern matching support.
I lost my cursor here.
Ah there we go.
Sorry.
So yeah Haskell and OCaml were introduced
with pattern matching support,
and in fact it's actually at the very core
of most functioning programming languages.
And more recently,
many mainstream languages
have also adopted the feature.
The most popular ones from my perspective
are Scala, Rust, and Swift.
But other languages like C#, Java,
have also picked up
pattern matching as well.
And of C++,
there's been two efforts that I know of
towards introducing pattern matching.
The first one is a paper called
Open Pattern Matching for C++
by Yuriy, Gabby, and Bjarne,
which was published in 2013.
And it shipped with a library
solution called Mach 7,
which involved a lot of macros.
In 2015, two years ago at this conference,
John Bandela introduced Simple Match,
which was a C++ 14 library
solution with no macros.
And that's about it.
So 35 years into C++, almost 35 years,
we still don't have the full extent
of what pattern matching
gives us at our disposal.
So I have a question,
which is what is the most
common answer to this question.
Why doesn't C++ have feature X?
The most common answer.
Michael?
- [Michael] No one's proposed it.
- No one proposed it.
Okay so we did a standards proposal.
Some kind of proposal to
actually add it to the language.
So in 2016,
David Sankel wrote a standards proposal
to introduce pattern matching,
and his paper included a language level
variant specification as well.
The next revision will
tackle pattern matching
separate from language level variants,
for which I'll be a coauthor.
So look forward to that.
Announcements will be made later on.
I don't know what the
plan is for that yet.
So what's the purpose of this talk?
From my experience in
the community so far,
I found that a lot of
people actually aren't
exposed to functional
style pattern matching.
And so I want to introduce
and familiarize the idea
within the community
so that we can actually
have a conversation
where we actually know
what we're talking about
when we talk to each other.
Secondly I'm seeking to find out
some answers to some questions.
First one is why now?
Why do we care now?
What's changed?
It's been 30 something years.
Why do we need pattern matching now?
And the other thing is,
can a moderns library
solution be good enough?
We've been adding more
and more language teachers
and libraries,
we're getting more power to be able
to write more powerful libraries.
Can we actually have pattern
matching in library form
available?
And we have a bunch of facilities
that are considered good enough right?
Tuple, variant, pair.
Not good enough to everyone,
but by the committee, by the community.
There's not been enough push back
against having library solutions,
and it seems to be working okay for us.
So on the other hand,
there are also libraries
such as Boost Lambda,
which warranted a language level feature
because it just wasn't gonna fly.
And
lastly if a modern library solution,
if we deem a library solution
to be not sufficient,
then at least we can gain experience
from having built one
to guide the language (mumbles).
At least for me so that I
can guide the language side.
So this is the overview of the talk.
We're going to have one slid
on algebraic data types.
I'm gonna try to explain
what pattern matching is
at a more fundamental level
and what it's trying to achieve.
We're gonna see various
forms of pattern matching.
Very restricted forms of pattern matching
that you see and use in C++ today.
Then I'll introduce my library.
We'll go through some examples,
and I'll show some interesting features
that I can't leave out because
they're so interesting,
to me.
But I think you'll find
them interesting as well.
So algebraic data types.
It comes in two forms.
The first one is a product type.
So when we say a product type,
we're talking about a data type
that can compose,
that can be composed
of multiple data types.
And they hold an instance of both,
or however many types you throw at it.
So examples are like a tuple
where you can say tuple of x y,
and so I store an instance of x and y.
And the name product comes from the fact
that we're counting
the number of possible
states of these types.
And so for a product type,
the number of possible states
for a tuple x y can be in
is the number of states that x can be in
times the number of
states that y can be in.
On the other axis,
we have sum types where we can have one of
the types that we gave it.
So an example if this is a variant x y.
It's going to store an x or y,
and it's not gonna store both.
Okay I will start with a claim.
My claim here is that pattern matching
is the best tool for decomposing
algebraic data types.
And we'll see examples,
and back it up.
So what is pattern matching?
This is a quote from Haskell Wikibooks.
It says,
&quot;In pattern matching.
&quot;we attempt to match values
against patterns and,
&quot;if so desired,
&quot;bind variables to successful matches.&quot;
Heres a trivial example from Rust,
written in Rust.
We declare a point class
with fields x and y.
We declare a variable of type point
with value zero, seven and zero.
We're gonna match on the P,
and we're gonna provide the cases
that we want to match in order.
So Rust is actually going to check
these cases in order,
and in this case it's going
to fall into the second case
because the y is zero,
and we're gonna print x-axis as seven.
Pretty simple?
Everyone follow.
Good I see now it's.
So a traditional way in which
we would write this in C++
might look something like this right.
We could say,
we could manually go and
check the individual fields,
and do the correct thing.
So what's the difference?
Well pattern matching is
a declarative approach
in lieu of manually testing for a value
with sequence of conditionals
and extracting the desired components.
Alright so the color coding here,
I'm trying to emphasis that the patterns
that were pink before is a
sequence of conditionals now,
and it's a declarative way
of describing your conditionals.
So we have a more elaborate example here
which is to evaluate expressions.
So this is an expression tree.
We have the expert based variant here,
which is going to store one of int,
negative, add, or multiple.
The box is basically a pointer
because it's going to be a recursive type,
and we would have to
do this in C++ as well
as we will see.
So this is one way to
write our eval function.
We can take an instance of expr,
match on the expr, and provide our cases.
We can say if it's an int,
bind the value inside the int.
To value that I'm going to
return that for negative,
recursively call eval
and negatively evade it.
Add, multiply.
Right evaluate both sides,
do the right thing.
Pretty simple.
And you can see that on this axis,
vertically we are matching sum types.
The alternatives of the sum type.
And horizontally,
we're decomposing a product type right?
So we have a bunch of values
and we're going to compose
that into fields that,
and give it names that
we want to refer it to
to use it in the right hand side.
And note that patterns are composable.
So here we have one layer of pattern,
which is the sum type pattern,
and then within the sum type pattern,
we're going to decompose the
product type into the remains.
So that's two layers of nesting.
So this is the analogous example in C++.
I'm gonna use a shared a pointer here.
You might have noticed that.
That's just for me to cheat later on.
You'll see how I'm going to cheat later.
I'm not gonna hide the fact
that I'm cheating here.
Okay so this is one way
in which you might
write you eval function.
You check for every case that you have
with the dynamic cast,
and you rely on the implicit conversion.
And the fact that dynamic
cast is going to return
(mumbles) pointer
if it's not actually a
thing that you asked it for.
And the challenge here for the programmer
is that the if statement
is a very flexible
(mumbles) cost rate right?
The operation that we're trying to perform
is actually quite structured.
And we're trying to use an
overly general language feature
to solve this particular problem
and try to get it,
and relying on the
programmer to get it right.
And often times you don't get it right.
And it's often source of bugs.
And it's also very unlikely
for there to be a warning
if and when there's a missing case.
Mainly because the hard key is open,
but also because the cases are spread out
as different predicates
in an if statement.
It's really hard for compilers
to be able to reason across
and actually tell you that
you're doing something wrong
when you're using something this general.
Alright you might say,
&quot;Aw come on M Park.
&quot;No one's gonna write
this code and practice.
&quot;Who would actually write this?
&quot;I would use visitors of something.&quot;
Alright so here,
I'm just illustrating that
we're testing manually
for with a sequence of conditionals
what we want,
and then this is our imperative approach.
Okay so yeah.
No one would write this.
This is code that I pulled out of LLVM.
(audience laughs)
Didn't cast.
It's faster,
but it's essentially (mumbles) cast.
If it's not storing the
thing that you ask for,
it's going to give you a bell pointer.
And this is all over LLVM.
Not to say that that's bad perse,
LLVM is a high quality C++ project,
but this is something that's
pretty difficult to maintain
in my opinion.
You might say that visitor is better.
Well that's a lot of code
compared to what we had before.
We have this weird returning values idiom.
Yeah I don't want,
I don't want to do that.
Okay so a little insight from Swift.
While I was doing research for this talk,
I found this quote that
I thought was insightful.
And it said,
this was from the document which was used
during the design face of introducing
pattern matching to Swift.
And it said,
&quot;Pattern matching was
probably a forgone conclusion,
&quot;but I wanted to spell out that having
&quot;ADTs in the language,&quot;
algebraic data types,
&quot;in the language is what
really forces our hand
&quot;because alternatives are so bad.&quot;
And we just took a look
at the two alternatives.
We just took a look at two alternatives
that were so bad.
And I think that this
answers my first question
which was why now and why do we care.
What changed?
Right?
Algebraic data,
the support for algebraic
data types in C++
is pretty new.
We had tuple introduced in C++ 11,
and variant only came in C++ 17.
Before that we had struts,
which you couldn't decompose
in any reasonable way
aside from actually naming the fields.
And we had cost hierarchies,
which we didn't look at as a sum type.
We kind of came from the
object oriented side of it
and didn't consider them to be
part of the algebraic data type family.
So I think that answers my question
for why we want it now.
Okay various forms of pattern
matching you may have seen
in the wild in C++.
Let's start with simple types,
then well talk about how product types
and sum types are matched in C++ theory.
So matching simple types
is pretty common right?
We use switches all the
time all over the place
to match integrals.
In this case,
I just have an integer x
and I'm just going to use a switch to
perform some actions.
And of course someone will come along
and say why can't I match a string?
And you'll be like well,
it only works for integrals.
Efficiency?
But if it's actually what you want,
you should be able to do it I think.
So here's an alternative way to do it.
This is Python style switch, right.
You can construct your own map,
and then you can index into it
and call whatever action you wanna call.
Two ways of doing it.
Here is what it looks like
to decompose product types.
So to set it up,
let's have pair x y called p,
and you will have a
tuple of pair x y and z.
Cool.
In C++ 17 and we have still apply
which will allow you to
unpack a tuple, a pair,
or any tuplelike type.
And we can decompose that into
arguments into a function.
Alright so we can say apply of ds lambda
which takes x and y, give it a p,
and it's going to expand
the p into function
as the arguments.
On the right hand side,
I'm showing what it looks like if you have
the nested destructuring.
It's not pretty.
You have to apply the first level with t,
and then you have to expand
again with the second layer.
And you're gonna have,
let's suppose you have
four layers of expansion
that you wanna do,
you're gonna be going in--
Depending on how many
spaces you end up with,
if you had ended with two,
let's say four levels.
You're at eight to 10 that deep right?
There's also a new way to do it.
C++ 17 introduce structured bindings.
We can now say auto x y equals p,
and we're going to expand the p
into the variables x and y.
On the right hand side,
I'm try to show how t would be decomposed.
The last line is something that you
might think that is possible intuitively,
but this is actually not supported.
And so again we have
multiple levels of nesting
when we try to actually
decompose complex data types.
So let's change the hierarchy a little bit
and use a variant
so that we can visit it easily
rather than having to
do the whole setting up
of the visitor hierarchy.
And the code looks more of less the same
except they're using visit now.
And again I'll point out
that if you were to nest
your classes,
your types.
For example if you want to reach into neg
in the negation case,
and visit the expr inside of negation,
then we're gonna have to do
another layer of visitation
which is nested inside.
Okay this brings us to my library.
So I have four goals for the library.
One is that I want this to be declarative.
I would like to avoid having
to spell out the state
that I'm looking for.
I want to just describe what I want
and then see if it matches.
Number two,
I want it to be structured.
The chain of it, as we discussed,
the chain of (mumbles) are overly general
for the task at hand.
Cohesiveness.
The reason why I showed the,
I think it was seven
or eight different ways
in which we can compose
algebraic data types in C++.
We just went through three tasks,
and each of them had two solutions,
at least two solutions.
So the various forms of
pattern matching we explored
are very hodgepodge,
unsatisfying solutions to
pretty simple tasks at hand.
And each of them had their
own limitation switches
with their integrals visitation,
with their difficult to nest,
as well as their structured bindings.
Even the newest stuff has limitations
that we're not all that happy with.
And most importantly,
it needs to be composable.
And the reason why
patterns must be composable
is because ADTs are composable.
Because algebraic data types--
Because we build values
out of smaller components
and we build on top of each other,
we need to do the same thing with patterns
to be able to,
to be able to reduce
our cognitive overhead.
So syntactically this
is what it looks like.
Just a basic structure.
All of the,
you can just include
mpark slash patterns.hpp.
The using namespace is going to pull in
all the necessary components.
You can match on multiple expressions,
and then provide multiple patterns,
provide your bindings,
and it's going to execute them in order.
In order.
First fit execution.
Okay so let's get back
to the point example.
We has point x y.
We're going to instantiate a (mumbles)
of point with 7 zero,
and we're gonna match p.
And we're gonna provide the ds pattern.
This is the destructuring pattern.
And we're gonna say the
first one needs to be
zero or anything.
The second one's gonna
be anything or zero.
The last one can be anything anything.
And you can see the variable bindings
being introduced as the function
parameters of the lambda
on the right hand side.
So arg is the binding pattern,
and it's going to match any value
and pass that value onto the handler
on the right hand side.
And if there are multiple args,
it's going to pass them over in the order
in which they appear in the pattern.
Pretty clear?
And getting back to the
evaluating expressions example,
we now specialize variant size here.
And this is to communicate to the library
that I am a variantlike concept.
That I've modeled a variantlike concept.
So just a side note,
if you're using structured bindings
and you, for example,
say introduce a struct
that inherits from a tuple,
and you try to use
structured bindings on that,
that will give you an error
because of this exact issue.
It's going to say that your type
doesn't model tuplelike
because you didn't specialize tuple size.
So tuple size and variant size
are the customization points
for which you communicate
through the language feature
or my library that you are
tuplelike or variantlike.
Okay so this,
the evaluation might look like this.
We match on the expr.
And the as pattern is a new pattern.
And this is going to be the matcher,
the pattern that matches
polymorphic types,
variantlike types, and any like types.
By any I mean the stood any
that was introduced in C++ 17.
So you could thrown whatever into any,
and get stuff out with an any cast.
With this pattern,
you could get stuff out of an any cast,
sorry you can get stuff out of an any
by the library executing
an any cast operation.
So here we have a variant,
or at least something we opted
into as a variant like type,
and we're going to
provide our alternatives.
And we're also going to provide
the nested pattern here.
We already saw the ds pattern,
which was the destructuring pattern,
and that appears within the as pattern.
And we're going to bind the desired parts
onto the right hand side.
Here's another example.
Here I have an option flag example.
Supposed we have an
optional command light flag
and we're going to analyze the input.
So we match on the flag.
And the first one--
And so the new patterns here are sum,
none, any of, and a different use of arg.
So let's go through it.
So sum is a pattern that
the syntax would be sum
of sum pattern inside.
So I could say sum
underscore, which sorry,
we could say sum arg which would say,
if you're an optional
and you have a value,
then give me the value.
You could say sum underscore,
which is a wild card which I
haven't actually introduced,
but it's pretty intuitive.
It just ignores any
value that binds to it.
And so if you say sum underscore,
then it's going to match an optional
if it has a value but it's
going to drop the value
for example.
None will match if the optional is empty.
And of is typically seen
as the pipe operator
in functional languages.
It's the alternation pattern.
You can give it multiple patterns,
and the whole thing will match
if one of the patterns match.
The use of arg here is basically.
Okay so what we're trying to do
is we want to match the optional,
if the optional is set and
the value is underscore v
or under under verbose,
then we want to dispatch to this case.
Now once we get into the case,
how do we know which one matched?
Right we don't know whether it matched
because it was an under v
or because it was an under under,
dash dash verbose.
So by wrapping the arg around the pattern
that matches the value,
it'll pass the entire value
that matches at that level
onto the handler.
So on the right hand side,
we get the string that matched.
It'll have to be dash
v or dash dash verbose.
The second, oh, oh.
The second one,
in the second case we
introduced pattern guard.
So the when clause there that's bolded.
So the sum arg that I
described will match a optional
that has a value
and pass whatever value it's
holding onto the handler.
And the when guard will execute,
and we will only execute the handler,
if they when guard succeeds.
So if the when guard fails,
the difference between a when and an if
would be that in if,
you would still stay in that handler.
Whereas if a when fails,
you fall through to the next case.
Does that make sense?
The odds?
Good.
Okay.
Alright so summary of the patterns
that I've introduced so far.
We have expression pattern.
So the expression pattern we saw
in the first example when
we were matching zero right,
we saw arg and wild card.
Yeah we saw wild card.
Did we?
- [Man] Sum underscore.
- Okay yeah, yeah, yeah.
I introduced that.
Okay cool.
So the v structure pattern we saw first,
and the v structure pattern works,
operates off a concept as well.
And the concept is an array,
and when I saw array I mean a c array,
and aggregate, or a tuplelike.
And by tuplelike I mean the
tuple size, the tuple element,
and get interface that
structured binding uses.
So this is really useful,
and that's why our initial point example
worked without any boiler plate
of introducing tuple size
and get and stuff like that.
So the as pattern as I
mentioned matches sum types,
polymorphic types,
variantlike types, anylike.
Optional matches pointerlike types.
So the example that I showed
was with a stenoptional,
but you can also match
pointers with it for example.
And alternation,
yeah it's gotta match
one of the n patterns,
and then dispatch if any of them matches.
Okay.
Let me take a sip of water here.
This is our big example.
Okay.
So what we're gonna do is
take the expression tree
that we've been working with,
the latest one that we
saw with the variant.
And we're gonna simplify
the expression tree.
And the simplification rule is that
we'll cover our double negation plus zero
multiply by one, and what's the last one,
multiple by zero.
And we want to keep the
original tree in tact,
and we want to share as
many nodes as possible.
Okay so just the review.
Just the refresher on the expression tree
that we've been looking at.
Nothing's changed.
So the first case,
we'll go through it case by case.
So the first one is a simple case
where we're just simplifying an int.
We take sum expression tree,
we match the expression.
And if it holds an int,
we don't even care what it holds,
we're just gonna return
the expression directly.
So if that's the expression,
our result is gonna be
just another shared pointer
that points to the same integer.
Here's a double negation case.
So,
so just a reminder on the colors.
The purple is the value
that we're matching.
The pink is the patterns
that we're matching with.
The yellow is the variables
that were introducing.
And the green represents the tree.
So the double negative case.
So we're gonna first match the outer layer
with the negation node.
Within that we're going to destruture
the negation node into it's part,
which it only has an expression.
We're going to then
because it's a pointer,
we're gonna access the
value inside the pointer,
which is the expression,
with the sum.
We then match another negation node.
So if any of these fail,
so for example,
if the shared pointer in null,
which in our case it can't be,
it would actually not
match this pattern at all.
Alright so we're not running into
sum null pointer exceptions,
or anything like that for null.
You referencing null pointers
or anything like that.
And then when we get to
the part that we want,
we're going to dispatch
by the value that we want,
and we're going to simplify it.
Simplify that.
And that's gotta be our result.
We gotta recursively simplify.
Throwing out the two negations,
we're gonna simplify the expression
that lies, stands two levels deep.
So this is our result.
Alright we simplify the
minus minus 17 to just 17.
Go through the rest of it a
little bit faster perhaps.
Here's plus zero.
Now we have two cases.
We're gonna match zero of anything
and anything with zero.
So we have the as pattern again.
Match the add.
We're gonna destructure that add
which has two components.
So the left hand side,
we're gonna say if that
thing actually holds a zero,
we're going add int
and also checking that that value is zero.
On the right hand side we don't care.
And then we're gonna throw out the zero
and simplify the other side.
So in this case we have
expr left to right,
and our result is going to be just 17.
Multiply by one.
It looks exactly the same except
we change it to multiplication and one.
Same case.
So we'll just skip over that.
The multiply by zero case
is pretty interesting.
So similar pattern right?
We have multiplication
and we're gonna inspect both sides.
Zero or anything, anything or zero.
And we don't care what the anything is
'cause we're gonna return zero.
But remember one of the
requirements I mentioned
at the beginning of this exercise,
which is that we want to share
as many nodes as possible.
So what that means is when we have
we want to return zero here,
but we don't want to create
another node for zero.
We want to actually reuse the
zero that's already there.
And so this is where I use the arg pattern
that binds a bigger portion of
the pattern that you wanted.
So here I'm actually saying,
look down and see if there's a zero.
And if there is one,
give me the entire shared pointed
so that I can copy that right.
'Cause otherwise I don't
have access to the actual,
the pointer to zero.
So in this case I'm gonna actually get
the result to be zero.
And it's not gonna be a new zero,
it's going to be an existing one.
Simplifying negative in the general.
So when it's not a double negative,
then we're going to,
the case is actually pretty simple.
So we're going to first try to simplify
the thing underneath the negative.
And if that changes anything then--
If it doesn't change,
then we're just gonna return the result.
So basically the first case here,
the first case of the conditional is
the left hand side diagram.
And we try to simplify e,
and the simple e comes out to be
oh it's the same thing.
Okay well if it's the same thing
and you couldn't simplify,
then just return the
thing that you had before.
If you try to simplify,
and you don't get the same thing as e.
So here we have the simple e at 17
as opposed to each which
is pointing at the plus.
Right so if it's not the same
then what that means is
that we were actually
able to simplify some stuff.
And so we need to make
a new node, reattach it,
and then this is our result back.
And this is because the first requirement
which was that we want to keep
the original tree intact right.
We don't want to modify the top level,
minus two point at 17,
we want to actually keep that one in tact
and leave it alone.
Looks more complicated,
but it's exactly the same thing.
We simplify the left hand side,
simplify the right hand side,
and we check whether anything changed.
If nothing changed,
then we returned the result.
If something changed,
in this case for example,
then we're gonna create a new node,
reattach, and return the result.
And multiplication here
is exactly the same thing.
So if it didn't change,
return the tree.
If something changed,
create a new node and
reattach the returned result.
And putting it all together,
I've consolidated a bunch
of the cases with anyof,
which was the alternation pattern.
And so you can see that
if it's negative negative,
negative negative v, v plus zero,
or v multiplied by one.
All of them we just ignore the thing
and simplify the expression.
In the zero case it's a little different
where we bind a sub portion of the value,
and we saw some of the more complex cases
in the generic cases.
So what I'm trying to
highlight with this example
is the nesting right.
Maybe that's obvious,
maybe that's not.
But I would like to
point it out explicitly
because that's really the
power of pattern matching,
which is describe the full state
of the value that you're looking for.
And we just see if it's there.
And this is basically what I just said.
The real power of pattern matching
is that the patterns are
built the same way as values.
I want to show you a few more things.
These are some of the
challenges I ran into,
and some of the cool things I found.
So one of the challenges
is identifiers right?
Introducing identifiers.
There's only so many
contacts and so many ways
that you can introduce identifiers in C++.
And in functional languages
like Rust that we saw,
well Rust isn't a
functional language perse,
but they allow you to introduce
identifiers within the pattern.
Alright so when we said point x y,
we're introducing new
identifiers in place.
And we can use that on
the right hand side.
We don't have that luxury in C++.
I can't just introduce
identifiers within an expression.
And so my current solution is to say
alright let me just fill
it in with placeholders
called args here,
and then actually give it
the name of the lambdas
on the right hand side.
But from the examples,
based on the examples you saw,
you can see that it's
actually very verbose
and difficult to read
because when you look at the pattern,
you just see a sequence of args right?
Arg, arg, arg, arg, arg.
Okay what's that right?
If the fields actually have some meaning
and you were able to give them names,
it would actually be a lot better.
So patterns do not have any identifiers.
So can I do something about this?
And before we get to whether
we can do anything about it,
this is,
you saw example of pattern guard before.
Simple example of Fibonacci where if it's,
if the value is less than equal to zero,
then we're just gonna return zero.
And the reason why this
when is inside the lambda
and not closer to the pattern,
which is what most of
you would probably expect
is for the when to be right
new to the pattern right?
Because that's the whole criteria
for which you need to satisfy
before the handler gets called.
But this when is inside the lambda
because if I were to pull the when out
and put it beside the pattern,
then I need another way
to introduce identifiers.
And this way I'm actually
leveraging the fact
that I already introduced the identifier
with the lambda,
and I want to just be
able to refer to that
and use it immediately.
It's inside because we want
to reuse the identifier.
So this is another experiment
that I've conducted.
And I actually quite like it,
but I haven't flushed it out fully yet,
but I still think it's pretty cool.
So you can introduce identifiers
before the entire thing.
So it's a little unfortunate
that you have to be,
you know you have to list
off all the identifiers
that you're ever going to use
in all the patterns upfront,
but it's really,
it's really in the lambdas
or before the expression so.
This is the only other place
that I was able to find something.
And we can,
let me go back.
And we can use the identifiers
that we introduced,
x and y, inside the pattern.
And so for example,
if you have more meaningful names,
it would be helpful inside a pattern
because you no longer
just see arg, arg, arg.
Maybe you just see first
name comma last name.
So the cool part here.
So the identifiers,
it's all in caps which means?
- [Man] It's a macro.
- It's a macro.
Why is it a macro?
If it's just the arg,
why couldn't we have just deco type arg
and then just gave it names.
And the fancy thing here
is that repeated identifiers
mean the values have to be equal.
And I find this really cool
because most functional languages
actually don't support this.
And it's interesting because I think
the reason why they do it
is related to the fact
that they're introducing
identifiers in place right?
They're introducing identifiers in place,
and they're saying well you
introduced another identifier,
you have a duplicate
identifier essentially.
As opposed to saying,
okay you said this thing again.
We'll take that to mean that you want that
to be the same value as the first one
that you declared right.
Same reason why we can't
declare multiple variables
of the same name in C++.
Whereas if I declare it upfront,
when I refer to it twice,
it's pretty clear that I'm referring to
the one that I already
introduced previously.
So here's kind of a dumb example.
But we have a triple:
one oh one, two oh two, one oh one.
And I can provide my
patterns, x x x or x y x,
and it's trying to do what you expect.
Good.
I find it fun.
Back to the pattern guard.
Because I've taken on this burden
of having to introduce
identifiers beforehand,
I should be able to use it right?
Somehow.
So this is a way in which you
can create lambdas in place,
similar to how Boost Phoenix would do it,
where you have some magic placeholder,
you perform some operations on it,
and I'm gonna hijack those operators,
and create a lambda for you.
And so this is implemented and works.
I don't know about the EDSL approach.
I don't know which one
I hate more basically.
The when macro inside the lambda,
the distance is a bit far in macro.
And this one's,
it's kind of nice but DSLs.
So mist of experimenting.
This is the cool thing that I found
that I wanted to show you.
Variadic pattern.
Another thing that a lot of
functional languages don't have.
But a lot of functional languages
also don't have variadic templates so.
That's, I think, where the difference is.
So this pattern can appear
exactly once inside a ds pattern,
the destructure pattern.
And it's going to repeat itself
how many ever times it likes.
How many ever times it needs to.
So it could even appear
as the first argument
in a destructure pattern,
and it's gonna expand itself
how many ever times it needs to.
So here's a quick example, another triple.
And we're going to say
decomposing of the three parts,
or I could also say variadic arg.
And it's actually not any shorter.
So that's unfortunate.
Where it gets interesting though
is when we throw it inside of a template.
So we create a template here,
we have some tuple.
And with the arg comma
arg comma arg approach,
well we would have had to
somehow get the tuple size,
expand that into an index sequence,
and then have the arg
expand out that many times.
This way we can actually
just say variadic arg,
and that will just expand how
many ever times it needs to
based on the tuple that comes in
that it gets instantiated with.
Pretty cool?
What does it look,
it looks kind of familiar.
This is actually C++ 17 apply right.
We can take some function, a tuple.
We match on that tuple.
We're gonna expand it,
and our handler is just the function
that we're calling right?
Cool.
We can get a little fancier though.
This is almost tuple_cat.
The almost part,
we'll see if anyone sees why.
So we have another triple.
This is just showing that,
remember we can expand the tuple
with ds variadic arg.
So if we can expand a single tuple
with ds variadic arg,
then if we have end tuples,
how do we expand all of that?
We would have ds variadic arg comma
ds variadic arg comma
ds variadic arg comma
dot, dot, dot right?
Okay so that looks similar.
So we'll have variadic ds variadic arg.
We'll take how many ever tuples we get,
expand all of that.
The arg is going to expand how
many ever times it needs to.
They dispatch to the handler
in order of how they appeared.
And it only appears once,
but once it's expanded,
they appear multiple times in order.
So we pass all of those
along, create a tuple.
Where is the almost?
It looked fine to me,
but tuple_cat does this weird thing
where if you take tuple of x's
catted by tuple of y's,
it will always give you
tuple of x's and y's.
Whereas here,
if I have,
if I have tuples that
have references in it,
for example,
this will always create
a great values right?
It's gotta decay all the elements
and decay construct values
out those references.
So it doesn't exactly do tuple_cat,
but hey it's still pretty cool.
People might be concerned
about performance.
So to be honest,
I don't care about performance.
And what I mean by that is
for this particular project.
And that's because the open
pattern matching for C++
that Yuriy, Gabby,
and Bjarne did has a lot of research
as to how we can actually get performance
out of these constructs.
So what I'm mainly focused on
is how do we introduce
identifiers and where,
and how do we actually
get this API to be usable
because if you actually
compare the library examples
to some of the examples you'll find
in functional programming,
it's actually very verbose.
So I'm more focused on how to get,
focused on getting the API clean,
and experimenting with what
kind of patterns are out there,
what kind of problems
exist specifically for C++.
But I did this experiment
because I was curious.
So I went on (mumbles), plugged this in.
This is a simple
implementation of fizzbuzz
where I iterate through one to 100.
I'm gonna mach three, mach five.
Do the multiple values match,
zero, zero fizzbuzz.
Zero anything buzz.
Zero anything fizz.
Anything zero buzz,
and anything that's printed number.
And here's how I would've written it,
the same program in regular C++.
Basically the fastest way
I know how to write it
while maintaining the same semantics
as the previous program.
And they both complied out through
this exact sequence of assembly.
They generate the exact same code.
There are other examples that don't,
but this one does.
And I think,
(audience laughs)
and I think this is more
so credit to the compilers,
not to me
because I actually didn't do anything
to make it performing.
So kudos to compiler vendors.
And that's actually
true for Clang and GCC.
So some future work.
I want to,
so what I haven't shown you
that you typically expect from
a pattern matching mechanism
is expanding lists ranges.
The difficulty there is that
for functional languages,
lists are recursive.
They're essentially just
pairs or pairs of pairs right?
You just (mumbles) them onto each other,
and you break them up into two parts.
Whereas for us,
there're actually ranges
which have begin and end.
And that's fundamentally
different way to represent lists.
So when,
so for example if I match a vector,
how do I break that apart?
I can say arg to pull
out the first element,
and then how do I describe rest?
I need to describe somehow
the notion of tail.
And we don't really have a good mechanism
to describe tail in a range.
Maybe Eric's range library will help.
Experiment further with identifier.
The current state,
it's okay but it's not great.
And lastly exhaustiveness checking.
I haven't done any work there,
but it would be really
interesting to figure out
or find out what we can do there.
So that's all I have.
I think we have 10 minutes
left for questions.
If we don't have any questions,
I have some more stuff.
Alright any questions?
Yep?
- [Man] What's the status of the ts?
- Well there is no ts.
So the question is what's
the status of the ts?
The answer is that there is no ts.
There was a proposal that was written,
but I don't think it was discussed
at the committee meetings.
We'll see what the next
iteration of that paper.
- [Man] What about the--
- Which tuples--
- Sir?
- [Man] Oh sorry, the language variant.
- The language variant one?
Oh yeah, yeah.
That's what I'm talking about.
So that paper, I'm not
sure if it was the skills.
- [Man] You mean David's paper?
- Yeah.
- From last year.
It was discussed (mumbles).
- Okay.
And was there any news about that?
- [Man] There was discussion.
(audience laughs)
- There was--
- [Man] I think we're waiting on him
to come back for revision.
I don't remember.
- [Man] Yeah I think the main takeaway
was to split the paper into multiples.
- Yeah, right, okay.
So the question was,
what's the status of the
paper that was written.
The one that I mentioned at
the beginning of the talk.
The status of it is that
it was discussed in ULU.
And we're waiting for the next iteration,
for which I'm a coauthor,
so I guess some of
that's on me to deliver.
Yeah Jason?
- [Jason] So you're talking
about introducing identifiers.
- Yep.
- [Jason] But in a more general sense,
you're introducing free variables.
Because you're introducing that--
- Yes.
- You actually represent
the same value.
- Right.
- [Jason] 'Cause it would be automatic
equivalency checking points so.
So what's missing is type.
It's not just x,
it's the free integer x.
Or the free string x or whatever.
And you might want that,
so you don't really care what the type is.
- Right.
- [Jason] But you might not right.
So do you like specifically
want string (mumbles).
My pattern won't match
x that's an integer.
- Right.
- [Jason] So I think maybe
you may have actually just
stuck your spade directly
into why this has to
be a language integer.
You're introducing a common
identifier in the variable.
- Okay so let me try to summarize that.
So Jason's comment was that
when I talk about introducing identifiers,
what we're really talking about
is introducing free variables
for which we're trying
to resolve it's value,
and it might be type constraints
that we want to put on
them and stuff like that.
In conclusion,
he thinks that I may have hit on
the reason why it needs
to be a language feature.
Yeah maybe.
I don't know.
But the only comment that I have is
the comment about constraining types,
like oh I want this pattern to be an int.
Right now in the library I
actually get some of that
with the type checking alright.
Like if the,
if the rest of the values are integers,
like you can't,
the type system won't let
you put anything else there.
And so,
so for example,
if I have a three tuple
and I try to match that with
a pattern ds underscore underscore,
that's not gonna compile.
It's not gonna just
ignore it and continue,
it's just not gonna compile.
And so in some sense,
the type system helps me with some
of the exhaustiveness checking.
In the sense that we have patterns
that are never gonna match
because the types don't match,
and I'm not gonna execute those because--
Well I'm not gonna tel you and not compile
because I know this is never gonna work.
Never gonna run.
So it's kinda of, yeah.
It's kind of interesting.
- [Jason] In the sum cases right,
it's very similar to visitors for variant.
If we can disambiguate on the type--
- Yeah.
- [Jason] Then it's a type switch.
- Yes right, yeah, yeah.
So the comment was that the visitor,
or that the as pattern introduces types
and the alternatives,
and that's very similar to visitor.
And it's essentially a type switch.
And if we can do
exhaustiveness checking on that
then we can,
it would be very useful.
Any other questions?
Okay if there are no questions,
I have five minutes if you guys wanna see
a peek of the implementation.
- [Audience Members] Yeah, yeah.
- Alright.
So this is a structure,
as I described earlier.
So the pattern here is going to generate,
or give a return a value of this proxy
intermediate class,
which has an operator equals,
which returns something that it probably
shouldn't be returning.
Namely its not pattern ref,
we're returning something
completely different type.
So we're returning some case,
and this case comes from this equals here.
So considering that whole thing,
we return this whole case.
And the match is going to
return this intermediate type,
which the operator paren.
Which is how we are able to
actually make that second
set of paren calls.
And it's going to take
a sequence of cases.
And it's going to store the values,
and once we actually
call the operator paren,
then we're going to try
each of the cases in turn.
So the try match function there
is the customization point
to introduce new patterns.
So did I hear something?
- [Man] Oh I'm sorry.
I'm just realizing that
this is where we could do
best match instead of first match.
- This is where we can do best
match instead of first match.
- [Man] You have to
pass over and (mumbles).
- Pass over the what sorry?
- [Man] You have to pass over the cases
and scoring in the first
and do best matches.
- Right, right.
Yeah, yeah, yeah.
So yes.
And this is where I would do
exhaustiveness checking as well.
So the reason why I'm not so concerned,
why I'm optimistic about
exhaustiveness checking
and stuff like that is because,
because I know I have all the
information in place right.
It's not like oh I don't know
if I'm actually gonna
have enough information
to be able to do it.
Like I know I have it,
I just need to read papers
to figure out how to do
exhaustiveness checking,
and then we'll figure it out.
Best match I think is something
that I don't really want to do.
If you look at,
so like,
so we have a lot of pattern
matching mechanisms in C++.
Namely like overload
resolution, for example,
is a best fit pattern matching.
And overload resolution
roles are so complicated
because they're trying to
figure out which one is best
as opposed to just saying
oh yeah this one's fine.
This one matches.
And so we have all these rules
around integral conversion sequences.
And I think the complexity that it adds,
I mean it's nice,
but I'm not sure if it's worth it.
Okay let's move on.
So okay so hold on.
First of all,
this would have been two functions
if it weren't for, if cost expr.
And I'm using cost expr
like all over the library.
I think,
so last night I counted and I have like
I have 48 instances of cost
expr at the library right now.
Which is like my new
favorite language teacher.
So what is this result here?
This result has type match result,
and it inherits from optional forwarder.
And the forwarder thing is there
because optional doesn't let
you put references in it.
And my, when I call the function,
it might actually give me references.
And I actually need to forward
that reference out properly.
So that's why that's there.
We're not going to look at forwarder.
It just forwards stuff through.
It's not all that interesting,
but it's a tricky thing I write.
So here's match result aware invoke.
So if you call regular invoke,
then you're going to have to deal
with void and stuff like that
'cause Matt hasn't finished his paper.
- [Man] The paper's done.
- The paper's done.
Matt is trying to push
regular void through C++.
It hasn't happened yet.
So for now,
I have to deal with this stuff.
If it's void,
then create the void type
and make sure you propagate
the void type properly.
If it's a match result,
don't layer match results
on top of each other.
Et cetera.
And this function is what
the try match function uses.
So here's the implementation of try match
for expr pattern.
And the expr pattern is just a name right?
It's gotta pick any value
and just compare equality of comparison.
So we take some value.
We perform equality comparison
with the value that we're trying to match.
And the third argument there
is the function that you give all of the
values that you want
to pass to the handler
to that function.
So in this case,
an expression pattern
doesn't match anything.
It's going to just call f with nothing.
Whereas the arg pattern,
which actually passes the value over,
is going to,
if you look at the
invocation for match invoke,
it's going to pass that
value over to the handler.
And if the arg has,
if it's the more (mumbles) of arg
where it has a nested pattern,
then we're gonna just delegate
that to the inner pattern.
Make sense?
And that's it.
This is not for you to see.
Thank you everyone for coming.
It was fun.
(audience applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>