<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: T. Winters &amp; H. Wright “All Your Tests are Terrible...&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: T. Winters &amp; H. Wright “All Your Tests are Terrible...&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: T. Winters &amp; H. Wright “All Your Tests are Terrible...&quot;</b></h2><h5 class="post__date">2015-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u5senBJUkPc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right welcome everybody on this
Tuesday afternoon
you are almost halfway through with the
conference congratulations
welcome to all your tests are terrible
tales from the trenches this might also
be titled the Titus and Hyrum show I'm
Titus winters I'm home right and
together we work on we're among others
we work on the C++ library
infrastructure at Google so we spend a
lot of time dealing with other people's
code and seeing other people's tests and
cursing other people's tests and so this
is something of a confessional session
as well as hopefully a learning session
as well but we don't really want to be
just pointing fingers at Google
engineers or anybody else we we
understand how bad tests happen it's not
it's okay right but we we would like
this to be educational and kind of point
you in the direction of having some
better practices so to that end we're
going to introduce our guests for the
afternoon
you may remember them from such things
as being six years old
this is goofus and gallant how many of
you remember goofus and gallant a few of
you all right
these were kind of comic figures like in
the literal sense not that they were
actually funny but they were in comics
from I knew them from highlights for
children and they're kind of this like
preachy goofus does a bad thing talent
does a good thing turns out we've
followed them through the years and they
grew up to be software engineers so we
will describe the software engineering
practices of goofus and gallant over the
course of the talk so there are five
properties of tests that we'd like to
talk about today they aren't the only
property some good tests but they're
five that we've identified as tests of
how these properties are good for your
code and will help you actually test
correctly right so we're gonna start
with correctness this
is kind of the the number one well maybe
number two thing that you're really
looking for in a test right this is
testing that your API is actually
behaving correctly right good place to
start so the second one is rebuilding
write your tests actually don't have
tests of themselves and so your test
should be readable they should be
correct by inspection to both yourself
as well as another reader which may be
yourself in like three weeks right so we
want to make sure our tests are readable
we need to make sure that tests are
complete all right that is you can't
just get away with writing one quick
test one demonstration of an easy case
and call it good right we need to make
sure that you have tested completely
through your API and only your API also
tests are a great opportunity to
demonstrate the monster ability so tests
give you the opportunity to show how an
API should be used and if your tests if
you can't write tests without shortcuts
maybe it's not that great of a test
maybe that's not that great of an API me
it's not that great an API you write
also and this is gonna be kind of our
favorite part of the talk resilience
this is the notion that your tests
should only fail if the thing that they
were actually intended to demonstrate
became false and should fail for no
other reason this is kind of critical
okay so but all of that it's kind of
this is these are these are goals these
are ideals these are things that you
know we want all good tests to have but
in order to have properties of good
tests first off you have to have tests
so goofus doesn't write tests gallant
actually writes tests right this is step
zero right if we learned anything from
CP Khan last year it's that we're having
trouble writing tests as a community
rate so if you aren't writing tests none
of the rest of this talk actually
applies very much right so go write a
bunch of tests come back and watch the
talk on YouTube whatever end and take
these principles to heart rate but
please write tests all right this is
non-negotiable okay write tests
don't be goofus yes okay goofus will
just write any test that passes all
right ah it's green let's be good so
gallant takes the opportunity to
actually think about what he's testing
right to think about what is the purpose
of the tests and what the tests should
actually be customer this is getting at
correctness our first major bullet point
all right test you must verify the
requirements of the system are met
goofus instead writes tests that depend
on known bugs we have certainly seen
code that is morally equivalent to this
and many of you probably have to write
so goofus writes the square function
which is undocumented but assume for a
moment that it squares an integer right
and then he writes the corresponding
tests and he's very happy because these
tests pass it's green it must be good we
can go home right no we cannot go you
can't go I say I really truly hope
number one that your organization's all
are writing tests that is number one for
sustainability I will give a whole talk
on this later in the week number two is
you have to have code review and you
have to take it seriously so hopefully I
test this blatant never gets into the
code base right even if your code
reviewer skipped double-checking
doofuses work
on square he sure as hell should have
validated that the test makes sense
right so hopefully this was caught in
code review and the tests were fixed if
nothing else
so hopefully gallant comes along fixes
goofus is poor test and then we were
problem right gallant has just caused
the tests to fail right everything is
red haters are going off you know build
cops or screaming right the tests are
failing but where is the bug is the bug
in the tests or is the bug in the
implementation right as an organization
you need to make a decision whose job is
it to fix when a test fails do we just
blame the guy who caused the test or the
the programmer that caused the test to
fail or do we think about why is this
this actually failing is the test now
more more correct and what does that
tell us about our actual implementation
now when you start approaching tests
with an awareness of what the goals are
correctness among other things then you
can start to have a mechanism to like
look at the change in question updating
this test and decide rationally who
should be blamed when the test goes red
right and whether or not you need to
roll back and etcetera etcetera rolling
this back is not going to help anyway
you can guess who gets lots of the blame
at Google sometimes in theory we're not
pointing finger goofus also has a
tendency to write tests that don't
actually mean anything they don't
execute any sort of real scenario right
so imagine this case in which for some
reason you're testing the world and you
know the world is a very expensive thing
to set up and build and use in your
infrastructure takes day it takes days
to create the world but
so you you mock out the world right
create a mock world has maybe slightly
different characteristics than the
actual world like maybe it's flat and
somewhere in your test you start
depending upon this fact that the world
is flat in your test environment right
now that's all well and good if you're
shipping a flat world to your consumers
right presumably you're not and you're
actually using a real live system and in
that case this you've traded you an
environment in which you're not actually
testing the actual system under test
right you're testing the fact that your
mock actually works correctly right
you're not actually testing the system
right so this test just has no point
having gone from correction goofus does
not think about the reader of his test
gallant takes the opportunity to
understand that his tests are correct
and writes tests to make sure his
readers know that they are correct one
of the reasons that we focus a lot on
readability incidentally is because your
tests will be read many more times than
they are written let's all pay oh yeah
presumably you're only writing the test
once maybe run many times but you it
would be read over and over and over
again by you or somebody else right so
optimizing for readability is actually a
win even if it takes a couple of more
characters gasp so some ways that goofus
has failed at writing nice readable
tests goofus has a tendency to write
tests that have lots of boilerplate in
other distractions this is a little hard
to come up with slide code examples for
but it's not unusual to see a test like
this you have okay my test this is a
test of big system and according to the
top the point of this is that some call
is actually unimplemented then I have a
whole
bunch of setup I'm setting up the
storage system I'm setting up my thread
pools I'm getting things started I'm
initializing this is both about the but
about the but oh and there is one thing
that is actually pertinent to the
purpose of that test right that is
completely lost in the noise right it's
very very very hard for a reader of this
test to separate like meaningful
important part of the mic claims that
are being made by the test from just the
rest of the line noise in the setup in
this instance we have significantly
hampered readability by just cluttering
the whole thing up goofus can also on
alternate Thursdays go completely the
other direction and hide all of the
relevant context far away from the test
itself this has a tendency to look like
this I'm going to test my big system
there is some function that initializes
the test system and all of the test data
it's hopefully somewhere in this file
but maybe somewhere else in some other
library for you know test setups and
then I'm gonna assert that the private
key is 42 is this correct
who knows right there is literally no
way to tell whether or not this is
behaving as intended and a good test
should be correct by inspection right at
minimum a code reviewer of this test
should complain rather loudly that there
needs to be a comment saying it is part
of the API guarantee of initialized test
system and test data that the private
key that is set in here is 42 but it's
probably better just restructure the
whole thing so the other pitfall in this
is to use the advanced features of your
test frameworks right we're all
engineers and we all like to use the new
shiny whenever it comes along and this
can apply to advanced features in your
test framework so imagine a test that
looks something like this
the actual test is down here at the
bottom the test fixture we're using G
tests throughout this these slides which
is what we use internally at Google and
so this is the text the test fixture for
the test right and you can see there are
some we set up with some setup code we
have a constructor to the actual text
fixture in fact I think the next slide
has the yes right so G test actually
gives you multiple ways of instantiating
test information test the test fixture
itself right and we're using both of
those ways here why because we can like
why not right the other problem is that
big system or big system down here is a
protected member of the test framework
itself but it isn't referenced anywhere
else in the test class right we're not
calling anything on it right we're just
constructing one and we just have it
right so that's kind of funky I'm pretty
sure pretty sure that all of the this
the whole test is completely identical
to this as far as the test passing goes
it's a part of me that's not a hundred
percent sure is the question of whether
or not writing data specifically the
string hello world new line into the
file foo bar Baz has some weird
undeclared impact on how big system
initializes right if this test doesn't
pass then clearly there's something
going on in that you know loading file
system state etc etc etc now we need to
do that but we can do that explicitly
right here in you know one line right in
the test and that's going to make
everything far clearer right see that so
there's a balance to be struck when
writing tests right test should read
like a novel I love a test like I love a
good novel it should have a beginning a
middle
on an end a novel should introduce its
characters a test should introduce the
set up right what is your test data
what is your initialization a novel /
Cupid progresses with you know the plot
your test progresses with actually
calling the thing in question what is
under test your novel concludes with
everything kind of wraps up the hero
defeats the villain the girl gets the
boy etc etc your test concludes with
given those setups and the call
you should have clearly that end result
right much like it would be bizarre in a
novel for you to have set it up as a
total you know romantic comedy and then
in the final act
a dragon swoops out of nowhere I need to
meet the protagonist it's kind of weird
when we can't tell in your test why the
results are what they are like are what
that you're claiming them to be it's
kind of like if you have to jump back
and forth all over your test fixture or
different files to find out what's going
on it's kind of like reading a
choose-your-own-adventure from front to
back
yes that so those are thoughts on test
readability goofus writes tests for two
simple cases and then goes running with
scissors
gallant uses his scissors to create nice
tests that demonstrate edge cases right
that he thinks about what is the domain
of my tests my system under tests and
where do I need to test that where could
it fall down as programmers we are
really really good about knowing where
the pitfalls are in our systems and
we're really really good about not
writing tests for those pitfalls because
then you think we'll fix the bugs
all right so imagine goofus writes the
tests for all of the easy cases right so
I mean this this should get past code
review I guess in some respects right
it's correct by by by inspection only
the implementation looks something like
that
right goofus has written the minimal
implementation that will pass these
tests right what does that say it says
that we need to think about writing
better tests so the implementation has
to be a little bit more thorough gallant
on the other hand does a thorough job
gallant includes those simple basic
tests but also includes edge cases like
zero edge cases like 12 factorial is as
large as you're going to fit in a
regular int factorial of 13 overflows
let's make sure that we got back
whatever we're specifying for that let's
make sure there's no internal state that
gets screwed up once you go to overflow
so we'll call factorial zero again just
to be sure it's not gonna cost this much
except for you know 10 CPU cycles or
whatever and test the crazy cases
factorial of a negative number I had to
go look it up on Wolfram Alpha I don't
know what it's supposed to be but
apparently infinity is good so I don't
have time to to comprehensively test
every single input especially for any
non-trivial function right but think
carefully about the edge cases that you
want to test and where are those where
you're the discontinuities and your
inputs may lie right what kind of test
cases will generate interesting results
and in some respects what kind of edge
cases your users are most likely to
encounter those need to be in tests
we I find that as engineers as
programmers eventually as engineer as
programmers we really desperately want
the code we just wrote to work and it is
really kind of antithetical to all of
our hopes and dreams to then go write
tests to try to poke holes and
demonstrate that it didn't work and this
is a reasonable impetus for write tests
first because then you don't have to be
so antagonistic with what you just
created but ya saying there's a little
bit of goofus in all of us yeah so the
other part of completeness is making
sure that you only test the things that
you are responsible for right so this
the author of this test presumably
goofus decides that you know it's really
important to know whether vector
functions as advertised this is part of
my filter function depends on vector so
I need to write tests for vector as well
and they're not even segregated that's
like in the same test method right so as
part of my test for a filter with vector
I'm going to make sure that I can add
stuff appropriately to the vector and
that it behaves as I expect it to right
the question here is where do you draw
the line right if he's not trusting that
vector works why is he trusting that his
allocator works why is he trusting that
the compiler works why does he trust the
operating system works or the cpu why is
there not something in this test to
mystically validate that his processor
is working correctly all right where do
you draw the line if you're not drawing
the line at what you implemented it's
perfectly reasonable to want to make
sure that vector works but in the test
for filter is the wrong place to do that
in the tests for vector is the right
place to do that high standard library
guys
so gallant only writes the tests that
make sure that his API behaves correctly
which in some instances is also we
exercising vector but that's not the
intent of the test that's the side
effect of the test goofus uses private
api's in his tests in ways that users
couldn't taystee gallant uses tests to
show the way that his api's should be
used right and this plays into the
concept of monster bility tests should
be a tool by which you can demonstrate
the proper use of an api to your
consumers right so goofus violates that
principle in all kinds of ways right
he writes tests that depend on private
private methods he writes methods that
he uses friend to make sure his tests
can twiddle secret bits inside of the
class under tests not really good ideas
and when you find that your tests are
doing this this is highly suggestive
that your API is probably bad to start
with we'll demonstrate so this test
goofus wrote this not a good plan
he's he's testing class foo right and
he's added a special shortcut method
right that can only be called by his
tests and that does some kind of special
initialization just for testing right no
consumer is ever going to be able to
call this and he's not actually testing
his API the way that an actual consumer
of the API would this means that even if
he happens to get most code coverage
he's really not testing this class in
the same way that his users are going to
use it right
those tests may not mean much it would
be far better for goofus to just write
the test in terms of the normal API and
drop all the rest of the crust
no I had a thought I lost it I don't
mean this guy um ask and then our
favorites goofus just writes tests that
depend on any feature of any interface
that you can find gallant writes tests
the only depend on the published API
right there are many ways to screw this
up and goofus loves to do all of them
Rufus loves flaky tests that is tests
that when you rerun them give you
different results they've never seen
these before in the wild in their system
okay good we aren't alone there are
people that have seen tests yeah indeed
indeed right so this plays into the
concept of resilience right tests should
be resilient so we talk about flaky
brittle tests tests are that are brittle
are kind of like balanced you know
seventeen rounded spheres on top of each
other and if one of those gets slightly
out of kilter the whole power falls
downright brutal tests depend upon
implementation details of methods that
you don't have any control over or any
business depending on or any bill get
infinite yes
that's where it depend on execution
ordering we will demonstrate but you can
kind of get this sitting just mocks can
be a big problem and non hermetic tests
but let's start with flaky goofus writes
flaky tests these are the ones that when
you rerun them the behavior changes
right so we have definitely seen tests
of this form we're gonna do some sort of
async update and we're gonna wait until
it's done there's no coordination
synchronization signaling anything like
that I just I'm pretty sure that that
should be quick half second will be
plenty this test fails all the time on
when you're say test cluster has high
all right as soon as the operating
system starts thrashing or your
scheduler goes out problems right don't
write this test what are some other
reasons for flaky tests while we were on
this slide it's not just a single resync
really it's not I mean we see flaky test
for time out reasons we see them because
people are reaching out to touch
production systems when they really know
no business doing that medic environment
flaky can all is actually a symptom of
lots of these other right other lying
causes right brittle tests on the other
hand if you rerun them at the same build
they're gonna pass the same way like
that they are nice and you know they
aren't necessarily flaky they're bad in
a different way so for instance goofus
may have written something like a tag
set it's something like an unordered set
under the hood he put some data in he
makes sure that he can get the data out
he's confused why the data comes out in
a different order because he hasn't
really thought through what an unordered
set means it's not so good at names and
so he's kind of scratches his head and
moves on
gallant on the other hand understands
what's going on here and updates his
tests so that it doesn't depend upon the
implementation details of something that
he may not have any control over
Galit possibly have been working on that
word positive to fix this test so if we
come across this fairly regularly
many of our hash functions are tagged
specifically may change at any time all
right because we need to change the
optimization properties or what not okay
every single time that we do that we
find that goofus has tests that depend
upon the particular order that you
traverse an unordered set for no good
reason I mean there's no reason why his
tag set should depend upon the ordering
of is a good reason he should use a
different thing that promises to be
stable right but by updating his tests
to assume that or to expect that the
unordered elements are and get the
elements right he avoids the brittleness
right another way tests can be brittle a
very strong code smell a very strong
suggestion that your test is about to go
in a bad way so when you start capturing
the logs before you run your test how
could this go wrong Titus
well Hiram let me tell you
turns out I've edited file dot CC once
or twice and if you change the line
numbers add or remove lines above 421
this test in some far-off file fails
right they didn't care about that line
number I guarantee they only maybe even
cared about file dot CC right the
interface that they're using is probably
to blame to some extent right they had
to go through some weird contortions to
start capturing the logs and get to the
point of doing this instead of you know
making a call and getting back an error
right but so there was probably a call
that was missing or lying about the
error or something right there's
probably an API bug in there somewhere
that excuses why they did this but this
is still the wrong way to do that right
a regular expression here would be far
superior yep hyrum has a great story
about image compression yes so
I spent much longer than it needed to
take to upgrade the image handling
library some of the inch image handling
libraries at Google right and one of the
reasons it took that long is because
some people depend exactly on which the
sequence of bytes that happens to be
generated by the image library right so
imagine that the compression algorithm
changes gets improved generates a better
light generates a better image right
maybe one that looks like this notice
the page the slide number change instead
of that we did actually encode two
different versions of this image yes
there's two different these are
different JPEGs right and the result is
that somebody's tête more than one
somebody's tests started failing when we
started when we changed the new
compression algorithm right
lots of goofus not lots of just goofus
goofus will pre compute the image that
he's interested in right store it
somewhere and then run his image
generation tests and compare it against
the pre computed image sometimes the
goofus does this by running his code
twice
I am NOT a fan yes the result is that
the precomputed image does not match the
one that is now being generated by a
more efficient the compression now you
have a smaller file and you're
complaining that your test now fails
right just one example of how brittle
tests updating stuff far away from the
actual tests right causes failures in
tests so what he really wanted to do was
to take some JPEG of rick astley clearly
and convert it to a bitmap and store
that as the golden output and then write
his test as i'd get some JPEG of rick
astley
i convert it to a bitmap and then i
Delta between those and so long as each
pixel is within epsilon right then I
probably got the right encoding at least
up until the point that we have enough
AI to recognize you know just assert
this is a picture of Rick Astley next
year that'll be cool but the notion of
run your code twice wants to store this
is the output that I expect to always be
there for all time and then do the same
thing every time you run the test that
should be a serious red flag right I
know I've done it it's not the right
thing to be doing all right
if you can't compute from first
principles what the correct answer
should have been your whole system is a
little complicated right but that's okay
it happens what you probably want to do
is express some boundaries of what do
you think is tolerable given what you do
know about the system and if you can't
even do that if all you can do is I
wrote the thing I ran it I got an output
I think it's right I'm going to declare
that it's right you probably want to
leave a comment in there explaining the
journey of self-discovery that led you
to this
poor life choice so that when something
changes in a couple weeks or a couple
months and someone has to go look at
what's going wrong you've at least left
them a hint because it may be you that
no I don't promise that the correct
answer is 5.73 Rick Astley's right you
say like my best estimate is this if
this changes you try recomputing here or
look up read this paper that sort of
thing right just running it twice not
the best plan so ordering right this is
fairly simplistic example but you don't
say yes I do say so the the second test
obviously depends on the first test
right so the first test touches changes
state in a global variable right in the
second test depends upon the fact that
that state has changed right this is all
well and good as long as your test
infrastructure runs tests in a
predictable way and as long as no one
ever breathes on it again right as soon
as somebody walks briskly past your desk
and the you know air Eddie's touch your
keyboard this test starts failing right
because you started changing the way
that the global state is mutated right
and while this may be a little bit
simplistic think about things like
changing the system clock or touching
files in the file system or reaching out
and touching state in a production
server or I don't know I could go on
like there are tons of ways in which you
can modify global state in your tests
and having tests depend upon said
modified global state is usually a bad
idea your test is both brittle and you
are failing to convey all of the details
of the test
you're definitely failing at readability
to some extent because you have hidden
dependency on oh yeah the other part of
this test must have run it run first we
see this really most commonly not with
Global int but with data written to the
file system by a test we run most of our
tests in tempo fests so they have access
to in memory file systems that are fresh
each time you run the test that's good
except each individual test within a
binary doesn't get a fresh copy so
there's been instances for sure of tests
that write out file state and depend
upon it in funny ways then there's of
course everyone knows your test should
be hermetic right don't call out to
production services try to avoid
actually doing file IO to disks try to
avoid touching the network in any way
the more that you actually have ones and
zeros escaping from your process the
less likely it is that your test is
going to survive the future this test
you may have written and this test will
work until you hire pointing fingers
this is goof is through goofus wrote
this and that worked fine until we hired
a second person right you can only
expect that storing data to a production
service works if only one copy of that
test is ever running at a time right so
once you add continuous integration into
the mix this is not a good way this
starts mailing and then there's deep
dependence and mocks this is kind of a
personal thing for me
so mocks are kind of amazing and every
test fixture every text framework that
we've seen has some notion of how to do
this well that talks on unit testing
yesterday we're talking about mocks man
and I'll and alike so that's fine except
for it can go horribly horribly wrong so
long long ago class file had a stat and
it had an output parameter for OK there
here's the result at some point we had
some storage system I don't know what
that required us to sometimes pass in
options for this and as sensible know as
as unwise but pragmatic and quick
software engineers what we did was we
just implemented a new stat with options
and we had a default behavior in the
parent class of ignore the options and
call stabbed okay you can imagine having
done this you've probably done this now
the problem becomes that someone in a
leaf project will write a test that is
using a mock file and will expect that
because of a call to stat with options
there is a call to the single construct
a single parameter stat right so some
far-off project has this test added to
it this means this test will fail if the
file they are using be it because of
derived behavior from the parent class
or just whatever file type they happened
to pick up at any point implements stat
with options directly alright that is
this test forbids the the implementers
of the file that they use from ever
actually doing the thing they wanted
great you see that don't do that all
right
they don't actually care that the zero
can or the one can one parameter stat
was called they're just saying most of
the time if it was called I'd like
something to happen right they've
totally failed at white box versus black
box testing they are peering deep into
the innards here and then asserting
behavioral requirements on the
implementations that they're using right
this is a maintainer Xin nightmare
yes anything more you want to say about
that don't do it so the observation here
is that if you have enough users you're
going to start seeing this we actually
have coined she tried the name there's
something else around Google now we have
a thing that we're calling Hiram's law
which he originally tried to call the
law of implicit interfaces there you go
which is essentially given an implicit
implementation right you have an
explicit interface it's a published
interface people should code to that
interface but sooner or later they start
depending upon the implicit guarantees
of the implementation whether it be
speed or memory consumption or file name
and line number yeah boy finally man
line number or you know API call
ordering whatever it is right people
will start depending upon this implicit
interface so with a sufficient given
enough users pretty soon you will at
some point have the enough people using
your using your system that the
implementation that becomes the implicit
interface there is nothing that you can
change without breaking somebody
somebody will scream if you change any
part of the implementation right this is
a bad world to be in unless you've gone
deaf to the screen you can guess where I
am these days gopher
screams awfully loud for one guy anyway
we'll recap so please write tests please
please please write tests even as bad
test is on average better than no test
at all it's possible to write negative
value tests it's possible we that's a
whole nother talk yeah right
right correct tests right right tests
the test what you wanted to test right
readable tests tests that are correct by
inspection to your reader which may be
you in the future
write complete tests test all your edge
cases and only your's not everyone
else's so use your tests to show how to
use the API that
make sure your tests are demonstrative
and write please write resilient tests
they should be hermetic they should not
depend on anything else and they should
only break and there is an unacceptable
behavioral change in something that you
rely upon right and that is unacceptable
behavioral change not the file name and
line number changed okay please don't be
goofus
so we'll take questions do we have a
micro should I repeat I'll repeat go
ahead so obviously this depends from
organization to organization and I
totally understand so the question is
right who writes the tests of the API is
it the implementer or somebody else
right and this clearly depends on your
organization I totally understand the
organization's that have split that
responsibility into someone builds the
test and someone builds the actual
implementation for us it's always it's
almost always the developer in question
right it is part of the initial change
like here is my change and here we have
the tests that go with it obviously
there is some variance and how
religiously that is adhered to but we
generally as an organization do a really
good job of it holding the line oh no
you can't submit that without some
change or without some test that tends
to work pretty well but there's a big
difference between yes there is a test
with this and there is necessarily a
good and well thought-out test so we we
definitely crossed the hurdle of yes we
have a culture of tests tests are
important we believe that that that was
a great thing to have fixed like a
decade ago for us we're still working on
making it Universal the understanding of
what it's a good test so there is room
for advancement there in the back
right so the question there was if I can
paraphrase what how do you approach it
when you have a large class with a lot
of internal state and therefore it's
hard to have a you know nice concise
readable understandable test to do that
without exposing some of that internal
state I get that reason well you'll note
that we didn't call these rules we did
call these goals of good tests and I
don't know that you can necessarily
always get all of them and it is
certainly the case that it is more
important to have some testing than to
say as too hard I throw up my hands and
go away
that said it does kind of get to the
question of if you can't write a test
for it was that the right design and it
is probably the case like I believe in
tests first because that yields more
testable ap is more than for any other
reason like there's a lot of sunshine
and apple pie ponies and unicorns
reasons to believe in test-driven
development but the thing that I think
really shines the thing that matters
most is if you wrote the tests before
you wrote that interface you wouldn't
have wrote written that interface if it
is too late and many of the things that
our team has inherited you're far too
late then it is critical to just get
anything in place right get some tests
down to make sure that you can make
changes without breaking things
because you cannot maintain a code base
you cannot make changes to things
without having tests in place right it
is ludicrous
to try to maintain a code base of any
significance without having tests you
know regularly spaced throughout the
entire space does that get to it all
right over here please
flaky so there's two things so the
question is with the test that had an
async operation right you still want to
be able to test asynchronous operations
like how do you do that without making
it flaky alright and there's two things
that that test was really trying to get
at one was this asynchronous operation
can happen and generates correct results
hopefully maybe and two is there may be
a timeout on it all right
and you can address those two things
either in the same test or separately
but you shouldn't be addressing them at
the same moment right what you probably
want is some sort of actual
synchronization mechanism some callback
from that interface to say okay I'm done
all right
you could have a callback with a timeout
on it right and it's the timeout you
know goes off then the test fails right
you do also need to make it very clear
that those timeouts need to be pretty
large and only run in your kind of
release profile right we do a lot of
stuff with the sanitizers and when the
when we were the first wave of like get
the sanitizers turned on everywhere was
happening like the sanitizers have low
but non zero runtime overhead and tests
all over the place failed because they
had timeout requirements in them right
like you have to be done with this
operation in this amount of time we also
see that with
like there are tests that will fail
predictably at certain times of day
right
so most of Google engineering is based
in Mountain View and the America Los
Angeles time zone right and that means
that the build cluster is most heavily
used when those engineers are awake so
between say noon and 8:00 p.m. so so the
build cluster is most heavily used
during like that kind of window and day
and when that happens tests run a little
bit slower because the cluster is a
little bit overwhelmed and if you have a
test that has a timeout that's kind of
right on that boundary then during that
time of day those tests will fail and
when you run them at other times of day
or over the weekend or you know what
have you those tests will succeed and
off the top of my head I could name
probably about a dozen subdirectories
that I know will fail if I ran them
right now right and it wouldn't fail if
I ran them in four hours
this is particularly challenging because
these stuff that we use for flaky
accounting and bookkeeping depends on
and when we have spare cycles will go
rerun those and you specifically have
spare cycles when it's not that yes so
you never run into the timeout issue
when you have first spare cycles and so
they just get confusing so does that get
here yeah mostly
yep right and in my opinion so that the
retort there was when we're rerunning
these things than timeouts or we just
have to keep increasing the time on
increasing the timeout in order for it
to still pass reliably um when you're
test if the intent of the test is to
ensure that the thing runs quickly then
you only run that part once in a great
while in release mode if the intent of
the test is not to enforce that this is
a fast operation then yeah just set a
ridiculously long but not insane time
out so that you know you don't get the
flakiness because a test that you can't
trust is not providing you a lot of
benefit all right as soon as you become
inured to that test flaking from time to
time it's invisible it becomes invisible
to you thank you
you work out if you work you work out of
the Pittsburgh office so the question
there was how do you deal both
technologically and socially with the
idea that there are flaky tests that are
just kind of present ever-present so
part of so that's actually a big
challenge on the work that we're doing
because we're working at the bottom of
the stack and our our continuous
integration tools allow us to run all
the tests that transitively depend on
the stuff that we're working on so it's
very easy for me to commit a change to a
comment that technically read triggers
quarter-million tests yes or more and so
yeah so there's a couple ways of dealing
with that the obvious one that actually
works relatively well is to ignore them
so now we're getting into the
confessional mode of the talk so to
ignore flaky tests that's ideal right
it's not ideal but at the same time like
everyone's gonna say like I know what
I'm doing I'm changing my thing but I
know what I'm doing I'm doing a safe
thing I don't even need to run the test
but you should run the test we are
changing things at the very bottom of
the stack right if we change something
in a way that's unacceptable
it is unlikely that one test of two
hundred thousand is going to break right
it's more likely that the whole world
will break right and so it actually
becomes easier to determine whether our
particular change caused that test
failure because it's flaky or whether it
actually caused a bona fide test earlier
yeah right the social aspect of this is
something that I'm trying to work on
within Google so that some degree is
actually to push the pain of the flakes
back on to the people that are writing
the tests right and there's different
ways of doing that and some of that is
baseball
thanks goofus there's different ways of
doing that but some of that involves you
know filing bugs against those teams
being vocal to them about you know your
tests are failing and there's wasted up
making it socially unacceptable to have
flaky tests it's actually surprising the
number of people that actually like an
empty bug queue matters as much as a
green build right and so if you file a
bug against them about flaky tests like
they'll actually that's actually an
actual thing that they'll work on
there's also we also you know the point
was how do you ever get anything
committed we have scripts and bots of
the run stuff overnight when the load is
less and less likely to flake or we can
run our big set of quarter million tests
and then only rerun the set that failed
and if you do that iteratively then
eventually you get down to like you
actually never get down to a zero like
I've never done that and gotten down to
zero test failures on a non-trivial set
of tests but it's because people keep
adding new tests but you can actually
get to a point in which you can either
verify by inspection or these other
tools to determine that those are deep
flaky and sometimes you just think about
it because you know really when you're
like changing it how command line flag
parsing works in C++ when some random
Java file system test fails you like
unless your every once in a while
that'll get us a but like unless you're
testing the way that swig interacts oh
yeah no its way to file because then the
Java sailors actually matter right
because you've changed the interface
between the C++ oh the other thing you
can do is actually give it just left me
I'm sorry that answer your question
what are we used to stop goofus from
checking in new flaky and brittle tests
so mostly code review mostly code review
and in order to get the code reviewers
to do the right things there's a lot of
Education so like we our team has this
weird roll of both we maintain all of
the kind of lowest level interfaces is
very technical like you know we can make
a couple typos change that breaks the
world but we also operate a very high
level doing essays and classes and
training and outreach and education
things to try to you know affect change
over the whole population because as it
turns out when you have a population of
roughly 5,000 and C++ engineers you have
to rely on kind of network effects right
you teach all the new people or you find
all of the respected code reviewers and
teach them in particular and those sorts
of hands-on things are are what we find
helps most so this actually illustrates
an important point that if we can't
automate the problem away automation
that we didn't point out is tests are
just a part of the solution right this
is really a defense-in-depth situation
right so everything from code review to
proper testing to stack analysis to
dynamic analysis stuff like the
sanitizers to hardening your code to
canary meaning to production monitoring
like all this stuff right is part of
having a robust software system and so
I'm talking about all of that on Thurs
yeah
the hope is that you if you if someone
commits a faulty test or if the tests
don't catch up a great the stuff down
the pipe eventually does right and that
certainly isn't you know it sounds like
a wing and a prayer and on some level it
is right just like you know this is how
you know airplane avionics work right
it's just you're fencing that
defense-in-depth right yeah other
questions
yeah wing and a prayer other questions
yes everything right so the question is
how do you acculturate testing how do
you you know start there to actually be
a culture of tests at some point you
have to convince the people in charge
like not the head engineers maybe the
head engineers right but like the VP's
the CEO whatever like whoever signs the
checks that this is actually harming us
and our ability to ship code correctly
is slowed down because we cannot tell
whether a change is correct right we at
some point you have to get that sort of
backing and then hold the line all right
no we will not accept your critical
change we will not accept your feature
we will not accept your anything if it
doesn't have Associated tests right and
you have to be strong and you have to
hold the line this is tough for a couple
weeks and then it gets much easier we
have had no I hear stories from all over
the organization I hear stories from
little branch projects and people that
spun off and came back and startups and
all sorts of stuff I have got zero
accounts of places where they're like
you know what we're in a rush we're just
not going to worry about tests and that
working right
that does not work right in every
instance when you are falling behind the
correct answer is to hold the line right
there will be tests and that will help
you or velocity come back up it is not
time lost it is time spent there it is
time saved from the future and you have
to if they don't believe it then you
have to start documenting like okay
Bob made this change two weeks later bug
was found right we've almost ten million
dollars because it couldn't release all
right and you come back three months
later with a rogue's gallery of this
would have been caught with unit tests
this would have been caught with unit
tests this would have been caught with
an integration test right and you just
have to hold the line and it's tough and
it requires some bravery and it is the
thing that matters most if you want your
organization to survive if you're at a
little startup and your expected
lifespan is two years survival is not
actually the important part okay
you mean like your codebase can your
codebase can be a complete flaming pile
so long as you get funding alright so
you may want to take that into
consideration but you do have to survive
long enough to get funding so I would
still suggest you write tests but you
can make that choice or something so I
think we have time for one more so so
every different teams at Google enforce
those different ways our team so the
question is where do you draw the line
right if you just make a comment only
test or commonly change like does that
need to be tested and so every team at
Google enforces this different ways our
team actually says every log message has
to have a tested field right in which
you describe the tests you ran right
Ivan if it's a you commit every can
every commit message yeah and if you if
it's just a comment only change then
it's okay to say like I built it to make
sure that I didn't have Asperger's
character somewhere and like the build
succeeded right but it'll still go
through continuous integration you'll
still go through the CI system on the
back end rate which is nice but the
alternative is like I just you know
re-implemented the entire way that you
know distributed file system works like
I built it right is not the right answer
to how to test that right and so part of
it just relying upon humans to do the
right thing and reviewers to review the
right thing right to enforce them
yeah it turns out anything that's a
functional change should be accompanied
with the functional tests anything
that's a cosmetic change yeah probably
skip it not such a big deal depending my
comments certainly reformatting whatever
right build it it's fine a good place to
start is you can't mark a bug fixed
until there is a test that would have
caught the bug right because you're not
sure you actually fixed it well you can
demonstrate that which is the reason to
write write the test before you
yeah which is the reason I write the
test before it anyway we are out of time
we will gladly stand around and chat
more about tests thank you all very much
for your afternoon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>