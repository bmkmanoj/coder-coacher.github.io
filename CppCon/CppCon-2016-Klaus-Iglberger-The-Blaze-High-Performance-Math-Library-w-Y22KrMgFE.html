<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Klaus Iglberger “The Blaze High Performance Math Library&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Klaus Iglberger “The Blaze High Performance Math Library&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Klaus Iglberger “The Blaze High Performance Math Library&quot;</b></h2><h5 class="post__date">2016-10-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w-Y22KrMgFE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Klaus Iggy Braga I'm working
at C Jericho now a Siemens business and
today I'm going to talk about the place
math library so I know it's 9 o'clock in
the morning and you have chosen a talk
that has math in the title and so I
assume that you have real interest in
this topic so I highly appreciate you
being here I assume that you all have
some familiarity with supplies for small
fly pre-sales you probably wouldn't be
here or you have interest in using such
a library so I'm a curious curious guy
who of you is using currently using a
C++ math library in his code okay this
is 80% perhaps a little more curious
even he's using boost you plus by any
chance okay the eigen library ok most of
you any chance the place library ok not
yet I have to just do a good job to to
convince you alright so the primary idea
the essential idea of these math
libraries is to give you some very very
convenient intuitive interface to write
your code and at the same time provide
you the maximum performance that you can
expect so for instance if you want just
to write the CG algorithm you should be
able to take a textbook copy the
algorithm from the textbook directly in
C++ code and it should be it it should
run as efficiently as possible at the
same time it is it's readable as
possible so if you know the CG algorithm
if you do not don't worry if you know it
you can read it it's very easy to
maintain this kind of the the best of
both worlds maintainability and
performance at the same time in place
just happens to be one of these math
libraries so place is an open source e+
post mask library under the PST 3 class
license it is a math library for dense
and spent sparse arithmetic so distance
and there's forest
vectors and matrices available it is a
header only template library so all you
have to do is include a header use the
stuff and the code that you need is
generated on the fly and it is based you
might have guessed on expression
templates to give you a little bit of
sales information place is offering you
high performance both through the
integration of glass kernels so you can
use a blast library also lay back la
pack library in the back and it will
just call into these libraries also it
is having a couple of manually tuned
kernels and it's alternating between
these two alternatives depending on on
the situation it is using vectorization
explicit vectorization both ISSC AVX
Avox 512 this is the AVX for Knights
corner knights landing so the new Xeon
Phi architectures use multiply add and
into SV ml it provides parallel
execution of or by open MP C++ 11
threads and also boost threats so you
can choose whatever you have available
the price hopefully a very intuitive and
easy API so similar to the main specific
language you just use it as you think it
should be used probably works probably
unified arithmetic conventions already
Darras things in sparse vectors and
matrices and you can combine them your
for sparse matrix and the dense matrix
you can multiply them no problem
an authorial tested so currently there's
over 6000 executables for testing this
is not number that is convincing but it
shows we put a lot of effort in testing
this library and we try to be very
portable and provide high quality C++
source code okay in 2012 the first
version of place was released so this
was about four years ago in August 2016
so just one month back we released the
current version 3.0 which is written C++
11 and 14 so meaning you need four
then he can use this version if you need
anything
c-plus us 98 specific and just used the
older the release one one before version
2.6 this is still providing C++ 98 okay
this might be the thought I'd hear in
your mind come on another math library
because you might know there's actually
quite a lot around so I mentioned the
boost library so boost you plus the
eigen library the MTL armadillo there's
a bunch of them why just another math
library please hold the thought I will
answer it hopefully however I would like
to put this question at the end all
right so give me some some time to
explain why first of all I would like to
explain a little bit about the
benchmarks because of course I'm going
to show you benchmarks and you should
know how the work and what exactly is
benchmarked here so I use an Intel
Haswell 10 cores e on with 2.3 gigahertz
as well because this architecture
supports few small to play ad this is a
very important yeah quality of the
processor turbo boost is disabled which
means I stick to 2.3 gigahertz it will
not increase B to 3 point whatever and
this is also important to have reliable
determining performance results I used
both the gnu compiler in version 6.1 and
the clang 3.8 compiler to show you some
differences this is not a battle of
compilers I just want to highlight a
couple of very important points and so
even if one kampala seems to be better
this is not what I what I want to tell
you and this is how a compiled so just
showing this with cheapest plus I use
AVX explicitly if u s-- multiply add of
course no debugging and c++ 14 is
required this is all alright then this
is one example of such a benchmark
kernel so for instance a vector addition
of two vectors there is a loop that
repeats these benchmarks and every
benchmark is running for at least two
seconds this makes sure that
we everything is leveling out all jitter
of OS etc should average out samples so
I collect samples in each of several
runs so there's really several runs
they're important and I collect several
samples as you see here and also all
cash problems are hot the operation
whatever it is is computed once before I
start then if possible everything is
already in the cache and then I start
the benchmarking okay so the libraries
that I do benchmark is of course played
3.0 the release was in August
those 2016 then the eigen library that
most of you use I choose chose the beta
2 version for a very specific reason so
better always sounds a little
half-finished but only the beta version
supports a DX and FM a else I would only
have SSE this would be pretty unfair
comparison I have chosen the armadillo
library in version 6 a 7.3 released in
June 2016 so also the most current
version boost you pass the most current
version so I used boost in version 126 1
but the last update to u+ was in version
one point five seven so November 2014
since then nobody has added changed
anything here the good old plaits Plus
last library a couple of you might
remember this was the first expression
template based math library the last
change was in March 2014 so quite a
while back GMM plus plus in version 5.0
admittedly by now they have five point
one out but unfortunately was a little
late for the benchmarks so this was July
2015
MTL 4 in the latest version 4.0 point
nine five five five light has changed in
May 2014 and as a comparison this is not
an expression template based math
library of course but as a performance
comparison intel's mkl in version 14.0
also there's there's a new version 50
is already out but unfortunately didn't
have access to this shouldn't make much
of a difference though okay
now hopefully you know everything about
the benchmarks let's start very slow yes
it's still morning with blast level 1
meaning just vector operations and for
plus level 1 place provides a couple of
vector types the first one is static
vector static vector is a fixed size
vector so you give it a size at compile
time it will always be a for instance 3
dimensional vector it will never change
its size and so the size is also
available compile time can be used at
compile time for certain optimizations
the opposite is dynamic vector it uses
the dynamic memory as the name suggests
you don't give it a size at compile time
but of course at runtime you can resize
this vector it's very convenient for
light reactors something in between is
hybrid vector hybrid vector uses static
memory of at maximum end values but you
can still resize up to this size in this
is pretty convenient if you have some
means of to determine the maximum number
and it's hopefully a small number
because use new static memory but still
you don't know exactly the size so for
instance this might happen if you have
two or three dimensional arithmetic yes
a question ok that so the question was
what is the parameter bool TF this is
the transpose flag I know ok it's a
little cryptic you can have a row vector
or a column vector place makes a big
distinguish it's a distinguished pretty
much between column vectors and row
vectors for instance if you add two row
vectors result is a row vector you
cannot add a column and a row vector you
can multiply them through so if you
multiply two row vectors
it's a row vector if you multiply a row
vector and a column vector
it's an outer product if you multiply a
row vector and a column vector it's an
inner product there is also dot function
an outer function if you don't like this
particular syntax but
it's it's possible so if I want for
instance as I just showed you or explain
to you add two vectors then this is how
it looks like you create vectors to
dynamic column vectors I always use
double ABC all with the size end and I
just add them very convenient very easy
this is what you used if you use I can
too so I think that's exactly the same
thing alright now you're curious how
this performs well this is the
performance graph for Clank 3.8 what you
have to look out for if we want to see
the place results is the red curve so
you very nicely see that the different
cache sizes so the performance goes up
for so this please note that this this
is logarithmic scale it goes up in while
it's still in level one then as a
performance drop and this is level two
size then a performance drop again
level three and last but not least four
very large rectus you have to work from
main memory the first thing that is
obvious okay for small vector says a
high jitter but this is kind of do you
expected it's very small vectors place
is not running bad in level one so for
some time it tops all the other
libraries there is not a big difference
here in level two but very obviously in
level three cache and from a memory
place is about 20 to 25 percent faster
than all the other libraries I'll
explain why so one other thing that is
strange I don't know why the MTL does
not pretty well work with the client
compiler there's something going on this
is not what it usually performs so this
must be a problem of the combination of
empty MTL and in the client compiler if
you use GCC instead this problem is gone
and you see that the general picture is
pretty similar but play still in level
one dominates the performance
I'm pretty surprised actually that boost
you Plus has such a high performance for
very small vectors this is a little bit
surprising but ok
I didn't invest this further again in
level 3 and in my memory place is about
20 to 25 percent faster question this is
actually true but as I said I don't want
to compare compilers so the question was
I'm doing better with clang 3.8 then
with jesusí this is true the maximum
performance is a little better but as I
said I don't want to compare compilers
the important point here is whatever
compiler you use plays in level 1 will
be the fastest library all right how
many cores to be used at this point this
is an easy question 1 cor how many cores
do we have available perhaps you
remember with 10 cores available this
seems to be pretty wasteful nine-course
are idling well one core is computing a
vector addition okay let's just turn on
parallelization because today you will
not get a one core CPU anymore
everything is it's multi-core even my
phone has several course so it's
reasonable to assume that by using
something like OpenMP performance will
improve I will not use all ten threats
of course though but for the decision
was made because I believe you do not
have 10 cores available at home there's
a pretty expensive CPU but four should
be available everywhere so I just used
for why not and I make sure that threats
are pinned this is actually something
pretty important so I make sure that the
first four course are used none of the
others all right this is what it looks
like that's just one lie prated actually
benefits from paralyzation in this place
however it does not benefit immediately
so for small medium-sized vectors
perilous
is actually counterproductive
performance would drop
why because vector addition is not a lot
to compute it's just very fast and the
setup of threats the distribution of
work takes time too so there is a
certain additional overhead but once the
vectors are large enough parallelization
truly gives you benefit there's a
threshold where you can tune this invent
in place so that's the question I just
use so the question was do they use
OpenMP and the NPL MTL I'm sorry
I just use openmp in the in the command
line if this activates it in okay
alright then okay so the MTL is has an
actor of extra flag for it okay good you
know all right
however what you see here is perhaps a
little bit surprising this performance
is pretty low because we actually hit a
memory wall at this point we cannot go
beyond it this is why all I praised have
exactly the same performance
it's not about computation it's about
the memory but using several threads
actually gives you a performance benefit
and the performance benefit is because
you can on modern architectures only
achieve high performance with several
threads so you can only go beyond this
memory world with several threads and so
this is why you actually gain
performance for even for a vector
addition something as simple as that but
as I said only um from a certain point
on so for larger vectors alright now we
might be interested what's going on in
the behind the covers this is the kernel
that is computing so the question
oh I don't know so the question is how
many memory bans does the machine have
so 32 Hardware threads to load okay
memory okay so the answer would be for
memory channels all right all right so
this is the kernel that is computing the
vector addition what is happening here
so the first thing that place is doing
it is actually using padding padding
means I add a little a couple of extra
elements at the end of a vector this
doesn't make it the big difference or
any difference at all for large vectors
but for small vectors it does so as long
as I'm still in the range perhaps a
hundred elements you might see this no
remainder loop is necessary then the
kernel is based on iterators that gives
the compiler actually quite quite some
opportunity for the additional
optimizations so with index based access
I do not gain this performance then
Manoel loop unrolling yes Chandler would
not be proud Chandler has the opinion
that this is something that should be
done by the compiler yet you see that it
depends on the compiler some compilers
do this fine clang does apparently other
compilers you don't do it since I do it
manually I'm not depending on compiler
optimizations so much you see this
actually pays off this here is something
that is expression template specific
load something that is happening in the
right side this is basically the same D
vectorization this is happening under
the cover so you have to believe me that
this here execute simply operations in
the India quitting instruction set that
you've chosen
for small vectors everything's manual
Android for large vectors this doesn't
pay off anymore because we're memory
bound but what we can do instead is
instead of simple store operations we
can use none temporal stores
non-temporal stores mean I do not
directly write back to cache but I
circumvent the cache write directly back
to main memory which is pretty clever
here because I don't need these values
anymore
and by doing this I actually free up a
little bit of bandwidth and this extra
bandwidth that I gain actually makes up
for these 20 25 percent in performance
all right what you do not see here is
parallelization there's nothing here
that would indicate that this is
actually running in parallel no
threading no openmp pragma nothing let's
just have this as an interesting piece
of information right now I will go into
much more detail later yeah just
remember kernels are not parallel we do
not contain parallel information all
right now you might argue well but
Fortran is of course fast and so
something like the mkl will probably run
much much more efficiently so let's
choose something that the mkl provides a
daxpy operation just as a comparison so
Dax V is vector plus equal another
vector times a scalar this is something
that was directly available in MK and so
the orange graph now is Intel MKL
and interestingly for this particular
operation place is even quite a bit more
efficient than the MK l due to manual
loop unrolling vectorization etc but of
course Clank the Conklin compiler has
quite a bit of quite a bit of additional
part in this there is no streaming going
on here no non-temporal store so the
performance in the end is exactly the
same for our libraries you cannot use
streaming for this operation this would
be bad for performance because you still
need the stuff so the target is part of
the operation it's not just not just
written to
GCC very similar picture but in this
case the MPL and plays are approximately
the same of have approximately same
performance there's one more library
that actually has the same approximately
same performance this is GMM + + alright
parallel at some point again it is
worthwhile to use parallelization the
Intel MKL is of course parallel to and
so there's these two libraries that
participate in the parallelization place
a little faster but in the int the
performance is exactly the same but the
manual optimization is good yes question
so the question is this place you have a
decision when to make the
parallelization yes it does there's a
threat show that they can manually
configure which decides at which point
paralyzation is considered below this
threshold no paralyzation is used above
the threshold paralyzation will kick in
ok question so the question is do you
use any kind of thread pools for open
impede this is part of the open MP
system if you use the C++ 11 threads
instead yes then I use a thread pool you
do not have to create threads all over
again
the disadvantage of the thread pool
approach is that you have to take care
of pinning the threads yourself this is
not part of C++ 11 yet and also not part
of place yet unfortunately so as long as
you pin the threats to certain core you
should get approximately the same
performance
any other question ok the question is
how to align the memory so the memory is
aligned but just properly allocating the
memory meaning there's special functions
like memoizing Emmaline that you used to
get aligned memory this is essentially
what is happening here in this case it's
a dynamic
I get dynamic memory from from these
functions whatever is required in SSE
you have you need everything to be
sixteen by the lined in AV exit 32 by
the lined and every X percent 12 64 and
so on so I take care of aligning
properly alright so what level one check
works well blast level two this means
that we also have some matrices and
quite for symmetry reasons there's a
static matrix you pass it a number of
rows and a number of columns at compile
time and this will be the size of this
matrix until it's until it's in the
dynamic matrix doesn't take compile time
arguments but of course runtime
arguments you can resize the dynamic
matrix as you want the memory will be
preserved so there will be not be a
relocation if you resize as long as you
stay within the size you've already
acquired so it has a capacity hybrid
matrix again is something in between use
the static memory but you can resize it
up to the given size so the rows the
number of rows is limited by M the
number of columns is limited by n okay
the operation that I want to show is a
matrix vector multiplication I have for
instance a dynamic matrix column major
matrix a size n by n and a dynamic
vector of n elements so two of them a
and B and as you would expect I just
multiply the matrix and the vector
assign it to another vector very
convenient mr. performance that you get
so the orange line is Intel MKL the
purple line is the eigen library the
blue line is MTL with clanked 3.8 you
see for small in cache matrix vector
multiplications vise has a approximately
factor of two a performance advantage
it's a slight advantage in in level
three cache there's no real difference
here anymore when you are in in the main
memory but
place is always at least as good as DM
kale which is it is kind of the
comparison not too bad
the picture doesn't change much with the
canoe compiler again place is by far the
fastest library in cache it is obviously
faster in in second level third level
cache to again the performance is of all
libraries is the same from memory
because the operation after all is also
memory bound alright if you use
parallelization for that question
sorry so the question was is it dense or
sparse collimator it is a dense matrix I
use a dynamic matrix so it is the dense
matrix vector multiplication color major
matrix alright parallelization it takes
off as I said when a threshold is
crossed but as you see it is actually
working quite nicely for some time
actually you gain an additional benefit
due to the fact that if you paralyze
this give it to four threads then the
parts off that you give to certain
course are small again so you can again
benefit from this high performance that
you have four small vectors this carries
on for quite a while but again in the
end to the performance of Visio
libraries that do parallelization is the
same again there's one more library here
that the armadillo library this library
also calls the mko internally and so the
performance is virtually identical to
the mko okay why is it so efficient the
place library for small and large
matrices the reason is that we
distinguish between small and large
matrix vector multiplications small
means and below a certain threshold also
something that it can configure and I
don't care about memory optimization
cache optimization I know it is
- and so I can exploit a couple of extra
information this is why it runs pretty
efficiently as soon as I cross a certain
threshold I call a lodge Colonel this
can be if you want to the mkl it can
also be the places own Colonel so what
you've saw seeing the performance is
actually places on girl there's no not a
big difference between the MPL alright
let's move to the maximal level blast
level 3 which means we have only matrix
matrices and of course this is about the
matrix multiplication so if I have now I
have 3 matrices dynamic matrix double
column major three column H a matrices
all n by n I just multiply them this is
matrix matrix matrix multiplication
admittedly this graph is a little boring
the curse there's only two possible
alternatives that this is working either
the codes call the MK L which is quite
quite a reasonable thing to do or
alternatively they call their own curled
since only one library that uses own
coil this is the the eigen library this
one specialty here for small matrix
matrix multiplications again places lot
faster because we have specifically
tuned kernel for small matrix matrix
multiplications again we distinguish
between small and large you can
configure what small means as long as
we're in the small range the small
kernel is Houston this gives you a
performance benefit at some point I
simply switch from the place kernels to
DMK l you see then it's the performance
is identical the picture doesn't change
much if I use GCC instead of clang
simple because DM KL is call so the DMK
Lawless is always the same this is
pretty compiler independent it changes a
little bit for these manual kernels of
the place library alright using
parallelization this is what it looks
like at some point parallelization kicks
off in the case of place a little
earlier than in case of the MK l in the
end however again the performance is the
same but
what you see is you really get what you
would expect in this case four times
performance improvement in case of the
matrix multiplication is as possible
because the yeah it is really arithmetic
ly bound there's a question the question
is is blaze always calling the m'kay
occurrence from a certain point on yes
for large matrices yes place does not
have a kernel it can match the
performance of TM Cal no time no there's
no chance that we can ever match this
performance for small matrices however
we can actually surpass the performance
because we can throw so much more
information from so much more
performance from the information that is
in cash so we deal with this differently
so approximately so at this point here
we call the okay at this point here's
the parallelization starts but at this
point you definitely call into the MK oh
yeah I'm at the parallel version of the
MK oh all right
I told you we have also compressed
matrices meaning sparse matrices yeah
there's one currently one sparse matrix
type in place called compressed matrix
compressor the type you pass it whether
it's raw color major and you can use
this matrix type and combine it with any
other matrix type so for instance also
with dense matrices so let's for
instance see what happens if I have a
compressed matrix a and the dynamic
matrix B and C so I multiply sparse
matrix with a dense matrix as far as
times dense and assign the result to
dense matrix because the result is
expected to be dense this is what the
performance looks like there's
approximate so at some point a
performance benefit of about a factor of
two but overall place never is slow than
any other library so you can count on
the fact that it will be
let's play a little game shall we
I have four libraries here but you only
see three if there's five libraries here
but you only see four graphs where's the
fifth one okay you might have spotted
the green claw of the x-axis this is
actually boost u Plus u plus comes with
an amazing zero point zero mega flops
for this benchmark with so zero point
zero is something which is not what you
would expect so promise me if you ever
happen to come across as far as matrix
multiplication or spas dense matrix
multiplication of those two plus replace
it with something else
it is definitely not what you what you
like the question okay so it was a
comment that the MTL does not have an
intrinsic kernel for sparse times dense
matrix it converts the sparse matrix to
dense matrix great so this is why at
some point performance is simply what
was too slow so I stopped it the same of
course happened with boost applause so
the question was do I know one
application of this and one one proposed
application is cholesky right yeah okay
but okay I don't know either
all right what you can do now just this
experiment is okay I forgot threats if
you use threats this is an operation
that's also paralyzed kind of everything
in place is running parallel and of
course you get a significant performance
boost from that again it only kicks off
after certain threshold but after that
you can count on the fact that you get
approximately a factor of four
performance improvement
okay that's it let's do an experiment I
have a complex matrix which is
column-major now okay questions sorry
okay the question is does the compressed
matrix have the standard representation
or is this something special something
homebrew it is something in between it
is not a classical CRS or CCS format it
is something that allows you to fill it
very efficiently so it allows gaps this
is also why you are not allowed to
traverse all elements but only row
column wise depending on whether it's a
row a column major matrix so it's kind
of special there is plans for now to
also add the traditional formats the
best thing you probably can do is first
set up with a compressed matrix and then
converge to the traditional formats yes
question they have sparse matrices some
combinations or do not work I don't
remember exactly why this this
particular benchmark does not does not
apply all right so now I have a column
major compressed matrix and a row major
dense matrix this is kind of the worst
case that I have column major sparse and
row major dents on the right okay so how
do the libraries perform this particular
case you see all I press that provide
this half problems perhaps because there
is a conversion to a too dense matrix
and also the performs pretty similar
perhaps the approaches is equal place
however performs much much better
surprisingly surprisingly good
what's reason behind this okay and also
of course this is running parallel to
and again you gain approximately a
factor of four in performance which
gives you a huge
performance benefit what is happening
here so this is the kernel for this
particular operation there's not much
there's a simple trick if you big again
beyond a certain threshold yeah then I
realized that it would be very bad for
performance that to compute it just like
it is so I create a temporary I create a
temporary for the sparse matrix I turn
it from a column major to row major then
I have a row major sparse times a row
major column row major sparse matrix
times a row major D'Oench dense matrix
and this actually works pretty well
this is why the performance does not
drop so so rapidly all right this is the
classical operations platter one two and
three
you got an impression okay this is
working well however one of the benefits
of these libraries is that you can
actually create complex expressions
complex expressions mean you can
concatenate things now I've chosen one
example that is yeah pretty interesting
I have a dynamic matrix a and a dynamic
matrix B column-major both of them and I
have three vectors a B and C I combined
him in this fashion
a times B times a plus B all right
usually this is evaluated in following
order left to right matrix times matrix
the result is a matrix again this matrix
is then used to multiply its multiply it
with the vector addition this is a
matrix vector multiplication however
much much more beneficial for
performance would be if a would evaluate
it from right to left first evaluate the
vector addition create a temporary
whatever is necessary then compute a
matrix vector multiplication with B the
result is a vector and so then I can
compute a second matrix vector
multiplication a times this temporary
vector yes of course this is much more
efficient simple
I have reduced the order of my
computation from order 3 to order 2 okay
second I told you about these
temporaries how do these libraries
actually evaluate this expression do
they really create temporaries do they
allow this expression at all in case of
place you just write it like this
and can hope for the best other
libraries approach this problem a little
differently so for instance boost you
plus does not allow you to multiply a
matrix product with anything else so
what you have to do simply because it
doesn't compile is you have to create a
manual temporary so the first thing you
do is you compute B times a plus B and
the temporary result is then multiplied
with 8 all right Jim plus plus does also
not allow this combination in this case
you even have to explicitly add the two
vectors first into a second temporary in
order to be able to compile it so you
add a and B you mult B with this
temporary vector and then the resulting
temporary vector is used and multiplied
with a a similar approach is taken by
the MTL yeah which is good which is
absolutely reasonable Armadillo allows
you to write this expression directly a
times B times a plus B yes that's again
that's a little different so you should
specify the no alias here but else it
looks pretty much alike alright this is
what the performance is of these
libraries again placed due to the strong
matrix vector multiplication performs
pretty well in the end most libraries
come up with a approximately similar
result what is not working so well again
is boost you plus int pretty
surprisingly the eigen library eigen
does actually not recognize the fact
that you should evaluate it in different
order yet the syntax allows it to write
this expression which is dangerous this
is something that actually shouldn't
work if it is not available in the right
form because probably there's a lot of
people out there that would write
exactly this expression and would be
surprised by the fact
it's not running efficiently at all
hopefully that benchmark first and
realized it's running it's running
pretty slow of course we can repair this
defect pretty easily we can just create
the according temporaries yeah we can
compute the vector addition first first
matrix vector multiplication and the
second matrix vector multiplication okay
eight times tempi then of course the
performance is what it what it's
supposed to be and an eigen has again
the same performance in the in case of
memory main memory computation for large
matrices alright
this can be paralyzed to again the place
library has a pretty significant
performance advantage for sometime in
the end however it's it's approximately
the same performance oh it's a little
worse than d mkl so both the other
libraries call the mkl it performs a
little better admittedly so what is
happening behind the covers in the place
library wide can you write this
expression because place actually from
the very beginning implemented
restructuring expressions so usually
expression templates that just built and
then you evaluate the tree place it's
actually able to restructure the tree so
to really express ins to whatever is
necessary this is the kernel to turn a
matrix matrix multiplication into
something else if you have a matrix
matrix multiplication so this dense
matrix happens to be some form of matrix
multiplication and this is multiplied
with a victor dense vector then what is
done here is I just set parentheses
explicitly please evaluate the right
part first and then multiply with the
left hand side matrix this is only
possible if you have built the
expression template library such that
you can actually rearrange this this is
not always possible alright
performance is great now you might be
interested in a couple of more features
what is available so these views also we
have a couple of use sup vector sub
matrix row and column sub vector of
course being something that you can
apply to any kind
Victor sub-matrix row and column being
something that you applied to matrices
so for instance what you can do is you
can assign a sub vector of some vector
with its dense or sparse to a row of
matrix a so assign to the second row of
a the sub vector from X or a subject of
X starting at index five with the length
length of eight pretty convenient you
can also assign for instance to the row
of a sub matrix a certain certain
elements as you can be pretty specific
please assigned to these elements these
these values we're alternatively you can
of course well of course it's possible
you can assign a subject of why the a
sub vector of a matrix vector
multiplication for instance well this
sounds seem to be a feature that is not
particularly important yet this is
believe me or not the basis for the
parallelization in place place you can
apply sub vectors sub matrices all kinds
of use to any kind of expression this
actually enables quite quite an
interesting effect so for instance this
particular expression would mean please
assign to the subject of the left-hand
side vector from index two with the size
of four something in the right which is
exactly this block this is if you take a
close look at it exactly the same as
assigning a sub matrix of a times the
vector X if you have such a mechanism in
play then actually you can just sub
vector sub matrix anything so you get me
a vector I can create a sub vector from
this to give me a mini tricks I can
create any kind of sub matrix on this
and this is what has happened this is
how the parallelization is built this is
the parallelization kernel so now you
see actually a pragma and an open p
pragma okay forget about all the other
stuff the important part is down here
this line here please create a target
this is an aligned sub vector of the
left hand side
rent and please create a please assign
to this ass effector off the right hand
side expression whatever this is can be
anything I don't care so what all I have
to do in order to paralyze all kernels
is to write this kind of wrapper apply
stop Victor to wood or sub matrix for
matrices to whatever right hand side
expression I have it will be evaluated
accordingly then and everything can run
in parallel so in place not the kernel
as a parallel the architectures made
parallel aware yeah so there's just
three or four files that or we have an
opening pragma whereas we have about 200
curls all right performance great you've
seen that with fuse we achieve the
parallelization now allow me to cover a
couple of flexibility features that
might come in handy because very often
users of these libraries have special
circumstances and I cannot use it
because of this special treatment of my
code so this special property so let me
start with custom vectors and matrices
there's two classes to dense vector a
dense vector and an additional dense
matrix that are called custom vector and
custom Atrix these two classes are
supposed to be used whenever you have
something special going on and you can
not use the native data structures of
the blight library so let me give you an
example you have some kind of special
memory something that cannot be
allocated by by the library itself
something that is well special to you
wherever it comes from still you want to
be able to use the kernels of the
library what do you do you just create a
custom vector and pass it this kind of
special memory you give it of course
also the information how many elements
are making the vector and additionally
you have to pass a couple of additional
information so is it aligned or not if
it's potentially unaligned you just say
so it will be treated accordingly
is it padded or not meaning can I use
some additional elements or not in this
case I say unpair it then of course it
might be necessary to run a remainder
loop okay if you have some other special
memory you can also okay let's say if
you have some alight memory then we can
for instance create a custom matrix we
say okay this is a line memory I know
for sure it is aligned I also know that
there's a couple of elements that you
can use for extra padding yeah you have
to you pass this memory to the matrix
for instance give it the number of rows
the number of columns how many elements
can be used for padding additionally you
can also pass it at the litre please
take care of the memory after you're
done no this might be important for
various purposes it is a custom deleted
you can pass anything anything that just
cleans up this memory and this custom
vector when it goes out of scope will
not touch the memory this custom matrix
when it goes out of scope will properly
dispose the memory custom operations
very often people might not find the
operation that they need in the library
so you might need a square root function
okay this is available but just as an
example it might not for whatever
purpose anybody can do is you can
actually run a for each this gives you
the ability to write custom element wise
of component wise operations a is some
dense or sparse matrix and the second
argument is for instance a lambda
please compute for every element of a
the square root of the element assign
this to be this operation is not vector
s but it is at least running in parallel
this is good
what if you want to vectorize this as
well well it's just a little more
complicated instead of passing a lambda
which is just a 1 when function yeah we
have to create a functor
we just do exactly the same as Salaam
Dada's forest so a little more
boilerplate code we write a function
call operator which returns a square
root we can all add a function called
load this load function is triggered
whenever I can
actually vectorize this expression so it
has passed a simply type so simply
double type is this is one of the place
intrinsic Sindhi types for my guest
double precision elements you are passed
a double element you can pass this for
instance to the AVX square root function
and it is returning an according Cindy
data type now it's running vectorized to
now of course this is only working for a
DX so if a BX is not available then this
function is so this intrinsic functions
not available and that's so nice
and also this function is always parsed
so as soon as a VX is not available
there's a compilation error this is the
problem might be a problem for this
purpose you can additionally add a Cindy
enabled function the context per
function so it must be a context of
function returns a bool and within this
function you return whether or not Cindy
should be enabled or not so it can be
anything in this case I just need the
information is AVX available then return
true else return false this function
here should be a template as soon as
it's a template there's no compilation
error anymore if it is a V X because
that will be parsed yes but the
availability of the of the intrinsic is
not does not matter anymore so as soon
as AV X is enabled and this function
returns true the load function will be
used if you have double elements of
course and suddenly this operation this
for each is running in parallel and
vectorized so you have the ability to
create custom operations that are
vectorized and paralyzed okay
whilst not at least I would like to
point out that is also a feature to
customize the error reporting mechanism
wise is supposed to be potentially used
on very large machines
parallel machines so machines with a
thousand course thousand CPUs even in
such highly parallel environments
throwing exceptions might not be a
particularly good idea yeah yeah
unfortunately so this the primary error
reporting mechanism of the place library
is to throw an exception if you add two
vectors with different sizes an
exception throw because the Desai's does
not fit for instance if you want to
change this behavior in order to enable
is this for very large machines and all
you have to do is to actually change
this little macro there's a macro called
place throw it has passed an exception
and usually just throws the exception if
you want it to behave differently you
just define this macro before you
include any place header for instance
lock some information and then
gracefully abort with an MPI abort for
instance a question so the question is
if place conditionally is no except for
for this kind of customization no it's
not
yep all right so remember the question
from the beginning come on another math
library well ok of course you can
contribute to another math library bet
you've seen a particular
quite a number of advantages of this new
library because I believe there is still
room of improvement first place really
fulfills HPC requirements so you can to
some extent at least put your own Cal to
rest which is not quite true if you have
dense matrix multiplications then you
should still have the Mk around because
place calls these currents but you don't
have to call them directly anymore you
can wrap these calls with the place
library place it's less dependent on
compile optimization than probably all
other libraries
now if you have you saw this in the
beginning with the vector addition if
you just have to see you know GCC
compiler you still get close to the
maximum out of it
also place is probably the only library
that prides full support for multi-core
CPUs it doesn't matter what you try to
compute you can run this in parallel
other libraries only provide this for
mainly matrix matrix multiplication
yes true this benefits most has the most
benefit but all operations somehow
benefit from it eventually and also
place is smart enough for the average
user other libraries can be misused to
some extent you might run into trouble
because there's no optimization of
certain expressions wise tries to be
truly smart it tries to restructure
expressions to an extent that you might
not have realized that it's actually
better to write it in a different way so
I believe places participation in the
race benefits everyone it's a good thing
that we have an additional library just
because of this presentation if the
eigen guys view it then on YouTube they
will realize oh there's a bug in a
library it'll fix it so even then you
might get a benefit just because places
around right and with this I'm happy to
take a couple of more questions so
please so the question is do I have
sparse decomposition no not yet question
so the question is today at any pint
point compare this to MATLAB no I did
not okay so the comment was that for
small matrices the performance is like
the MPL performance for for the large
sorry okay question so the question is
do we have armed support there will be
armored support yes we don't have it
right now unfortunately
okay the question is if I use such a
custom matrix that is say an easy way to
find the proper alignment there is the
according typed rates in place that he
can use to so if you for instance use
double use a line oft or not the
standard align off but alignment of the
standard one but the place one it will
return the proper alignment for AVX
whatever you choose this is probably the
easiest way question so the comment is
that the criticism constructive
criticism is I did not show in a
performance time left for CG this is
true yeah
okay so if you need a couple of more
performance results from so for instance
row major matrix times vector now this
is what okay then just please take a
look at the website that if you go to
the website to the very first sighted
user there is a link to benchmarks if
you click on this you will also get row
major matrix times vector results you
will actually see that in this case
place performs by a factor of two better
than in for in cash then other late
libraries this is about the same yes the
question so the question is is place
also faster to compile first you know so
there is of course a trade off so it
does a lot more things with the
expressions and other libraries and this
of course has a penalty in terms of
compile time it is probably slower than
the competitors however it also provides
more features in this regard right the
session is over officially so I would
I'm glad to take any questions
afterwards please just come to me all
right thank you for being here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>