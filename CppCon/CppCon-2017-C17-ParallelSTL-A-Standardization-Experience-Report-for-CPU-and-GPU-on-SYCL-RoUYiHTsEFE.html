<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: “C++17 ParallelSTL: A Standardization Experience Report for CPU and GPU on SYCL” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: “C++17 ParallelSTL: A Standardization Experience Report for CPU and GPU on SYCL” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: “C++17 ParallelSTL: A Standardization Experience Report for CPU and GPU on SYCL”</b></h2><h5 class="post__date">2017-11-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RoUYiHTsEFE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- My name is Michael Wong.
Among other things I'm
the concurrency technical
specification editor as well
as chair of SG14 and SG5.
This is not just any
talk about ParallelSTL,
there has been a number of
them, and they're good ones,
teaching you how it is possible
to run the standard template
library algorithms in parallel.
We're gonna show you how
it's done not just on the CPU
as it's specified in the
standard specification
in C++17 right now, but show
you a vision of the future
where it can be done on GPUs.
And indeed, we've done it.
We've actually implemented it
so it can be executed on the GPUs itself.
This is where the power
of this specification
really comes in.
We're gonna show you a
demo of how that works
and also give you some directions
about where the future direction is.
A lot of these talks
are gonna be beneficial
because we gained experience and benefits
from a lot of people's help,
and this is no different.
We share committees on
C++ as well as Khronos.
Khronos codes some of the specifications
that are dealing specifically
with heterogeneous computing,
and indeed, both committees
are gonna benefit
from sharing knowledge in both directions.
But there are gonna be necessary things
and mistakes are remaining,
and I claim all of them for myself.
I'd like to claim all the
good stuff for myself,
but in honesty I can't
allow myself to do that.
The usual legal disclaimers,
I'll go over that.
Our company is Codeplay.
We work on heterogeneous computing tools.
This is why we've gained
so much experience
and we've been participating
in both standards
for quite some times.
We have products that generally allow you
to distribute to any OpenCL devices
but they're using very modern C++.
In this talk we're gonna give you a bit
of the history and background
of STL, or ParallelSTL,
because we have been
there since the beginning
being involved in driving
some of the directions.
It actually started with
a question back in--
I forget which meeting I was at,
but it was just after C++11,
and I raised the question,
&quot;When can we get the C++ standard library
&quot;to be working in parallel?&quot;
And from that point on it galvanized.
I'm not saying I'm the only one.
There were a number of
other people as well too.
It galvanized a number of collaboration.
Because we've been following it so closely
because it affects us so
much, especially at Codeplay,
we have been looking at--
This is actually an excellent
example of what it takes,
what it takes for a
technical specification
to reach C++ standard ratification.
You see all the changes.
Now up until now people have been saying,
well technical specification,
on the one hand,
some people might be
saying it's 99% there.
On the other hand some people are saying,
well it's just implemented and
there are a lot of changes.
And by the time you get to a standard
it might be very different.
The reality actually as you will see
is somewhere in between.
We're gonna show you how ParallelSTL
works on CPU very briefly.
You've probably seen a hundred other--
Well, maybe not a hundred,
but quite a few other talks
already about that.
The concept is remarkably simple,
so really it doesn't take
a huge number of slides.
And then of course we have one more thing.
We're gonna show you
how it works on a GPU,
what it means.
And again, it's actually
remarkably simple.
The implementation isn't simple.
One of our engineers
who has implemented it
here under the team
that we have at Codeplay
is gonna show a live demo.
And then we're gonna do
one more thing again,
and that is, I've been
leading this group called SG14
which has been driving
for heterogeneous C++
and distributed C++.
And I'm gonna show you where we're going
with all the different proposals
that we're gonna foresee
in that future direction.
Just a few things about ParallelSTL,
I've already mentioned that.
Essentially there was a wish
by the group after C++11 came about,
and recall C++ gave us a very simple,
basic thread interface,
multi-threading with atomics,
with an atomic and a memory model,
but nothing else about
the library capability
is towards parallelism.
There had already been directions
in that outside the committee.
For instance, GCC had a ParallelSTL
implemented using OpenMP.
There was another at Texas
A&amp;amp;M University called Staple,
in Bjarne Stroustrup's group.
So people have tried that,
and they've already noticed
certain things you could do in parallel,
and certain things you don't do.
You can't do pop, push and pop over again,
because that's fundamentally sequential.
But there was some other algorithm
that didn't involve push
and pop a great deal
that you could do in parallel
fairly quickly and easily.
Accumulate was in fact one
of the top on the list.
It seems reasonable to
look at the industry
and gather some of that experience.
In today's current desktop technology,
you see that there is
a combination of explosions
between the number of cores
and the GPUs that's either
attached in a discrete manner,
or in some of these cases, with Intel,
the GPU core is more or less
combined with the CPU cores.
There are other directions right now
towards more massive parallelism
where you have millions,
essentially millions of GPU cores,
depending on whether
you're doing things like
high-performance computing,
or distributed computing.
If you're doing distributed computing
you might be indeed attaching
multiple nodes here.
And in that direction, Hoppen
Keiser had a great talk
talking about how you could use HPX
to essentially do initially
distributed computing,
and subsequently he added
heterogeneous computing capability
on top of that as well.
And we have always headed towards
a heterogeneous direction,
and whatever we took from C++,
we would always try to see,
okay, how do we do this using
heterogeneous computing,
how to send it to multiple GPU cores.
We've been hired by various
companies to do that,
especially when they're
doing embedded processors
that are embedded GPU cores.
From a software point of view,
various companies have already
put software on the table
back in as early as 2012, even
before standard ratification,
that can allow you to run
the library on the GPU.
So we're talking about two things here.
One was do it in parallel.
And I talked about it
with the GCC libraries
that was using an underlying OpenMP layer.
Beyond that is also doing it on the GPUs.
And doing it on the GPUs,
there was demonstrations of that
using NVIDIA's research
divisions, called Thrust.
AMD had a similar one called Bolt.
Microsoft and Intel of
course had the Intel TBB,
thread building block.
And of course there was
parallel pattern library
and C++ AMP.
So the world had a large number
of mixtures of experience in this,
but they were non-uniform.
Some were really only working on CPUs,
some were working in parallel,
and some were working on GPUs only.
Things like Thrust and Bolt
were mostly for the GPUs.
The other ones were mostly for the CPUs.
But nevertheless they were all
trying to do it in parallel.
So the committee, being
mostly conservative,
started trying to do it,
and because the C++
standard really doesn't
admit anything about any
other separate memory
other than the CPU and a flat memory,
essentially a flat memory layout,
all of what we did was
essentially the proposal
centered on having it run
in parallel on just CPUs.
So the first proposal appeared in 2013
under a Parallel Algorithm Library,
and essentially combined the experience
from TBB, PPL, Thrust,
to just essentially
abstract out what it took,
what it meant to run this
algorithm on the CPU only.
And over time there
was a lot of iterations
until it culminated in
Jacksonville in 2016, I believe.
Yes, it says that. I'm
just going by memory.
That looks interesting there.
You see the long timeline in between.
There were a lot of--
Obviously something like
this going into the standard,
you might think it's simple,
given that there's a
lot of prior experience,
but trying to fit it into the standard
resulted in a number
of interesting changes.
And indeed, there were actually
many clarifications and corrections,
even after this point
before it reached 2017.
And indeed, most of these
are much more noticeable
because they were decisions about changes
to a TS that already has been published.
So the result I would say is not really
that you can't change things after TS,
and indeed the learnings from
TS is extremely valuable.
This is the takeaway that I got from it,
that TS is an extremely
valuable instrument,
especially and only if you
have implementations already
using it, trying it
out, and demonstrating,
and giving feedbacks to the committee.
And indeed there were--
Let me see, do we have that slide?
That slide about the number of
implementations is later on,
but there were actually about
five or six implementations.
Some had already implemented from before.
Some implemented after
like we did at Codeplay.
So the number of changes
were actually quite a large number.
This is actually a summary,
mostly from my recollection,
of all the changes.
I'm gonna go over each one of them
because they're kind of interesting.
Once we got it in the TS form,
the first thing we looked at
was this idea of a
dynamic execution policy.
This is the idea where
the execution policy,
which is the part
when you say whether
the parallel algorithm
is to be executed in
parallel or sequential,
or parallel or vectorized manner.
You could substitute that
with a variable name,
that you can change
dynamically, in flight,
during program execution.
We changed that because we felt
that this would effectively
change the state
of the algorithm
and we didn't want that to happen yet.
We might want that,
but we have concerns
about that as a committee.
So that was the first
thing that was dropped.
The second thing that
we changed was the name.
For a long time there
we wanted to look at--
We had three names.
One which was seq, for sequenced,
meaning that it's running
in a sequential manner.
The second one was par,
meaning that it's running
in a parallel manner.
The third one was par_vec,
meaning it's running in a
parallel and vectorized manner.
For a long time there we looked carefully
at that third name, par_vec.
Well it conveyed the
name relatively clearly
that you want to run it
parallel and vectorized.
There was concerns that this
is not always gonna be true.
The ability to run these
is relatively optional,
especially at a higher and
higher parallel manner.
Some par might not actually
have the capabilities
to support vectorized capabilities.
In such cases the algorithm is actually
allowed to drop you back to parallel,
and if parallel was unsustainable
it could actually drop
you back to sequential.
The friendliness of your implementation
might actually inform you
of that, but it all depends.
At that point we decided to
change the name to par_unseq
for parallel and vectorized.
That was a good change
because it really is saying
that this is parallel,
but the predicate functor
that is the deciding
quality of your algorithm
is gonna run in an unsequenced manner
interleaved on multiple threads.
So it was more descriptive,
where as the name vectorized
could mean different things
to different implementations.
The next thing that happened
was a flurry of changes.
Those first three changes
actually happened fairly far apart.
Then starting in mid-2016
in Oulu, Finland,
a number of changes started
coming much more rapidly
because of more recent
implementation experience.
And this tells you the value
of something like this.
The next two changes came
because someone really looked closely
at the exception policy of these guys.
And the exception
policy, the way it worked
was that for every exception
we would generate an exception list,
and the exception list
essentially is a vector.
Hanging off the vector would
be these exception pointers.
Now this in theory sounds
great, like most designs,
but when it met the metal,
imagine you actually had a
parallel algorithm running
and hundreds of exceptions
starts pouring out.
This vector would grow
almost uncontrollably.
That's the key problem there,
this growing of this vector.
Now, vectors can grow,
and that's very nice,
but if it's going dynamically
while the program is running,
can you imagine would that would be like
in an actual operating environment?
Yes you can.
So we killed it.
We said that's just unsustainable.
No one else actually
implemented except one company.
Codeplay actually implemented it. We did.
We also thought it was
weird that this was decided.
So certainly this was an important thing.
Key among this was Bryce Louback
who actually was working
at Berkeley at the time
and he certainly came
back with the opinion
that during his implementation
attempts of working at it
he felt that it was unimplementable.
So we definitely killed it.
We made it so that if
any exceptions came out,
it would just terminate and unwind.
Now before you think that's a bad idea,
in parallel, please note that
C++ has no policy right now
to handle exception in a
concurrent environment.
We have exceptions in a
sequential environment,
but we have no idea
how to handle in a concurrent environment.
Neither does the rest of the world.
In such an environment we
have to essentially pull back
and use the most coarse
grain method available to us,
and that is just to terminate.
I can draw that parallel
experience with other standards.
In OpenMP for instance,
if you have an exception that
escapes a parallel region,
that's all they would do as well too.
In many other parallel models
this is exactly all they do
because they can't think
of doing anything better.
The world actually does need
a better exception model
in a parallel environment,
and we're working on that.
The next thing was that we noticed that,
as a drive-by shooting we thought,
now that we changed that,
what are we going to replace it with,
this exception policy?
Well we don't know, we
don't have enough time.
We're gonna go to standard ratification
in a couple of months.
In fact the cutoff had already happened
in the last meeting, in
the Jacksonville meeting.
This is after Jacksonville.
So you don't have time to
redesign things now at this point.
What do you do?
Well we can't let exceptions
sit where they are right now,
which is that they were
attached to the algorithm.
That meant that if we tried to replace it
we would have to replace
it for every algorithm.
That's just not a tenable proposition.
So we thought at that point,
let's move it so that it's now attached
to the execution policy.
The execution policy are these names
like sequence, seq, par, and par_unseq,
that's the name that's been changed.
So by attaching to the execution policy
now it gives you the
flexibility, in hindsight,
of being able to change
this execution policy
as we invent a new one.
So to change the exception
policy as we invent a new one.
From that point on the changes
come even more rapidly.
Bryce in particular
noticed that there were
incongruent names involved
with the numeric aspects,
the numeric aspects of STL
that was under the ParallelSTL name.
Mostly names like transform, reduce.
So most of that was changed
under the next two papers.
And then at that point,
ParallelSTL was mostly incorporated
into the draft C++17 specification.
And so this was basically
about late last year,
and C++17 went to ballot
for international voting.
At that point a whole new
flurry of comments come in,
and that include the bottom two.
Principally Switzerland
came in with two comments,
which was extremely interesting.
And this demonstrates how it works
as an international standard.
Switzerland noticed
that some implementers,
namely with Codeplay SYCL,
which is essentially
a modern C++ on top of
that allows you to distribute
to heterogeneous environments immediately,
we would actually have
to copy the argument
into another place.
It's a heterogeneous environment,
the argument might come
through from the CPU.
It would need to be copied
into a neutral place so that
it could potentially be moved
to the GPU for execution.
You'll see that.
We don't copy everything over,
we only copy what's necessary over.
So this cloning is actually
not allowed by the specification.
I think they had the
intention of this happening,
because Thrust was one of the backgrounds,
NVIDIA's Thrust, which operated on GPU,
but it was meant to work only on a CPU,
so they never thought of
designing it like that.
So one of the comment was that
you couldn't make this copy.
So we had to argue for quite
some time around the committee
so that we could enable this
cloning of this argument
in order to enable SYCL to work properly.
In reality we actually didn't need it,
and I'll show you why,
because the reason it
worked, we worked it out,
is because we didn't use
any of the standard execution policies.
What these things did
was that it enabled you
to do them on standard execution policies,
the named ones like
par, par_unseq, and seq.
We didn't want yet another implementation
where you couldn't do
things in a standard way,
because most of the
implementations at that time,
like HPX which implemented it,
and SYCL which implemented it,
was actually not implementing on the CPU,
that wasn't the point.
They were trying to do
it on distributed nodes.
The problem also exists
on distributed nodes
because they also have
to clone the arguments
to the different nodes that
they want to send it to.
Just like on heterogeneous computing
we have to clone the
arguments to the GPU node,
and HPX would clone it to the CPU nodes.
The standard did not allow that,
so we had to fix that area.
There were other additional fixes
that actually had to be done.
There was the idea that you
couldn't invalidate these--
Because it's a vector.
Vectors normally cannot be invalidated
and kept in the same shape.
So this changed the...
I'm on power save.
Maybe this is on power save at the moment.
I hope this comes back.
I'll keep talking.
So what they did was essentially--
So it had to change the
idea that you had to do
instead of forward iterators,
so we had to switch
between forward iterators
and random access iterators.
In combination, all these changes enable
the final version that got
into the C++17 standard.
So as you can see, what got
into the C++ 17 standard
is non-trivially different
than what's in the TS.
Although most people might
miss all these little details,
I was painfully aware of every one of them
because I found that it
was necessary to drive this
so that it was an enabler
for future directions
for distributed and
heterogeneous computing.
Thank you for reviving this.
Next part.
I don't know why it's playing,
but that sometimes happens.
After all that let's take
a look at what it means.
I probably should have
done this at the beginning
so you have what concept
of what I was saying.
But it's essentially, the whole
idea is relatively simple.
You have all these STL algorithms
and if you say nothing at
the beginning like up here--
With sort, if you say nothing,
it's assumed the same as before.
It's sort of run in a sequential manner.
If you put the execution
policy of seq in there,
that's the same as if you said nothing.
It would still run in
the sequential manner.
If you add the par execution policy
it will now run the predicate
in a parallel manner.
And these execution policy
creates a new namespace
that is called the
sequential, the parallel,
and the parallel unsequenced.
As I said, parallel
unsequenced's name was changed.
So sequence is just do
it normally as in C++11.
Parallel adds the ability that the caller
may span to other threads,
but on a single thread the
invocations do not interweave,
whereas with parallel unsequenced
it relaxes it yet again
so that the invocations of the predicate
is going to be interleaved
on a single thread.
Now as I said, these
fall back on each other.
If the hardware is not
capable of implementing it
it just keeps going down
so that it's always
going to be executable.
This actually opens the door
for many other execution policies
and indeed several others
are now being talked about
in the next parallelism TS2
with the idea of SIMD executions.
When you're doing SIMD executions,
which still right now
only talks about the CPU,
but we were always keeping in mind
that it has to work also on the GPU
because that's all GPUs is
about, is about SIMD execution.
We have invented additionally--
(coughs) Excuse me--
additional policies that's
gonna go on top of these three
that is gonna help us
manage SIMD executions.
I mentioned that there were
a large numbers of implementations
and indeed, most of them have been ongoing
between Microsoft, HPX, HSA,
Thibaut, NVIDIA, and Codeplay.
We think that there's still
no Clang implementation.
We welcome if we're proven wrong.
Using execution policies
is extremely simple.
You just have to add this
additional parameter at the beginning.
What's interesting is that
there is a custom vendor policy
and this is the escape door, back door,
that most of us at HBX and SYCL use
because we created our own
vendor execution policy
which allowed cloning,
all the special kinds of iterators
that wouldn't invalidate,
so then we actually can
make it work properly
in that special environment.
Propagating the execution
policy into the end user
is quite simple using a
library or template here.
Not all of the parallel
algorithms, libraries
would implement them all of course.
Only a few were not implementable
in a parallel manner.
So as a result, almost all of them were--
I don't think we have
a list of what was not,
but it's not hard to find I suspect.
There is a couple of
new ones that we added
such as for_each and for_each_n.
Some of these are fairly
easy to be reimplemented
using OpenMP, for instance.
Some of these could be just implemented
especially if the execution
policy is par_vec,
with the OpenMP parallel
region for using a SIMD
and that's probably all you have to do,
and attach a following form to it
and that would all that you would need.
You could also do this very
easily in OpenCL as well too,
or whatever favorite
parallel language you have,
whether is CUDA, or whatever is it,
or HSA, that you wanted to implemented.
So this is why there is so much
industry experience in this.
What this does differently
is that it elevates
all those many different
implementation styles
into a higher abstraction,
so that you just have one high
abstraction that can do it
and then it doesn't matter
what your underlying policy is.
You assume that they're
going to optimize it
for the best underlying hardware.
And this is exactly the point of C++.
And the point of what I had
been talking about yesterday,
one of my talks about
parallel programming,
and that's to elevate the generality,
the generality of your design
such that you can increase productivity.
Now you're no longer focused
on the underlying details
of which GPU or which CPU
I'm executing this for.
There are other additional
new algorithms like reduce.
And these guys give you
the ability to do things
that are very interesting
parallel programming patterns.
I'm working on a different
talk that deals with this.
But this is essentially a
serial reduction algorithm
because we're doing this
with no extra parameter,
as you can so, so it's run in sequential.
But very easily you can think of
what is very common place now
in the parallel programming world
where you have a parallel
reduction pattern
that essentially do things cooperatively
between multiple variables
that you can now reduce down to one.
Every parallel language
I've been involved with
between OpenMP, OpenCL, has a
specific name for these ones.
Usually they're just reduce,
so it's time to create a standard one.
There are also transform algorithms
which is essentially a map,
and that's essentially the idea.
You map it to something else
through a specific function
that you would give
that you would operate
on every single element
and you can do that in parallel.
This works only particularly well
if it's particularly
interestingly parallel.
If there's dependencies among them,
that's basically caveat emptor.
And this is what this is saying.
You really have to know your
data to use these things.
You can't just sort of apply it blindly
because the compiler transformation
or the library transformation
will apply pretty blindly.
At this point in time, any
things of flow dependencies,
or output dependencies,
or anti dependencies
is all up to you.
There are also additional algorithms
that was added for transform_reduce.
You know that's one of the defects
that we had tried to adjust
was to set the names for these things.
With transform_reduce
it's a fairly good idea
in that while you're doing a map
you're also doing a
reduction at the same time.
This is also, as you can see,
combines the two previous
parallel patterns.
What can I do with a parallel for_each?
Well the traditional case is that
for_each uses only one core,
rest of the die essentially is unutilized.
That's too bad,
because we do have all
these flops lying around.
So you could do something
like a fill or for_each
so that the workload is now
distributed across the cores.
One of the things about Amdahl's law
and Gustafsson's comment
about Amdahl's law
is that you have to expand the workload
as your parallelism increases
in order to fully be
able to be productive.
And so the question of
course is in this case,
what do you do with the GPU part?
That is still, right now, unutilized.
And in other talks that I've given,
and Shawn has given,
you notice that most of the gigaflops,
in fact I dare say almost 75% of gigaflops
reside in that area of the die,
and it is totally unused.
That's a shame,
and that's something that
we hope that we can correct
and that you guys all can
help us in that direction.
So the next part I'm going to show you is
there's one more thing.
It's gonna be about how it works on a GPU,
and for that my colleague,
Gordon from Codeplay
is gonna come up and do a really nice demo
that I hope you can see
for the first time parallel
algorithm on a GPU.
Thank you Gordon.
- Thanks Michael.
Everyone hear me okay? Good.
My name is Gordon.
I work senior software
engineer at Codeplay
where I work on Codeplay's
implementation of SYCL
as well have contributed
to the standard itself.
So I'm gonna give you
sort of a brief overview
of what SYCL is, how it works,
and then how we used it to
implement ParallelSTL for GPUs,
and then I'll give you a
little demo of that running.
So continuing on from
what Michael was saying,
if you want to utilize the
GPU side of the die as well
what we've done is we've extended
the standard execution policies
of C++ to use SYCL policies
and these policies allow, through SYCL,
execute the work on a GPU, or a CPU,
using OpenCL underneath.
So this is an example of
the fill_n or the for_each
using the SYCL policy.
So essentially this is all you need to do.
You switch in the standard
policy for a SYCL policy
and then the work is using
the SYCL implementation
executing on the GPU cores.
There's an additional
policy we also provide
with the heterogeneous policy.
So this kind of goes a little bit farther.
Rather than executing your work
on single GPU or single CPU,
allows you to distribute the work across
two GPUs or a GPU and a CPU
and work share across the cores.
In the ParallelSTL implementation
using SYCL we have,
it's publicly available
in the Khronos GitHub,
this is the functions,
the algorithms that we have
implemented at the moment.
I'll give you sort of
a very brief overview
of what SYCL is.
Traditionally if you wanted to (mumbling)
a open standard with written code for GPU
you would use OpenCL.
OpenCL is traditionally a C-based language
so this involved quite a
lot of boilerplate code
and it was quite a lot of
work getting things to run
on a GPU or some other
heterogeneous device.
We wanted to create a standard
for single source C++ programming models.
So the idea is you write
your code for your GPU
in standard C++ in the
same application as,
the same source file as
your application code.
So we designed SYCL to
be based on top of OpenCL
but using standard C++.
This is kind of an overview
of what the ecosystem looks like.
Generally with SYCL you would
implement your C++ application
using template libraries using SYCL APIs.
So you'd write algorithms in libraries
using the SYCL template library.
And you run this, it uses
OpenCL to run your code
on a various number of
different OpenCL devices.
So any OpenCL device can use SYCL.
I'm gonna give a very quick example
of what the SYCL interface looks like
using a sort of vector add example.
So here the first thing we do--
The first thing we do is-- Whoops.
Not sure this is working.
It's not working very well.
So the first thing we do is we
include the SYCL header file.
So this includes everything that you need
in order to write your
kernels that run on a GPU.
Here we have this function
called parallel_add.
It essentially takes two input vectors
and returns an output
vector with the result.
The idea is it will add each element
of the two input vectors together
and then send it to the output vector.
In SYCL we have this idea
of buffers and accessors.
Essentially what this allows us to do is
when you create a buffer,
it's essentially maintaining
a piece of memory
across the host application
and one or more devices.
And then we have this idea of accessors,
which I'll show in a moment,
which allow you to define
a particular kind of access
to that memory on a certain device.
And what that allows
SYCL-run things to do is
very efficiently track the
dependencies of your application
and make the data
available when you need it.
The next thing we do is create a queue.
This is essentially the object
that will manage the device
that you're running on,
so this is what you
can queue your work to.
Next thing we do, we create
this what's called command group
and a command groups essentially
are C++ functional objects,
such as a lambda or a function,
a callable function object,
that it defines three things.
So it will define the function
that you're gonna execute.
It will define the accessors,
so it defines the memory
you're going to access on the device
and it'll define the range of executions
so the different number of iterations
of the function you want to execute.
First we're defining these accessors.
So essentially here we just request access
to the buffers that we have defined.
So there's two inputs and one output.
The two inputs provide read access
and the output provides write access
so what this does is it
allows the runtime to know
what do we do for what you're writing to.
So if you were to read
from multiple places
at the exact same time, you
can do that concurrently
but if you're writing to something
it will prevent you from writing
to it from somewhere else
or reading from somewhere else
because it's being modified.
And then finally we define
this parallel_for function.
This takes a lambda or a function object
that defines the actual function
that will be run on the GPU.
The first thing it takes is a range.
The range specifies the number of elements
that you want to execute across.
So here we're using
the size of the vector,
so whatever the size of the vector,
that's the number of iterations
of the function that will be executed.
And then we have this lambda function.
The lambda function
takes a single parameter,
specifying an ID.
For each iteration this
ID represents the index
into the space that
you're executing across.
And then finally we have the
actual code for the function.
This essentially here,
we're just using subject
operators on the accessor,
so the accessors can be
used very much like pointers
and here we're reading from
each element of the two inputs
and sending it to the output.
Another property of the buffers is
buffers will synchronize
your memory on destruction
using (mumbling).
So when this function returns,
the buffers are destroyed
and the memory automatically waits
for the function to finish
and it will synchronize the memory
back to the original
pointer you provided with.
So that means that after
your parallel_add returns
the computation is done
and the result is stored in output vector.
Jump back to ParallelSTL.
This is a site overview
for what the ParallelSTL
execution policy looks like.
We provide a couple of
different ways of doing it.
The first is if you just
create a policy by default
it will pick a device for you.
So one of the features SYCL allows
it will pick a device,
the device that it thinks
is the best device for you automatically,
or allows you to fine tune,
pick a specific device
for what you need.
And then it'll provide
some other functionality
for retrieving the name of the
queue that you're running on.
The way we implement this is
we provide an additional
(mumbling) for the algorithms
like for_each here
that take the input
iterators and the function
and then we pass it
onto our implementation.
From that point essentially what we do
is we take, from the execution policy,
we take the queue, the SYCL queue
that was picked to run on
and then we calculate
the number of iterations
that we want to execute across.
What we do here when we're
calling this function
that is getting the max_work_group_size.
What this does is it
creates the OpenCL device
for what the maximum number of iterations
that you can execute
at once on that device.
And then we do a calculation based on that
to see what the optimal range should be
that we're executing across.
The next thing we do is
create this command group.
It looks slightly different here
because we're defining the lambda first
and then submitting it.
So essentially this is the
same as what you saw before.
We define this command group
and inside we create an accessor
to get access to the buffer.
The buffer wraps the memory of the vectors
into the vectors that we've passed in.
Then from that point
we call a parallel_for,
and inside the parallel_for we
retrieve the index from SYCL
and we pass that on to the algorithm,
we call the algorithm, the
function that the user provided.
Now I'm gonna give a quick
demo of what this looks like.
I have to switch screens.
I have to zoom this in a little bit.
There it is.
Can everyone see that okay?
- [Man] Yeah, looks good.
- Okay.
So here we have all of the
header files that we include.
So there's the standard
algorithm execution policy.
We also include the SYCL
execution policy as well
to provide the additional policies
that we provide for SYCL.
And we define the size of the dataset
that we want to work over.
In this example I'm going
to show the sort algorithm.
So we have this function here, run_sort.
It takes any execution policy.
Essentially what it does
it is initializes a vector
with a random number of
elements specified by the size.
So a random value to each element.
And then it runs the-- here,
it runs the sort algorithm
with that vector and the execution policy.
Essentially the rest of the code
is wrapping it with a timer
to output the time that
it took to execute.
And then we do a check
to see if it was sorted.
Moving down to here.
We run this function in four
different execution policies.
The first one is the
standard sequential policy,
the second one is the
standard parallel policy,
and the third is the standard unsequential
so this be run and vectorized.
And finally we have the
SYCL execution policy.
So the first thing we run on the CPU,
the standard C++ policies,
and then the fourth one
we run on SYCL on the GPU.
So if I run this...
I'll try that again.
This is quite small so
it may be hard to see.
The first three algorithms
have very similar times.
The fastest one is the unsequential.
Here notice the SYCL execution policy
is actually slightly
slower than the other ones.
The reason for that is
one of the common problems
with executing on the GPU is that
the overhead of the data
to the GPU and back again
is quite high.
If the amount of work that
you're executing across
isn't enough to outweigh
the overhead of moving data
then it's often not worth it.
But if we go back here
and switch to a higher...
Higher size of data set
then run that again.
This time it's taken a
little longer to run.
This time there's very similar results
from the three standard policies.
So 1.8, 1.7, 1.6.
The unsequenced is a little faster.
However the SYCL policy
is running approximately
four or five times faster
than the standard policy on the CPU.
This shows you you can
get a lot of speedup
by running the code on the GPU.
So it actually fits very
highly powered code.
However you have to keep in mind
the amount of data that you're--
the size of the task is very important
because if it's not a large enough task
it can be outweighed by the
cost of the data movement.
I'm gonna pull the slides back up again.
If you're able to see the results there,
I've put the results on the slide here.
The first example is
running two to the 16 size.
This was where there wasn't--
was very little or no gain
from running on the GPU.
Then as the size increases
the amount of speedup rapidly increases.
Now I'm gonna invite Michael back up
to talk about the future
direction of the standard.
- Thank you Gordon.
This is actually quite amazing.
So it's actually almost five times faster?
- Around four or five
times faster with this.
- Just to hang on for a
moment there with that one,
you notice that this
is actually on an Intel
where the GPU and CPU
is actually combined.
A couple of things to note.
I'm not trying to bash
different companies here,
but potentially if it's an NVIDIA
you actually have even
further data movement,
bigger data movement,
but the NVIDIA GPU will be more efficient
and be able to do things a lot faster.
Would that be fair to say?
- If it's a discrete GPU the cost--
- Different configurations will have
significantly different results,
so you might have to
need a bigger data load.
If it's an AMD GPU for
instance, which we have,
but not on this laptop,
then it's an integrated GPU
and the results would also be different
but it will still be faster.
- The results will vary
quite greatly between
running on a discrete GPU
and maybe like in these APU
or the Intel embedded GPU.
- Thank you, thank you.
(applause)
I'm absolutely amazed
the direction of all this
that it's taking.
I've very excited about where
we're gonna go with that.
When I see results like this
I really want to make sure
that everybody is aware of
what directions we can go to
with taking advantage of
that last bit of die surface
that is still inaccessible to us to C++.
I never want to get to
the point where C++,
it's just gonna become a wrapper language
and you have to descend into
some vendor-specific language
so that you have to access
to the fastest part of your computer.
So in that respect, one of
the things that we want to do
is to get C++ towards that direction.
Now, it's not a straight path there.
We know that it's gonna
be a bit of meander
and sideway movement.
And personally the committee
all agrees with us already,
to some extent,
except that until we set the direction
is was all kind of a
silently-believed direction
but nobody announced that
until we actually said,
&quot;Yes, we want to go to this point.&quot;
If we didn't say it we might get there,
but we might get there like this.
And I don't want it to be like that,
I want it to be in a
relatively straight line,
relatively speaking.
But even though it's a
relatively straight line,
it's not straight.
To get there we do have
various hurdles to do,
one of which is we have
to get executors working.
We're gonna show what executors look like.
In fact there's a talk about executors
at the end of this week
with Gordon and me.
Gordon's gonna talk about in deep detail.
We've been on calls every other
week with a bunch of people
who are deeply interested in
getting executors working,
from NVIDIA, from Google, from
NASDAQ, and from Codeplay,
meaning there's discussion.
Part of the reason is because
we've all implemented those executors.
I'll show you what they are.
We've implemented it in SYCL.
NVIDIA has done it, Google has done it,
and so has the NASDAQ people.
Once you have executors going
you have to get asynchronous algorithm
because that enables latency hiding.
That is fundamentally what's
gonna be important for this
to work well in the games
industry, in the fintech industry,
as well as in the future
of self-driving cars
or machine learning.
Next thing we have to do is
create this execution context
which may be an actual context object,
or it might be an abstract.
But certainly we've been
talking across the board
with the high-performance
computing community
across all the US national labs
like Sandia, Livermore, Argonne.
In fact in supercomputing coming up
we're gonna host a (mumbling)
on heterogeneous and
distributed C++ computing
with one of the ongoing
national lab people Hal Finkel.
So when we took these
proposals to try to get
a general sense of the directions
to the standard committee
because these proposals
effectively makes it different
that we can't talk about
flat memory anymore.
Now we have to talk about
either separate memory,
or hierarchical memory.
We realized that this
would be a very invasive
surgical change to the C++
standard that we all love,
but we feel it's unavoidable
because we can't let the
standard lag behind so far.
And we don't want to get
out there to 2020, 2023
and C++ is still living
in the CPU stage only.
They came back and told us,
&quot;We can't look at inaccessible
memory yet at this point
&quot;because we haven't figured out
&quot;how to deal with affinity yet.&quot;
That's reasonable,
so as such we are now
retasking our directions
towards addressing a way
to deal with affinity.
Affinity in the community
is generally dealt
with something called hwloc,
which is a function you
can call on Unix machine
that you can sort of get a configuration
of the cache configurations
and memory configurations
and things like that.
We've been working with
Sandia National Labs
on this design called
context and affinity.
And the other thing after that
is we have to work on
something that allows you--
As Gordon mentioned, the
heaviest price you pay
for that nirvana of five times speedup
is that memory data movement.
And this is actually on a system
where the CPU and GPU is
mostly fused together.
Can you imagine how it would be like
on a totally discrete system?
Now we don't know what the future hides,
what the future holds for us,
whether it's going to be
all distributed systems,
or whether it's distributed
CPU and GPU combinations,
or whether it's gonna be
a combined APU system.
That really is trying to pick a winner,
and we're not in the
business of picking a winner
in the industry.
We kind of want to serve
everybody as much as we can.
So we have to have some sort of way
of governing this policy,
whether we do it through
allocators, which is possible,
or we have to do it through
some sort of library routine
that allows you to move data
around in an efficient manner.
But don't forget, it's not
just moving data around,
it's about synchronization as well too.
And finally we are working on this idea
of an exception in a
concurrent environment
so that we can make sure that
we can handle that particularly well.
This is a quick view of executors.
Executors act as an interface
between the various
parallel constructs we have,
and there's so many of them
so we don't get an end-by-end explosion
with the execution resources
that we would have.
And there are many of
these execution resources.
Right now C++ is only pretty
much concerned with this.
We have provisions for this,
and we have nothing to handle distributed
or heterogeneous right now.
And that's the future direction
that once executor is there
we can enable those directions.
Come to Friday's talk and
we'll tell you all about it.
So in summary, we talked
about the ParallelSTL TS.
We talked about the history of it,
how it integrated with the parallel
of the C++ standard library.
For that we have a class,
I think this Saturday,
Sunday, with Chris
Tabella, that you can join
that we're gonna talk about the parallel--
Sorry, the C++ STL in-depth,
but we're gonna also add
a way for you to do--
I think we might have a hands on,
I don't know if we do, but
we're planning some way
that you can also play with
the ParallelSTL as well.
We also show how STL runs
on CPU. That's boring to us.
It's an important part,
but it's the other thing
that we're excited about
that we can now try to do it on a GPU.
And we also talk a little bit
about the future directions
towards heterogeneous and distributed ISO.
As somebody from Sandia said,
their goal is to retire
OpenMP and MPI permanently
so that we can all do it in standard C++.
Thank you very much.
(applause)
Any other question?
Chris, come on up.
In case you guys want to
ask any questions of us
about some of this stuff.
It's exciting, but it's a lot of work.
Yep, go ahead please.
- [Man 2] Just a quick question.
- Yeah could you-- The mic.
- [Man 2] So just a quick question.
I saw in your demo that
you tried to call GPU
to do the sorting or
whatever you wanted to do,
does it have to hold up a CPU thread,
or can you like give
it a callback function?
- The question is when you do this demo,
do we hold a CPU thread to keep it going.
- By the CPU thread you mean
in order to schedule the...
- [Man 2] Yeah, can you
like schedule it for example
if it takes like an hour to use,
can you not hold up the CPU thread?
- So essentially the
second implementation,
our implementation has a scheduler thread.
So when you create a queue
for executing work on the GPU
there's a thread behind
the scenes that runs,
single thread, schedule thread, that runs,
starts working, goes to work on the GPU.
- [Man 2] When it finishes the workload,
how does it send the signal back?
Does it send a software
interrupt or something like that?
Or do you have a callback
function that you can provide it?
- So the question is how
does it send a signal back--
How does it synchronize?
- There's two ways that
we synchronize the work.
There's an explicit way so
the queue has a wait function
where you can explicitly block and wait
for all work in that queue to finish.
And the other way is that any buffers
that are maintaining memory
that's been used on a
device in a SYCL execution,
when those buffers are
destroyed those will block
and wait for the specific
work to finish completing.
So what will happen is
then the schedule thread
handles all of the event
synchronization behind the scenes
so whenever there's a host
application synchronization
that will trigger for the
schedule thread to wait.
- I want to add that this stuff
is out in the hand as a community to use.
We have an open community
edition for you to download,
and there's now 4,000 downloads.
And we now have SYCL
implemented on TensorFlow,
the machine learning language from Google.
So it now can distribute
to non-NVIDIA GPUs.
The original TensorFlow was
only going to NVIDIA GPUs.
It's all working very nicely.
So it's more than just a proof of concept,
and we're usually hired to adapt it
to any company's specific
GPU or PGA's configuration.
That's why there's a lot of confidence
in the execution of all this stuff.
Thank you very much.
There's a couple of
other ones, so go ahead.
- [Man 3] Hi. This is really
cool technology, by the way.
The demo is really awesome, so thanks.
I have a question more for the design
of the parallel algorithms
from the perspective of a
third-party algorithm developer.
Pre-C++17, I could create my own algorithm
and it would kind of just appear
inside the ecosystem of algorithms.
But in 17 with these execution policies,
the barrier to creating a
version that can do parallel
or parallel unsequenced
becomes a lot harder.
Has there been any thought given to that
in the design process?
And the thought that comes to mind,
last year Ben Dean gave
a talk where he showed
you could reimplement all of
the STL algorithms effectively
using std::accumulate.
So is the idea that maybe--
So my thought is, should there
be a core set of algorithms
that provide these underlying mechanisms,
and then all the other algorithms
are implemented in terms of them?
How can we provide tools to people
to make leveraging this
cool technology easier
without needing to know
the guts of SYCL or OpenCL.
- So the question is it
used to be a lot easier
to reimplement algorithms
for your specific needs,
and now with ParallelSTL,
it's actually become
the barrier of entry is a little higher.
Is there a way that we
can sort of partition it
so that it can be represented
in different parts.
I don't know if that's been talked about.
There are other library experts here
that might be able to talk a little more
about that direction.
And I think that's actually good advice
towards the library groups
so that we can do these kinds of--
allow the users to be able
to do these kinds of things.
I don't know if that's been
talked about in the library group,
so I have to admit that I
don't know the answer to that.
- [Man 3] Okay. Thank you.
- Over here. How are you doing?
- Yes, is there any kind
of development or support
for Vulkan to kind of minimize
the communication pathway
between the GPU and the CPU
so when you're feeding
large number-crunching
or GPU instruction sets
that you don't have to
necessarily plugin the
pathways, you can do that once
and then have the GPU
work until it's finished
and then return the data to the CPU?
- So the question is is
there a way to minimize
the data movement between
the CPU and the GPU.
Did you mention Vulkan?
- Yes.
- (coughs) Sorry, I have a cold.
So this is a runtime design.
The runtime actually mediates
so that it's intelligent,
it doesn't move data back
and forth unnecessarily.
It keeps the data on the GPU
until it's absolutely needed.
So if you need to go back
to the CPU to get some data
it will only do that
and not have to copy
everything back and forth.
The second part about Vulkan,
do you want to take that, or...
- So what was the question
specifically regarding Vulkan?
- [Man 4] Basically the question was
can it use the Vulkan APIs and libraries
to kind of minimize the instruction set?
- So I think the answer is
right now it's now implemented
in terms of Vulkan,
it's implemented in terms
of OpenCL and SPEAR,
but it could be implemented
in terms of Vulkan,
and indeed that's a direction
that's been asked of us a number of times,
because it does give you
much more efficient path capabilities.
And since we're in the Khronos group
and I was just at the Khronos
meeting last week in Chicago
we've been talking a lot about that.
- SYCL at the moment is based on top of--
It's designed to be
based on top of OpenCL.
- But nothing says it can't generate
PTX, NVIDIA, or Vulkan directly.
Because it's really an upper layer.
- In fact one of the open
source implementations of this
was originally based on top of OpenMP.
- The open source SYCL implementation--
We have the commercial one as
well as a free download one.
The open source one is called triSYCL
and it is based on OpenMP underneath.
Thank you. Over here?
- [Man 5] So is OpenCL,
or SYCL as it were,
is this a library-based solution
or is this a separate compiler?
- The question is is
this a library solution
or separate compiler.
It appears as library in code
but it's actually done using
the compiler front-end,
using Clang.
This is normal in C++.
A lot of library section is
actually compiler generated.
Most of exception is in the library part,
but this is actually compiler.
So in this particular case you
see that it's a library call,
but it's actually a compiler
that's transforming all of that stuff.
It's much more efficient.
Do you want to add anything?
- I didn't really go into much detail
about how the actual runtime
and compiler system work in SYCL.
Essentially it's a single-source solution.
So you write your code once
and it's compiled by two compilers.
So it's compiled by the host compiler
and the device compiler.
The device compiler generates
some form of binary representation.
We use SPEAR, it could
also be something like PTX,
and that's compiled to run by OpenCL.
- [Man 5] So presumably that's
why you have no limitations
on using lambdas in these functions,
because you can compile it
for both the target device and the CPU?
- So there's some limitations
on what you can do
in the function based on the
limitations of OpenCL hardware.
For example you can't do recursion,
and you can't throw exceptions,
but in general the
majority of what you do--
- But that's only because OpenCL
has enclosed those things,
but in other formats, other back-ends,
you wouldn't need to have
that kind of problems.
- If you're interested we
did another talk last year
where we went into much
more detail about SYCL
and how the compiler works.
So if you're interested, check it out.
- So just a reminder.
SYCL is a language that's under evolution
just like anything else in C++.
It's just that it's under
evolution in Khronos, that's all.
(coughs) Excuse me.
- [Man 6] When I was deciding about
using either CUDA or OpenCL
I saw that NVIDIA is supporting CUDA
and most of the functionalities
are unavailable in OpenCL.
So with this effort do you
think these capabilities
will also come to the C++ standard?
NVIDIA is willing to support--
They are not supporting OpenCL that much?
- I understand the question.
Given that CUDA is mostly
tied to NVIDIA efforts,
and some of it is
implemented using OpenCL,
your question is is this
going towards the standard.
- [Man 6] Yeah, because
this will become standard,
but OpenCL was also standard,
but they were not supporting.
- So the answer is that's
definitely the directions.
SYCL is one of the framework
that we want to use
as a guiding experience to be implemented
into the future ISO C++.
But we're not ignoring
other ones, like HPX.
The national labs have two other ones
called Cocos and Rajaas.
There's a couple of other
frameworks like UPC++
and things like that.
They're all very C++ specific
frameworks, just like this.
And that's what we like about them.
That's why we're not looking
at things like OpenMP
because OpenMP is very
diluted in the C++ side.
OpenCL, while it's nice and heterogeneous,
is really focused on the C part.
CUDA is enriching constantly,
and it's rapidly moving
from C to C++ full support
in the last CUDA, 8 and
9, and things like that.
But it's really used as an NVIDIA way
of driving the directions of the hardware.
It's a way for them to implement,
to expose the hardware capability.
So we get a lot of learnings
from them as well, for sure.
But we can't rely on it as being anything
that we can modify or
control for the C++ needs,
whereas with something like this,
SYCL, or HPX, or Cocos, we
are willing to modify it
based on what C++ needs us to modify it
so that we can demonstrate
a reference implementation.
Next thing we're gonna have to do
is we do this affinity
thing, the context thing,
we would have to modify
SYCL to demonstrate
how this would work in a
modern C++ environment.
And that feedback loop
is extremely important.
As you can see from the
demonstration with ParallelSTL,
that feedback loop is extremely
important to demonstrate,
does this feature
actually work in practice?
When we get the exception
handling design done
for the concurrency TS
which I'm gonna propose
in the next meeting,
we'll have to implement it in something.
We can't do it in CUDA,
we can't do it in OpenMP
because we don't control that.
We could do it in SYCL.
The HPX guys will do
it almost immediately.
The Cocos guys will do it (mumbling).
This is really the most important part
of this feedback loop.
Hope that answers your question.
- Yep.
- Thank you. One more here.
This is amazing. A lot of good questions.
- [Man 7] My question
is regarding the demo.
In the last one, in the last case
where you use the SYCL execution policy,
does the entire sort, or the workload,
get run on the GPU?
- The question, does the
workload move onto the GPU
when you do a SYCL policy.
This is my colleagues kind of answer then.
- In the demo that I showed, yes,
the entire contents of the
vector is moved over to--
all the vectors are moved over to the GPU.
Then the entire sort algorithm
is executed on the GPU,
and the results are synchronized back in.
So everything is on the GPU.
Everything is moved to there and then...
- [Man 7] So a follow-up
question on that is
as I understand it there is a sweet spot
between how much you can leverage
the competition of the CPU
and how much you can do
the same for the GPU,
and it's important to use
both these resources at the same time
instead of offloading the
entire workload to the GPU.
That way we get the best of both worlds.
So is it possible, is it in one of the--
Is it a future goals that, you know,
when you give me an array to sort
and I sort part of it on the CPU,
so I keep the CPU busy and
part of it on the GPU--
- So this is a great question.
The question is can we divide up the work
between the CPU and the GPU.
- [Man 7] Yes, so one of
the resources are not idle
but they're all working.
- I'm going to actually tease
an answer to that already, the slides.
- One of the things that
I touched on briefly
as well as the SYCL policy
was of the SYCL heterogeneous policy.
The way that works is rather
than having a single SYCL queue
you have two SYCL queues,
and then a ratio of how you
want to distribute the work.
So you can say I want to distribute
the work across the CPU and the GPU.
And provided the OpenCL implementation
allows those devices
to be used concurrently
then that will execute
concurrently, run at the same time.
I wasn't doing that for the sort algorithm
because the sort algorithm
is not an ideal algorithm
for sharing across CPU/GPUs.
You have to kind of do a lot
of work on the CPU afterwards
to further the...
- [Man 7] My point is, so
this is still not abstract.
It is still up to the developer
who is using the SYCL library
to decide what goes to the
CPU and what goes to the GPU.
- SYCL doesn't make decisions about
how to distribute the work between devices
but it does make it very, very easy
to do work on multiple
devices at the same time.
The ability to having the
accessors we're describing
where you want to access data and when
removes a lot of the
complicated event handling
and synchronization
that we have in OpenCL.
So you essentially say you
want to do work in one place
and then another, or at the same time.
You just describe, you say,
I want to access this memory
here, this memory here,
and the runtime handles all of
the synchronization for you.
- Thank you.
Once again, if you want to try
download SYCL to experiment
it's free to download.
And like I said, we've had it available
since November of last
year, Codeplay had it.
There's now 4,000 downloads,
and it's been used largely by--
There is real experience
both from the open source
and we also use it in our
proprietary implementations
that people hire us to
do for their various GPUs
and whatever.
So anyway, thank you very much for coming,
and enjoy the break.
(applause)
Come to the executor talk
and the C++ library
class over the weekend.
Thanks. Bye.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>