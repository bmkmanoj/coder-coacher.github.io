<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: John Farrier “Performance Benchmarking with Celero” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: John Farrier “Performance Benchmarking with Celero” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: John Farrier “Performance Benchmarking with Celero”</b></h2><h5 class="post__date">2017-10-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dO-j3qp7DWw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Welcome to my talk on
performance benchmarking with Celero.
Other people pronounce this differently,
I'm just gonna go with Celero,
and if I'm wrong there'll
be comments online,
I'm sure, telling me that's not
how we're supposed to pronounce that word.
I've got six sections to the talk today.
Some of it's gonna be generic,
just benchmarking information.
There'll be some things
specific to the Celero library.
Real quick introduction, who am I?
I've got about 20 years of C++ experience,
not all that experience is created equal.
I'm reformed that I'm
doing much better now.
My career mostly focused
on modeling and simulation.
I wanted to build a library,
and I was interested in performance.
So I wrote a little benchmarking library
that works on multiple platforms
and can start to address
some of the issues that my team
was having with performance.
Who am I not?
That's also important to know
who we're not talking to.
I'm not the stig, but I
do care about performance.
So we will go on with
that performance-theme
throughout the talk.
Lots of folks have laptops,
the project is on GitHub.
It just requires C++11 and CMake,
and I've got that URL at
the bottom of all the slides
in teeny tiny print,
so if you don't type it in fast enough
or wanna write it down later,
it will be there for you.
Auto-correct has been challenging
while building this talk.
Celero is not produce, if
you see celery on a slide,
I apologize and I've missed it.
It's also not an old processor.
That also wants to get auto-corrected.
So if you see that, let me
know, I'll try to fix it
the next time around,
but I've done my best.
Celero is actually a
microbenchmarking library.
As I said, cross-platform.
I wanted to make it easy to learn,
so it looks a lot like Google Test,
so if you've used Google
Test or Boost Test
or those things, it's
a very similar syntax.
It supports fixtures as lots
of testing libraries do.
And it is a library, it is not
header only, unfortunately.
A few quick words about
microbenchmarking itself.
There's lots of different
kinds of benchmarking
and I just wanted to clarify what this is.
And this is trying to
measure the performance,
maybe relative or absolute of
a small piece of your code.
Maybe a critical algorithm,
maybe a critical path,
or maybe you want to do
some design trade offs
and do some measurements.
Microbenchmarks are a way of doing that.
No talk on performance optimization
would be complete
without quoting this law.
I put some of the text up from Wikipedia,
but the short of it is, and something
to always keep in mind,
and it is intuitive,
after someone says it for sure,
is that the amount of
performance improvement
you're gonna get out of optimization
is gonna be dependent
on how much that thing
you're optimizing participates
in the overall system.
So you could completely
optimize a way a function call
but if it's only called
once on the startup,
you won't see any improvement.
Because it just didn't participate
in the overall system that much.
So that's something to keep in mind,
whether you're doing microbenchmarking
or performance-based
measurements dynamic profiling,
those types of things.
Always have guided optimization.
That is measure the performance prior
to doing any optimization.
Make sure you have realistic conditions
and you're operating at a realistic scale.
'Cause a lot of problems,
you only find that scale,
so that's something to keep in mind.
There's always a bottleneck,
there's always the
slowest part of your code,
but there's other ways to optimize.
Right now, with this library,
we're talking about speed.
But you may want to consider
optimizing memory usage
or disk access or the amount
of power that you're consuming,
other things.
This is really focused on speed,
but there are other things
you could be optimizing for as well.
And the last point here is that you could
maybe gain some performance
with very esoteric coding,
but that may be very hard to maintain.
And if it ever breaks
or ever needs modified,
you need to always keep
in mind maintenance.
And don't do so much optimization
that nobody can ever
understand what you did,
or your system just won't scale
as you need to make
changes and modify things.
Microbenchmarking is just
one tool that we have.
This does not replace the other tools
you may already be using,
whether it's things like
Intel Vtune or AQtime or
other dynamic analysis tools.
Use them all, they all
will probably give you
different information in different ways
and give you different insights.
Just keep that in mind that
there's not a single hammer
to solve the performance
optimization problem.
There are other benchmarking libraries.
Google has one, Facebook has
one, and there's a couple
other projects out there on
GitHub that I'm aware of.
There is a nice little article
comparing many of them.
It's about a year old,
but it's a nice article.
Just kinda going through
some of the differences and whatnot.
I encourage you to check that out
and look at the updated
versions of all the libraries
'cause I think everything evolves.
Let's talk about challenges
for benchmarking.
The first problem.
Computers are really complicated.
So when we're trying to make measurements,
the operating system is always doing
something else other than running our code
and working on our measurement.
Lots of cores are being tasked
to do lots of other things,
the processor maybe
changing speed dynamically,
and other things are gonna
be impacting our hardware too
such as cache invalidation and things
that we may not have control over
while we're running the benchmark code.
If you wanna know the
relative or absolute speed
of some algorithm, there's
gonna be error incurred
to measure that just inherently.
We're gonna have to have some timer calls,
we're gonna have to have some loops,
we have to shut down the timer,
and all that stuff starts to add up.
If you just look at some pseudo-code,
just basically if we
were to run a benchmark,
we're gonna take a bunch of samples,
we'll put that in a loop,
and then for each of those samples,
we'll start the timer,
we'll run over our code a couple times,
maybe so it's using up some more clock
and then we can measure that.
We'll shut the timer down and then record
how long that took to do and
repeat the loop a few times
so then we have lots of samples
and we can look at those samples
and try to understand how long
the do work thing actually took.
But there's function calls,
there's timer stuff in here,
and it's all very fast but
it all adds up as well.
The computer has lots of
opportunities to interrupt
the things you're doing while
this little loop is going on.
Second challenge,
is that we all are complicated as well.
And sometimes we get in our own ways.
We constantly take shortcuts with the way
we're thinking about things.
We have lots of abstraction
we apply to the world.
We tend to use intuition.
We tend to rely on it
maybe a little too much.
And even though a lot of us
already know those things,
that doesn't stop us from still
making those same mistakes.
And so while we're measuring,
and while we're doing optimization,
we need to try to overcome our own biases.
And let's not fall in to traps
that there is certain lore
that you will find online
and sage advise that
maybe isn't actually true
in your situation, so
let's not make assumptions
about whether it might be
faster to inline or not,
or maybe we should
hand-and-roll the loops,
or maybe we shouldn't.
Let measurements and
benchmarking and dynamic analysis
answer these questions for
you and trust the results
that you're getting from those things.
Which is good measurements,
that help us overcome some of the problems
with the computer
interrupting us all the time,
and also helps overcome our
own preconceived notions
about where fast code might
be and slow code might be
and how we can maybe improve it.
Given all that, there's
one more challenge.
And that's the compilers
are also complicated.
They tend to be smarter than us.
They only make programs equivalent
to what we ask them to make.
And that they try very
hard to not waste time.
Sometimes when you're writing benchmarks,
we need them to waste time,
where it gonna be throwing away results
but we still want to incur the
cost of saving the results.
We may make calls that could
be easily optimized away,
but we don't wanna do that,
'cause that's the exact thing
that we're trying to measure.
So the library has a utility in it,
called DoNotOptimizeAway.
I know my library has
this, Volley has this,
and Google Benchmark also has it,
you'll see it in various forms
in a lot of benchmarking libraries.
The idea is to create a
utility that will prevent
unused variables and function calls
from being optimized out.
And yeah, you're gonna force your program
to do obviously useless work,
but that useless work
happens to be the thing
that you want to measure.
So this is the current implementation
inside the library for DoNotOptimizeAway.
There's maybe two different
versions of this in the library,
and I'm just gonna go over one of them.
The basic trick here, is
that we're going to make
a comparison that will always be false.
But the compiler won't know
that it's always going to be false.
And so within the guts
of the if statement,
we're gonna use the thing
that we've passed in,
so you can't optimize it out.
We should never get in the side,
the body of that if statement,
but the compiler should also
never be able to figure that out.
That's really the trick
for DoNotOptimizeAway.
In one form or another,
this is the type of thing
that all the libraries are
doing for optimization here.
So given these three kinds of challenges,
what do we end up actually measuring
within the benchmarking suite?
I made up a formula, and
it's not very complicated.
And I think you all could
all make up your own,
maybe with more terms, maybe with less.
But for now this is
what I'm gonna go with.
We've got t measured,
that's the total amount
of stuff that we measured,
between starting the timer
and stopping the timer,
there's a bunch of things that went on.
So that's what we've actually measured.
T actual is the time it
took to do the actual work
we're trying to measure.
If all else, if you could
make any changes here,
you would just have t
measured equals t actual
and that's the perfect number.
We just can't get there
because there's other things
that are always going to be in the way.
One of those is overhead.
That's the loop
construction, function calls,
kind of the scaffolding
that's in the benchmarking
architecture itself.
There's gonna be some overhead there.
There's gonna be work
that the system's doing.
Interrupting your process,
maybe moving you around,
invalidating your cache, doing
all of these other things
that maybe you are not interested
in measuring the time of
but happen anyway.
And the last one is just
measurement error in general.
We don't have that perfect
access to the clock all the time.
Between starting and stopping things,
there's just overhead there,
so there's gonna be some error
in just the measurement itself.
And so as we go forward, I'll talk about
how we can start to cancel
out some of those terms.
So we'll go through a quick example here.
And for this, we're just gonna try
to look at the fastest way to
convert an int to a string.
Let's pretend we've
done some measurements.
The program does a lot of int to string
kinds of conversions.
We've got lots of ways
to do this conversion,
but we don't know which
one we should go with.
So what are we gonna do?
We will create a baseline,
which is in this case,
what I've chosen is what's the
best we might ever hoped for,
for int to string conversion.
What's the ideal for that?
And then we'll create a bunch
of test cases to compare.
There's lots of different ways
you can choose the baseline,
but for this example, we're
just gonna try to make
the fastest conversion
theoretically possible.
So I come up with a very advanced program.
Just get a random number,
which I'm gonna convert,
and then I'm just gonna say that it's 42.
So I'm not actually doing any conversion,
but I'm going to say that we
need to look at random ints
and we wanna convert them.
I don't think, intuitively,
problem number two,
that we could ever get faster
than just creating an int
and then creating the string.
That seems really fast.
But when we compile that,
it does surprisingly little.
It doesn't actually build a string,
it does actually call
rand, which I thought
was a little surprising,
that broke my intuition.
But there's not a lot there.
So we have to add in
our DoNotOptimizeAway.
We had some extra scaffolding in,
and now we've got a bunch of assembly.
I don't read assembly.
But there's a lot of
stuff there and we can see
the call to standard chrono which is
the DoNotOptimizeAway
stuff that's in there,
and this goes on and is much larger,
and we can see some strings and things in.
I have a link there at the
bottom so you can pull it up
and look at all that glorious assembly,
if that's your thing.
But the point is that we've verified now
that our baseline is
doing the kind of thing
that we actually wanted to do.
And that helps us attack
challenge number three.
We have to inspect the assembly.
We added scaffolding to that baseline
and case through two calls
to DoNotOptimizeAway.
We're gonna make sure
that we have two calls
to DoNotOptimizeAway in
all of the benchmark cases
that we're gonna write
just to keep that even.
So here's how it actually
looks within the library.
We call a BASELINE macro,
we give it a name of a test suite,
group name, DemoToString.
This test name is going
to be called Baseline,
very creatively.
We're gonna take 10,000 samples
and for each of those
samples we're gonna iterate
over this little function 1,000 times.
We're going to call this function,
this three line function,
1,000 times and record how long that took.
We're going to do that 10,000 times,
and look at all those samples
and all those measurements.
The reason we do this
is to try to cancel out
some of these errors.
That first 1,000, the
number of iterations,
helps cancel out some of the timing error.
If you're too fast, you just won't be able
to start and stop your timer fast enough
to really get an accurate measurement.
So we're gonna run it 1,000 times
just to try to spread that cost
at over 1,000 calls to the function.
Then to help cancel out
some of that system error,
we're going to do 10,000 samples.
So if in one of those samples,
we decided to write some
data to disk somewhere
or things had to get invalidated,
we're gonna take 10,000 samples,
and hope that one of those is really good.
The overhead will be in those
DoNotOptimizeAway calls.
We'll have to just make sure
that we're repeating that
in each of the cases.
Hopefully that helps tie together
some of the factors in
our total measurement
and how that's expressed
within the library itself.
We want to drive as many as
those factors as we can to zero
and we do that through
just canceling them out.
If we do lots of measurements,
and we have the same scaffolding in place,
hopefully when we're all done,
those terms start to
cancel each other out.
Test cases look something like this.
They're gonna be in this
group called DemoToString
'cause that's what we created that way
when we run the cases.
They'll all be grouped together
and compared with each other.
We're gonna look at a
to_string implementation.
We'll look at strstream,
we'll measure that as well.
That's kind of all three of
those things put together.
So that's just what all
the code would look like
you'd need to take that very
optimistic baseline case
and then measure to_string
versus strstream.
I will do just a complete walkthrough
this entire test suite
if you downloaded it
through GitHub, you
would see all the code.
I am not an optometrist, and
I don't expect you to read
on the right what's going on,
but I'm just gonna step
through, that's the whole file.
But I'm just gonna show
a few of the pieces,
there's a very common header.
We have a macro called CELERO_MAIN,
that's a lot like what's in Google Test.
I'm gonna do some hand waving here,
there's just some look-up tables
and some other implementations
that are coded in
that we're gonna call in
some of the other test cases.
That's what makes up the
majority of the file here.
We've got our baseline,
the benchmarks we saw.
At the bottom there,
there's an extra benchmark,
for just the table look-up.
Two more algorithms that we wanna look at.
We're gonna compare all
of these different ways
of doing an int to string
conversion together.
The command line is very complicated.
You just run the program.
celeroDemoToString.
When you run that, you'll
get some colored output
in the table in your terminal.
Kind of going from left to right,
it's the group name that
we saw, DemoToString,
the experiment, so
you'll see the baseline,
the to_string cases and those things.
I'm gonna talk about
problem spaces and samples
and iterations and why those
are important in a second,
but let me just kinda zoom
in on the important parts
of this table.
There's gonna be a baseline measurement.
That's always gonna have a 1.0
value in the baseline call.
I'll also show the number of
microseconds per iteration,
that's how long it took to
call the body of the function.
And how many of those
you could do per second.
Yeah, the math is easy to do,
but I found it handy to just have
that number right there for me.
Now we're gonna look at
to_string, took about,
twice as long as the baseline.
The strstream example
took about 19 times longer
than the baseline.
Simple table look-up seems to be on par.
0.96.
The hopmanFast algorithm
is about one as well.
That's kind of what came out of this.
The baseline case is really critical.
And I will probably repeat
myself in this talk,
but those microseconds per iteration,
that's measuring the cost
of DoNotOptimizeAway.
So I don't know if I would really look at
the total number of microseconds here,
because it includes a lot of scaffolding.
But that baseline case shows
you the relative performance
of the algorithms and for this,
when we just wanna know how
we should update our software,
that's really the most
important number on here.
The other thing to note is you
will get consistent results
if you set your experiment up correctly.
So here I've ran it three times in a row,
and you'll see that it all
but the strstream cases
were getting exactly
the same measurements.
And I can theorize on why
strstream was not consistent
but the important thing here, really,
is that we are getting
consistent measurements
when you set your test suite up correctly.
And that means that hopefully,
I ran this on my little MacBook here.
If you take it and run it on
a big development desktop,
you should see some of
those same relative costs.
The microseconds might all change,
but those relative cost
may translate very well.
Of course, processor
architecture differences,
and depending on the
complexity of your case,
you may see some variance.
But this allows developers
on the same team
with different hardware to communicate
about relative performance of benchmarks,
which is important.
So let's talk about some
of the art for constructing
the baselines and test cases themselves.
I didn't feel like typing
up a test fixture definition
so I copied one from JUnit.
But we have test fixtures
and we can use that
to help provide consistency in the set up
of all of our test cases.
The API itself was modeled strongly after
Google Test.
So now we'll do something like
measure sorting algorithm performance
'cause that is only
slightly more interesting
than int to string conversion.
You could look this stuff up,
but measuring is also kind of fun.
So we're gonna create a SortFixture.
We're gonna talk about the
experiment values in a minute
but one thing I wanna show here
is that we have a setUp function
that's gonna happen outside
of your measurement.
We're gonna have onExperimentStart
which is gonna be called
and measured, so you can
do uniform initialization
for all your test cases
that needs measured there.
After that, your actual test
case itself will be ran.
The baselines, the benchmarks,
those little functions will get ran.
Then when they're done,
we'll call onExperimentEnd
and that time will be measured as well.
That makes up the guts of the sample
and when that sample is done,
tearDown will be called.
So that provides you a way to
get uniform setUp, tearDown,
and initialization for
all of your test cases
to help make sure that you've got
the same scaffolding
in place for situations
that are more complex.
Celero supports two different kinds
of baseline measurements.
One is the measured baseline
which is what I just showed.
You can also set up absolute baseline,
and I will talk about those.
For the baselines, if
you're gonna measure,
pick a standard example.
That's some of my advise there.
If you're gonna work
on a sorting algorithm,
maybe your baseline case would be sorting
a vector of random ints.
Because that's kind of,
you're not on optimized case.
You're probably gonna be slower than that
in whatever you're doing.
You could pick a naive example.
You could take your current implementation
that you wanna measure against,
and make that your baseline case
so you can see if things
are getting better or worse
as your code changes.
Or you could just use something
out of the standard library
as your baseline case,
if there's a similar
type of algorithm there.
So for our sorting random ints,
I've chosen the vulnerable BubbleSort
as our baseline case.
Just put an implementation of BubbleSort
in the body of the function.
You'll see that it has extra call there.
It's got the F on the end
to say I wanna use fixtures,
and then the name of the fixture
class is added into the macro itself.
Sir?
(audience mumbles off camera)
Well, we could argue.
The question was about, should we use
the same set of random numbers
or a different set of
random numbers every time?
I wouldn't disagree
that it would be better
to have the same set of
random numbers every time,
I just didn't wanna put that much effort
into building this demo.
So, yes, scientifically,
that would make for a better measurement.
But for here, this should
be suitable for our needs
for the briefing.
We can also do absolute baselines.
And that may be something
like, I just need
to get this thing running
100 times a second,
or 1,000 times a second.
I've got a target I'm aiming for.
You can just put that number in directly.
Nothing will get ran, it'll
just record your baseline case
and the number of microseconds that takes,
and move on, and start
to compare everything
against a fix number.
We've also got problem spaces,
and I've kinda glossed over
it two or three times so far
in the talk but I will get into it now.
And that's a way to address the fact
that many problems require
just an extra dimension
of the testing.
Because some solutions really
scale better than others.
And maybe the fastest at a very small case
is not gonna be your optimal
when you have realistically sized data.
And so problem spaces allow
us to build a test vector
to run the baseline and
the test cases against
kind of uniformly and
see how a problem scale.
(coughs) Excuse me.
Inside the test fixture there is a call
to getExperimentValues and that returns,
inside there you'll build up a pair
of, in this case, will
be the number of elements
that we want to sort, and the second value
helps control the way the test runs.
And I will demonstrate that.
Here we're going to start
sorting an array of size 64
and then scale that up to
an array of size of 4,096.
And now, when we create our benchmarks,
they're all using the test fixture,
and they will all be sorting arrays
that will get larger
as the test progresses.
And those times will get measured,
and, excuse me,
and sorted accordingly.
That second parameter allows us to vary
the number of iterations that
we're gonna perform each time.
Excuse me.
(coughs)
The total number of iterations
you may want to scale,
because things are gonna
get longer and longer
as size increases, so here's an example
where a problem space is increasing.
1,000, 2,000, 3,000, 4,000, et cetera,
and then I'm going to lower
the number of iterations every time.
That way, I'm not using up as much time
to collect the measurements
that the body of that
function is gonna take
a lot longer to run as the
problem space increases,
so I'm not gonna run it
as many times per sample.
That way we can kinda
control some run times
and those types of things accordingly.
And I will do another complete
code walkthrough of this.
Again, same headers we saw before.
Then we have our test fixture.
The getExperimentValues is filled out
so we can scale up the size of our vector
as we are progressing.
We've got a setUp function that's going
to set the size of our array,
resize the array itself.
And then we will
effectively start the timer.
OnExperimentStart will get called,
that's gonna generate
different random numbers
for every sample there,
every iteration, I'm sorry.
And at the end, we'll clear out the array
and we don't really have
any tearDown actions here.
So by the time we get in to our baseline,
our BubbleSort, it will be ready to go
with a sized vector
with random ints in it.
I went ahead and set up some constants
for the number of samples and
the number of iterations here
as I wasn't varying that for this example.
We will do a SelectionSort
and InsertionSort.
And then QuickSort and standard sort.
So that's a good sampling of
standard sorting algorithms.
When we run all that,
you may be able to see that
the BubbleSort gets ran
several times, each with
an array of different size,
and then we mark all
those baselines as 1.0
and measure the microseconds
per iteration for each of those.
Just with the sample of the output here,
you'll notice everything
is green below that,
because BubbleSort is the
worst way to sort these values.
Everything beats it.
Some guidelines whether using this library
or another library, is
make sure that you're only
measuring Release/Optimized
types of builds.
Make sure that the
scaffolding that you've added
to your baseline is
repeated in your test cases
and vice versa.
Make sure you're using a
realistic problem sizes
so you can see if things
are scaling appropriately.
Make sure you use more than one tool
regardless of what you're picking.
So with any of these tools,
for me something that
tends to get left out
is the part we actually
wanna look at the data.
A lot of people end up
writing post-processors
and parsers and little MatLab utilities
and all sorts of things to
suck data in from applications
that generate good data but don't give you
any way to look at it.
And so in my library, I wanted to try
to address some of that.
You have to still
understand the data though.
So I mentioned this before,
but remember that the
scaffolding is included.
And if you have lots of extra
calls to DoNotOptimizeAway,
I don't suggest you really
put a lot of credence
on the absolute times
that are measured here.
Instead, look at the baseline
and the relative performance.
On the other hand, if you
don't have a lot of scaffolding
and it really is just your core algorithm,
you can look at those absolute times
and start to make some judgments on that.
When you run, beware of
suspiciously consistent results.
This is a demo application that you'll get
that tries to measure how long it takes
to pass a variable into a function.
Now that should be very fast
and very hard to measure.
Here I've decided to pass a 100 variables
and measure that time, and
I measured it 1,000 times.
And what you'll see is that those numbers
are all so suspiciously consistent.
And they're very round,
and I would expect that
we'd see some more variance
in the numbers,
and I think what's happened here,
again going back to intuition,
for better or worse,
is that this has happened so
fast, I've done so few of them,
I'm just really measuring
how long it takes
to start and stop the timer.
So, we're gonna bump up
the number of iterations,
here I can barely read it
even on my screen, 100,000.
And now the numbers have
some more variance in them.
And I start to feel better about this.
So I'm not even sure that
this is right at this point,
but beware if your results
are overly consistent.
I would question that.
If you just look at the
Celero command line,
there's a standard list test,
you can look at all the groups
that are in there.
There's a couple outputs
that you'll be able to get.
And one is just gonna be a table output
and others JUnit, and there's also
an archive file that I will talk about.
The CSV output is really easy to read in.
It's got a couple columns in it.
The first set of columns
are kinda just set up
what you were seeing at the command line,
your group name, experiment
name, and the problem space,
the number of samples,
the number of iterations.
The next couple columns
are gonna be the actual measurements.
And then at the end are
going to be some statistics.
Now I'm giving you statistics but realize
that we should not be looking at the mean,
we should not be looking at the max,
the number that we care about is the min.
Do not average your results here.
Only take the minimum result.
Why do I say that?
'Cause if you remember our fake equation
that I came up with,
we want all those terms
to be as small as possible
except for the actual measurement.
So anything that's larger than
that is a source of error.
So you want the least amount of error
which will be the smallest measurement.
I do provide the statistics here,
because it might be good to
know what kind of spreads
you're getting if we need
to do more iterations
or more samples to make
sure that you're getting
good statistically sound results.
(clears throat)
But really, you just
wanna look at the minimum
for absolute costs.
Then you can pull that in,
here I pulled it in to Excel,
set my axix to be log two
and plotted the results
of our sorting test.
Very easy, didn't have
to write anything extra,
just open it up in Excel, build a chart.
There's also another
GitHub project out there
called pycelerograph, it uses
Python to read those files,
and gives you a nice webpage
with Bokeh graphs on it
that you can zoom in and zoom out
and pan around and all
those types of things.
It's very cool,
and I appreciate those
folks for writing that.
There's also an archive file.
And this is something that
I've been playing with
to try to help integrate
this type of testing
into continuous test.
So Celero's gonna update the archive file
every time it runs, keep
some historic information,
and also keep the most current information
as far as measurements go,
and store some more
statistical measurements
along with that as well.
Again it's CSV, so it's
gonna be very easy to parse.
It's got a lot of the same information
that this table output
along with a lot more
statistics information,
and then information about
your historic maxes and mins
and those types of things.
But if you don't wanna write a front end
to look at that thing and monitor that,
we'll also output just Junit results.
So if you have continuous
delivery stuff already in place,
chances are it can read JUnit outputs
and so you can just
look at accelero results
directly in those tools.
Which brings us to
integrating all of this stuff
into continuous test.
Which is what kind of started this for me.
It was getting very difficult
to know in the code base
when things were getting faster and slower
and when somebody checks something in
that looked like perfectly legit code
but completely killed
performance in critical ways.
It took too long to find
out that that happen,
and to trace back to what
the actual cause was.
So how do we get this
into continuous test?
Chances are, a lot of you
are already doing this.
And there's a lot of third
party tools out there to help.
I've got experience
with Bamboo and Jenkins,
but there's others out there as well.
I think we could also
write new plugins for these
to help visualize these
results specifically
because there are a
slightly different flavor
than a lot of just rugged
or unit test results are.
Something that I think could be improved
is that Celero should be able to track
failure conditions for JUnit.
That is set up a baseline that
you don't want to go below
this measurement, or
you don't want to exceed
you best performance by more
than 5%, things like that,
so it can actually set up a JUnit failure
and get those failures
reported in your test system.
And there would just be some
an enhancements, I think,
to the archive file
and some of the set up.
But I think that would be something
that would be very helpful to add in.
However, benchmarking is
different than unit testing.
It really wants to have more
control over the machine.
I know our servers get
bombarded various times
throughout the day with
other people's work.
It's not such a big deal
when you're just building
and running unit tests,
but if you're executing
performance measurements, it is critical.
And so what are some of your options
for dealing with that in your
continuous test environment?
Spoiler alert, I don't
have the perfect answer.
One of them might just be scheduling.
We schedule our builds to
happen at different times
of the day, maybe four in the morning,
the servers are generally
not doing anything.
Let's run our performance
measurements then.
It may also be possible
to check for CPU activity
before running the benchmarks to make sure
that the systems are relatively idle
before you get started.
You can also just be greedy.
And set your affinities
and process priorities
and stop everybody else's builds,
but don't worry about it.
That can be very dangerous.
You've got a really high
priority, and you screwed
something up in your test,
and everyone will let you know about it
because the system will still be hung
when everybody gets to
work in the morning.
And then it's very hard to kill,
and all those types of things.
I think there is probably
a happy median here.
You can probably bump up the priority,
and that's something that
we can add to the library.
But I'm not sure it
completely solves the problem.
Unfortunately, where we're at on our team
is we just have not integrated
this into continuous test.
We'll just run it ourselves 'cause we have
a lot more control of
our individual machines,
and I'm going to lunch,
I'll run the test while I'm at lunch,
close my internet connection,
shut everything done, I've
a relatively clean machine,
and upload those results.
It's not as automated
as I like things to be.
But right now, that's kind
of where we have to be
on our team.
Just to recap.
There's a lot of
challenges when we're doing
microbenchmarking, independent
of the architecture
that you're using for that,
always measure performance
before beginning optimization,
and inspect the assembly as you're writing
these baselines and test cases
to make sure things aren't
getting completely optimized out.
Celero itself, very similar
to Google Test in its API.
Can use measured or absolute baselines,
which is interesting.
Try to drive for consistent results.
You build up a test suite,
run it two or three times,
make sure you're getting the same numbers,
and if you're not,
you're gonna have to play
with the number of iterations
and the number of samples
to try to get that
to be more consistent.
What needs done in the future?
I think, DoNotOptimizeAway
itself is a little heavyweight,
and we need a lighter
weight way of doing that.
I've lacked the creativity or desire
to spend much time looking
into a better way of doing it.
Automatically tuning the number
of iterations and the samples.
There's a little bit of
that code in there now,
but I think someone with a
strong statistics background
will be able to help with that a lot
to kind of be able to tune
those things automatically
so that you would be guaranteed
more consistent results
right off the bat without having to play
with those numbers yourselves.
Being able to add goals or
some type of failure condition
to the JUnit output to
make it easier to integrate
into continuous test, I think
would be a big improvement.
So that's it for my talk.
Thank you all very much
for showing up.
(audience applauding)
Thank you.
And we have a few minutes for questions
if anybody has one.
If not, then enjoy the
rest of the conference.
- [Man] Hi, thanks for the presentation.
Can you talk to how the
test are actually run.
Does that table come up all at once,
or does it incrementally come as results?
Or do you interleave the
test that you register?
Right.
So, that table that you
saw, the colored output,
it displays that as it's figuring it out.
So when one baseline completes
all of its iterations, all of its samples,
it outputs that while in a result.
So you can kind of
monitor it as it's going
and seeing if things are getting horrible
and you need to stop it or whatnot.
Right, I'll be available afterwards.
Thank you all for coming.
Thank you.
(audience applauding)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>