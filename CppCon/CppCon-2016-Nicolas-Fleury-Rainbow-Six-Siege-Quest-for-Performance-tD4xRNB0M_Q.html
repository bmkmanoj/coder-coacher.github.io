<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Nicolas Fleury “Rainbow Six Siege: Quest for Performance&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Nicolas Fleury “Rainbow Six Siege: Quest for Performance&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Nicolas Fleury “Rainbow Six Siege: Quest for Performance&quot;</b></h2><h5 class="post__date">2016-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tD4xRNB0M_Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so let's start so I've won my name
is Nicola figma technical architect at
Ubisoft Montreal's another to be with
you today two years ago I made a talk at
CPP con and it was about how we use C++
and the big Triple A games that we make
at Ubisoft Montreal but today I want to
talk about the game have been working on
for four years now which is Rainbow six
siege so what is Rainbow six siege
tactical first-person shooter and the
main mode in Rainbow six siege is really
five versus five and it's running at 60
FPS so 60 frames per second so less than
17 milliseconds per frame so we can
imagine our precious every millisecond
is the game is built around the concept
of the siege which is often switch to
defense and procedural destructions the
core gameplay mechanism so basically in
siege when you breach inside a room you
don't only go to or door or window ball
so through a wall or ceiling and I can
say that the game is actually success
was released on December first last year
so over nine months later with the lab
over 1 million players playing the game
each day this kind of numbers that long
after shipping again these are never
seen numbers ever at Ubisoft and a lot
of things that we made to ship a 60 50
FPS game that worked really important
but I won't not cover them in my talk
but I want to at least mention them
quickly so I will not talk about our
checkerboard rendering technique you can
see my friend Jolla GDC targeted but
basically we completely render only half
the pixels in a frame in the
checkerboard pattern we made
optimizations in a bunch of different
systems a lot of people did that but
three specific to our game I won't cover
tasks your dueling strategies now in a
60 FPS game not everything must be
abated at 60fps could update some system
at 30 2015 11 2 fps and I want to
discuss completion flag swing time
optimization and this kind of thing that
we've used to make or gain
as fast as possible so I want you to
talk about performance and focus on
things I think are interesting for any
C++ programmer interests in performance
not only people in the gaming history so
that we're on the same page I will start
by making an overview of our situation
and also the workflow for performance on
Windows is then forming relocations I
will discuss are we both reduce a class
and our number then we'll conclude with
some large resolution that we've used
and a lot of stuff in my talk so please
keep questions for yen so first our
situation and workflow two years ago I
presented this picture of some
frictional adware made of six cores we
three different letters of memory cache
the point was just know where the core
is accessing memory it might need to go
to different layers of memory cache and
the deeper needs go the slower it gets
and basically you should design your
software according to this reality so
with the permission of Microsoft and
Sony I can tell you today wet sore
reality for the current generations of
PlayStation 4 Xbox one and it's not that
far from that so we have eight cores and
said and the cores are clusters of four
cores actually the l2 cache is on the
cluster and since there's only two l2
cache instead of having an l3 cache we
have instead a bridge between the two O
two caches but the bridge is slower so
chemists actually personally as a
software engineer I see it as if we had
actually an entry cache and since we
have two clusters or four cores can make
sense
to assign specific jobs to a specific
cluster if for example the job is
anywhere accessing an atomic values it's
better to compete for the exclusivity of
the cache line and to end up with the
speed of the l2 cache instead of the
bridge the speed of the bridge being two
to two or two caches so every specific
stuff you might assign me to specific
cluster the last quarter don't have
access to it so the console is using it
for reading system whatever call it the
seven core is very interesting on both
consoles we get partial access to it so
we get access to it for certain
percentage of the time
which means it can be preempted very
aggressively in the middle of something
maybe not really cool while adding for
example the ownership of a mutex so
basically all the code the core that you
run in the seven core it must be like
free but the real definition like read
must be possible at any moment without
affecting the first six cores otherwise
we end with the priority inversion so
there's various really specific systems
that would run on the score and the
casein dosage should force me to
introduce some largely solutions so real
specific stuff on the seven core stuff
afford on this persist course maybe on
this first cluster and of course we
could add affinity on different course
then I want to mention very quickly or
different targets because I will mention
them again in the talk so then my first
grab or debug bill I mentioned two years
ago we use one level of mining and debug
so obi-wan compiler flag with the
Microsoft compiler the realest bill is
what you release on the production floor
this is not something released on the
public so basically the same code they
still as a search in the bug tools it's
just that a code is optimized so when I
say release target for the rest of the
talk this is kind of a debug build
optimize the profiling bill has not that
much utilities left I just as the
profiling utilities but otherwise it we
try to have a performance as close as
possible as a final build which is what
we deliver to the public so of course we
haven't been more target than these win
our final bill with link time
optimization or a final bill with more
information in crashes release build
with some projects to optimize but
overall these are all four main targets
one of the core system that was used on
Rainbow six siege on the production to
make a sexy FPS game is a unified
telemetry system the concept is to have
a single channel for all telemetry data
coming from different systems so this
system will use timestamps that you are
universal not only across them but
across different processes and machines
as well so we can log that tell me two
data on the server or locally if needed
the point is
if people are playing your game want to
be able to reproduce want to be able to
fix performance issues without having to
reproduce the issue so once you have all
the information for any drop or frame
rates happening on the production inside
you deserve without having to reproduce
the issue so if suppose here it's not
playing that well and suppose we have
people playing the game it's possible
for programmer to navigate through
different sessions to different players
and they to see if there's been any
framerate and that doesn't look normal
and as we dig here we have it you can
see destruction event this is some
spikes in particle system you dig more
you can see there's a CPU spike and
under it we something blue it's a screen
shot profiling screen shot made by the
engine when it's realized that the frame
was longer than accepted then you can
click on that and it will open that
screen shot and what we see here each
row is at red and we can see the
different function that been called and
one at the top is basically the task
from our scheduling system and other it
these are functions that were called
these are not actually exactly functions
the are these are profiling tags that we
define ourselves in our code so we'll
try to define the tags for something
taking more time than let's say a
microsecond because I can this may be a
bit too short the system support events
as well we're just like punctual things
happening so if we look at the
screenshots we don't see that well but
there's a few circles at the bottom
these are events we don't use that often
otherwise this is all the tags and we
see two rows are our two different
threads the system support printf style
panics but they are done offline in the
tool because we want to minimize any
performance effect in the engine so in
the end we end up with a system which a
pretty low CPU overhead and preemie
little memory usage as well
this is really important because we keep
a circularbuffer all the time in the
engine even on consoles to be able to
dump
these profiling screenshots at any time
it support context switches on all
platforms so basically if a core is
preempted by the system or waiting in
the mutex things like this when there's
a deadlock we can ask the system
dynamically in the engine for the two
profiling tags that seem to be
conflicting with each other so basically
the one had been running for the longest
time so that we can create a bug that is
more specific to that deadlock instead
of having a single bug with all that
locks any supports work we can see
counters to make graphs as well so then
when it comes to use all this
information to improve performance
improving performance and readings a lot
of people to some what some people
already said is that to me is a lot
about measurements event when you want
you know what you're doing you need to
measure performance improvements I
really give an example the string class
in our engine is really simple as the
shard pointer pointing to a buffer in
the heap at the moment we have no member
with the size of the buffer size of the
string and we had some performance
issues and you I called this pipe is not
copy friendly and it were copying
strings just too much so I decided to
try very quickly to add Nemec ref count
of two bytes at the beginning of the
buffer then I run tests and the result
was the UI code was running now 15%
faster but the overall engine was 130
percent slower
I was able to reduce that wonderful
percent something pretty much zero by
not using and chemicals when the string
wasn't at the edge share so but
basically put a huge value meaning your
count is one but it's not yet shared but
still if it's a zero percent improvement
over all that it'll make sense to go at
that change so then we'll optimize the
UI code instead 12th added a subclass
for a string class call and play string
that we have an embedded buffer that
will be used if the string is small
enough and string format which is or
version of s and printf under the hood
is using a circular at read local
circular buffers buffers and this is a
technique we've been using for a long
time in our engine to return char
pointers by values just that the caller
needs to understand this is just valid
for a certain amount of time
but then are you measure such a change
when you make a change and you want to
know very quickly if it's a good change
so in that case or soos control is
perfectly and we run a test we ask for
performance tests this will launch a
tool that we call performance tester and
in the upper left corner we can see that
the two setups that will be compared
basically by default one is without any
of your changes while the other one is
with the changes you just selected and
your probe corner will choose in which
platform which tasks fall along so
typically in my case I will run the test
for 10 hours be on my PC before leaving
at night then I will come back in the
morning and have HTML report open and I
could see the results when such
terracing here is at all the different
individual test results are actually
embedded in the HTML as Jason the reason
we do this is to be able to remove worse
results for best results from the
numbers that were seen when we first did
that we thought maybe during the night I
don't know Microsoft Outlook is due
increasing indexing and it will affect
my tests but then we realize that what
we did care was actually the worst
results the reason is that in Rainbow
six siege on the console the game is
running a little odd at an average of 70
FPS so there's a 10 FPS buffer for
things to happen to our AV Destruction
things like this to happen and when we
have a framerate drop under 60 it's
typically because all the planets are
aligned in a way for example that the
same mutex get locked on every thread
the same time so we do care about the
worst results so actually ideally
performance improvements will improve
both the average and worst results which
we could afford if it will make the best
results worse we are added later in a
project and we embedded a graph inside
the HTML so we just sliders because very
quickly remove mostly best results
in the end were ending with the tool
really easing integrating and
performance improvements and also kind
of prevented some bad or unworthy
optimization to come in especially when
we started introduced some large
resolutions sometimes we just change a
type of contention we have and there's
been situation we're expecting a change
to improve worse results but the great
best results but when we tested where
our real application it was the other
way around so it actually prevented some
optimization to come in that you know
with normal test they will look like
they were doing fine so so that's about
a workflow for performance but a lot of
our performance issues are with memory
allocations so we address them by both
reducing that cost and their number so
why do we care about memory locations
this is a screen shot of profiling
screen shot from Rainbow six siege a few
months before shipping what we see here
here on all the light grey areas they
are all these are all context switches
so basically the CPU is in that case is
waiting in a mutex that if you look
closely it's inside of a cuter so we can
see all the amount of wasted CPU we have
we cannot afford this what we add an old
generation set up so it's box 360
Playstation 3 we're using a branch of
the SSS green engine and all the smaller
sizes they each have a dedicated
alligator under up to 64 and over that
we have a few sciences with dedicated
alligators as well and we have a bunch
of gorillas characters the first thing
we tried was pretty much to replace
everything by Jam a lock which as under
the hood already these no pages for
small allocations but it was giving us
worst results so it became obvious to us
that we need to iterate over or existing
set up but back in the day all these
small sizes alligator
we are only at 500 Meg's of RAM on
previous generation so we're using them
for mostly two reasons to reduce the
memory consumption since you have no
errors no through applicator with size 8
its you don't tell any other and also to
remove fermentation we don't want to add
these small allocations inside more
generic allocators but now we have 10
times more memory it's less of an issue
what we want the cpu power back so we
can sacrifice memory so what we did is
that we made all the fixit's idolaters
lakshmi even a weight free and we we add
a queue inside each of these alligators
and when we delete we pass a pointer it
needs to find the alligator so there's a
correspondence to made so we make sure
that all of this was largely as well and
we also simplify a bunch of code layers
when allocating memory so then the
function choosing an alligator is
typically in line since often when a
cait memory you know the size is not
like a part time offense the size of a
class or sizable class multiplied by
constant so make sure that in all these
cases it's directly the alligator that
appears end up in the code two years ago
I mentioned that for arrays arrays being
or vector version of STD vector if the
arm the stack we use a special alligator
since you know it's very silent we have
something on the stack using the heap
since it's very temporary usage and we
kind of improve over that in the last
two years so what we do now we will have
a tread low caloric heater and won't we
don't put that much memory in it and
it's basically like a second stack so we
just but we don't unstack we just move
forward and that buffer but haven't ever
at the end of a task which is LaunchBar
scheduler we just reset that alligator
so in the release bill will you know add
additional information to count the
number of our number of the elites and
our girls right sure that at the end
when you reset it's actually zero but i
want is in the final bill the leading is
doing nothing so under the hood the area
will look if the disappoint errs on the
stack and if
it will use that tree local alligator to
look here for pointers on the stack we
avoid system calls
they tend to cuss too much so what we do
is that we just add a shredder called
variable that we just update with the
bottom of the stack or very close to the
bottom let's see when we create a trend
and then later we can compare with that
tre local variable no way from the snap
but when you go to compare with the top
and the thing is a lot of most of our
areas are not on the stack so what I
even want to avoid accessing the trailer
code cache line for nothing so the trick
we do is that we look if or 1 Meg away
from the top of the stack and if not we
don't even bother looking at the thread
the other trailer called variable
critical variable so we end up with
something very casually we reuse the
same cache lines over and over for
different tasks very fast allocation is
just one luttrell on call access and
moving the pointer forward and then note
radical access if the array is not on
the stack frees the wing nothing there's
no contention it's all trade local so
reducing the cost of allocation is nice
removing them all together is even
better and the first tool we have to
address this is our telemetry system so
our telemetry system is supporting to
log every room location where the
complete call stacks it's a bit heavy
it's reducing maybe the frame rate by a
third in our case but we have a 60 FPS
game so it's still playable so we'll
make a game and then I could see all the
allocations they will you know we're
shown in the tree like this with the top
of the stack first so it's very easy to
see which line of code is allocating too
much and also which product is
allocating too much since you know the
thing to have a book to for product one
product is internal or internal
attributes laugh it's basically our
internal flash player they were
responsive as producing their number of
locations Rainbow six siege
divide them them by 100 so for five
months five minute game they went from
300 thousands allocation to 3000 3000
this is one allocation per second for
flash player responsible of IUI you see
in the game I think that's pretty good
need to go further I think we need to
get rid of flash so Dan yeah so then I
use the tool to course fix a lot of
issues all around the place but then it
became obvious has were closer to ship
that I needed to sort these allocations
by allocator because what I want to
address are the allocations with the is
cos it's not only about this no it's the
CPU the circle thread it's about the
contention they create and I already
said all the fix it's ours alligators
there are wait free so that contention
cast is a bit smaller so I need to
address you know the allocations in the
most generic alligators and the one with
the most and I'll be honest for some
systems that were just doing too much
locations didn't have time to refactor
them so I just gave them an alligator to
not mess with our educators and then
there are the errors so I can array is
like our STD vector like class and it's
the same concept when you add elements
you need to reserve before you know you
push back elements otherwise as you add
elements it will relocate and reallocate
and reallocate the buffer if we even
ignore the fact that it's copying things
for nothing you're also creating
contention with order treads with other
course with no good reason but how do
you know the reserve size is up to date
I even see code some time doing this
kind of the same code pad twice first
time to know how many elements will be
added and second time to add elements
but in any way you do this how do you
know it's up to date two years ago I
mentioned we use something called
in-place array with compile time size so
basically it will use the embedded
buffer it as if it's small enough
otherwise it will see
Hiep but that sizes compile-time
and in a game like Rainbow six siege the
real life is the five versus five games
it's an order for programmer to know
what is the usage of that area so for
example if that X are a ninety nine
percent of the time it end up it's
ending up with less than nine elements
that this is a great line of code but if
eighty percent of the time it's actually
bigger than eight then this is probably
slowing things down so what I will want
is statistics for all the area
declarations in the entire code base and
I was thinking about it and I said this
is what I want that they have in c-sharp
when you can put an attribute on an
argument and the compiler will pass the
file and the line number that is calling
the current function in that case the
area constructor but I'm not finding a
way to use the professor easily to have
the same behavior but I did found
something and it's different on
different platforms on the windows
platforms it is the return address
function so return address is returning
the instruction calling the current
function so if you disabled in learning
is especially the era constructor but
you could disable a learning all over
the place and this is giving something
very close to you an ID for the array
declaration so suppose I have a function
with two arrays it will be two different
instructions calling the two different
array contractures in the case of a
member and it will be the instruction
inside the class constructor then you
might say if you have multiple
constructors then you end up with
multiple IDs well yes and no when you
analyze on your tool or the information
you will see these hard constructors and
it will construct the array in the exact
same order the order of declaration
inside the class so we can merge all
these IDs together so under the hood
inside the outer class there's a
aerostat structure being if left for
that specific feature and then the
different area functions we are just
feeling the statistics and the
constructor we call a macro so to pass
the return address the name of the value
type it's really nice a type ID actually
works for compile-time stuff even if no
case with usable or TTY we added a linen
for subclasses of re because nor case we
have some like in place IRA ptra so we
pass also the name of the area type then
letter in the destructor when all these
stats have been filled we just copy them
to a butter is very simple we just a
locate one gigabyte and we have an
atomic index that we just move forward
so then we can dump statistic merge them
in a compact format and we can give a
bill to tester to make a 5vs5 game and
they come back with ten files one for
each player and we open these files from
our tool call the Aaron Eliezer and what
we see is exactly what we wanted these
are all the array declarations entire
code base with all the statistics so
when I ready here will be shown as this
is the first siren that function or the
secondary that function or the sixth I
ran that function and you can see the
type of the area so just with these two
information it's already pretty obvious
which array it is in the code but even
there we even added support for
double-click to to use PDB information
and just open the corresponding line in
visual studio but then you want to sort
these areas and there's so many you want
to start with a worst offender but
there's so many ways to be an offender
so the first thing we did is to add a
combo box at the top with different
situations that will result in different
ways to sort Gary's so for example the
first one easy Reaser fix reserve count
is always more than one second one big
reserve not enough we need to initial
big size gary whatever size even bigger
third one no brainer in place the array
is always on stack and the resource
science is always small so we should
just use the in place harry class i've
shown earlier and we didn't count in
some area functions or are doing linear
searches they're not after enough time
you're doing them so then we can if
there's an i number of elements in the
area who can tell maybe here
should consider using a different
container suppose I take an example this
one doesn't have that much statistics so
it's easier to show this is a third IRA
inside a function so it will alight and
read the statistics that seems to be
wrong and in green what you should look
at the fix it
so here the reserve count was 232 times
so this be 32 instances of that array
and in the destructor when we register
the statistics has always been reserved
twice the deme account is not always the
same thing you can see the distribution
so it was five four time seven nineteen
times and eight nine times so this is
the number of limit in at the moment we
use just a stick in the destructor the
peak buffer size we can see it as been
eight four times ten twenty times and
their first reserve size here you can
see not a constant in the code it was
for four times and six twenty eight
times
so obviously the code is trying to take
you know a proper decision but obviously
it's not the first time I saw this
increased four to eight and six to ten I
was a bit surprised I thought were
increasing or every size with a factor
then I look in the code and actually
within sundar sixteen we just had four
so of course when you first ran the tool
were finding much worse cases than this
one no cases where no reserve at all and
a lot of allocations but these
allocations will you might still find
them with the telemetry system I've
shown or normal profiling at the ayat'
cost but I think we'll find them what I
like about this one is that I think you
will not fix it at all without that tool
and you're still dividing by two
youngberry applications and plantation
contention you are creating with other
course so we ending up with the tool
were optimizing arrays all over the
place and the changes could be made by
much more junior parameter so the costs
of fixing all these usage of Ares was
much lower than before now quickly want
to mention an issue with SD function you
know a steady function typically it as a
small bub object
buffer optimization in it and who make a
place meet new the functor you assign is
small enough but then you know your code
evolve and suddenly it's no more using
the object buffer it's now and it's not
using the heap so I imagine I'm running
that each for each frame I already told
you our flash player is doing one
allocation per second there's no way
like said that this line is doing 60
more times allocations so I discussed
the issue with on the study group 14
dedicated to low latency and basically
everybody added read done their own
implementation of s3 function to avoid
issue so I'm at a quarter of a proposal
with call cook and it moved forward at
the meeting today so we proposed to add
in the standard library in place
function and that's as the exact same
interface sixth Abbot will never use the
heap so if you assign something too big
it will not compile you will get a
static assert and then you can you have
a national template argument that you
can use to ask for a bigger buffer so
well obviously we miss the C++ 17
deadline so if you're interested don't
feel free to grab or example
implementation who basically took the
good ideas of a lot of people and put
them all together and now I want to
conclude the talk with some large
resolutions I always said the seven core
for SAS you put three solutions but the
ended up being useful much more than
just a seven core but before I want to
present a tool that is very similar to
the array analyzer tool I've shown it's
our tool call the lock analyzer getting
an ID for an area was a bit tricky but
from new tags I think it's much more
simpler in our case mutexes they tend to
be inside managers which are single
turns as sometimes you mean tax is
static global
and three where we put them in objects
and even we do that the object will live
for an entire frame and when we lock
them which means that the pointer is you
know we already good enough ID and when
we lock them we often use a macro like
this one so in that matter we have
access to the name of the mutex and also
the file that is locking let me text who
we all add which we add all these
informations and start the profiling
files we can then parse them on the
network for different plus sessions and
in our tool called like an answer it can
deduce all the different flocks in the
entire code base so for each one we can
see you know I mean are much time we're
wasting waiting on them it's supposed to
take an example here this is our endo
manager lock and it's showing a call
stack this is a profiling tag call stack
and we can see at the right it has been
lock for most one millisecond in a frame
at some point when I'm clicking on this
actually there's somewhere else in the
tool I'm seeing all the worse situations
with the first one being the very worst
one and there's a polling for each north
case of tools called your studio and
will zoom the region where other stuff
happen so I click on it and I can see
what happened so again all the library I
reinstall all context switches and we
can see tree treads fighting for the
same mutex inside our endo manager so
earned all manager is based on object
IDs and at the moment it's not let's say
100% lock free but actually I added on
the project a lock free and all
reference system so it's basically like
atomic share printer and weak pointer
but with a bit more intrusive and the
Scopes being entire frames so here the
solution is to just use that smart free
in dog system instead it's based on
pointers and not object IDs and it works
because in that case the shape is
actually created on the runtime we don't
need the burden of checked IDs and this
last we end all in reference system was
is
based on the hood by with two simple
large food containers that we use and I
want to quickly present them from now on
I'm showing a bit more code in my slides
don't read the code too much we focus on
what is highlight in red of the code is
read there for reference for later also
I mean I will use a default memory order
for the Atomics just for simplicity so
our first container is the last week you
and it's a good example of how we like
all our free code this is really simple
it's just tree item X variable and you
also define the maximum tread count so
basically a concept is if there's only
that left in the space and the buffer we
just consider the queue full that way we
can simplify the code even more so when
put elements in the queue the code is
very straightforward we just modify two
atomic values and if the queue is full
would just crash when we remove from the
queue really straightforward again we
just modified two atomic values what I
want to point out is that I don't think
you should modify an atomic values
inside the loop since you could create
no contention with other cores over and
over but this is a common use case with
a queue to want a lot of thread feeding
the queue in a single one consuming
everything so to do that where we added
this very simple object we just declare
on the stack the constructor is loading
dynamic values and the destructor is
consuming them so we add support for
fermions loops for convenience and then
there's our luxury pool typically non
large people will use the same memory
space for the node and the objects so
basically a pool would point to their
first three node will point to the next
one and so on
so the node will be as big as the type
of objects you want in your pool and
when I've seen these typically people
when they want to lock people instead
they have a buffer for the objects and
they combine that with a lock for Q but
I wanted to avoid the memory overhead of
a queue my idea at first was that you
add in the pool not only the index or
the first we know ball so the next next
we know the idea was to just put these
two things in the same 64 bits and
update them at once make some tests and
actually that wasn't exactly working
still add a a be a problem what's
possible for treads you come back to the
exact same values and the solution is
much more classical and simpler actually
is to use a version counter so if you
don't know about a version counter the
concept is to have a part of the atomic
value variable that is updated each time
you update it so the only way to create
a bug is is that value is looping around
but with 32 bits you will need thread to
sleep while all the arbitrate are doing
four billion ish changes are coming back
to the exact same sticks for his bed I
think we're safe so then the code is
pretty straightforward this is very
normal
kyouko a pool cue a pool code sorry
so we suppose you want to take the first
node and give it back as an object you
just want the pool to point through what
is the next three node after that and
you turn the node as an object so then
the code is pretty straightforward you
just copy the 64 bits you work and then
you make your contains change if they've
been or contains change between these
two lines of code then it will pass
otherwise you retry your conference for
loop things to mention here this is
where the data policy inside the pool
can decide to draw it if it's supported
and also we increase a person counter
before updating doing the company
exchange when we want to give vana back
an object to the pool we want the pool
to point to that that node that was the
object before and it will point what was
before the first three node so again
it's straightforward capitis it's four
bits and we work and you make a campaign
change we will pass which to seed and
there's been no campaign change between
these two lines of code that are in red
things to note in here the pressure
encounter being incremented but also you
want to update the index inside a node
before committing the node to the pool
so is it worth it to have that flag
people compared with large
EQ and an object mushroom well the lack
vq is actually can actually be weight
free it's shown an example where it's
weight free so it could be a better with
maybe a lot of course a lot of
contention but it adds a memory over it
and there's a consequence to that is
that the large people is actually
accessing one less cache line for both
operations so in a real life scenario
and the case from the game like Rainbow
six siege if that cache line is cold
then the large people would perform
better even if the cache line is art is
still removing you know cache line for
something something else that could be
hot so do you really need to test your
real application so basically if you
don't have that much contention I think
it'll actually pulls a written
interesting solution if you have a bit
more then yeah it you have maybe you
want to use a weight weight freak you
combine it with a buffer but you have
even more I just want to point out that
I think you want something to add lock
Hall combined with whatever maybe it's
at radical queue that is using in worst
cases a global queue and then you need
to decide when you update these trade
local queues so for example a profiling
system it's all waking working with read
local data we don't want our profiling
system to add contention it's there to
measure all of this so my point here is
that way free you know with a la mix
it's not free if a lot of course
accessing the same atomic value you will
end up with the the speed or of the
cache it is no slower so that can be
ultra cache PC or the bridge between the
two wheeled caches I've shown on
consoles I want to open a small
parenthesis two years ago I've shown a
locator that we use to debug memory
corruptions concept is then when we
locate an object would put it on the end
of a page and make the next page with
only so if you right pass objects will
crash in the guilty code and when we
free that object we'd make key the piece
this page we don't leave for some time
with Q and kind of improve over that now
we use uncommitted
pages and they are committed forever so
it's the memory addresses are just and
valid so he cannot even read seven
better and when we free the object we
Justin come into the page forever we
don't need a cue so the code is very
straightforward it's just very heavy on
paging file size but otherwise we can
play our entire game with this and
someone asked me two years ago why now
where it's not using Page heat from
Microsoft and but to be honest we didn't
know about page sheet but we do
appreciate to have an elected or encode
that can you know trigger for specific
allocations and what I give an example
where a crash in March on a Rainbow six
siege only in the final bill the reason
it was only in the final bill is a bit
funny when we switch to 64 bits we use a
trick from Bruce blossom to reserve the
first four gig of memory addresses but
from the beginning it was if def to not
be in the final bill and we completely
forget to remove it there the
consequence is that all appointees in
the non final bills they were always
starting with one but not in the final
bill in the memory corruption was a one
getting written in a word only used by
pointers so we're in a bit of a panic
and we had crashes with no real players
around the world and my friend GP said
you know it's almost always crashing in
a sound class but nothing change in
sound recently so I asked him what is
the size of that class and he said
seventy two and already told you nice
size 72 as its allocator so I told him
replace that allocator by the page
protector locator and we found a bug
right away it had nothing to do with
sound it was a third party that was
expecting an object to live a bit longer
the object was created single time so to
what have been a real pain to find
without this so why am I telling all of
this this concept is really useful for
the pool if someone is riding inside an
object that wasn't the pool
by mistake after it has been deleted um
then he might corrupt the next he index
up the next node then I will end up
debugging the pool thinking I have a
lock free bug I don't want that so what
I did is that I added a policy that will
allocate complete pages for elements
that can trigger manually like this in
the code so the pages will be put that
will only when you know when the node is
free and when it's not then we change
that what I would do actually is that is
in the release Bill I will write special
values and all the bytes of the node and
when I'm giving back the known as an
object will validate all these bytes the
next index I could store store it
elsewhere to validate it as well and if
any of these bytes is you know the body
tradition is finding I will tell the
programmer assert and Teller primary use
that policy and you will find the bug
you currently have that policy is not
only used for that I check surely in our
profile in our final bill or pool
doesn't grow so we'll pass the size and
that's it if it's bigger than that it
will just crash but on the production
floor
it isn't that cool so we support to ask
ten times more elements we just log in
our database the call stack when we
first reach that size yeah so quick
recap so you can think about your
questions so gave an overview herbs to
initial Nana in a workflow seven core
for capacitors Dimitri perfect Esther
I'll reduce memory location or they cast
a number so some larger alligators tasks
liquor tell me to ionizer in place
function and some large solutions as
well so I think we have now time for
some question before taking questions I
want to thank some people I made a talk
inside Ubisoft
with Sebastian C and noisily Pascal II
and Ivan took some earlier slides
Sebastian work on the low-level memory
optimization and with Pascal de ballet
on the
implementation and Mauricio Pascal a
work was the architect behind the
unifying telemetry system and I want to
thank also John an email with which
pretty much made all the analysis tools
I've shown so thank you we I guess we
have timer now some questions how do we
know question is all the we allocate
okay I don't there's other questions are
you asking are we instantiate it's good
you seem to ask about alligators for
specific types but we don't have this
okay so the question is about our fixes
alligators are there I don't even know
if there's a template argument for the
size itself it's actually the function
choosing the Eiger is just very basic
code with switch cases and if-else and
then we have different instances there
are different alligators for different
sizes and and the code itself I don't
even know if it's a constructor argument
or or compile time okay cool
okay so the question is if our
performance tester is running test
locally or distributed we don't support
distributed yet we support running on a
Playstation 4 and Xbox one or PC the
next step will be to support distributed
yeah but it was good enough to run it
for the entire night so kind of live
with that all these tools were developed
you know in the last four months of
their project I would say maybe six
months so yeah so we didn't go with you
but sometimes I'm Fantasma about having
all this do we have the possibility to
push for profiling build to players that
I have issues the answer is no at the
moment we're discussing be able to
triggering some stuff in the final bill
we know for example we look at the code
of Google Chrome and they have a feature
to get call stacks their own their own
feature to get call stacks you know back
to be in the process and we were
discussing recently if we should do
something like this but we definitely
cannot deliver profiling a bit like it
is because they're shrinks all over the
place so at the moment there's no way
we'll use that but we could maybe in the
future have something yes
okay so if we're working Windows if you
look at using different tools on the
Windows platform to profile right we do
as well we tend to work a lot with the
profiling screenshots I've shown since
they work on all platforms and it's
often you know which task is doing too
much so the more classical profiling
with tools like V Tunes and all these
kind of tools we use that much less but
of course some programmers are doing it
as well and we use offshore of course
consoles too sometimes the tool or
console is also regrade since they re
care about you know you optimizing for
their platform so yeah it's it's very
organic we use a lot bunch of different
tools any more question yes so is the
input for the area noisy tool coming
from the telemetry their answer is no
yeah actually right now it's funny it's
actually in XML we were at first dumping
in XML and by merging you know all the
the distribution it was small enough so
we didn't even bother putting it in
binary so yeah so yeah at the moment
it's just a file we dumped and that's it
so it's not going to geometry but anyway
we give it we could do it into a mitri
but no since it's very special occasion
we make if that make a special build we
give it to testers so we can just grab
them the file from different testers so
no we're not going toward to meter
system any more question well thank you
for coming thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>