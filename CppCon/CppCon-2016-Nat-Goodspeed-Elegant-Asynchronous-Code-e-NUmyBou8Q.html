<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Nat Goodspeed “Elegant Asynchronous Code&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Nat Goodspeed “Elegant Asynchronous Code&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Nat Goodspeed “Elegant Asynchronous Code&quot;</b></h2><h5 class="post__date">2016-10-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e-NUmyBou8Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello oh the mic is live thank you all
for coming I'm glad you found the room
because you know if you ask a web
browser 404 is not found so here we are
I'm mad good speed the last nine years
I've been working at Linden Lab on
Second Life during that time I've
developed a sort of fascination with C++
concurrency you might almost say an
obsession mania but I'll let you
characterize however you want once we're
done to design a non-trivial program you
start off with pretty broad strokes and
a lot of hand waving you say grandly
that this subsystem is going to handle
this problem and this other subsystem is
going to take care of that problem C++
is an object-oriented language it
encourages us to compose objects
representing various layers of
abstraction these objects rely and still
lower-level objects and so forth to be
able to reason about a program's
behavior it is critical
at any given layer of abstraction to be
able to gloss over the fine details of
the layers on which it's built
similarly still higher-level layers must
be able to ignore the implementation
details of the layer we're talking about
at this moment what you do not want is
for the flap of a butterfly's wings in
one subsystem to set off a hurricane in
a completely different subsystem would
any of you guys consider a bit of pro
bono work to clean up the world weather
system it's huge it's old its massively
parallel and got a lot of Global's I
think we can all agree that this is not
the kind of unintentional coupling we
want in our own programs this is mom and
apple pie this is programming 101 but
programming 101 has been struggling to
cope for some
time now when you look at an
event-driven program it's organized
instead by when it must return to the
main event loop and when it hopes to get
control again an event-driven
applications have been with us for
decades moreover we're being asked to
integrate more and more functionality
consider type-ahead hints while the user
is typing into some input field the
application matches the partial input
thus far
against a list of likely candidates that
list is even more useful if it's being
dynamically fetched from a back-end
service instead of baked into the
application that involves fetching and
updating the display in real time while
the user is typing users have very
little patience for an application that
periodically freezes up while it
consults its back-end hosts they wanted
to be responsive 100% of the time even
if it hasn't yet finished fetching the
answer to its question in other words
users expect their application to handle
a lot of different things at the same
time most especially lively UI
interaction along with snappy web
service requests since C++ 11 to be
honest since well before then it might
seem obvious that the best way to
achieve concurrency is to introduce new
threads you can spin them up and join
them you have lots of available
synchronization primitives you can use
package tasks or promise and future to
manage background computation of a value
because the operating system manages
threads it can transparently do the
right thing when you issue a blocking
i/o call it blocks only the calling
thread not the whole process trouble
with threads is that there are two
simultaneous threads are truly parallel
not merely concurrent sharing resources
between threads quickly goes becomes
very tricky
a trivial example is incrementing a
counter we might write counter plus
equals one and think no more about it
but a machine instruction level that
increment operation might be implemented
as read add write and if two different
threads happen to try to increment the
same counter at the same time they might
both read the value both increment their
respective registers and both write when
of course the counter should have
reflected both increments and that's
without even considering the wacky
instruction reordering performed by
modern processors so we need to defend
every shared resource in any of several
ways some of those ways could be
expensive I do not work in an
environment with thousands of
semi-independent tasks I personally am
more concerned with the racing problems
with threads than with their performance
implications but some of you do and by
this time you're all too aware that is
the number of OS threads becomes much
much greater than the number of
available cores on the machine the
system starts thrashing because the
context switching overhead starts to
dominate those of you who work in a
32-bit address space with thousands of
semi-independent tasks are particularly
aware of the cost of stack space maybe
this is only really an issue with the
386 processor family as you well know a
typical c++ function call in a 386
machine involves decorating the stack
pointer so let's look at a 32-bit
address space a clever operating system
reserves a pretty good sized chunk of
address space for the process stack not
to scale it doesn't have to commit all
that memory not yet not until the
program actually uses it whoa big stack
allocation there and the next function
call pushes us over the limit not to
worry
the clever operating system seeing
there's no conflict simply grows the
range of address reserved for
process stack unless the program
continue this works fine as long as the
operating system is clever enough to
keep the programs other allocations well
out of the way what about when you
launch a new thread again you don't have
to commit all that memory immediately
but you do have to reserve a prudent
address range how about when you launch
another thread there's only one place in
the entire 32-bit address space where
you can hope to grow the reserved
address range on demand for any thread
other than the initial one once you hit
the limit you're done game over
you can't move a thread stack there are
potentially lots and lots of C++
pointers referencing its contained
objects you don't know where all those
pointers are so a clever operating
system will reserve a big chunk of
address space for a thread say a
megabyte but you know there are only
4096 megabytes in the entire 32-bit
address space and a lot of that is
already used for the program code itself
and for other data this approaches stack
management may strike some of you is
overly simplistic we could have a
fascinating discussion about alternative
implementations for a C++ programs
logical stack but the bottom line is the
operating system doesn't know them
doesn't use them and even if it did it
couldn't assume that your your
application used them this is still a
problem even if your system has lots
more physical memory than that the
limiting factor is the 32-bit address
space corollary is that a 64-bit address
process shouldn't really face this
problem there the ultimate living
limiting space factor will be the
system's virtual memory I don't know how
much longer those of us swimming with
thousands of concurrent tasks will have
to stay in a 32-bit pool but for now it
is an issue
let's talk a bit more about data races
and prevention a straight forward would
be a straight forward approach would be
standard unique lock of standard mutex
when the mutex isn't available we engage
the kernel
the calling thread is suspended some
other thread is resumed on the processor
the original thread makes no progress
whatsoever
while the mutex remains locked by
someone else and when it's finally
unlocked the OS switches context back to
the original thread I submit that we
don't often consider the impact of using
threads on the development process
itself there's a way of thinking about
potential data races that is a little
bit of a step up from what I would
consider the industry norm there's
certain defensive patterns certain
idioms that must be learned you guys are
probably way ahead of the curve on this
one this is an area in which
unfortunately the weakest link
phenomenon can really bite your but
innocent maintenance by someone unused
to dealing in these terms can silently
undermine the robustness of the entire
process so you make doubly sure to get
your code reviewed right you all do that
right naturally your reviewer must be
someone versed in the potential issues
such people can be a scarce resource on
your development team a big part of the
problem is that the language itself
provides very little help compilers
don't emit warning potential data raise
at least not the compilers I use the
trouble is that an undefended access to
a resource that might inadvertently be
shared between threads looks perfectly
natural
it looks just the way C++ code is
supposed to look nothing jumps out to
call your attention to it like a big
ugly reinterpret cast syntax or anything
it's actually a properly defended access
that sticks out like a sore thumb so
part of the cost is scouring your code
to ID
five possible race conditions depending
on how long the code existed before
threads were introduced and the weakest
link to have maintained it since then
that might or might not be a significant
part of the overall development cost so
what happens when you overlook a
resource inadvertently shared between
threads two words as you well know this
can take almost any form from an
erroneous counter through program cache
as you also know a crash is the good
case
appearing to run sanely while producing
results that are subtly wrong is the
worst possible kind of bug actually even
that's too sunny how about a program
that appears to run sanely while
occasionally producing results that are
subtly wrong threads anyone as I said
the C++ language does not as yet provide
a whole lot of help here there are a
couple of tools available for instance
he'll grant DRD and thread sanitizer
these tools instrument your program to
try to detect data raises at runtime
thread sanitizers home page states that
it typically slows program execution by
a factor of five to fifteen times while
consuming five to ten times the normal
memory which would be great if it could
guarantee a clean bill of health but you
already know that runtime trapping can
only catch bugs that you actually drive
during that session with timing problems
it's worse than exhaustively exercising
every code path which is already
effectively impossible for non-trivial
programs a guarantee of thread safety
would require considering every possible
interleaving of each threads
instructions with
those of every other that might make a
good basis for a cryptosystem
so if we need our applications to
product process both user input and some
number of back-end queries
simultaneously but for any of a number
of reasons
we're dubious about introducing lots of
new threads what other options are
available
classic blocking i/o or synchronous is
based on a very simple API you call a
function the calling thread stops dead
until the function has results available
in the case of web services this may
take milliseconds to seconds but the
straightforwardness of the API is
perfectly aligned with everything we've
learned about good program organization
async IO or an async API in general
although IO is the communist is based on
calling a function that initiates an
operation then returns immediately
before results are ready the operation
continues in the background in parallel
with whatever else you're doing in the
calling program finally the operation
notifies the caller with results
generally by calling it a callback
function that's it that solves the
problem this is how we get more
concurrency without adding threads with
all their attendant problems with a good
async API the completion callback is
called at a well-defined time not
between two instructions in the middle
of the code for some C++ expression the
trouble is now significant parts of your
logic anything depending on that async
result must be extracted to the
completion callback this can have a
disruptive effect on your carefully
designed program organization suddenly
in the middle of a lower-level
abstraction layer you must abruptly
return to the applications main event
loop then you must be able to re-enter
that layered logic at the point where
you left off it gets worse how about
when a subsequent async operation
depends on the result from the first how
about
a third operation depends on the result
from the second maybe you're thinking
lambdas done and have you ever seen
JavaScript code based on nodejs wait
wait sit down sit down please don't run
screaming from the hall I promise I
won't mention it again what tends to
happen is you evolve chains of callbacks
each with a bit of the relevant code a
bit of the relevant data you start
thinking too sparingly yourself but it
used to be so nice it was all carefully
arranged into different subsystems each
with their different layers of
abstraction you might say aha how about
the future then I can link one operation
to another have you noticed how there
are already proposals creeping end for
chaining a subsequent operation only on
error or only on success how about a
generalized conditional chaining
operation could we introduce a looping
construct we're trying to layer a whole
new language on top of C++ we've already
got for the run the preprocessor the
runtime language template language and
context per language reports from the
c-sharp trench's suggests that chains of
then are not much more fun to maintain
than chains of plain old callbacks you
might say aha
how about state machines I can build a
machine class that manages instances of
state classes each new entry transitions
to the next state we even have a couple
boost libraries for this I've been there
trust me it gets opaque pretty fast you
have to reverse engineer the state
transition table to figure out what
should have been obvious control flow
you might say but how about a big switch
statement you set an int representing
state every time you re-enter the
function you immediately switch on the
state variable if you cleverly lay out
the case
values so that subsequent states are
adjacent and if you squint a little it
even kind of sorta looks like normal C++
control flow in fact courtesy of a bit
of preprocessor magic and one of the
odder syntax anomalies from classic C
boost Ezio supports a pretty veneer over
the big switch statement it's called
Duff's device and in fact boost Ezio is
only one of a number of readily
available implementations of the idea
using that your function looks even more
like normal C++ control flow no
squinting required the trouble with this
is that it obscures the fact that you're
actually returning from and re-entering
the function every time local variables
get reinitialized every time any state
you would have kept in local variables
must be moved somewhere else say a heap
struct in fact Microsoft Visual Studio
2015 introduces compiler support for
this very concept for a resumable
function it generates code that in fact
stores locally declared variables in a
generated heap struct it implicitly
performs the big switch statement for
you this is kinda sorta great as long as
you're working in a single layer of
abstraction it's all you need where it
has difficulties isn't supporting
multiple layers of abstraction the
caller of each resume will function must
be aware that it might either produce a
result immediately or return nope not
yet
you might be thinking I know I know pick
me I know where this is going boost
co-routine and you're close you really
are but I will show you a still more
excellent way the boost fiber library is
the best way note organized code already
based on async i/o
why the qualification why not just say
the best way to organize code period
it's because of blocking i/o code that
wants to share its host thread with
other fibers must avoid making any calls
that will block the whole thread when
you call say receive on a socket control
enters the operating system it doesn't
return until data have arrived or the
socket is closed it doesn't pass control
to the fiber library to say got anything
else to keep busy with in the meantime
but once you're committed using async
i/o you're good fibers are independent
threads of execution being very careful
to use that phrase in the sense intended
by the C++ standard but in any other
context whenever I say thread I mean an
operating system thread by user land I
mean that the operating system is not
involved in context switching between
fibers this means that a fiber
explicitly gives up control it suspends
allowing other fibers to run cooperative
suspension has another important
implication on a single processor system
of course at most one thread can be
running at a time - but OS threads use
time slicing to interleave thread
execution in a way that from the codes
point of view is completely
unpredictable you don't know when code
on some of the thread might touch a
shared resource cooperative fiber
context switching means that application
code can know that it's not being
unpredictably interleaved with other
fibers on the same thread code accessing
a shared resource can know that no other
fiber on the same thread is trying to
access that resource at the same time of
course the scope of this exclusivity is
with any given thread if you share a
resource between fibers running on
different threads you have the usual
issues fibers use what isoh has recently
been calling suspend by call or so
bend downwards I'll say more about that
in a minute like standard thread like a
boost co-routine each fiber run it's on
its own separate stack asymmetric Co
routines like boosts co-routine are
great when the application has a kind of
handshake relationship with the co
routine a generator is a particularly
application code starts one up talks to
it for a while and then destroys it a
fiber doesn't necessarily have the same
close relationship to the code that
launched it it's particularly valuable
in a fire-and-forget kind of scenario
some function f launches a fiber
expecting the new fiber to continue
running
even after function f returns if you
wanted to try to build such a thing
using busca routine you'd need to
introduce some kind of manager to own
the co routine instances when a fiber
context switches away it does not
specifically resume its invoker unlike a
co routine instead it engages a
scheduler to decide which fiber to
resume next you can think of the fiber
library as kuru teens plus a manager
plus scheduler if you're one of those
people swimming with thousands of
concurrent tasks in a 32-bit pool you
may be thinking but Stax so yes each
fiber does have its own stack that's one
of the strengths of fibers and you still
can't move them I just have a couple of
things to say about fiber stacks first
unlike with standard thread you can
explicitly specify the stack size for
new fiber I'm not a fan personally
because how do you know how do you know
your prior analysis is still valid after
the last round of maintenance how do you
know your QA caught the deepest
recursion case on each fiber the fact
remains that if you must fine-tune your
stack consumption you do have that knot
in fact if you want still finer grain
control you can provide your own stack
allocator
I'm more enthused about the ability to
engage compiler innovations such as
segmented stacks
it's a classic time-space trade-off when
you set the slider too fast use more
space when you set the slider to small
you use more cycles but here's the point
you can leverage technology like this
because fiber stacks are managed by
userland code you don't need the
operating system to grasp the concept
several different people have suggested
hey why not use Microsoft resumable
functions as a back-end implementation
for fibers then the whole issue of stack
allocation vanishes because their stack
lists I could pause here to let you guys
work out exactly how to make that go I
have no doubt that many of you are
brighter than I am
the world would be in your debt so
what's the problem
resumable functions suspend by return or
suspend up and out fibers like stack
Ville co-routines suspend by call or
suspend down a fiber call some function
that function calls another function
that other function might choose to
suspend transparently
to the caller in practical terms the
caller of a resumable function must
always be prepared to deal with a not
yet answer standard future is one way to
express that the Khoa wait keyword is
the magic syntax that wraps up that
ambiguity hmm the answer is not yet
available I myself must suspend until
then but every resumable function that
might suspend must be able to
communicate that ambiguity to its caller
which means that its caller in turn must
be prepared to suspend its viral every
function in the call chain must have
signature and is body ready to propagate
that possible suspension then every
function not in that call chain must
also change the way it calls any of the
affected functions and outward it goes I
don't know about you but in our
organization QA is a scarce resource and
developers if I fix a little bug in a
leaf function and in the process change
the semantics of 30 functions at five
levels of abstraction that's going to
cost so what do we have to retest well
everything you could say well if
resumable functions require a pervasive
annotation clearly the set union of
resumable functions and fibers would be
to require pervasive annotation for
fibers - but no I use fibers because I
reject pervasive annotation if that was
your solution to unifying fibers and
resumable functions why bother
what does it actually buy you over
resumable functions on their own in case
you haven't yet guessed I strongly
believe in the benefits of having a
stack for each fiber most importantly
they give us the ability to write code
that looks and acts just as if we were
performing classic blocking i/o we get
to keep our clean subsystem design with
our layers of abstraction we get to keep
our local variables you might have
wondered well what about thread-local
storage isn't that going to be shared
between all the fibers on a thread it's
true I might point out that this can be
useful they share it without racing for
it some functionality wants to be thread
scope rather than fiber scope you might
be interested to know that sharing
thread local data is typical for present
implementations of other fine-grained
concurrency such as GPU or simply
support but so far I've gotten what I
want by using local stack
variables instead of thread-local or
fiber local storage so yes there is
fiber local storage the fiber API is
based on the standard thread API
reflecting the role of fibers as user
land threads it supports the usual
suspects in addition we get these fiber
aware communication cues as you might
suspect from their names
a producer fiber can push an arbitrary
number of items to an unbounded channel
at risk of arbitrary memory consumption
a bounded channel imposes a
user-specified limit suspending the
producer if it tries to exceed that
limit but I'm not going to walk you
through each and every one of these
because quite aside from the fact that
you can read about them yourself the
point of mirroring the thread API is you
should be able to infer their semantics
from prior knowledge of the thread API
so why all the duplication why do we
need fiber variants of these things at
all it's because the standard
implementations block the entire thread
as I noted earlier if you try to run
more than one fibre on a particular
thread you pretty much want to avoid
blocking the thread the fibre library
provides objects and operations that
block just the calling fibre allowing
other fibers to continue running on that
thread I might also point out that fiber
context switching is far lighter weight
than thread context switching but
wouldn't it be cool if we could
customize the way that standard library
operations suspend then we wouldn't need
redundant implementations of all of
these things I'll leave that as an
exercise for you guys what I'd rather
talk about are usage patterns how to
integrate the library to solve problems
that you actually faced for starters how
about you sink i/o I keep talking
grandly about how fibre is let you write
code
looks and acts like you were using
classic blocking i/o I talked about
necessity to use async i/o but isn't
there kind of an impedance mismatch
callbacks push blocking functions
you know return results so let's work an
example here's an API within an it read
call accepting a callback here's an
adapter we declare a promise of standard
string we capture that promises future
then we pass to the init read function a
lambda that binds our promise using
initialization capture the lambdas
signature matches the callback required
for init read then since in it read
returns immediately we call future get
this blocks the calling fiber until the
futures fulfilled one way or another
remember this is a fiber future
eventually the operation completes and
the lambda is called if the past error
code indicates success we can call
promise set value on with the past data
but if the operation failed we can
instead call promise set exception
either of those calls will wake up the
future get call if it was set value our
read function just returns the past data
item if it was set exception get throws
that exception so that seemed like a lot
of boilerplate to adapt a callback
but it's localized that's part of way
like so much about having a stack
encapsulation just works
then there's non-blocking i/o for
instance you can set a socket in non
blocking mode if there's no data present
instead of blocking a read operation
returns e again or a would block that's
rade but what then you don't get a
convenient callback in practice you have
to keep polling here's an example
non-blocking api notice that in addition
to returning an error code meaning no
data yet this read method might also
return less than the desired length of
data in other words in addition to
polling until it returns an actual
length rather than a woodblock you might
also have to keep polling past any
number of additional a woodblock returns
until you've retrieved the desired data
length here's an adapter will call it
read chunk because it might only read
part of the desired data it calls the
non-blocking read API and as long as the
non-blocking read keeps returning a
woodblock we yield to other fibers on
the same thread if your other fibers
have too little work to do and your
non-blocking read takes too long you
might actually consider sleeping the
fiber instead of just yielding we return
when either we get a legitimate length
or some other error but of course that
length might still be less than the
desired length so here's a wrapper
around read chunk to try to read the
whole desired length until we've read
the full length call read chunk again
and break if a of' or error as long as
we keep getting success append the
current chunk and loop back to check
again I know perfectly well that there
are more efficient ways to concatenate
string data that's not the point here we
return when either we've read the
desired length or we get some kind of
error remember it won't be he would
block and finally we can wrap all that
up in a nice application facing function
that's guaranteed to return data because
it throws if it gets an error other than
UF it doesn't strictly have to be a
separate function from read desired I'm
just pointing out yet again that you can
layer these idioms as need
one of the nicer extensions you've seen
floated around futures is the ability to
trigger some action on fulfilment of the
first of an arbitrary number of futures
for instance you might want to fire off
redundant queries so several semi
reliable backends and accept whichever
answer arrives first discarding the rest
but like future then that feature is
callback based consistent with our quest
for cleaner application architecture
let's invert that let's say we want to
launch some number of functions each on
its own fiber each of which will wait
until it's done we want the calling
fiber to wait until the first of them
returns not necessarily the leftmost one
but the one that finishes soonest so
this will be a wait any function if in
fact we're sending off redundant queries
we're probably reusing the same function
just passing at different arguments but
for fun let's pretend we might have to
invoke different functions the result we
want is the functions return value I'll
present a container based wait any
implementation only because it takes
fewer slides in a very attic one of
course the return types must all be
compatible and just to raise the stakes
let's say we have to allow for the
possibility that one or more of the
tasks functions might throw an exception
we want the first successful result I'm
going to present this function in two
acts because it's going to be tricky
enough to squeeze everything onto a
slide since this template accepts an
arbitrary sequence container functions
I'm going to make the caller explicitly
specify the desired return type the key
is that we're going to collect futures
representing the result of each function
make an unbounded channel of those
futures we a shared pointer because wait
a knee is going to return before the
last of the fibers completes
that's the whole point of wait any we
loop over the functions in the container
counting them as we go it's an arbitrary
container it might or might not have a
size method for each function we launch
a fiber with a lambda binding the
function and the channel shared pointer
we detach the fiber immediately that's a
common pattern that distinguishes a
background fiber versus one that you
might join later in the lambda body we
instantiate a package task or the bound
function package task is a wrapper that
runs a function capturing its return
value if it succeeds or its exception if
not then we directly call the package
task this is somewhat unusual use of a
package task normally you might think of
launching a package task on its own
fiber using its future to wait for
results but here we're already on a new
fiber the point is this we want to
lambda fiber to wait until the bound
function either returns or throws if we
were to immediately push the package
tasks future to our unbounded Channel
then wait any would always always prefer
the leftmost function not necessarily
the one that completes soonest so we
delay pushing the future until we know
it's fulfilled only when the package
task completes only when its future has
been fulfilled either with the result or
an exception do we push it to the
channel so far so good yes the question
was are we only using the future to
capture the distinction between the
return and/or the exception and the
answer is yes but as convenient the
package task will grab that for us okay
we've launched a bunch of fibers in fact
exactly count of them now for act 2
we're going to loop at most count times
why is that important because if every
single one of count functions throws an
exception we could find ourselves trying
to consume a value from a channel that
has no more producers that would turn
wait any into wait forever
assuming we haven't yet exhausted all
hope we pop the next future from our
channel naturally this causes wait any
to block until that future is pushed
then we check whether that future was
fulfilled with an exception or value the
exception pointer returned by future get
exception pointer will be null pointer
unless that exception was called and we
know this future is fulfilled once we
know the future is fulfilled with a
value we can close the channel that
means that no subsequent producer fibers
will succeed in pushing values to it
which is fine we only want the first and
then we can return this futures value of
course if we really do burn through
count futures and every single one of
them contains an exception we should
probably communicate failure to our
caller somehow naturally it would be
helpful if we captured one or more of
the exception pointers from the fibers
this is just a placeholder you might
think that wait all wait all would be
only a slight revision to wait any and
in fact you could implement it that way
but I have to show you a very attic
implementation I think is just really
cool since wait all necessarily waits
until the last of the functions is
completed you don't have to worry about
reordering them you can present results
in the order the functions are passed in
fact you can just state the type of the
result you want and return the whole
thing BAM in one swell foop in fact and
I think this is seriously cool you can
even pass functions with heterogeneous
return types and collect their results
in a struct
so all we have to do is make a bunch of
async calls then pass all the futures
down to a helper function which merely
initializes the desired result type with
the results from the past futures tell
me that isn't seriously cool just for
grins why do we even need the helper
function
why couldn't we use this implementation
instead it's because of the compilers
implicit iteration over the argument
pack this implementation makes a single
pass it launches the first function at a
separate fiber then immediately waits
for it then it launches the second
function on a second primer then
immediately waits for that in other
words the implementation above
serializes the past functions it's
effectively the same as this
implementation which doesn't even
attempt concurrency with this
implementation you get two different
passes through the argument pack the
first pass launches all the fibers
collecting all their futures into
another argument pack over which the
helper function makes a separate pass to
wait for each of them notice the
similarity to the loops in act 1 and act
2 of the wait any function I will now
share with you the mystical incantation
to integrate a classic event-driven
program with the fiber library that's it
that's all you need to do to ensure that
the fiber library can divert some cycles
to the fibers you've launched in general
the rule of thumb is just make sure you
call this fiber yield often enough if
there are no other Reddy fibers that
were returned quickly but if there are
it will run each of the Reddy fibers
until it's next to spend point I want to
point out something about that call that
might not have struck you
for most purposes the main function and
the functions it calls can be treated
the same as a fiber that you've
explicitly launched you don't have to
move your event loop into an explicitly
launched fiber in order to be able to
make that yield call the same is true
for a function launched as a new thread
it's simply the default fiber for that
thread just as the main function runs on
the default fiber for the default threat
but what about when the main loop is
inaccessible embedded in some other
application framework we use boost as
you as an example even though there's
more to say about integrating
specifically with booze Desio a typical
application performs some setup than
just passes control to io service run
the run call won't return until
application shutdown this illustrates a
pattern we see with a number of
application frameworks you have no
direct access to the driving loop one
approach is to intercept control with a
callback each time around the embedded
main loop with a zero you can affect
that by continually reposting a handler
that calls this fiber yield of course
you have to get the ball rolling
initially a potential problem of that
approach is that when all the fibers are
blocked on pending IO so the IO service
has no ready handlers as you own fiber
could just spin the thread CPU passing
control back and forth to each other if
the framework you're dealing with
supports timers Azio does but that's
just our example you could use a timer
instead that trades off busy spinning
with responsiveness to IO completion the
framework you're using may or may not
manifest the spinning issue one of the
implications of a userland fiber
scheduler is you get to play to the
library is scheduling algorithm is a
customization point all you have to do
is derive your new schedule
are from the skid algorithm abstract
base class and pop it in place with used
scheduling algorithm for understandable
reasons this must be the first fiber API
call on a given thread I'm not going to
take you through the details right now
just know that there are several
different examples in the library's
examples directory for instance your
custom scheduler can associate custom
properties with each fiber one of those
examples introduces a priority property
and makes the scheduler honor the
relative priorities of different ready
fibers that's one obvious use case for
this kind of customization another
example shares a group of fibers among
participating threads the next ready
fiber is run by whichever thread calls
next that about wraps up what I wanted
to share with you today one additional
point of interest the boost fiber
library will be part of boost 1.62 which
will be released any day now it's out in
beta 2 and I'll entertain questions the
more entertaining the better yes the
question is do fibers use all of your
cores or are they limited to one CPU and
the answer is all the fibers on a given
thread are going to behave just as if
you were running all of that code on
that one thread it does not increase
your core utilization one way of getting
the one-to-many effect or the n2m effect
as it's sometimes called is to share
fibers among threads in the way
illustrated in the examples directory of
the library but fibers share control
within a thread typically and you have
to use a custom scheduler to get them to
migrate to other threads other questions
yes
it is not part of boost 161 it will be
part of boost 162 thank you for your pay
oh wait there's a question performance
comparison so I don't usually lead with
this because for me fibers are more
about maintainability of my code than
they are about performance however I
think the numbers look pretty darn good
now I will say this that I don't believe
fiber context switching will compare
favorably with Microsoft resumable
functions which as you probably saw
yesterday afternoon can be optimized out
to crazy minimalism but I think they
hold their own pretty well with regards
to other concurrency frameworks in user
land space ok I see two questions the
one in the back first and then does each
call to yield use more stack so can you
get into a situation where you're kind
of yielding two different fibers and
using more and more stack the quip okay
since I don't have to repeat that
question I won't the know what happens
is that when you launch a fiber it gets
a stack and they're after yielding
between fibers only switches between the
stacks that exists there isn't an
increment in stack usage with each yield
and then there was another question
midway back towards that person so
hypothetically could a could a standard
library implementation of stood thread
be based on
user Lanterns a fiber style
implementation and could you eliminate
the redundancy that way that's a really
excellent question and the answer is
that there exists an implementation that
does something like that you have heard
of HP X from the Louisiana State
University it isn't based on fiber but
HP x thread is actually and I'm not
entirely sure how they accomplish this
but HP x thread uses their own
lightweight context switching and will
scale outward to use os threads when
necessary so the answer is it is
certainly possible to do that and in
fact the HP x people ask why would we
need a fiber library when all we have to
do is make the standard thread library
be a lightweight it's a good question
but until the standard goes there
we have Buddhist fiber Wow okay so okay
Robert
okay the question was why are we not
seeing in the performance chart a column
for plain threads as opposed to these
other technologies I think that it's
because it's orders of magnitude more
expensive to context which threads and I
think that oliver deemed that it wasn't
even worth you know presenting the
comparison I have not looked the
question was have I looked to see what
makes Haskell an order of magnitude
faster
I have not thank you it might be that
everything is lazily evaluated so it
just didn't evaluate the things that
we're not being looked at by the
benchmark
oh one more over here
how much is this potential games with
the what case so the CPU use is supposed
to be increased at this little base so
do we have any numbers about that
compared to normal threading okay so if
I understand you what we're doing is
we're using each thread more efficiently
than if we were waiting in a particular
thread kind of depends on your
application mix the fact is that you can
keep a thread busy with stuff you're
doing in fibers while a particular
operation is suspended but you're not
blocking the thread waiting on that
operation but the reason that we
benchmark
with specific you know algorithms is
because it totally depends on your
application mix okay well thank you all
for coming
glad you found it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>