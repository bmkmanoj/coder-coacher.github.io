<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Hartmut Kaiser “Parallelism in Modern C++&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Hartmut Kaiser “Parallelism in Modern C++&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Hartmut Kaiser “Parallelism in Modern C++&quot;</b></h2><h5 class="post__date">2016-10-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6Z3_qaFYF84" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody my name is Hamid Kaiser
I'm working at Louisiana State
University and I'm leading group there
it's called stellar group where we focus
on writing or focus on parallelism and
everything around C++ and that we go
today I want to talk about our
experience was implementing different
kind of parallelism in C++ very much
oriented on the ongoing standardization
forward and starting with the
parallelism TS and other other things I
essentially what what the main outcome
we what we learned when while
implementing all of that is that you can
use task based parallelism as a basis
for all higher-level api's and C++ and
that works very well even if you can
implement it all kind of a very broad
kind of parallelism like a synchrony
continuation based parallelism for joint
parallelism and so on I would like to
focus on the parallelism TS today so
those parallel algorithms which now have
been or will be added to C++ 17
hopefully okay but first allow me please
allow me to to give you three slides
about HP X which is a runtime system
we've been using to implement all of
that it's a library we've been
developing for years and I really give
you these slides about HP X just to give
you a background where all of that has
been implemented and to show you that
what you can use to to play with it
today to take the parallel algorithms as
they will be in support for 17 today and
just try them out run write your
applications with them and later on all
you have to do is to change see
name space from HP X kalaam kalaam to
stomach : :
and you get exactly the same behavior
same semantics HP X is a general-purpose
runtime system for applications of any
skill and by that I mean applications
which run on the Raspberry Pi on one end
or on the petascale machines on the
National Labs on the other end hundreds
of thousands of cores and two cores on
the Raspberry Pi and the same runtime
system is usable on all of those it's a
general-purpose runtime system for
applications of any scale as already
said we we made a very large effort to
implement a API which is absolutely
stunning conformant in terms of
semantics and syntax so it gives you a
very uniform and standards oriented api
to write your applications on the other
hand you can write fully a synchronous
code using hundreds of millions of
threats and even on my laptop I can run
applications which run hundred million
threats without any problems
the interesting part here is that since
we want to run on the big machines as
well not only on on the on desktop zone
on smaller embedded devices we provide
you a unified syntax to write
applications which abstract the notion
of locality from you so you code looks
the same regardless whether the code DB
the operation has to be performed
locally to the invocation or remotely on
a different node just a short rundown
what HP X actually consists of and you
will see that essentially what we
implemented there is a set of features a
set of facilities which are known for
decades and just by putting them
together we kind of get new completely
completely new emergent properties which
allow us
- to implement these nice nice things
which I will talk to about in this talk
it sits on top of a global global
address space that might not be that
important in the context of this con and
the ill and the context of this talk but
it's very important when you are talking
about distributed applications running
on thousands of nodes because what you
want to do you want to do local global
load balancing of your data and that
means you want to move around things
from one node to another and by
providing a global address space that
moving around of data doesn't change the
address of that data item which is very
important on the other hand it's built
on top of fine grained parallelism and
very lightweight synchronization the
overhead for creating is thread
scheduling it running it destroying it
is in there in the range of 800
nanoseconds so it's very cheap to create
a thread and to run things on that we
combine that with a work queue based
message different computation not only
local locally like you know it from TBB
or from silk but also remotely so it's a
departure from the common bulk
synchronous programming paradigm which
people use nowadays to write their
distributed applications mostly based on
MPI I already mentioned - for semantic
equivalence of local and remote
execution and we have explicit support
for accelerators and and vectorization
and they will show you a couple of
examples later on it's completely
open-source you can download it from
from github it's being released under
the boost license so no strings attached
whatsoever you can use it forever for
whatever you want we certainly would
like to hear back when you use it but
you don't have to towers even and the
nice thing we believe why it might be
interesting for you to to look into HP
exits you can very nicely use it for
platform for research and
experimentation and you can on the other
hand already today we'll play with the
features which your compilers will give
you hopefully tomorrow
a quick overview about what the
structure of hpx is but I'm not going to
drill down too much into the details for
subsystems shredding local shredding
synchronization the address space and
the networking layer sitting on top of
the operating system and exposing a C++
standards conforming parallelism API on
top all of that combined was a
performance counter framework which
allows you to measure all kinds of
different things in in the runtime
system in the application and the
operating system hardware performance
counters as provided by the chips expose
through a uniform interface and some
tooling support which allows to tie in
what we call policies which look at
these performance counters and allowed
to and to do the runtime adaptive
decisions on what to do next and turn
the knobs in the system itself
things like hues lines white times
memory distributions and so on and so on
and you can can analysis at very nice
name
in this talk today I will focus on the
parallelism ap is and is specifically on
the parallelism TS they are parallel
algorithms in general the API hpx
exposes as I said is oriented strictly
on the existing standardization
standards and on this standardization
effort so we try to track many of the
proposals related to parallelism and
concurrency in hpx early on and give
feedback develop extensions which we
feel are very interesting for us and our
application development and try to feed
that back into the standardization
process well in short essentially
everything you have under standard you
have something very equivalent in hpx
and you might ask hey why did you
reimplemented stuff why did you
reimplemented new text futures async and
so on and so on
well there are several reasons why we
did that
first we wanted to support distributed
computing from the very beginning and we
wanted to provide these facilities in
distributed computing so that you could
just bind a function on one node send it
over the wire and invoke it on the other
node or just do a async which gives you
a lightweight future which is
implemented on top of a very lightweight
writing system and gives you the Layton
sees and the overheads very the low
overheads I already mentioned we
implemented a couple of extensions to
those done it's a pis and as I said we
are trying to defeat that information
back into our work was sg1 was SG 14
okay so if you look at the picture at
the big picture what does it mean to
have parallelism and and what are the
things you're interested in controlling
when you are when you want to expose
various facets of the parallelism to you
to you using on the uppermost level the
application probably will deal with
fairly high level things like parallel
algorithms some fork/join parallelism
some essentially some continuation style
programming and so on and so on all of
that we have implemented on top of three
facilities essentially futures async and
a function which we call dataflow and I
will talk about what dataflow actually
does in a second and these three
facilities
kindness sit on top of a three concepts
the execution policies that are those
objects which are already defined in the
parallelism tiers and which have been
added to super 417 and those execution
policies are there to express
restrictions of your of your code you
want to run a parallel essentially if
you use the sequential execution policy
that means
it tells a system hey the code I provide
as a lambda for my parallel for loop
does not is not allowed to run in
parallel or concurrently it has to run
sequentially if I use a parallel
execution policy I allowed the system to
run things concurrently in parallel
execution policies are tied to
executives that's an ongoing discussion
and currently there's a lot of work
being done to coin out the details of
those executives what we have
implemented is an early proposal which
was developed mostly by Nvidia and which
is a basis for the executor
standardization work which is currently
being discussed executives are
essentially objects which represent the
where and when of execution where do I
want to run this particular task and one
do I want to run it what we edit you to
it is what we call executive parameters
that sings which allow us to further
modify the behavior of their executives
other things like grain size so how many
of the iterations of this for loop do I
want to run and one go and how do I want
to chunk up the work this kind of things
and many other things and I will show a
couple of examples later on if you look
at what the standardization how the
standardization landscape looks today
then we see we have the parallelism
tiers which is mostly concerned with
iterative parallelism which has been
moved to be included in C++ 17 and I
really hope it will make it into C++ 17
the concurrency TS which is defining
extensions for a synchrony for task
based continuation style parallelism
there are task blocks for fork/join
parallelism of heterogeneous tasks
that's a proposal below submitted by
Intel which is currently being discussed
as already mentioned executives and last
but not least resumable functions or
coroutines with compiler support for
quality
which are very important for us as well
and it will show a couple of examples
for that what's currently missing in the
in the standardization from our
perspective is the integration of all of
the buff so these things are all very
interesting but they are not very well
aligned one with it another there's a
question of creating or extending the
the Rangers proposal
into the parallel world nobody is
working on that as far as Inc as I can
see so far vectorization is already
being discussed and there are still
extensions for GPUs many core
distributed or high performance
computing and that's where we try to to
define and find abstractions which can
be used efficiently to integrate all of
these things into one big package in my
point the goal has to be to make
parallelism in C++ independent from
helper libraries like open MP or open
ACC if I had a wish by C plus 20 I'm not
sure if that's possible but that's what
what we are striving for hpx itself as a
library it allows us to make C++
independent of MPI as the underlying
networking or programming model for
distributed applications as well okay I
mentioned futures and just to be sure
that everybody is on the same page here
and everybody understands what the
future is I and because futures are for
us at least in hpx kind of the central
very crucial facility we use to build
all kinds of parallelism all kind of
synchronization on I would like to talk
about those for a second so in essence a
future is an object which represent it
represents a result which has not been
computed yet essentially what happens is
you have one thread of execution which
creates that future object and when that
future object is created a new thread is
being spawned which executes another
task and the original thread can go
forward
executing other work until two point
when it actually needs a result of that
spawn threat and at this point it looks
at the future and if the future has
received the result of that parallel
execution already then it just goes
ahead if not then the current sweat is
put it's put to sleep
and will be resumed when the when the
result arrives this is a very nice
synchronization model and allows us to
completely abstract ourselves from the
notion of a thread which is very
important because I personally believe
that dealing with low-level threads is
very very painful and people shouldn't
well people are just not wired to think
in terms of threats and if you have to
deal with two threads or ten threats
that might be okay but as soon as you
have to deal with millions of threats
this is completely impossible to to
model in your head
how that works so this kind of model of
all using future objects enables us very
transparent synchronization with a
producer it hides the notion of dealing
with threats which is in my book the
most important thing it makes a
synchrony manageable and it allows for
composition of several independent tasks
in building a task dependency tree as we
will see in a second the last point
turns concurrency into parallelism I put
that in parenthesis because futures
actually don't do that but as soon as
you start using futures for the
synchronization you kind of end up with
a programming model where you have to do
less with concurrency and you can focus
more on the parallelism so it's a very
very nice way to to write code where you
where you in my experience have a lot
less concurrency problems like data
risks and so on and so on so a small
example we want to calculate the
universal answer to life to universe and
everything else while we know that it's
42
but we also know that calculating that
answer requires 7.5 million years
unfortunately if you follow the
literature so let's so let's create a
function which we call deep thought
that's our computer and what that
computer does it just cause acing and
launches that I think that it universal
function that function which calculates
a result on a new thread async returns
as a future object which represents
answer which the function will deliver
at some point in the future then we go
on with our lives for 7.5 million years
because well we have lots of other
things to do while this answer is being
computed and at some point we come back
and say hey I'm now the 7.5 million
years are over now I want to get the
result and what you do you call get on
the future and two things can happen
either the result has been computed and
has delivered has been delivered back to
the to the future object then this will
just grab the value and go ahead or the
value has not been calculated yet it has
not been returned to the future and then
get will internally suspend the current
thread completely invisible for the
caller and assume it as a value is
delivered this read will be resumed and
the the call returns from get and
returns and continues as if nothing
happened so the caller will not even
know whether the synchronization whether
the thread was suspended or not all it
knows I said one get returns it has a
result of the of the function and that's
a very nice way of abstracting these
anonymous consumer produces scenarios
and as you can see there's not a single
notion of threat or of synchronization
of whatever
it's very nice straightforward code and
in HP X we use futures as a central
building block for all kind of
synchronization for all kind of
parallelism and that worked very well
for us ok Pirlo algorithms
you might know that the parallelism key
s essentially defines to implement probe
specifies these eighty algorithms also
which we all know and love from from the
STL and it specifies them to to be
parallel or to be paralyzed about what
the essence there is that they are very
similar to standard library facilities
we know for years but they have one
additional first argument which is the
execution policy and the standard
currently defines three execution
policies if I remember correctly I will
focus on two of them at this point the
most important ones are power and SEC
well power means that the code is
allowed to run in parallel my my lambda
I passed to the to the for each is
allowed to be to run in parallel and seq
just means I know parallelization is
allowed I'm not allowed to run that code
concurrently execution policies have a
standard default executor associated
with them and this is how we implemented
it in in HP X so essentially power is
associated with a default parallel
executor and seq sack as associated with
a default sequential executor what we
also implemented is that you can rebind
the executor and the executor parameters
to the execution policies and you can do
that in a very simple way that code
example shows you a parallel till I left
that namespace parallel in front of it
even if the standard doesn't do that
under standard it still stood : : for
each o state : hello and fill but just
to make it clear that I'm talking about
the parallel versions of those
algorithms I usually have the parallel
kolonko on the namespace in front of it
so as you can see the last arguments are
the same as for the old venerable state
find fill we we know but you pass that
additional argument there as a first
argument which means hey that fill can
be actually executed in power law as
simple as it is so if you want to use a
different executor you can do that very
simply by doing power dot on dot on
allows you to bind a new executor and I
will talk about executives a bit more
detail later a new instance of an
executor to this parallel execution
policy which will and if that executor
for instance limits execution to a
particular Numa domain in your system
then this standard fill algorithm will
run code only on this particular new
model main or if that executor limits
execution to a GPU then sted fill will
run the code on a GPU and so on and so
on and the nice thing is you as a user
can provide your own and develop your
own executive objects which will have a
very simple interface and you can
control the the operation of this
parallel algorithms by providing your
own executor object which encapsulates
your particular needs in terms of
execution where remember executives
means where and when our thing's
executed the dot voice is an extension
we implemented in HP X which allows you
to add additional parameters to the
executor to tell the executive
additional parameterization things like
what's the chunk size what's your
chunking policy and and so on and so on
and I will have a couple of of other
examples in on the next slides so again
what you can do take our Batory
execution policy rebind only the
executive instance or rebind only the
executor parameters when you want to
rely on the default executors which is
already pre bound to the parallel
execution policy or you can rebind both
it's as easy as ad
or if you go to our power example again
so power power was a rebound executor or
was a rebound
both of them rebound to your executor
and your parameters object okay
what we additionally edit in in HP X to
those execution policies which the
standard defines and there's currently a
proposal which is being discussed and
sg-1 which we submitted the paper a a
proposals currently being discussed to
turn the algorithms into a synchronous
algorithms normally when you invoke a
parallel algorithm it will return only
once all the algorithm has been finished
computing but wouldn't it be nice if you
could just kick off the computation let
it run somewhere and get a future back
which represent the the the finalization
of that particular algorithm execution
and the way we we did that is just by
allowing to specify a task modifier with
the execution policy so if you write
power of task then the algorithm
actually will not run sequentially at
our synchronously as you know it but it
will just kick off the operation and
give you a future which represents you
the all execution of the of the
algorithm and the same we have
implemented for the sequential execution
policy as well that gives you two new
execution policies a parallel task
execution policy and a sequential task
execution policy in all those cases
formally synchronous functions written
on the future the parallel construct
will be exit exit executive executed
asynchronously on a side and you can go
ahead was on the current sweat was other
operations and synchronize on your own
time was it was a return future which
allow us to integrate these parallel
algorithms very nicely into other a
synchronous control flows which is very
very power
for feature another thing we did there
based on the data power work which is
currently done to integrate
vectorization with with C++ we
implemented two other or four other
execution policies which we for now
called data power and data port task
that might conflict with other names so
please take all those names with a grain
of salt we can discuss those we can
change those but just for the sake of
the discussion let's assume that the
execution policies are called data power
and data SEC and those execution
policies don't do anything in terms of
parallelization or not polarization but
what they do they transform the code in
a way so that your lambda can be
vectorized so if you have mostly
arithmetic operations or other
vectorizable operations in in your in
your lambda then the code will be
transformed in a way that your lambda is
not invoked with the value type of your
containers but with a special type which
is a vector tag essentially which is pre
initialized by the underlying
implementation and your lambda will work
on the vector pack instead of the
original value type and I will give you
some examples later on about that
currently we have built that on top of
an external library which is we see it's
a library floating by matías fretes one
of the authors of office of one of the
vectorization proposal and which is
currently being discussed i am in
contact with joe it's Joel here Joel
focu I think he has a talk and talk yeah
he's talking currently and we we want to
do the same integration with boost's MD
which is an equivalent library he is
currently been developing the only
caveat des said it requires the use of
generic lambdas or polymorphic function
objects because the algorithm transforms
the code and the data types your your
lambda now has to be instantiated with
several different data types for the
plain value type and for the pack and
and so on and so on but that is
shouldn't be a problem it's just nice to
know so a bit more about executives what
is an executor executives must implement
one function in our implementation at
least note that might change as I said
the there is currently a lot of
discussion going on in the standards
community we have a weekly phone call
where these things are being discussed
Michael talked about that this morning a
bit more detail for us executives need
to implement one function which is
called async execute and you give it a
very radical or in this case it's an
honorary function object but we
implemented a very etic interface which
launches that function with a given set
of parameters and gives you a future
back and that's how that is done it's
completely to the up to the executor
instance you're dealing with and we
access those executive functions through
a trades object and this might change as
well people in the Ennis and the current
discussion might go in the direction
that there will not be a single straight
object but a special overload for each
of the of the functions but that's
details so and and that trades object is
important because what you might they're
essentially four functions you or five
functionalities you want to get from an
executive a fire-and-forget operation
run that at some point I don't care
about the result run that as
synchronously give me a future of which
represents a result of that function run
that synchronously I will wait for it
and two functions for bulk async and
bulk sync operations which allowed to
run many tasks which is important in the
GPU context or or for loops and things
like that and what the traits do the
traits are able to simulate part of that
functionality if the executor doesn't
implement all of those functions so you
it's very simple to implement an
executor in the end because all you have
to do you implement one function and
everything else can be emulated on top
of the
one function and the traits will do that
for you yes Oh both of the async calls
return a future the single function
async and debulk async return the future
so you can synchronize with with the
work just to give you a couple of
examples of executives we implemented
sequential executors obviously parallel
executor that's the default ones which
are bound to the execution policies is
this read executor which guarantees that
swings are executed on this read a
distribution policy executor which
allows you to tell hey I want to run it
over there on this node or please here's
a set of nodes you decide on your own
where you want to run things a parallel
executor I will talk about that a bit
later in more detail which allows you to
specify the new model main or constrain
the set of course you actually want to
run that code on very important or a
kuda executor which you can use to run
things on a particular device connected
some anywhere a new system and so on and
so on ok a bit worried about the
executor parameters we implemented the
same scheme as for the executor executor
traits pair we call the parameters and
executor parameters traits various
execution parameters are possibly you
can be specified some examples there are
as I already said control the grain size
of work how do you chunk up your your
loop when you paralyze it that's very
similar to what open Peters was the
static dynamic guided chunking policies
but it allows a much more fine control
because you can define your own champion
policy easily just by implementing one
function which tells the the algorithm
how to chunk up things we use it for our
parallel algorithms obviously to to be
able to control how how things are being
chunked another example if you deal with
certain systems and and GPU integration
and the compiler toolchains I was not
too great nowadays we have to deal with
several vendors cura is one circle from
code players one HC or MD has their own
tool chain to integrate GPUs and all of
them have some quirks where you have to
specify or give give your caramel which
has to be run on the GPU a unique name
because compiler is not able to figure
that out so the programmer has to give
that name and that would be for instance
one of the executive parameters giving
the GPU kernel a name another example
and I will should have an example in the
very end some measurements but for that
is do simple array of prefetching so
that you can automatically prefetch
certain iterations ahead of time so that
when when the execution comes to that
iteration the values will be already
loaded into the caches and things like
that again you can write your own
executive parameters and implementations
of that easily and customize the way
these algorithms actually work ok that's
one part namely placement of execution
where to run things how to run things
went to run things another problem when
you deal and when you want to make
parallelism efficient is that you have
to be able to control data placement
where is my data which is a spec on a
single note that might not be that
important as long as you're not dealing
with new mud effects in your algorithms
because no matter where the data is
placed all course can access all all
data in the main memory but when you're
dealing with five GPUs connected to your
system you better place the data on the
GPU where you're going to run the code
otherwise it will fail so data placement
isn't very important second facet and I
would like to give you some ideas what
we can do in that regard and how we can
expose data placement to the user in a
very very generic very nice way
generally there are different strategies
you have to implement for different
platforms as I already said for
Newmarket Texas you use one way for GPUs
used something completely different and
for distributed systems if you want to
do data make data placement efficient
and and do load balancing across nodes
then in distributed systems you have to
do completely different things
what we decided to go for is again the
same as we did with everything else we
just took what this done it has in this
case is done at a locator and the
allocator traits and extended those for
for the use cases we had in mind to make
make it more efficient essentially what
we did we added additional functions to
the standard a locator for bulk
operations for allocation the allocation
construction destruction which is very
useful when you want to work with data
on the GPU for instance but generally
it's it's really just a a linear
extension of the of the a locator
interface
additionally we edit our own
implementation of or yeah vector I'd say
implementation and a data type which we
called partition vector we added the
vector with a special yeah was full
allocated support and which which takes
advantage of the extensions of our of
the allocator extensions we implemented
to give you the same interface as a
standard vector which allows to manage
the data localities through that a
locator and which uses the execution
target and I will talk about targets in
a minute for the data placement the nice
thing here is that it allows to do
direct manipulation of data on a GPU for
instance from the GPU and from the CPU
at the same time so it has special magic
implemented so that we can access the
data from both ends partition vector is
the data structure we implemented mostly
for the distributed case essentially
it's a
object which looks like a standard
vector but the data is not contiguous
its chunked up into partitions and those
partitions can be located on different
nodes in a system or on different GPUs
in the system and the the the chunks or
the partitions taken together form the
data managed by this petition vector so
it's a segmented data store where the
segment's themselves are the hpx vectors
again obviously and you can use
different things to decide how to
distribute these these partitions over
over the machine and so on and so on I
will give you a code example in a minute
yeah as I already mentioned it involves
extending the allocator traits as well
well if you extend the allocator you
want to extend the traits as well which
was functionality which allows us to
copy data that's very trivial for CPUs
but as you know when you work with GPUs
you you have to explicitly move the data
from CPU to the GPU in order to work on
it and we extend the allocated trades
which allows us to do that in a very
very efficient way allowing to overlap
the the move the data movement from the
CPU to the device and back was with
local work on the on a CPU well and
distributed that maps onto network pause
possibly a remote DMA operations and so
on and that's implemented in our our
networking layer I'm not going into any
details here we added the functionality
access single elements again on a CPU
that's trivial on the GPU if you want to
access a single element over the from
the device from the CPU that's certainly
expensive but for debugging purposes
sometimes that's nice the same is true
for the network if you really want to
grab only one value you can do that you
might not want to do that all the time
but for debugging purposes that might be
useful okay
we introduce the type which we call
execution target and again the name can
be changed
we are not attached to it but what we
need but I mean by a target is
essentially just a place on the system
where the data can be located and the
execution is kind of connected to it and
we use these target objects to a create
an alligator
to do the data placement and be to
create an executor to co-locate the
execution was with was a site where
where the data is located several types
of those targets for instance kewda
target or a host target or a distributed
target and so on and you can implement
your own then we initialize a locator
from those targets which gives us a new
model main target when we use a host
argot or a GPU device a locator which
allows to allocate data on on GPU or a
locality based target allocate data over
there on this node place the same target
object will be used for creating the
executor and that guarantees that the
executor will run things exactly what
the data is located and that's what you
want to achieve in the end so this
target allows us to abstract the data
placement and the execution placement
and give the user full control well
still utilize all the pre defined
facilities and the parallel algorithms
on other paralyzation facilities in the
system so it's really just a set of
customization points essentially we give
you as a user so that you can control
what the parallel algorithms and all the
other parallel facilities we expose do
and how they work and and what are the
specifics ok let me give you some some
results and that might might clarify a
lot of of these things this example is
taken from Shawn parents talk a couple
of years ago and what what he did there
is said ok
I have a sequence of elements and some
of the elements have a flag said and
some of the elements have not arrived
the flag not set and what I want to do I
want to grab those elements I want to
plug them into the middle for instance I
mark up my emails and my email client
click click click click click drag them
and put them somewhere else
so if you want to implement this kind of
algorithm you probably would come up
with a naive implementation which would
look horrible but as it turned out you
can very simply implement that by
splitting that problem into two problems
the problem above the red line and below
the red line and now you can see that
it's really just two partitioning steps
I want to do above the red line I want
to get all elements which are not marked
first and then the marked ones and below
the red lines I want to get all of those
which are marked and then the unmarked
ones and luckily the standard template
library already gives us a facility for
that which is called stable petition and
I created or Sean created a function
which he called Gaza which is called
with four arguments the first the last
of the whole sequence PSC required
insertion point where I want to drop the
marked elements and the binary predictor
the the the predicate is a function
which will be called for each element
which will decide whether that thing has
has been marked or not so cos stable
petition from beginning to the insertion
point and that's a partition was he was
inverted predicates and then the stable
partition from the insertion point to
the end was it was a straight predicate
stable partition gives you back the
point of the of the partitioning so the
overall function gatha
gives you back the region of the object
where the the marked objects have been
inserted into the overall sequence it's
just two iterators which would give you
those now what I want to show is that
what you can do and how you can create a
synchronous algorithm which does the
same thing but in a synchronous manner
so that you can launch that thing let it
run on the side and you get back
the future which will present the
execution of these two things well I
just called the function get gaza async
now and the difference is they are all
the arguments are the same but it now
returns a future representing the pair
of iterators we use stable partition
with the task execution policy that
means that this will be launched on a
side I get a future which will present
the whole result for the one partition I
do the same for the other partition I
now have two futures representing the
two executions of the two partitioning
steps and the interesting thing is that
I already get a additional
parallelization effect here because not
only each of the algorithms runs in
parallel but the two algorithms run at
the same time because they really just
kick off the operation of those
petitions and the next code looks a bit
more complex and I will explain it and I
will simplify it and in a second so
don't get too upset about it what
happens here is dataflow is a function
which is very much like async
it gets a function to call on a new
thread and a set of arguments and if one
or more of the arguments to dataflow in
this case f1 and f2 our futures then
dataflow will wait with launching the
new thread until all the futures have
become ready so it will just delay the
the scheduling of the function which in
our case is that return make are 1 or 2
and it will pass those arguments
directly to the function the unrep
facility here you can see that lambda is
put into an unwrapped just gets rid of
the futures adjust since we know that
the function will be invoked only want
that at the point when the futures have
become ready we can simply call get and
unwrap does as for us and forwards the
actual values so dataflow will return a
future to result of the function which
in our case is the future to the pair of
of iterators we want to have well you
might say that's complex code I don't
want to write this kind of code
well.here course proposal of coroutines
comes in because if you use coal wait
which is the one part of the of the
co-routine proposal you get exactly the
same semantics in an absolutely straight
way essentially what we've done is we
turned our straight code which Sean
wrote three years ago into the
equivalent a synchronous code just by
changing two small things we changed the
execution policy and we added the co way
to to get the values out of all those
futures and we did another thing if you
think about it what does that function
do that function gives me future or
presenting the pair of iterators and
that future depends on two other futures
which are returned from the partitioning
steps so essentially this function gives
you an execution tree which represents
the execution of your algorithm you have
inside so we turned our file our code
which was straight synchronous execution
into a code which does not do anything
but giving you an execution tree
representing the algorithm and when that
execution tree is executed you actually
do the work and you get the result so
essentially it's out of parallelization
because the execution of that execution
tree can be performed at full speed it
will just flow there will be no
synchronization it will just fully
paralyzed utilize a system in them to
the full extent and that technique which
we call future ization can be applied to
arbitrary complex algorithms so you take
your textbook algorithm apply the future
ization technique and you get a fully
automatically paralyzed algorithm just
like that just by using futures as see
data dependencies because futures
represent data dependencies nothing else
okay another couple of another examples
you might have heard of the stream
benchmark the stream benchmark is
usually used to assess the memory
bandwidth available
on a particular architecture the
original stream benchmark is available
on the web and usually uses for
assessing the memory bandwidth on a
single no it usually uses openmp so I
will compare these stream benchmark
results with the original open P
benchmark was implementing the same
thing without parallel algorithms the
stream benchmark essentially does four
things it's for loops well it's three
years and you perform four loops over
those arrays you do copy step a scale
step you add two of the race and then
you have a try out step where you access
to arrays and multiply one of the
factors with it and assign it to the
third one so the best possible
performance in this case on a single
node is certainly only possible when you
have new more awareness when you when
you place a data close to your Numa to
your socket while you run the code on
because as soon as you have crossed Numa
domain traffic you you just kill the
performance in Oatman P that's done
implicitly by a so-called first touch
policy so you have to write a rely on
very arcane low-level features to make
sure that this read has really run on
the same socket where they where the
corresponding data is located and you
use that pragma OpenMP parallel and you
schedule statically and we start with
parallel algorithms that looks like this
you just have three vectors you init
data you run the copy step who is a
parallel execution policy you run the
transform which is doing the scaling
another transform which is doing well a
binary transform which is adding the two
arrays and another binary transform
which is performing the triad step
nothing else so it's really just for
parallel for loops or for parallel
algorithms one after another and that
mimics hundred-percent the way of my
peers is measuring that memory bandwidth
now let's do that on it on on a CPU
target I create an object which is
called host target give it some
initialization tell it a run on new
model main zero for instance
I create a executor and a allocator from
their target which will make sure that
the code will be executed on on Numa
domain zero and the data is allocated on
the same very close to that socket and
then I create three vectors a B and C
using that a locator which makes sure
that the data is located on that memory
close to this new model main zero and
then I create a execution policy where
rebind the executor and tell it to do
static chunking well and the rest is
history the rest is really the same code
as before except that instead of the
power execution policy I use my rebound
execution policy and that's it if you
look at what we get when we run it on
one Numa domain the Green Line is what
we get with the parallel algorithms the
blue line above is what the original
openmp benchmark gives us so we reach
about 97% of the performance and we
haven't looked too closely into it to
optimize it even further but I think we
can still squeeze out the remaining 3%
so essentially we get equivalent
performance to what openmp gives us
today was full control of a memory
placement from a very high level when we
run the same code on to nuuma domains
you you see very similar behavior there
are still a 5% abstraction penalty I'd
say we currently have to deal with but I
think we we should be able to and we
will be able to find or to optimize that
and and reach the full of memory
performance
I know beyond a set this morning we want
to have a speed-up of 10 I would like to
have a speed-up of 10 as well but please
keep in mind that openmp really
perfectly utilizes a memory band was
here so we are dealing with hardware
limitations not was code limitations so
reaching the open p performance is the
best we can do now let's run the same
thing on a GPU this time we create a
target where we specify which device
actually to use create a different type
of allocator and the
type of executor create local data on a
CPU which we will copy over to the to
the device create our three vectors a B
and C these three will be allocated on
the device then we copy the data from
the CPU over to the device and look we
use standard copy and that's done that
copy just copies the local data over the
two device and please note if we were
using instead of power if we were using
power task as the execution policy that
copy operation would give us a future
which would allow us to completely
overlap the the operation of copying
data from this CPU over to the device
behind code on the CPU so we wouldn't
have to do anything special there and
then what the stream management as it
was before there's no change there so
almost the same code the only difference
is that we have two different types for
the executor and for the allocator here
and things will run on a GPU and if you
look at the data we get here we can see
that for very small array sizes the
overheads off are significant and that's
probably because of and that's compared
with was native CUDA the overheads come
mostly from the synchronization with
with the CPU code where we rely on CUDA
invoking a callback function in the user
code and this relies on operating system
context switching kewda itself doesn't
rely on that
so the overheads come from here but for
decently sized the race you can see that
we essentially reach the same
performance as a native CUDA
implementation of the of the string
benchmark for on the right hand side so
if you zoom in on the on the largest
array sizes here you can see that for
compared to Oatman P we reach 95% as
said and for kewda will reach 99% of the
performance possible so again the
simplest possible
how powerful they are and as you can see
they're very
versatile and completely under your
control don't impose any significant
overhead over the native implementations
of the underlying runtime systems which
companies have been developing for 25
years I don't know okay
the last example vectorization and I
think I'm five minutes right let's do a
dot product of two vectors just floats
for simplicity here and we can use in a
product which is parallel algorithms
which is part of the parallelism Cheers
and I hope everybody knows what a dot
product is you take pairwise you
multiply the two vectors pairwise and
add up all the results essentially so
it's it's a basic operation needed for
metrics of multiplications mainly and
and for for other linear algebra
programs so the inner product gets two
we pass two operators to it the operator
to be used to do the pairwise operation
which is the second one and the operator
which is used to sum up all the results
which is the first of the two operator
so it's essentially plus and multiplies
and please note I specifically used C++
14 generic lambdas here and this is just
giving you parallel performance parallel
execution of that dot product which is
very nice but if you change just
execution policy from power to data
power you get in addition to
parallelization you get vectorization
and now these lambdas are being invoked
with vector packs so with with a special
data structure which consists out of
four of the of the values and the
operations are performed vectorized on
these four or eight depending on the on
the underlying hardware platform values
so you you essentially gain value you
get vectorization for free so let me
show some data here this is what you see
for two vectors of 100,000 floats
for the the y-axis is see they speed up
and the x-axis is a number of course
this loop runs when you run it on war on
one core you can see that just by
changing the execution policy you get a
speed-up of three which is significant
so all you have to do we change power
for data power and you get a speed-up of
three isn't that great in addition when
you run this parallel loop on more than
one core you still see a speed-up but
the speed up is not as significant
anymore and the red dotted line gives
you the ratio between the blue and the
green lines so the speed-up from
parallel to vector eyes and parallel
code is in between 1.3 and n3 roughly
this is a fairly small array 100,000
points and here the parallelization
overhead you can see that the
paralyzation already takes in fairly
fairly massively so you don't really
scale up as well as as you might want so
if you look at the same for a array size
of 1 million you have almost perfect
scaling for the parallel case and you
see now that the the speed-up of the
vectorization isn't breaking that fast
anymore this problem still fits into
level 3 cache so you can see for certain
array sizes you can really gain a lot
from from just changing that that
execution policy and the last one is for
10 million points and here the problem
doesn't fit into the caches anymore so
additional memory access overheads have
to be accounted for so the fact is not
as as as massive anything anymore as
before but you still can just by
changing the execution policy you can
still see you speed up of at least one
point 3 to 2.5 or so very nice result
because it's it's essentially cheap
right all you do is just instead of Tory
write data per and you get some speed up
that might not be completely optimal but
you get a very very nice first step in
optimizing your code
let me yeah so I'll skip the the example
on was there was a partition vector
because time's running out
essentially what what this code shows
you how to you can use a partition
vector and distribute the partitions
over several notes and by changing the
target object do the same on a bunch of
GPUs so no code change and you
distribute the partitions over the GPUs
and then you run the parallel algorithms
on those partitions in parallel was was
without you having to deal with the
partitions and so on and so on so very
nice the very last example and then now
I'll stop if you have this for loop and
you have power execution policy and as
you can see this loop works on three
arrays wouldn't it be nice if we could
prefetch B and see a couple of
iterations ahead of time so that the the
Hardware prefetching could make sure
that B and C are in the caches when we
need them well the easiest solution is
this we just say power was prefetch bin
C and that's it so you can write very
powerful means of customizing that the
way these parallel algorithms work and
you can really drill in and optimize
things and turn knobs as you wish and
and get very very nice performance
because what this prefetching does is
just gives you about 10-15 percent speed
up just by prefetching these two arrays
and I think I'll skip that I think
that's it so I thank you for your
attention if you have any questions I'd
be happy to answer those</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>