<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: D. Dechev &amp; D. Zhang  “High Performance C++ Concurrent Transactional Data Structures&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: D. Dechev &amp; D. Zhang  “High Performance C++ Concurrent Transactional Data Structures&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: D. Dechev &amp; D. Zhang  “High Performance C++ Concurrent Transactional Data Structures&quot;</b></h2><h5 class="post__date">2016-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uDNb8JL0vv8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone so people are
still coming so I'm a as well as the
skillet started I'm going to talk about
transactional data structures which is a
step beyond non-blocking
or concurrent data structures I'm Kelly
and I recently graduated University of
Central Florida and joined Microsoft and
most of the talk is based on research
projects that's been done in UCF
together with my advisor dr. demian that
you so in this talk I'm going to first
this discuss what transactional data
structures are and why we need them in
concurrent programming and I'll explain
three piece of methodologies that will
enable us to build high-performance
transactional data structures in c plus
plus the multi resource lock lock free
transactional transformation and the
multi-dimensional list now after that I
will show you some experiment result
with the components and conclude the
talk with a summary and discussion on
future work now at the beginning of the
mantiqueira we know we have to exploit
parallelism and now we are already at a
doorway of Manu korero
here is a picture of the Intel Xeon Phi
processors with 72 full-fledged X 80
score as eighty six cores
now this processor is still on the
market in the following month is and in
the same time we have this year's number
one top 500 supercomputers goes to a new
system built in China which has 260 risk
of course the main core chips are here
do we have admin software to take
advantage of them for the past two
decades researchers has been proposing a
large name of non-block in data
structures so these data structures
besides the great scalability the most
appealing aspect of them is that they
support strong progress guarantees
such as weight freedom log freedom and
abstraction freedom with freedom
requires that all thread in the system
must make progress in finite number of
steps so that no one will starve and the
weaker log freedom requires that at
least one thread is making progress so
the whole system will not starve and
obstruction freed and only requires a
thread to make a progress without
competing threads
we'll be focusing on logarithm because
it provides just enough progress
guarantee without the overhead of weight
free synchronization now the non block
and data structures they have new seller
for correctness properties the
invisibility is the most widely adopted
one inner eyes ability is compositional
meaning that we can put multiple
linearizable objects into a container
library so that the library as a whole
is still in the risible as a result we
already have many of these libraries
available on the internet such as lips
EDS turbo and interest thread building
blocks here is the graph showing the
linearizable operations we have to
thread access into linearizable objects
a and b so by identifying the
linearization point indicated by the
arrows we could obtain a valid
sequential history at the button so that
the whole system is still in a riceball
now all looks good and it seems we have
been doing a lot work with this
non-blocking data structure so can we
stop researcher and put them to work
unfortunately all existing concurrent
data structures they miss a very
important functionality which is allow
users to compose multiple objects
together correctly this is something we
do effortlessly all the time with a
conventional sequential data structures
so here is a very simple example of a
move function that tries to delete a key
from set a and insert it into set B in
sequential scenario this code will be
trivially correct but in concurrent
execution the operation might get into
our interrupted right after deleting key
from set a thus leaving the two set in
an inconsistent state now what if we
only have one container that's not going
to help
we still need additional synchronization
in a second example we try to complete
some value and put the new key value
pair into the map only if the map
doesn't already contain the key the
thread might get interrupted after the
condition check so the operation might
wrongly overwrite an existing value in
the map a more complicated example
involves two
hash tables consider program tries to
maintain a forward map from a contact
name to phone number and The Weavers map
from the phone number to names if you
want to update a phone number then what
you do is you update entry in the
contact map and you delete old entry
from the full map and insert a new entry
all this have to happen in one atomic
steps but all concurrent there are
structures they don't readily support
this kind of transaction the problem is
that linearizable operations are still
not composable and the main programs are
not aware of this issue so given the
above example if the two hash tables
were using logs then maybe the user
can't get it to work by acquiring all
logs at the same time but this require
the maps to expose the internal logs to
the user which breaks encapsulation or
if the maps are constructing a lock free
manner then there is nothing much user
can do besides ask the library either to
provide a new method that's manual
composition which leads to state
explosion so in order to support
reusable software design in modular
fashion we need to have a transactional
transactional execution support in
concurrent data structures now similar
to a database transaction we say a data
structure support transactional
execution if we can execute a sequence
of operations atomically and any
isolation atomicity requires that if one
operation fails the entire transaction
should abort and isolation requires that
the concurrent execution of transaction
appears to take effect in some
sequential order that respect the
real-time ordering now if the data
structures satisfy these two property we
say L is strictly serializable this is
analog of linearise ability for
transactions now let's review some of
the current applicable approaches that
we can use to build some preliminary
transactional data structures given a
sequential data structure we can of
course easily apply software
transactional memory any code that's
been wrapped between the TM Begay and TM
and will be executed in the atomic
fashion so here I show a simple example
of in so
function taking from a sequential order
list what we need to do is to notify the
STM when we read and write a shared
memory location by using the TM read and
the TM write macro we also need to like
the STM take over the memory management
so in principle an STM instruments read
memory accesses which were calls the
location of thread weed in reset and the
rights in the right set conflicts are
detected among the read white set among
different thread so in the presence of
conflict only one transaction is allowed
to commit and the others will be aborted
and restarted so because STM instrument
slowly mow a low-level memory accesses
it has large runtime overhead and this
is one of the reason that STM hasn't
been so widely used yet and a more
important aspect is that the low-level
memory access conflict they do not
necessarily correspond to data structure
level semantic conflicts so this way STM
causes way more ports than these two
here is an example consider we have a
linked list and thread wire and thread
two tries to insert key 4 and 1
respectively now any good concurrent
implementation of linked list should
allow this to operation to proceed
concurrently because they actually write
to this joint memory locations but in an
STM they do have read write conflict on
node 0 so STM has to abort one of them
to address the semantic conflict issue
hurricane koskinen they propose the
transactional boosting which is actually
a very cool technique to obtain
transactional data structure from base
in res body or a structure in
transactional posting all we need is
linearizable data structure to start
with and each operation will need to
acquire an abstract lock before actually
executed operation so the abstract locks
are designed based on the knowledge of
commutativity of the data structure
operations so that they ensure that non
commutative operations will never occur
concurrently and in case of a
transaction fell the transaction will
invoke the inverses of already
operations to restore the abstract state
of the data structure now here is the
specific specification of a Scylla data
type we can see that insert and remove
function they are actually committed
with each other as long as the key the
access are different so the abstract log
of a cell Adel type will be associated
with each day lucky and for example if a
thread tries to insert key two and three
at the same time in the transaction it
needs to acquire actually two locks one
associated with key two and the other
associated with the key three now the
problem with transactional boosting is
that although it provides significant
impaired performance the ASTM the use of
locks degrades the progress guaranteed
when you supply to log three data
structures and as this graph shows when
transaction failed to acquire a lock it
has to roll back already
a transaction in this case it needs to
insert the node nine back into the data
structure which causes additional
overhead because this rollback process
does not contribute to the overall
throughput so I'm going to introduce
some of the research what we did in
order to build path performing
transactional data structure in C++ the
first piece is multi resource log which
is a centralized lock manager that
allows you to acquire multiple locks at
once and this can work together with the
transaction boosting technique for
legacy log based data structure to
convert them into law lock based
transactional data structure and the
second piece is the log for a
transactional transformation this is
actually a methodology to obtain
high-performance transactional data
structure from lock free based data
structures so after the transform
formation the data structure will still
be log free while supporting
transactions now the third piece is a
multi-dimensional linked list which is a
brand new search data structure designed
from ground up that is optimized for
concurrency and transaction support so
lastly we need to put all these pieces
together so that we can have a framework
to support cross container
actions and the user can compose
transactions among multiple instance of
data structures
now before diving into details our
briefly review some of the techniques
that we used in the log free programming
so the compare-and-swap is a atomic
primitive that only updates a memory
location if the current value matches
our expectation and most clog-free
programs are based on caste based with
high loops in which the operation
continuously to try to update a memory
location using cache and until it a
succeeds now the descriptive object is a
small memory structure that will store
the exclusion contacts of the operation
in case the execution of the operation
is delayed or interrupted another thread
can pick this up and continue the
execution the cooperative execution is a
mechanism for thread to help enjoy the
finish unfinished pending operations
so a typical log free code will look
like this operation starts with a new
operation and creates a new descriptor
for this current operation and it reads
the current note and see if there is any
pending operation if so it will sell out
to help finish the pending operation and
finally what tries to update the
description current note using the cache
now remember that transaction the
boosting require an operation to acquire
a abstract lock before execution so for
multiple operations in the transaction
the transaction poustinik
technique requires a data structure to
acquire magical locks so if we treat
each day item as a resource then in this
case we can actually reduce the
transactional synchronization problem to
resource relocation problem which tests
that given pool of key resources that
would fire exclusive access each thread
we may request one to K resources and
the thread shouldn't remain blocked
until all the resources available now
this can also seen as a generalization
of the famous dining philosophers
problem generally there are two strategy
to approach this problem we can use
locking protocols and assign a lock to
individually
this and acquired the locks one by one
so this locking protocol has long been
adapted by the database concurrency
control such that we have the two phase
locking resource hierarchy into this
locking we're only allowed to acquire
locks in one phase and release them in
another and we suppose hierarchy we
assigned Toto order to other locks and
all threats should acquire them in the
same order now the problem with these
locking protocols is that especially
with two phase locking is that when
running on the shared memory system they
are prone to conflict in a retry if the
memory contention is level is high so
another strategy is to actually acquire
multiple locks in a batch and spatch
walking Emma lock is such an example but
before explaining it in details I will
briefly show you the extended ta-tas
lock which is a simplified version of
Emma Lock now in the extended EITS lock
each thread while we encode it Swiss was
we passed in a bit set and each piece
will represent one resource so the
conflict among resource requests are
detected using bit wide and operation
between the value P and R and if there
is no conflict the lock value is updated
using the bitwise operation and the
cache so this way the lock can handle
requests in batch but there are a few
drawbacks it provides no fairness
guarantee
so all words are contingent ending for
the same memory location so whoever wins
the cash wins the access to the request
and it causes heavy contention that's
the same reason because it has only one
memory location for you to contend most
importantly the only support a limited
number of resources because on a 64-bit
system you can only cast six four PS at
the same time now in the Emma log we
introduce a array based lock three 5oq
to replace the actual memory location so
that each thread has now allocated a
cell to write its resource request this
way we can guarantee five of Veronese am
contenders and it can support unbounded
number of resources and can handle
contention value so how does Mr Locke
works the graph on the Left shows that a
thread well in Q is resourceful passing
to the Q and it now scans from the hell
of the queue for resource conflict with
any outstanding resource requests for
example in cell 5 a thread is requesting
to access the last resource so when it
scans from the head is this no conflicts
now it's good to go it gains access to
that resource but when another thread in
queue is request in cell 6 it says
conflicts with the outstanding requests
in cell number 3 so it now it has to
wait either is ping or whatever then
when the cell when we win a thread
holding the lock on cell number 3 we
listed by selling the beasts to 0 the
the thread now well scan on cell number
4 because there is still one outstanding
resource conflict now the data structure
of the analog for each cell with torn
bits that request as well as a sequence
number which will be used as a
synchronization signal and the analog
data structure itself contains only
atomic head and its help index values as
well as the fixed size battery now
during the initialization we allocate
the buffer and store all the beads to
ones and set the value of all the
sequence number to equal to their cell
index the acquire function contains two
parts the first part is a cash based
loop where we try to use cast you
increment the tail position by 1 and if
we succeed at this it means that we
allocate a new cell for this request and
we can write the request value into the
bits set and increment the sequence
number to indicate the cell has been
taken now in the second part is a while
better spin loop the spins from the head
of the queue to detect
and you outstanding resource conflict
until reach the position allocated to
this grad now once the local choir
function returns it returns to the user
a spin position which will serve it as a
lock handle for four to be used in the
release function so to release a lock
the use of has a single lock handle and
the cell the bit set in that associated
cell will be set to zero and next we
need to do one thing we need to clear
from the head to advance the head index
but we only do this if the bits that in
the head cell has been already cleared
and if so while we set the beats in the
cell to whines and we well increment the
sequence number in the bits that are in
that cell to indicate the cell is free
to be used again so here is how the
sequence number changes throughout the
lifecycle of the ring buffer initially
we use the sequence number two to do
balance checking and initially they were
set to the index of the each cell with
each in queue the sequence number
increased by one so when the tail wraps
around the deceased that the current
tell index does not equal to the cell
index so the cells taken it won't
overrun the overwrite an existing value
when the tell when the head advances and
release a cell it will increase the
sequence number by size minus one so
that the cell index now equals to the
tail index this indicates the cell is
ready for reuse again the concurrent
update of the ring buffer is safe
because through the use of the sequence
number will guarantee that head always
precedes the tail and the tail is always
larger than head by at most n where n is
the size of the ring buffer now the
important thing is that we don't need to
use atomic operations to update this set
anymore as we did with the extended
t-80s log because the bit sets are
initialized to ones and there are only
there will be only one writer each time
to update a bit set so in the presence
of
go writer intermediate value of the bits
that you're in the right operation
represents some super sets of the
request resources that means we always
block enough resources okay here's the
very code quick code example to show you
how to use a Murloc on existing
concurrent data structure suppose we
have a concurrent set which is
implemented as a fine-grained locking
list and we want to access key to seven
five eight at the same time all we need
to do is we create a resource request
and pass the ITM a lock once the lock
returns we can operate on the container
as normal knowing that these kids won't
be accessed by any other thread until we
unlock so let's take a look at the
problem of supporting lock free
transactions given a lock free data
structure we want to support
transactions while maintaining the log
free progress guarantee so we cannot use
locks anymore or we cannot block any
operations anymore so the illustration
here shows you the challenge giving a
lock for a linked list let's say we want
to insert key for one and seven in one
patch but at the same time there might
be another threat concurrently deleting
node one from the data structure before
you finish the transaction so this
actually leaves the first transaction in
inconsistent States which breaks the
isolation property so the challenge for
lock free transaction synchronization is
that we need to effectively buffer the
write operations so that they are not
visible to operations outside the scope
of the transaction and we also need to
efficiently row backfield transactions
in order to restore the abstract data
structure state now I'm introducing log
three transactional transformation this
is actually a methodology and to
transform existing lock free data
structure code into log free
transactional data structure when you
apply this methodology what we can do is
we obtain a lock for each transaction or
a structure it
supports lock-free semantic conflict
detection and it was supported well has
a functionality to do logical status
interpretation to limit actually it
remains row backs and it has cooperative
transaction execution to minimize the
world so all these combined it provides
confusing bad performance than the
transaction boosting or STM or an
existing data structure transaction
approaches so currently this methodology
can be applied to set and map data types
and node based linked data structures
such as linked lists binary search trees
skip lists and under lists so in an oft
T we use node based conflict detection
for each node in the data structure we
embed a node infrastructure which
contains the operation ID and the
reference to a transaction descriptor
and in that transaction descriptor it is
shared among all those participating in
the same transaction so in it will all
store array of operation types and
operands as well as a transaction status
the transaction status can be either
committed active or boarded the concept
here is that whenever a thread tries to
modify a node it has to first read the
note infrastructure and hence the
transaction descriptor to see to make
sure that the previous transaction is
not active this way we prevent
concurrent modification to the same node
which is equivalent to prevent
noncommutative operations from executing
on the same key in a set now in the
transform the insert function here is
the workflow the great blocks are
extracted from the base data structure
so we keep those code we use the base
data structures no Travis algorithm to
locate the node content in the key K and
if the node actually does not exist
well well insert it as usual but if the
node does exist we will introduce a new
code pad to update an old infrastructure
under a node if any of these two
operations returned need to reach high
while we start the whole process again
so in the newly introduced update node
info function what we do is that we
first check if the node info has been
pretty marked if so it means that the
node has been logically deleted and we
need to invoke the base data structures
delete function to physically remove the
node from the data structure and then we
test if the previous transaction is
still active and if so we will help
finish that transaction this way we
enforce the civilization at this point
so that current transaction will always
read the last final result of the
previous transaction after that we can
test if the key is logically present in
a set or not I'll explain this in the
next slide but for an insert function if
the key does not exist it means that we
can actually use cast to update the know
the infrastructure on this node now here
is the truth table for interpreting the
logical status of actual key so the
intuition behind this is that we don't
actually need to execute the inverse
operation like we did for a transaction
the boosting technique in order to
restore the data structure abstract
state all we need to do is that one when
a transaction fails the subsequent
operation only needs to inversely
interpret the logical status of the key
in order to recover the correct abstract
state so for an insert function if the
previous transaction is committed we
think the key is present in the set an
aborted transaction means the key does
not exist and for an active transactions
we only consider the key to be present
in the set on for this photo operations
in the same transaction so the
interpreter for a delete operation is
exactly the opposite and the inspiration
for fine operation is that we think the
key always present because this is a
read-only operation will not change the
sellers of the key now once we done with
the transformation of the base data
structure code and up
in transactional data structure what we
do to execute operation is that we
create a transaction descriptor and we
specify the operations in descriptor and
in that execution function it-- well
execute the operations one by one it
will continue to the next one only if
the previous one has successfully
returned and after all that it will
atomically update the transaction status
using the cache to either committed or
aborted depends on if all proceedings
are successful so there is one
additional step is that we need to beat
mark all the know the infrastructure
unsuccessfully didn't note this way we
can physically remove them afterwards
this is optional but sometimes this
helps performance the low the notes will
not interfere with the correctness of
the algorithm is just improve the
performance if we remove the extra nodes
from the data structure now there are a
few salaries here very rarely a thread
may stuck in endless open and this may
because there are cyclic dependencies
among transactions themselves so we
detect and recover from this by using a
help stack so before setup to help the
other thread thread records the current
transaction descriptor in the local help
stack and cyclic dependencies are
detected as a duplicate entry in the hop
stack and then we will abort one of the
transaction to break the cycle now if we
disable the helping mechanism at all
then it means that we have a abstraction
free transaction execution the upside of
this is that we can now keep the
transact the execution contacts in the
transaction descriptor and we can use
many of the existing contention
management schemes such as aggressive
polite and camera so here is a graph
showing you how oft to works with a very
simple example three threads
trying to execute three transactions
thread one finish the first transaction
by inserting node three and wine into
the link
and three to insert in note 4 but it's
currently working on note 2 in the
meantime through a three star deleted no
three by updating the note
infrastructure on that note when it
tries to access note 4 it actually sees
that this note is being accessed by
active the transaction so it has to sell
out help finish in t2 before resuming
delete in old port so wunst 3 3 3
completes its operation both t2 and t3
should all be committed and the note at
the key 3 &amp;amp; 4 will be interpreted as
absent from the set by subsequent
operations now this is a code symbol
showing you how to actually use the
transformed data structure given three
sets instances and all we need to do is
create a transaction descriptor and fill
in the descriptor with a desired
operations for each set and execute it
now we're ready to talk about trans are
two ways to implement transactional data
structures and some data structures they
can benefit from log 3 or the concurrent
execution but some cannot for example a
stack because of these are inherent
sequential semantics we it cannot obtain
much performance benefits even if we
come early to log free but concrete Maps
is one of those data structures that can
benefit a lot from lock free and
transactional institution because of its
semantics is inherent parallel so in
sequential scenario these maps are
usually implemented in by binary search
tree skip lists and hash trees now we
definitely want to a better performing
concurrent map this will improve the
baseline performance because which have
Earth's transaction synchronization
scheme you choose this will bring down
additional overhead and all existing
implementations
they all have concurrency
patience so let's take a look at the
binary search trees one problem with
that is that is the binary search trees
the logical other wing of the nodes of
the keys does not necessarily correspond
to the physical layout of the tree this
creates a very traumatic for fine
operation in lock-free execution so
consider of where tries to find node 7
on the BST as shown on the bottom and
when which node 9 it gets interrupted
and in the meantime another threat
deletes the node 3 from the tree and
change the layout so when threatened
with zooms it does not find node 7 so
now it has to decide if node 7 does not
exist in the tree or it has to start all
over again this problem to solve this
problem researchers has been proposing
embed logic authoring into the denote
themselves or use external trees to
store all the values at a very button so
that they have a fixed position but all
those introduce additional overheads and
the more troubling issue is that the
sequential bottleneck introduced by the
rebalancing process required by a BST so
BST needs to be balanced to maintain the
optimal search time but this process
involves modification to a large number
of nodes and this will likely store
other concurrent operations to the best
of my knowledge there is no log free
implementation of rebalancing algorithm
of a BST but people have been proposing
relaxed dancing and background passing
to alleviate this issue so a Paller
candidate for building concurrent maps
are using is using the Skip list which
is a probabilistic balanced alternative
to BST in a skip list and contains nodes
with multiple links and at the very
bottom the nodes form linked lists and
the upper level links are created with
exponentially lower probability so that
the guarantee logarithm search time and
also the upper level links they are
are actually redundant shortcut links
into distant nodes so one problem with
skip list is that the insert and delete
operation there were modified multiple
nodes well I mean how do I mean by this
is that consider we are inserting node
four and six in the Skip list insert
node four requires us to modify links on
node one and three in certain node six
well requires to modify links unknown
one three and five if this is a if this
is a simple linked list this to
insertion should have no dependency or
interference but in the in the Skip list
they all need to modify node one three
when on one of them will succeed and the
other one we try so skip list introduce
additional access conflicts among
supposedly commutable operations now
what makes an efficient lock free map
based on these observations we want to
maximize the disjoint access parallelism
and the data structure preferably should
have low branching factor so that we can
avoid hotspot and I should have
localized updates to avoid access
conflict and fixed key position to
simplify the process query now in the
proposed multi-dimensional list the
lower level nodes will have smaller
branching factor which brings down the
average branching factor of the data
structure and insert delete function
only modifies two adjacent nodes at most
it has a fixed layout for a fixed set of
keys and does not require balancing or
randomization now the intuition behind a
model is this simple here is the link
list and the insert and deletion
function on a linked list is very
efficient requiring only one cast
updates on the pointer but the search is
linear and which is not ideal for large
key space now what we can do is we can
rearrange the linked list into shorter
sub lists
and to search within each sub list in
this case for this example we have 16
nodes so we arrange the linked list into
four columns well if we could first
determine which key which column a key
belongs to and do a linear search within
a column that would be much faster now
based on this observation we came up
with the 2d list we replace the links
from the button nose to top notes with
links among the top notes and we
introduce a two-dimensional vector which
serves as a coordinate for each node the
nodes are arranged by the lexicographic
ordering of the coordinate and in this
case the worst case search time becomes
the square root of n if we could further
generalize this into higher dimension we
could further reduce the search time now
here's just the definition of the
multi-dimensional linked list for your
reference afterwards I'm going to skip
into the example of a 3d list which will
make things easier now in this example
3d list we have three dimension D 0 D 1
and D 2 along each dimension the nodes
form a sub list so for dimension 0 we
have one sub list and for dimension one
we have four sub lists which are those
vertical columns and we for dimension D
- we have several sub lists which are
those sub lists pointing backwards into
the slides so in order to support
lexicographic ordering of the keys we
need to first map the scalar keys into a
high dimensional vector this mapping
function should be injective and
monotonic so that we can preserve the
other wing of the keys now the map
function should also be preferable it
should be uniform so that we reduce the
average search time of the whole data
structure in our case we can simply use
a numeric based conversion to achieve
this goal so let's choose a base based
on the key universe which is the tone
animal for keys thus who are the teeth
root of the key in Versailles as the new
base and then we convert the death
number two the new base we treat each
digits as the coordinates for the for
the M the list so here we have a q
numbers of 64 keys and the
dimensionality of data structure is 3 so
the new base is 4 when we convert the
key 63 in decimal to 334 in base for we
obtain the actual coordinates for that
key the find operation an analyst while
traverse recursively traverse 1 sub list
on each dimension until it reached the
target note so the green arrows they
highlight a path from the root node to
the node 3 3 2 when traversing each sub
list it only compares one coordinate at
a time so when the search function
traversing the D 0 knows they only
compare the first coordinates of the
nodes with target notes and which
travels in the Y notes they only compare
the second coordinate now the worst case
search time in this case is bounded by
the total number of dimensions
multiplied by the maximum number of keys
on each dimension if we choose the
dimensionality of this data structure it
will logarithm of the key space view
then we can obtain logarithm search time
for the worst case now the insert
function consists two steps the first
step of note splicing actually connects
the new node to predecessor node this is
very similar to what you do with a lock
free link list and this can be done with
a cast pointer update so the producers
quarry while we trimmed the produces
node and the current child of the
produces node and the dimension of the
new node and new dimension of the
current child now in this case we're
inserting node 2 0 0 after a node 1 0 2
the original child of 1 0 2 which is 2 0
1 will be pushed back down to dimension
2 in this case the child the child knows
marked by the red arrow
well no longer be accessible from node 2
0 1 because this lot has been given to 2
0 0 so we have to do an additional step
to transform these to transfer these two
child nodes back to the new nodes so the
child adoption of process is only
necessary if the dimensionality of the
current child is changed during the
insertion process which means that if
the DC does not equal to DP so we we
should transfer all the child in this
range to the new node and since this is
a process we require modification to
multiple pointers on the current child
node we use the descriptor to
encapsulate the contacts and allow
others rest you have finished this
process if this process gets delayed now
normally we could delete node from the
data structure by doing the inverse work
of an insert function but this wouldn't
work for a lock free version because the
insert function may promote or demote a
node which is to increase its
dimensionality and the delete function
may promote node that is to decrease its
dimensionality so during the due to the
helping mechanism several threads may
access the child adoption on the same
node with without additional
synchronization we cannot know if all of
them have finished so there are races
may arise among ongoing child adoption
tasks and the child transfer process the
solution is to keep dimension change
unidirectional so we introduce a
symmetrical delete which is the logical
deletion process that coupled from the
physical removal of the node so here we
want to delete node 1 0 2 we first lot a
bit mark the node to indicate as being
logical to be deleted but the node is
still in the data structure and is valid
for rolling now we only purge the node
from the data structure when a new node
is inserted immediately in front of this
node this way we can transfer our
existing child nodes of the deleting
nodes to the
in certain notes using the same child
adoption process this simplifies to help
protocol and simplifies the
synchronization so since we are using
logical deletion the abstract states of
the map becomes two parts we have a set
of all nodes and a set of logical EDD
you note the abstract state of the map
becomes the and set em - set P so the
linearization point for an inside
function is when the cast updates
predecessors child pointer and the
linearization point for delete function
is one cast marks the processor child
pointer and linearization point for the
find operation is when the child pointer
is grid now we did our experiments on a
sixth for for Numa system with for MMD
CPUs and we compared our source code and
and benchmark using GCC 4.7 with level 3
optimization so for the experiment test
emilich we randomly Iraq for the Mr lock
and it's alternatives we randomly
acquire up to 64 to 1024 resources and
for the data structure obtained by oft T
and M the list we test them under write
dominate read dominate and mixed
workloads so here are the alternatives
which shows for the amarlok for two
phase locking we use STD lock and
together with this mutex and boost lock
with boost new tax and for resource
hierarchy we use it with the STD mutex
as well as the Intel's TVB Q mutex now
these two graphs here it shows the
execution time for acquiring a million
logs
Aquarion releasing a million locks on 16
threats on the Left we have the
execution time for 64 resources what we
can say is that the x-axis represents
the resource contention level which is
which is the number of requested
resource divided by the number of total
resources as we increase the number at
the resource contention level the
locking protocols
including to the smokiest was hierarchy
the extreme time all increases linearly
but for analogue and extended TI TS lock
the execution time keeps almost constant
this is the major difference between
batch locking and locking protocols is
that the batch locking is insensitive to
resource contention because whatever how
many resources you are requesting is
only handled in the same way
overall the analog outperforms the best
performing out alternative which is the
resource hierarchy with a queue lock up
by up to three times for a 64 resources
and up to five times for a thousand 24
resources one thing to note is that when
the resource level contention level is
very low
actually the locking protocols they have
a huge advantage because the batch
locking they have a fixed start up
overhead and here we show how the
execution time scales with the
increasing number of threat now the as
we increase the number of threats the
first the graph on the Left shows for
resource contention 50% and 64 resources
the extended t-80s lock actually
performs really well with the small name
of a threat but as the number of quest
increases the contention level rises at
a single memory location and eventually
on 32 threats the amarlok begins to
outperform the extended t-80s block now
afford the thousand 24 resources the
amarlok outperforms the alternatives all
the time now the next benchmark is to
test the transactional data structures
we test two transactional data
structures one transaction or skip list
and one transactional link list for skip
list we compare three versions the log
for each transaction transformation and
the transaction boosting as well as the
object based STM
and four linked lists we compare with
the transaction boosting and a word
based STM now in this graph we show the
throughput of the of the data structures
for mixed workload on a Numa system with
1 million ki now the higher throughput
the better so and also we append a
number to the named to the name of the
data structure to indicate the
transaction sites so oft 2 means log 3
transaction transformation with the true
operation for transaction now overall
the our FTP approach obtain the average
of 60% speed-up over the boosting
approach and 3 times speed up over the
STM and we can see that as we increase
the size of the transaction the overall
throughput of all approaches drops
slightly and this is because when we
increase the size of the transaction if
one operation fail with the larger
transaction it four-phase actually a
larger amount of work so this will cause
the job in throughput when we increase
the size of transaction and now the
pollen half the graph actually shows the
number of spurious reports for each of
the approach the spurious boards means
that the number of boards number of
total ports not count in those caused by
failed operations this is actually a
measurement of the efficiency of the
contention management scheme now since
we are using helping so for skip list
most of the time if we encounter a
conflict that we're all set up to help
each other hence there will be no abort
but for STM and the boosting approach
the number of posts is pretty high so
here we show the throughput for a linked
list with mixed workload with 10,000
keys so for start-up with the small
transactions we only obtain a very small
advantage over the boosting approach but
as we increase the size of the
transaction we are getting more
performance benefits over the boosting
because in the case of larger
for for the linked list it is actually
very expensive to execute a Weaver's
operation and a linked list then in a
skip list so the performance of the
boosting approach will chop
significantly after we increase the site
of the transaction where we encounter
more upwards and retries now for the
number of spurious ports we do encounter
it's known a small number of spurious
ports in this case for larger name of
threat but it's still two to three
orders of magnitude last and the
alternative approaches so for the M the
list based dictionary we compared
against the log based and log 3 and our
Cu based PST we also compare it with the
three log free skip list implementations
now this graph here shows the throughput
of the Amulet of the all the maps with 1
billion keys and 50% insert 50% delete
operations so this is pretty white heavy
workload and we can see that we obtain
as much as a hundred percent speed-up
for sixty resources because the MD list
there has really optimized for right
operation with only it only modifies on
most two nodes so for this kind of
workload is just performance the best
and if we decrease the key size to a
million and increase the read operation
with our mixed workload we can see the
MD lists can still obtain up to 50%
speed up over the best-performing skip
list now when we come to your case where
we have a really small key wrench let's
say a thousand key and the really read
dominate workload with 90% fine
operation this is when the BST really
begins to shine because BST have really
shallow depth in this scenario and for a
thousand key the dimension which use for
the MD list which is 16 is too large
this has too many overhead so here this
graph shows actually perimeter sweep on
the dimensionality of the data structure
as well as the number of threat
the thing to know is that this is for a
right to dominate workloads with 50%
insert and 50% delete we can see that
the maximum throughput of the end list
always converges through the 20
dimensions this means that the
throughput of this data structure is
independent from the concurrency level
but dependent on actual work level so
what we can do is that we can tween this
data structure according to your
specific workload and then when you add
in more threads it will scale
automatically so to summarize the
performance characteristics the AMA log
excels at medium to high levels of
resource contention and is suitable for
large pool of resources and the L of T T
based transactional data structure
excels at the larger transactions and it
has improved the success rate with
minimal sprigle support and M the list
excels at high level of concurrency and
a large key space it is optimized for
write operations now here is no view of
the Liberty Act II at the very bottom
level we have three unmanaged containers
which are log three data structures
without memory management scheme these
data structures will be used internally
or externally by the memory management
module and the log three profile or
logger
our management means that their memory
will not be reclaimed until the data
structure itself is being it's destroyed
so on the upper level we have two
strategies the analog we provide a
resource mapper that allows you to map
abstract logs into the pit set in
yemalog to be used with the conventional
log free our lock based data structures
and the RTG we provide for data
structure building lists and lists skip
lists in the hash table with the two
type of interfaces set in a map now in
the future we plan to support wait free
data structures the work to be done is
to add width we add a procedure to
support width we update on the note
infrastructure and to have weight free
memory management system we plan on to
also support a non linked data
structures
but this way we have to associate the
know the infrastructure actually with
the HDL item and we may be also we can
do automatic code transformation to
actually obtain from a existing
lock-free data structure just to apply a
transformation allow you sir to identify
the code blocks and we will keep the
transactional data structure as an end
result now here's some references to the
contents and the link to the source code
as well as our contact emails that
concludes my talk and I'll take any
questions you have
all right questions
yeah
so the question was are we proposing
anything for the C++ standard is that
right and not right now no we are not
proposing anything we're just building
this as a experimental external library
for user to use for experiments with
concurrency
all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>