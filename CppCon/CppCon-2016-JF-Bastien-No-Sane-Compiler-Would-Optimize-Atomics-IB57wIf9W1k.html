<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: JF Bastien “No Sane Compiler Would Optimize Atomics&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: JF Bastien “No Sane Compiler Would Optimize Atomics&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: JF Bastien “No Sane Compiler Would Optimize Atomics&quot;</b></h2><h5 class="post__date">2016-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IB57wIf9W1k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thanks for coming my name is
Jeff Passan I'm a compiler engineer and
also a member of the super stands
committee so today I'm going to talk
about an interesting topic Atomics and
specifically no single compiler would
optimize Atomics now this is a very
clickbait title with a clickbait
abstract false so the premise that I
have is that compilers do optimize
Atomics and it's not a silly thing right
they don't just talk to my stomach's
they've optimized around the tonics they
use a bunch of architecture specific
knowledge to optimize things and it's
kind of fun to optimize things when it
gives you a performance win and
furthermore I think that hardware
vendors should give us no atomic toys
taught to my stuff better so we can you
use multiple cores in an interesting way
and the standard should also get better
when it exposes
Atomics right so I try to convince you
today that that's actually a good thing
all right now a quick disclaimer here
the this is this is my mental model of
lottery code right so my goal today is
not to convince you to use Atomics right
my goal today is maybe to dissuade you
from doing so because it's actually kind
of hard and I recommend that you watch a
fatalist
talk that he gave maybe Monday or
something like that and the talk that
Hans gave yesterday because they dive
into how do you actually decide whether
do it or not and how it actually works
right what the the technical framework
is around it what I'm trying to do today
is just show you assembly right and I
think my talk will have the distinction
of having the most different types of is
a assemblies in this whole conference by
like two or three X so it'll be fun
alright so I think you should instead
probably just use like the pals in TS or
mutexes or something like that right and
again the standard should get better in
what it exposes to you now I'm gonna
steal for it freely from faders taught
yesterday and show you you know when do
you want to use Lots free Atomics right
there's he had a nice decision diagram
and the really hilarious thing is the
node that says does it work right now
now I asked him afterwards and that was
actually a mistake right so he made a
snarky flag without realizing it it's
hilarious right but it really describes
really well when you should go a lot
right so what I'm gonna do first is just
do a quick review and I'm gonna go
really fast through this review just to
kind of show you what's in C++ what our
tools are and what we do with it
so in general C++ 11 added a few things
to the language ad first a memory model
and threads in added classes to
communicate between threads right so the
thing to remember is before C++ 11
threads did not exist right now you know
that's the standardized view that there
was something that existed and some of
what was exposed had to kind of take
that into account now what's interesting
when we usually do like free programming
or just you know multi-core programming
in general is is that the mental model
that we have is actually not super
related to how hardware actually works
right so the way we like to think about
about how the hardware should work right
is in terms of sequential consistency
right and that's a pretty naive model
right but the model the mental model is
really easy to understand it's a zipper
right like you have stores that happens
and and they just kind of zip together
and neatly right now sequential
consistency is that thing it's real easy
understand that's not actually how
Hardware works and if you try to use it
this way it would be kind of slow right
so C++ has sequential consistent
accesses and they happen to not be the
best thing at that point you may as well
not use Atomics long and and it's really
overly restrictive and expressing what
you want to do and I'll try to give you
a few examples later and phaidor in his
talk also had a few examples of you know
not sequentially consistent but that's
close to what you want and the reason
it's slow is because it really limits
what the hardware can reorder and limits
with the optimizer can do alright and in
a lot of architectures and ups ends up
adding fences and fences depending on
the architecture cause from 2 to 200
cycles right that's a ginormous cost for
something that were you're trying to be
fast and so the way c++ fixes that is by
exposing a model called sequential
consistent for data arrays free programs
right so that means is if your program
doesn't have a data race then you get
sequential consistency just kind of
magically all right now that it's about
a bit so what's a data race well a vaper
is what happens when two accesses the
same memory location by different
threads aren't ordered now that's really
technical and at least one of them
stores the memory location and at least
one of them is not a synchronization
action now that's taken from the from
the standard itself right I won't
explain what that means in details and
why that is because that would be a
wallet text right the key takeaway to
get here is that data races are
undefined behavior if you don't have
data races then you effectively have
sequential consistency for your data
race free program right so the standard
defines this in the section called
interval multi-threading it's 1.10 if
you go there it's beautiful right I
really recommend you watching hunts at
the same recommendation yesterday in his
presentation now atomic operations and
Tomic operation is indivisible with
respect to all other atomic access to
the object and we have a bunch of memory
orders in c plus plus we have relaxed
relaxed is actually kind of interesting
so I mentioned you know sequential
consistency relaxed kind of breaks that
models mount model a bit so relaxed
allows you to have racing accesses so
one race one relax access the same
location is another one can't can't be
reordered but relax essa sees the
different locations can be reordered
with respect to each other now at the
same time the property of relaxed is
that it is still indivisible right so
you you can't see word tarry you won't
see half of a right or something like
that or your right won't just have
succeed through relax and that that's
usually the fundamental property people
think about when they say atomic they
usually think about indivisibility but
there's a bunch more stuff that actually
means and relax doesn't force cache
coherency
the thing about relaxed though is that
it doesn't guarantee visibility right so
hardware usually it's kind of
well-behaved where like you'll have a
cache and it'll become visible pretty
quickly but you could actually have an
implementation of c++ and hardware where
the cache doesn't really make the the
story that you do visible until you tell
it like hey okay maybe you should make
that visible and we'll see implications
of that a bit later
so hunts had had an interesting quote
relaxed in his talk yesterday he said
that C++ 14 replaced the offensive
wording about relaxed with hand waving
now that that's kind of an interesting
quote coming from someone who like was
heavily involved in creating a language
he also said something else when you use
relaxed well it's probably incorrect if
you care about the result right
so the it's kind of interesting but
there are valid uses of relaxed and his
basically his entire talk was about that
yesterday so the basic tools that we
have in C++ for doing atomicity are the
atomic objects right so there's basic
overloads right so it's just the class T
that takes just whatever and then
there's an integral overload there's a
pointer overload and maybe in the future
we'll have floating-point overloads as
well right and the idea in C++ is that
whenever you access things atomically
you always do so through an atomic
object right so atomicity and c++ is
based on the memory location object
being atomic it is always atomic it's
not in this part of the code i'm yeah
i'm using this atomic li in this other
part of the code i'm using the same
memory location non-atomic li and that
has pretty deep implications to the
programming model they you either use it
and other systems have different
programming models for example the Linux
the programming model is based on the
accesses themselves or it's a one part
of the code could be atomic the other
part could not be a ton and that's a
fundamentally different way of
expressing things now atomic T has a
bunch of operations on it and they're
actually kind of simple right you can do
load store exchange compare exchange you
can see if things are a lot for you or
whatever
atomic integral has a few more things
add subtract or whatever T star can add
and subtract only and then floating
point maybe can do and and subtract
we'll see maybe we can do more maybe
we'll have like atomic FMA that'd be
kind of interesting but weird maybe we'd
have comp exchange or something like
that otherwise you'd have to copy it
into integer side into the competent so
after that you have fences and hunters
make and model fences is just don't use
them right the the usual recommendation
is that there's not really
good way to use fences or a good reason
to use fences there's one explanation
for where it is valid is when you use
sec locks now there's big disagreement
around there but I'll give you the
example later where that's valid and
then there's signal fence which isn't
really about parallelism but it's more
about preventing ordering when there's a
signal that happens all right so a
single that interrupts your current
thread and then there's a whole concept
of lock freedom if you remember atomic T
can take just any T right so any T could
be ginormous now there must be deep
magic if you can take a struck that's
like a page wide and just make that
atomic right just do the right and one
go and what C++ does is it says well the
hardware supports one single operation
they'll make that right happen
atomically so indivisibly and that's
cool right then the access is a lot free
but if if there's no instruction they'll
do that and most architectures just
don't have a wave barring transactional
memory of making a whole ginormous right
to be visible in one go then what c++
does is behind the covers it has a bunch
of locks hidden right and so if you
access Atomics objects are way too big
to happen in one shot the atomic object
actually has a lock behind the covers
now that's kind of silly I'm just not
really going to talk about non lock
three Atomics it's kind of a weird thing
to even try to make it work but that's
what the language does now lock freedom
is only guaranteed for one type in C++
and that type is atomic flag and it's
really not that useful it does a few
things but all the operations I showed
earlier not not all of them are
available on that type and and the way I
see it and I may be wrong right but the
way I see lock freedom is the other
context change on your architecture that
I'll do you know that sighs all right
it's on x86 you'll have like up to
complex chained 16 B now that that still
doesn't mean it's fast right can't be
changed 16 B in some cases is kind of
slow and then then yes so I'm really
gonna focus on on lock 3 things for this
is this stuck and yeah so I just kind of
well show you a quick example of what
lottery code looks like it's a horrible
example don't write this it's not good
code right but the basic idea is that
you you have like a publish/subscribe
thing in this case right so you have a
writer that writes a bunch of data
notice that none of the rights of data
are atomic the only thing that's atomic
is the flag that we have
toys right so the writer writes his
stuff says all right here's the thing
I'm done release all the the things and
the reader says oh hey I'm just gonna
wait until that flag is set and
afterwards it can read all the data
right so spam bacon and eggs only get
read once the flag is set and you will
your if you use that protocol you're
guaranteed to have the results you want
now the idea here is that you run this
into two threads and there's a there's a
synchronization between the two threads
between the the store release and the
load acquire right and so it means that
the write that happened before in the
the writer thread are seen by the reads
that that succeed the the load acquire
okay it's fairly simple right so that's
all the tools we have within C++ to do
atomicity all right so we have our tools
but why is the language set up that way
well it happens that it makes it easy to
prove certain properties about the
language and about like what can happen
right and so someone actually spent a
bunch of time writing a PhD to to create
math to represent that memory model and
proved a bunch of interesting properties
about them remodel and and found a bunch
of failings in the memory model that God
addressed before C++ a lot right and
it's actually kind of rare in designs
like this that you can use maths to like
prove that something works in a certain
way so I think that's actually kind of
cool now the other idea is that like C++
doesn't get to dictate how Hardware
works right it is it's an important
language but it just kind of does stuff
right it kind of has to follow what
hardware is able to do right so the
abstraction was designed with the goal
in mind that like you can write your
code and it'll work pretty much the same
on all the architectures if you fall if
you don't fall into undefined behavior
right and then the idea is well if it
doesn't work correctly you should use
tooling like the sanitizers or something
like that to try to find the bugs right
now what I'm going to do is I'm going to
go through a few architectures and try
to show you how they implement Atomics
all right so on x86 64 it's actually
kind of straightforward load is just
move basically and then store can be a
lock unit or something like that and
then fences are usually nothing write
fences are effectively the way I see a
just compiler barrier the next 36 unless
it's the sequential
consistent offense so compiler doesn't
move loads or store around and what that
means is that the hardware just kind of
executes things in the order that they
are in the program because they don't
move around in the actual assembly and
afterwards they get observed in that
order there's just kind of a magical
property of x86 64 that's what makes
writing a kind of tight code like that
and exit assist and a nice because it
kind of kind of does what you expect at
the same time it means that if you wrote
code for x86 and then the line assembly
translating it to another architecture
is kind of a headache right so having a
high level language like C++ makes it
easy to to you know translate code to
another architecture now notice that I'm
not talking about comp exchange or some
stuff like that but cop exchanges is
directly an instruction on x86 it's
literally like one instruction that does
contain exchange now on power it's kind
of different like power is kind of a
weaker architecture and so you kind of
have to say when you synchronize to the
hardware right and you also have to
instead of using a comp exchange there's
these load linked and store conditional
instructions all right so it looks like
this and it's kind of complicated and
when you do a complex change you end up
having a loop or round things right so
it means they'll do a load it'll acquire
a reservation station or something like
that and then I'll try to do the store
that fails the store the the loop goes
around and tries again and again right
now I'm not showing 64 bits because
that's even more complicated now on an
arm v7 it's actually kind of similar to
power right and MIPS is actually really
similar to that as well it you know you
look at that it kind of looks the same
now arm has kind of extra interesting
things where it has a bunch of fences
there's like a bunch of different types
of fences so DMV at the bottom is the
basic type of fence that happens to be a
system-wide stance that like tells the
entire machine including hardware that
C++ doesn't know about like hey
something could have changed there's
actually a bunch of other offenses they
exist an arm like DMV ish if sound
stands for inner shareability not like
try defense and in in you know there's a
bunch of the defenses they exist and
then they also have like instruction
synchronization barriers if you know
touching the the instruction you know
that I cache and stuff like that and so
it's actually more complicated to
program in these are
chirps now rv-8 tried to make things a
bit simpler our v8 was designed with the
upcoming C++ memory model in mind and so
it actually is kind of interesting right
like it has these load acquire and store
release instructions and so it's kind of
like the life imitating the standard all
right so the standard did a thing and
armed v8 kind of did the same thing and
so it means that it's kind of a
straightforward mapping in a lot of
cases from D now Itanium is another
architecture that's actually kind of
interesting and the mapping and i Taemin
was actually kind of straight forward as
well so that's kind of interesting now
the last the last set of assembly I want
to show you is alpha so who's familiar
with alpha here and a few people so what
do you expect for alpha as a mapping
from you know suppose was Atomics 2 to
the I say not straightforward yeah not
at all right it has a reputation of
being kind of complicated so what I did
instead of trying to do that lowering
right is is I took an email from from of
all people linus torvalds explaining how
alpha works now what's interesting is
that email has no profanities and it's
actually super informative
all right oh I just missed it that
jumped around there we go so I won't let
you read that email it's really long but
it's super informative right he actually
explains how it works in the Linux
memory model now alpha happens to be
kind of hard to implement for C++ and
the the C++ design kind of abandoned
targeting alpha efficiently right
because alpha just wasn't really gonna
be a thing and so they were like yeah we
don't really care all right but it does
mean that some hardware can't actually
handle the C++ memory model very well
right so I guess that's the one
exception that proves the rule that CFOs
bus is wonderful now if you remember
like in the six memory organs that we
have there's this consume thing right so
who's heard about consume all right so
this consume worked well yes now I just
feel after now okay so it consumes kind
of the butt of all jokes in C++ I'm
sorry Paul if here in the room but it it
was really well intended and and the
idea that consume had when it was
standardized was look there's this thing
called the address dependency rule and
Alice traders for
because that's the one I know better but
it's similar in other architectures and
and the idea there is that you have a
load right so you load from R 0 into R 1
and then you load from r1 into r2 right
so if you look here R 1 is a dependency
right that the address of the second
load depends on the result of the first
load and that's consumed that's all it
is
right so the idea of consume is you look
at that and you say hey the hardware
can't reorder things at least in some
architectures right that's not true in
alpha but when there's a dependency the
the is a explicitly said there's a
dependency there for the the loads will
or will observe the stores in the order
that they occurred right and that's kind
of interesting it means that you don't
need a fence between them like you would
in other circumstances and it's not
quite true what it does is it orders
those to load with those two loads which
with with respect to each other it
doesn't order any other loads so if you
had a fence that would order all the
loads but without the fence it only
orders the dependent ones right now the
problem with that is that C++ doesn't
actually work that way right at the very
high level you could write code that
looks like this the compiler will not
generate code that looks like this right
if your code is a bit more complex is
never try to optimize around things
because it doesn't know that you're
trying to express a dependency way down
in the the back end in assembly right
now there's even more complex
dependencies that you can use in certain
architectures for example an arm this
works right
still no offense but what's interesting
is you're doing a load to r1 and then
you're creating a magical 0 you're
ending R 1 with 0 right and this is a 0
value and then you you're doing a
register register load the bases R is R
3 and you're adding R 1 to it RR 1
happens to be 0 now right
but that still maintains the dependency
between two dependent loads right
without having the Bary now we're gonna
try to fix consume in the standard but
but right now it just kind of lowers to
to load acquires instead so it has kind
of the barrier
instead in basically every compiler
right now I'm going to show you
something really horrible where I was
working on code recently and I kind of
wanted to have consumed because it
turned
doubt that in the code that I was
working on we had a barrier that was
really slow and I was just trying to
order reads between two flags barely
ever changed right
so what I did is I actually implemented
consuming the most horrible way I can
think of and I didn't even clean it up
to convince you that it's horrible so
and don't do this at home it's really
really horrible right it's really
objectionable now what's interesting is
I talk to people are like oh yeah it
looks like it'll probably work right
right Richard now the problem is he also
said well except like the aliasing
violation that you have but whatever
right there happens to work it's
actually kind of fast like I benchmark
the GC I was touching and some specific
benchmark in that GC got faster so
that's kind of interesting but don't do
this at home but we'll try to fix it in
language at some point and we actually
added some some wording to the language
it says maybe don't use consume right
now but we didn't fully deprecated it
yet so you know we'll see maybe I'll
come back now I'm gonna jump in into you
know some architectures specific stuff
right so in general you know you program
it at your high level in C++ and then
then you're on your when you compile you
can tell your compiler like Caleb I have
this specific architecture all right so
if you're targeting say x86 or arm or
something like that you can tell it's
this version right there where it's
exactly this CPU what's interesting is
that the compiler can be smart and say
like oh hey look I had this instruction
that does exactly what you want so I'll
give you an example on arm 32 bits when
you do a load of a 64-bit value right so
it's a 32-bit architecture you can do
loads of 64-bit values but what it does
is it loads it into two registers all
right so there's this instruction called
LDR D log two things now if you want to
do that atomically writes if you don't
want the load to tear you have to
usually use LDR e ex LDR de ex or
something like that
right because you tell it do an
exclusive load and don't care now if you
if you happen to know that your
architecture has a long pointer address
extension I don't know why it's tied to
that but that's what it is then you
don't need LD LD Rd X you can just use
the regular LDR ID right so if you
compiler smart there's a bunch of
interesting things you can do by
getting your specific architecture and
even gets more interesting right some
architectures have like transactional
memory or something like that some some
processors don't actually implement the
ISA the way it's specified they
implements something that's much
stricter because they wanted to write so
you go to your hardware people and you
say hey I put a barrier here is that the
best thing and say no actually like we
lied and we didn't follow the spec and
we made this thing much more strict than
other things that'll be faster if you do
this others thing all right so I've
worked on an architecture before that
was supposed to be a super like weak
memory model but it happened to be like
super like it was actually sequentially
consistent when it executed just
magically right and so so it meant that
like if I wrote code with architecture I
didn't basically need barriers anywhere
I didn't need any special instructions I
could just like use regular instructions
right as long as the compiler didn't
reordered them everything was fine in
that specific architecture and then
there's interesting stuff so when I used
to work on Chrome at some point I was
looking at the the way chrome used
Atomics and there happened to be a
really old CPU bug in old AMD versions
147 or something like that so the really
old Opterons and that's like point zero
zero two percent of all the the chrome
users and it had to have a workaround
for a specific bug in the way Atomics
work so in that case what chrome needed
to do is that needed to add an extra
fence in places where the architecture
said you don't need a fence right now
that's unfortunate because it makes all
the other users of chrome slow right but
but it's kind of like your compiler
should know better in a lot of cases
right and so so basically you know
sometimes you have special instructions
and sometimes it's even more tricky
right you want to write a spindle for
example what's the best way to write a
spin loop for your architecture it
really depends on the architecture and
the specific version of art the
architecture you're in and then yeah as
I was saying sometimes you can write
code that's nominally incorrect that
happens to work in that specific
architecture it's guaranteed to work in
that specific architecture but not in
another version all right so different
arm versions or something like that and
then you know I'm working around CPU
books
all right so what's the takeaway from
this well C++ exposes things in a
portable manner but the hardware changes
over time right you'll have bugs and old
versions they'll be out later or you
know you'll have kind of emergent
patterns and
people write code that mean that you
used to write code a certain way
Hardware adapted to that way now that's
fast or something like that or you do
things and it's super inefficient so
hardware five years later or something
like that gets faster right so you can
actually go to hardware vendors
sometimes and say hey like I do this
it's slow and I don't have a way to make
it fast and then they're like oh let me
think about that and five to ten years
later you get an instruction for your
thing you've probably forgotten about it
by then but someone else gets a profit
from your foresight and then you know I
have no clue with that it's whatever and
you know C++ in general just kind of
works on on on data structures which
kind of need compared to like the a
horrible assembly that I had earlier
right so so it's kind of nice to program
in that that model and also means that
you can target what I call virtual
assets right so I used to work on this
thing called web assembly where we would
execute C++ code on the web now the web
happens to work on a bunch of hardware's
right I think that v8 compiler targets
nine platforms so right nine individual
is a is there different so it has arm
and x86 and a bunch of other stuff and
there's no way assembly is going to work
in that platform but what does work is
very high level things like the C++
memory model right so if you take code
that's compiled for C++ then it kind of
just works in the virtual I say like
this which is kind of neat now how do we
actually optimize Atomics right I gave
you like a bunch of individual tricks
but a very high level how does that work
well the C++ standard has this thing
called the as if rule right and so so
the high-level rules of data race
freedom gives you sequential consistency
is basically what we kind of hinge on to
to do optimizations and the basic idea
is that you're allowed to make things
more atomic if it won't violate forward
progress right that's one one kind of
benchmark for whether the optimization
is worthwhile or not and at the same
time we're allowed to make things less
atomic if it doesn't add non benign
races which weren't already there all
right so I'm gonna assume your code
doesn't have races and then I'm not
gonna add more races basically that's
the the yardstick for the
now at the same time this is all
beautiful in theory right but the
compulsion just go in and optimize
things because it can without actually
giving you any benefits right the point
of saying I will add any benign races if
you don't already have any isn't like
I'll catch you if you do right the point
is it should be actually useful to
optimize things right and so I think
that's another kind of common
misunderstanding like the compiler just
doesn't just go in and try to optimize
your code into being incorrect or
performing the wrong thing right the
compiler tries to optimize things cuz
it's faster and useful ways right and
again like tooling is there to try to
help you catch ways where you make
mistakes right so thread sanitizer or
something like that should allow you to
catch races in your code now put another
way programs should just continue
working if they were correct before
after we've optimized them right first
put correct before should be it should
continue to be correct afterwards now
another way that I see it is is is that
really the the compiler should only be
able to optimize what the hardware can
do in a certain way right so if the
hardware could reorder your instructions
despite you know whatever you do then
the compiler should be able to do that
as well right and so if you have say a
loop that has a bunch of non-atomic
things nothing atomic in that loop then
that loop should be optimizable loads
and stores should be optimizable as long
as it doesn't create extra races and
pretty early on when the C++ memory
model was being standardized they
actually found a bug and I think GCC
where GC would kind of speculate that
the loop would execute at least once
right and so what it would do is it
would perform one iteration of the loop
right because it was like oh I'm gonna
guess this will execute once it happened
that the loop could execute zero times
and so what it would do afterwards after
the loop executed it would check did
that did this execute at least once if
not it would undo the first iteration
right now that happened to make the code
faster but it also happened to make it
incorrect right so that's not a good
optimization and so just see how to fix
that optimization which is it's a bit
unfortunate but it's better to have
correct code than fast code all right
now let's go a bit into what the
compiler can actually do and I'll try to
show you a bit more code here
all right now as I said I see it as like
what can the hardware do one thing that
you'll remember is I said well-armed v8
was kind of designed with a similar
memory model in mind is what C++ has
isn't that a bit circular right like the
hardware is falling with C++ does C++
pause well the hardware does and I'm
saying the compiler can do whatever the
hardware can do right that's kind of
circular that's kind of neat also I
think now you know yeah maybe we can do
more than what hard work can do and I'll
give you a few examples of that
afterwards now
I'm sorry few examples and I'll show you
a very simple thing here I have a bit of
code that just doesn't increment adds
one to a location and I have another one
to this two increments right now the
obvious optimization for this is to just
plus two and step right so that's kind
of straightforward right like wouldn't
you be mad if your compiler didn't do
that right this it whether it's atomic
or not doesn't really matter like this
isn't adding a race right like it adds
at on this city but it can't hinder a
forward progress so it must be correct
and in this case like you know one fewer
instruction and there's no way that a
valid code could have could have
observed that you added one and then
added one right so that's why I think
two is fine right the only racy code
could have observed plus one plus one
right and so what the compiler says is
I'll just but assume you don't have
races there is therefore no way you
could observe plus one plus one right
therefore I'll just do plus T right
straightforward right now here's a
similar example right I have one version
that adds one and I have one version
that adds just a value right now I'm
gonna lower that to x86 it's going to
look like this right one of them doesn't
Inc there's an instruction for adding
one to a thing and the other one doesn't
add now this is interesting right those
of you that know x86 right now should be
shaking your heads right first we're not
going to win the Turing award for doing
this because it's kind of a dumb
optimization but at the same time okay
they have a lot prefix both instructions
happen to be three bytes that's cool but
it may not actually be a good idea to do
an ink instead of doing an app right and
naively you look at x86 and
say you have an instruction to add one
that must be a good instruction to use
well not really because it happens that
ok it uses one less register but it also
kind of partially writes the flags at so
x86 has a bunch of flag registers and it
just partially writes one of them and so
it happens in a lot of cases add could
be faster than ink all right and so you
can't have to benchmark those things
before implementing them so yeah maybe
the compiler knows better maybe it
doesn't right but you would hope that
your compiler vendors
Tunes tune their thing to try out to see
whether that makes sense or not now one
thing that happens pretty often is is
inlining right and even with really
tight atomic code inlining is really
really important right and so so that
exposes certain optimization
opportunities sometimes right so here's
an example of code that does a compare
exchange and then sees whether the
expected result is the same as the
desired result right and imagine that
happens through inlining right so yeah
right dump code it just happened to be
done because you in lined it right that
happens really really often in in just
general code doesn't hat it's nothing
specific to act on the city well that
code could be changed to a compare same
shrunk that just returns the result
because it happens that instruction
already does what you want right and on
certain architectures what's interesting
is that so for example on x86 comp
exchange returns both both the value
that are loaded and it returns a flag of
whether that was the thing you wanted or
not right so if the compiler is really
smart it can have both of these at the
same time and it happens that some
compilers don't really represent such
things very well so it's actually kind
of hard talk two months right but that's
another problem the compilers can get
better at now here's a tricky one it'll
work for any memory order but release
and acquire release right so what I'm
doing is I'm doing something where the
desired value is the expected value and
I do a context change with just whatever
memory order I wanted to put in there
right now that happens to be the same as
doing this thing well is it actually the
same because I have this little dagger
here right not quite the compiler has to
like transform this and what it calls
quote unquote a release sequence now
release sequences are this magical thing
back in 1.10 and it also has loops sorry
1 bits too
and and and yeah so anyways it actually
has to kind of do a bit more work to do
this properly right so I don't think any
compiler actually does this optimization
even though it could now here's another
example I have a load and I have a store
what does that look like any other
instruction you would use instead no
there's this exchange thing that you can
do instead right so you have a memory
location you do a load and you do a
store what you're doing is swapping two
things what i'm doing here is i'm adding
atomicity right remember there's no way
this could have synced the first one
could have synchronized with anything
right little loan in the store but by
making this more atomic and going from
one into two instructions right one that
was a pure load instruction whether it
was a pure store instruction to having
an instruction that is both read and
write at the same time and in some
architecture it happens to be better to
have a write after read than it is to
have an exchange instruction that does
both at the same time right so that that
doesn't necessarily pay off right now
okay if you want to write like deep down
the assembly code maybe you can do that
maybe you know the fact that this isn't
faster or not right but I'm gonna say
this like this takes a lot of work to
prove if every programmer from writes
inline assembly has to figure this out
whether it's faster or not and in which
architecture which architecture in which
version of an architecture then we're
kind of wasting our time collectively as
an industry right where's the compiler
could just get it correct one so just
get it wrong and then get a bug file and
then get it correct afterwards then like
everyone having to do that work right
Hey just to be quick
compilers get this wrong all the time
right so you should just file above if
it's wrong right you figure out that
it's wrong you just like final button
and eventually it'll get fixed or you
fix it yourself that's also cool okay so
so far I've had kind of just toy
problems but I'll try to go into a more
interesting problem so I had a co-worker
maybe a year ago or something send an
email to a mailing this and saying I
have this interesting problem right and
and the reaction you should have when
someone says this and it's like you know
people interested in current currency
and compilers and stuff like that that
this is the reaction I have
right yes tell me more
and we he said is well we had this code
and it does this thing and and it uses
Atomics but not really it's like it's
like using multiple cores but it it the
code doesn't use the tommix at all right
and so he explained his problem my code
doesn't read then it doesn't modify and
then I was right it was based on this
paper that got published awhile to go a
while ago called hagwa and the idea is
that you have a bunch of threads and the
huge data structure right and so hugely
the structure of floating-point values
and what they do is they just kind of do
racy reads and writes alright so they do
a bunch of computations and then that's
very big data structure they have very
sparse updates and statistically it's
really really really unlikely that
multiple threads will try to update the
same location at the same time in their
specific application and that paper
actually went out and proved that that
was really statistically unlikely and
that if it happened that you did a read
then I modify and then I write right so
not in the atomic operation but you did
these things separately then
statistically losing one update out of
so many doesn't actually matter right
that's interesting right now we're not
in sequentially consistency land anymore
very different right but it's an
interesting property of that specific
code now the problem is the code that
they had looked like this who sees the
but no one there are no Atomics in this
code head-on I said multiple threads
updating this data structure
totally not atomically that happens to
be a race it happens to be okay for
their code but it's still undefined
behavior right and but what's
interesting is that on x86 the code that
gets generated is this right like that's
pretty sweet code right now it's it's
not a read modify write it's a read
semicolon modify semicolon right those
are two distinct instructions right but
it happens to be really really fast so
so they're like well we'd like to you
know be standards compliant but the
problem is when we do that it's really
slow
right it's the first thing they tried
what do you try when you do this well
you add volatile of course right and
they did that and and the code that got
generated was the same they're like
sweep we're good and we were like what
but not do Paula told I gave this big
array and it's all volatile things and
like volatile doesn't really do what you
want and we'll go into that a bit later
but then they were like all right well
if we instead do this is that correct
right so we have this atomic float or
this big array of atomic float and we do
a load that's relaxed and then if we do
the modification and we do the store is
that is that correct well kind of right
so if you remember when we were talking
earlier relax doesn't you doesn't really
guarantee that things will propagate
right this isn't guarantee observability
right but it happens that on the
platform they were on say x86 or
whatever well it happens to hold the
property that like once uses store it
gets its visible pretty much there and
then right so it happens to be correct I
guess right and the idea was well we do
this thing and that should generate
exactly the same code as we had before
and now we'll have fast code and will be
standard pedants compliant right that's
awesome that's a great property right
they they wanted that right and the
problem is they did that and then they
got this code right and that's three
times slower like what the hell right
like we wanted to be all standards and
stuff and that doesn't like what do you
what right so that's when they decided
send email the mailing list and then I
had the face that I showed earlier
gene Wilder face earlier and I said let
me try to optimize this right so I went
in and got my hands dirty Intel VM and I
don't really know that part of the code
base I just made it faster weight
optimization for the win now the part of
this story that I don't usually tell now
tell it to you guys just you know for
true shits and giggles is that I did
that and I think how long was there like
two months later there's actually a bug
caused by an opposition because I forgot
to do one thing right and then that guy
fixed it it was awesome
took forever to debug it was awesome
right Chandler
yeah awesome right so like there was a
bud sorry but like the rest of the code
optimizer really well it's like if you
some really complicated thing in the
completely different context that the
optimization didn't work out so well
right but but what I want to get to is
that you can actually write correct code
that would be fast if you look at the
code if it's not fast then you're like
kind of file a bug or like try to stick
the compiler that's possible right now
let's look at a few other things the
compiler can do a bunch of other stuff
right it's just usually when it sees
atomic or it sees vault or whatever just
compiler throws its hand in the air and
it says well I'm not gonna touch
anything but if it were bits my looks
smarter it can do a bunch of stuff it
can do inlining constant propagation it
can do a bunch of interesting tricks
right where like if you you know do all
ones and you do an end and that's kind
of like a load right it can do a bunch
of interesting stuff like this right now
those are all kind of really
straightforward compiler transformations
and some of them are actually
implemented in compilers not all right
so what's the takeaway here well if
simple things are hard right like some
of the stuff I showed you is kind of
hard then like your assembly codes
probably not correct anyways there's
probably not that fast and it probably
won't remain correct or as fast as it
could be right because as you upgrade
your architectures it won't wear as your
video compiler and your architectures
things usually tend to get faster for
that new architecture right and usually
the the push bag that we get is compiler
writers and there's like people who give
thoughts is oh well I don't trust my
compiler well I'm sorry to break it to
you but you are already trusting your
compiler right like you kind of have to
it's compiling your code right you don't
trust it to generate fast code but you
trust it for everything else it's kind
of silly
right it's you're really kind of
deluding yourself Niki's and really like
you know that's the question we get what
is the compiler image suboptimal code
what do you do file a bug all right or
complain on a hacker news that's also
good
right but not on the comments for this
YouTube video don't I just don't look at
the comments now at this point of the
talk as I'm going to talk about sequence
locks they're a really cool little
atomic optimization the problem is Hans
talked about it in his talk yesterday so
I'll just refer you to the YouTube video
because he did a much better job than I
did because like all these slides are
kind of taken from his paper right so
I'll just skip over that right it's a
really cool data structure if you don't
know about it you should check it out
it's a thing aimed at just read mostly
data structure it's things that like a
bunch of threads just read all the time
but you update it every only every so
often I mean the code looks kind of
tricky and it has a bunch of different
things well the me the really
interesting thing here is that we kind
of want a release store and there is no
really store in the language right and
so Hans came up with this really cool
thing called reads don't modify right
it's the cool optimization the compiler
can also do and what it does is do a
fetch ad with zero so what does the
fetch add with zero does what does is it
just kind of add zero to a location
which does nothing right and that's the
way to tell the compiler well hey I want
to have a load but I want this load to
be release right now that's kind of a
ugly hat but it happens to generate kind
of interesting code or if the compiler
was smart it would write so there's
different types of code that get
generated in different circumstances for
that if whether you use the fence
manually or whether you use the read
don't modify right and the interesting
takeaway here is that if the compiler
does its job well you get nice speed ups
right so I'll refer you to the paper
it's a really interesting thing but like
in different cases you write different
code the compiler optimizes it
differently and then things get faster
or slower and what's really interesting
in this case and that is that for SEC
Lots it's not just that things get
faster depending on the compiler doesn't
what code you wrote
it's that it'll depend on the
architecture you're on and it'll depend
on how many cores you have right so in
this case it's it's the read don't
modify write instruction it's it doesn't
really scale with the number of threads
that you have so that's kind of another
thing to keep in mind when you try it
out to my stuff and you know when the
compiler in its code is is that the code
that you generate also highly depends on
how many people are going to try to
access the location at the same time
right now as I was saying you can do a
bunch of other compiler optimizations on
Atomics you can you know maybe do dead
storm Latian right if I do two stores
back to back to the exact same location
right and it's back to back I don't do
anything in the middle then the compiler
can go and say look you did two stores
in the same location you're kind of dumb
right like I'm gonna remove the first
store and keep the second one because
you know what unless you had a race
nobody could have observed the first
store or yeah yeah so you can do a bunch
of other stuff you can do strength
reduction and things like that and you
cannot even optimize relaxed accesses
right so here's an example where you
have to read kind of this tricky code I
stored 2x and wide but interleaved right
now
relax doesn't guarantee ordering between
relaxed stores so what I'm allowed to do
in this case is I'm allowed to call less
than even though x and y were separate
right they were kind of interleaved like
this and I moved them together and then
collapse them and compiler should do
this I think LLVM does I'm not sure if
Jesus he does yet now that leads to a
kind of interesting question when I
initially wrote to the paper that
created the stock Hans Hans Baum kind of
flipped that he was like whoa
compilers do what like I can't just
write a proper S bar like this that's
what it means right so what I'm trying
to do here and sorry I started had
relaxed updates but I'm trying to do a
bunch of really heavy work right so I
have this loop that does heavy
computation and then I just increment
progress right now now what's
interesting here is that I don't have
any other atomic operations in that
heavy amount of work right all I'm doing
is I'm doing heavy work and I'm telling
the progress bar oh another thread hey I
did one more work right so the idea here
is the compiler should be able to unroll
the loop and do a bunch of optimizations
to the code because none of that is
atomic except the the progress thing
right now the problems here is that the
compiler in a lot of cases is allowed to
do this optimization right so create a
local copy of the progress thing that
will just increment a local copy a
million times or something like that and
then just do the final updates your
progress bar looks like most progress
bars do they go from 0 to 100 in one
shot
right now that's not really what you
wanted right but that's what you've said
in this code and and so so we added some
wording to C++ 17 to discourage this
kind of optimization now it's super hand
wavy wording it's in a note it's totally
not normal it and says compiler really
shouldn't do that in like weird cases
right but but in most cases like you're
actually doing synchronization you're
not just like fire and forgetting writes
and so in most cases that doesn't happen
right it's just kind of an interesting
corner case or like you know this this
is same code right you really will
people expect this to work and it
happens that standard says that this is
doesn't necessarily work now I don't
know if compilers that optimize this
specifically but as we start like adding
new optimizations we'll have to be
really careful and not break real-world
code right now how do you disable the
reader reordering or fusing that we saw
earlier well there's a few ways to do it
right now this is super objectionable
right don't do that
like I just don't write I say don't do
that and I showed you code when I did
consume that did exactly this right
that's leadership now you could do
signal fences but you're like what is
the signal fence anyway right and signal
fence was kind of put in there to kind
of help like prevent this from happening
because if a signal happened in the
middle and whatever but it's kind of
weird right like has anyone ever used
single fence or seen in code like when
you see it you're like what what is that
right that's kind of weird you could use
volatile now that won't work either
that's just a dumb idea it won't work
and then you know just don't use relax
maybe but then like you're making this
heavy computation like a tiny bit slower
or something like that or you could
synchronize with your progress bar right
like increment and then the progress bar
decrements and you look at the amount
and whatever it's kind of weird there's
a bunch of stuff you can do but it's
weird so let's look at a bit more of
what you can do those are all kind of
like regular low-level optimizations but
you can do higher level stuff as well
right so compiler could go in and that's
all kind of blue sky thinking or
whatever but the compiler could go in
and tag non-atomic functions and
optimize around right so the compiler
has already mark things as not doing
agrees or not doing any rights or
something like that it marks a
thing other stuff like that it could
mark functions as whether they have
Atomics or not enough to Mai's around
that that'd be kind of interesting right
because usually functions kind of act as
a barrier optimization unless you're
doing interprocedural stuff that's kind
of a basic kind of low lying through
that we could do that's kind of
interesting it could do something more
interesting where it proves time to cook
regions that don't interfere between
each other and kind of optimizes around
and that's not just about atomic sauce
about the non Atomics there are under
them right so you could have optimized
things by proving that things don't just
kind of touch each other then you could
also optimize fencepost positioning
especially in in languages that have
like right barriers or something like if
you have a GC running concurrently and
something like that you end up having a
lot of fences everywhere and the fences
aren't always positioned optimally you
have a big control flow and you have
fences everywhere you could kind of
prove that this the slope ad has yeah
you'll put fences and slow paths and
remove fences from the the harder path
without removing correctness right so
it's basically like just balancing the
the fences in the control flow graph you
could do that then you could prove
things like oh this doesn't escape it's
on the stack so there's no way you could
be sharing it with everyone else or this
isn't TLS or something like that right
so there's a bunch of interesting stuff
that we can do here now at this point
you're all like okay well Atomics are
really complex and like I'm not trying
when you use them anymore I'm just gonna
stick to stood mutex for sanity right is
that a good idea well I think it is a
good idea right it's easier to use
correctly in theory right but but but
the thing is you could still optimize
stood mutex whether the compiler could
do this because basically the way I see
mutex and I may be slightly wrong but
the way I see it is when you lock you
create an acquire and when you release
you when you unlock you create a release
right that's kind of what mutex is do
now the problem is from the compilers
point of view it doesn't actually know
about stood new text right like in in in
in the standard library there's two
mutex but then the compiler parses that
just like anything else and then just
generates code it doesn't really know
that stimutex it kind of lies to itself
it doesn't look at the name and say this
is stood mutex right there look right
and the thing is a good stood mutex
doesn't just have like a spin loop
that's
stupids - new tax right a good student
new text has things like calls to
pthread and or colonel calls or
something like that to make the mutex
faster in the case where you can't get
the lot in a reasonable amount of time
right and any time you touch the colonel
compiler just throws its hand in the air
and said the colonel can do whatever so
I'm not I'm gonna assume you did
whatever right so so reasonably I can
think well stewed meat eggs doesn't
optimize anything but we're trying to
add this thing called synchronic to the
language and that may change this this
this thing and we could actually start
optimizing around stimutex and that's
kind of interesting because there's a
bunch of interesting optimizations we
can do kind of a parallel to that if you
have a shared pointer for example and
the shirt pointer you know you pass
there around so it does a bunch of
increment and a bunch of decrements it'd
be kind of interesting that the compiler
could optimize these because if you
think about it
shirt pointer the only thing actually
matters is going from one to more than
one and then back to one all right
everything above one just doesn't matter
for a shared pointer right it's just a
ref count it like doesn't matter right
and so so and the compiler can look and
say look I did want increments so it's
above one and after that I can call s
all the increments and all the
decrements and it's kind of difficult to
do that an interesting way for the
compiler because it has to look at the
decrement side and say oh I'm you know
going
I am decrementing and you there's always
a branch that says like hey are you
going unders 1 or something like that
right so that's kind of hard to optimize
but a high level that's kind of
interesting optimization right now we we
know how to do better than still new
texts right the stimutex just knows
better than you do usually now I'm going
to skip over this really rapidly because
it's objectionable but we could talk
about volatile right and the basic idea
of volatile is like memory mapped i/o
and whatever and it's kind of selling
isn't really mapped to what C++ does
anymore right it doesn't really mean
anything that's very coherent right so
it prevents those store motion does it
do arithmetic or whatever it's just it's
really weird to do a volatile and and
does it allow you to just share memory I
don't know because it gets better all
compilers model volatile differently or
a lot of them do all right
this version GC there's one thing MSB
see there's another thing and what it
does is just really crazy right like it
gives you a bunch of guarantees unless
you break a rule and in that case this
just doesn't do anything right so so
actually emulates that weirdo behavior
them SVC has right and that's like an
old and mistake that they made and they
try to fix it when in did the arm
version of isn't and then SVC but like
it can't really fix it because people
currently rely on the behavior of
volatile right what does it really mean
I don't know like I have no clue with
volatile means right it has a few things
that it says that it does but like it
really depends on the implementation and
then there's a bunch of weirdo questions
that can it be lock free I don't know
can it do read-modify-write no not
really
and then are there portable uses of all
well I would say that maybe there are
right maybe these three uses are valid
some other people would object but
whatever right and then there's also
volatile atomic that's kind of weird
right and and what what do compilers
really do it's weird its weak so what a
compilers really do well the standard
library looks like this it has C atomic
or stood atomic and then it kind of
lowers to instructions that do built-ins
right so the compiler looks at this and
it's like okay well I'm gonna like kind
of just generate those built-ins and
I'll optimize a few things in the front
10 but it's really with the middle land
where things happen right so it has
these instructions in the Nolan LVN
knows about loads and stores and it tags
those loads and stores as atomic right
and so as soon as you see it sees the
Tomica know like is it just gives up
it's like nope not going to touch it
right there has other operations for
read modify write and complex change and
stuff like that and it has pencils and
then in the IR it says like oh is this
simple is the volatile is it atomic and
then just don't really touch it alright
so what we've been doing is adding
things where it actually touches stuff
in very tricky ways like it we actually
think a lot about the patches before
touching things and then the machine ir
to remember the example i gave you
earlier about hog-wild that's where i
modified what the x86 operations do
right so it's really similar so in
general conclusion i'm almost done
what's the takeaway from my top well I
kind of want the Standards Committee to
assume optimizations occur and actually
encourage them right like not everyone
the committee assumes that that's the
case for Atomics and and I want us to
standardize existing practice right like
as I was saying not everything that we
have actually makes sense for everyone
right the goal is that
C++ allows you to express things as
close to the metal as possible right I
would like the language to do that
better and then this is madness to use
in a lot of cases right so I'd really
like the libraries to get better at
making it trivial to use these things
right like kind of higher level stuff
that allowed me they'll allow me to
express then current same perils and
really well write the goal is to make it
hard to get that wrong right Atomics are
easy to use the wrong way especially
relaxed but if the library exposes
something then that should make it
easier than not to use it correctly
right now most of you in the room are
developers what should you get out of
this talk well you should really drop
inline assembly except when like the
language fails you write if the language
fails you sure go friend line assembly
it's cool but at the end of the day like
it'd be great if everyone used a Atomics
in the higher levels of the language has
and then if it's wrong just file a bud
right it's not this there's committee
like in a lot of cases will do things
without having any clue of how people
actually use it right we'll have one use
case for a thing and we may just get it
wrong so it'd be great it's like you
know people gave us more feedback and
there's all this tooling that you should
use right like thread sanitizer and
stuff like that that would be great now
I don't know if there are any hardware
vendors in the room I saw a few like
Intel people earlier but like it'd be
cool if like we could use more stuff
right like showcase what you have and
like make it useful and in the stands
committee we've actually been trying to
do that alright so one of the things
we're trying to add from next version is
indie support right and we're having
kind of hot debates on how to do Sindhi
properly in the language because it's
actually not that easy and then then the
last takeaways for compiler writers like
you know get back to work
because there's a lot of stuff that we
can do and obviously like there's stuff
that we can do we should focus on the
ones that are useful there's all these
optimizations that we can do a lot of
them just aren't that useful right so we
should find interesting code and make it
faster alright so that's it I I have a
bunch of references on github if you
want to check out like I have a
bajillion papers and other stuff if you
want to read more about this because I
know I went kind of fast over a lot of
stuff but if you go to github right now
in the readme
I have a bunch of references at the end
some of them are C++ tyrants committee
papers some of them are just kind of
regular publications or like you know
the the references to what different
compilers do and stuff like that right
so any questions all right 100 percent
clear thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>