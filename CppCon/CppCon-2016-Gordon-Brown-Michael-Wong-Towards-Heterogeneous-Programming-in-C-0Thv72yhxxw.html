<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Gordon Brown &amp; Michael Wong “Towards Heterogeneous Programming in C++&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Gordon Brown &amp; Michael Wong “Towards Heterogeneous Programming in C++&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Gordon Brown &amp; Michael Wong “Towards Heterogeneous Programming in C++&quot;</b></h2><h5 class="post__date">2016-10-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0Thv72yhxxw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey hey hey everyone yeah thanks thanks
for coming my name is Gordon Brennan I'm
a software engineer at Coldplay software
hi I'm here with our VP of R&amp;amp;D Michael
Wong and we're going to talk about
heterogeneous programming in C++ so
first of all the acknowledgment
disclaimer oh the content for these
slides has been contributed by other
members of play as well as the standards
groups like the C++ Crippen and Chronos
obviously any any mistakes there they're
obviously mine
now obviously legal disclaimer that
these are these are the views of myself
not necessarily of called play in
general so so who are who are we who are
could place who could play our primarily
involved in developing solutions for
heterogeneous systems or we're involved
in in many different standards bodies
including Kronos HSA and the C post
finest work we're also recently we've
also been involved in a standards for
safety critical applications such as the
ISO 26262 and EDS we also have a suite
of for for developing heterogeneous
solutions called compute suite so this
has a computer which is our sort of core
technology which targets are a wide
range of of different heterogeneous
systems and can support our many
different standards and we have compute
CPP which is our implementation of the
the seco standard which I'm going to
talk about but later on let's talk so so
first of all why am I here to talking to
you today
so as of the the CEO so standards
meeting in Jacksonville February this
year we we now have a mandate to bring
head range computing to C++ so what is
hedging is computing so hedge computing
involves gaining performance through the
utilisation of systems which make use of
more than one kind of processor each
with specialized processing capabilities
to handle particular tasks so it's all
about gaining parallelism over Britain
tasks on hardware this is the same
specifically for so why should you be
interested so
Hedges computing is is everywhere it's
driving the technology of the future yes
it's been used for a lot of things such
as image processing machine learning you
can see in self-driving cars in drones
and speech recognition animation medical
imaging and telecommunications and many
others so what I'd like you to take away
from today's talk is a sort of a vision
of what the future of Hedges can be an
NC plus course is going to look like so
an overview of what I'm going to talk
about today so I'm gonna start off with
a sort of a brief histories or where we
are and how we got here then I'm gonna
start I talked about C++ as a
programming language for a change
computing and then gonna sort of look at
some of the the major challenges we face
in hedging use computing and some of the
solutions for them and then I'm gonna
look at cycle so a new standard for a
programming heterogeneous aim for hedgy
news programming in C++ and some of the
approaches that takes then I'm gonna
omit Michaels going to close off with
top of the looking at the future of
heterogeneous programming in C++ and
where the standard is going so to start
with who we got here so back in in 1989
we were at this point called so if you
could sort of you can refer to is there
the single Cordillera so this was a
point where we were gaining steady
performance schemes and in CPUs based on
on miroslaw as a principle that if so if
every year the the CPUs would have more
and more transistors which would give us
a V performance gains this is often
referred to as the the free lunch the
earliest sign of a heterogeneous
computing was is back in 2001 where the
the introduction of the first user
programmable shaders allowing you to do
general-purpose computer on GPUs in 2005
the the the free lunch was over the sort
of the the performance game
from from miroslaw stop a wall where she
performed CPU performance stopped CPUs
stopped getting faster and this
performance game stopped so where to
look to new ways to gain performance
shortly after this Intel announced the
first range of dual-core CPU so CPUs
that would gain performance through
having multiple cores on on the same
chip that would run in parallel and
around her in 2007 this is quite hard to
have an exact time for this but there's
there's a point where I came evident
that the performance gains from adding
adding multiple cores to a CPU adding
more cores wasn't giving us the same
performance gains as we did with Moore's
Law and although there are some
applications that take that they take
advantage of having more coarse and as
there's architectures that have a large
number of course they generally the for
course was the optimal and beyond that
we're starting to see our diminishing
returns shortly after this a Kudo was
released so initially as a CC API for
offloading code to NVIDIA GPUs and then
2008 we had open CL which is very very
similar but it was open to a CPUs and
GPUs and it was wasn't restricted to
just in video this was open standard and
then in 2009 with DirectX 11 was
released with Derek computes this was
shaders that she could do
general-purpose
computations this is this is generally
used for four games in 2011 in DNS the
first range of their ap use so these
were devices where rather than having a
CPU in a discrete GPU you would have the
CPU and GPU on the same work with shared
memory has changed the way that you have
to program them clear in 2011 open ECC
was released so opening SEC was
originally branched off from openmp in
order to support offloading to
accelerators so the time openmp only
supported a multi-core CPU parallelism
and then in in 2010 I all Tara and I'm
just open CL support for their FPGAs and
2013 open MP also announced support for
offloading to accelerators
2015 Texas Instruments and Oh support of
open sale support for their DSPs and
then in 2016
recently each the HSC standard was
ratified so HSC sign is quite similar to
open clas a lot more lower level it's
more focused on defining the
requirements for heterogeneous devices
so so so the question so what is the
most popular language for at Eugene is
computing and there so I tried to answer
this question and so to do this sort of
my myself and some some college came up
with a list of heterogeneous programming
languages and models and there so the
sort of the criteria for this would be
any any language or moral as I like a
language in itself is not a wrapper over
another language and it would have to
support more aim heterogenous devices
not just multi-core CPUs so we took
these and we the release the dates that
these were released and we map them out
over so a year by year how many
languages were available we came up with
this graph and this is this isn't a
definitive representation it's a cross
sample of the languages and models that
we that we could think of so there's I'm
sure there's there some that aren't in
here but this is a representation of of
the trend of programming models and
languages for a teams computer the thing
we see here is that from our in 2003
onwards there's been a steady gain and
in programming languages for
heterogeneous computing particularly in
C and C++
this is this another side with you with
the same data the thing we see here is
that even as far back as 1999 the C and
C++ where the dominant languages for
heterogeneous computing so for a long
time C was the most dominant language
harbor in recent years C++ has rapidly
overtaken it the other thing to notice
is that in the last few years while C++
has been so wrapped or taking the other
languages other languages have become
sort of plateaued and if they've not
been increasing as much
so the next thing I'd like to do is look
at some of the the major challenges we
face and head to G's computer and so
these are the the four four majors those
challenges I want to focus on there's
there are other other things that are
important for a genius computing but
these are the ones I want to focus on
and so first of all performance
portability so we think of performance
portability we think of writing code
once and then running it everywhere so
it's performance portability across
Hetal use devices really possible and
the answer is it depends on how you look
at it so the heterogeneous landscape has
a wide range of different devices now
there's accelerator CPUs GPUs DSPs AP
use FPGAs and there's many others and so
to look at some of these all closer
so a typical CPU architecture you have
so you have a small a small number of
cores that are running separate
instructions on each each core
independently and it's this low
bandwidth memory ranked random access
and then you have is this is generally
suggested for task parallelism and then
if you look at a typical GPU
architecture this is uh you have a large
number of of execution units with a
single instruction running on a number
of execution units and this is this is
generally in lockstep GPUs have a more
hierarchical memory structure so you
have multiple different regions of
memory with different affinity to the
computation and then you have the GPS of
a higher bandwidth bandwidth memory and
have a more predictable memory so and
then and these GPUs are generally
suggested for data parallelism if we
look at FPGA is a typical FPGA
architectures and if FPGAs are very
different from from CPUs or GPUs the
main the main difference is that whereas
whether CPU a GPU would copy your code
over to the device in order to execute
it
when FPGAs are a reprogrammable at
runtime so they could the code that you
execute is actually synthesized onto the
hardware itself and this is done through
having these configurable logic blocks
that are used for both memory and
computation and these are
connected through configurable ratings
that provide different levels of
bandwidth fpg is a very low power
consumption and they have a sort of a
stream execution model where rather than
running a function function once you
have got something that's run
continuously with with data streaming
and you know as DSP as well so the DSPs
are also very different from a CPU or
GPU these are these are very sort of
purpose-built compute engines for
performing things like add subtract
divide multiply very very quickly and
they're generally used for a digital
analog audio data stream processing so
to answer the the question is
performance portability possible the the
short answer is is no architectures by
designer are so different that there is
no one solution fits all in reality you
have you have to find a balance between
performance Portability and productivity
so performance being the the scalability
and efficiency of your your application
portability being the the range of
hardware that your application can
target and productivity being the range
of tasks your application can perform so
generally if you have if you have
performance and portability if you are
able to achieve sort of a good amount of
performance portability this usually
comes at the cost of productivity so so
the long answer to this question is
performs portability possible it the
answer is yes the with the right
programming model you can write you can
represent your problem in a way which
can adapt to different execution models
and different memory models of different
kind of a heterogeneous devices so
the next thing I'd like to look at is
heterogeneous offloading so how do you
offload code to a heterogeneous debase
so so this the best way to answer this
question is just to start with the the
C++ accomplish model so it's obviously
yet you have your C++ source file you
have a pastor suite compiler and you
have an object file and you pass it
through a linker and have the executable
and then you execute all runs on on the
CPU but what happens if you want to also
target a GPU I know that had Gelinas
device how how do you adapt this
completion model in order to support
that so there's there's generally two
two approaches to this and one one
separate source in a single source so
separate source compilation as an
example we're going to use OpenCL so a
separate source completion alongside
your C++ source file you have a device
source so a separate source in this
let's compile separately this is this is
then compiled by an online compiler that
is linked to your executable so the the
main thing to notice here that this
means that because this is is compiled
by an online compilers compiled at
runtime which means that you your device
source has to be shipped with your
executable so to go back in and look at
a single source this so you see possible
sample as an example so the thing to
notice here is rather than having the
device code as a separate source file
it's all in embedded inside the same C++
source file then alongside your CPU
compiler you compile your the same
source file with a device compiler in
this little generate alongside this CP
object some form of device intermediate
representation or object file this is
then linked together with the CP object
and then you end up with an executable
executable with some embedded
intermediate representation or object
that will execute on your device so some
of the some of the main benefits of the
the single source model over the
separate source is that there the host
CPU code and the device code are are
same same c++ source file unless hello
is compile time evaluation of you device
code which gives you type safety across
the host cpu interface and the supports
generic programming
I also removes the need to distribute
your source code so the next next thing
or two look is how to describe
parallelism so there's many different
forms of parallelism there's different
ways of describing and you're in your
source code so these are the three
things we'll look at so the first is
directive versus explicit parallelism so
on the left here we have an example of
directive parallelism with OpenMP and
the right we have an example of explicit
parallelism with c++ amp so the way that
director parallelism works is you have
some sequential C++ code like some form
of loop and then you have a pragma that
you attach to it there is a hint to the
compiler and the compiler then uses to
paralyze it either bias or transforming
the code to run multiple threads or
transforming it to some D operations
which with the explicit parallelism you
have some from if API where you you
define a function and then a range of
execution and then that's this function
is executed on the hardware across for
that number of vexation so next through
a look at task parallelism versus data
parallelism so on the left you have an
example of task parallelism TBB an
irregular an example of idea piles and
with CUDA so task parallelism is where
you have multiple potentially different
tasks they're running in in parallel and
with the data parallelism you have the
same task being performed across a large
data set in parallel so next next
there's a cube queue execution versus
stream execution so on the Left you of
an example of queue execution with CUDA
on the right we're an example of stream
execution with brook GPU so with qu
execution you have a function that's
placed on a queue and then that's
executed and then that returns whereas
the stream execution your function is
executing a continuous loop with data
streaming and and out of it so finally
the last thing I want to look at is data
locality and movement so one of the
biggest limiting factors in
heterogeneous computing is
the the cost of data movement and both
time and power consumption so it can it
can be considerable I might attain to to
move the data from a whole CPU to
hydrogens to basically want to execute
on this this depends on the architecture
the the bandwidth of a device can also
impose bottlenecks which can affect the
throughput of your device as well
additionally it's very important to
ensure that the performance gain you
achieve from performing the computation
on the heterogeneous face as opposed to
the host CPU is larger than the cost of
moving the data to the device otherwise
it's not it's not worth the performance
on the device the other thing to
consider is that many devices such as
GPUs have a hierarchy of different
memory regions in each of these region
has a different memory size affinity and
access latency so for example global
data's generally is DDR and has sort of
a very large amount of data but has a
sort of a low affinity so very high
access latency whereas private memories
generally sort of registers or on chip
statigram where you have them so it was
a much smaller size but has a much
higher affinity to the computation so
much lower access latency so no one
having your data as close to the
computation as possible can conversely
reduce the cost of video movement so a
look looking at the cost of data
movement in in terms of power
consumption this is a slide taken from a
talk by Bill dally and 2010 from in
video this is this is the measuring the
cost of performing computations Earth
meta computations and moving data on an
NVIDIA GPU so the thing to notice here
is that in the top left corner you have
a 64-bit double-precision operation
that's cost in 20 packaged jewels and
then just below that there's the the
blue dot is a read of 464 64-bit and
operands for that operation and that
costs 50 packages then so if you like to
move these for 64-bit operands one
millimeter across the chip that would
cost you 26 package jewels over here to
move it across the entire chip that'll
cost you a nano jewel and then if you
were to move it off onto d around
sixteen annuals so the the Causton in in
power consumption from moving the data
can can can have a dramatic impact on
the performance of the Vera execute on
the device so this is how do you how do
you move data from the whole CPU to
advise so there's there's a couple
different ways of doing it there's
explicit data movement and implicit data
movement so on the left is an example of
implicit data movement the c++ amp and
the right is an example of explicit data
movement with cuda so implicit data
movement works by having data structures
that are cross core CPU and device that
can be used so on this this is this has
to be used with a single source
programming model so you have a single
structure that you use on both the host
and the device code and then this is
this implicitly handles the data
movement for you whereas with explicit
data movement you have these explicit
API calls that you have to explicitly
pass pointers and say I want to move
this day on over to the device so the
other thing is how do you how do you
address memory across the host CPU in
the device and there's a few different
ways of doing this so in comparison
there's the first of all there's there's
multi address space although where
pointers have attributes or structures
which specify where what sort of memory
region that data should be stored as all
those fine are clean control over where
your memories allocated or moved to but
it does mean you have to define
explicitly and there's noncoherent
single address space this this works by
you have pointers that address the
shared address space across the host CPU
in the device and they do this by having
map operations a API switch allow you to
so say you're accessing a pointer on the
host you then have an API which says I
want to map this data across to the
device and then you could use the same
pointer to access the same address in on
the device and there is with cache
coherence single address space this is
similar to the the noncoherent single
address space the difference is this is
the way you when you address
a pointer on the horse and device you're
generally either addressing the same
physical memory and hardware or you're
accessing a cache coherent Hrunting
which performs the synchronization at
the cache level so let's allows you to
access the horse CPU and the same
pointers on the host CPU and the device
concurrently
however we are given up a lot of control
over when the date has moved so so this
can be an efficient in some cases so
next I'm going to look at talk about
cycles this is a new standard that for
heterogeneous computing C++ so first of
all open CL is a standard from the
kernel script that gives praise a C API
and icy language that allows you to
offload code to many different
heterogenous devices and circle is a
relatively new standard that aims to
provide our provider a single source C++
programming model that allows you to
vary standard C++ and target this range
of this large range of heterogeneous
devices through open CL so the way the
ecosystem for cycle works is so you have
your C++ application and you have some
template libraries and then some of
these simple average may implement an
algorithm using sickled this is then
compiled and executed via OpenCL on a
wide range of devices so first of all
how do cycle improve the hedging is
offload and portability performance
porkbelly so the first thing is cycle
isn't entirely standard C++ cycle allows
you to compile to spear and cycle to
support some a multi compilation single
source module so in order to explain the
the multi they said the multi
compilation single source model I'll
first explain the single compilation
model so the way this works is you you
wrap around your so this is the same
single source model from from earlier so
the way single compilation works is you
wrap around the CPU compile and the
device compiler so you have a single
single source host and device compiler
this is quite quite simple
you have a single compiler that you pass
here you compile with your source file
and you end up with an executable that
you can run on the CPU and a GPU the
problem with that is that this is tight
to a particular compiler chain so say
for example you want it to have an
application that supported a AMD GPU
nvidia GPU and some DCP description so i
were to do that you would have to write
first of all you could you could write
some C+ or some code and compile that
with a suppose for some compiler you end
up with an extra cute bowl that can run
CPU and execute on an AMD GPU and then
you write some critical source compile
it with a cuda compiler you end up with
an executable that can run on the CPU
and execute on an NVIDIA GPU and same
again openmp source you would compile it
would open MP compiler and you have an
executable around CPU and run in 70
operations the problem with this is that
you have three different compilers
generating three different executables
with three different sets of language
extensions and these are generally not
interoperable with each other which
means that if you want to have a single
application that can target all of these
different hardware it makes it very
difficult to build this generally it has
to be the device you want to write
execute on has to be decided at compile
time so some of the things that sickles
tried to do to resolve this the first
thing is cycle is entirely standard C++
so there's there's no no attributes
no additional syntax no pragmas and no
keywords it's just entirely standard C++
that can be compiled with any C++
compiler the next thing is that cyclical
can target through spear this appears
another standard from the Chronos script
Center this a standard portable
intermediate representation this allows
other circle to target compiled to a
single binary and target a large number
of hedging devices so to look at the the
multi-core the multi compilation model
and say this works so first of all here
you see that the device compiler is the
second compiler and let's generate spear
now the way the multi compilation model
works is we separate the host compiler
and the device compiler the first thing
this means is the host compiler can be
any standard C++ compiler so GCC clang
Visual C++ in Tosa goes soakin fitting
with any existing tow chain the next
thing is that you generally want a
single executable with
embedded spear and that can then execute
across a wide range of heterogeneous
devices and this can be to the device
can be selected at runtime because you
have the standard intermediate
representation you this adds a lot of
performance portability however circle
specification doesn't mandate you have
to use spear you can use any
intermediate representation or binary
format is supported by an open CL
implementation so for example if you
want it to support PTX you could have
you have the same cycle source code you
pass it through protection of the same
compiler or another compiled compiler
and then generate PTX and that can be
linked into the same executable and then
you can select that or run thing so so
the next thing is I'd just cycle support
different ways of representing
parallelism so cycle is an explicit
parallelism model and it has queue
execution mode on later versions of
cycle made me try to do a different
additional forms of parallelism a cycle
also supports both tasks and data
parallelism so these are two of the API
sin in cycle the first one is a single
task this performs a pass a lambda or a
functor object here and has executed
once and then you have another parallel
for which you pass a functor lambda
object again but they also pass a range
let's remain specifies the the the work
work range that you want to execute
across the next thing is how the SEC
will make the a movement more efficient
first of all the cycle separates the
storage and access of there and another
cycle allows you to specify where you
want your data to go on the device in
cycle also allows you to create
automatic data dependency graphs so the
first thing is cycle separates the
storage and access of data through this
relationship between buffers and
accessors so buffer is an object which
maintains data across off the host CPU
and one or more heterogeneous devices
you then have an accessor which is used
to describe the access on a particular
device so here we have an access which
our CPU then you can create another
access or to a GPU and buffers an access
there's a booth
template there typesafe across
Austin this is what allows the tape
safety across Austin device so one of
the things access letters allow you to
do is access allow you to specify what
where you want the data to be stored or
allocated so you have a kernel function
you want to execute and you have a
buffer you can create a global access so
this will store the memory in the global
memory region so say you have a constant
access this will store the memory and
the read-only memory region well if you
can have a local access or the switch
store memory and the group memory region
so you can have quite fingering control
over where your your data stored the
next thing is access is allow you to do
is access it specify how you want to
access video so whether we want it to be
read write or read write or anything
else and then that using this this the
the runtime can create these data
dependency graphs so as an example of
this if say you have 4 buffers and 3
kernels so the kernel a reads from
buffer a and race to buffer B can all be
reached from buffer a erase to buffer C
and then kernel see you reach from
buffer B and C reads writes back to D so
the runtime is able to automatically
determine the colonel and colonel B can
run in parallel because there's no
dependencies there but kernel C has a
dependency on both a and B completing so
then it has to wait for them to finish
before it can execute so some of the
benefits of this is their dependency
graphs is you you allows you to specify
your problems in terms of relationship
so you don't need to perform any
explicit data copies also removes the
need for complex event handling between
executions because it's automatically
constructed via the accessors finally
allows the run team to perform data
movement optimizations so it can
preemptively copy data to a device where
you're going to need it it can avoid
unnecessarily copy the data back to the
host if you need to use again on that
same device layer and I can avoid copies
to and from the device if you don't if
you're not interested in the original
values of the idea so let's finish off
what does cycle look like so I'm going
to go through a simple example of cycle
application so we're going to try to
implement a parallel ad function that
takes two input vectors and then I
vector so the first thing we do is
include the header file so this includes
in the entire runtime api so the first
thing the first thing we have to do in
the function is create both the buffers
so buffers handle a maintain a pointer
across a host and multiple devices I
also performed synchronization so using
Raia when buffers are destroyed it
synchronizes the data back to the host
so the the buffers lifetime was this
function so when the buffers are
destroyed it into this function will
synchronize the data back to the vectors
that you passed in so the next thing you
do is create a queue in a queue for
executing work and from the queue you
can create what's called a command group
so a command group defines the device
code you want to execute as well as the
data dependencies you need for that
via that function so the first the first
thing you do inside the command group is
you create these accessors so create
three accessors for the three buffers to
two input input buffers have read access
and I hope the buffer has as write
access then you use a parallel for so
earlier in order to create a device
function so this takes two parameters
this is first arrange to describe the
the range of execution you wanna do so
here we take the size of the access the
vector in order to determine the number
of work items want to execute and the
second parameter is a lambda function
describing the device code so the the
lambdas lambda is the code that's
compiled by the listicle device compiler
in order to execute on its what's
executed on the on the device so this
takes a parameter as well takes an ID so
for each execution of this function on
the device the ID represents that space
and the and the range of execution so
the thing to notice here is that the
parallel four takes a template parameter
and the reason for this is that because
circle has this a supports any standard
C++ compiler and any device compiler
into our intro operation between them
the there's no standard way in C++ to
name lambdas so every every see photos
compiler has a different way of naming
lambdas
so we need a wait for the host compiler
and the device compilers to be able to
communicate regarding the device
function so we we add this template
parameter in order to name the to
excellent so this is this kernel T here
gives names this this function so
finally we add the body of the function
so we use the subject operator of the
accessors to read from the input
pointers add them together and assign it
to their pointer and that's it and
finally you you initialize your vectors
and then you call the parallel add and
because the buffer synchronizes the data
when this function returns you have your
result in a and that's them so how we
lookin for time yep so so I'm gonna pass
over to my code to talk about the okay
thanks everybody that was great I mean
when I saw I've been involved in open MP
accelerator design as well as a lot of
things when I first saw sicko I was
really impressed with how it does
everything at a very high abstract level
very much like what C++ needs that
ultimately I think is the big benefit of
sicko you notice that it has it
implicitly allows you to do data
movement and supposed to exclusive I'll
say explicit isn't as bad some cases you
do need it but in other case in this for
C++ it's a lot better model in that way
so one other thing I want to talk about
is about where C++ is going so what does
what is the future of program uc+
supposed to look like given the fact
that we have all these possible models
to learn from and it's time to add that
to C++ so couple of things is that of
course this is still being talked about
in the committee and we anticipate that
it will be three or four years before we
can get to a technical specification the
thing that will make that happen are
these things we actually already almost
60 80 percent of the way there but
believe what all of this so we we so
just to recap where we are with c++ 17
we got two parallel algorithms and some
progress guarantees the basic guarantees
and the parallel forward progress
guarantees the blue
stuff is what's now in the queue to go
into C++ 20 you got things from
parallelism called database parallelism
task based parallelism execution agents
on the right hand side the concurrency
stuff we've already got the future plus
for us with the when the wait when all
executives people out there talking
about co-routines I'd lead the group on
transactional memory synchronic atomic
views things like that latches and
barriers none of this is for GPUs you
know that right
all of this stuff is only for CPUs okay
so how do we get from there to the to to
GPU computing indeed we have a mountain
to climb but it's actually not as bad as
it seems okay
so what we can do is that if we add the
the futures and continuation model gives
us an interesting starting point by
extension extending C++ 11 features with
this Microsoft style dot n continuations
which adds the sequential and parallel
composition capabilities when you have
when all when all when all the futures
comes back that's great so join or when
any futures comes back then you can do
something with it that is called a
choice okay so these are compositions we
also have these additional utilities
which I don't really want to talk about
because they don't really add any
semantics Anthony Williams is talking
about them right now
downstairs and he can do a great job on
this stuff so heterogeneous computing
for sick hole has couple of choices that
are already built up okay I don't want
to use CUDA CUDA is a great example
because they've done a lot of pioneering
work and in fact everything looks kind
of like CUDA but there's one problem
with CUDA what is that
it's C it looks like C it looks like
functions procedures line by line this
is not looks nothing like C++ we don't
want that we want something that is C++
like that's template like that enables
you to do static and and and and and
dynamic polymorphism that can make it
work with templates with concepts
and all the other nice stuff that's
coming so the peel of the groups that we
we have sicko is actually a really
really good example at a very high level
of allowing implicit data movement on
the one hand okay as well as automatic
data scoping it's ideal from things like
heterogeneous computing in for deep
learning neural networks machine vision
self-driving cars okay but we also can
ignore the distributor computing aspect
that's offered by LSU's hardwoods talk
on using HP X they do a terrific job as
well - of doing this in a very very C
plus in a very C++ way but they do they
use an explicit memory movement the room
directives ok nothing wrong with that in
some cases you do need those things so
what else do we need we also need what
NVIDIA has and really it had these ways
of create these things called bulk
dispatch once you get to the CPU
by the way Nvidia doesn't care how you
get to the GPU because why should they
okay
they just think they just know that once
you get to the GPU they know how to
blast everything in a bog dispatch fire
off a million threads and make it all
run in high throughput manner how you
get the data that you know what remember
coordinates talk about the data move and
how you get the data that's your problem
thank you very much
just get it there and we're gonna make
it run really fast we need that the
other thing of course is I I repeated
hpx cuz I love it so much but that was
an accident
but the heterogeneous computing compiler
from HSA why do we care about that the
AMD guys too initiated the designs for
for what they call APU integrating the
CPU and the GPU onto a single chip what
that gives you what that offers you is
tremendous low latency one of the
biggest problem you'll remember is this
data movement you know the chart that
the golden was talking about apu solves
that problem in a unique way using user
cues okay and we need to learn from that
if you if you know I share this SG 14
study group and in order for C++ to work
at different architectures whether it's
a discrete CP GPU or an integrated GPU
we need to be able to learn from that
and that's why I'm taking a look at
those four models okay
ultimately what's gonna come out is that
we're gonna be able to hopefully support
massive parallelism on multiple
distributor notes which CPU or GPU and
baseman head starts in games and
graphics I won't go over to this too
much because this is essentially about
with how it all came or it can't come
from but what I do want to talk about is
how we're gonna get there in order to
get to GPU computing we need executives
executives are these unique creatures
are essentially to function execution
what alligators are to memory allocation
they're also what iterators are to STL
they marry iterators marry between
containers and algorithms executives
marry between the constructs of
parallelism like for loops parallel
regions with the resources that you're
gonna use to execute that maybe they
might be a GPU node here an open MP
cluster okay a bunch of threads a thread
pool if you don't have executives we're
gonna have what's called an end-to-end
relationship where you're gonna have you
know every for each we'll have to have
something that says this for each is
used to access a thread pool then you
have another for each for openmp runtime
then you have a sink for fibers you
don't really want a world where you have
that end to end between the control
structures and the diverse execution
resource if it can all go through an
executor the executor can effectively
select the execution resource based on
you your policy parameters you can now
select where to run this stuff when who
run it and how to run it okay the how is
already supplied essentially by the
controls control architectures C++ now
already have quite a few house we can do
async we can do package tasks we can do
basic threads we can do futures we do a
couple of other things okay
but right now C++ has none of the
concepts at the bottom we don't have
thread pools we don't have ideas about C
PGP we only know about CPUs all right
so there are several computing competing
proposals and they be running around
each other for the last three years we
last this last summer we started talking
to them and making sure we've been
having you know I almost bi-weekly
telecon calls and we're making really
good progress and right now this is what
it's coming down to looking like it's
gonna be look like a minimal proposal
that supports this kind of chart
hierarchy where it's a foundation for
later proposals for heterogeneous
computing this is only glimpse of some
of the work we're doing we're gonna
submit a proposed a paper for Issaquah
for this so I don't want to go too
deeply but you'll notice that in this
diagram you essentially have an executor
which has one or more of execution
functions that would create lightweight
execution agents which then execute
execution these instruction streams on
the other hand at the very top year
these contacts which runs these
execution agents and these contacts
would manage these execution resources
aka like a thread pool and the and they
would have execute them on instances of
execution platforms I just said all that
sorry just a quick recap sigh my got
about ten minutes left now essentially
there is also simply pair ilysm that's
required one of the key markers of GPU
computing is that it's it's almost all a
lot of it is Cindy in nature you can't
do it without it so that was the other
component that was missing okay without
it we're not gonna go anywhere we don't
have a standard for Cindy computing
there's boost's md every vendor has
invented some sort of sim D capabilities
but everybody programs that using
built-in languages dopin of function
calls that supplied by the vendor it's
terrible every you know you see you
should you switch from sse2 to a BX or
to IBM's
I can know what they are even now used
to work for um you know you would have
to be right there all your color that we
love doing this own stuff so there's
been two proposals against here there's
a competition there's been two proposals
put out one is based on boo Cindy Jo Val
who I think is gonna talk about all has
already talked about it's by somebody
named Matias Kanade okay and then a
competing proposal came out based on the
vc library also by someone whose name is
very similar to my peers in this case
here's Kratz so it gets very confusing
to say whose proposal this is it's it's
Matias is proposal okay and what it is
is now it's settling to the point that
it's gonna look like something like this
it's gonna be a Dana paw that has a that
holds elements and elements of type T
that's written particularly for Cindy
ratchet register okay
now this morning a gentleman asked me
what if I have a different ABI will day
is an a B there's a default that ABI
here assume but you can there's an ABI
parameter that you can add in if you
have a different ABI and this is really
important because you know that this is
a fundamental thing with Cindy I'm not
gonna go too deeply into this there are
built-in operators there's no promotions
but some of this stuff you can get from
the slides I do want to get to the final
thing this is that with this we're gonna
be able to have the ability to with the
minimal proposals for executors we're
going to be able to get to a space with
sim D to a point where we can eventually
get to this this this space now we do
have to do other things we do the things
that golden talked about how do what do
we do about the address space c++
assumes a single address space single
flat address space we have to either do
move to some sort of either multiple
address space that's either cache
coherent or cache noncoherent we have to
make decisions as to whether we should
support either implicit data movement or
explicit data movement I kind of think
we need both we have to get to a few
other stage about should we support
legacy GPU devices they screed or or or
just integrate it we probably need to
need almost all of them to some extent
because we don't know whether we're
doing on an FPGA okay or just a pure
accelerator so there are a couple of
things but ultimately I want to take the
thing I want to take you you guys want
to take away from this is that
heterogeneous computing has been coming
for a long time
and that sickle right now today and a
few other candidates like HP X like HTTP
is able to supply heterogeneous
computing on C++ okay but depending on
the on the type of devices they might
the
support my very and finally that we're
also trying to add this to the C++
standard okay and that's pretty much
closed as the talk and I think the only
thing that I want to mention of course
is that now we just last week the
company has been working to code play
company has been working really hard at
releasing compute cpp as a community
edition which is now a free for download
and you can just go to this address it
essentially is an open source sickle
project that has the compute CPP SDK
which is a collection of sample code and
integration tools it also is the one of
the only one of the few implementations
of parallel STL the thing that's already
in C++ 17 okay and it's a sickle based
implementation the difference is that it
actually impacts on this GPU these these
these these parallel SCO teen runs on
the runs on the GPU not just on the CPU
can also run on the CPU as well to most
implementation would only run on the CPU
because that's all the standard actually
mandates it has something called vision
CPP for vision Processing's for
self-driving car this is unbelievably
useful okay it's the basis of why things
crashed and why they don't crash okay
and because we're using C++ algorithms
to process images at a high enough speed
such that they can be responsive enough
for safety critical demands you know
your brake fails you have to have that
light show up on your dashboard within
less than 5 nano milliseconds ok
otherwise you're not you're not safe the
other thing of course is the eigen C++
library something that is that allows so
this is all this is demonstrating what
you can do with sickle you can build on
top of sickle
such that you can execute it for deep
learning neural networks why is this
important if you haven't heard about
what's going on in the in the world now
almost everything you're using right now
uses some sort of deep learning when you
do a Google Translation it's doing some
sort of deep learning going back to the
central server and responding to you
when you're doing image recognition that
is all C++ deep learning using something
called tensorflow and you can now
potentially do and in my future talks
I'm gonna talk a lot more about what
these things can do and that's the
potential that we can get to in this
future world
that we're envisioning right now so
hopefully you guys can join us and
download some of this some of this stuff
our team is ready to support the sickle
but in the meantime my job isn't
necessarily to my company's horn but is
to try to bring this stuff to the
standard not necessarily sickle but
it'll be something that combines with
sickle and other things like what we
know about from HP X what we know about
from Nvidia what we know about from from
HTC I know Google already also has a
cuda implementation we're trying to take
all that learning and add that to the
C++ standard so the ultimate result is
something that works really well for C++
thank you very much
questions I don't know if I have any
other slides ah this has been conclusion
I'm also leave that up here okay
discussions questions oh the one thing I
do I I'll take one question but there if
you're doing a fpga with sickle there's
an open-source one called tricycle that
is done by silence that is doing it go
ahead please yes I have yes yes I know
I totally understand what you're saying
um the question is could it in the last
couple of years have improved
significantly for C++ and they have they
have implemented most of C++ so I would
be saying that secretly it's rude has
always been seen okay
of that I'm I don't think people would
disagree in the last couple of years
they've done tremendous progress
- what C++ in particular we admire how
they've been able to do passing
functions from the host to the to the
device but they are essentially the
entire compilation a compilation unit so
yes they still so that's why I wanted to
mention CUDA and I know Google's effort
on could CUDA that we're still also
learning from them as well - so no we're
definitely not ignoring CUDA the one
thing though CUDA is driving - what is
more F a unified unified virtual memory
okay and that's the latest thing so and
that's the latest thing that they are
trying to do so that's one model we're
also at apt adapting as well to be using
that stuff that unifies which will never
system okay they do have good stuff they
let the industry for quite some time and
they probably will continue yeah go
ahead
that's it so the question is the sicko
offer a notion of time control like real
time I assume this is for real yeah so
in the current cycle specifications is
this there's no M there's no API for
measuring time one on spaces so cycle is
based on the feature support of OpenCL
so cycle can only provide the right sort
of hardware support for open sale
devices can support so the so the
question is can you provide sort of
device specializations off of certain
functions yes so that is possible in
cycle year there's there's M so there's
macros you can use to see how I want
this this code to be defined only for
the GPU they're only for the the device
basically so you can specialize host
host a database a good
so that's the question if you don't
specify the device and does it choose an
automatic device so there's so the
example I showed has just showed the
queue and because I saw the huge ass or
a default selection picks a device for
you but there is sort of a range of
different options you can do and you can
configure it to pick a very specific
device where the device selector object
which is basically like a a functor
object where you provide a heuristic
that allows you to see sort of fine to
me pick a a device that's a GPU from
this vendor or supports this number of
execution units so you can you can be
quite specific about it the the other
thing that the cycle provides is you
don't actually need a an open sale
device in order to takes you so every
every cycle implementation requires what
could we call host device so everything
that can be run on a device in cycle can
also be run on the host with the same
execution and memory expectations so you
can do debug on any standard C++
compiler so how do you have you sort of
program differently for FPGAs in terms
of terms of bits so the moment the LED
so the open sale standard is still still
working towards supporting FPGA and this
there's a lot of fpga vendors in opencl
so there's there's a lot of active work
in the standards just now for trying to
provide sort of a better
standardization to cover the different
sort of memory model execution model of
every GS
so there's there's still some work to be
done there and sickled will continue to
support all the versions of OpenCL so
the moment cycle one point two supports
opencl 1.2 hardware the layer layer or
cycle will continue to support layer
versions of open sale so all advances in
an open CL and hardware and software
capabilities will be represented in
cycle as well
so so so the question was so they're the
example that showed at the end so so
that was so that example you could you
could do yourself if you if you don't
load it compute CPP so that would work
work now with the current version of
computer view so at the moment computes
equation also yeah so the question is
there any particular so what target
platforms to be support and at the
moment compute CPP the Community Edition
supports you been to 1404 and CentOS
operating systems and we for Intel CPU
and AMD GPU so we've tested we are
certain number of different OpenGL
drivers but the if when you download
went down the compute PP the integration
guide and Abel will give you some
specifics of what drivers are supported</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>