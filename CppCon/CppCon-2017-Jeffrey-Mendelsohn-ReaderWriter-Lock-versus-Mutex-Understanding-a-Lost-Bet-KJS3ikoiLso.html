<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Jeffrey Mendelsohn “Reader-Writer Lock versus Mutex - Understanding a Lost Bet” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Jeffrey Mendelsohn “Reader-Writer Lock versus Mutex - Understanding a Lost Bet” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Jeffrey Mendelsohn “Reader-Writer Lock versus Mutex - Understanding a Lost Bet”</b></h2><h5 class="post__date">2017-10-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KJS3ikoiLso" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">All right, good afternoon all.
I'm Jeff Mendelsohn, I'm from Bloomberg.
I'm a team lead there,
and I work in the software
infrastructure group.
Recently I was working it an intern,
actually, sorry, little
nervous, let me start over.
I want to make sure we
have the same definitions
for these things before I
get started with my story.
A mutex is something which
provides mutual exclusion
to an object, it's a lock.
Most common example would
be standard mutex in c++ 11.
A reader-writer lock is a bit different
in that it allows one writer in,
or many readers at the same time.
Most common example is standard
shared mutex in c++ 17.
We expect higher concurrency
when we use reader-writer lock.
In other words, where with a mutex
we can only allow one thread in at a time,
with the reader-writer lock
we can have many, many, many readers
going through at the same time.
However, when a writer comes,
something different has to happen,
and that writer has to
be given an opportunity
to get the lock eventually.
That's what I'm referring to
with the read write preference.
Some locks will allow readers
to just continuously reacquire the lock.
You know, a reader comes in,
and there's other readers
that currently have the lock,
they'll just keep getting the lock.
This will starve out your writers.
That's a read preferring lock.
A write preferring lock,
when a reader comes up,
he'll say, &quot;Hey are
there any writers here?&quot;
If there aren't, he'll continue
through and take the lock,
and if there are, he'll say,
&quot;Okay, I'm gonna wait for
that writer to go through,&quot;
then all the readers
will go through at once.
Something I heard around the office once
was that apparently a
mutex is all we need.
Even though there's a second type of lock,
this reader-writer lock,
which is supposedly more efficient
when you have multiple readers,
in practical applications
that's not the case.
At least so I was told.
I heard that any time you
have a short hold time,
which you should have cause
we're good programmers,
and the fact that a reader-writer lock
must have some overhead for
having this extra flexibility,
these extra capabilities,
it turns out that the overhead is greater
than the improvement in concurrency,
and therefore a mutex
will tend to be the best
and most optimal thing you can use
when protecting a data
structure, for example.
I've heard this, we have
discussion boards at Bloomberg,
so I've seen this in discussion boards,
I've seen this on the internet
since being through this.
As it turns out, it's very easy to verify.
So, what sort of started
this whole conversation?
I was working with someone on a component
which basically had a map as
part of it's data structure,
and he had a search of this map,
a very short search, but
still a little search.
He gave it to me for review
and it had a mutex built into it.
I said no, this can't be
the right thing to use.
You have readers who can come through
and access the thing at the same time,
and they should be allowed to do so.
Building with only a mutex
allows only one person through at a time.
As if I was educating him.
He turned around and
said no, you're wrong.
It actually works much better with a mutex
than with a reader-writer lock.
Now I was surprised, I
said show me, of course.
And he did.
I said, well there's something wrong here.
Let's go through your code
again, what's wrong here?
And we couldn't find it.
I was getting a little agitated,
cause you know, I'm just
that kind of New York person.
I said, let's look at your benchmark code.
Surely there's something wrong there.
And he went through
that and he proved, no,
this is good benchmark code.
And then I said well, let's
look at the implementation
of the reader-writer lock we're using.
And I looked at it, I said,
yeah, there's a lot of overhead here.
There's a lot of things it's doing
which it doesn't need to do.
That must be the problem.
Surely if you made the
reader-writer lock more efficient,
this would go away and we'd
have the correct implementation.
I'm not a quiet person at work.
I tend to speak out very vocally,
and a peer of mine, another team lead,
said, no, I don't think you're right Jeff.
I think you're wrong.
Even after you do all this work,
it's still not gonna be any better.
I said, no, you must be kidding me, c'mon.
We're gonna fix this,
it's gonna be better,
we'll prove it, we'll be done.
He goes, no, in fact I'm
willing to bet with you.
Which was odd, because he
never is willing to bet.
He said, I'll bet with you Jeff,
that you can't make
anything better than a mutex
for this application.
I said, you gotta be kidding me.
I said, there's a pretty long hold time,
relatively speaking.
Searching through a short map
has got to take a couple of iterations.
I could see where the
problem was in my mind.
I knew how to fix this.
So we bet, and he being
a very particular person,
he wrote out the bet.
It was like four pages.
Delineated how we're
going to evaluate this.
He was very Linux oriented.
We used the component
we were talking about,
that started it,
and we were going to test over
a wide variety of scenarios.
But all these scenarios
seemed biased to my benefit,
so I thought this was an easy, easy, win.
So I very loudly took the bet,
and I was very happy to have it.
The only little tricky thing about it,
was it was going to be
developed by an intern.
Intern was excellent.
He actually produced an excellent lock,
exactly what I wanted.
But the reason I bring that up is,
again it was like a four
page written document,
he had a clause in there
that after the intern left
I would be able to go in there and fix it
in case it didn't come out right,
but there had to be some limit to that.
Some time limit.
After that time limit expired,
even though I had exactly
the lock I wanted to build,
I had to pay up for this bet.
Fortunately it wasn't too bad,
it was just a lunch at
my favorite restaurant,
but, that's good.
So as I said, I lost the
bet, I paid for the bet,
it kinda still upset me.
Why wasn't I able to build
a better reader-writer lock?
It would make sense that you
should be able to do this.
Right, I mean otherwise,
why would people have
ever thought about them,
why would people wanna use them?
Surely we would get more concurrent
than just one thread at a time.
So I built a new benchmark test
which can be more detailed,
more insight to what was going on.
The benchmark I'm gonna be using today,
and I'll be showing you
later, a little bit,
or showing you results from later, a lot,
simulates access to a
shared data structure.
In other words,
don't think of this as
taking work from a queue,
think of it as getting the work,
applying it to some data structure,
and going on, doing it as fast as you can.
Which is a little awkward.
You normally wouldn't
hit this data structure
as fast as you can, right?
You wouldn't just get
a request, do the work,
come back, there'd be some pause.
A key thing I think to this story,
is that we're hitting
it as fast as we can.
It's back to back requests.
So anyway, a thread'll come
in, it'll work independently.
It'll take the next work
item, and I say take,
I have that in quotes because
it's just a counter going up,
it does a little bit
of math to the counter,
it figures out whether ones
have to be read or write work
based off a parameter of the program.
So it'll lock appropriately,
then it'll do a short
little busy loop of work,
which was the load parameter
I'll be talking about.
Again, this load is for when
you're holding the lock.
There's no delay from
when you release the lock
to when you try to get it again.
Then when you're done you'll unlock.
That's one what thread is
doing as fast as it can.
Overall these bunch of threads,
however many we're running,
we run them concurrently,
but they have a set duration to run,
that'll be controlled
by a separate thread.
In other words, the
separate thread will start,
you'll set a global atomic
to be from zero to one,
everyone will be waiting for
it to go form zero to one,
everyone will start.
Then it'll switch it from one to two.
Then everyone'll be watching
for it to go from one to two
to mark when they stop.
This was the best way I could come up with
to make sure all the threads ran
for the same amount of time.
The cost for this, as I'll show later,
is actually very trivial.
Reading an atomic variable
is very efficient.
So I'll do this test
for some amount of time,
I'll repeat the whole
thing a bunch of times.
Maybe a hundred, then I'll
look at the 80th percentile
and I'll use that as the measurement
that I'm reporting to you guys.
This code is available through Get Help
on Bloomberg's public site.
Okay, are there any questions so far?
Please feel free to
interrupt me at any time.
It'll be a very short talk if you don't.
Okay.
So, I've actually run this
on many, many platforms
on many, many setups.
Initially I'd use Bloomberg internal code,
it's production code, to do this.
It's becoming open sourced,
that'll be available to you guys
if you wanna see it later.
I worked on a c++ 11 version,
which is what I have posted on that site.
I've also done some 17 work
to test shared mutex and all that.
To be blunt, the first time
I wrote this presentation
it had all the graphs and all,
it was about a four hour talk.
My wife was very kind to sit through it,
but I really didn't think
it would be appropriate
to put you through all that.
So I cut it down,
and do so I mostly focused
on the Linux results
cause I thought that'd
be the most interesting.
I will point out two
outlier results as well.
Not surprisingly on Sun and on Darwin.
But I'll leave further
investigation up to you guys
if you choose to do so.
I think the only platform
I really haven't tested on
which is sort of a big hole in my results,
is I have never tested it
on a Window's platform.
Which I know is kind of faulty
considering what's
right across the street.
So I apologize for that,
I'll get to that eventually
and update whatever I have in my talk
or my paper or whatever.
Good to move on?
Any questions on how I benchmark?
Going on.
Oh sorry, yes sir.
- [Audience Member] (mumbles)
So the question is am I
changing the percentages
of read and writes.
For a particular run,
for a particular number,
it's a constant.
When I show you the graphs,
I'll show you a set of
graphs from one percentage,
then I'll show you a set of
graphs from another percentage,
and I'll get kind of annoyingly
exhaustive at some point.
That's a very good question though,
because as it turns out, I
don't want to jump ahead,
when you do all reads versus 90% reads,
there's a huge difference in performance.
It's a surprising
difference in performance
and I'll show that very shortly.
Does that answer your question?
Anything else?
So let me go ahead and manage
expectations for a moment.
I've already made the joke about my wife
sitting through four hours,
so I can't say that again.
I wanna be clear that the
benchmarks I'm showing you
is a very specific usage case.
This is not a general purpose usage case.
I'm not saying this gonna
solve all your problems.
I'm just saying if you find that a mutex
is generally a good
solution for your problem,
maybe you should try
this reader-writer lock
as an alternative.
So I'm gonna have way too many graphs.
I don't know what the count is,
I think I have like 40 graphs.
To sort of back up my story
and to prove my points,
not to bore you to tears.
I'm gonna show that
synchronization is very expensive.
In fact, I'm gonna show it's so expensive,
that it might bring you to tears
if this is your daily work.
You have to weigh these
observations for what they are.
This thread's continuously
hitting a data structure
as fast as they can,
which hopefully isn't what you're doing.
Hopefully you're not running
16 threads at the same time
hitting the same thing at the same...
It's just...are you laughing at me
or are you laughing at the concept?
That's good, I think.
Maybe better than me.
I'll of course discuss this implementation
the intern gave me, the
losing implementation.
Again, I wanna point out it was perfect,
it was exactly what I asked for,
and it will be in our
production code in the future.
Of course it does solve
certain usage patterns
better than anything else I've seen.
The so called winning implementation.
I of course didn't give up with a loss.
It's just not in my personality.
So I'll go through what
we came up with at the end
and show how it's better.
But I will be confirming mutex
is a very good solution in some areas.
It shouldn't be dismissed
as a random gossip
and made bets over.
And the end, I'll show you something
which is a small loss versus using a mutex
in some scenarios, but in general,
a huge win for these really
high contention scenarios.
So to start this out I
wanna compare two things.
I wanna show you what
the cost of one atomic is
versus doing nothing.
It actually do nothing,
but it's probably unfair,
I should've made it read the memory.
But it actually is doing nothing.
So to make this more of
a reader-writer lock,
for this thing I'm
calling the atomic lock,
which again is not a lock,
it's just one atomic access,
the lock and unlock will
just do a fetch add,
they'll increment a value and
use acquire release semantics.
The lock shared and the unlock shared
will just load the value.
So this is much less than
you can ever do for a lock,
but it clearly depicts
why we have such a problem
when comparing ourselves to a mutex.
So I left out a whole bunch of
things I was supposed to say.
Okay, so backing up for a second.
I have this load value,
which determines how much work it's doing
while it's holding the lock.
A hundred is sort of 100%.
It's doing what the load was from the bed.
Ten thousand would be
a hundred times that,
and one would be a hundredth of that.
That's the scale I do.
I do one, ten, a hundred,
a thousand and ten thousand
in the graphs.
Hopefully I won't be showing
you all of them all the time,
but sometimes I will.
So first graph.
I wanna point out some things
of what we're looking for
in these graphs before we go
through this particular one.
We expect as we increase the number
of threads doing a particular project
that the amount of work
we do increases, right?
So it should be sloping upwards.
Actually, I guess that's the bulk of it.
So, in this example, I have 100% read.
Every axis is a read, so for
the null it's doing nothing.
For the atomic it's doing the atomic load.
And we see both of them
behave basically the same
when this internal delay is very large.
In fact, going to a 90% read scenario,
we still have that behavior
when that hold time is very long.
Continuing down to 0%.
I'm sorry, the Y axis is the, I apologize.
The Y axis is the work, the X
axis is the number of threads.
And by work I mean the amount
of work done per second.
As I said, I have that one
thread which measures time,
it starts and stops everything.
It figures out how long it
was actually sleeping for,
divides the total of the
work by that period of time,
that gives us the work per second.
So this is work per second
across the whole system.
And you'll notice the values on the Y
will change pretty dramatically,
I'll try to point them out when it does.
But from the previous graph,
no, they're all about the same.
I'm sorry, right, I apologize.
I'll point that out later.
For a very long hold times
the amount of work you're able to do
is pretty much constant.
Which makes sense, right?
Most of the work is doing
this bogus calculation
while I'm holding the lock.
There's no effect of the
overheard of the lock here.
So lets move down to
where I do I have effect
of the overhead of the lock.
Here I'm doing still, 100% reads,
this should be the best scenario.
The atomic operation, just the one read,
is keeping pace with the do nothing.
You can see there is some cost here.
But overall I would almost argue
these are relatively the same
in that they increase the
amount of work you're able to do
the more threads you use.
Move one step further to now 90% read
and you see something very different.
This is what I'm trying
to point out in this talk
is that even just with
a few atomic writes,
we see all the scalability
disappear, right?
The one going up to the right is the null.
That's what we wanted to see.
That's where we wanted to remain.
When we added that one atomic read,
in one tenth of the work,
we immediately lost all
concurrency and it went flat.
Is that clear?
Yes sir.
I'm sorry, can you speak
up a little bit for me?
- [Audience Member] (mumbles)
I'm sorry, one more time.
Oh, there's a speaker here,
you should come up here.
- [Audience Member] It's not quite clear
what null means in this context.
Here, null means we're
doing absolutely nothing.
When I call lock shared or unlocked shared
or lock or unlock, it does
nothing, it's in line zero.
- [Audience Member] There's
(mumbles) implementation
of shared lock without any--
Exactly, there's literally doing zero.
So it sort of gives you the upper bounds
of what the machine can possibly do
in this benchmarking scenario.
- [Audience Member] So you
have that implementation
in the library.
Yes, not library, but in
that GitHub repository,
it's all there for you.
- [Audience Member] Okay.
And all it is is the four method names
and empty declarations.
Any other questions at this point?
No, okay.
So let's take one more
step down this rabbit hole,
and we'll go to 0% read.
This is all locks.
Now, if you'll notice, we went
from the atomic being able
to do about 20 thousand
work operations per second,
down to about 18 thousand.
So what we do see is a
change in performance here,
but it's not dramatic.
And most importantly,
the character is that's
still not scalable.
Which we would expect it not
to be given the previous graph,
but okay.
Yes sir.
- [Audience Member] I
apologize for the interruption,
No, I need the
interruptions, please, more.
- [Audience Member] So, how
many courses do you have?
Depending on the machine
I was running it on,
this Linux box had, it
was on a slide, it was 20?
- [Audience Member] 24.
Does anybody remember, 20?
- [Audience Member] So
the interesting thing here
is between the 20 and 30.
So you have 30 threads
running on 20 course.
So why would you expect at
least any kind of scalability
beyond the number of course?
Um, good question
I did have hyper threading,
so it might've had more logical cores.
Also, to be honest, I had to
run it on a shared machine,
and while I ran it, you know,
as much a quite time as possible,
it's possible some other
people were sneaking on it too.
This is not an aggregation though,
this is just the Linux results.
So it's a very fair point.
- [Audience Member] Thank you.
- [Audience Member] (mumbles)
Well, there must have
been some other overhead,
or I was able to do hyper threading.
Very good question, thank you sir.
I was supposed to repeat it too.
The question was,
why do we still see an
increase past 20 threads
when there was only 20 cores?
Yes sir?
- [Audience Member] Just a continuation
to the previous questions.
If we're not using atomic support
at synchronization (mumbles),
are we using some system cores?
Here at this point in time,
I'm just showing you the
effect of one atomic operation.
So it's not a true lock.
I'm showing you that if we do nothing,
the performance is here,
if we use a lock, if we use
this an atomic we're down here,
if we actually use a mutex,
we're even way further down here.
You're a little ahead of me.
And again, the question was,
- [Audience Member] My question was,
when we compare it without saying
that you don't use atomic
or mutex in that case,
my question is what are the
synchronization primitives
we are using on this implementation?
Right, so the question is
whether the synchronization
primitives we'll be using
further in the talk, which
will be, I'll show you,
the next set of slides will
be mutex and semaphore,
then after that we'll go into
the losing implementation,
and after that to the
winning implementation.
Wow, everyone is, okay, I
asked for a lot of questions,
I guess I deserve this.
- [Audience Member]
Excuse me, I was wondering
if you could repeat
what does the load mean?
How do you--
So the load was scaled to be
so that load is a hundred,
means it was about the same
amount of work as the bet.
In other terms, it's about
330 or so math operations.
- [Audience Member] I see, so
what are the math operations?
Uh, adds and multiplies.
That is one division also, sorry.
There's one module that's
equivalent to division.
Otherwise it's about
300 adds and multiplies.
- [Audience Member] I see, thanks.
Sorry, am I supposed
to repeat the question
when they use the microphone?
I don't know.
The question was what is
the load equivalent to.
Any other questions?
Oh okay, this was, following
up on your question.
Now we're gonna look
a the cost of a mutex.
But I wanna point something
out before we start
is that the, I know I'm
comparing it to the known,
comparing it to the atomic,
so we went from do nothing, to an atomic,
now to an actual lock,
an actual primitive.
Mutex or a semaphore.
The bottom line is actually an overlay
of the mutex and the semaphore.
And when I say semaphore, what I mean is
that the semaphore
initialized with one count
and a lock is a wait,
an unlock is opposed.
Is that good?
That's good.
So this is 90% read with
a very long hold time.
This is 90% read with a
much shorter hold time.
And here we can see what
we're expecting to see,
but also see the difference
between a mutex and a semaphore.
So we don't expect a mutex or a semaphore
to scale very well, right?
They only allow one
thread through at a time.
We show that the mutex is more
efficient than the semaphore.
Okay, that's expected too,
otherwise why would have a mutex?
We would just have semaphores.
It shows that a single
atomic operation is, what,
four or five times more efficient.
Although it doesn't constitute a lot.
Any questions here?
(coughs)
Sorry, I'm getting over a cold.
All right, so let's talk about
actual reader-writer locks now,
and get out of these basic things.
What's the design criteria we might have
for a reader-writer lock?
One would be fairness, and that
can be construed in two ways
for this particular problem.
On first sense of fairness
when you're talking about a lock is,
as threads come in,
do they acquire the lock in the order
that they arrove there?
A second sense of fairness is,
with regard to readers and writers,
when one finishes do we
hand it off to the other,
or do we allow the same
side to keep holding it?
I don't know why I keep looking up there.
Second design criteria
will be the CPU utilization
when a thread is blocked from the lock.
This is to sort of address the
concept of spin locks, right?
If while I'm waiting for the lock
I choose to just keep hitting it,
that means I'm using up
my whole time and slice,
I'm using up that CPU,
other work cannot proceed.
In general, I'm gonna
be showing you things
where we actually suspend the thread.
Where the operating system suspends you
and other things are allowed to run.
(coughs)
'Scuse me moment.
It's easy to design a
lock where, you know,
if you're expecting one operation
in a million to be write,
essentially you'd have
it optimized for that.
But that doesn't have
a real usage scenario.
(coughs)
I think just this topic doesn't like me.
That's kinda clear, I'm just
gonna move on from there.
So when we built this
losing implementation,
the goal was to have it
have minimal overhead.
It was basically a couple
of atomic operations,
and try to be as efficient as possible.
I think I left out some terms
from the previous slide.
It was writer preferring.
In other words, when a reader came in,
if there was writers
waiting, it would block.
It wouldn't allow that
reader to get the lock
no matter what happened.
This is important to prevent
starvation of your writers.
When a writer unlocks the lock,
it prefers to give access to readers.
So the goal of this was
to try to alternate.
Have a writer go, have
a bunch of readers go,
have a writer go, have
a bunch of readers go.
(coughs)
Sorry.
To address party inversion,
which is something I was
supposed to talk about before,
we wanted to use a primitive
to select the next writer.
And party inversion is important
because you could end up having
a very high priority thread
waiting for a low priority
thread to do work.
The upmost operating systems
give you some ability
to adjust the priority
of your threads coming out of the lock,
to have the highest priority of itself,
or threads waiting on that lock.
Does that make sense?
I see one nod, a lot of blank faces, okay.
We wanna get that ability if we can.
In other words, using
the atomic for waiting
would probably be bad
for us in that sense.
So I'll give sudacode next,
but basically the state of this machine
is that attracts a number of readers,
attracts number of pending readers,
readers who are waiting
for the next writer to go,
and writers, both where
there's an active writer
and any pending writers there might be.
That uses one semaphore to block writers
and one semaphore to
block readers, as needed.
So what are the four methods look like?
The lock, which is the
mutual exclusion lock,
increases the count of writers.
If there are any readers or other writers
it waits in the semaphore.
So this is a thread coming in saying lock,
we'll follow that procedure.
And for unlock, it'll
decrement the count of writers
and move all the pending
readers into a reading state.
It'll also post to that semaphore,
which is blocking those pending readers
so that all of them go through.
Otherwise, if there's no other readers,
it'll go ahead and allow the
next writer to go through.
Does that make sense?
So basically it shifts off
to the readers if it can,
or it goes over to another writer.
On the other side, on the shared side,
a reader will come in,
if there's no writers,
he'll just increase the count of readers
and go on with it's life,
go ahead and do whatever it has to do.
If there are writers, it'll
join the pending readers.
It'll wait.
It'll wait on a semaphore.
On the unlock side, it
decrements the count of readers,
and if this was the last
reader, and there are writers,
I have to tell the next writer to go.
So I post to that semaphore.
I know it's a little quick,
if you need to get more detail
you can check out the GitHub
or talk to me later.
Let me go ahead and show
some results for this.
Oh good, my smiley face shows up.
So I added these two
arrows to all the graphs
from this point forward
because there's other
information on this graph
which isn't the primary reference point
of what I'm talking to.
So one will point to the mutex
which will be our reference,
and here is the bottom blue line.
And one will be the losing implementation,
which is the top purple
line in this graph.
The third one is the P
thread implementation
of a reader-writer lock.
It is biased towards readers
when used right out of the box,
and that's how this is.
In other words, if readers
currently have the lock
and other readers come in,
they'll just go through.
They won't wait for writers.
It supposedly gives
you better concurrency.
That's how I allowed it to run.
Also, I'm giving you
my opinion of the graph
with regard to whether we're beating mutex
or losing to mutex.
I hope there isn't a
legend needed for that.
Is that clear?
Okay.
So here we are at 100% reads,
and very long hold time for the lock.
Again, a ridiculously long hold
time for most applications.
We see that our reader-writer
lock as expected
is outperforming a mutex.
So this is a good slide for us.
Here I decreased the hold time of the lock
by a factor of 10.
We start seeing a little more shape to it.
Still interesting, but still scalable.
As we increase the number of
threads we get more work done.
This is again a good slide.
Here's where things get a
little more interesting.
At the scale of work we
were doing for the bet,
we see it's not a very scalable process.
This atomic operation based
reader-writer lock plateaus.
It doesn't have as much
scalability as we would want.
We're still outperforming
mutex which is great,
but it's really not giving
us what we would've expected
before starting this process.
Yeah, lock the doors, I
don't know he got out.
Again, still, it's a good
win for reader-writer lock.
I'll just show you two more graphs
which basically say the same thing.
Decreasing the hold time of
the lock changes the graph,
but still basically the same result,
and again, but here there much closer.
So you have that neutral face.
So to recap for so far, for
exclusive reading we did well.
This losing implementation did well
when we're doing 100% read work.
And we crushed the mutex when
the hold times were very long,
which was expected as well.
So let's move on to when we
have only 90% readability.
When we have some write
work done in this workload.
This first one again is
a very long hold time,
so we expect a reader-writer
lock to perform very well,
and it does.
Moving down by a factor of
10, something odd happens.
The performance of this
losing implementation
is now actually losing at the
higher concurrency levels.
Let's keep moving.
At the level of the bet,
we see a strong loss of
this reader-writer lock
compared to mutex.
Even though it's 90% read,
we're still not as
efficient as a mutex here.
Not much else to say.
It's very shocking to me.
I don't see you guys weeping or anything,
but I certainly was.
Just moving down this path
showing you even more badness.
If I hold the lock for
just a few math operations,
this is the graph I have at 90% read.
Again, showing that this
losing reader-writer lock,
even though it's 90% read,
can't outperform the mutex.
So to recap the 90% reads,
mutex was generally better.
Only with a very long hold time
did the reader-writer locks outperform.
This is not what we want.
Go ahead, yes.
- [Audience Member] (mumbles)
How is it possible what?
- [Audience Member] (mumbles)
It's not zero, it's just scaling.
It's actually probably
around, I don't know (laughs).
I'll have to look at the actual numbers,
it's probably around--
- [Audience Member] (mumbles)
No, it has to be around 300 I think.
If I remember correctly.
- [Audience Member] (mumbles)
Yeah.
So what we wanna do,
is we wanna minimizes the
loss to the mutex in scenarios
where a mutex is conceivably
a good thing to use.
In other words, in unfavorable conditions
for a reader-writer lock.
But we wanna outperform the mutex
when we're doing primarily reading.
That's makes just logical sense.
That's what a reader-writer lock is for.
Now we really wanna crush the mutex
when we're gonna hold the
lock for long periods of time
and doing primarily reading.
So as an example, let's join them.
If you can't beat 'em, let's join them.
We're basically gonna build something
which is a mutex plus a
little bit of other machinery
to try to outperform the mutex.
So I'm kind of in a chicken
and the egg situation.
Which came first?
I wanna tell you a little
bit about the lock,
but I don't wanna tell you too much yet.
As it turns out,
what really made this lock
implementation work well
was the ability for a thread
to reacquire the lock.
In other words, I come in, I
get the lock, I do my work,
I'm then able to reacquire the lock again
as quickly as possible.
Whether I was reading or writing,
and whether I'm gonna be
doing reading or writing,
this ability to reacquire
that lock was very important.
That's where we got the efficiency from.
Having said that, it's gonna
be kind of designed as,
on the lock side you can
imagine being just a mutex.
I come into lock, I get the mutex,
I'm done with the lock,
I release the mutex.
On the read side I'm gonna
lock it and release it
on the lock shared operation.
Obviously there has to be some
other machinery involved now
that keep everything kosher, so to speak.
That's basically the idea.
As such, since basically
this is one mutex,
it's not gonna be reader preferring,
cause readers will not be
able to acquire the lock
when other reader have it,
nor will it be writer preferring,
because when a reader
comes in it won't check
to see if there are writers.
It'll still try to get
that mutex lock first.
Technically not true, but it
gives you the right concept.
I wanna use this mutex to
select the next reader-writer
to handle priority inversion
and whatever other concerns
I might have for performance.
One thing I have to mention though is,
it's explicitly neither reader preferring
nor writer preferring.
So one might say, well what is it doing?
What is it experimentally doing?
One would expect since I'm
using basically just a mutex,
all the readers and writers come in.
If they're 50% one type, 50% the other,
I would get out of it equal probability
who the next user is, a reader or writer.
Experimentally it seems to prefer writers
a little bit more than that.
But perhaps not statistically significant.
The basic state of this
is I'm gonna attract
a number of active readers,
number of pending writers
and whether or not there
is an active writer
in atomic state.
They'll be a mutex which
every thread which comes in
will centrally access.
If I'm a writer and I
come in, I get the mutex,
I'm then gonna go to a semaphore.
This is in reverse of a really
well known old implementation
where the semaphore came first.
But let's just move on.
Oh, one thing to note here,
since the writer that
comes and gets the mutex
then may have to wait on the semaphore,
it's only gonna wait on the semaphore
if there's readers currently
accessing the lock.
As such, it's possible that we may have
an interesting performance issue
when you're low number of readers.
In other words, the
lock may perform better
when there's 0% readers, and
worse when there's 10% readers.
So we'll test for that.
So the sudacode is,
again on the lock side,
increase the number of impending writers.
Every writer that comes in
is presumed to be pending.
Lock the mutex.
Switch the writer from a pending writer
to an active writer in the counts.
And if there's any readers,
wait on a semaphore.
And unlock is just erase
the indicator of the writer
and unlock the mutex.
On the reading side, we do
have to do a short circuit,
an atomic short circuit for,
if there are no writer or
pending writers and I'm a reader,
just go ahead and take the lock.
Without that you have a
big performance drain.
As I said before, you're
gonna lock the mutex,
increase the number of readers,
unlock the mutex all on the lock shared.
And the unlocked shared,
you'll decrement the number of readers,
and if this was the last
reader and there is a writer,
post that semaphore,
release that trapped writer.
It's very simple implementation.
If you look at the code in that, GitHub,
you'll see they're all like
five, eight lines of code.
Very straightforward.
So let's start on the easy path.
Again, a very long hold time.
All read work load.
We're comparing, on
the bottom is the mutex
which we expect to have no scalability
and to kinda not do well on this workload.
And the top is this
winning reader-writer lock.
For reference, I include the
other two reader-writer locks
from previous slides.
They don't have arrows next to them,
and they're both on that
second line at the moment.
But the P thread implementation
in the losing implementation
will be shown as well.
So this is good, but very expected.
It's a very easy workload
for the reader-writer lock.
Decreasing the hold time of the
lock to something still long
we still see good scalability
and good performance.
Moving to slightly more interesting,
at the workload of the bet,
we see that same shape
as before, of course.
Well, not of course, but we do
see the same shape as before.
But we still see a very strong win
for the reader-writer lock.
Decreasing again we still see a win
for the reader-writer lock and
go into a trivial workload.
They all kind tie out.
So let's move to a 90% read now
and see if we can do better than before.
So on this ridiculously long hold time
we are outperforming the mutex.
However, we're not the best
on the graph, and that's fine.
But we are outperforming the mutex.
Moving down towards more
reasonably sized workloads,
we have this graph where
essentially a mutex
and this new implementation
are doing the same.
One step further, this is
the workload of the bet.
We see that this new implementation
is outperforming the mutex quite handily,
whereas the previous one,
the one the intern did
is the bottom curve,
we can see things have
changed here dramatically
from previous implementation.
Going to a very short hold time,
we still see the new
implementation winning.
And going to a ridiculously
short hold time
for 90% read work, we show
the new implementation
beating the mutex quite handily still.
So this is sort of good
results, I can sort of end here.
But let's go further and
check out the other side
when we have 0% reads.
In other words, this is what
a mutex should be good at.
These numbers are all basically the same,
this graph has no meaning.
I don't know if you can see the scale,
it's basically between like 16 and 17,
for (mumbles) to worked on,
so it's not really useful.
Scaling back to a long
hold time, but not crazy,
again, this is where 0%
read, so it's all write work,
this is what a mutex
should be optimal out.
We see this new implementation
and the mutex being basically the same.
Going down to the workload of the bet,
we still see them being the same
even though it's all write work.
Finally, at short hold times,
we see mutex starting to edge
out this new implementation.
And then at a ridiculously
short, few math operations,
we see the mutex outperforming
this reader-writer lock.
But again, it really should, right?
Cause we're really just
the mutex plus overhead.
We should be losing in this scenario.
So as I mentioned before,
we have the semaphore inside
the mutex for the writers,
which is only used when
there are readers present.
That can cause a negative effect
when there are a few number of readers.
So let's just go through
and check that real quick.
So at a 10% read load, a
very, very long hold time,
not much difference between
the lock implementations.
Moving down to still long hold time,
not much difference in
the implementations.
Again, this is not much difference
between mutex and reader-writer lock.
The losing one is substantially worse.
At the load of the bet,
we see the new lock outperform the mutex.
In other words it's able to capitalize
on that 10% of reads versus 0% reads.
It's not a cost, it's a benefit.
And the same goes as we
reduce the workload again.
And down at the shortest
possible workload I can do,
which is a few math operations,
the mutex and the new implementation
are basically the same.
So to do a Linux recap, to
reap those last 20 slides,
and all those scenarios,
there was one time mutex
significantly outperformed
the winning, this new implementation.
There's once time mutex
slightly outperformed it.
Did I read that right?
Significantly outperformed
and slightly outperformed.
Eight times where the two were kinda tied.
And 10 times when this new implementation
was substantially better than the mutex.
If we focus now just
on the work we expect the
reader-writer lock to be good at,
those 10 slides that we did initially,
the 100% read and the 90%
read, there were two ties,
and eight times when
the new implementation
was significantly better than the mutex.
I read it from the other
direction for some reason.
Is that pretty clear as a win, I hope?
I think so.
I wanna show you two other
platforms real quick.
I tested on many platforms,
I tested with many versions of the code.
One platform which was
consistently bad for me was Sun.
To start off, I'm showing you just on Sun,
what a mutex and semaphore
look like on these graphs
for the hold time of the bet.
A workload of a few
hundred math operations.
The mutex outperforms the semaphore.
With 100% read on a same workload,
all the reader-writer locks
outperform the mutex on Sun.
Unfortunately, when you
go down to 90% read,
you can't beat the mutex on Sun.
I'm not sure why this is the case,
it's obviously something specific to Sun,
our IMB machines and Macs and whatnot
all perform as I want them to.
But I felt like this is
one of the two operations
I had to show you.
The second operation was on
this particular Mac machine.
Here I'm showing you the performance
of a mutex versus a semaphore.
And here you notice the semaphore
is much better than the
mutex, which seems odd.
Again, the semaphore implementation
is it starts with a count of one
and waits or posts as
it's locking or unlocking.
So since semaphore is more
efficient than mutex here,
I switched my implementation
to use the semaphore
as opposed to a mutex.
Then goodness came out of it.
We see that 100% reads,
the new implementation
handily beats the semaphore.
And at 90% reads we
have the same scenario.
So in summary I hope I've showed
that synchronization is very expensive.
Doing anything, even
just atomic operation,
one right atomic operation,
will drastically effect your scalability.
Mutexes are pretty good
in these scenarios,
these high contention scenarios
where there's low hold times,
but we can do better than the mutex.
Are there any questions?
I guess we got 'am all
out during the talk.
Very good, thank you
very much for your time.
(audience applauds)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>