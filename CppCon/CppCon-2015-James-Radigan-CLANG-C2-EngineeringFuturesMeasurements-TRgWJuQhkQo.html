<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: James Radigan “CLANG + C2 - Engineering/Futures/Measurements&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: James Radigan “CLANG + C2 - Engineering/Futures/Measurements&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: James Radigan “CLANG + C2 - Engineering/Futures/Measurements&quot;</b></h2><h5 class="post__date">2015-10-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TRgWJuQhkQo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Jim Radigan I am the dev
manager for the C++ code gen
technologies team and they've asked me
to come out and talk about some of the
things that we've shipped and primarily
to talk about and touch on some of the
aspects of the Klang hook up that we're
doing for the Windows platform so in a
nutshell i'm going to talk about some of
the really great coat gen technology
that we shipped in we just shipped in
june and then i'm going to step back and
try to demo some of the klein compiler
that we've built okay and this will be
what we're going to ship in November so
the first thing I want to do is
acknowledge the clang and llvm community
I I really have a sincere respect for
everything that's been accomplished it's
been amazing I've been looking over my
shoulder since I don't know 2007 and
watching this thing grow and it is truly
truly amazing I think it started with
chris lattner and evan chang at apple
making some pretty huge contributions
and then recently we've been talking to
folks from google with Chandler and two
folks are here today from google to talk
probably after so that is the the full
full-on acknowledgement and NS sincere
thank you so the mission for the talk is
the new I'm going to go into details
about the new code generation in 2015
then I'm going to talk about how we use
it to build the entire company and then
I'm going to talk about it also is used
for building all the universal
applications for C sharp and I'm going
to talk about how basically that work
that we do every product cycle
internally to build the company first
before we retail the code generator
a cruise to visual studio and now what's
going to happen is I think that this
work will accrue to the clang and llvm
based compilers for Windows and it will
also accrue back to clang and llvm
itself as we try to help get that onto
windows so everybody's seen this before
right that's a that's an intel pentium
processor and if you were to scale it
today to the semiconductor processes
that we have in flight that's how big
that scales and the point I want to make
with that is two-thirds of that is
compile is is controlled by a compiler
basically the sea to dll at microsoft
right and so that's a very linear way of
describing the world but right now
cogeneration is getting seriously far
more complex than the standard CPUs that
I was showing you right there and if you
think about it you really have to begin
to worry about FPGA we're worrying about
it now as far as being able to build
acceleration in the cloud that's going
to be a major part of Microsoft's
business we also have to worry
tremendously about something that's akin
to inserting automatically MPI
primitives so you can actually do
distributed computing and we really
really have to be realistic about
attacking heterogeneous computing
because the GPUs are just getting to be
readily available and on chip with
shared l3 cache so that's a long-winded
way of saying cogeneration really
matters and we have to get it right and
we have to be headed in the industry on
performance and more importantly we
really really worried a tremendous
amount about securr'ity now probably
more than anybody else could ever
imagine so the compiler matters from a
business point of view as well so if you
look at the amount of money that's been
spent on windows licenses and hardware
alone in the last
five years that's what I got from
Bloomberg that's just what's been spent
on windows licenses alone and then
what's been spent on Intel hardware
specifically so that red arrow is the
compiler so we worry about throughput
absolute correctness anybody catch that
and then I just making sure was paying
attention code size code quality we
build all of these across the company we
build them probably we build Windows
quite a bit but we build a sequel in
office at least every three months to
make sure everything's and go through
the enormous validation that's required
to make sure that everything is correct
so how we work is important to know too
because as we do a lot of the things
that we do we're working within a
context that is extremely difficult in
terms of making sure we prove
correctness at every step so when we
innovate we don't add switches to the
command line can't afford the testing so
what we have to do is throw it at the
wall probably every day right and so
what we're building is live compilers
against the live windows sources and the
windows the windows sources has at least
five to six hundred developers pounding
every day with massive code flow that we
see so right there anybody anybody have
a guess for what that is yep that's
twenty four cores with a huge amount of
memory and it's only building one of the
five full stack builds that we build
every day live top a tree for the
compiler and live top of stack for
windows and that's before we do all the
post-processing and then go on to run BB
tease and app compat where we stage
everything and distribute the images to
about 750 machines and we get that back
within a 24-hour period and if that
comes back in that 24 hour period with
one failure we shut down checking into
the compiler okay
that's the environment that we innovated
in daily so within that context for
visual studio 2015 we shipped new secure
code generation a new asynchronous
programming model for c++ new auto
vectorization a whole new framework for
incremental whole program analysis and
we did a tremendous amount of work on
bill time you can imagine that in our
world build time is a very very big deal
so your security this is the simulation
I did at the build talk and I didn't get
in trouble so I'll do it again here this
is how a couple guys in China hacked the
google gmail client so right here you're
going to create an image and you're
going to create an onload event and
you're going to pass that to f1 here
you're going to make a copy and you're
going to fail tit to an address so
what's going to happen is that line
right there is going to destroy the
image tag object what happens behind the
scenes is the heat gets sprayed the V
table gets hacked we're going to call f2
asynchronously and both image and event
go out of scope so they should be
reclaimed so you've got a use after free
and so what happens here is you're gone
that's it so we had to do was come up
with a way to deal with this and I named
a slide Chum in the water because we
generate this code or something similar
to it for every indirect function call
and the first time we put this out there
we r e compiled and reissued huge parts
of Windows 8 1 after it went out in
order to deal with many of the hacks
that were basically targeted at the
Internet Explorer and we didn't have to
document much because the community did
it for us it was about a half
hour before we actually got people
watching blogs to see that everything we
had put out was disassembled completely
to the bone so that's why I call it Chum
in the water but basically you can see
what's going on here the opp the notion
is that the pointer to the function
comes in and ECX we shift it and then we
use it to do a table lookup to see
whether or not a bit is set indicating
that the eight bytes on x64 would be a
valid target call it valid call target
so in order to do this that's a graphic
there's the bitmap and that what I'm
showing there is the bitmap guarding
those bytes in memory and now what's
happened is to really ship this so that
it could be using a production
environment we spent far more time
dealing with the overall system and
interop so what we had to do was modify
the memory manager and windows to make
sure that the page tables didn't have
backing store we had to actually modify
the OS loader we had to modify the
linker and the compiler and those had to
be paired as we distributed those with
the OS loader and the reason for that
was that all of this has to interrupt so
that you can link against code that has
not been compiled with guard and that
was a that turned out to be a nine-month
saga just that one interop story so then
we had to make small binary format
changes of course for all the other
tools and we had to modify all of our
binary static analysis we have tools in
the company that allow us to crack all
the binaries post-process them write new
code into them and so reading these
binaries actually got perturbed so all
the static binary processing all of the
operating system and the tools all had
to go out in one atomic lump sum across
the company so guard is a new compiler
switch it required close integration
with the operating system and it does
it's completely interop it should be
seen
unless you can do whatever you want
across dll's in the can and this whole
thing works and the the response you can
imagine has been it was it was unnerving
to be the first one to actually say
we're going out there and be proactive
to fight the bad guys and so everybody
held their breath for about six months
and one of the things we had to do is go
down and recompile Adobe and adobe flash
and all the digital rights management
and all that and so far it's been out
there for a little over a year we have
moved the surface area of where these
guys have to go we've created an
inconvenience as what they've called it
but nobody's been able to crack the real
mechanism itself so next topic on
cogeneration the cut the clock rates not
increasing so everything is going much
more multi-core much more parallel
programming model in Visual Studio 2015
we've introduced yield and a weight
we've created true co routines for
supporting these higher-level language
constructs and they're far more
efficient than anything that's out there
or anything that we've done previously
so here's a really really simple idea of
what a co routine is co routine is just
a way of for the sake of the talk making
a stack frame putting it in the heap and
then allowing that routine to suspend
and then since the heap contains that
frame you can always resume it for very
cheap so here's a great example that's
Fibonacci okay there's Maine and you'll
notice the yield statement so what
happens now is every time we
fib we're going to go to the routine on
your left and you see the yield
statement yield a what's going to happen
then is that will yield back a and it'll
set the co routine up so that I can
resume at the next instruction in that
loop three straight forward so what
happens is during execution we're going
to actually save all the context
registers in the code generation so that
we make the cogeneration for creating
this the co routine all be in the
compiler in the back end will grow the
frame then when we call will actually
create the co routine on the heat and
then we're going to suspend and then
what we'll do is save the registers that
are in the co routine body will unwind
and restore the registers and we're back
that's really pretty quick that's what
happened on each iteration of that loop
so that's yield and this is a weight I'm
running a little left behind so I'm
going to go quickly here but that's a
great way of writing a completely
asynchronous TCP reader and the impact
of this is enormous across all the
system programming internal to Microsoft
now this is being adopted by windows and
the colonel it's going to be adopted by
sequel it's going to be adopted by Bing
and a lot of the network driver people i
think are working with gore nachav today
i was the famous c++ author of co
routines so one of the other pieces of
code generation that we did is we spent
an enormous amount of time vectorizing
complex control flow on the left is a
loop that's very typical in FC plus
language so we have to actually deal
with control flow the basic concept is
to actually mask
the operations and then run both sides
of the loop and then what can happen is
you'll or things together with the mask
there's the other side which is the else
so we execute both the left hand side
both left hand sides each iteration of
that loop it's just that the results are
masked to either use the original value
or the new one that's computed based on
what the condition and that if would
have been for that particular iteration
so this really matters for black Scholes
for example in the financial industry to
do monte carlo simulation what's going
on here is on your left is the code that
i've sort of summarized and then on the
right is the straight-line vectorized
code that you can get out of Visual
Studio 2015 and this had a tremendous
speed ups so black Scholes is six
hundred percent numerical recipes we've
had some huge speed ups it goes on and
on and on being able to vectorize
control flow super important so moving
right along to build times we actually
took on the developer scenario and in at
great length but there are two scenarios
that I want to point out one is the
developer scenario where you're
incrementally building which is really
important if you're a game developer the
other one is the build lab scenario
where you're going to build windows
every night top to bottom two completely
different profiles so here's our results
for build time at a glance for 2013
versus 2015 you can see for xbox it's
had a tremendous impact there's Forza
there's chrome and there's the compiler
itself those are some huge huge changes
so one of the things that we did was
incremental whole program analysis sorry
I reverse the order these slides
this is a chart that indicates the speed
ups that we've achieved in 2015 we're
2015 is the orange bar now incremental
whole program is super hard what that
means is if we're doing whole program
analysis we do a closed world assumption
on pointers and aliases and so well
first build a call tree and then we'll
use the call tree to do a complete
propagation of information up and down
the call sites until that stabilizes and
then what we'll do is optimizations and
then what we'll do is final code
generation so even when you change one
line of code or actually add white space
we were forced for correctness issues to
completely recompile everything what
we've done in 2015 is developed the
technology so that you can make
modifications that are actually very
meaningful and still get the right
answer and we will only at sometimes
compile want recompile one file so
here's an example of what I was talking
about so if you look at the slide on the
left whole program analysis can prove
that a B and C are not aliased and then
imagine about a hundred thousand other
unchanged functions across 500 files
it's an enormous program right which is
not a typical of what we see and so
let's just say that you make a small
change so what happens is foo goes from
what's on your left to what's on the
right and we've added a small read like
that before and with most compilers you
got to recompile the whole thing if
you're compiling whole program so today
we'll only compile one function so what
we've done is we've changed the workflow
for the compiler it used to be that for
a full build that's what you would do
you get CIL which is our dump from the
reader from which we would build the
call tree then you would run the
compiler generate objects length them
and of course you'd have your exit E so
now what happens is we have an
incremental PDB and the income
incremental PDB in a sense stores a
highly some
form of data flow information and
Delta's that we can use to calculate for
each iteration the minimum number of
things that we have to recompile so here
are some of the scenarios that are huge
wins for us from 2013 2 2015 one of the
things we added in addition I'm trying
to go really quickly because we run slow
is debug fast link and this was
incredible for being able to do full
system builds and cut the debug debug
builds literally in half for many large
programs and basically what we did is we
left the debug information in the obj
files and we created a dictionary PDB to
cross-reference back to it and then of
course we had to change the tool sets to
handle that ripple effect so we also add
a profile guided optimization for xbox
one as you can see one of the claims to
fame early on was that Forza got a
fourteen percent improvement and if
you're somebody that makes money on just
eking out frame rate that was a really
really big deal so this is where I'm
going to transition the talk now I'm
going to start to talk about
architecture so windows we build windows
every day and we target for different
processors right now so the code
generators that are building windows in
the we the code dinner is we ship our
building windows every day they're also
used to build sequel office olive dotnet
all of visual studio and many windows
futures internally okay so cogeneration
to a lot of people is responsible for
instruction set architecture and are in
other words get generating the right
instructions and just performance but in
reality cogeneration is responsible for
getting the ABI right security interop
runtime semantics debugging making sure
GC is encoded correctly across different
domains and
godley amount of errors and warnings so
a lot of people look at cogeneration
that way right which is just simply
you're going to compile some c++ you're
going to link it and then what's going
to happen is you're going to run that
exit on the hardware no big deal pretty
straightforward that's the world I see
every day I see compiling for frameworks
and run times as well as the executive
layer of the operating system in
addition to all the other things that I
just listed and we do that for the
manage side as well because we take in
MSIL so the API and the ABI gets
complicated because it's cross barred
with the hardware that we have to target
so if you think about it all of this is
unique to the hardware target so that
user dot exe has to run against those
runtimes that NT dll for that particular
hardware and all the code that we
generate has to make sure that the
interop between these runtimes and
frameworks is correct so there's an
example of what I'm talking about back
in this slide yellow arrows here's a
great text example for sort our source
code example on your left is C++ calling
into c plus plus CX and then you've got
c plus plus CX calling in to c sharp all
this is asynchronous there are three
different objects semantics and there
are three different exception handling
semantics across all of this and so when
something goes right wrong we have to
tear down the frames and make sure there
aren't any security problems and we have
to also make this extremely debuggable
so
that gets complicated across all the
architectures that we're targeting right
now and we have to have every one of
those be mission-critical correct so top
down what I talked about here is a
bottom-up view of cross-platform that we
kind of see all the time and we're
terrified by frankly there's another
kind of cross platform which is top-down
and that's across platforms actually
across operating systems so as you can
see the clang front end is used on iOS
and Android and on Windows we use what
we call c 1 x.x and it's been around for
a really really long time so what we had
to do was create something like this for
cross-platform and the point is this
takes care of the top down view that I
just showed you and this takes care of
the bottom up view that I showed you
with the four different architectures
that we target so all we did was create
something pretty simple that is obvious
that we go we take the clang ast s
regenerate llvm I are we don't do
anything else to it and then we quickly
go ahead and generate see two tuples in
our which is the tuples are the name for
our IL so what we want to do is in
November ship this so for all four
architectures will have a top-down
cross-platform story across for code
that would run on Android iOS or Windows
and then C 2 takes care of all of the
ABI and API and runtime semantics so
that's what we're shipping in November
so Clank for c2 this is about the status
we are at ninety-nine percent correct
for all of our internal testing across
four architectures that's with fully
optimized code
and with full debug support we are also
compiling in the stage in the stages
right now of compiling many large
applications and going for stress so in
v1 in November there'll be many local
changes we were under pressure to get
this thing done and out the door as soon
as possible so internally we've got
debug info structured exception handling
a couple of lines for the bridge we want
to work with the community and
contribute back as much as we can
because simultaneously we are also
committed to making sure that clang and
lvm itself gets onto Windows we really
welcome the community it's just that we
have a different responsibilities so I
think side-by-side we're working
together and a lot of a lot of what
we've finished here we're going to be
contributing back we're still trying to
figure out how to work with the
community and we're going really slow to
make sure that we're taking or we're
actually being as deferential as
possible and as respectful as to as
possible to what what it's like to work
in the community because it's a new
experience for us all and we want to
make sure we do that right so in
February we're going to hopefully at
that time have contributed everything
back there'll be no local changes except
for our three line hack to get us into
UTC and at that point we should be
discussing a whole set of new
discussions about a lot of the new
architectures that I showed you in that
diagram where we're talking about FPGAs
and heterogeneous computing so for those
of you in the audience that are flying
oh s s clang open-source people this is
the three lines of change okay every I
see people shaking their head which
means that's good i spelled the function
calls right and I got the drift of files
right so basically what we're doing is
we're loading the sea to dll creating a
client object we're initializing c 2 and
then finally in koh gen action which is
pretty obvious we call clang invoke
compiler pass
w for Windows that name isn't very
imaginative so this is where we are
today and this is one way that we could
go we could actually open it up so that
llvm becomes far larger and contributes
much more to the platform in terms of
optimizations and also being able to
consume bit code which I'll get to the
other way this can go is which is super
cool and if you're a nerd I'm dying to
run this experiment which is to run both
optimizers at the same time that's the
supercharged and I think that you know
barring anybody caring about compile
time would be a really really
interesting experiment so again we're
going to be doing this while we're
helping the client and lvm community
proper get onto windows the other thing
that's really important to call out and
we want to take full advantage of this
and this is some of the other futures
that I'm going to get to is that we'll
be able to read and write llvm bit code
so in a sense you can say that llvm we
will at least set up a system where llvm
bit code could be seen as a universal
input to cogeneration for all the
windows api s and a B is that we have to
deal with I think that's an exciting
longer term thing to discuss so for the
community we promise we will never fork
we will help climbing an lvm clang c2 is
read-only and three lines of change
we're never going to try to push this
back to the community we don't think
that anybody in the community would ever
take this and test that path and one of
the things that I think after talking to
Chandler Carruth I wanted to make sure
was crystal clear is that we're not
going to try to propose changes decline
an lvm that would be bothersome
unique to the sea to dll implementation
want to completely treat the internal
data structures inclined llvm is
read-only and we will we will deal with
any difficulties through versioning so
we should be off the radar screen but
for the other project we have people
helping to contribute I think people are
helping with the debug information right
now I think people are helping with
structured exception handling and I
think Gore you're helping with a weight
or people are talking to you about a
weight all right those are just some
examples of the collaboration that have
just begun it's a new it's a brave new
era right we're all one big happy family
Apple Google and Microsoft all right
what the hell so so c++ universal
language universal across platform
except for this part of the language
ifdef right that throws a monkey wrench
in through it whoops so it turns out
that if you're writing a bunch of
cross-platform code you wind up putting
if thefts in there just to deal with
implementation details so this is a
really big deal for us the strategy for
the company is that if all of this code
runs on android or it runs on iOS it
should run over here with out having to
do any of those if deaths but that's
that swiss cheese on the left is the
reality okay so our goal with the clang
see to hook up is all of these yellow
ellipses are going to go away so here's
an example of four categories that I
could find for if defs when we looked
across minecraft and we looked across
bing maps which I'll show you in a
little bit that contains a huge amount
of cross prop cross-platform library
code
so there are if defs for missing c++
standards conformance there are an
enormous number of if deaths four errors
and warnings everybody should raise
their hand right everybody wants
warnings to be as errors when you're
billed right nobody if defs anything off
and then there are if deaths in there
for bugs and there are if desks for
dealing with libraries and library ap is
based on compilers so here's an example
of what will now compile well this will
get rid of a whole bunch of those yellow
ellipses all of this code if it were if
if this were to appear in that Android
code the C++ Microsoft compiler couldn't
compile very very attic templates
variable templates sorry I'm sorry
that's our front end guy so then
non-static data member initializations
for aggregates and nested namespaces
those are three prime examples of
missing conformance then things like
this will go away this is the inverse of
what you'd expect so I throw I showed
this first we actually see this this
will go away this kind of a warning and
of course naturally these types of
warnings will go away and then here's an
example of a template bug that would go
away and imagine how long it takes to
figure these things out just that we
treated this we chop it and don't treat
it as a true 64 bit and it winds up
having to be cast to make sure that we
get the right type so that it comes out
to be four billion but that's just one
example of a compiler implementation bug
that causes that Swiss cheese that I
showed you on the left so measurements
as of Friday this has been this project
has been deeply studied the here we are
with the compile time what I wanted to
do was actually look at compile time as
a broad spectrum what you have to and
every time I go and do performance
analysis it never ceases to amaze me
you're going to be a good engineer how
incredibly rigid you have to be about
snapping a really good baseline and
being absolutely objective so bear with
me this is not as full as you would like
because I just didn't feel since friday
that i had been doing the due diligence
required but everything i do present
here is something that I do believe is
absolutely correct so if you're
compiling see what we did is we compile
GCC and as you can see we broke down the
overall compile time for respect to k
six GCC and then this is the front end
component and then this is the bridge in
the back end component and you can see
that if this is 1 cor no multi-threading
no parallelism in the build system and
in its 02 without Pogo that those
numbers show that for see it's starting
to look like to see the Microsoft see
front end is faster if you just go and
compile specter k-6 zal ank right that's
a real good representative for C C++ I
would never write an XML parser that way
but it's a great example of C++ and
right now it looks like this is where
the two front ends or break even and
then we have this other benchmark which
is computing eigenvalues and it is super
heavily templated c++ and here we can
see that what happens is clang c2 winds
up being significantly faster so if you
were to actually look at it and it kind
of instinctively makes sense if you look
at the basically the age of the
compilers right as you go from the left
it seems like the Microsoft front end is
faster and then as you go to the right
it seems like lying becomes faster it
makes sense because of the parsing
technologies upon which each of the
front ends is built so I feel pretty
good that that makes sense so what's
really cool is if you look at the bridge
itself where we convert llvm to our IR
tuples in situ it looks like here's a
vtune profile it's not until you get
down here in a profile that you can
actually begin to see the bridge itself
after we read and so the punchline is
that that yellow column represents all
of the cycles that we use to compile the
eigenvalue benchmark and it's 56
trillions so it's 56 times 10 to the 9
and that's two hundred and twenty six
times ten to the six so basically it's
really about Oh point zero or three
percent so far just for the reader clean
up so all up it's less than one percent
oops
you
so here's
the code generation that we're producing
today we've got what we feel is we're on
par for things that really really matter
it turns out that the vector Iser has to
jump through hoops in order to vectorize
the key integer loop in this and when
you do you actually speed it up by two
good luck on vectorizing that that this
loop alone when we generate code for the
inner loop in this we generate about 53
runtime disambiguation checks before you
enter into the key loop and it still
speeds it up by two and the code size is
you know quadruple then this is a this
is another integer vectorize ER and so
plex that's another integer vectorize er
so we've got a really good handle on
this and we think that when we ship in
November will be on par with o2 for the
existing microsoft compiler and if you
think about it we started in January
that's pretty good and one of the things
we have this is a real litmus test
there's a real this has been seriously
painful for us and I'm sure it hurts at
other compiler teams if you want to
vectorize this loop you got to rewrite
it to just do the analysis I have to do
the analysis and you find out you can't
vectorize it then you have to not impact
the performance after that rewrite which
was basically fake to just do the
analysis so now we've got to get a
primary induction variable out of this
loop and make all the memory references
have symbolic expressions before we can
actually automatically affect your eyes
and then automatically parallel eyes
this loop and that's really key to
performance today without that you can't
really compete in a coach in space on
all the chips so no nothing would be
complete without a demo so how many
doing for time okay so one of the things
that I'm going to start with this is not
why we did clang c99 is probably when i
looked i couldn't believe
it was somebody put it out there on user
voice that we didn't compile c99 we hear
you we care about see and then when I
look back today there are over a
thousand user up votes on that one so
that wouldn't actually a really big deal
so and demo that so that people believe
in me because I said that we were going
to do that six months ago and so what
I'm going to do is I'm gonna don't see
yourself
so there you go there is some good
old-fashioned c99 every time you hit
these keys house out there we go all
right so there you see complex double
complex double this is a call into the
sea library doing a complex absolute
value and as you look below you can see
all the c-style initialization for
complex and imaginary or the real and
imaginary and so no demo would be
complete doing c99 without there i am
calling as you can see standard see 99
and then i type it'll brought XE and
then there we go yay complex for real
okay so coming back what are the other
demos I'm going to do is structured
exception handling one of the things we
found it was really really easy to
quickly bring it up except exception
handling using the 36 compiler and what
we're going to do is once the guys at
Google get it to a new design to
completion will consume that and will
throw away whatever we've done
internally locally but one of the things
that's really difficult and you can
imagine I think we just in order to help
the community put all of our structured
exception handling tests out there and
so one of the tests I was going to show
you is except for you that's a good
example of some structured exception
handling
on the left and you can see it's nested
and it's got a complex filter and there
are over 95 permutations of this and
with much deeper nesting with go tues
with wreaths rose you name it this is
one of those files that I'm sure
nobody's we didn't release this little
tidbit right but this started back in
nineteen ninety and this thing has grown
and when when something would break in
this file that guy would actually send
you email personally on the compiler
team we all lived in terror when whether
this is coming up so what I'm going to
show you then is that we can compile
except for you we have except for
everybody right so I go quickly there we
go I packed compiled that before but I
want to actually we're about to run out
of time basically the whole thing passes
it passes 02 and now the other thing
that I wanted to show you really quickly
which is much bigger yeah was
this is big maps and it is an enormous
program and it consists right now of 10
cross-platform libraries all of which
compile and run on Android and what I'm
going to show you then on another laptop
is that in fact this will you can bear
with me hopefully this is still working
you guys see that or not I'm Yuri thank
you very little and I duo urea bottle of
wine okay so what I wanted to show here
was we're breaking in read token and if
you look at this at ten we are here all
the cross-platform libraries off to your
left where the cursor is moving and what
I've done is I've just stopped here on
what is a JSON token and if I go to the
right and then pull this down I can
still use the JSON visualizer right
there hmm it did work yesterday is that
right Oh shift of 5 f
yeah there's the reader being
constructed in klang or in json with the
clang compiler and then if we are so i
just hit f10 and now the token f10 did I
go the wrong way there we go so the
point I just wanted to make though was
that within the context of an enormous
source base with ten cross-platform
libraries compiled with clang c2 we're
running big maps and this is to indicate
scale and credibility of the project
before we're about to ship it in
November so with that I want to say
thanks that's an overview of the the
project and any questions I oh there are
there is no OS support whatsoever for
this what happens is that the support
for the abstraction is completely
carried out by the back end of the
compiler working in conjunction with
libraries and the only overhead is
actually creating a frame in the heap
for a co routine so there's 0 5 or 0
awesome
exactly and that's why we consider it to
be very innovative and very impactful
because the overhead for a large
asynchronous source base source basis
gets cut in half at many times and one
of the things that's really important is
that that size savings for asynchronous
programming is huge on phones so
asynchronous programming on a phone with
no very very tight constraints actually
this is an enabling technology to get a
lot of applications to fit so dot then
in c++ CX will every instance of that we
found was originally taking how many
bytes was that Steve about 5 K when we
first looked at it for each instance of
dot then and now what it could be is on
the order of 40 bytes oh the guy showing
me the sign repeat question the question
was about co routines yield in a weight
and whether or not we were working with
fibers or any other OS abstractions and
so the answer is 0 0 Gore is the gore is
right here gore is the gore is the real
genius behind the implementation I just
worked for Gore as a code generator guy
but we wound up building something that
I think is really pretty significant for
asynchronous programming in C++ so sorry
that's what we're trying to shape for I
don't know about that though there is
going to be a need to filter Windows
header files right which is your knob
for how much of that stuff you want that
knob has to stay I think what's the see
one thing Oh No uh the so that in a
couple of years like say five years out
it would be great to converge on one
front end the size of the legacy source
bases and the amount of uniqueness in in
the in the c++ that we compile just for
sequel office in fact dev give itself
one of the one of the interesting there
to there if I am about my team is about
to ship there are three places that I
hold my breath on across the company
when we compile one is the dotnet
framework because they think everything
all over the place and use azum and you
can imagine what that's like on for
architectures the other part is
structured exception handling and
windows and then the other part is C++
exception handling in sequel the way
those teams use that and all of that and
all the asynchronous and all of the
asynchronous complexities to get
correctness is quite a ways off given
how unique the source bases are two
windows and having been around for five
to ten years oh I'm sorry
we have to it would be a goal and it
definitely is a very strong desire to
converge on climb it really would be a
win but I can assure you that it's a
long ways off and in the meantime we
have a responsibility to the stockholder
right which is to continue to build the
company continue to innovate in security
and performance and bring a lot of those
source basis to new processors that were
coming out with so that's why we're
really interested in getting clang an
lvm onto windows while we're doing this
simultaneously everybody wins if there's
fewer front ends we can compile parts of
each of them but it's you know it's not
significant enough right now no way in
fact maybe ten percent of Windows Steve
you have to yell steve is the front end
dev manager and the IDE one of the
things that
really hard in the end the IDE
environment when you're typing in right
the front end that parses the language
is actually shifted into a whole nother
mode where it's parsing with incomplete
and incorrect by default right so that's
another large ball so it'll go in there
we want to and converging actually saves
a tremendous amount of money I know it
would be really cool to actually have
the community be able to contribute
directly to the things that we compile
now compile with to be honest with you
internally we're just like you guys
right it's common sense we like to work
with other smart people and I think
we've always been this way and but also
one of the things you should know is and
this is a real hidden aspect but we have
unbelievably tight schedules and we run
with really really small teams so even
though we have the best of intentions
right they have to allow us the time to
actually do the collaboration and so
that would be what I would consider to
be the only change really you should
have been here in June when we shut down
I've heard about that yeah and now that
we're here I we actually have the
ability to
experiment in some really really
interesting ways I think like you're
pointing out so if we go back to that
slide which i think is really an
important thing for the audience
you guys see that yeah right we can go
that way or we can go that way again
it's a matter of correctness we have to
guarantee mission-critical correctness
across an enormous amount of code what
we what this compiler code compiles if
you look at what the numbers are from
telemetry its enormous and it's gotten
to the point where people actually think
if there's a compiler bug it's like a
hardware bug and you guys have lost your
mind what are you doing right and so the
overhead for more you add the more stuff
you break become something we really
look at so we could allow that switch to
be out there but we haven't figured out
a model where we could ship things where
it's sort of as is question yeah
I didn't want to embellish that one at
all absolutely in fact that's one of the
big value adds you know having you know
not been exactly the sharpest template
programmer in the world I could actually
see the difference right away and was
able to be much more productive with the
warnings that I n errors that i got from
the klein front end so that's a personal
endorsement right there right that's a
whole day of pain and templates right
yeah we're shipping stl with this and we
are yeah yeah go ahead we want to be
to actually use every bit of the
technology and that in the Klein front
end that we we can the difficulty is
going to be you know realizing that the
implicit contracts and in explicit
contracts that we usually have for
shipping compilers is going to have to
change so there is a thing that we're
going to have to go through internally
and culturally which is we're going to
ship in November that'll allow everybody
to kick the tires and then finally and
that'll be as is that's going to be out
of band but within visual studio and
then by februari we culturally should
have an idea of how to handle the
contract with a customer Chandler okay
we we have found though that oh boy
you're right okay so anybody any other
questions any any other any other
follow-up just feel free to email me at
Microsoft it's jay Ratigan at microsoft
com okay thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>