<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Michael Caisse “Practical Patterns with the Networking TS” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Michael Caisse “Practical Patterns with the Networking TS” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Michael Caisse “Practical Patterns with the Networking TS”</b></h2><h5 class="post__date">2017-10-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2UC6_rfJuAw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- [Michael] My name's Michael Caisse,
I work with Ciere Consulting,
we're a small consulting
group spread across
five inter time zones.
We work on really full stack software,
and by that we mean very
small processors with
two or 4k of memory, and of course no OS.
All the way up to thousands
of cores spread across the
internet doing something,
hopefully, useful.
Our group spends a lot
of time doing networking.
Or using, previously, Boost.Asio
with different devices that
we have linked in as if
they fit inside that model,
whether it's a CAN bus or
some weird serial device or
some proprietary thing for
robotic control of some sort.
We try to bring everything
back into that model.
It's a model that we've found has been
very useful for developing
and keep more sanity in.
So everything should
have three easy steps.
One of our clients, if
we don't actually have
our very first set of screens
that we provide to them with
this wizard that has
three steps, he just says,
&quot;Go back and start all over again.&quot;.
So everything's gotta
have three easy steps.
Of course, for us, the
three easy steps would be
we should probably just use Rust, right?
Nicole would say, &quot;Just use
Rust&quot;, that would be it.
So in Rust, what would we do?
We'd make a cappuccino first,
because we don't need
all three steps in Rust.
(audience laughter)
That's where we would begin.
Then we would write about
two lines of code and
we would then mock everybody
on social media that
wasn't writing this in C++ or something.
I just want to put a side note in here.
I actually tried to find
the two lines of Rust that
would do something useful in networking,
and as far as I can tell,
it just doesn't exist.
So while this slide looked pretty good and
I tried really hard to find something,
it didn't quite work.
So instead, we're just gonna use C++,
and we don't actually
need three steps, either,
so we're gonna start off
making cappuccinos, too.
It'll make us feel better.
So once we have our cappuccino,
we'll write a few lines of code.
It won't be two, a few.
But then we're gonna be very polite and
we're gonna go back and
we're gonna be nice to
those nice Rust people
and let them know that
we were able to do it in
some small piece of code.
So here it is.
This is our few lines of
code to get something going.
Wow.
Now if you were next door,
you'd be hearing a movie.
Am I the only person hearing the movie?
- [Man] Can you do it with iostreams?
Can we do it?
We're already complaining about iostreams.
Hold on there, buddy.
(audience laughter)
Alright, so with iostream looking things,
we can get a stream that is connected to
something or another, a server.
And we can go ahead and send
some things out to the stream,
we can read the stream like
we normally read streams,
and then print the output.
So a few lines of code, we
basically have networking.
None of you are convinced yet,
we'll talk about that in a moment.
So Networking TS.
It's based upon Asio, and Chris'
work inside Boost.Asio went
in back in March of 2008.
The library's been around
for a very long time,
it has a lot of users.
The final publication of
the TS was in Toronto,
and so it's now published,
you can go grab it.
And it's not changing constantly,
you can take a look at
it, you can read it.
And you can get an implementation.
Chris has an implementation
inside of GitHub that
implements pretty much most of it.
And you can go ahead and
start using the Networking TS.
What does it have, what does it include?
It includes TCP, it includes UDP,
including multicast.
Client and server applications is kind of
what it's looking towards.
It's scalable to handle
many concurrent connections.
It's flexible with iPv4 and iPv6.
It handles name resolution,
it's got buffer management
in it, it has timers in it.
Now if you're familiar
with Asio, it doesn't have
a lot of the things you might
have become accustomed to,
dealing with asynchronous files or
serial ports or things like that.
This is a slimmed down version that
is looking towards networking.
It also doesn't include encryption,
protocol implementation.
All kinds of other things that
you might want, it doesn't have.
This is a library to build upon.
You're going to build things
on top of this library.
You can think of it as very low level,
it's a very low level library.
So let's start off with the stream,
the thing that nobody liked.
Alright, so we have our stream.
What are some of the
problems right away when
we look at this code that are
gonna be difficult maybe with the stream?
(audience whispering)
What's the most obvious
thing that's missing, maybe?
- [Man] Error handling.
Error handling, we don't
have any error handling.
So there's no error
handling, anything else?
Number two thing that's
normally asked for?
- [Man] Synchronous?
Well it is synchronous,
but that's what we're gonna
expect from our streams.
(laughs) We'll catch you later.
- [Man] It looks like we don't have
control over the timeouts.
Yeah, we don't have
control over the timeouts,
number two thing usually asked for.
First thing usually asked about is,
&quot;Where's the error handling?&quot;,
and the second thing is,
&quot;Where do we handle
things such as timeouts?&quot;.
So let's go ahead and see if
we can get those in there.
Let's take a look at what
we've done for timeouts.
We can tell the stream,
and this is very generic,
again, it's a very high
level idea or concept,
that the stream functions,
the things that the stream will be doing,
they're going to expire after
a certain period of time.
I just picked five seconds,
they're gonna expire after five seconds.
At least now I don't have
to worry about whatever the
OS implementation has, but they're gonna
just be five seconds.
Now as far as error handling,
we have to do errors the
same way that we normally
do string handling,
errors with stream handling.
So we're going to check whether or
not the stream returns up boldly in false.
And then if it is false,
then there's an error when we test it.
And we can go ahead and get the error out.
's.error', any idea
what that might return?
What would you want it to return?
(audience whispering)
- [Man] An error code.
An error code, what kind of an error code?
- [Man] (talking distantly).
A what?
- [Man] (talking distantly).
Yeah, we've got them like built in, right?
We would like to actually
have the errors that
are built into the standard now,
and so that's what we're gonna get.
So how many people are not satisfied?
That's fewer people than I would expect to
be not satisfied, actually.
(laughter)
Really, so you got...?
How many of you just wanna go out and
just use streams for
doing your networking?
How can you be both?
How can you be wanting to do the
streams and also not satisfied?
This is a very confusing group.
How many of you were not paying attention?
(audience laughter)
- [Man] We just want
networking in any form.
Okay, so, yes, networking
in any form, in every form.
So what is this error code?
An error code entered into C++11,
an error code is really a great idea.
And the idea of error code
is that we could talk about
error categories, errors that
are of a specific type that
we might be interested in.
And those categories are
these high level concepts.
And at the same time,
we can capture platform specific errors.
So we're not gonna lose the
platform specific error,
that'll continue to bubble up.
And we have something that
we can be more general about,
and we can actually make decisions upon.
So that's a great idea
that's often a problem.
You write code for a
very specific platform,
you try to run it on another platform,
and then you've got to come
up with another scheme to
deal with the errors on that platform,
because you're dealing with
platform specific errors.
So most of you are dissatisfied,
but this actually handles
the Time to Hello World.
And Time to Hello World is how we
make buying decisions all the time.
Most people reject the original ways that
we've been doing networking
in Asio immediately because
they look at it and it's crazy complex.
They want something easy.
But as soon as I've
shown you something easy,
you wanted timeouts and
how do we handle errors?
And you're thinking about,
&quot;I want this to be asynchronous.&quot;.
You want the complexity.
You know why?
Because you're a bunch of
C++ Programmers, that's why.
You're dissatisfied all the time,
that's just kind of how we live.
(audience laughter)
We want something that looks easy,
the syntax is beautiful, and
we want ultimate control.
That's one of those places
if I had a really deep voice,
ultimate control.
We want it all, right?
And if you spend a little
bit of time thinking about
things that you might
have to do occasionally,
sometimes you just need to
slap together a routine that
goes out, grabs something, pulls it in,
and does some work on it.
And I don't know about you,
but I fall into maybe some
other language, scripting
language of some sort, to do that.
And we're getting to the place where
we don't have to necessarily do that.
This would satisfy,
potentially, some of my needs.
So we have needs that are this simple,
the Time to Hello World needs.
But most of us, not so much.
I've got three daughters,
they're all adults now,
but when they were younger,
they would make me
coffee, and it was great.
In order to live in our
house, at the age of 5,
you have to work the cappuccino machine.
Otherwise, you have to move next door.
So daughter number one
would come in and I'd say,
&quot;Hey, please make me a coffee.&quot;.
She said, &quot;Sure, dad.&quot;,
and off she would go to
the cappuccino machine.
Hear frothing of milk, all these things,
but meanwhile, I'm still
working, I am writing code,
I'm doing things.
So time passes and she
comes back into the office,
&quot;Here's your coffee, dad.&quot;.
Thanks.
That was an asynchronous
transaction, that was awesome.
Alright, daughter number three.
So daughter number three
was very enthusiastic about
making coffee, but we
would both get up and
walk to the machine, and
as she would make coffee,
I would supervise.
Which means I wasn't
allowed to touch anything,
I probably shouldn't say anything either,
but you know, I'm going
to make sure things are
going the wat they want to,
something's not ending
up in my coffee that
I didn't want in my coffee,
things of that sort.
And then we would both
walk back to my office,
and she would deliver the coffee to me,
and I would say, &quot;Thank you.&quot;.
That is a synchronous transaction.
And we do those all day
long, it just makes us sad.
So when should we do synchronous I/O?
What are some of the times that
we might want to do synchronous I/O?
- [Man] One off tasks.
On off tasks, is that what I heard?
- [Man] One off tasks.
One off tasks, okay.
- [Man] Yeah, the whole whirl,
I need to get something and done.
Yeah, need to get something, and I'm done.
One off tasks, that's one.
Yeah.
- [Man] You don't have anything
useful to do but consult.
Yeah, you don't have
anything else useful to do,
you might as well just sit there, right?
Absolutely.
There's no reason to add
a lot of complexity in
your system to add
complexity in your system.
- [Man] (talking distantly)
Okay, yeah, there's already
a background task going.
In essence, you hear
some other task running.
- [Man] You need to
synchronize for other projects.
Okay, potentially need to
synchronize for some other things.
so, often simple programs, the
defaults at your system are
appropriate for what
you're trying to solve,
those are two goods times.
The other is fine grained control,
and sometimes it's the one that we forget.
So if you need really find
grained control of something and
you have a keen understanding of
how the underlying thing works,
blocking synchronized
I/O is going to have a
lower latency and be faster for that
one thing than anything else.
And that might be what you need the
answer for, that might be.
The synchronous example
with my daughter is
actually not a bad example,
I needed some fine control
over that process, right?
I didn't want the coffee to
end up wrong along the way,
there needed to be maybe some fine tuning.
So synchronous example, now
we're barely fitting on a slide,
things are getting a little bigger here.
So let's step through this,
what does it look like?
At the top, we have this io_context,
and we're gonna talk more
about the io_context later.
But we're gonna create an io_context.
If you have used Asio before,
this is very similar to an I/O service.
And so we create an io_context,
we pass that off to each of the things,
each of the objects that
we're creating along the way,
and it uses different aspects of that.
But in this particular case,
we're gonna pass that to
a socket as we're creating a socket,
we're going to pass it to this resolver.
We're going to then connect,
we're gonna call it connect,
and then we're gonna send some data out.
And after that, we're going
to read_until some condition.
The last one was the header,
and then we're going to
read the body again, which is
just reading everything else.
This io_context is provided to
each of the networking objects,
and it contains a variety of
different things and policies.
And when we get to synchronous, excuse me,
when we get to asynchronous,
we'll talk a little bit more about it.
Socket, notice socket's
coming from TCP socket.
TCP socket meets the internet
protocol requirements,
and there's also a UDP.
It is basically a stream
socket socialized on TCP.
There's one that's also for UDP,
which is the basic datagram socket.
The resolver is a basic resolver.
And the purpose of the
resolver is to take a host and
a service name and convert
that into an endpoint,
or the other way around.
To take an endpoint and give
you the associated host and
service name that's with it.
The resolver is the thing that's going to
get us the end point, and the end point is
what we need to connect with.
So once we call connect,
we're going to pass it the
socket that we want to
use for the connection,
and we're going to use the resolver to
resolve the end point for us.
It's going to associate,
then the connect will associate that
socket to the end point.
So in this, where's the error handle?
Yeah, it's an exception.
For the synchronous calls, we
basically have two versions.
We've got one that has
this error code at the end.
The other one is going to throw an
exception that's going to match,
the type is going to match the type of the
system error that would've been generated.
So if you don't provide an
error code, like this signature,
you're going to have an exception thrown.
With this signature, you're
going to provide the error code,
a reference to an error
code will be taken,
and the error code will be
set if there's an error.
You can test afterwards and
if there's not an error,
you can do something.
If you believe in
exceptions or flow control,
you actually might be
at the wrong conference,
but if you do, perhaps reading and
using exceptions is the way to go.
But most likely what you're
going to do is use error codes,
and you're gonna pass that along so that
you can get the error back-out
and make decisions about
what you need to do next.
Yes.
- [Man] (talking distantly)
Yes, so the comment is that
both of these operations,
the resolving and the
connecting can throw errors.
And so I'm guessing the question
is, what's going to happen?
Well the exception will be propagated up,
you're going to get
the system error that's
appropriate for the
exception that was thrown.
- [Man] Where do you
get the exception from?
The exception will match the
type of error that you have.
- [Man] Okay, thank you.
Yeah, different exceptions.
Alright, so writing,
what kind of error handling
do we have for this write?
Exceptions, because we're
not passing in an error code.
So on the right, we are
passing it the socket and
we're passing it a buffer.
I/O is performed through buffers,
all I/O is performed through buffers.
Buffers do not own the data.
The data is not owned by the buffer.
I'll probably say this a lot more.
When we're debugging problems
and helping clients out,
one of the biggest things we are
fixing is this ownership problem.
The wrapper is going to
generate a buffer sequence that
does not own the underlying data,
it simply describes the underlying data.
So please do not let your data source go
out of scope, alright?
Everybody just shake your heads 'Yes'.
&quot;I will not do that,
Michael, I will not.&quot;.
- [Man] (talking distantly).
I have not considered
anything, no. (laughs)
(audience laughter)
Alright, so what kind
of buffers do we have?
There are three types of
buffers in the system.
There's a mutable buffer, constant buffer,
and then dynamic buffers.
To create a buffer,
you're going to use one of
those proxy's or factories to get one out.
And there are a whole variety of them.
So in order to get a mutable buffer,
the signature looks like
the thing at the top,
where you call buffer,
you pass it something,
and out of that, you're
getting mutable buffer.
What kinds of things can you pass?
Everything from a void* with the size to
your container types and what not.
We have then constant
buffers, and as you can see,
there are quite a few more of those,
because string view gets to
play inside of the constant land.
And then we have something
that's a little bit different.
If you're familiar with Asio,
the other two will just
kind of fall in place.
These are a little bit different,
and they are dynamic buffers.
Dynamic buffers will, as they sound,
increase the size of the
buffer based upon your needs.
And so we can create dynamic buffers from
vectors and from strings.
And there are two variations for each.
You'll see there's one
that takes a 'size_t'.
The 'size_t' is the cap,
how large you'll let the
container grow and then it needs to stop.
So if you're into just
letting your networks get
flooded and your programs crashed,
you want to use the one without that.
(audience laughter)
So here's our write.
So we are iterating
through, and we're writing.
By taking an using our buffer to create a
buffer sequence and passing
that off to the network write.
So there's this thing
called the delimited read.
Read until, we're gonna
give it the socket,
and we're going to give
it a buffer to read into.
In this case, we're going
to give it a dynamic buffer,
passing in headers.
So it's going to take the string,
so we can just keep growing
it as large as it needs.
And then we're going to
give it a pattern to find.
And it's going to read_until the
delimiter is contained
inside of the buffer.
That wording is important.
So until that delimiter set is
contained inside the buffer.
Delimiter is a character
or it is a string_view,
those are the things that it can be.
- [Man] (talking distantly).
And the question is, in general,
do you check to see if
you've made it to the end?
- [Man] (talking distantly).
Yeah, the question is,
how would you know if
you've got it if you return?
So if you have not received
the thing you're looking for,
the delimiter sequence, it'll block.
And so the only way it's gonna return is
that there's an error,
and right now we didn't
pass an error code,
so we're going to be
throwing an exception.
How much memory will
be consumed with this?
Yeah, I don't know, right?
We've not capped it.
And if we're concerned about
that, we should do that.
So we've got read here,
and we are going to now
read from the socket,
giving it a dynamic buffer,
and we're also going to
give it this error code.
So how is error being
handled in this situation?
Well we've given it the error code,
so it will fill in the error code with
an error if there is an error.
Otherwise, it'll just return,
we can test E and E will return true.
We just told it to read, and we gave it
a dynamic buffer without
a size associated with it.
So how long should it read for?
- [Man] (talking distantly).
Yeah, maybe into file, network, timeout,
until, in essence, an error.
I mean, there's some
condition that's going to
cause this whole thing to stop,
at which point we could probably look at
our error code and
figure out what that was.
Alright, so it's going to read,
remember this wording here,
it's going to read until the delimiter is
contained inside of the buffer.
It's going to 'read_some' internally.
So if I just 'read_some',
which is going to
give me everything that is
available right there now,
inside the system,
what's inside the buffer.
Let's just think about this
as maybe DNA down below,
and it's DNA into some buffer,
and 'read_some' just goes and
grabs whatever's available in the system.
It's got some bytes, alright.
If you have those bytes and
you have your delimiter sequence,
what else might you have?
- [Man] Some other stuff.
Some other stuff, yeah.
You could be really sad if
you thought reading with
a delimiter just stopped right there,
and then the next buffer started
right after your delimiter.
In fact, that's like one of the
other very large errors that
we find over and over again.
So when we're debugging code,
there are a couple things we look for,
and this is one of them.
It's a 'read_until' being used,
and if so, it's likely
being used incorrectly.
We could replace what's there with,
instead of this delimiter
match that it's going to make,
we can provide to the read
a completion condition.
So we just say, 'read_',
give it the socket,
where to put the information.
And here is something that you can
figure out whether we're
done reading or not.
What this takes is a function
that takes the error code,
and the total amount of bytes read so far,
in the entire operation,
and it's going to return the
number of bytes to read again.
So can anybody think of some
super inefficient algorithm
that we could do this with,
if we could just search for...?
What's that?
- [Man] One at a time.
One at a time, let's do that,
because it's super inefficient.
So we could just replace
our delimiter with
a handler, excuse me, with
a policy that will tell us
whether or not we are done reading.
So it's going to search and it's going to
check to see whether or
not we have an error.
If we don't have an error,
and some other things,
then we're going to return
to one, read one more,
read one more, until we say, &quot;Read zero.&quot;,
at which point the read will be done.
Now we wouldn't want to write this.
We want to be a little smarter.
But this illustrates this
idea of the flexibility that
we have on determining
how much to read in.
Alright, let's talk a little bit about
some asynchronous things.
So at the top, we've been
creating this io_context.
The io_context is an Execution Context.
And the Execution Context
points to an Executor.
The Executor provides the
policies and the rules of
how to execute items that are going to
be in the completion queue.
There are lots of papers on Executors and
a lot of idea about Executors,
and we're not going to
talk about any of those here.
You can read all those papers. (laughs)
We're going to have a very narrow view of
what the Executor is, okay?
The clicker is not clicking,
I'm going to be tethered here.
For us, when we have an
io_context, by default,
you haven't seen that we've
been creating Executors or
associating Executors, by default,
we get the system_executor.
And the system_executor
represents a set of rules in
which function objects are permitted to
be executed all at the same time.
So if there are multiple threads of
execution associated with this thing,
and we have multiple
function objects that are
queued up to be executed,
they could just all execute whenever.
It also say some other things,
some of them are a little
more surprising to you,
maybe at first.
Which is that if you
throw an exception and the
exception is caught within the Executor,
the system_executor will
politely call terminate for you.
So there are some things
that you might want to
find out about your system_executor.
We're gonna post some work,
try to figure out how to
use this thing a little bit.
So I'm going to create an io_context.
Post lets me just take a callable and
post it right on the
Completion Handler queue.
So I'm giving it the
context and some Lambdas,
and I run this, and nothing happens,
it doesn't print anything out at all.
Nothing's there.
What's that?
- [Man] (talking distantly).
I exit it immediately.
Okay, I definitely exit it immediately,
and I didn't actually give it
a thread of any sort at all.
So the io_context needs
a thread, poly_run,
and that thread that you
provide to the io_context is the
thread or threads in this case that
the Completion Handlers will be ran upon.
So you're gonna provide the
threads to the system that
you want to be used for running
your Completion Handlers.
So I do this again,
and actually it still
doesn't print anything out.
Just like runs right to the
end and it's done, it's done.
- [Man] (talking distantly).
- [Man] (talking distantly)
Okay, well I've got to call Run after
there's some work to do, and I call Run,
and then I give it some work to do.
And Gor always asks me,
&quot;Why do you do that?&quot;.
Because if I didn't,
then it would just work,
and you wouldn't notice,
that's one reason.
(audience laughter)
But the other reason is I
find that in our systems,
we set up our threads first,
and then eventually they
get some work to do later.
And that's just the common way that
we set things up for the
way that we're doing work.
So we need to somehow
tell the Executor that
it has work, there's something to do.
It appears like your queues are empty,
you have nothing to do, you
might as well just return.
But the reality is, I
don't want you to do that.
And the way we do that
with the Networking TS is
we use a work guard.
So we can use make_work_guard,
we provided the context, the io_context.
And then the object that comes back,
either the lifetime of that
object or, as you can see,
we call reset when we're done with it,
that will say that it's
not interested anymore.
Or on the destructor, it will call reset.
That will remove the work object out from
keeping the queues alive.
So we need to give it this work queue,
now everything works great.
So we kind of see what the setup here is.
We've got threads that are
going to be calling Run,
those are the threads that
are going to be utilized.
And we probably want to
get a work guard in there
before work occurs so that the
threads do not just return immediately.
So everything inside the
asynchronous system starts
off with initiating function.
We just saw post.
Post is, in essence, initiating function,
but it's not going to
actually do much work.
The initiating function puts us inside of
this operation outstanding state.
Things will sit in there until
resources are available to
do whatever needs to be done.
So as soon as the resource is available,
like maybe we're doing a
network read or a network write,
so the resource is available,
then the operation can be performed and
we will be able to see the side effects.
Once the thing is completed,
then the Completion Handler
is submitted to the Executor.
That Completion Handler
will sit inside of the
operations completed state until the
Completion Handler is called.
The Completion Handler will be
called based upon the type of
Executor that you might have and the
policies that are associated with that.
We just learned the system one the
policy is associated with,
it's happy to run
everything all at one time.
So however many threads are available,
and if there's something inside the queue,
it'll go ahead if there's a
thread available, and run it.
Things that are async_, start with async_,
those are initiator functions.
- [Man] Spelling error.
Thank you, yeah, sure is.
That's stinks.
What if we didn't want that?
So the last couple slides,
we've been doing I/O
output to the console,
and we're doing it al at the
same time on multiple threads.
We probably have mish-mash
whatever, we've got a mess.
How many of you have written
from multiple threads to
the same TCP socket at
the same time before?
Let's say by mistake, by mistake,
you didn't know about it.
Or on purpose, because
you thought I'd be cool?
Still fewer hands.
It's a mess, don't do that, alright?
It just makes a mess.
So if you have a situation where
you want to control differently,
you can provide a different
Executor to do that,
and strand is one of those.
Strand's policy is one in
which only one item will
run at a time, that's
associated with the strand.
So we've added the strand,
and now what we do is instead of passing,
before we were passing
the context to post,
we are gonna pass the strand to post.
Strand is an Executor,
it's going to now have
the policy of how this will be ran.
And even though we have two threads,
both of the items that we've
posted are associated with
the same strand, so only
one will be ran at a time.
Unlike, if you're familiar with Asio,
unlike Asio, there's another
guarantee that's in the
Networking TS, which is
that ordering is maintained.
So the order in which
they write up the strand,
post it to the strand or
give it to the strand,
is the order that they will run.
So far, this last item has
been, this CompletionToken,
we've seen Callables there.
But they can actually be a little fancier,
we can put other things in there.
For example, we could put in 'use_future'.
And now what happens is, this says,
&quot;Return a future, please.&quot;.
And the future now can be used in order to
determine when the operation is complete.
What is the type of the future?
Well it's not going to be the error,
it's going to be whatever,
other than the error,
would normally be passed
to the Completion Handler.
So Completion Handlers an error and
something else, something useful.
It's the useful part.
The error will still be received as an
exception that's thrown,
when you do the get.
Alright, so I am not gonna
go into great detail on this,
because we're gonna run out
of time, and it is a mess.
And I'll explain later why
I think actually trying to
do a lot of asynchronous things using
futures is a mess.
So you want to probably know,
does this work with goroutines?
You probably tried to say go-routines,
but this is goroutines.
Goroutines and stateful coroutines.
Does this work with the same thing?
Well actually yeah, it does.
So this one actually is the
goroutine version because Gor wrote it.
So we can see that we can use
async_send, async_read_until,
async_read_until again, with co_await.
And now it looks a lot like what our
synchronous code looked like,
but it's happening asynchronously now.
So how does this work?
Well these are not the
Networking TS versions of these calls.
Gor also wrote some wrappers
to help along with that.
These wrappers are reusable,
so I can just continue to
reuse these wrappers with
async_connect or send, or read_until.
And if you want to find out
a whole lot more about this,
Gor will be speaking about
naked coroutines live,
with networking, tomorrow at this time.
Alright.
So another way that we could do this is
we could use Completion
Handlers and callbacks.
So we're set this up very quickly.
We've got the same types of things that
we've been setting up before,
that same boiler plate.
And we're going to have
this home_page_getter,
we're gonna pass it the io_context,
we're gonna do get_page, passing it what
we want to get the page from.
And it returned a future,
we can then get on the
future the result back-out.
So what does it look like underneath?
In this case, this is a class,
and this class has a few
things, has this get_page.
But privately it has a connect method,
read_header, read_body, and send_request.
It has a few other things.
A reference to the io_context,
it's gonna contain the
socket, the header, the body,
and then the promise that
was being fulfilled later.
So remember we have this
initiating function that's
going to get everything going.
And after the asynchronous
activity completes,
the Completion Handler's going to sit on
the operation, is going to be in
this operation completed state,
and then it's going to get called.
And when that gets called,
the normal thing to do,
using this model, is then to initiate the
next asynchronous item
that you want to do.
So during the completion of the
previous asynchronous activity,
start off the next asynchronous activity.
I finished reading the header,
and now I want to start reading the body.
When do I start reading the body?
In the Completion Handler
of reading the header.
So we're gonna be able to
chain these things together.
Get_page just sets some things up,
calls connect, returns
the future out of promise.
And what's connect going to do?
Connect is going to use a
resolver, call async_connect,
and pass a Lambda that
is going to take the
error code and the end point.
It's going to check the error code,
and if there isn't an error,
it's going to send a request,
passing the host, and then
start the read of the header.
Otherwise, it's going to set
the reception for what the
error was that it received on the promise.
The completion, the Lambda
is the Completion Handler.
The Lambda will get ran after
the side effect has occurred.
So what does send_request look like?
Send_request, we've got the request,
we're gonna call async_send.
I don't really care about
the Completion Handler,
because I don't care what it does.
Pause another second.
What's that?
- [Man] (talking distantly).
Bam, good job.
Thar is a bug.
Alright, what is the bug?
- [Man] (talking distantly).
Okay, what if it doesn't send anything?
What else is a problem?
- [Man] (talking distantly).
Yeah, yeah, the buffer I
a big problem, isn't it?
Yep.
So here I am passing this request,
which is a temporary string, to buffer.
And so I'm gonna get a
buffer sequence of a temp,
I'm gonna be pointing some
temporary something or another,
which is gonna go out of scope,
probably in the middle of the send.
And life is gonna be bad.
It's gonna work maybe occasionally,
it's probably going to work at my desk.
But it will not work,
like occasionally for QA,
but you know how that works.
So this is that typical bug.
People get very excited about this,
they pass it off to buffer, and bam.
So the read_until, for the
header, we're gonna start that
off giving it the same tokens that
we were looking for
before, the same delimiter.
And if we don't have an error,
we're going to go ahead and read the body.
If we do have an error, we're
just going to end there and
return the exception.
So this is that chaining
thing again, right?
And if we look at
read_body, what does it do?
It is satisfying the promise,
one way or the other,
it's gonna satisfy the promise.
So either it's done and it
got the answer, or it's not.
What done chaining look like?
Is that familiar to you?
- [Man] (talking distantly).
It' callback hell, that's
what it looked like to him.
Anything else?
Yeah.
- [Man] (talking distantly).
Thank you very much, Gor.
(laughter)
It looks like beautiful state machines.
I wish I could say it like Gor says it.
Beautiful state machines, yes. (laughs)
Okay, let's talk about
a couple more things.
Let's talk about a chat server real fast.
We're have this client connect
in to some generic server,
it's got a Thar Asio in there,
that's because it works or either.
And the generic server's
going to hand off the
client to the chat_handler.
And another client's
going to connect in and
it's going to now hand that
off to another handler.
The handlers are
representing the state on the
server side when talking
to this particular client.
They're gonna maintain the connections.
So the steps are, it comes in,
it connects to this listening,
the listening gets handed
off to the Client Handler and
it continues to do the communication.
Who owns the Client Handler,
who owns this thing?
- [Man] I'm guessing it would own itself.
You are right.
Pablo suggests that
this should own itself,
it should own itself, that's right.
So in a lot of places that we
see code, there's a manager of
some sort that owns it,
and the manager occasionally has
some cleanup call it goes
through and it checks to
see what isn't active
anymore, and it reaps those.
Out of college, my vert
first manager out of college,
we're sitting in the design review,
and he got very upset and says,
&quot;Managers are not
supposed to do anything.&quot;.
I was thinking, &quot;Oh
yeah, yeah, that's right.
&quot;Managers are not supposed
to do anything at all.&quot;.
So don't make manager classes.
The Client Handler should own itself.
So now we need a
mechanism to make sure the
Client Handler owns itself.
What keeps that Client Handler alive?
Well it stays alive as long
as it has work to do, right?
As long as it has something
to do, something to
read or something to write, it
might as well hang out still.
So thinking about our chaining
of Completion Handlers,
we are going to queue this async_read,
which is our initiating.
And on completion, we're
gonna call Read Done.
And what is Read Done do?
Well, if there weren't any errors,
it's going to start off
the async read again.
And if there were errors, it's going to
not do anything anymore,
it's just going to fall out.
The canonical way to set
this up is to inherit from
shared_from_this, enable_shared_from_this.
And what that's going to allow
us to is take a reference,
a counted references,
to ourself and bind that
up into the Completion
Handler that we give off.
So when we give these things off and
they're hanging out
inside of the Executor,
or they're waiting to be
given to the Executor,
that objects lifetime will be maintained.
So async_read_until, and
now when I form the Lambda,
I'm going to take shared_from_this,
set that equal to me,
so now it's bound up in
the there, it's captured.
And that will extend my life.
The lifetime of this object
will continue as long as
this asynchronous operation is queued up.
And when that operation completes,
and the Completion Handler
is given to the Executor,
it still has a reference.
And when the Completion Handler is called,
it still has the reference.
And when the Completion Handler is called,
then it should probably
queue up another task,
or it should do nothing, right?
It's done, it's reached the
end of what it needs to do.
And that separates the
read from the write.
Reads can make decisions on their own,
and writes can make
decisions on their own,
and when there's nothing else to do,
then the whole thing can
come back down again,
it can destroy itself.
- [Man] (talking distantly).
It controls the ownership of itself.
- [Man] (talking distantly).
We'll get philosophical
here, I'll be Tony.
Do any of you own yourself? (laughs)
(audience laughter)
Alright, let's actually
kind of move on here.
Actually, let me do this.
It's this thing that we
want to look at again,
the initiating functions
when we're inside of
our Completion Handler,
when it's being called,
that's when we want to start
the next initiating function.
And the Completion Handler for that,
we want actually bound up
into that reference to the
shared_from_this, in order
to extend the lifetime.
Okay.
When we're assembling these things and
putting them together, we want to
start decoupling our services.
So we want to decouple the fact that
we are processing the
communication, excuse me,
that we're receiving
the communication from
the processing of the communication.
And so there are a variety
of different ways to
do these layered designs,
and the way to decouple them.
We can use the Executor
as a processing Executor,
we can give it work to do.
We can start combining things with
Boost.MSM or SML or Spirit.
The first two are state machine libraries,
because you know, at the end of the day,
what we're trying to do is
actually create a state machine.
What I'm trying to do is
cretae a state machine.
I'm trying to convince you
that you want to do the same.
So you want to choose your
abstractions based on the
complexity of what you're solving.
Sometimes actually, the
stream abstraction that
I had at the beginning is what I need to
solve the problem that's at hand.
I don't need anything else,
that will do exactly what I need.
I shouldn't make things more complicated.
Sometimes that's the right answer.
Sometimes synchronized
communication is the right answer,
so I just do that.
Sometimes it needs to be asynchronous,
but when it's asynchronous,
I need to choose what fits there, too.
Because there a whole variety of
different ways to solve this.
So this is my problem.
Most of the communication
problems I see that
people throw up are
simple, very easy to solve,
and chaining futures is easy to do.
This is a very easy
communication protocol that
had to be implemented.
It's actually, if you saw
Taka's talk last night,
this is ..., this is what
it needs to go through.
It runs on little itty
bitty embedded thingies.
So here's the hierarchal
state machine for it,
and the one on the right hand
side, we haven't blown up.
This is the blow up of that.
Communication protocols are much more
complicated than sometimes
we think they are,
and you start falling into a trap when you
chain things together that
you're not sure how to get out.
But that's not actually the real problem.
So let's see what the real problem is.
By the way, this is that state machine on
the right hand side.
If we use a state machine
library to write this,
we can just write tables.
The tables are super easy to understand.
Not only that, but we
could figure out whether
they're correct or not,
because we know what's
missing when we look at it.
We can actually verify that
we handle all the states with
all the possible events
coming in that are required.
And when an event comes in
that we don't know what to
do about, we know how to handle
that, because we're gonna
use a hierarchal state
machine type implementation.
So we can handle our errors at layers that
are appropriate for the errors.
So don't try to destroy
the state machine that
already exists, just give in,
and use a state machine library.
So let's think about futures for a moment.
What is the concept of a future?
A future is a
synchronization thing, right?
It's an object that helps me synchronize.
And on the left hand side, at the top,
what you think about at the top
is the part that's initiating
the asynchronous activity,
it's going to start
something else happening.
And on the right hand side,
as it runs on through,
there's a shared state, and
we call the right hand side,
the shared state of the
right hand side, the promise.
But that's the asynchronous provider.
On the left hand side, somewhere else,
something wants the result
out of that shared state,
it wants the result from
that asynchronous operation.
And we call that the future.
But the way the futures
work is that at some point,
we need to get the answer back out.
And to me, this is why it breaks down.
I need the answer back out somewhere.
Where in the last few examples have
we needed the answer back out?
We don't need the answer back out because
it's an event-driven system.
I don't have a place
where I'm gonna call get.
If I have a place where
I'm gonna call get,
it's on some thread
that's sitting somewhere,
going to call get.
So for us, most of our networking and
our asynchronous communications
that we deal with,
they want to be event-driven.
So the question is, how about coroutines?
Does it fall apart with coroutines?
Because when I look at this,
it looks like it' starting to fall apart.
I've got this read_until,
and it looks like
it's just taken this whole concept of
asynchronous event-driven stuff over here,
and brought it on in and just made it
serialized with some sugar.
So will this do that?
I have this FSM process the event and
passing in the parsed message into there.
What we haven't seen yet is,
how is the coroutine running?
What is running the coroutine?
And in essence, the coroutine is
running on the same run thread.
With the future, I'm trying
to synchronize things from
the run thread, from the io_context,
from the asynchronous activity that's
going on, into something else.
And that's the synchronization point.
With the coroutine,
it's all running still
inside of that run thread.
And as a result, it looks serialized,
it looks like this nice procedural code,
but it's still kind of, in essence,
doing my event handling for me.
So I can have this small
little event loop in which
I'm processing events, and
I'm passing them off to
the state machine that's going to
actually figure out what to do with it.
And if you still love Completion Handlers,
you can still do that,
in which at the end of reading,
you go ahead and pass of the
event to the state machine.
So this also handles of course, obviously,
it handles the event-based
type processing.
So we've got two mechanisms that will
handle that very smoothly,
and then we've got one that
doesn't necessarily fit real well.
But it fits really well when
we need other types of processing.
So if you need event-based processing,
don't try to use futures and promises.
Do something else.
And if you can use futures and promises,
those are a lot easier to use.
For synchronization devices, use that.
If you can use the synchronous, use that.
And if you do streams,
everything fits on one slide,
and you're like a hero.
And you make fun os the
Rust people. (laughs)
(audience laughter)
Alright, some takeaways.
The Networking TS implementation
exists, go use it.
It exists out of the base of Asio,
which has been around forever.
And so go grab it, play with
it, see how it works for you.
It has a flexible mode
for handling execution.
Supports events, coroutines,
futures, other mechanisms.
That CompletionToken
that gets passed can be
customized for all kinds
of very interesting ways to
handle your completion routines.
If you know Asio, you almost
know the Networking TS.
Alright, it's time for a
few questions it looks like.
- [Man] (talking distantly).
I don't know, what is a what?
Another typing error?
You making fun of my typing errors again?
- [Man] Yeah.
Yeah, stop making fun of my typing errors.
(laughter)
- [Man] Just wanted to make sure it
wasn't a concept I wasn't aware of.
I thought heckling was going
to be coming from over there.
(laughter)
Any other questions, other
than spelling errors?
(audience applause)
- [Man] In one of the
slides, I think you had,
I think you may have gone back too fast,
it was a while back, so done
bother trying to find it.
You had the issue where the
buffer was a local variable and
it was gonna go out of scope.
I don't remember if that
example also had a Lambda.
Do you want to talk a little bit about
the scope of the Lambda?
Lambda's also a local variable, isn't it?
Okay, yeah.
It also had a Lambda.
The Lambda's going to be moved into it.
- [Man] I see, okay,
it's going to be moved.
Yeah.
Taka?
- [Taka] You mentioned,
don't create a ....,
in the chat example.
In the chat example?
Yeah, the chat_handler
is owned, then service.
But your example is chat means ... chat.
Finally, the receiving message should
read the other chat_handlers.
So finally we need
something management ...
What do you think?
Yeah, yeah.
So the question basically is,
what do you do when you have
to have management, right?
- [Man] Yeah.
So what we do is in this generic thing,
where we've got a generic listener that's
handing things off to the chat_handlers,
the generic listener has
a callback handler into
the system somewhere.
So the system generates one of these,
it passes in as a template parameter,
what the Client Handlers look like,
and it calls like Set Connection Handler.
And every time a connection comes in,
it actually gets passed
a shared pointer to
the Connection Handler.
I'm sorry, to the Client Handler,
and it can store off weak pointers,
and that's how we handle the problem when
we need to still have management
back into the handlers.
We keep a weak pointer to it.
- [Man] Okay, thank you.
- [Man] In one of the slides
you had send_request that
returns right away, and
you didn't care about
the Completion Handler,
and then the read_header.
That made me itch a little bit,
because usually I would
do a send_request in that
Completion Handler, then I
would start reading the header.
Talk a little bit about why is it okay to
do it the way you showed it.
Okay.
The reason I did it
this way is just because
it's trying to mimic what
the previous examples did,
where it's sending out the HTTP request,
and now it's waiting for the responses.
So the request, I don't really
care when it goes out and
when it finishes, because
the responses back are
going to come back on the
read_header and the read_body.
Normally, you would
set this up so that the
Completion Handler would
start off the next read,
or it would look to see if
it has some new message that
it has to take care of.
I'm sorry, the next write,
or check to see if it's a new message.
That's what the normal...
- [Man] I don't think
it would be an error,
but you can't really
handle errors, for example,
when you send a request.
No, no, you're right,
not handling errors at
all, that's correct.
- [Man] Hi, do you still need
to feed a bunch of threads to
the context if you are going
to use futures and coroutines?
Do you need a bunch of?
- [Man] To pass a bunch
of threads to the context.
So the io_context needs to have at
least one run called on it,
because the run methods that are
called are the threads that are going to
handle the completion queue.
So there has to be at least one,
and one is probably fine.
Do you have to have them for coroutines,
was that the other question?
- [Man] Yes.
Yeah, because the coroutine,
that's actually the thread that
the coroutine is working on.
So even though we've got
this magic here in the
top one where we have co_awaits,
and there's some other part, right?
These two parts pinging back and forth,
that's all running through the run thread.
- [Man] But in this case,
only one thread will
runs the continuations?
That's correct, only one thread is
running the continuations.
- [Man] Okay, thank you.
Oh, and my session's over just in time.
No, sorry David, go ahead. (laughs)
This is the question I
don't want, I'm sure.
- [David] Yes, yes.
I don't think you'll mind.
So this read_until thing,
it can add extra stuff to your buffer.
Yeah.
- [David] And then the
alternative you showed was
reading one byte at a time,
and that has its own problems.
So what is the solution?
Okay.
So if read_until is unsatisfying
and reading one byte at
a time is unsatisfying,
what should you do?
Great question.
We use read_some for everything
and build on top of that,
because that reads
whatever is available that
the OS has already read in,
and we just get that and
start reassembling the
packets the way we need to.
So it returns immediately, or
it blocks if there's no data.
And if there's no data,
it blocks until the first full buffer,
usually it's a packet
size from TCP, is ready.
- [David] So you just have to do
a whole bunch of extra work to..?
Yeah, we've got to build
our protocols on top,
and we've found that that's
the fastest way to build,
or the fastest running protocol.
That's it.
- [David] Thanks.
(audience applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>