<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Alfred Bratterud “Delegate this! Designing with delegates in modern C++” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Alfred Bratterud “Delegate this! Designing with delegates in modern C++” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Alfred Bratterud “Delegate this! Designing with delegates in modern C++”</b></h2><h5 class="post__date">2017-10-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QMb4RRFrY-o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Good to see you, everybody.
Today, I'm gonna talk about delegates.
And this is intermediate,
no, it's a beginners
to intermediate-level talk.
I have no idea if it actually is,
but I'm not going to
cover like the advanced
topics of actually implementing delegates.
I want to focus on how they are used,
basic usage, and how
we use them extensively
inside of IncludeOS
So, my name is Alfred.
I'm the CTO of IncludeOS.
It's a library operating system.
Most of you should know that already,
if you don't you can look
at this black screen, right here.
(audience laughs)
It's flickering a little,
hopefully that's not going
to be a recurring problem.
Okay, so I wasn't, I actually got
into this whole idea of using delegates
when I was starting to implement
or design IncludeOS, and
in particular the IP stack,
back in, roughly, 2014.
So what I was looking for was a way
to achieve complete
isolation between layers,
because I'm making IP stack,
and the RFCs are pretty strict about that.
You're supposed to isolate.
I talked yesterday about layers.
I don't think you can have
hugely long, horizontal,
clean layers, but you can
definitely have isolation
between the protocol
handlers inside the IP stack
and that's a good way,
I think, to design it.
So I was trying to fulfill
all these requirements,
how can I pass a network
packet from Ethernet
up to IP4.
I don't want any of those
classes to know about each other,
because that makes it more flexible.
I also need to have state in both.
So for example, I might
have four or five IP stacks.
So they can't be just a,
I just can't keep global states.
Ideally I also want to be able
to rewire during run time.
And the main reason for this is
that if you make
something very hard-coded,
you've seen these object hierarchies,
I think it's really hard to get right.
And I was absolutely
certain I wasn't going
to get it right the first time,
so I thought, let me
just implement one class
at a time, make a flexible system
for wiring them together later,
and I can actually make different versions
of the IP stacks with different
types of wiring, et cetera.
And also the hard
requirement, zero overhead.
So, possibilities we could use,
or we could consider function pointers.
You could pass the state around,
I think that's what Linux
would do, or any C program.
Just have normal functions
and then pass the state
as probably the first
parameter in a struct,
or something like that.
But I don't think it's a very nice
object-oriented solution.
I was considering std::function.
I was looking at that,
and getting to know that,
this is back when I was new to new C++,
and I couldn't find way
to make them point to member functions.
'Cause that would be,
I had this clear idea,
if I could just point
to a member function,
then the state could
actually be maintained
inside of that instance of that object.
Polymorphism, that's overhead.
Might not be that much overhead,
depending on how you design it.
I'm not complete against polymorphism.
We use it some places,
but we try to avoid it in
really, really hot paths.
And networking, passing
packets up and down,
that's some of the hottest paths we have
inside of IncludeOS.
So Google, right?
That's what we do.
I found this article,
it's called Member Function Pointers
and the Fastest Possible C++ Delegates.
So this is an old article, it's from 2005.
It was written by Dan
Clugston, Don Clugston.
And it's really impressive
amount of detail in there.
And he really explains what is
actually a member function pointer.
Anybody familiar with that?
Can anybody just right off the bat,
write a pointer to a member function?
Okay.
You guys are, you guys are good at that.
The syntax isn't, it's
not perfectly intuitive
but it's good, this is a good introduction
to how you can deal with them.
Before C++, this made before C++ 11,
so lots of macros in there.
That's not an optimal solution,
so I kept looking.
One guy made something that
was more standards compliant.
Not using macros, but it
had severe limitations,
as far as I could see, you'd have
to make a separate version of the delegate
for each of the function signatures.
This was before the variadic
templates, et cetera.
So, but it was something,
and then Google a little bit more,
and then you finally found,
I finally found this,
which is Impossibly Fast
Delegates in C++ 11.
You scroll down on that page.
It's a nice read, people
are super-excited,
and they're saying it's
faster than std::function,
and it's, the speed and
performance is actually identical
to calling via a pure function pointer.
So no instantiation of an object,
not calling via via, it's as fast
as calling via a single
C function pointer.
I did some measurements myself,
and I found that to be the case as well.
Difference isn't huge, but I noticed
we had identical performance
to calling via a normal function pointer.
So, copy, paste, right?
So that was the first
delegate implementation
for IncludeOS.
I said last year, as well,
I owe the person a beer
who implemented this.
This was really a good
starting point for us.
That person never made contact.
If you know the person,
or if you are that person,
please let me know,
and we will buy you a drink.
So let's look at how we use them,
and start with the basics.
So this is a very trivial example.
I'll get to more interesting
examples in a while.
We have a nice
std::function-like signature.
So we're just saying delegate.
Okay, so that's not
optimal for the example.
I'm saying decltype to just get the type
of function one, but
you could just specify
any function signature you would like.
Exactly like you would std::function.
And then it's compatible,
mostly, with std::function,
so in this case,
I'm actually assigning the delegate
to a std::function,
which I'm again assigning
to point to function one.
This is a trivial example.
I'm just pointing to normal functions now,
just to get started.
And it has a call operator,
so you can call it,
just like you would
call a normal function.
If you call it when it's initialized,
it's much nicer than a function pointer.
It's actually going to check for nullptr
and it's going to throw an
exception, so that's nice.
And the output from here, you can see
first we're calling function one twice,
and then we're calling function two twice.
So we actually assign the
delegates and the std::functions
to two different functions,
and we could change
where they were pointing
during run time.
So that's kind of promising
for the modularity
and the flexibility
that we're looking for.
Lambdas, there are no surprises here.
Or not really, I'm saying auto func
and then I'm creating a lambda here,
in the case of the delegate,
I'm actually specifying in
the signature of the lambda,
and then I'm specifying
the body of the lambda,
and I'm calling both of
these and the results are
exactly as you would expect.
Some slight things you might notice,
if you say auto on this function,
it turns out you can't reassign
because it defaults to const with delegate
obviously, you can't use auto because auto
would resolve to, and if you say auto
and the std::function
signature, that would result
to a std::function and not
to a delegate, obviously.
Are you with me so far?
This is okay?
This is completely trivial, right?
So I'm keeping my promise of making
this a beginner-level talk so far.
So let's do the more interesting part
and actually add delegate
stuff to member functions.
And for this example, I'm making a neuron.
You guys are familiar with neurons?
They are the tiny little
cells inside of your brain.
So if you look at that
little tree structure there,
in the center, that's
the center of the cell,
the cell body, it will
receive electronic pulses
at the bottom, I think, and
it will build up a charge
inside the cell body.
Once it reaches a certain threshold,
it will actually fire it
to all of these terminals
up the tree there.
So that's essentially how
your brain is wired, right?
So we'll make a tiny, tiny
little brain for this example.
So we can look at the implementation.
Now I'm specifying the type of a delegate.
So I'm saying that the delegate I want
for actually having one neuron fire
to the next neuron, it's
actually just a function
that doesn't return anything,
but it takes an integer as a parameter
where the integer represents the charge
I want to transmit.
And then I have the
actual delegate member.
This is a public member,
I don't know if I'm breaking
any core guidelines there,
but for the sake of
example, it fits nicely
on the screen when you don't
actually just have a setter for it.
But the point is you want to be able
to wire the neuron from the outside.
So I want to be able to
say this neuron connects
to that neuron, and that neuron connects
to that neuron,
and it's around build-up
kind of a structure
in that way.
So this is the algorithm
for receiving an impulse.
I'm incrementing my charge
and then, if the charge, the total charge
from my neuron now is above my threshold
or if above and or equal to my threshold
and if I actually have a
delegate to the next neuron,
then I'll actually just call that delegate
and I will send my charge.
And I'm actually, I'm removing
some of the charge here
because you can imagine
it costs a little bit
to, I can't imagine in your brain,
every time a neuron
fires that it just keeps,
that all the electric
charge just stays there.
I'm guessing there has to
be some loss along the way.
So, is it okay?
It's a simple program.
So let's make a little brain.
So and we instantiate three neurons,
neuron one, neuron two, neuron three,
of course these are very simplified.
A normal neuron would
have hundreds or thousands
of connections, but
these ones have only one.
So they're very simple neurons.
So we instantiate,
and this is how you wire them together.
And this is actually how
you assign a delegate
to point to a member function.
So I'm saying that neuron
one has a fire target.
I'm assigning the fire
target of neuron one
to neuron two.
So I'm specifying first the instance
I want to point to, and then the address
of the member function.
Okay?
And the same goes for neuron two.
I'm assigning neuron two to
fire towards neuron three.
And then I have a small little for-loop,
where I'm firing pulses
but I'm only firing pulses
at the first neuron.
Okay?
So the idea is that I
want the firing to happen
into neuron one then I want
it to build up a charge
after a couple of firings, and then,
it's supposed to fire to the next neuron,
and that's supposed to build up a charge,
and that, and then that is
going to build up a charge
and fire into the last one, et cetera.
The result is like this.
So it's as expected the first pulse.
The first neuron reaches the charge of 50,
that's half its capacity.
So nothing more happens.
Pulse one happens, the second pulse,
and then, and one reaches
the threshold and it fires.
And now n2 is charged up
to 50 and so on, and so on.
And at the very end you can see
that the third neuron finally
also reaches its capacity,
but it doesn't have anywhere to go,
so I guess it just keeps
its electric charge
until it blows up or not.
Is it okay so far?
So we're making a tiny
little neuron network
out of delegates, wiring neurons
together using delegates.
So rewiring these during run time,
that's interesting and that
is a requirement I had.
I thought I was kind of a,
I thought I could see use cases for that
and we're going to look
at an actual use case
in IncludeOS later.
So what I've done now
is added another class.
So I think it's, one of
the things I like most
about these delegates
is that it doesn't know
about what kind of what type
of class it's pointing to.
It only knows about the signature.
Have you noticed?
I never told the neuron that it was
actually having a
delegate that was supposed
to point to another neuron.
It was just a delegating
to a function signature
that had void return and
integer parameter value.
Okay?
So that's really cool, you can wire these
any way you like as long as
the function signature matches.
And in terms of information hiding,
I think that's really great.
It means you can make really,
really isolated components,
all we have to do is, you have to know
about the signature on the
one they're talking to.
So this is a completely new class
but the function signature is the same.
And I also named it differently,
just to make it extra clear.
So now we have the initial wiring.
It's the same as before.
A neuron one fires to
neuron two, neuron two fires
to neuron three, but
then now, neuron three,
it actually fires to this ground,
or the terminator, the bucket at the end
where you can actually take
all the electric charge.
And more interesting is that
we are doing this algorithm
and we are sending the initial pulses,
and we're sending it
only to the first neuron.
And it builds up, and
it spreads throughout
that neural network.
But then, here, we actually do a rewiring
and we haven't restarted our program.
So the programming is still running
but now I'm saying
that neuron three should instead point
to a completely new neuron
that came out of nowhere.
And oh, you brain was so lucky today,
it actually suddenly got a new neuron
and now neuron three, it
actually connects to there.
Okay?
But then neuron four now
connects to the terminal instead.
And the results are exactly like before.
So it's a bit hard to see probably,
I think we are here
just seeing that at the,
probably don't we don't
get all the results
but at the end, at least, you can see
that I woken up neuron number four.
And neuron number four wasn't
there in the first iteration.
Okay?
So I'm still just doing all
the firing towards neuron one,
and neuron four suddenly was becoming part
of this neural network during run time.
I think that's an interesting observation.
So they're completely anonymous,
for-free, flexible object coupling.
I guess that's one thing you can use
to describe it.
Could we do this with std::function?
Some people pointed out std::bind.
Std::bind has a way of pointing
or you can,
you can create,
what are they?
Are they variables, are they functions?
You can create a function auto f3 here,
and you can bind it to the
member function of a class.
But the interesting thing,
because I said what is it?
And it's actually interesting
that the return value
of a bind, this, it says
that it's a function object
of unspecified type T.
So that means you can't
assign it to a std::function.
So there might be some tricks there,
but I didn't find any.
I tried to assign it to a std::function,
and I wasn't able to.
So, and also if you look at the signature,
it's not nearly as nice, in my opinion.
I have, it's nice for other use cases.
I'm able, for example,
to fixate the parameters,
or fixate some of the parameters
while leaving other parameters flexible.
So in this case, I'm
fixating the parameter 95,
but I'm using place
holder, underscore one,
for the last parameter,
and then I'm calling it
with only one parameter.
So I fixated the first parameter
and the last I kept flexible.
So there might be some
black magic or some tricks
you could use to actually
use bind in this context,
I haven't found it.
These Legos are smelling an opportunity.
They want human brains now
because I showed you now that
I can make a neural network
so maybe we should try
to make a human brain.
Yeah, it's hard to think
with only four neurons.
So we could try to do that.
I was actually considering
doing that for this talk,
but it turns out you
need 86 billion neurons
to simulate a human
brain, and it needs yeah,
I don't even know how
to mention that number.
It's 0.15 quadrillion connections,
and that averages to 1500
connections per neuron.
That means if each of these
neurons needed 16 bytes each,
you would need 2.4 petabytes
to store all of them.
If you had 16 gigabyte
computers, you would need
approximately 2.4 petabytes of memory
and you would need
150,000 of these computers
to store those neurons.
That would amount to
approximately nine billion watts
per machine, but the human
brain uses only 20 watts.
;I just thought that's quite interesting
in case you were considering
making a human brain
with these little techniques
that I have showed you.
So I don't think that's viable.
Yeah, so you're not getting
the brains, unfortunately,
but we can use this to
implement an IP stack,
and that's quite a lot more complex
than the little neurons I've made so far.
That's not as complex as a brain,
but it's certainly a complex machinery.
So let's look at how
we are using delegates
to wire the IP stack inside of IncludeOS.
This is actually just snipped from Github,
you can go and see, it's all open source.
You can see how we're doing it.
And we are first, at the top, I'm actually
just making all these delegates.
So I'm saying that each
of these protocol objects
they have a top and a bottom,
and I'm saying that the network interface,
it receives an ARP packet at the bottom,
and that ARP packet
should then be delegated
to the ARP handler.
When the network interface
sees an IP4 packet,
because it's the investigate,
the Ethernet header,
well then, that type of
packet should be delegated
to the bottom of the IP4 protocol handler.
Similarly, with ICMP,
with UTP and with TCP,
and then we go the other way, back, down,
and we do the same type of wiring,
but then for the return path.
And also for the path where
you actually initiate
packet creation inside
of your own machine.
So one interesting thing,
I thought it was very hard
to decide when I made IncludeOS,
should I do context
switches right off the bat,
should I start with threading,
that is not obvious that you shouldn't,
but I though it's nice
to see how far you can go
with only one thread.
So I'm going to just briefly look
at how these delegates work
in a callback-based system.
So this is the event tree.
This is actually showing
a graphic representation
of the wiring I showed you in code.
So this shows you how all
of these protocol objects are connected
together using delegates.
And remember that I can
rewire these anytime
during run time.
Right?
And it's also very easy
for you if you think that
you want an IP stack
where you don't need HTTP,
or you don't need TCP, or
you don't even need ARP.
You're making a completely
different IP stack.
All you need is to make a
new small wrapper object,
like this INET, internet object
that I showed you code from.
We're actually rewire it, differently
to your own tastes.
So what's interesting here
is that the full event path,
and now I'm trying to address the question
of how callbacks and async works,
as opposed to preemptive multitasking.
And it's interesting here
that when we have used these delegates,
the full path of all these
events is already laid out
in memory from the get-go.
And it's also then possible
to create new paths during run time.
So at the bottom, you'll
see an interrupt happens.
And that's why I find this quite natural
for the computer to
actually work in this way.
Because if you don't have
a classical operating system kernel
that's doing multi processes in between,
we're just putting these
delegates directly on top
of the interrupts.
Okay?
So if you think about what happens
when you receive, when you
click on include&amp;lt;os&amp;gt;.org
and you receive the Web
page, what's happening?
I mean, it's a sequence of events,
it's a cascading sequence of events,
where lots of smaller events
happen in the subsystems
and they are actually building up state
quite like the neurons.
So I've tried to represent this
by having thicker arrow for the delegates
that have been called more often.
And we're now imagining that
I'm receiving a network packet
on the network interface
that's triggering interrupt handler,
and now, of course, we have a choice
whether we want to preempt the driver
and actually say that, to call the driver
and force the driver to
actually fetch our packet,
and handle our packet,
or if we just go up to the event manager,
which is what we are doing by default,
and just set a little flag saying
that this interrupt has happened
so I will need servicing.
And that whenever the
event loop gets back to you
after having handled its previous event,
it will then actually call a delegate.
And you see these blue lines
with the rings at the end?
I'm using those to represent delegates.
So the event manager knows
that the interrupt has happened
and it calls a delegate to the driver
that's subscribed to
that particular event.
The network driver has a delegate pointing
to the Ethernet object,
the Ethernet object
has then a delegate
point to the ARP handler.
So that's the first packet
you're likely to see
if you rebooted your computer.
So the network gateway wanted to send you
this response from Google.
it would ask you what your
MAC address was, et cetera.
But then we traverse the path and we go up
to the IP4 handler, not
much state in there,
but when you get to TCP
there's a buildup of state.
So there will be a first packet
that you will be getting, or depending
on who initiated the connection,
assuming they are
initiating the connection,
you'll first get a send packet.
You'll answer with an act packet,
and then you'll get a send/act packet
and that means several
events are happening
in the subsystem, quite like
with the neuron example.
I'm building up state inside of TCP
and TCP then might start reading,
or actually now in this case,
the HTTP is subscribing to events
from the TCP connection,
and the HTTP handler is
going to start reading
and not until it reaches the point
where it has actually
received a full HTTP request.
This is going to, is it
actually going to trigger
something to happen up in user space.
Yeah, so on this next
slide, I've tried to show
that I've kind of heated
up the protocol objects
that have received the most attention.
So these, some of these objects
have gotten more packets
than others and you can see
that TCP is probably the hottest one,
in terms of state.
It's keeping up a lot of state
and building up a lot of state.
So if you'd look at callbacks as lambdas
that they are nesting
lambdas inside of each other,
then it doesn't feel
like it's very natural
for the computer.
But if you think about
callbacks as delegates
that you are wiring like this,
I think it feels quite natural
for the computer, actually.
When it comes to performance,
this is also another reason
why I think it's a good idea
to start with a callback-based
asynchronous system,
it turns out, or actually
this is from John Bandela,
he did a talk last year
where he did some performance measuring.
Real interesting talk, and
he was comparing callbacks
versus go channel, versus, bruised fibers,
versus std futures.
And the results were quite grim.
Callback were really hard to beat.
But everybody agrees that
callbacks aren't great,
and I'm going to get to an example
where it really, really gets messy.
Even with the delegates.
So our assumption is that
the zero overhead choice,
which is what we're
looking for all the time,
that the zero overhead choice is to start
with a callback-based system.
There is no context switches,
there is nothing else going on,
I'm just following the path laid out
by these delegates and
that's all I'm doing.
But our assumption is that having channels
or something similar built
on top of stackless coroutines
is probably going to be an
equally performant alternative
with probably, in many cases,
a more intuitive way to program.
So it's not always smooth sailing,
and this is an example of that.
I'm going to get dizzy
trying to explain this.
So we're using a lot of delegates here
and what we're trying to
achieve is to transfer file A
from connection B chunkwise,
when both the producer
and the consumer are callback-based async.
Okay, so that's when it gets kind of hard.
When you have all this linear event paths,
it's quite easy.
When you just try to make loops
in that event tree, it
turns out it's much harder.
Might be some new abstractions we can use,
but so far, this is
something we came up with.
So this is, I didn't
implement this a couple
of kernel developers on our team did.
So essentially you
start by making a lambda
and you have a shared
pointer to this lambda
that will function as
sort of a job controller.
And you have to then keep in mind
that everything is async so
you can't preserve anything
with stack, the stack room goes,
it gets destroyed the moment
this function returns.
And actually, so but
you can pass stuff in,
so we're passing in, we're
preserving a weak pointer
to itself, inside of this lambda.
And then we pass, we actually
produce a shared pointer
to ourself, and then we
do a call to the read,
to the file system, we
actually do that call,
which is also async,
so then we have to pass
in the lambda that's
holding a shared pointer
back to this next, this original caller.
And you can see this seems quite messy.
And if we're done already, then we return.
And it continues.
We pass the shared pointer back to next,
to the consumer callback.
When the consumer is done, it calls next.
Asking for the next chunk,
completing the actual loop.
So, I've made some graphics to try
to explain this even better.
But, as you can see, I don't
think this looks linear
or intuitive in terms of programming.
So and all this algorithmic stuff,
it's really hard to keep track.
Where am I?
Which part of the sequence am I in,
because it's absolutely not sequential
when you have lambdas
nested inside lambdas,
and nested lambdas.
And then, of course, the actual starting
of this loop starts at the very bottom.
So fortunately the real
brains are even messier,
so these mouse neurons,
these are actually mouse
neurons that have been generated
automatically into 3D
renditions of real neurons,
and yours are even worse.
And I think that there
are also lots of loops
in the real neural networks
and I'm certain that in
my brain there are lots
of these feedback loops.
And, yeah.
So let's look at how this
algorithm actually works.
If we try to organize it a little
and pull these neurons apart,
and it doesn't really get much nicer,
but what's nice is that,
well essentially this next,
that's the, a lambda that
actually just represents a kind of job.
And it also weak pointers to itself,
and it's able to, from that weak pointer,
produce shared pointers to itself
that it can hand to the producer
and the consumer, respectively.
And what's nice about it, of course,
is that once this whole shifting,
once this whole loop is finished,
it will actually, the reference count will
eventually go back to zero,
so there will be no more
shared pointers pointing
through this next function,
and it will actually disappear.
So it's quite nice, you have
all these events happening,
at some point you trigger,
you need to trigger a transfer event
where you're trying to
hook up a loop between two
of these various events happening,
and this is one way of doing it,
but it's absolutely not intuitive to read.
It took me quite a while
to actually just try
to parse, to parse that
column, because it's,
in that sense, very different
from how our brain works.
So all of that would have been possible
with a very simple for-loop
using sackful fibers
and blocking, or blocking calls.
So we're not arguing.
We're not trying to convince everybody
that a callback-based
solution is the perfect way
to do it, but we think it's
a zero overhead solution,
the simplest and least
complex way to do it.
And what's also interesting is
that with our current solution, we're able
to handle multiple
thousands of connections
concurrently with zero context switches
and using only a single stack.
And that's, I don't know any other way
we could have done that.
So if you don't like the
algorithm I just showed you,
if it puts you off, I
mean, I don't, we don't run
into that at lot.
You'll run into it every once in a while,
where you need this solution
where two async events
that are supposed to be hooked together,
but there's an interesting
library called Elle
and that's built on top of boost::asio.
This is former, formerly
it was a some companies
now acquired by Docker, so they've built a
really nice, very, their aim
was to actually alleviate
or to remove this whole
callback-based programming paradigm,
So they built something
on top of boost::asio
where everything
automatimagically just gets fixed
for you so that you can
write code very sequential.
And they have implemented
boost::asio backend
for IncludeOS, so that
they were able to run
this library on top of IncludeOS.
The integration isn't complete,
but they have been able to do it.
So then you can get stuff like this
where you just say you want
perform a HTTP request,
and you say finalize and
deserialize whenever it's done.
And when you do finalize,
that's a yielding call,
so it's a cooperative multitasking
where it will just yield
and then it's going
to be a stack there that's
going to be switched out
and something else is
going to be switched in
and they have a whole schedule or system
to deal with all that.
And of course, no callbacks.
So even if we build a
callback-based system
using delegates to
connect stuff doesn't mean
we're stuck with that.
You can have stuff like this on top.
So let's take another look.
This is the typical thing you associate
with callbacks, at least it was
for me coming from, for example, node JS,
if you've ever used node
JS and looked at examples,
you see a lot of this.
And that's kind of, you have
level one of lambda here,
and then you have level
two of lambda there
and the problem is that
it looks sequential
in the sense that it is
syntactically sequential,
but obviously during run time,
this is not the sequence,
necessarily the sequence of events.
So if you have, but if you're
looking at it like this,
I mean, we're essentially
doing the same thing.
I'm saying, I want to,
I'm just saying that
whenever, I've just
actually phrased the name
of the functions differently here.
So what I'm saying is
that whenever TCP wants
to transmit a packet downstream,
then just, then you want that to go to IPv
and not to the top of IP4 object.
So if you can, if you have,
if it's possible for you
to instead of nesting lambdas like that,
it's possible to use delegates
and to have one protocol object point
to another protocol
object, then it doesn't
necessarily have to feel as async.
And this is, of course, sequential code,
although the underlying system
is completely asynchronous.
So it's also nice, of course,
that these objects don't know anything
about each other except for
these delegates signatures.
Another really interesting
and nice feature you get is unit testing.
So in this case, we're actually
unit testing the IP part
of our IP stack.
And this is something that we're doing
in Linux user space.
So this is a snippet from
one of our unit tests.
So we instantiate a mock link layer,
that's just a mock of the
network interface card,
and since it's using
delegates, it's really easy
to create that mock.
And now we can build a full IP stack,
instantiate a full IP stack
on top of that network interface.
And then we can rewire
this IP stack for testing
instead of rewiring it
for actually talking
to real protocol handlers, we can wire it
to call mock protocol handlers.
So that's what happens here.
So you can see that I'm
setting the drop handler,
IP has a drop handler.
For example, it's getting a packet
where the source address is broadcast
that's a illegal packet,
You can come from broadcast,
you can send to broadcast, but not from,
so IP has a drop handler
that it's going to,
by default, send packets
through that drop handler,
or we can keep track
of how many packets are
being dropped and why.
So we're making a mock drop handler now,
and we're just saying, well, I don't want
to do anything with this packet,
except I want to increment the counter,
and we can then make a expect drop marker
on top of that.
And now when I send a packet to the bottom
of the IP stack,
completely synchronously
it will be just dropped
and it will be, the drop
count is going to increment
and that means this
unit test can then pass.
So some observation,
delegating to a lambda,
it allows you to pass
state via capture clauses,
which is what we used
in the async for-loop.
I don't think I was able
to explain all the details
in that async for-loop, but
it's not actually the purpose
of the talk, but you can take a look.
The point was that it's
using delegates inside,
not using lambdas inside of lambdas.
It's not the easy or
intuitive way to program
They can look deceptively sequential,
although they're not.
If you design as a tree of event paths,
it might be simpler.
It's not necessarily that you
can always do it like that,
but in our case, it was
possible to do it like that
and it feels quite intuitive to work with.
So I'm getting close to the end,
but I'm going to show you an example
of something we can do with IncludeOS,
where we can actually
do something real here.
So this is from yesterday.
A simple web service.
And you can see that
we're using a simple
HTTP server abstraction
on top of our IP stack.
And the first thing you'll notice,
is that this is service start,
and it's async in the sense that
when service starts, you're supposed
to just exit right away.
So what you're supposed to do inside
of that service start is
just to hook up events
or to, well hook up
event handlers to events.
And in this case, we are
hooking up a very complex chain
of events by saying that we want there
to be an IP stack, and
on top of that IP stack
we want there to be an HTTP server.
Now I'm using a normal
callback system here
to say that when this
server receives a request,
this is what I want to happen.
I just want to cue the response object
that's being provided for me,
where all the delegates
going downward are wired
already, from the HTTP
response all the way down
to the Ethernet network interface.
I want to write just some
HTML down that return path.
And now I'm saying that I want the server
to listen to port 80.
This is just a simple HTTP abstraction,
and then the event loop exits.
You boot, mine is --create bridge .
And we get Hello web!
So I showed this yesterday, no surprise.
So yeah, this is how we do configuration.
This is just an aside.
We do configuration now
outside of IncludeOS
and not inside of IncludeOS.
So if you have used IncludeOS in the past,
and you have noted that
you had to do configuration
in the code, we've changed that now.
So we're actually now
building the configuration
into the binary, but it's that
much nicer for maintenance.
So and that's also one of the reasons
why we made a cleaner
abstraction below the link layer
in order to be able to switch out drivers
without having to change code.
So anyway, let's do
something more interesting
using delegates to rewire.
So what I'm doing now is that,
it's essentially much the same thing,
but, so I'm making a web server.
I commented some of it out in order
to fit everything on the slide,
but what I'm doing is that
I'm actually now talking
to the IP stack and
I'm getting the network
interface card object,
which is also an Ethernet handler,
and I'm saying that I
want to reassign the IP4
upstream to this new IP4 handler.
This new IP4 handler, just
like in the unit test,
it's just a dumb function.
Right?
But this means that I'm
able to now reassign
how these delegates are wired
and this is actually the way we would go
about implementing a firewall,
and hooking a firewall into the IP stack.
So you can do this.
And then in this new IP handler,
you can see that it takes, it
has to respect the signature
of these upstream and downstream delegates
that we use for our IP stack,
but essentially, it just takes a pointer
or it's a unique pointer
to a network packet
and that unique pointer
is going to get moved
into this function.
So I'm also now, I'm casting this pointer,
as I'm doing a unique pointer cast
because I know that
this packet is, in fact,
an IP4 packet.
This might seem strange to some of you
that we do these type casts,
but that's actually how the packets,
how I think it's natural
to treat the packets.
A network packet comes
in and you actually have
to investigate the type of the packet
by looking at the type field,
and respectively the Ethernet header
and then in the IP4 header.
So you have the whole buffer all ready,
but you don't know what kind of type it is
until you are in run time
and you're actually
inspecting the contents
of that packet.
There might be other
techniques we could have used,
but we are now now, we've made sure
that we have safe ways
of doing type casts.
And I do have a guarantee
from the lower level
that since I'm the IP4 handler,
they have already checked
and made sure that this
is, in fact, an IP packet
and that's why they called my delegate
because I was the IP4 packet handler.
So all I'm doing here,
I'm not doing anything very interesting
but I am inspecting the actual packet
and I am retrieving the
IP address from outside,
from inside of that packet.
So it's, I think that's
also quite an intuitive
way to do it.
We have packets and we
have, and that's just a
completely normal class,
where you have getters
for all those different
fields in the IP header.
What does that mean?
15 minutes, yeah, that's fine.
Thanks.
So at the ...
So we're not doing much interesting here.
We're just saying that
we want to print out
who is packet is coming from.
Obviously what you could here,
if you wanted that firewall,
was to just say okay, let's look
at this IP address, and
it's from that person,
or that person, then we drop it.
You could look at the,
again, inspect what type
of packet this IP packet is.
Is it UTP or is it PCP,
and you could inspect the port numbers,
so you could actually go from here
and build a complete firewall.
And at the end, what I'm
doing is I'm just pushing
that stack, and I'm
pushing that same packet.
I'm actually moving it directly up
to the actual IP handler of the IP stack.
So now I'm saying, I'm
done with this packet
and it's fine.
I've checked that it's okay.
I didn't drop it.
If I wanted to drop
it, I would just return
and the packet would just go out of scope
and be released.
Go back to its buffer store.
But in this case I'm
actually just pushing it
back up to where it was supposed to go
in the first place.
So this is how packets
are being pushed around
in IncludeOS, and so
let's see how it looks.
Yeah, interesting though is
that I'm keeping this server.
Because I'm keeping, I'm keeping the path
from the NIC and all the
way up to the HTTP server,
I'm keeping that path intact.
All I've done is to plug
something in between.
Right, so in between the
network interface card
and the original IP handler,
I've just plugged my
own function in between,
so that I can lie there
and inspect the packets.
So the result when I boot is
still a normal web server,
but inside of the terminal, you will see
that it's actually printing
out that it received packets
from the gateway, in this
case IP address 10.0.0.1.
Yeah, that's actually all I had.
There are a couple of more talks.
There is one talk tomorrow
by the guy who implemented
our delegates, so if you have questions
about the implementation of the delegates,
that's the guy to talk to.
I don't know if he's probably not going
to cover that tomorrow, but
that's the guy to talk to.
There's also one talk tomorrow
by one of our developers, Ingve.
He's going to talk about
tools and techniques
to stay up-to-date with modern C++.
I'm going to go to that talk.
I think it's hard to stay up-to-date.
But please come find us.
And if you have any questions,
I'll be happy to take them.
(audience applauds)
Any questions, No?
- [Participant] I was curious.
Why did you choose to
create packet pointer type
as opposed to packet IPV.
We said that it's guaranteed to be IP4.
You made that guarantee when you said
that delegate handler.
Yeah.
- [Participant] So, what's the reason
you used pointer anyway?
Do you mean for the packet?
Well the packet has actually
come in in a predefined buffer.
So the network,
the network driver
actually has a buffer store
of preallocated buffers,
and it has to have that
beause we use DMA.
So that means the physical
device is actually going
to lift packets directly into my memory.
That means I have to have
already a data structure
with buffers populated,
and the buffers have
to be appropriately sized
for actually any type
of incoming packet, so you
use roughly to unit size,
usually they are 1500 bytes,
so you have to kind of pre-allocate those.
So it's bit complicated, but that means
we have buffers that
have been preallocated,
they have been populated with
data by the actual device,
and then the natural way is
to use some kind of pointer.
We used to have a shared pointer
because we thought we might need
to have several copies of packets.
There are places in
the IP stack where you,
for example, need to buffer.
In TCP, it's supposed to retransmit.
If the retransmit timer fires,
that means it hasn't gotten a response,
in a certain amount of time.
So initially we thought
maybe we need to hold
on to the packets.
It turns out there is
quite a lot of overhead
with these shared pointers
and passing them around,
so we ended up figuring out that most
of these cases where we thought
we needed to retain packets,
we could actually solve
that in a different way.
So now we're just doing a unique pointer.
And we're moving that
unique pointer so it,
the packet, that way moves
from one protocol to another.
Anything else?
(speaking off mic)
Packet rates.
I haven't measured packet rates.
We have for one core, if,
I've measured throughput,
roughly, and it's in the
few gigabits of throughput,
is what we can push
through, single, IncludeOS,
a unikernel on a virtual
machine with a single core.
If you do, I, we did
iPerf, I've done iPerf.
So we've also done low
balancing experiments
with at least, I think,
65,000 concurrent connections.
Thousands of connections per
second, it was quite fast.
Obviously, when we're bound
to, the way we're using it
by default, is that we're
using a single core.
But we do have, we also do have access
to several CPUs.
We don't have high-level
extractions for that yet,
but they're in the making,
and on this conference
in Germany, I'm going to talk about how
we can actually do parallels
from inside of IncludeOS.
There are several interesting
opportunities available
to you when you are in complete
control of the hardware.
We don't need threading.
You can actually put the functions
directly onto cores, for example.
(speaking off mic)
Oh, interrupt context, no
We can go back to the slide
about the event handler.
(speaking off mic)
We could choose preemptive.
There are some things we do preemptively,
let's see, I think it was here.
So when it comes in, you have a choice,
I mean, of course, the
interrupt actually preempts.
It always does.
There is no way around it.
That's what an interrupt is.
You can't prevent it from preempting.
So what happens here is
that there is a preemptive
interrupt handler
for IRQ 37, but it's
handcrafted to do nothing
except marking, setting
a bit that's available
to the event handler.
And we do that automatically.
So that means you don't actually have
to do a complete context switch,
because you just write and you keep track
of which registers you're
actually clobbering
and you're making a small
very, very easily maintainable,
and tiny, tiny little assembly function
that just sets this bit
and then it returns using IRF.
There are something we
are doing preemptively
so you can to context switches.
For example, we have a stack sampler
that's quite interesting.
If you set a timer to
one of these interrupts,
using one of the timer features in the CPU
or the, one of the timers
that are available,
you can set an interrupt to happen
at a fixed interval.
And now when you're preempting,
you're on the same stack.
So that's quite interesting.
'Cause now you're preempting,
you're on the same stack,
and you can just investigate
the stack from below you,
and you know which function
was running right now.
So, demoed that.
I think we showed it last year,
but it's, that's one of the things
we are doing preemptively.
And from there, it's
not hard to influence,
you could easily put
threading on top of there.
I mean, we already have the
context switching features
most of them, in many case,
but the complexity explodes, obviously,
when you do it, so our mission was to see,
how much is it possible
to do in this single-threaded,
very strictly event-based context,
and then we are now doing
experiments with running,
for example, we are
actually doing experiments
where we have several
CPUs and we are assigning,
for example, PCP connections
to individual CPUs
where they do TLS decryption, et cetera,
on individual CPUs.
So we're trying to avoid
this classical context switching scenario,
mainly because for virtualization,
it turns out that multi
threaded applications,
running in virtual machines,
perform much worse compared
to single-threaded
applications, as opposed
to when they're running direct number.
(speaker off mic)
Yeah, that's a way to do it.
You can have, I mean its', that's,
that's one possible way to do it.
But there needs to be
some kind of event handler
on that other core and I think
you could implement
something like a thread pool
on top of there.
So we've done experiments
similar to that effect,
where you have different CPUs
or actually just fetching connections
from a shared pool.
Another way we have done it is
to actually have separate
IP stacks running
on different cores.
So we have some interesting experiments
and some results that
we would like to share,
but they're not ready yet.
But the code is there.
So you can go look.
There are some examples of SMP running
actually just a lambda
on a separate CPU core.
There are examples of that in our story.
Yeah.
- [Participant] Do you
reassign the object pointer
separately from the function
pointer, for the delegates--
You can--
- [Participant] Define
them both at the same time.
How do you mean?
- [Participant] So like if
you have an existing delegate
and you disperse the
object of the delegates
through without switching?
Switching just the
object, can't you do that?
He's shaking his head.
I mean, you mean,
shifting, so for example,
if I had PCP connection object
and I want to switch
delegates from pointing
to one object to another object?
Is that what you mean,
but to the same function?
- [Participant] Yes.
Yeah, that should be possible, yeah.
Because it holds the pointer to the object
and to the function.
(speaker off mic)
- [Participant] Sure I
understand, but I guess I'm trying
to figure out what this
gives you over just capturing
your object in a lambda and then ...
(speaker off mic)
- [Participant] Okay.
Other questions?
I guess please just come up and talk to me
if you have other questions.
I'd be happy to talk.
If you have other questions
regarding IncludeOS,
as well, that's interesting.
And don't be shy to
come into our chat room.
I mean, we really like to
have people approach us,
and connect with us and
we'd be happy to help you
if you're interested and curious.
Also consider using IncludeOS
just as a network library.
It runs fine under user space Linux,
we also have a project now where
you can patch the whole IP stack on top
of a Linux tap device, so
that Linux has a facility
where you can pull Ethernet frames
directly off of, out from the kernel
without going through the Linux IP stack.
So it's possible to use
IncludeOS in your normal process
as an IP stack and many interesting things
you can do with that.
Okay, I think I'll just
say thanks, and ...
(audience applauds)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>