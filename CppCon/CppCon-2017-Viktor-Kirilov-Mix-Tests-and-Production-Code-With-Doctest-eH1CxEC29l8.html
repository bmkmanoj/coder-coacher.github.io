<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Viktor Kirilov “Mix Tests and Production Code With Doctest...” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Viktor Kirilov “Mix Tests and Production Code With Doctest...” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Viktor Kirilov “Mix Tests and Production Code With Doctest...”</b></h2><h5 class="post__date">2017-10-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eH1CxEC29l8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">My name is Viktor Kirilov
and today I'll be talking
Doctest which is a testing framework.
So, first about me.
I'm from Bulgaria with a few years
in the games and VFX industries.
And for the past two years,
I've been working on personal projects.
I use different compilers
and I use a lot of CMAke,
git, and valgrind and the sanitizers.
And I also think that GitHub
is one of the most amazing
places for software developers.
It's like a social network.
I'm very passionate about game
development and game engines.
Also, data-oriented design
and high-performance computing
and about good
software-development practices.
So, Doctest was inspired by the ability
of other compiled languages
such as D, Rust and Nim
to have tests written directly
in the production code.
And the mantra of the project is that
tests should be considered
as a from of documentation
and they should be able to reside
near the code which they test.
So, first of all, Doctest
is heavily influenced
by Catch and it models it's
interface to a great extent.
Currently a few big
things witch Catch has,
Doctest is missing, like
a reporter system to XML
and to other formats.
And also matchers, but Doctest
has some features of it's own
like test suites and templated test cases.
There is also this great
repository by Mike Martin,
which compares Catch, Lest and Doctest,
which are three very similar frameworks.
All three of them are single header
and support similar features.
So, in this presentation,
I'll first introduce you to the framework.
Then I'll continue with
implementation details.
For example how to
automatically register code.
Also a bit about the
preprocessor and warnings.
I'll talk a bit about how
to decompose expressions
with templates and some other stuff.
Then I'll finish with compile
time and runtime benchmarks
and how they were achieved and
examples of how to integrate
tests in your production code.
I will not, however, be
talking about how to do testing
in general, because that's
covered in a lot of other talks.
So, lets get started.
First of all, since it's
a single header library,
there are two ways we can approach it.
Either everything is marked
as inline and all the headers
are always included, or we
can separate the header in two
parts; the interface part
and the implementation part.
The interface part is included everywhere.
But the implementation part
is conditionally compiled
in only one source file
where we have defined
some identifier before
including the framework.
You can think of this as the
library being in two files,
a header and a source file.
But they're in one file
for easier distribution.
This is a common practice
for single header libraries,
it's sort of a buzzword,
and most of the includes
and dependencies go only
in the implementation part.
Which again, goes in
only one translation unit
of your project.
This way dependencies are contained.
Here is a complete example
of a program which compiles
and links to an executable.
First, we define some
identifier which tells Doctest
to implement it's test runner
here, in this source file
and also to provide a main entry point.
Then we include the framework.
And then we have a function
which we want to test.
There is a one single test
case which has four assertions
and the first one of them will fail
because we have a bug in the function.
Here is what the output looks like.
First we get told which test case failed
and then which assert failed.
And we even get the value of fact of zero.
And at the end there is a summary
of how many tests passed and failed.
So, what makes Doctest different?
In two words, it is very
light and unintrusive.
It has the smallest possible
footprint on compile times.
You can remove everything
testing-related from the binary.
There is no namespace pollution.
All macros are, or can be, prefixed.
Doesn't drag any headers with it.
I'm talking about the interface part,
not the implementation
part of the framework.
So, if you just include
it, and you don't implement
the test framework,
you don't have included
stubs train core anything.
It produces absolutely zero warnings
for even the most
aggressive warning levels.
And it's also easy to
integrate with user code.
The idea is, that even if you
include it across your entire
project, in each source file,
that you don't notice it.
Not in build times, not in
warnings, not in anything.
It is also very reliable.
It is per commit tested on
Travis and appveyor CI services
which are integrated with GitHub.
And each time I push, all
tests are built in Debug
and Release 32/64 bit modes.
With all these compilers,
on Linux, OSX and Windows
warnings are treated as errors.
I ran the test through valgrind
and through the sanitizers.
Builds are build in C++98 and 11 modes
with an without exceptions,
with and without RTTI
and it's statically analyzed
with five different tools.
That amounts to a lot of builds
and it took quite a lot
of time to configure.
So, all this, makes writing tests
in the production code feasible.
Which in turn, leads
to lowering the barrier
for writing tests.
You don't have to make
separate source files.
You don't have to commit
them to source control,
to add them to the build system
and equal to bunch of stuff
and link some stuff as well.
You can just write the
tests for some module
at the bottom of it's source file.
Tests can also be viewed
as up-to-date comments
because the compiler enforces
that everything compiles.
It's never out of date.
It's also easier to test
internals of your APIs,
which are unexposed through the headers.
And tests during development
becomes pretty straight forward.
Even if you don't like the idea
of mixing tests and production code,
the framework can still
be used like any other.
It has a rich set of features
which I'll talk about in a minute.
So first, there is one
core assertion macro
which decompose expressions.
So you don't have to give
it manually the left-hand
tried operands and tell
it, it's supposed to check
for equality or inequality
or something like that.
Also, test cases are
registered automatically,
unlike in the bottom case,
where you have to give it
some unique name, some identifier
and then register it somewhere manually.
So, we're keeping things dry, like in,
don't repeat yourself.
There are subcases which
were directly stolen
as an idea from Catch.
It's their way to share setup
and teardown code of tests.
So, suppose we want to
open database connection
and then close it and
do some stuff with it.
If we have multiple tests
that need that database
connection, we can just
write the opening and closing
code once, and write a few
subcases which use the database.
Instead of writing separate test cases
and duplicating our code.
There are also logging facilities.
For example, in this case if
some element from the arrays
don't match, we would like some context,
like which element was it.
So, we can log the counter
variable of the loop.
What is special, is that actually,
the string is not constructed
unless the assertion fails.
So, there is lazy
stringification for performance.
Here, there is no allocation,
unless an assert fails.
And the output from this would be in all,
at the end there will be with
context the value of I is 75,
if the assert fails.
You can also teach the framework
how to translate exceptions of user types
by registering a translator function
and I will show how that works later.
And when you compare custom
types in your assertions,
if the assert fails, you
could teach the framework
how to stringify your types as well.
There are also templated
test cases, in which case
you can instantiate a test
case on a list of types
and in this case, we
will get serialization
for int, char and myType.
In this particular case, we're testing
that for each of these
types, if we serialize them
and then deserialize them, that the result
is equal to what we started with.
There are also asserts for
dealing with exceptions
like checking if something throws
or something doesn't throw.
And there's also a utility class
to help us with comparing floating point.
There are also decorators.
In this case, we can
tell that this test case
should be marked as skipped
if some function returns true
or we can also tell Doctest
to mark it as failed
if it takes more than 200 milliseconds.
There are other decorators as well.
Just a second.
So, other notable features
are crash handling
with signals under Unix
and structured exception
handling under Windows.
Failures can also break into debugger.
And there's a rich command
line with lots of options.
For example, we can list all test cases.
We can filter them by
name with using wildcards.
We can filter them by
excluding them with wildcards.
We can tell them the framework
to abort after 10 failures,
how tests should be ordered,
now, that it shouldn't
break into debugger.
And we can even ask the binary
how many tests there are
and then we can do range-based execution
using the executable.
So, Doctest still cannot execute
tests in multiple threads,
but you can paralyze your tests
if they run slowly this way.
So, let's get into some details.
And also, the code is very simplified.
This is not the actual
code in the library,
it's more messy in there.
So, at the heart of
everything, sits the ability
to construct anonymous unique variables.
For this, we're using the anonymous macro
which concatenates a literal
with the result of counter.
Counter is an identifier
which is not standardized
but it's practically supported
by all major compilers
for the past 10 or something years.
And each time you use it in a source file,
you get a bigger integer.
It always starts from zero.
So, in this case, when we
use anonymous with Anon_Var
two times, we might get
Anon_Var_5 and Anon_Var_6.
So let's see how the automatic
test registration looks like.
This test case expands to the following
after the preprocessor.
First we have a forward declaration
of the void test function.
After that, there is
a global dummy integer
to which we assign the result of regTest.
That way we force some code to be executed
before we enter the main
function of the program
because all globals are initialized.
And to the call of regTest we
pass the name of the function
and the relevant information
like where it is written,
what's the name of the test case
and we also call get from TS,
which is a way to get
the current test suite.
I'll talk about that later.
And after that, follows the
definition of the test case.
So, here I'm using static for the function
and for the dummy integer because,
as I said in the previous slide,
counter always starts from
zero for each translation unit.
So, if these weren't static,
I might get link or errors
when I write tests in
multiple source files.
So, when we call regTest,
we need to insert
this test case in some registry,
but where does that
test case registry live?
If we just make it as a global variable
where the test run is implemented,
we don't have any guarantees
that it's already initialized.
So in C++ there's this thing called
static initialization order fiasco.
And in one translation unit you have,
guaranteed by the standard,
that all globals are
initialized from top to bottom.
But between separate translation units,
there are no guarantees.
So we need to make sure
that the test case registry
is initialized the first time
a test case gets inserted.
So for that, we're using
a getter function in which
it is defined as a local static variable,
and then we return it by reference.
So the first time this getter
is called, we actually create
the registry and it lives
until the program ends.
So in regTest, we construct a test case
and we put it in the registry
and then we return zero
to initialize the global
dummy integer, which forced
regTest to be called in the first place
before we enter main.
So, test suites.
By default, all test cases
are in old test suite,
but we can put them inside
of a block like this one.
Doctest provides some default get function
in a TS namespace which
returns an empty string,
meaning the test case is nowhere.
But when we put the test case
inside of a test suite block,
that, after the preprocessor
looks like the following.
We still have the default
get function from Doctest,
but now we have introduced
an anonymous unique namespace
in which we have made the get function
inside of a TS namespace
and then we close all these
namespaces and reopen the
anonymous unique one again
to reuse the opening
calibrates written by the user.
And then, when test case
macro expands and we call
TS get, we actually call TS
get from this unique namespace.
That's how this test case knows
it is in that particular test suite.
Let's talk about warnings.
Doctest doesn't produce
absolutely anything,
even when using everything for Clang,
level four on Visual Studio,
Wall is a bit too noisy,
and also Wall on Wextra for
GCC and over 37 other flags
which are not covered by Wall and Wextra.
These are the other flags
which are pretty hard
to track down, but there is
a trick to see which warnings
are left enabled or disabled
after you have passed Wall, for example.
And the line at the
bottom is exactly that.
These slides will be available
later on the internet,
so you can copy this line later.
So, Doctest here originally
produced some warnings
like some internal structure is padded.
And that's not a bad warning.
It's a diagnostic message.
And I still wanted to
silence this from the header.
So, each decent compiler nowadays
supports a diagnostic stack
to which you can push,
say that you ignore something
and then pop at the end.
So I surrounded everything in my header
with such pushing an popping
for different compilers
to silence the most problematic warnings.
But, what happens to
warnings which are generated
by the macros themselves?
When the user uses the test case macro
it's not expanded in the header.
It's expanded in his source code.
And he produces the following
two warnings for Clang and GCC.
Because of the global dummy
integer we use to call
regTest before we enter main.
The unused variable for GCC
and the global constructor swap for Clang.
So we can't ask the user to
surround his code with pragmas.
What can we do?
So, the preprocessor works
in the following way.
Suppose we have a source
file and a header file.
The source file includes the header file.
After the preprocessor we
get the following translation
unit, which gets fed to the compiler.
The compiler doesn't see
the two files at the top,
he sees result after the preprocessor.
So as we can see, the header file
is included in the source file.
There are no more comments,
the macros are expanded,
but there are still a few
lines starting with a hashtag,
which are left.
The pragma for the structure has survived
because it's actually a
directive for the compiler,
not for the preprocessor.
And also, there are these weird lines
with numbers and test
files, I mean source files.
Since the compiler gets
only these final result
after the preprocessor,
he still needs to know
what came from where.
And in this case we have a
variable with a definition
in the header file.
And in order for the
compiler to be able to report
the proper line numbers
and files for the error,
exactly these weird lines are used.
So, we can't just embed a
pragma inside of a macro,
but since C++11, there is a way
using the underscore pragma operator.
It's actually been available
in compilers for the past,
like 15 years, but now it's standardized.
So, here I've written a macro
which expands to some for loop
you might want to paralyze using
the open MP extension for compilers.
And when the preprocessor runs that
_pragma omp parallel for,
expands to the code on the right
and we get our pragma nicely
after the preprocessor.
So, here's how we silence the dreaded
global constructors warning for Clang.
We surround inside of our
multi-line macro the dummy integer
with such pushing to a stack,
ignoring condemn popping from the stack.
So but, in the case of
GCC the C++ frontend
is problematic in it's
underscore pragma usage
for the past six years
and it doesn't work.
There are quite a few issues about it.
So, I've been lucky that I
could silence the warning
by using an attribute unused
to mark the dummy global
integer as being unused
for anything else except
for calling regTest.
So, let's talk about subcases.
In this case we see that
setup is printed three times.
It's at the top of the test case.
That means that the test case
has been reentered three times.
We also see that there are
a bunch of nested subcases.
So they resemble something like a tree.
And there are three leaf nodes,
if we can call them like that.
So the execution of
test cases with subcases
resembles something like a DFS traversal.
We go deeper and deeper
until we find a leaf node
and then we pop out.
And if there are anymore
subcases left untraversed,
we reenter the test case again.
So this is very useful for
sharing setup and teardown
code like opening database
connections, opening files,
or initializing data or whatever.
So, these subcase macros
expand to the following.
I should also mention
that they're identical
to sections in Catch
and the data is taken from there.
The subcase macro
expands to the following:
If statements, and in these
If statements I construct
temporary objects of type subcase,
which I give the information,
like where is the subcase defined.
And then I assign this temporary object
to a constant reference
to extend its lifetime
until we have left the entire If statement
and the Then block, if we
entered the Then block.
In the constructor of the subcase class
based on the information we have given it,
it uses global state from
the test runner to determine
if we should enter this subcase,
if it has already been seen and et cetera.
And it sets a Boolean
inside of the subcase class.
So, when the operator bool
is called on this temporary
subcase object, it returns
either true or false,
so we either enter the If
statement, or we don't.
I mean, the Then block
of the If statement.
Also, as you can see, subcases
are lazily discovered.
They are not like test cases,
which are registered before we enter main.
Subcases are discovered on the go.
And we don't know how
many subcases there are
within a test case, unless we enter it.
So, in the first example I showed how,
a complete example where
Doctest brought it's own
main function and the
implementation of the test runner.
But if we're writing tests
in our production code,
we already have a main function.
In that case, we should
tell Doctest only to bring
it's test runner, but
not the main function.
And to call it, we need to make a context,
then probably set some default options,
then we can parse the comment line,
we can even override some options
and then we call run on the context.
So, when we mix tests
with production code,
there are three scenarios
we should support.
We either want to run only the tests,
only our program, or both.
So, if we have passed the
exit flag to this executable,
after we have called run on the context,
when we ask it if we should
exit, it returns true.
So we shouldn't continue with our program.
If we have passed the no
run option, it that case,
when we call run on the
context, no test will be run.
And when we call should
exit, it will tell us false
and we continue with our program.
By default, all the test cases will be run
and then we'll continue with out program.
If your unit test executes very fast,
like for half a second,
it's very convenient
that each time yous start up your program,
you will run also tests.
It's nice to see that green light flash.
But if you write your test
in your production code,
you might want to remove
them out of the binary
when you ship it to
customers and in that case,
you can use the disable
identifier globally.
And then all test cases are turned into
uninstantiated templates.
So even in debug builds
the test cases are never
instantiated and they're not
present in the final binary
and that speeds up the linking time a bit.
But we can take it a step
further because even if
test cases won't be
instantiated, there's still
a lot of code which has to be parsed.
So we can also turn every
assertion macro into a no-op
using a void of zero which
is also used to require
a semicolon which the user
had written after the assert.
Also subcases just vanish
and most of the test runner
is removed from the binary.
Only a few stds are left because
if you had called Doctest
on your own in your main function,
that means you have created a context
and you have called run on it et cetera,
so these functions are
still present as stds.
So, let's talk about the
expression decomposition.
In this case, the assert
at the top expands
to the following code on the bottom.
First, everything is wrapped
inside of a do-while loop,
which loops only once because
the condition is false.
I used the void to zero comma zero trick,
to silence the conditional
expression is constant warning,
for Visual Studio.
Also that way, this is a
common practice to wrap
multi-statement macros inside
of a do-while-false loop,
to make them a single statement.
So they can be nicely used
after If statements without blocks.
So here first, inside of a
do-while loop we construct
a ResultBuilder object and
we initialize it with like,
where the assert was written
for information we will use later.
And then inside of a Try block,
we will evaluate the expression.
If anything happens, we
will catch the exception,
we will translate it,
as I will show later.
And then, if the assert
has failed, either because
the assert, you know,
these things are not equal
or something true, we break
into debugger right here.
So, how does expression
decomposition work?
We construct and expression
decomposer object
which has the left shift
operator overloaded for it.
And since in C++ this
operator has higher precedence
over the comparison operators,
actually the left operand
of the comparison is swallowed
by the expression decomposer
left shift operator and
that operator then returns
a different type which
is the type left operand,
which holds the left operand,
and it has overloaded
operators for comparisons.
And that way then, it
captures the right operand,
in this case b, and it
does the actual comparison.
And that returns a result which is set
to the result builder if nothing throws.
So, here's the expression
of decomposer class.
It's just one method, which is templated
to capture the left operand
and it returns, as I said,
an object of a different
type, of type left operand
and that left operand type is shown below.
It holds a reference to the left operand
and it has the comparison
operators overloaded for it,
and they're also templated.
So when we get the right operand as well,
in our comparison operator
we construct a result object.
That's where we do the actual
comparison and we also call
stringify on the left and
right sides of the comparison.
So, the result object is just
a Boolean with the result,
and also a string
decomposition of the expression
if anything fails.
And the stringify function we
called on the left and right
operands just calls to string
on the left and right operands.
And there's a way to teach the framework
how to stringify your
types, but by default,
you get a question mark if the framework
doesn't recognize your types.
So, if we use a function
which throws an exception
inside of an assert, the
framework can be thought
how to translate that exception.
And that happens in the catch clause.
We call the exception occurred
method of the result builder.
So how does that work?
First we have an ITranslator
interface which has a single
virtual function called translate.
It returns a bool and it
also returns the result
of stringification through
a reference string.
You know, you can return a
pair of a bool and string
that doesn't matter.
Then we have translator
class which is templated
and it inherits from the
Itranslator interface.
In it's constructor, we
except the function pointer
to a translator function
for a specific type,
which is the type we will be templated on
and we have also overridden
the translate function.
And in there, inside of a
try statement, a try block,
we rethrow the current active exception.
And then we try to catch it with the type
the translator class is
currently instantiated for.
And if we succeed in catching
it by that specific type,
then we use this function pointer
we have been constructed with,
we translate the exception
and we return true,
otherwise we return false.
So, the user may register
a translator like this,
using the Reg_Translator macro.
And like a normal function call, I mean,
normal function definition, he
says what type of exceptions
it will be translating.
And this expands after the
preprocessor to the following:
Again, like registering
test cases we first make
a forward declaration of
the function which the user
has defined, then we make a
global dummy integer again,
to which we assign the
result of Reg_Translator
and to it, we pass the name
of the translator function.
And after that follows the
body the user has written.
So, the Reg_Translator
function is also templated
so it can capture the type
this translator is written for.
And inside of it, it
creates a translator type
as a static local variable,
which means that it will
be alive until the program ends.
And this translator type is the
one from the previous slide.
And here we instantiated for
the current type of exception
and we gave it this function
pointer which is the translator
the user has written.
And now can pass through the test runner
a pointer to this local translator object,
which will be alive
until the program ends.
We can pass the pointer
to it to the test runner
because it shares a common interface,
the ITranslator interface.
So the test runner will not
know about your specific types,
but it will know, it will have a list
of pointers on Itranslator objects.
So, when the exception occurred
method of the ResultBuilder
is called, we call the translate function
which first iterates through
all registered translators
and tries to translate
the current exception.
And if all that fails, it continues
with default translation which again,
rethrows the current type of exception
inside of the try block
and tries to catch it
as a std exception, as a
std string, or a const char
and if all these fail,
it returns the default
message: unknown exception.
So, let's talk about compile times.
They're like the Achilles Heel of C++.
This is a benchmark
comparing how much it costs
when you include the Doctest
header and the catch header
in an empty, totally empty source file.
Doctest is always around 20 milliseconds,
where Catch is almost half a second.
The reason for that is that Doctest
doesn't include absolutely anything
in it's forward declaration
part, the interface part.
So after the preprocessor,
it's only 1,200 lines of code,
specifically for Visual Studio.
And Catch is around 40,000 lines of code.
This because...
Yeah, I said that before.
I don't wanna bash Catch
because it's an amazing project
and actually, Doctest
wouldn't exist without it.
I've been piggybacking on
all the features and ideas
from Catch and also, Catch
is still much in development
and they're incorporating
many of the ideas
I introduce in Doctest.
Also Boost.Test also has
a single header form,
but it is madness.
I mean, it's like 10 or 20 times
slower to compile than Catch.
I mean, nobody uses it, I think,
in it's single-header form.
So, I need some types
from the std namespace
in my interface, but I don't
want to include anything.
For example, for std ostream,
there is the iosfwd header
but in it's case, I mean in
the case for Visual Studio
when I include that header,
after the preprocessor,
without comments and macros,
I'm left with 9,000 lines of code.
And that's like, eight times more
than my interface entirely.
So, I don't want to include that header.
So, I just forward declared
std ostream myself,
that's against the standard,
but it practically works everywhere
and on all those compilers I'm testing.
Like from GCC 4.4 to seven,
all Clangs and all Visual Studios.
And even if something
goes wrong, there's some
configure option to
actually use this header.
Which, for other compilers
is actually much smaller.
Like for Clang, it's 500
lines of code not 9,000.
And also there are other
libraries which forward declare
std from the std namespace.
For example, Boost.DI does the same,
it includes absolutely nothing.
So, lets talk about the cost
when you include the header
and you also implement the framework.
Again, Doctest is faster
to compile than Catch,
but I don't consider this
important because this is done
in only one source file.
You instantiate the test runner
and you compile it only once
until all the other source
files you have in your project
you use the interface, not the test,
you don't compile the
test runner everywhere.
And also, as features
are added to Doctest,
it will also get slower
in it's test runner,
that's inevitable.
And also, something else we
can see is that the compilers
differ very much between each
other and also debug versions
release is wildly
different in compile times
and that's quite interesting.
So, let's talk about the
compile time of the asserts
because how much it costs
to just include the header
is only one part of the story.
Doctest supports the normal
expression decomposing
form of it's asserts, which
you don't have to explicitly
say the left and right
operands with a comma,
but for performance freaks,
there are also other macros.
For example, there are
binary ones which skip
the template machinery to
decompose the expression,
you give it explicitly what's
on the left and on the right.
There are also the fast
binary ones, which in addition
to not having to decompose expressions,
they also don't evaluate the comparison
inside of a try block.
And if we define the
super-fast asserts identifier,
then they become even faster
and I'll show what that
means, a bit later.
Catch has the expression
decomposing asserts
and it also has the fast compile option,
which also removes some of the
functionality of the asserts,
like in my case, it removes the try blocks
and it's also a bit, like
the super-fast asserts
which I'll talk about later.
So, in the benchmarks I'm about to show,
I compiled 500 test cases with
100 asserts in each of them,
and that amounts to a
total of 50,000 asserts.
So, here are the results.
Oddly enough, GCC is
quite slow in the release,
both on Windows and on Linux.
There are very much
differences between compilers.
The two red lines are for the two forms
of the asserts for Catch,
and the other four bars,
which are the blue ones,
and the green and brown one,
are for Doctest.
And as we can see, you
can get crazy performance
with the fastest form of
the asserts for Doctest.
I mean, you can get 50,000
asserts to compile for like,
less than 10 seconds.
So, this is what normal assert,
which is decomposed expressions,
looked like in the first
version of the library.
And it was terrible code bloat.
Then my asserts were a lot,
lot, lot slower to compile
than those of Catch.
And that's because a lot of stuff ended up
generated after the macro,
which didn't have to be there.
And in Doctest 1.1, the same assert
looks like the snippet at the top.
Just the bare minimum, the do-while loop,
the ResultBuilder object,
the try catch statements,
and an If statement to check if we should
break into debugger exactly there.
The fast binary asserts
again, are even less code
and that's why they compile faster.
And they're again,
inside of a do-while loop
because we have two things we have to do.
We have to call the assert function
and we have to do an If statement,
if we should break into debugger.
And when we define the
super-fast asserts identifier,
those same fast binary asserts
become a single function call.
And the only downside is
that when the assert fails,
we don't break into
debugger at this exact point
in the source code, but one
level deeper in the call stack,
which means, in the fast
binary assert function
and we just have to go one
level up in the call stack
to see where we are
actually in the source code.
And in my opinion, that's
a pretty good price to pay
for the compile times
you get, if an assert,
expands to a single function call.
So, very much depending on the compiler
and on the configuration,
those 50,000 asserts,
the normal ones, which decompose
expressions may compile
for 20 or more than 200
seconds, which is roughly,
around 50% faster than Catch.
And the fast version of
Doctest, the fastest,
maximum possibly the
fastest ones, compile for
between three and 16 seconds.
Again, depending on the compiler.
And that's pretty good for 50,000 asserts.
And to achieve these
results, a lot of functions
were marked as a explicitly
not to be inlined.
Many constructors, destructors,
the fast binary assert
function itself, many things
were marked no inline.
But this doesn't have big
impact on runtime performance,
which is the next topic.
And these benchmarks
were done two weeks ago
with almost the latest
versions of the frameworks.
And you can also check out
this link how they were done.
So, here we see a loop
which looks 10 million times
and it compares an integer to itself.
Pretty stupid, but it
can't get optimized away
because there might because
there might be side effects.
And it is not optimized away.
So, one interesting thing
to see is that Visual Studio
in debug takes a lot of time to compile.
But in all other cases, both
frameworks are pretty fast
and both frameworks have
made significant progress
in their runtime performance
in the last five to six months.
Actually, first Catch
started doing that, and I saw
that they're doing that and I
got the idea to do the same.
Because some users were
reporting that their unit tests
run for like, 10 minutes, which is crazy.
I mean, I don't know how
many asserts they have,
but maybe it's a problem for them.
So the latest version of
Doctest is more than 30 times
faster than it's previous version.
But here, I'm talking
only about the common case
where no assert fails.
And that's the common case, right?
If you have 10 million thousand asserts
and only 5,000 of them fail,
you should optimize for
the ones which don't fail.
The biggest gains I got were
by not constructing strings
if the asserts don't fail.
So I reordered, a bit, my
logic around the asserts.
The second biggest thing for gains was,
since I don't think would
std string in the interface
of Doctest, but I still
needed some string class,
and I implemented my own.
And for it, I implemented
the small string optimization
based on the ideas of Nicholas Ormrod's
Strange Details of
std::string at Facebook,
talk from last year's CPPCON.
It's a great talk, very insightful.
I highly recommend it.
So, these two are practically the reasons
Doctest became 30 times faster
compared to it's older versions.
Move semantics don't play
any role in the common case
when nothing fails because
there are no allocations
and move semantics are helpful
when there are deep copies,
and when nothing is allocated,
you don't have deep copies
because everything's on the stack.
And one other minor performance gain was
by not using any local
statics on the hot path
when asserts don't fail.
So, if you have been
convinced that it's practical
to include this framework
in your production code
and just mix production code and tests,
you have several options.
If you're writing an
end product for users,
not for developers, you can
just mix the code and tests
and then, when you finally
make the final build,
you can define the disable identifier
and there are no tests
in the binary anymore.
And you can also not remove
the tests, but disable them
by default by setting
the no-run option to true
and that way, the user
will still have the tests
in his binary and for example,
if there is a problem,
like something doesn't, you
can't reproduce it on your side,
but only the user is experiencing
it, you might ship him
a binder with tests and he
can run them on his platform
to see if anything fails there.
And also, this won't impact
any runtime performance,
no dragging these tests
along with the binary
because they're cold code, yeah.
Especially if you use
profile guided optimizations
they won't be any near the
hot path of your application.
So, if you're developing software
which is not for end
customers, but for developers,
and if you're writing
header-only libraries,
you might put your tests at
the bottom of your library
surrounded by some
if-def you have written.
And so your users might
just include the header
and use the functionality
or they might define
that identifier before including it,
so they get the test as well.
And they can even use a version
of Doctest of their own,
if they have included
it before your header.
It's also a good idea when
you're writing a library
for other developers, with testing it
to either prefix them in their names,
with the name of your library,
or put put them in a test suite
with the name of your
library, so your users
can filter easily the tests
for a particular third party.
In the case of compiled binaries,
let's talk about firstly,
about shared objects and DLLs.
You have to instantiate
and implement test runner
in each DLL or executable.
But that makes integrating
all the tests, hard.
Because you have a bunch of test runners
and a bunch of test registries
and you can't easily
integrate them, yeah, you can't.
Unless you use this identifier globally.
Using it, you can define the
test runner and implement it
in only a single binary.
It can export all the symbols
and all other binaries can
link to it, so only a single test registry
lives in your application.
So, that's about shared objects and DLLs.
In the case of static
libraries, there are issues
with self-registering test cases.
And these issues are common
to all testing libraries
and benchmarking frameworks
which register called automatically.
There are ways to combat that
and you can read more about it
in the FAQ of the documentation.
Here is how I use Doctest.
I have a proxy header which includes
the actual header of the framework.
I first remove the short
versions of the macros
and also those, then say
to, that I want the maximum,
possibly fastest versions
of the fast binary asserts.
And then, I define the asserts
in the way I would like to use them.
And then, I just include this proxy header
and write my tests along
my production code.
Most of the effort in making
the framework went into
firstly, familiarizing myself into testing
on other testing frameworks.
Also, making sure that nothing
is dragged in the public
interface of the library, that
no header is included there.
Also, the 300 plus different CI builds,
there might be even 400, they took like,
literally at least three or
four weeks of compacted time
and just waiting for three
hours to see if something fails
or if GCC 4.7 tells you
that something got inlined,
which is not a warning, it's a diagnostic,
but it's only or GCC 4.7 et cetera.
Also, the strict overflow
on level five was very fun,
because it got triggered only
on some compiler versions,
only in release builds
when something got inlined
and inside of it, I have
some problematic arithmetic
and in the report, it says
that it's on line zero.
I mean, I don't know what got inlined,
and I don't know on which line, you know?
So, that was fun to track down.
But I'm clean from strict
overflow on level five.
I'm not sure if anyone uses
that in real production,
but yeah, Doctest is clean.
I also found my first unique bug in GCC.
It's integration of the
sanitizers and they were also
quite a bit interesting
toolchain problems.
Even now, I have a few builds
mag test allowed failures
on Travis because the
sanitizers for GCC 4.10
and there are always fix from
home-brew, they don't work.
So, what lies ahead?
There should be a reporter
system because currently,
Doctest only reports in the console watch,
what passed and what and what failed
and returns either one
or zero as an exit code.
There should be xUnit, jUnit
et cetera, style XML output.
It should be cool if tests can execute
in separate processes,
especially under Linux.
That's not just to combat
crashes, but also to isolate
tests if they use global state.
Death tests, which is an
idea taken from Google test.
Generators for data-driven testing.
Matchers, more command line options .
A big one, is integration into IDEs
because currently, Doctest
doesn't have adapters
for Visual Studio or XCode or C line,
or anything like that.
And another big feature
which is coming sometime,
is thread safe assertions.
Where you could have, not
having multiple test case
since running in parallel,
but if you instantiate
a number of threads
within a single test case,
and if you wanna call assertions
from the multiple threads,
that should be possible in the future.
And there is a huge roadmap
like, with more than 200 ideas,
or no, 100 ideas, we fought
to add to the framework.
But these are the biggest ones.
So, the initial concept got into my head,
like three years ago and when
I quit my job two years ago,
I started accelerated
development of the framework.
Those are the three major
version where the first one,
was with the focus of the
compile time of the header,
then the compile time of the
asserts and the last one,
was with features and
runtime performances.
It's a bit late to the party
because all the major testing
frameworks are already been written
and lots of projects are using them.
But I believe that these results
wouldn't have been possible
without starting from scratch.
Especially, the lightness of the header.
I mean, not including
anything in the public part
of the header is, you have to
do that from the ground up.
And a modest goal for Doctest,
is to make it a defacto
standard for unit testing in C++,
almost like a language feature.
The idea is that you can just
include it and not notice it
and write tests if you want
to because it's so light
and you don't see it.
So, this was it.
You can check out the slides,
the project, my personal site,
GitHub, Twitter and et cetera.
Thank you and if there are any questions,
I'll be very happy to receive them.
(applause)
So, any questions?
I don't know where the mic is.
- [Audience Member] Okay, first of all,
thank you for the demonstration.
It's not working right.
I was using Catch, alright.
Seems that you did kill Catch, right now?
What, what?
- [Audience Member] I was
using Catch on my project.
It seems that you already not sure
of the features from Catch?
Yeah, most of them.
There are a few important
ones which are lacking with,
it depends, yeah.
- [Audience Member]
Okay, so are you working
with them to build...
No, I think we should be,
because I've been piggybacking
on some of their features
like the signal handling
for crashes, I took it from them directly.
- [Audience Member] Okay,
and about your roadmap.
How would you prioritize the features,
because I'm very, very interested so,
how can we impact the way
the roadmap is going on?
So, the question is, how users can impact
what gets implemented first.
Well, issues, I don't
know, just submit an issue
on GitHub or write me
on email or in GitHub,
let's discuss it.
And I think, I'll prioritize
now the bigger features.
Like integration IDEs and tying to make
thread safe assertions,
but that will require C++11
and it will be conditional,
but I'm open to suggestions
on how to discuss what's next.
- [Audience Member] Okay,
did you plan or such,
to match some mocking framework?
The question is, if I plan
anything read to mocking
and yes, but I first
should familiarize myself
with mocking in general.
Because in my previous jobs,
we didn't do any testing
in mocking and I'm catching
up on many of these areas.
But, it is planned.
Also, property-based
testing is interesting.
I think it's related
to data-driven testing.
- [Audience Member] Okay,
thank you very much.
- [Audience Member] Is it
possible to register tests
and not in static insulization,
but after main is executed?
Yes, but it's not exposed,
but it is possible.
The question was, if you could skip
the automatic registration
of tests, right?
- [Audience Member] Ah ha.
Yeah, it can be, but it's not yet,
it doesn't have public API yet.
- [Audience Member] And
second question, do you have
any special considerations
for storing tests in libraries
that are dynamically loaded?
Yeah well, the question
is if tests can be stored
in dynamically loaded libraries.
Well yes, if you use
that identifier I showed,
I'll show it now.
This identifier, when you
build all of your binaries
and separate shared objects,
only one of the binaries
will export the test
runner and all of the rest
of the binaries, will have to link to it,
to share the same test runner.
Is this your question?
- [Audience Member] Ah ha, so I mean,
all libraries are loaded
like, we use deal open
or load library.
Yeah, I'm even using that.
I mean, in my toy game
engine, I have plugins
which are dynamically
loaded and they can have
tests in them, and it works.
- [Audience Member] Thank you.
- [Audience Member] Hi, I was
really curious about your use
of forward declares
and how you're avoiding
header-includes in general?
I was wondering if you could elaborate
a little more on that?
You kind of alluded to it,
in one of your first source code slides.
So the question is, to elaborate
on the forward declaration
stuff from the std namespace?
Or in general?
- [Audience Member] Well, in
general,'cause you would also
have some from your
own source code, right?
Yeah, so in the interface with the library
there are a bunch of macros.
I mean, two thirds of it is macros,
like 3,000 lines of
code maybe and the rest,
is a few forward declarations.
The translator stuffs, which
are, you know, templates.
They have to be in the header as well.
And there are a few
things from the standard
which are forwarded, correct?
So far, it's only std
ostream and the nullptr type.
Does this answer your question?
- [Audience Member] Yeah.
Okay, anything else?
Well, I guess we should close the session.
Thank you very much.
(applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>