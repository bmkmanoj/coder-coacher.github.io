<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Honggyu Kim “Understanding the runtime behaviors of C++ programs using uftrace tool” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Honggyu Kim “Understanding the runtime behaviors of C++ programs using uftrace tool” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Honggyu Kim “Understanding the runtime behaviors of C++ programs using uftrace tool”</b></h2><h5 class="post__date">2017-10-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s0B8hV2O8ps" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Hi.
My name is Honggyu Kim,
and I'm working at LG Electronics.
And the title of this topic is
Understanding the runtime behaviors of
C++ programs using uftrace tool.
I did the Lightning Talk last year,
and I'm doing the same tool and then,
and I'll be more focusing on the
analyzing the C++ programs this time.
But at the beginning, I will
show you the basics first.
Okay uftrace is an open
source tracing tool,
and then you can download it from Github.
Yeah, and uftrace is
created by Namhyung Kim.
And then he's one of the
active developers of
Linux perf profiling tool.
So this tool is just
like a perf-life usage,
so at first you have to
record all the function trace
and then you can analyze it
using the replay command,
or record command, those kind of things.
And you uftrace is able to trace C or C++
user space programs,
and library functions,
and Linux kernel functions inside as well,
and some of the system events.
But I'll be more focusing on just the user
space programs and library functions.
So first, the function tracing
is just like if there's
a source code like that
and the main function equals full,
and full course bar.
And you can just compile it.
And you get this kind of binary.
But if you recompile
it with minus pg option
and it serves all function calls
at the entry which function.
And uftrace uses these function calls.
And after compiling,
you can just execute it.
But if you put a uftrace record
at the beginning, you can
trace all the function traces.
And you can replay it.
Or just do the live comment,
it's just record plus replay.
Because the different command is live,
and then you don't have to put it.
This is a core chain,
this is a straight ID.
And this is a function execution time
for each function core chains.
So and if I put a library function course
in bar function, you can also trace it.
So in the bar, there is
another function call.
Getpid, but at the end
there is a PLT simple,
it means that it is not calling the
function inside a binary, and it is
going outside of the binary.
So there is a kind of table
and then uftrace just hook the table
to trace the library.
In that case, you don't have to recompile
your program with minus pid option.
Library can just be
traced without a problem.
But if you recompile with your program
with minus pid option,
you can also see the
user space functions as well in this way.
And you can also trace the
multiple processes as well,
just there is a fork and wait,
and getpid, those kind of things,
and then if you see, and there are
two different thread IDs.
And you can also trace the
Linux kernel functions.
You need to have route privileging,
and then if you put minus k option
you can see the output like this way.
And then in print f
there is a two page fault
and fflush, and there is a system com,
sys_write.
The next one is event tracing,
I'll just make it very simple.
There are many different types of events,
but if you put minus E in Linux schedule,
you can trace all the scheduling events.
It doesn't require the route privilege,
and then you can read
it from the user space.
And I'll introduce the filters.
Okay, if you put a minus D 2,
it limits the depths
of the function calls.
And if you put a minus F foo,
you will just filter only foo functions,
and its children.
And you can just hide some
specific functions using minus N.
This way you can hide it.
Or this is a time filter in this
because in a big program
there are too many small functions
you can just remove
all the small functions
using this way.
You can just put a time stress folder
and then on to the time stress
all the function traces just disappear.
You can save a lot of data
size as well using this one.
And report.
And after compiling it, recording,
and you can use a report command.
It's just a summary, the
entire execution summary,
and at first and then this is by default
sorted based on the total time,
but if you want to change it,
you can sort it based on the self time,
and you can sort it as
the number of calls.
The next one is a bit,
very small actually,
but I just wanted to show you that
there is another feature,
because uftrace can record
twice and then you make
a diff in the summary.
So if you examine it and you can see
plus, minus symbols and then
this is (mumbles) actually.
But just I'll skip it.
Okay, there is a Fibonacci example.
I showed you before, but
I'll just go very quickly.
It's just like that.
And then if you want to see the arguments,
and then you can use this option,
you can also see the return value as well.
This is a kind of a formal definition,
how you can specify the types,
because it doesn't require
the debug information,
you have to explicitly specify the type.
But if you don't specify the type,
it uses the default.
At first, you use symbol,
and then some specs,
and then for arguments,
and you have to specify
the number of arguments.
The place.
And there is a format as well.
You can also specify some
kind of string character,
or a city string as well.
And first you record it,
and then you can also dump it.
This Chrome option just provides this
kind of JSON format.
This JSON can be opened
in the Chrome browser,
and if you go to this site,
you can just open it in the browser.
This a built-in function
in the Chrome browser,
so it doesn't require
any other insulation.
This is the timeline,
this is call depth.
And some of the people can
just load it as a JSON file,
JSON format, but the better way to use it
is to translate it into HTML file.
You can go to this site,
and then download it,
and then you can just
convert the JSON format to the HTML file
using this way.
Then you can just download
it in the website,
and then it's a lot easier to read.
Or once you record it
and then you can make
a frame graph output as well this way.
This is a different Chrome tracing viewer.
This is not timeline based,
this just aggregated data.
So the next one is the, we can also
trace the pre-built binaries,
because we don't trace
the user space programs
in that case, but by
tracing some libraries,
and then we can still get
some useful information.
So let's say there is a trace user binary,
this is pre-built.
Then we don't have source code,
and then trace use
binaries just compiling it.
And then if we put it,
it just shows this kind of error
because this is not
compiled with minus pid,
or FN format functions.
If you put --force,
you can trace the library in this way.
But it shows only the
first level library course,
just like an air trace.
And if you put a minus T,
200 microseconds, you can see that
just the big picture of the execution.
If you use --auto-args,
this is an onboard GTN.
It is a working progress.
You can print the arguments
and written values automatically.
Because uftrace can understand some
well-known libraries so you don't have
to specify all the libraries and types.
It's not necessary in that case.
And you can also make
it as a Chrome format.
It's probably some kind of information.
Because at first,
cc1 just translates the source code
into the assembly file.
And AS translates the assembly file
into the object file, and then
at last, it caused a linker.
And then you as you can
see, the linker is PC,
in that case.
And you can also trace
the nested library calls.
Because as I mentioned,
by default it traces only the first
level library calls, but in that case,
and we miss a lot of library calls.
This is this option just follows
all the library calls,
inside the library calls.
So this is a cling binary tracing.
And then if you trace the cling binary,
you can see this kind of nested
library function calls.
This is also pre-built binary.
And so we can see the
function trace and then so
what's good?
So there is some use cases.
And then there are QT QML engine,
there is a bug and then so
some friend just fixed it.
And then he did a Lightning
Talk actually, so (laughs)
I think just only, I'll just show you
just the simple thing.
And then he found some king of buggy case,
and then there is a broken case
and the working case, and then he found
the difference and then he found
the exact function, and then
he just fixed it, and if you want to
know more, and then you can just watch
this Lightning Talk.
And the next one is just a C++ example.
The constexpr function.
Let's say there is this kind of example.
The Fibonacci number is calculated,
can be calculated at compile time,
because I put the constextpr.
But in this case,
this case can be
calculated at compile time,
but what do you think?
This can be calculated at compile time?
But if you see the
function trace, it's clear.
After compiling it, and if
you see the function trace,
it's just cause Fibonacci
functions at run time.
At first, the first one was
calculated at compile time.
But that's not the only problem.
If there is a constextpr,
the first one was
calculated at compile time.
But the return type is const int.
Not constextpr.
In that case, there is
some strange behavior.
If you compile the same
program with clang,
and then the first example is also
executed at run time.
This is kind of a strange behavior
between the GSCC and clang, maybe,
but if you see that function trace
and then you can see it more clearly,
there is another example, std string view.
Let's say there is an example.
Const character pointer is a pressed
into the one function.
And then the function just
accepts std string reference.
In that case, if you
see the function trace,
there is a basic string,
the constructor and destructer.
If you optimize it only just to
operate a new (mumbles) as shown here.
Anyway, the memory location is done.
So that's why the
C++ 17 has a string view.
If you pass it through the string view,
it makes some basic string,
it causes basic string constructor,
but if you optimize it,
everything is gone.
So that's why the string view
is necessary in that case.
So the next example is STL container
performance comparison
between the vector, deque and list.
There is a benchmark, like that.
We can just put a lot of iterations
and the type is STD string,
and then we can push to the vector.
And we can push it to the deque as well.
And then at last we can
push it to the list.
So there is an iteration count.
If you record it,
using these arguments,
and then you can see the callgraph
using the graph comment,
the output looks just like that.
It's kind of summation, and then it
shows the all the call chains and
okay, as we can see,
this is the execution
time for each core chain.
And this is the core chain as I mentioned.
And this is the number of calls.
So if you see the replay output,
a lot of patterns are repeated,
so you cannot see one single screen.
So the vector, as you can see,
the vector spends 2.1
seconds in that case,
and deque spends 700 milliseconds.
And list spends 2.4 seconds.
But the good thing is that you can
see the internal, the performance
difference in that case.
So there are a lot of
mental pieces in vector,
and then at first vector,
and then I colored in in red,
and spend a lot of time here.
I'll show you the difference.
And the vector pushes back,
if you see the replay output,
at first it locates a
(mumbles) device buffer,
and then it does a memcpy
because I could push the hello string,
it's five bites.
And if you do the push back once again
and then it locates a six bytes
and it doubles the size, and the it
copies the original
memory to the new buffer,
and then geolocate it.
If you push it once again,
and then it doubles the size,
and then it copies the old original
contents to the new buffer
and then it repeats over and over again,
and then the last example is
memcpy, called the five.
This is just the pushback was success
without the memory
geolocation in that case,
because the STD string is the
size of STD string is 32 bytes.
So if you see, the number
of memcpy's are increasing.
Repeatedly, if you go more,
and then most of the calls are memcpy.
In deque case, it's a bit different.
If you see it here and the memcpy's
are kind of stable, and then
there is another memory location here,
because deque, the data
structure of deques
are a bit different, and then if one
buffer is full, they will locate
a new chunk of memory,
and then are located, they don't copy
the original contents to the new buffer,
that's why they can save
a lot of memcpy here.
And their pattern is quite stable,
and then it's the same,
that's why they can save a lot of time
compared to vector in that case.
In list, whenever the pushback happens,
and then they have to locate
the 48 bytes of memory,
including the pointers,
and they copy it.
And then as you can see the number
of mallocks is the same
as the iteration number.
That's the problem, but I
would just show you another example.
I push it just integer instead of string,
in that case it's a bit different,
because if you see the call graph,
integer is, it's just a trivial copy,
so it doesn't have to just call bank copy
over and over again.
It can just call just one single memory,
when they are copying
the original contents
to the new buffer, just memory's enough.
That's why the difference,
the performance difference is
not that high in that case.
Okay done, next example is the
in C++17, there is a parallel algorithm.
So I just downloaded recent G++,
and there is a sort repair.
So you can just exclude this example.
It's based on the open MP.
You can see this kind of output.
As I mentioned, this is timeline,
this is the call depth.
And these are the different strengths.
So as you can see,
maybe you can get some ideas how
each is (mumbles) interacted in that case.
Maybe ...
Okay.
All right, this is ...
It's just loading.
Right.
This is the Chrome trace.
You can just zoom in that case.
You can zoom out.
You can see the multiple
frames in that case.
This way.
I executed this algorithm in some
(mumbles) which has 40 calls,
that's why it is a lot of stress.
You can get some ideas by just
seeing this kind of tract.
Okay the next one is the,
we recently added the
Python scripting feature.
So now uftrace is able
to run the Python script,
so one of the functions is entered,
and then you can run some
specific Python script,
or whenever the function is returned,
then you can also run it,
but to reduce the performance all right
and then Python scrip there is the
data and then you list
up some of the functions
you specify, the specified
functions that are executed.
The Python scripts are invoked.
Accepted and then it doesn't
go to the Python script.
It's called the function filter.
And script can be executed
during record time,
and in that case, and then you can
get some more data, more,
you can read some kind of systems tests,
or something like that.
But it hurts the original proponents,
so it's a bit slow, and then this way
we recommend to use it
for the recorded data,
so there's a separate command called
uftrace script, you can use this command
to read the recorded data,
and then you can also, the process
function entry and exit in the same way.
It provides four functions,
and uftrace entry, it's invoked
one of the functions it entered,
uftrace isn't invoked whenever
the function is exited, and then
begin and end, those
are executed only once.
And there is a simple example.
Count variables initialized as zero,
and whenever the the function is entered,
and then we increment just,
and then when the program is accepted,
and then we print the count.
We can count a number of functions.
Function calls.
So if there is a simple example,
it can be the call traces
just looks like that.
If you use this command,
uftrace minus capital S,
and then you pass the Python script here,
and then there (mumbles)
flows just like that.
And then at first the Python script,
uftrace begins executed,
and main function is executed.
And main function is entered,
that's why the Python script uftrace entry
is executed, and full,
uftrace entry,
and by the returned anyway so
uftrace exit is executed this way.
It comes over and over again.
And then finally, it calls a uftrace end.
It looks like that.
Then you can print the first three.
In here, uftrace entry and exit
accepts arguments, so their arguments
can contain those kind of informations.
Read ID and call depths times 10,
and function execution time,
and address and symbol name,
and some specific arguments
and written values.
So there is another example.
I just traced, just (mumbles) node.
Node GIS.
And I passed one single Python script.
And the Python script, there is some
kind of comments in
uftrace option in here,
and then so we can write some kind
of uftrace record options.
In that case, you don't have to do
explicitly specify the options outside.
You can specify in some
of the informations
inside the script.
In that case, you can
draw this kind of chart.
Using the matplotlib.
Maybe this is just a (mumbles),
those kind of examples, but
the pertains that you can use your
own functions, in that case,
and then just (mumbles) by log and freeze,
that's kind of a fairly common technique.
It's not that new, but you can use
your own functions.
It's in that case.
Thanks, and
do you have any questions?
(audience applauds)
All right, yeah, thanks.
We have five more minutes.
Yeah?
- [Audience Member] So uftrace
is instrumenting profile using minus PG?
Yeah.
- [Audience Member] What does it compare
to core, which is the sampling profiler?
What thing is the advantage of uftrace?
The pof is a systemized profiler.
And then it's sampling based.
And so it doesn't hurt the original target
program's performance
but it's less accurate.
And then it can account a number of calls,
and then it can print a function
argument and return values.
Those kind, it has those
kinds of restrictions.
- [Audience Member] So uftrace is better
if you want to get an accurate timelines
which doesn't miss any function arguments?
Whereas, the overhead, especially for
line functions, or small functions
is probably larger (mumbles).
So that's why we recommend to use
the time filter, and then to
remove the small functions.
But there can be some small functions
which is very important.
In that case, you can explicitly specify
some kind of triggers to pin the
specific functions, in that case,
even if the time stress is,
even it takes just a very short time.
We record the trace, in that case.
Yeah, please.
- [Audience Member] What
kind of performance hit
do you take from using uftrace?
I'm sorry?
- [Audience Member] How much
of a slowdown do you get?
It depends on the number of records.
If you record every single function calls,
and then it's kind of slow,
but we did some kind of optimization
inside, and whenever
the function is entered,
and we don't directly write to the file,
and we first locate the shared memory,
and then we just write, records
the shadow memory first
and then in the background,
uftrace program is just gathering
all the memory and then
write the file as a chunk.
And if you use a minus T option well,
and then you can reduce a lot of data.
In that case, and then
slow down is nothing much.
If you are curious about it,
I can just show you some kind of example.
Just, Clang's a big program, right?
Just Clang be traced.
Uftrace just like, we can
just compile Hello world.
In that case, uftrace is recording
every single function calls in Clang.
It may take some time.
But I will use the time filter option.
So now it's done.
If I use a time filter, it's a lot less.
Need to make a space.
Yeah.
In that case, I remove all the function
calls data that is under one millisecond.
You can see just a big picture.
Right.
This is the entire execution.
This way.
Every function traces
over one millisecond in that case.
Yeah, please.
(audience member asks question)
Yeah, it understands heretics,
so you can just put the first
name space and then just
a dot star, that's it.
Yeah.
(audience member asks question)
I'm sorry, I cannot hear you.
(audience member asks question)
Ah, MPI programs.
You mean is multi-process programs, right?
It's separately executed
in multi-process programs.
So we are targeting only
single process programs,
and then so if the single program
creates some fork, and then we follow it,
but basically we don't
trace multiple traces
at the beginning.
Yeah, so we can make it, yeah,
we can make it in there.
So we can merge it, the data.
I think it's that time is over,
so maybe if you have questions,
I can answer here.
Okay thank you very much.
(audience applauds)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>