<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: P. McKenney, M. Michael &amp; M. Wong “Is Parallel Programming still hard? PART 2 of 2” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: P. McKenney, M. Michael &amp; M. Wong “Is Parallel Programming still hard? PART 2 of 2” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: P. McKenney, M. Michael &amp; M. Wong “Is Parallel Programming still hard? PART 2 of 2”</b></h2><h5 class="post__date">2017-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/74QjNwYAJ7M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- [Paul] So, we've talked a
lot about Laws of Physics,
and how they are inconvenient
for hardware people
and thus inconvenient for software people.
But you know, one thing about laws,
almost all laws have loopholes, okay.
And the Laws of Physics are no exception,
there are some loopholes
we can take advantage of in some cases.
tUnfortunately, now like
loopholes, and normal legal laws,
there are limits to these loopholes,
but still they are loopholes
and we should take full advantage
of them whenever we can.
So, let's go through some of them.
One really cool one,
that I have taken shameless advantage of
over a period of more than 25 years,
is the fact that read-only data
is replicated in all the caches.
So, if a whole pile of different
CPUs read the same data,
that data gets pulled into
all those CPUs caches,
that means all the CPUs have
high-speed access to that data.
And we show that here, we've
got the little black squares
that are in that little green area.
The same variable just everywhere,
and so all the CPUs
can get access to that.
I mean of course that make
things kind of strange,
especially if you are as old as I am,
I grew up in a at a time when
we had this thing called
UNIX, is before Linux,
they ran it on one of these fancy PDV 11s.
the thing was, you had
to configure by hand,
your kernel to exactly what
hardware you had, exactly, okay.
And that meant that if you didn't
compile in the fact that
it had a floppy drive,
you couldn't access your floppy drive,
and then maybe your system
wouldn't boot either.
You had to compile into the kernel,
where the swap rendition was.
In fact you had to get
three different places,
in the kernel corrected
where the swap rendition was
otherwise it would do interesting things.
I know that because I only
found two of them once.
And then in fact if you forgot
to mount the floppy drive,
and you try to use the floppy
drive, that was a kernel panic
which usually irritated everybody else
who was trying to use
the machine at the time.
I know that because I did that too.
Well, things have
changed, what happens now,
you got a laptop, you slam
a memory stick into it,
and memory sticks here.
You can off-line CPUs
if you are using Linux
probably Windows too, I don't know.
You can connect to the network,
or disconnect to the the network.
Stuff could like appear and disappear,
And the system just notice it.
It's amazing what can you do,
if you have if you have more than 64 k
of address space alright.
Well, what this means is
there's a lot more data,
that almost never changes.
There's as a bunch of
data that keeps track
of what hardware you have
because the kernel is built just to take
whatever shows up mostly.
And so, it has to keep track
of what do I really have,
vs. what I might have.
And that could change at any time.
You can throw a memory stick into it,
or pull a memory stick
out anytime you wanted,
but you don't do that very often,
and so that data if you are
really, heavily using your IO,
would tend to be replicated in the caches,
and your access to those data structures
describing what the system
had and didn't have,
would be quite fast.
So, what's happened is the larger memory,
has changed the way that the
machines actually operate.
there's much more data,
lying around in the system
describing what's there,
whereas the old days, that was all code.
There was either there or not compiled in,
you had to very carefully
select what you had.
Life's much better this way,
than it was then, believe me.
Okay, and then Real Estate,
of course the three most important things,
are location, location, location.
And in parallel computing, is very similar
locality, locality, locality.
If you can make it so that you
split your problem up nicely,
so that the data can be
local to a given CPU,
or given socket, or given system,
your life is gonna be much better.
Things are gonna go much faster.
Okay, one question is,
well, geez you know,
we got all this hardware,
we got all transistors,
can't the hardware help us?
And in that it does quiet a bit.
And these are just five
of the larger big animal
optimizations it does,
Big caches of course is
a very popular thing,
you've seen these things grow
from small amounts kilobytes
to many, many tons of
megabytes these days.
We'll talk about Store
Buffers, Speculative execution
Big Cachelines, a kind
of a two edged sword,
in fact lot of these,
optimization in general,
is kind of a two edged sword,
and we'll see that some of these are too.
Cache prefetching is another one
that can be very good or very bad.
Alright, so the good thing
about Big Cachelines,
so you got CPU 0, he
wants to read variable a,
well variable a, happens
to be in Cacheline
down there at the bottom, is
gotta be a c, and d in it.
That said CPU 1 right now,
cause maybe CPU 1 was the
last one to write to it.
So, CPU 0 says, &quot;Hey, I wanna read a,
&quot;give me the cacheline.&quot;
And then that's the arrow coming down,
so that's why remember we had
over and back speed of light,
well that's the over part,
and then CPU 0 wants zero, yeah
Cacheline, okay here it is.
And that's the back part.
So there's this big
delay, this Long latency
between the time that CPU 0 says,
&quot;Hey, I wanna read the value of a.&quot;
And the request goes out for
the Cacheline containing a,
and that cacheline comes back
and now wants us back there,
CPU 0, actually can read a.
The cool thing is that if CPU 0,
was next gonna read b, c
and d, do it right now.
That's right there you've got it,
almost no additional latency
to get the rest of that
data in that cacheline.
So, I think a really big cacheline
would mean you could just
get all sorts of data,
if you pay the price once,
that speed of light is gonna get you once,
but once you've paid that price,
you get your money's worth,
the rest of the data in the cacheline.
That's a good sign.
There is also a bad and ugly side.
Now could be that we've
got a, b and c in there,
and CPU 0 wants to write to a,
and CPU 1 wants to write to d.
Well, they are both of
in the same cacheline,
and so that cacheline is
gonna ping back and forth,
between those two CPUs, and
this is called false sharing.
How many of you people have
heard of false sharing?
Or like run into it?
How many of you people have had to beat
their head against the wall,
to figure out what the heck
was being false shared and fix it?
Yeah, yeah, you know
what I'm talking about.
And the bigger you make cachelines,
the more likely you end
up with this situation
where several CPUs are fighting
over the same cacheline,
because they wanna modify
it at the same time.
Alright, so it's an optimization,
it's the same as all
software optimizations.
You know, you use a
hash table that's fast,
on the other hand you don't get
to do rain searches anymore.
It's the same kind of
trade off they are facing,
that we are facing in
software all the time.
Alright, so Big Cachelines,
they can help (mumbles)
but they are getting larger.
Alignment Directives are what
we do to get around that.
So, what we could do here,
is we might use Alignment Directives
to make sure that a, and
b, are on one cacheline,
and c and d, are on another.
Once, we've done that,
well CPU 0, still takes the Long Latency
to get that cacheline
from CPU 1, to begin with.
But once that's happened, it
can write to a, all it likes,
it's just right there.
Likewise, CPU 1 has c and d,
and it can do whatever
it wants there as well.
So, the big way to get
around false sharing issue
is the alignment.
Okay, that's really nice.
We've also got Prefetching,
which should also be really good.
It was good when we picked up a,
and got b, and c, and d for free.
If we have Prefetching,
we can still have the two cachelines,
and maybe have them with different CPUs,
but we can also, if they
are both at the same place
and clearly the other guys
are going consequentially.
We can send both of them at once,
we kind of cheat the speed of light
with the second cacheline.
Because we anticipate that the
other CPU is gonna need it.
And so then it reads a, and
then waits for a, to show up.
And then it gets to read all
of b throughout, like that.
Cause, the prefetching happened.
And that's wonderful.
Except that it's got the same downside,
don't get me wrong, hardware
these days has really,
really clever prefetching heuristics.
And they would generally
avoid this sort of thing.
But, you show me hardware
with whatever heuristics,
and I'll show you software
that will break those heuristics, okay?
I mean that's the way of life,
you win by the heuristic,
you lose by the heuristic.
And you can end up with this sort of thing
where they fight for a
little bit, you prefetch it,
but you shouldn't have
sent the circular one over
cause he's not gonna go to it,
and you need it back
where you will start from,
and you can take an extra latency,
because of that prefetching.
So, it's an optimization again,
it can work very well on some
cases, it can hurt in others.
Store Buffers.
Well, one of the problems,
if you just store,
and you didn't have a Store Buffer,
you would try to use the store,
you would have to wait for
the cacheline to come to you,
and in this case the
cacheline in the CPU 1
and would have to get to CPU 0.
And, you'd have to wait that time.
What a store buffer
allows you to do is say,
&quot;Okay, we are gonna store this,
&quot;so we'll just record the
address and the value,
&quot;in my little Store Buffer locally,
&quot;well as for the cacheline,
&quot;it will get here sometime or another,
&quot;and when it gets here,
then we'll do the store.&quot;
And the cool thing about
that is the CPU can proceed
with these other operations,
while the store is pending,
and some machines have
really big Store Buffers,
many thousands of entries.
In fact, on some machines,
they call it the Level 0 cache,
it's really a giant Store Buffer, okay.
Depending on some machines (mumbles)
there are some machines,
they call the Level 0 cache,
is really a huge store buffer.
And that's really nice,
but as you might expect from
the previous several slides,
this is only half of the story.
This is a big source
of memory misordering,
and while we have to
have these memory models
and everything else.
Because let's suppose the CPU 0
writes a:(42), it gets
stuck in the store buffer,
and then it goes and it
does much stuff and reads a,
well CPU 0 says, &quot;Oh, yeah well I got that
&quot;in the Store Buffer right here
is 42, and I destroyed it.&quot;
And so as far as this is
concerned, it wrote 42 to a,
before it read from say e, okay?
The other CPUs though,
they don't see that right?
Until the cacheline gets
there, and gets back to them.
So, there's gonna be a disagreement
about whether the store to a,
or the read to e, happened first.
CPU O, knows it stored a,
first and then it read e.
But there is no evidence of that store
going to any other part of the
system, until months later.
And that means the
other CPUs think that a,
happened after e, okay.
And this is even the
tightly couple systems like
excuse me, tight memory models,
like x86 or the mainframe
or Spark or, any of the
other ones that are TSO,
they do this, because
they have Store Buffers.
Because the Store Buffer optimization
allowing system recede,
allowing the CPU to proceed,
with these pending storage
that's so valuable,
they can't do without it.
Okay, well what this means,
is we have to use order and directives,
in C 11 of course, these
are the memory order,
you just get subsequently
consistent if by default,
and you have a memory order requirement,
or memory order release
memory order at parallel,
memory order that's consumed
and memory order relaxed.
Although relaxed isn't
gonna help you much here.
It's not gonna do much ordering for you.
Okay, and so what's happening?
This is one of the cool
things about C++ Memory Model,
is it's allowing pretty much
for the very first time,
you could argue a job you
got there first, okay.
But it was only subsequently
incident earlier on,
it added the weaker
options after C 11 did,
The really cool thing
about C 11 Memory Model
is it allows portability for ordering,
and pretty good performance for it.
I mean you might be able to
write tightly code assembling
get a little bit better
but in many situations,
just using a straight thing,
I figure we'll be talking about later,
it allows us to do that,
and what we have is, we have the hardware,
which is often very different.
Power PC, titanium mips,
x86 mainframe and so on
have very different
hardware memory models.
We have a Toolchain on top of them,
and that Toolchain is responsible
for providing the C 11,
C 14, C 17 memory model.
And so unifies, provides unified
view of all that hardware.
and that means it allow us
to do some of these ordering things
in a portal (mumbles) and not
lose that much performance,
in some cases maybe none,
maybe lose no performance.
Okay, so next thing we'll talk about,
is Big caches and Speculative execution.
Big caches actually work pretty well,
it's hard to find fault with it unless
you happen to be one of these people
that are using a battery-powered system,
in which case the bigger your cache,
the shorter your battery
lifetime, up to a point.
But even so, the people doing those things
has gotten very clever
about turning the caches off
when the system is not being used.
So, it helps a lot, you
have the bigger the cache,
the more data it fits in the cache,
or the closer it is to the
CPU, and life is better.
However, if you have bigger caches,
usually you have to have
a taller cache hierarchy,
the bigger the cache,
the more time it takes
to figure out the amount
of the cache you want,
and so that's why is this L0,
L1, L2, L3, sort of a thing.
And of course that means
you have to check more levels of caches
you're trying to figure out whether
the thing is in the cache,
and your caches alignment
seeking is getting larger,
so it's an optimization
there are trade-offs ,
but it generally works pretty well.
Speculative execution, is
also you can hide latencies,
we'll see an example here in a bit,
but the trick is that
one thing you could do,
is every time you see some
kind of ordering directive,
the CPU says, &quot;Okay I'm gonna
wait until I until I know,
&quot;the stuff I did before
is visible everywhere,
&quot;and only then will I go ahead.&quot;
Well, that might've been a
successful strategy in 1990,
but if your hardware player today,
that's probably not a winning player.
What they tend to do, is go
ahead and execute anyway,
and then if they see
evidence, that somebody saw,
or had the potential to
see, something backwards.
They cancel the speculation
and start over more carefully.
Now, what that means
is that in many cases,
you can hide the latency,
you otherwise see
from directives required
to enforce Memory Ordering.
The downside is that you
have worst-case latency
because you may have
to do work, cancel it,
and do the work again,
possibly in multiple times.
And also doing that work multiple times,
is going to give you
poor energy efficiency,
and so again the battery powered people
would be perhaps less aggressive
about this sort of thing.
Here's an example, there's
this one in the back there.
This is a standard message
passing type of a thing
you've probably seen it before.
So we have a data and a flag,
we set the date and do the the value,
and we set the flag to one to
say that data is available.
And we have memory order release there,
to say make sure that the flag happens
after all that stuff before it.
On the other side we have a while loop,
usually wouldn't spin
waiting for the data,
but let's keep it simple.
So, while the flag is not
set, we sit there waiting.
Once the flag is set, we pick up the data,
we use the memory order
acquire for the flag load,
and therefore the data is
fetched after the flag is red
which means we get 42,
we get the value stored
in the data reliably.
If we left those directives off,
we made them both to relaxed,
you can actually their tool sets allow you
to run that on the machine and
see what happens on the x86.
It won't work too badly on other systems,
you will see things get out of order,
and you'll see the flag being one,
and the data being
pre-initialization garbage,
that was in it before.
The thing is though,
if the system is aware,
if both the cachelines, both flag and data
are sitting on the first CPU,
and those that are there
nobody has access to them,
they can do those stores
in any order it wants,
because nobody can see
the intermittent state
unless it lets them, okay.
Similarly, on the other side,
if both the cachelines are sitting there,
and it knows that nobody
else has access to them.
Again, it can do the reads
in any order it wants,
because nobody else can be changing it,
because it's got the only copy.
So, the hardware can look
into the cache current protocol,
the protocol used to
keep the values constant
to divide the memory
model across the machine,
and can determine when it's safe to cheat.
Do things out of order, and
if it senses the condition
that might allow the software
to see something bad,
it can cancel the
speculation and start over.
A lot of people get kind of annoyed
because I tell them
Lokomohaila can't do something
without messing me up as
a parallel programmer,
well the hardware doesn't.
Well the reason a hardware
gets away with it,
is cause the hardware
has a lot more visibility
into the state of machine
and it can sense when
something is going bad,
and it can pull itself back out.
If you've got a compiler that
can do that too, okay great,
but until you do, there are
some things the hardware
can get away with you can't.
Okay, we'll take a look at
Maligned Workhorse: Locking.
This is kind of leading to somethings
just some simple things lead into
Maged's presentation a little bit.
This is stupidly simple lock acquisition,
but it's one that has
been used in production,
you just do a exchange with ...
And I apologize for the C,
but I managed to fix
it on a previous slide,
but didn't notice this one.
I'm after all C programmer,
working in the link kernel.
So we spent waiting for
the value not to be zero,
Basically the idea is to
locking starts out being zero,
that means nobody holds it,
You do an exchange, you
take whatever is there,
and put a one there.
If there's already a one
there, you get a one back,
you know somebody already
held it, so you do it again.
If you get back a zero, that
means that nobody held the lock
and now you hold it, and the value is one.
The release is pretty straightforward,
you do an atomic storm
memory order release
of zero to set it back,
An this good amount of lock,
it has really, really, really terrible
high contention behavior.
If you do this, if you
(mumbles) lock this way,
it's on you to make sure you
have almost no lock contention
because if you do have lock
contention this is gonna hurt.
But still, this is
something that you can do.
We can make something a little smarter.
The problem we were doing exchange
over and over and over again,
so we were just banding cacheline around
through all the CPUs,
and even if some guy gets the lock,
and tries to release it,
it's probably somewhere else
and it's hard.
What we can do, is we
can just do a wild loop,
and just essentially it's been waiting,
and only once we see that it is released,
then we try to do it.
And I think haven't got those
files in this thing right,
but I'll fix that later.
So what we're doing is, instead
of doing remodify right,
on the cacheline each time,
we're spinning waiting
for the value to change.
And then once we see we
have a chance of getting it,
then and only then do we do
the atomic remodify ride operation,
that yanks it out of it's real cacheline,
and tries to acquire it, this
is still sub-optimal though.
And an example of where it can go bad.
Thing is that you lock it,
and they see it's locked,
and CPU 1 sees that it's held as well.
So, CPU 1, it should be CPU 2, sorry,
The thing is that CPU 1
knows it can't lock it,
but it's gotta read copy of the cacheline,
that means the CPU 0 can't release it,
until it gets exclusive
ownership of the cacheline.
Because you don't want the same variable
to have multiple values
in different parts of the machine usually,
not with atomic instructions.
It will be okay if it stores,
they can have the store
in different buffers,
that will be alright, but
this is atomic remodify ride,
and it's suppose to look atomic so,
everybody has to agree on the value.
Now, what happens is that when somebody
does unlock there's a
whole bunch of bus traffic
that has to happen
for the cacheline to get
yanked out from the readers
and get sent to the guy
that wants to release it
so that the readers can actually get it.
And one way to fix this,
is use Queued locks,
and the idea is they
actually make a queue,
I'm not gonna go through
the code in detail,
we don't have time for that.
There is a paper a year
along this post slides
you can look at it with the kind of the
fundamental paper on the MCS locks,
but the idea is you make a
queue out of these things
so that you got the CPU 0 holding lock,
so we've got three states and
two transitions between them.
CPU 0 holds the locks,
CPU 1 is spinning on its
own little node there,
and CPU 2 is spinning on it's node.
So, when it's comes time
to release the lock,
CPU 0 has to fight only with
CPU 1, not all the CPUs.
Of course there's only three CPUs,
they are saying so what?
I have had problems on
systems with 4096 CPUs,
and believe me, if you have
4095 CPUs fighting with you,
you're in a world of hurt,
and so things like this
can make it work better.
And then once it hands it
off, CPU 1 holds the lock,
and then it hands off CPU 2,
and again you have most two CPUs
contending for a given location,
and life is a lot easier
in a high contention realm.
You know what except there's
one thing that's even better
and that's not to have high contention.
The cube locks usually have
a little bit more overhead
and being set up, you need
to do one more operation
and that caution the low contention case,
if you can make your algorithms
just have low contention,
you are in a lot better position.
In addition, just spinning all the time
may not be what you want
especially user space code,
at the current I get away with it,
because I've gotta hold the CPU
and it's not going anywhere.
In user space code, you can
block while you are spinning,
you can guard holding it lock get blocked,
and horrible things gonna happen.
So, you really would like an
adaptive spin/sleep strategy,
this is one of the things
about throwing in the slides,
I could talk about this for a long time.
But one thing you do
is have the algorithm use futex system
calls in Linux kernel, other
systems have similar things.
In order to spin for a little
bit and then go to sleep.
So, that if it's available
right now, you get it.
Or it's available soon you get it.
If it's gonna be a long
time you get out of the way
of the people that have it.
Okay, Reader-Writer Locking,
it sounds wonderful.
Read-side parallelism,
you know if you have
read mostly stuff use a Reader-Writer Lock
and life is wonderful, alright.
So, what you normally
have for these things,
there's been papers,
there's been a huge amount of ink spilled,
on how you can make different
kinds of Reader-Writer Locks.
This is just kind of a
canonical single variable thing.
And there's a bunch of variations on this,
the flag bits they have all
sorts of things in them,
but you have some number of Readers,
you have some number of writers,
you have some Flag bits for
fairness, if you want to Read,
you at the very least
have to say you're there,
you have to increment the
number of Readers in the word,
and because you have multiple
of people doing this,
this needs to be a
Read-Modified right instruction
that manipulates it in one way or another,
might be in comparerance swap,
might be atomic fetch and
add or something like that.
And the problem here
is that the readers are
having to Write to memory,
Write to contented memory,
in order to announce their presence.
And so you end up with
something like that,
every time you go and get the read lock
you get the Cache Miss.
Now, if you remember from
our slide way earlier,
Cache misses from one socket to another,
are way worse than two hours
of magnitude more expensive
than just a normal operation.
And so unless you have really, really big
read-cycle critical sections
with thousands of instructions in them,
you've taken a lot of overhead.
In fact it's likely that you might as well
use exclusive lock because
by the time you acquire
and release the lock and do something,
you weren't actually overlapping
with anybody else's critical section,
because the critical sections are short,
the acquisition and the
release of the lock are long.
So, one thing we can do,
is what it comes down
to the key thing here.
We mentioned that the hardware people
are up against the Laws of Physics,
and that means they need
a little bit of help
from the hardware.
There's a bunch of ways of doing this,
I'm just gonna give one fairly simple one,
and that's a Thread Local Storage variant
of a Read-Writer Lock.
What we do is we give
each thread it's own lock,
so you do a read-lock by
acquiring your threads lock,
and only your threads lock,
and you release it by releasing that lock.
So, if everybody is reading,
they are acquiring each their own locks,
their lock remains in their cache,
and they get fast access to acquiring
and releasing that lock.
Yeah, there's memory varied overhead,
there's atomic instruction overhead,
but you don't have Cache Misses.
Of course, this is like
any other optimization,
for this to work the writing
threads have to acquire
all the locks for all the threads,
and then release all the
locks for all the threads.
So, it makes it things
very nice for the readers,
and too bad for those poor writers.
And is kind of a picture
of what's happening
instead of having this
big cache Miss explosions,
like we had on the slide
a couple of times ago,
each CPU is just looking in its own cache,
and things can go very fast.
Again, what we've got something
is we've got something
that is a Readers-Writer Lock,
that works very well for the readers,
but we've lost a little bit of generality
because the writers get
such horrible performance
especially if they have
large numbers of threads.
And that by the way is one of the reasons
that Maged Michael came
with Hazard Pointers,
and I came up with RCU to
deal with that situation,
but that's another presentation
and we've given those before.
At this point it's time for
me to hand off to Megad,
and he's gonna give us a single-producer,
single-consumer buffer, thank you.
(audience clapping)
Let go of that and ...
- I selected really a
simple data structure
with a very simple interface
to be able to dive deep
into how we design it,
how we deal with the trade-offs
between performance
complexity and generality
and to apply the concepts
and the techniques
that Michael and Paul spoke about,
and complement some of the issues
that the were not covered.
So, basically it's
a single-producer, single consumer buffer
is for communication between
a single-producer and a single-consumer.
It's a buffer with a bounded size
and this is the interface
that we want to provide
at least for now, I'll
get to blocking later.
So, this is a non-blocking interface.
the simplest implementation
would be a single lock.
So, there's one lock
that each of the operations
has to acquire and to
operate in this structure
using just the sequential algorithm
that you would expect
if the thread is just operating
by itself under the instruction.
So, this is what it looks
like, you have the buffer
and the pointers, but
then you have the lock,
that's the only thing that
is allowing concurrency.
So, the problem with this is
that there's no parallelism.
Like the producer cannot
operate in parallel
with the consumer.
Another problem is that the lock itself,
whatever implementation, like for example,
what Paul showed,
both threads would be writing to it
back and forth and the
cacheline will keep bouncing
cause they are both writing to it.
It's enough that one of them is writing,
but they're both writing.
Another problem is that
it's lock acquisition
requires either a read
modified write instruction,
or a fence,
practically it is read modified
write atomic instruction.
So, that's the least of the problems
but it is still like a (mumbles) slower
than regular loads and stores.
So, these are the problem
with that single lock,
and around that and the
measurements on the machine
on a single socket with 2.2 gigabytes
is like 4.3 million handoffs per second.
So hopefully we can do better than this.
Of course this solution is very general,
like you are just using standard language
and nothing not portable at all.
Now we get to Lamport algorithm,
a classic single-producing
consumer algorithm,
instead of the lock it is dealing with
this kind of atomic variables
and has all these loads and stores to it,
that can go in parallel
and this just gives us
the safety of this ideal structure.
So, let's look at what are the features
that we get from this?
We lost the lock, we don't
have the lock anymore,
with all the negative things about it.
the suspicious single lock,
it's not fine-grained locking,
it's actually course grain locking
with all the negative things about it.
So, we'll see from this,
that there is parallelism
there's nothing really prevents
the producer and consumer
from proceeding in parallel, that's good.
The other thing, let's look at all this
upload and store operations,
they are load acquire
and store release.
these are quite cheap on x86
and not as expensive
as full fences on the Exit
6 and other platforms.
But there's a caveat, I
mean these instructions are,
yeah, they are fast,
but only if you have good cache locality.
The problem here is that
we have cash line bouncing,
and that's because we have data
that is being written by one thread
and read by the other
threads and vice versa.
So, this is the downside
of Lamport algorithm.
But it is much better we have,
as I said there's parallelism
and we're avoiding the read
modified writer instructions.
So, what we get in
performers, we improved a bit.
Let's see if we can get better performance
at the cost of more complexity
and less generality.
So, there's that written
by Giacomoni et al,
from 2008, and basically
they switch things.
So, instead of the pointers being atomic,
they made the pointers like
you know they are private.
but instead the buffer itself,
the items in the buffer
itself, each is atomic.
And this example, just to simplify things
I made it like a pointer,
but it's quite easy
to change it to general data with a flag
like what Paul what showed.
So the features here, we
we do have parallelism,
we didn't lose that.
We have fast instructions as
you can see the load and store
are acquired under released,
and we have no cache line bouncing
because you look at the
producer is only dealing
with tail in a pointer,
and the consumer is managing
the the head pointer.
So, there's no cache line bouncing.
Another thing is that
we have improved false
sharing, less false sharing,
and that's because the price we pay here,
is like we have to do something
more or less general
of like aligning the data to cachelines
to avoid false sharing.
Without that, we do lose performance.
So, let's look at the
performance results that we get.
If we didn't do an alignment,
actually we do worse than Lamport,
because the head and tail,
even though they are managed separately,
they are actually, if they
are on the same cacheline,
there will be false sharing
and cacheline will keep bouncing.
So, by aligning them separately
to avoid false sharing,
we are able to double the
performance of Lamport.
So, the lessons we learn from this,
is that we can achieve high performance
at the cost of the high complexity
and reduced generality.
And we look at our cache locality,
false sharing, alignment,
these kind of issues.
So far that was
non-blocking like basically,
the consumer had to be spinning,
even if it is doing
like pausing
but still consuming CPU cycles to await
items produced by the producer.
so not all applications
can the utilize that
and some might need to
have blocking consumer.
So, we look at this and
the first thing we look,
is like okay we'll use a single lock
with condition variables.
So, this is kind of like
the straightforward solution
and we added the condition variable,
but then the problem with this is that,
we have a notify a quite expensive
step that is in the critical
path of the producer.
So, that's the downside
and the performance is quite bad.
It's because of the single lock
is the non-blocking implementation
is quite expensive,
but it is worse.
Let's do something kind of more complex
with Futex and that's so
now we are losing generality
is not something that is
available in every system,
and it's not like
language supported.
So, let's apply to Giacomoni.
I mean I already applied it to Lamport
but I'm not gonna show that.
So, the difference here
is that we added a Flag
or atomic variable to
the items in the buffer.
And every in queue operation,
it will wake up the consumer
in case it is asleep.
It doesn't know and the state
is like either empty or full.
So, as we see here that
the steps are similar
to the align locking
but then what is added
is that now the producer
would wake up the consumer.
The consumer doesn't
necessarily always go to sleep
it's only when you needed it will.
So, it's not in the critical path.
So, let's look at the performance here,
and I applied it to Lamport,
and it is slightly better
than the like condition variable,
and Giacomoni like it is
really slightly better
but really not that much.
Let's apply something a
little bit more complicated,
but we want to avoid
this very expensive step
of waking up the consumer
from the critical path.
So, for this we change the algorithm
to use three values instead of two,
and it's conditional.
so the variable is being
set by both the producer and consumer.
As I was showing these are
a little bit more expensive,
or complex, but then the
wake-up is only conditional.
It's being selected
when to wake-up the consumer.
So that will be a good advantage.
There is a slight disadvantage here,
because both the producer and the consumer
might be actually updating
the state concurrently
then we need to have
read-modified write operation.
So this will limit the
gains that we can get.
And it is in the critical path.
So, we have compare exchange operation
that the producer has to do every time,
just to make sure that they didn't miss
whether the consumer went to sleep or not.
Let's look at the
algorithm for the consumer.
It's basically, it's doing everything
and only when like after it gives up,
and it doesn't want to
wait, it with block.
But it doesn't just go straight to wait,
it has to actually make
sure that the stage is set
using a read-modified write operation,
in case there is kind of a race
condition with the producer.
Again, this is not in the critical path,
so actually it doesn't
affect performance that much.
Now let's look at how the
performance is affected.
Lamport, I didn't show the code here
but Lamport basically because
the synchronization is done
on centralized variables
it actually doesn't really
scale or improve that much,
but Giacomoni because we were doing
this synchronization on
every item in the buffer
we're actually able to achieve
much better scalabiltiy.
That being said, it also shows us that
how the choice of being ...
Like if someone says do you want
to have blocking supported or not?
Is that you have to be really careful
we're losing futex performance,
just for the possibility of blocking.
Even without using blocking,
just for the possibility that the consumer
might want to block,
we're losing that much.
So, actually if you're
designing a library,
you should provide both, because it's like
some user might actually
have a very streamlined
communication between the
producer and the consumer
and actually, want the
consumer to be awake,
They can't afford to have the consumer
to be awake all the time,
and the stream of communication
between the two threads
is actually keeping the consumer occupied.
So, that's another lesson here,
and
to summarize what we went through
in this talk like so,
Michael discussed the landscape
of the parallel programming
and how it is changing, and
how it is becoming easier
but not quite easier than the challenges.
And Paul discussed the
hardware and software issues
that still remain and how they improved,
and how they interact.
I went to through this example
that I hope it demonstrated
what some of the concepts
that we discussed
and demonstrating how
these kind of trade offs can be worked out
to balance generality and the
complexity and performance.
And so, thank you and if
you have any questions
for Paul and Michael an myself.
(audience clapping)
- We have microphones there and there,
I think if anybody's got questions.
We have contention of microphones,
this is demonstrating load
imbalance, just so you know.
- [Man] Yes sir could you or maybe Maged
could you point to any open source
for high quality ESPC and
PMCQs that you would recommend?
- I plan to provide that,
I haven't.
Probably soon, yeah.
- [Man] Hi, Majed question
for you about the times
that you were showing
the performance times.
Was that with high
contention or low contention?
- Basically, there are two threads,
we actually say the buffer
was like 1024 items,
it was like of course two threads,
I limited to one socket because actually
we were losing performance on multi socket
I mean there is no need for that.
And it was yeah I mean basically,
it's equivalent to the producer
giving the consumer a pointer
and the consumer kind of
like depositing somewhere
unless you're turning around
to the the other item,
so it's quite high contention.
- [Man] With the relative performance
of the different solutions
change depending on the degree
of contention in your system?
- Let's say there is an imbalance between
the producer and consumer then yeah,
that will be like a different bottlenecks,
but what you expected
from something like this
is like the producer and
consumer are really balanced
either their best or
the producer is actually
has more pauses and then the consumer
will just have to go to sleep
and in that case would want
to use the blocking version.
- [Man] My question is probably for Paul,
so the question you are
saying that if we have
bigger caches of course
performance is better,
because most of the times
you use it mostly for data
from the memory, so at
the same time the size
of the L1, L2, L3 is not
changing in recent years
and if we set the verb it
says that it's expensive
to build bigger caches,
but how much expensive,
what kind of test you did at Intel
and what are the prognosis?
- For Intel, I have to
direct you to somebody
who works for Intel, I'm
actually working for IBM.
- [Man] Oh, for IBM sorry,
or in the market at all.
- But I can tell you kind of in general,
what they tend to do,
they have analytic models,
but they also run traces
on a much different applications
and actually look at okay,
if we had this geometry
what would happen, there's trade-offs.
If the upper level caches
tend to remaining small
What happens is that the if you have
a physically tagged cache in other words,
what happens, you have a cacheline,
you have to have the address
that the cacheline
corresponds to in it's data.
And there's a question,
should that address be a virtual address
that the user is using or
should be a physical address
that the hardware is using.
And there is quite a bit of
controversy, 30 years ago
20 or 30 years ago, and the
physically tagged people won
pretty much, as far as I can tell.
What that means is,
that if the cache is big enough,
then you need to use
bits that get translated,
So, if you have a 4 k page size,
if the cache is big enough,
or you need some of the
bits to be translated,
you put the translation
of the critical path
so that innermost cache,
and that really slows things down,
and so they tend to keep
the close caches small
so that they can just grab the Law of bits
that don't get translated anyway,
apply this to cache
and be translating the
address in parallel,
so that's one of the reasons
why those top caches tend to remain tiny.
It's kind of like a hash
table, or hardware hash table,
so you kind of have buckets,
and you can have more stuff in the buckets
but they don't do links
very well on hardware
and the problem is if you
have more stuff in a bucket,
you have to do more comparisons,
you have to have bigger fan
into the fan out of the gates
that are checking the addresses,
and that starts hurting your performance
and also increasing energy consumption.
I can't really give
you exact answer to it,
but from a software guy's view point,
that's kind of where it is.
- [Man] Thank you.
- Any more questions?
Well if not you guys have an
unfair advantage for break.
- Thank you, thanks everybody.
(audience clapping)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>