<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2014: Andrei Alexandrescu &quot;Optimization Tips - Mo' Hustle Mo' Problems&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2014: Andrei Alexandrescu &quot;Optimization Tips - Mo' Hustle Mo' Problems&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CppCon 2014: Andrei Alexandrescu &quot;Optimization Tips - Mo' Hustle Mo' Problems&quot;</b></h2><h5 class="post__date">2014-10-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Qq_WaiwzOtI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright you know what's the worst thing
for a speaker there's two things that
are really bad for a speaker one is a
near empty room and one is a room with
people standing in the back because the
expectation is so high and it up there
is some of you who are standing right
now I'm going to leave it's like I'm
three getting tired this guy speaking
like what the hell let me kind of you
know just go you know the talk so thanks
are high but I have the cutest email in
the world look at that alright so I'm
very honored to be at the first edition
of the of CVV con which looks absolutely
awesome
what hands up if you liked the
conference so far all right
there's always time to disappoint so
I'll ask after this talk and let's see
so I'll get right into it I have some
tips regarding regarding optimization
for all of us things that surprised me
quite a bit I talked about optimization
in the past and I derived quite a few
insights from my work on h h um at
facebook HH game is our JIT compiler for
PHP and hack and it's a high profile
product at Facebook we open sourced it
recently it's a high profile product in
the sense that every 1% that it does
better it's going to save the company a
lot of money in terms of power and in
terms of acquisition of hardware so only
power costs can be a high figure to that
end there's going to always be people
who do care about that extra umph that
again from how is likely not the Billy
stack does versus of a list actors
they're all actors in the Billy's that
are pretty good that are just there and
everybody wants to see the a Lea factors
same here you know we could have used
second-rate techniques and we got so far
so sophisticated things like linker
scripts and the oddest
you know the other streaks imaginable
including some you're going to see today
so inlining lang is one optimization
because it essentially enables and arm
enables and interacts with all others
all optimizations interact with one
another but inlining is sort of the
first step in in a long list of
optimizations once you line code a lot
of things start to happen a lot of code
gets exposed white box and then other
optimizations start kicking in so it's
not surprising that once you do one
stage of inlining what's going to happen
next is many other optimizations are
going to kick in and the code at the end
of the whole process is going to look
like nothing like the inline function
who's kind of familiar with this kind of
stuff did you ever disassemble code and
stuff I'm in the right place fantastic
alright the last hand living perfect so
it's very hard to estimate and it's very
hard for a compiler writer to say here's
my cost function for like here's the
cost function that makes me decide
whether or not I'm going to learn a
function and subsequently inline
transitively other functions inside of
it right and this kind of very
interesting thing that when you
benchmark comparison you often benchmark
on a micro benchmark a small benchmark
on a small application or media
application at most and you know the
estimates that you give from
benchmarking the performance of inliner
are going to be quite different from
what you see in a finite in a finalized
application so far so good
all right here's a tip this is not I
mean this is not the essential tip but
it's kind of a small tip the micro tip
we are we took a long time at Facebook
to migrate from GCC 47 to GCC for eight
because GCC for eight I like that
horrible like 5% regression in
performance and 5% like just you know
it's in the millions it's just a huge
regression for us and until we kind of
you know got to discuss about in those
Oh actually there's quite a few things
that control the inlining in GCC
and once you pass these parameters to
GCC for a to get a win instead of a loss
you will get a 2% way so it's this was
7% difference on a large application
just to show you how sensitive the
inliner is to to a variety of
parameterization and tweaking and twigs
do matter you know I mentioned this in
the past
twigs do matter so it's many people
think oh I get it - the proof of concept
stage and then tweaking is going to give
me like 10 percent more or 50 percent
more actually tweaking can get to 50 X
more right can get a lot so tweaking
does matter
all right so let's sort of getting into
it and this is going to kind of put
inlining on the spot there's some dark
matter going on in every see process
program which is constructors and
destructors so first of all you know one
bad thing about constructors in
destructor see that everybody says
they're good right that was a joke
kidding here you know contrast it's a
humor technique Seinfeld I don't know
so iris is a good mother of an apple pie
so you should define constructors
destructors you rely on constructing a
destructor Rawi all that good stuff so
they are kind of good in a way that the
program technique is good for for a
variety of sort of quality reasons
however it turns out quite surprisingly
at least to me it was that constructors
and destructors are a major enemy when
they ally with inlining for your code
size right so the way it goes is well
you have your right constructors and
it's destructors and often they're going
to be implicitly generated without chief
one kind of you got to think a bit about
it because you have a for example i was
strucken so it's a strap it's you know I
didn't write any constructor but at some
point to change or charge start to a
string and all of a sudden
a member of this drug has a constructor
and the destructor and all of a sudden
destruct itself be is bestowed with a
construct not destroyed am i making
sense right okay so all of a sudden we
have this tragedy defect like you know
you have one well you know we have one
drop of it's like homeopathy right you
have a one drop of good stuff in a well
of water and the more water there is the
worse it gets I mean the better it gets
you know there's this joke about a put a
drop of like dirt and homeopathic effect
would be disastrous because we reach the
ocean that drop is going to be a
disaster right is the reverse of
homeopathy so anyhow it's the same with
members right pretty much the same so it
goes is once you have a member that has
a construction instructor a bunch of
stuff is going to generate from it and
transitively to to the to the structures
that contain it so it it gets worse from
here because very often these are
automatic generator so the compiler has
complete control over its complete white
box that compares is going to be able to
inline those guys
there's ik also I have this guy so
what's you know what's a few stores
between friends there you know with
Russian accent you know that would be
perfect right so you know just a few
stores they're you know going to store a
few things and whatever it costs so but
then it gets bigger and it kind of its
kind of a boiling frog kind of effect
and you get to the point where hmm so
all of these implicit generated
construction instructors that are
trivial and what happens then is you
have your code the code that you're
looking at contains dark matter in the
interstices right contains a bunch of
stuff that you didn't write you kind of
have to think that it even exists and
it's kind of present everywhere and it
gets worse because it's in line so
there's a lot of fat you know that bad
like brown fat or whatever the bad fat
is right they have all that stuff in
between the inter
of your code in between analyze this
code going on in this code and cronic
code and that has really bad effects at
scale I'll come and get a bit into
detail on that but that was beyond
traditional device because the
traditional advice is well don't over
copy objects so you want to copy too
many objects you don't want to kind of
overdo it you want to avoid you want to
use our references you want to pass by
reference all that good stuff but it's
not only that it gets worse because for
example nobody advises you to not write
destructors whether you got to write it
because it's there right however these
destructors ones present and once in
line they increase your code size
sometimes considerably so I promised
I'll get into detail about why that is
bad why is it bad to have bad code it
turns out that the major liability @hh
VM and I suspect that many other major
projects is I cache spills i cache is
the instruction cache and the way thy
cache works is like just like any cache
you blow the instructions into it except
it's read-only so it'sit's some way
there's less pressure in it than a
traditional data cache so a lot of the
instructions into into the cache you
execute them and every once in a while
that you're gonna have to jump elsewhere
and load some other stuff into the cache
and you're going to have to evict the
cache on that good stuff right cache and
in micro benchmark you're not going to
see I cache because micro benchmarks is
not going to be like oh let me write the
1 million lines of benchmark right
nobody's going to do that right so micro
benchmarks are not going to exercise
mania I catch effects I kind of you know
look at like the you know itsy-bitsy
stuff and all that good stuff and you
know now here's the thing you're a
compiler writer what you going to
optimize for those two customers who
have big programs how much money King
they give you they optimized for the
year you optimist yeah once you have
good benchmarks they look good in
articles right you're on jumped in I'm
serious actually that was not a joke
and this wasn't what okay this is like
negative day for me so you know you want
to look good you want to you know you
want to outdo come competition and you
want to satisfy many of your customers
right nobody's going to say well I'm
going to write the compiler that that's
going to work great for Google and
Facebook only and the help with the rest
they don't matter right all right
nobody's going to do that
so therefore sorry Google Mike Goodwin
all right so it's it's kind of difficult
to say you know what what do I measured
measuring this compiler against I have
to add some sort of a reason the phantom
bullet here you know we open sources
each VM and a lot of good software is
open sourced so I think there's a good
trend because it means compiler writers
are going to be able to say well you
know GCC the newer GCC you can actually
test against h AG vm and other large
programs and say am i doing well on a
large program like that that's realistic
and does something interesting so um you
know I hope that's that's going to be
more of a trend so tip number one goes
simply like this beware inline
destructors beware it doesn't mean done
don't do them I'm not saying you to not
do them I'm saying to be aware of what's
costing you so inline destructor so
destructive czar going to be called
everywhere implicitly as an aside that
they add significant code to functions
that may throw because you need to have
like that the whole cleanup code that's
why it's nice to you know adorn your
functions with no except whenever
possible right and they're not
reflecting a source code in the source
code size and that's the transitive
effect so it gets pretty thick pretty
soon and they're often ejected by the
compiler which means the compiler is
going to be eager to inline them
compiles are very eager to inline quite
a bit
so therefore watch they started
sighs carefully and here's here's a
simple thing you can do I don't have a
slide for that this is a very simple
thing so you know people say oh I'm not
going to write the destructor because
it's actually about tip I think in herbs
on my book probably we should recant on
that but it's like you know you don't
need to if you need to write our
destructor maybe you've done something
wrong elsewhere because you're too low
level ideally to have your members such
as unique point and SharePoint or
whatnot detector whatnot take care of
the destructor for you and it's
generator and it's called and
everybody's happy actually everybody is
not happy sometimes it's nice to just
declare the destructor in the but in the
in your class semicolon right and then
define it in the CPP file such that the
compiler does not have easy access to
get its grubby little hands on it and
align it and you defining those we say
equal default and your or you just put
the empty braces and you're good right
it's just that your explicit thing you
know what I know you could default I
know you could define this for me but
I'm going to you know define it
nevertheless and it going ahead and
defined in the C++ file and then of
course the story goes did never ends
linked I'm aligning you know it's like
we try to punch them here and they punch
us you know it's just it never ends
there's no guard ok so then the guys
come we'll come up with link I mean
lining I say ah there it is
and it looks like an empty body so
what's a few stores between friends
right
all right so watch this doctor is sighs
carefully and you know you might want to
just disassemble sometimes your
destructor I see you know how big this
is this guy and how much does it affect
me right so be one line destructors and
just to give you a taste actually is
this readable at all no okay unanimous
okay let me see I think I can read it
all right
so this is a this is a program we use
that transition is a team dedicated it
it's a big deal it's a program that we
use for for assessing performance of our
of our JIT compiler and essentially if
you in line one destructor the top line
is what matters and it says this it says
CP instructions
- 0.3% CPU time plus 0.4 percent which
means you fine line 1 destructor in a
structure in my large program 1
destructor literally of one type it's a
widely used type nevertheless but you
know my cherry-picking here but it's one
destructor and the bottom line effect
has been the number of instructions I
retire has reduced why there's no more
function called sequence call em return
they are gone so as a rule whenever I
inline anything the number the total
number of instructions is going to go
down right it's going to go down because
there's no more call and return is less
code there's a pure instructions to
retire there are more instructions
present but therefore we instructions to
retire right right so the trace is going
to be smaller and that's sustained by
this by this test here so - 0.3% so I
got to say like 1 now you know AGG and
TC minus 0.3 percent say this is not
good we're not going to take that it's a
loss it's a regression it's already a
regression up to like 0.2% you can say
it may be in the noise so let me repeat
the experiment zero point friend above
is like already know okay so but that's
not what matters you know it's nice to
have a few instructions because fewer
instructions are a good proxy for faster
code you'd think right however somewhat
paradoxically at first sight CPU time
has increased by 0.4% which is
definitely in the range of 1 you know
this is bad this is a regression so 0.4%
like oh so we're losing a few you know
quite a bit of man I can't say the
numbers heydo so
we're losing quite a few quite a few
dollars on just power costs alone and
ventilation all that good stuff so no
that's a bad thing so why do the CPU
time increase although the CP
instructions decreased I cache
that's a nice Apple product in the
making i cache all right so indeed
although the number of instructions
retired has been reduced measurably
considerably in fact that total
execution time has increased because the
the memory subsystem spends all that
time evicting and loading the
instruction cache so not good and it's
interesting that again it's it's a line
of code that I changed in a destructor
to actually change all of the stuff it's
not the semantics I change it just the
lining as a simple tip here's how
controlling lining on the two actually
three because GCC and LVM do in ensign
clang bleed the same so GCC and clang
you say oh AC line oh okay so there's
about like four key words in GCC are
controlling lining which kind of
underlines the importance of the concept
right I guess it's also probably bad
design on a few artifacts but
essentially if you got a right inline
attribute always in line just to make
sure that it's always in line and to
tell you the truth is not always in line
I'm not kidding
so it's not even though I say always in
line it's not always going to be in line
and with and continuing with GCC say
never in line is they have to be no in
line and of course sometimes is going to
be in line I'm not kidding so these
things are even though I say always an
ever they mean almost always and almost
never but those are too long to type so
we said whatever in V C++ you do force
in line or dick aspect of my line and
that's the way the cookie crumbles
alright now here's what I do here's what
I did I took a cheesy BM and I defined
never in line to be nothing you know
they call it be thanking it take the
fans off right so they can kinda you
know they can't do nothing right so I
defined never in line to be nothing well
the effect has been red which means bad
so I got an increase by 0.9% in
instruction count why was that huh
function call overhead and I also got a
CPU time increase of 1.6% right so
that's like a huge regression and you
know mind you I should add that never in
line are hints introduced by engineers
so it's manual work so this manual work
has paid off this much it's easy to
measure you can say well those people
spent hours doing that or have gained
one point six percent in CPU time and
now let's all do the other let's defend
always in line so let's make always in
line do nothing and with that we're
going to get indeed a reduction of CPU
instructions by 0.2 percent but we're
going to get an increase of CPU time of
0.8 percent so both actually make things
worse if you ignore them so sometimes
the better train line stuff sometimes
better to not in line and it's it's
essentially at this point it's a hard
enough problem that you need to take
control in your hands and measure and
assess how you address it all right and
now I'm going to to discuss all of this
construction destruction inlining issue
I'm going to run a simple case study
which is like a sort of a user-defined
shared pointer it's shared points I have
been such a popular topic for C++
everybody has written a couple you know
we there's good understanding of what's
going on
and so it's sort of a simple example we
keep in mind that the effects at work
are more general than that
so SharePoint is that you know the
obvious solution to reference counting
and it's a SharePoint in particular the
standard one it's optimized for a blend
of needs so well I want you know I want
to actually share a pointer among
threads without aggravation right so
therefore you need to do atomic
reference counting you know I want to
support custom des liters and I want
just for weak pointer so I can break
cycles right and there's no support for
intrusive reference counting and that's
kind of a bad thing because you know if
you're serious about it you want to have
intrusive reference counting it's it's
sort of a good thing for large
applications now I've in a past you know
past okay I gave this tip which is the
first cache line is where it's at what
do I mean by that right so you have you
have a data structure like a class or a
stroke right and you want to sort you
want to sort the members of that data
structure by hotness meaning you put the
most attractive first a comma
okay you put them all the hottest first
right so you want to put the most access
the most often first and there's often
cases in which just swap a couple of
members and you have like a huge
increase because once you so you have
you out the address of the object and
essentially the first cache line which
is how many bytes in chorus 64 thank you
very much
next time you sing it okay drill a bit
okay so 64 bytes the first 64 bytes of
an object is where things happen what
the action goes and a lot of your like
the first member is a bull like messes
up the alignment completely like you
know it's a boolean that takes the word
actually because of the alignment and
it's like oh was initializer some that
is test once you know it's tested once
and does nothing in the forest of
lifetime so you want to put the mole
access stuff in your objects at the
front of the object front load eeeh the
hardness right so guess what the
reference counter is very hot and often
in a shared pointer if you don't use
make shared is going to be elsewhere
meaning a completely different cache
line completely different everything
right ideally you would put the
reference count right at the front of
your object as the first member so if
you can afford the interest of reference
counting that's where we want to be make
sense all right awesome
so itami is the matter so I've just been
in this great talk about lock free
program and I was very high felt
vindicated because that's all these
people say oh you know Atomics atomic
reference you know plus + and minus -
they're pretty much as good as you know
everything else and they're not I mean
it's like this no matter how I measured
like I couldn't get any you know any
faster than 2.5 times right and the toe
before I was like ten times
that's very happy like okay so he even
overdid me so I'm very this is
conservative numbers because I didn't
want to annoy her bill that much who's
not here okay great it's like you know
I'm not going to go to that talk because
I hate that guy
all right and think of this compiles
know what plus it plus plus and minus
must do compares I've known for 40 years
what plus + or minus - do and when they
find them in tandem when they find out
next to each other they know that
they're you know they cancel each other
and they often take them away right it
does it's not the same for Atomics
Atomics must be visible because they're
you know they're shared effect and you
see your fetch and add then you see a
fetch fetch add with plus one or minus
one is not going to be this it's not
going to look like the cancel each other
and they're not going to be optimized
the way however her be if you were here
he would be bouncing on you know he will
up and down like a guy in the list you
do Soleil because he said no actually
Microsoft compared dodged all that good
stuff and etc etc and maybe STL can
confirm that one
right that's the right answer
okay so actually herb kind of you know
push back on that so you keep that in
mind all right
so well how about I'm waiting sharing so
I want to define a single phrase share
point because I can't afford 2.5 to 10x
cost so therefore I want to define our
you know single threaded shared pointer
I share but not the cross threads kind
of thing so you know this what if I do
pass these internal thread and actually
there is a solution that which is easy
you just store the thread ID at the
first axis with with a smart pointer and
then you assert on it you insert at the
thread idea of the guy that accesses it
is the same as the thread ID that
initially created a smart pointer and
your cert on it and all of that code is
just in the bug mode right and the
classical implementation goes of course
like this I'd like until 4:15 4:15 is
like the oh yes perfect so I have sync a
single thread pointer and we go with the
absolutely classic structure with two
words two pointers one is pointing to
the payload and that's P and the other
is pointing to the counter that's C and
always good okay so we initialize with
now whatever we go well if we initialize
this guy by one the reference count is
one right but if the point is not going
to also initialize this guy small and
the copying and everything goes as
everybody knows this so it's fairly
trivial hmm anything in here you just
construct this is the move move
construction it copies the two pointers
and resets them right after and the
destructor is going to do the expected
things right
okay so herb gave a talk that's
available online called atomic weapons
and he has a very similar implementation
of a ton of reference counter pointers
where he has pretty much the code above
except he uses atomic oven science star
for the counter and uses fetch out and
sub or plus plus and minus ones
respectively very nice so our task is do
we take this naive implementation no
Atomics involved we share only within
the thread make it faster ideas this is
the time where you know there's power in
numbers come on group like ten you know
that roll that side this side let's do
something ideas how to make this faster
put a count at the head I'll make it
inclusive we can't afford let's say we
can we don't we can't afford it's a good
point oh yes those thread-local
references go on now I don't you want to
make me work no I want you to work so I
call thread-local run with it I don't
think I have okay something else ideas
all right I have an idea let's read the
paper okay so I read this paper down for
the count which analyzes reference
counting down for the count a nice
ponder and they look at a number of
applications and look at how many you
know how what's the largest reference
count you ever get and this is the
actual number of reference comes like 0
1 2 3 whatever and they get to the
interesting realization that for like 90
percent of the application doesn't get
before above like 3 so most objects have
very low reference count all right now
that we know that so you know there's
some insights there which is interesting
most objects have very low reference
count
what's an idea yes in the back people in
the back are all is the best come again
steal some of the bits from the object
from the pointer and put them you know
count them as the reference count you
know what I deleted those slides because
this is a 90 minutes talk and I read I
compress it to 60 minutes so yes but
there
no slides it's a good idea and people do
that you can steal some bits and say up
to 16 what do you do about 16 you
saturate you log and you leak I'm in the
program just make write saturate you
saturate the counter to 15 or whatever
the maximum was you log the event you
say you know here's this this goddamn
object was what the hell right and and
then you leak it you let it be you never
decrement the reference count again and
the application continues to run with
the leak and everything there's there's
a log that witnesses what happened but
the application still behaves normally
still does work and until you kind of
have to reset because what the hell that
like 20 gigabytes of RAM for that
application you know and you know this
is a fact of life that sometimes you got
to do that it's a great idea thanks for
stealing my four minutes with this
although I did have no slide other ideas
great we're starting to get like really
interesting here ideas all right I have
one yes Stefan you could use yeah you
could just make sure so that that's a
sort of applies everywhere in any word
whenever you can they actually allocate
up the reference count next to the
payload and make the reference count
point like one step above you know in
front of it and that's all nice because
you only do one allocation so that is
great whenever you can do it you should
but what what optimization can you
derive from this particular fact that
very many objects have very low
reference count Fodor you can optimize
transition 0 to 1 and 0 1 to 0 guys ok
ok you're getting somewhere
ok that beard will never quit all right
so let me show you for example so we
kind of decrement this guy composite so
if the pointer is not null and we
decrement this guy why would why the
heck would you decrement if you know
it's one huh ok let's kind of see if I
heard that code I hope I do all right so
oh let me make a few preparatory
comments to all that because I
discovered this in our code base at
Facebook you know people avoided Auto
PTR so it's sort of a fact of life that
total PTR didn't work out so then back
then in the Dark Ages before shared PTR
you're like oh well what I have before
unique PTR people like what I have here
what I have TR one shared PTR booth
shared PTR as a replacement right select
clone gangrenous a share PTR here
although all I need is the one reference
you know unique reference kind of
pointer because I don't have the unique
PTR same so angle is this guy even
though I know the count is going to be
always 1 and 0
I'm 1 and then going away right
interesting and I'm D notice at earth
designs that just put shared PTR there
for future flexibility like well there
is one owner right now but what if in
the future you know when that hoverboard
was the hoverboard in the future back to
the future ok so you know for the future
I just put that flexibility in there
write your own it could be right reasons
that could be wrong reasons
it's not my place to discuss that right
so ok interesting so let's change the
code to do first of all lazy reference
count allocation I'm going to get to a
feather so idea in a minute but let's do
this actually let's make this whenever
the counter is now it means it's really
one huh so I get you a new pointer for
free does your ship where's your
sharepoint or does your share pointer do
that very well
why aha
weak pointers right right but every
machine has double the double wide
locking
alright let the record state that he
said it's not impossible ok anywho so
we're not worrying about weak pointers
here but we are worrying about
optimizing this particular single thread
pointer so let's do this well whenever I
look whenever I start with a pointer I
say well I know for example I'm
constructing I'm copying a pointer and
if the counter is not I'm allocating
with two nice because it's already 2
because 1 means really not right and
otherwise I'm just going to increment it
and there's a subtlety here which goes
how about this guy
why do you think I commented that
Alton's wrote I needed keep in mind I'm
famous for having mistakes in my slides
so that may be a mistake but it may not
be why do you think I afforded that and
that's an extra that's a less of a right
which is a big deal actually at scale
right it's an a storage why is that
exactly you you have this invariant that
says well if the if the payload is not
you don't even look at the counter so it
can be anything you can leave it
actually dangle which is dangerous but
fun right
nice so far so good and never the
destructor which does exactly what I
promised if the pointer is not mal I'm
not looking at anything else first line
awesome and then I have the faint my
famous go to sue sue me so you know and
stuff like that it makes it feel smaller
my friends I'm not kidding try to do
this without the go to in that you know
it's not gonna it's difficult so it goes
back and deletes to this guy and
whatever and that kind of stuff pretty
nice I'm not I'm not above using go to
if it has a advantage right very nice
huh so there's alternative to this
well now you need the C and in this case
you look at you know you look at the put
the reference Condor first and then you
then you delete both right and that's
interesting because it folds the test
for null into the delete code as I'm
sure know if you call delete against the
mouth pointer it's guaranteed to do
nothing so essentially it has already a
test for now let's inside of it and this
variant exploits that but now there's a
story to that because if you don't if
and you know before the delete then the
code is going to be smaller because
maybe don't call delete so the culture
date is kind of remote and you pay the
castle the call and the test is done
inside of delete which may be an out of
life function and so you paid up the
price for the call even though the point
is zero so the point is almost always
zero then you may want to actually not
take not take this path alright so the
performance dynamics goes like this if I
have one reference that means the
pointer is not null and the counter is
null or the counter is one and that
means unique reference how can the
counter be ever one because remember I
initialize it to to their current so
when I go back I don't compulsively say
oh did you get down to two then let me
kind of out from two to one then let me
you know delete it and modify why don't
I do that
No
right so I don't to thrash I don't say
Oh delete then I lock it again once you
allocate some memory it's good to kind
of hold on to it a bit right you pay the
castle rate so I better make some good
use of it you get your money's worth
right exactly thank you so once you
allocate then your allocated thing you
initial that too but when it goes down
from two to one you don't say let me
just delete this guy because I don't
need let me hold on to it okay nice this
is what I just said and go to awesome
everything constructors get smaller and
use your own initialization because I
initialize with with the null pointers
and everything and I can control the
number of copies better a number of
creations and this is an interesting
insight the number of creations very
often is like you know I need these
objects so I'm gonna but I can control
copies by passing by reference these of
the roll point out to the small pointer
so I have some control over how many
copies of the share pointer I'm making
but I don't have much much control over
how many copies anchor I'm creating
right how many creations how many
objects are creating so that's sort of a
nice thing because it indicates that
certain functions here are hotter than
others so this is not as hot as for
example this because this is going to be
delete as many times as its created so
nice
all right mp3 which we get to Fodor's
famous beard which is well skip the last
decrement when you get there you're
there so don't decrement that guy
because you're doing a right extra and
the right is expensive the worst thing
in computing are indirect writes I hate
them
right so indirect writes the anathema
you would never want to write in
directly just do everything in registers
okay let it do it yeah for kilobytes to
the man on the moon right 4 kilobytes
and right now that's like l0 cash right
it's the register file yeah sure key 4
kilobytes all right I was kidding here
so you can use memory
don't overuse it okay so skip the last
decrement in the destructor here we're
going to build instead of saying if
minus minus C equals zero so just just
look at that guy and see if it's 1 if
it's 1 then I know I'm done so I don't
touch it anymore big deal I don't touch
it anymore big deal so I'm going to do
the whole thing skip the last decrement
any more ideas
ok let's optimize this even more yes you
can cache the read of C of star C right
let's see if not C does loading a
pointer and start ok ah right
the compare is going to do it but you
know it's a sort of a good point you
want to disassemble this code and look
at it literally want to look at this
code and say well was this load internal
register and kept it kept around until
this decrement right it's a good point
if this is a sort of micro optimisation
but you know how about something more
interesting yes why do I want to make a
comparison against 0 my fair friend aha
yes
I starting to blow myself here yes you
want zeros why do you want zero
no I yes I mean no I mean sometimes the
instructions are going to be smaller so
we just talked about that let me kinda
skip ahead a bit right I'm going to go
back to note because we got into this
whatwhat's wrong with you okay so zero
is special special assignment so I
mentioned this already so I'm just going
to going to kind of punt a bit on it but
essentially zero is special there's a
lot of places where a zero is produced
and you know for example on RISC machine
how do you produce a 0 UX or a register
with itself right and it's got kind of
just going to be zero so how do you load
this so that's how you load this you how
you compare against zero you have a test
it's already there so you don't need to
compare it if you how do you compare it
against one oh wait I have a constant
instruction stream that I cache is
expensive so I don't reload the constant
instruction stream which is one and kind
of load it and look at it and compare
and all that stuff right that's hard
work I just want to say test and be done
with it
so it's very specialized very nice and
in an enum we want to make 0 the most
frequent value let me see if I have this
link here here I have this link you know
I'm gonna gonna take a risk here and
load this link but before I let me
explain what I mean so I'm going to load
this link and see if the whole
infrastructure works here so a weird sub
sub tip would be well make defaults
viewstate all zeroes because zeroes are
nice and I want the default state of my
objects to be all zeros because then
compile it detects that it's going to
add this is zero I can use a special
memset or special loads or whatever and
to wit I have this example for you Oh
God make it work oh okay
although I guess
alright ok this doesn't look good
ok mmm
okay I can't even see any of this
probably not right okay good
right so okay that doesn't help much all
right so I have a strong point zero here
let me highlight the code that I'm kind
of pointing out
I have struct point zero which
initializes very nicely it says point
zero the default constructor initializes
X by X by 0 Y by 0 and Z by 0 very nice
and the actual routine that uses point 0
is going to look like this BAM
took source returned because it returns
to the fund returns up let me show you
the function this returns a default
constructed point right and the content
of the function is like three
instructions took source Drake source in
the red very small and then I have a
point
thank you and then I have a point one
which says well let me in shunt to minus
1 because I'm a bad guy and I want you
know and I run change as done by minus 1
and then I'm writing a routine that's
going to a gun be called gun and return
point 1 and gun looks completely
different on a lot larger Andrea it
looks only one instruction logic
actually it's a lot larger because those
extra instructions are one byte or one
word I forgot but these moves are Li not
contain like the minus one which is a
word by itself and then the instruction
itself and all that bad stuff right so
completely different code generated I'm
glad this this work
alright so to kind of conclude on this
particular zero thing I think I
mentioned this in my previous post that
that I gave at the at Microsoft by
mistake I've I found it more natural to
swap these two values of any new and I
got the 0.5% regression because
everybody was testing against foo if the
sector equals foo and this is not that
is full if that is different from full
and all everybody was testing on food
not on bar and why
swap them who became one and everybody
was testing against one and guess what
guess what we're the code that was
testing was where was it in a destructor
I'm not kidding
all right let me turn back now I kind of
continue with the show here okay we're
worried as you're a special back back
back
alright so it's keeping the last
decrement so that's a great that's a
great thing
awesome so you know we're again
motivated by the fact that most objects
have low reference counts so out of how
many you know how many increments I'm
doing on a given counter it's going to
be the last one is going to be maybe
half of all increments right because I
only have like to copy two references to
that guy so it's actually fractionally
speaking the last decrement is huge
right all right just say that and one
other interesting thing which comes kind
of it's subtle I was speaking to Jason
Evans the the author of J malloc and he
said one of the worst mistakes that
people who implement malloc and free do
is to keep the metadata with the data I
see it not from living why is the doing
that bad it's all a same cache line and
the thing is when you free very often a
free cold memory when you call free like
delete right when you free stuff you
free very often it's called memory and
if you need to write to the metadata you
need to write to the metal to mark it as
free and at the that moment you are
going to write too cold memory so you're
loading that memory right with it's free
and then it's if you dirty the thing so
you kind of the worst thing ever right
so you dirty that the damn page there
and then you never use it again and the
CPU has no idea that you don't plant
trees it again right it's completely
weird so a good man a good allocator
keeps the metadata is separated from the
actual payloads such as own free memory
just touches only the metadata which is
of course always hot fed or destroy me
right right so the point was well if you
have a good malachite organ is free
lists which are going to take care of
that because you're going to reuse that
that cold memory over again well do you
like fragmentation no idea was you would
say no and I would say then don't use
free lists let that's the way the sketch
goes here right
no of course so there's there's there's
qualifications and qualifications
qualifications to everything right but
as the first level of approximation
essentially this is an issue with with
freeing at blocks of memory and let's
leave it at that for now and kind of can
destroy me offline just not in front of
everybody okay
all right so avoid during what this is
the argument that I just made and well
you know as I said we want to replace
interact acronym with an atomic read in
the variant that would be actually
thread shared and not only shared in
each other thread the same thread so
actually the impact of this optimization
in particular is huge for shared PTR htd
you know the guy that does the the
interlock document just don't know Tommy
Crede and by the way it on x86 that's
free because all reads are atomic right
thank you yes destroy me oh my god okay
whenever he raises the hand okay okay
because of that that weak point is
really kind of right doesn't it grind
your gears huh okay great so um we
talked about this right
okay we discussed this guy alright I
have two smaller tips it turns out that
using dedicated alligators it's just a
fact of life that many large
applications inevitably migrate to like
you know using their own allocation
scheme general-purpose alligators are
just not going to cut the master they're
gonna you know they're going to work
most of the time for most applications
etc it's just that for high performance
applications there's always going to be
something you can do that
Malick doesn't do right and allocate a
dedicated caters do a lot of good stuff
such as for example there's heaps that
store one control bit per counter and
store metadata packed together and it's
always hot so I have a very low overhead
for small objects in these cases we
allocate these small units all over the
place so you want to allocate them
really fast right and the a really small
really small block allocator can do
wonders there so you can have a very low
overhead for a 32-bit counter there and
of course the metadata is very cache
friendly and you can use free lease that
we just talked about as an alternative
you gotta have each the disadvantage
each counter has to be a one word
because it's got a store a pointer in it
so it's you know you got a measure and
think of it and really be careful about
it right and the last tip which is I
mentioned it's already a bit is odd
actually I have people like really
raising protests here like what the hell
are you talking about so the vast
majority of objects as we talk have less
than 16 reference then you can prefer a
smaller counter if you can pack it
together with something else and on
saturate you leak you log and everything
goes forward right
this blows up the code sighs what does
he blow up the code sighs all they're
logging okay awesome very nice very nice
Touche all right so to summarize there's
a dictum that we hear very often the
tsipras's community which is you're not
that special many people used to say
well you know I know the what the coding
standards say and whatever but I'm
special and therefore I'm entitled to do
all sorts of things whatever you know
whatever the scene was there was a
rationalization for it because you are
special right and you know there's been
like a push from the pundits in the
sequel's community to say well actually
not that special and actually I really
must cut myself had this like you're not
that special and think you know think of
your case as a board of a typical case
and a typical case I would say nowadays
there's the brass knuckles are on like
whenever you use whenever you use
systems programming in a large
application there's going to be huge
pressure for you to deliver on
performance because otherwise they do it
in Ruby that code did I they annoy the a
fennec okay uh Mike delete that okay
post-processing is a so now here's the
thing code I heard that this last night
Ruby is 50 times slower than anything
else which is kind of contradictory when
you think of it because it can be 50
times more than sign that's already like
10 times slower but it's speech and
slower than anything which is
interesting and Ruby and you know high
level languages are good fit for certain
applications but definitely at by the
point you care about this kind of stuff
that we just discussed you can assume
you're special you can assume you're
special I can assume that whatever is
standard and given to you and
recommended to most of us is not enough
and you got to kind of get there and
kind of really reach for reach for the
last ounce of performance and get it you
know scrapes is like squeeze that
squeeze that stone
okay until blood comes out of it all
right
thanks very much
however I want to finish high with
applause on a nice sentence there but I
have five minutes for questions so
there's any okay don't this is not
question ah the fun obvious for
single-threaded smart pointers I have
heard of linked smart pointers which the
standard says oh yeah you can use this
but nobody does for students or pro
turquoise it's multi-threaded for
single-threaded does that help link
smart pointers so instead of storing an
explicit account you kind of link them
together so doubly linked lists can
change I gather all the things that does
that the doubly linked list is too much
of a blow up in size and the singly
linked list is all event deletion so
it's between a stone and a hard place
it's not that you know it's not that
good actually
excellent I will file an issue to make
the standard say that use count is
efficient all right I guess you can call
this recording as evidence yes yes
okay so we've spent some time doing
micro optimizations to reduce like to
use zero instead of one in comparisons
does it really makes sense when anyway
we're still doing allocations and
deallocations does it give us anything
cause this is just academic an
allocation so vocational I assume the
questions to be heard allocations take
you about 150 cycles to get you there so
what allocation takes like in a
general-purpose memory allocator it's
going to take you know it's going to use
all of these good techniques every after
all is told there's going to be hundred
hundred fifty cycles that dargah with
that is not going to be able to measure
any difference by the zero there Thanks
questions questions oh by the way so you
care to continue on that there's always
after this talk there's always someone
who comes actually I tested that and I
had to want to replace it with a zero
and I saw no difference whatsoever well
you know
caveat emptor it's not always gonna
these are not gonna do miracles for you
every time right yes
so the question was so some of this
stuff is specific to Intel and how does
that this advice pour to other
architectures I would say maybe actually
zeroes are you know are pretty on
universally special to CPUs and in
general like these tips in my opinion
poured pretty well across modern
architectures so you can be saved that
they're going to help yes
all right so let's take the break and
yeah I'll be here for a few more minutes
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>