<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Sergey Zubkov “From security to performance to GPU programming...” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Sergey Zubkov “From security to performance to GPU programming...” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Sergey Zubkov “From security to performance to GPU programming...”</b></h2><h5 class="post__date">2017-10-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HdQ4aOZyuHw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Hello, my name is Sergey
Zubkov and I'm an allocator user
and some many other programs.
Anyone who uses standard
vector, standard string,
any standard type that does
what it does using allocators
behind the scenes.
Allocators have the well
earned fame of being difficult
to write.
I had to do that a couple times myself.
I'm not going to tell you that it was easy
but what I want to talk about
is what that work achieves.
So first a brief refresher,
what do allocators do?
Despite the name, they
do not just allocate.
An allocator is a customization
point that controls
five different policies.
It determines where an
initialized memory comes from,
how an initialized memory is disposed of,
how objects are created in that memory,
how objects are removed from that memory
and what it actually
means to point that object
in that memory.
I'll show a couple of
examples of how that works.
And of course, we now have
two complementary ways
to use an allocator in C++.
We can provide an allocator
as a template warranter
just like any other static policy.
And or we can use the
C++17 approach and provide
the allocator as a constructor argument
to one of the containers
from the PMR space.
In which case, all the
containers have the same type
even though they're using
different allocators.
There are benefits and
drawbacks from either of those
approaches and as always
it's an engineering decision
how to use each one and
going through implementation
details of that will take
forever and we have plenty
of talks at this main
conference on doing just that.
In fact, a talk by Ivan Mertz
which just ended in Daya Hall
was talking about the details of PMR.
So, enough with the refresher for now.
Let's talk about what we
can do with allocators.
This is a short talk
so all I'm going to do
is look at a few of the
allocators available in the wild.
In particular, we'll try
to cover each of the ways
allocators used and show a few examples
of all of the approaches.
The simplest allocator
that I have encountered
that's not simply
instrumentation, it's not simply
an instrument.
It's an allocator used
for test and attrition.
It's something called secure allocator.
Reinvented over and over
in different libraries.
So what it does is it attempts
to make sure that assets,
keys, sensitive
information does not end up
in memory outside your control.
So, an allocation, it
locks memory pages so that
if your application swap out,
the persons are not in a
swapped file on deallocation
it undoes all of that and also wipes
the data out of memory
using appropriate APIs
because as the option now,
memory reset, is not enough in most cases.
Now this may be debated
whether this is a useful
security measure but it
is a pervasive pattern
that appears over and
over so let's demonstrate
how we use such an allocator.
So, imagine you have standard
string that holds some
sensitive data and in case
of some scope, inter scope,
we talk to the user,
maybe compare passwords.
Let's pretend this did not
come from a static (mumbles).
So at the end of the scope,
the string goes away,
the password is destroyed.
Right?
Or not exactly.
If you are able to get access to the heap
and you looked at it, you might find
that exact password still in the heap.
In this case, GHC had
fully wiped out the first
eight bytes but still we
have most of that password
right there.
So what do we do if we want
to get rid of it properly?
We just change our string to something
in this particular key's,
bitcoin support library calls,
secure string.
Nothing else needed to
decode and the password
is gone from the heap.
So what is that secure string?
It is simply a basic
string whose allocated type
was replaced by secure allocator.
This is a case of the most
traditional use of allocator
and it's a template.
It has no special state,
has no special rules,
just as simple as it gets.
And I'm not going to go
into implementation details
of any of these allocators
because I'm looking at this
from the point of view of the user.
Right?
How do we use allocators in code?
If you want to talk about implementation
there are plenty of talks
at this very conference.
This is, by the way, I
think is an example of where
the fact that allocator
affects the type of container
is useful because that
just is going to preempt
or make it harder for
us to move secure data
from secure into an insecure one.
Now, for something that I
actually had to do at one point.
Imagine you have a system
which has a bunch of producers,
consumers, and all sorts of actors
that are processing
events and when you don't
have a long uptime, you
have limited memory system,
in my case the uptime
requirement was one year.
I had exactly one year.
And I had a birth rate of
maybe a million nuggets
per second and we had
dynamically configurable system
because once again, we
cannot reboot, right?
So we had containers that had queues
called shared pointer events.
In this case, I'm gonna do
a little demo that shows
how it could have possibly worked.
So imagine I have a
vector of data in my event
and my producers create
shared pointers to those,
put them on the queue which is simply list
of shared pointers, right?
And consumer, we presume,
takes it off the queue
and all the files and (mumbles).
So, you can see the problem
if you're on this for a year.
Right?
The problem is memory fragmentation.
I had this little demo last week and
just with 50,000 events I
ended up with a system break
of 1.6 gigabytes at the end of RAM.
Double events, double the system break.
Those numbers come from our
infosystem called Linux.
This was just undefined,
usual plain (mumbles).
Of course we could fix that
by maybe using different map work
but the traditional
and successful approach
to dealing with memory
fragmentation has always
been to use pool allocators,
which are now thankfully
a part of standard C++.
So how do use a pool allocator?
How do you plug a pool allocator
into your program in C++?
We need to create a pool
object which has to have
a scope larger than the
scope of all containers.
C++ now has a whole set
of different pool types.
I'm gonna do this one which I'm accessing
from multiple threads.
I have to replace my STD
vector by steady PMR vector
and provide a constructor
argument which tells
it which pool to use.
This is an example of a stable allocator,
an allocator that takes
a constructor argument
and so every standard
container has constructors
that take allocator out of this.
If you have a constructor
for vector with data size,
that is a constructor that
takes it size and an allocator.
If you have a constructor
for vector that takes a pair
of iterators, you have
another one that takes pair
of iterators and an allocator.
And this applies to both PMR allocators
and the old allocators
of the part of the type.
So all you have to do is
provide it as a constructor
argument, provide container
type and that's it
except for to make
shared I had to spell out
what goes on behind the
scenes a little bit.
But if you are working
with standard containers,
that's very easy to plug
into a pool allocator now.
When I had to do it, I had to
write my own but as I said,
that's not what you want to talk about.
So now my system break doesn't grow
but my (mumbles) grows and it
stays a much reasonable size.
The effects of using pool
allocators is multi-dimensional.
There are many different
measures by which you can see
the effects of performance
and then if you want to hear
all about that, visit
tomorrow's talk by John Lakis
called, Local Arena Memory Allocators.
This is, his numbers
are much more reliable
than my one off demo I just had.
Now, speaking of those
dimensions of performance though,
one thing that should be
obvious is that you have
single synchronized memory pool
access for multiple threads
so obviously I should be
contention and this will not scale
if I increase the number of threads.
Solution to that is another
popular pool allocator
provided by Intel,
threading building blocks.
This allocator is a size
segregated pool allocator
meaning that each allocation
size goes in some pool
and it's also thread specific.
So each thread has its own set of pools
and there is, as you know, contention.
Many people use this
without knowing it exists
because all TBB containers use
one of their own allocators
by default prior to standard allocators.
But, you can also plug it
into your own containers
quite easily.
Once again, it doesn't
take any special arguments.
It goes into your template argument list
just as I state, for
displacement for the allocator.
In that case, if I go and
replace my vector int comma,
scalable allocator and I replace my list
and I replace my make
shared with allocate shared,
then even though this
demo wasn't really built
to demonstrate concurrency,
it was built to demonstrate fragmentation.
Still concurrency affects
it quite visibly here
with TBB allocator.
I don't see any change
in the time it takes
to transmit this large number of events
whereas the synchronized
pool allocator encounters
increasing contention, notice
the vertical scale here
is logarithmic.
Number of seconds it took to run.
So those are a couple of
pool allocators you can use
tody just now, just
right away except C++17
pool allocators, I don't think
they've been implemented yet
by any of the vendors
so I had to actually use
the boost implementation
of that to achieve it.
But hopefully it will be
available as soon as you have
full C++17 support.
So now for something a
bit more fancy, literally.
Imagine you have to process a lot of data
as fast as possible using
thread and safe code.
That's something I had to
do once and the solution
that we came to was to use shared memory.
We put all our data in a
large shared memory container,
created multiple processes,
each of which was manipulating
elements in the container
concurrently with other processes
so that within each process,
you have only one copy
of that thread and safe library.
And the contents of the shared
memory had to be a fairly
complicated data structure
which, in our case was
a map holding strings,
vectors, lists, maps (mumbles).
C++ container.
But for a second demo,
let's talk about vector.
What happens if you put the
vector in shared memory?
In this case, I'm gonna use boost APR
which is pretty straightforward.
You have, you get a shared segment.
You can call that
constructed net that it takes
the type to construct and
the constructor arguments
to forward.
And it just places that
object in the shared memory.
Then go ahead and do
that and another process
attached to the same segment,
pull out the same object.
I'm getting a segfault when I
tried to access its elements
so why does that happen?
So let's look at what
is actually in a vector.
A vector is a data structure
with four elements,
four data elements.
It has the data point and
the size, the capacity,
and the allocation.
The data points to a variety of elements
which exist somewhere
where the allocator told it
to exist.
In this case, in process number one I had
the shared memory segment
in which I placed the object
but the data pointer is still
pointing at my heap because
it used the default allocator
which was the secret
(mumbles) container.
My second process, I
attached to the same segment.
I saw that vector, started
integrating and another segfault.
So to fix that, obviously we
need to look at an allocator
to point the data back into
the same shared memory segment
as where the placing your own vector.
Now writing that was really
(mumbles) but boost has
ready made allocator that does just that.
In fact, a whole bunch of
allocators that do just that.
So to use that, it's
another case of an allocator
because part of it types,
they just supply it to the
container as a template argument
but it's a state allocator
so it has to use one
of those constructors
that every container has
that they can extract allocator argument.
So in my case, I'm going
to use the constructor
with the (mumbles) and I also
have to give it an instance
of an allocator or none
of this is convertible
to an allocator type.
And now, the vector is
constructed in that segment
and it's data is also in the same segment
and my other process attaches
to that leaves it just fine.
Alright.
But as I said, I had a massive container.
I had the map and other things.
So, if you try to do that
with, if I tried to just take
regular vector or vectors, for example,
and if we use a standard
vector as the row type,
we're gonna have a segfault
for the exact same reason
as before because even
though my outer container
uses an allocator that places
the rows in that same shared
memory segment, that innovator has no idea
that it's going to share memory segment.
It's using the default
allocator and it's still putting
its data on my heap.
So, another process I might
attach to that same vector
or vectors.
Start iterating and I get
my segfault just as before.
Now, if I go to replace the vector event
up there in the row type
with vector, comma allocator
for shared memory, it
wouldn't even compile
because that allocator
requires an argument
and there is no way for me
to pass that constructor
argument down to the row type.
Solution for that, boost
actually has a sound solution
which predates C++11 but
as of C++11, not 17, but 11
you have scoped allocator adaptor.
I really wish it were available
when I had to use this
but scoped allocator adaptor
does exactly what I want
to do here.
It takes the constructor
argument, I pass to the
outer container and the
forward the constructor
to the nested containers
across different row.
If I have a vector or
vectors or vectors of maps
of strings, all of them
will be constructed
with the same constructor
argument as they pass
to the outer container.
You might, in this demo,
I'm placing the vector
or vectors in shared memory.
I'm giving it my constructor
argument that specifies
which segment to use and
now I can do whatever I want
in that data structure and create new rows
by those codes and I will
use the same segment.
In that process I cannot touch it,
specify the same allocator
and use it and now we'll
see all my elements just fine.
Scoped allocator adaptor
is real intimidating
if you look at the spec.
So, don't look at the spec, just use it.
It's not that hard.
So even though I said I'm
not going to talk about
implementations, there is
one implementation detail
that is actually worth noting.
When you attach to the
shared memory segment
from different processes,
that segment appears
in a different address in
each process memory space,
which means the vector's data
pointer cannot be an address.
It cannot point to your process memory.
It had to be a relative
offset from something
that exists in that same memory.
You cannot have same shared memory segment
in case of (mumbles).
It's actually called offset pointer.
This is an example where
an allocator has changed
what it means to point to an object.
It's what's called fancy
pointers like that fancy German
white-haired pointer in the corner.
Or, synthetic pointers as Bob
Steagall likes to call them
and if you want to learn
more about those, don't miss
that presentation from
C++ Now which talks about
how well synthetic pointers are supported
by different libraries and
what you can do with them.
And shared memory is the typical
shared pointer that stays
but it's not the only one.
It's only the case when
you have special memory.
You need special meaning
to what it means to point
to something.
There's also something
that used in GP program.
Something called pinned memory.
Pinned memory is the memory that exists
in a specific address and
is used to communicate
with GP devices that's
visible from the host memory.
So you can use direct
memory access to communicate
and something you get in
CUDA with CUDA host Malloc
and you can get it in
C++17 (mumbles) flags.
And, of course if you
want to put the container
in that memory, you got to
use an allocator for it.
GP library has something with that.
Frost has pinned allocator.
Data Flow has an allocator
but that is not exactly
still compatible and most
computers have an allocator
which is also called pinned
allocator just like in Frost.
The one difference is that
most users have fancy pointer
which holds a reference
to the device buffer
and the minutes because
this library's built
to actually operate the
data at least on a GP
and that's one case where I
had to use non-standard vector.
The reason being, a
standard library is designed
for fancy pointers that
have a met into host memory
because when you call (mumbles)
vector, you're gonna get
a VO reference not a
fancy, synthetic reference
which would be able to
access them directly.
So this pointer is a
bit too fancy for them
but if you use Frost, it
has the same allocator
which uses regular, plain, ordinary T star
as the data pointer.
Now,
there are many more readily
available allocators out there
so just a first few search
results in the data core search.
I was on page three of 55.
And I could be talking
about them for all day
but the main point I want to make is that
you don't have to write
allocators yourself.
You can but you don't have to.
The whole reason allocators
being what they are
is that they are reusable,
packaged library components.
Components that quite
possibly have been written
and if they haven't, they
should be written in a way
that other people can benefit.
And, if you do want to
write them yourself,
there are plenty of talks
about doing just that
at this particular conference.
So I think we have time
for a few questions
if you have any.
- [Man] Should I use the microphone or no?
(man speaking softly)
Okay.
So one side of the coin
in using allocators
is plugging in an
already written allocator
into an already written
allocator (speaking softly)
but if you write your own
type or wants to be a user
of the allocator (speaking softly) before
and it's also an extremely onerous.
Like, allocators have
all these obscure traits
that you have to check very carefully.
You know, copy and move.
Do you have suggestions with that?
Is there any library that you know of
that makes this a little bit
easier that you'd recommend?
- So the question was that
writing allocator aware types
is onerous because of
all the allocator traits
that I have to look at and
the allocator propagation
semantics you have to take
care of and whether there
is an existing library
that simplifies that.
So, no I don't think there is one.
In fact, this talk by
(mumbles) just before this one
was about one of the ideas
of how it could be made
simpler in the future and I
believe talk by Pablo Hopper
will be about the details
of propagation traits
and all those related things.
But, yeah, the status quo
is that writing your own
allocator aware container
is not (mumbles) especially
if you want to be able
to use that allocator
into something that,
propagate it down a chain
into something you hold.
Anything else?
- [Man] It's an allocator
with the name random grows,
are they to be used a lot
in like testing, generating
random memory problems to be allocated?
- So the question was
whether there are allocators
that generate random memory problems.
I imagine yes.
I've seen allocators
and I've used allocators
that I used for
instrumenting and debugging
and analyzing and there were
problems like allocators
used for unit tests.
Haven't used the phizer allocator.
This would be a nice
segue to our next talk
which will be right after this one.
But yeah, it would be
interesting if there is something
like that available right now.
I haven't encountered it yet.
- [Man] Were the boost
compute allocator that returns
its own fancy (mumbles) splitter--
- Right.
- [Man] And then you have to use
a (banging drowns out
speaker) vector with that,
what do you get when you
de-reference one of those
fancy pointers?
- Okay, the question--
(man speaking softly)
Yeah, the question was what
happens if you de-reference
a fancy pointer that you
get from boost compute
device point.
What you get from that
is a special iterator
that they provide.
A device iterator, which is
essentially a fancy accessor
to the data.
It's not directly plugable
into standard containers
because it's not a (coughing
drowns out speaker) iterator.
You cannot just take a (mumbles)
and do pointer arithmetic on that.
Yeah?
- [Man] I couldn't understand
the chart very well.
There was one part of X (speaking softly).
Can you explain that problem?
- So the question was what
was the 1,000 X was about
in my slide on memory
fragmentation improvement.
So when I ran this demo using
standard allocator Linux,
I got my system break of eight gigabytes.
So when you use Malloc and Linux you have,
your allocation goes
either into system break
which is the first continuous heap space
or it goes into M map areas
which are all over the heap
and system break usually just grows.
It doesn't shrink.
And if you deallocate
something, you don't actually
incur a system call.
You just leave a hole in your system break
and if you're lucky it's
gonna be filled back in
if the Malloc understands,
if it is able to find it
next time it allocates
something of the same size.
I had many mismatched size allocation here
and those mismatches caused
it to build up a very
fragmented heap.
So I had a massive system
of three gigabytes whereas
my run through the pool
allocator I ended up
with not growing that system rate at all.
At the end of the run, I
had only three megabytes.
Now this is one of them
so your mileage may vary
but it represents something
that we actually saw
in production dealing with
something run for a year
at the time.
Alright, anything else?
Going once,
going twice,
alright, thank you for
listening to my talk.
(audience clapping)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>