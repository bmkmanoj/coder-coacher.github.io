<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Hans Boehm “Using weakly ordered C++ atomics correctly&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Hans Boehm “Using weakly ordered C++ atomics correctly&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Hans Boehm “Using weakly ordered C++ atomics correctly&quot;</b></h2><h5 class="post__date">2016-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M15UKpNlpeM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is Hans Bohm I work for the
Android part of Google I'm going to be
talking about how to use Atomics and in
particular weekly ordered Atomics and
c++ correctly which is sort of a
misunderstood understood issue and this
is an attempt to reduce that misuse a
little bit why do we care about Atomics
at all well the general context here is
multi-threaded programming in C++ in
general if we're writing a
multi-threaded program and C in C++ the
rule is that we have to avoid data races
we should make sure that a variable or
an object formerly a memory location is
never accessed while another third is
modifying it that's the default rule we
typically enforce that by using some
sort of synchronization mechanisms like
mutexes the cannot economical example is
up here on the slide so if we want to
increment a shared variable for example
we might protect that with the mutex and
then go increment the variable making
sure that nobody else has access to X at
the same time making sure that all other
accesses to X are protected by the same
mutex and everything works just fine and
that's the normal way of doing business
it has a bunch of advantages and a bunch
of disadvantages the big advantage is
that it operates at different atomicity
granularities so i can use the same
scheme to in order to protect
complicated operations that that update
complicated data structures so long as I
make sure that only one posts and
updates that data structure at a time by
using new Texas by using a mutex to
protect the data structure everything
works just fine that's the big advantage
here the disadvantage that people always
associate the disadvantage is that
people associate with this some real
sometimes imagined as you saw in some of
the the previous talks is
there are some concerns about lock
ordering and deadlock that get
introduced here those are those are real
and need to be addressed in the lock
case sometimes they can be there are
other ways to address those as well that
are very similar to locks here you
already hope is the transactional memory
technical specification that largely
avoids this issue so there are other
ways to avoid that but certainly that's
that's a problem with the lock based
approach the lock based approach also
basically doesn't work in the if you're
trying to provide mutual exclusion with
the signal an interrupt handler because
you fairly likely do deadlock if you get
a signal or an interrupt while you're
already holding a lock you're not going
to be able to acquire that lock again in
the signal interrupt handler another
disadvantage is that if you particularly
if you care about predictable execution
times it's possible that your operating
system schedule may be uncooperative and
pre-emptive thread while you're holding
it while you're holding a mutex in which
case for a while the other threads may
not be able to make progress either and
that occasionally causes problems the
biggie here at least in perception is
that people think of mutexes as being
slow and therefore they tend to avoid it
so before c++ 11 the experience well the
de facto walls were that really all
accesses needed to be protected by locks
this was the tool mutexes in some form
or another with the tool you had
available in order to prevent races
between threads on the other hand
programmers generally were not happy
with that but what we generally found
was that a lot of programmers ended up
circumventing the rules in one way or
another and often those rules were very
those violations those circumvention
swerve Airy questionable so the better
ones actually consisted of abusing the
the volatile keyword in C and C++ which
already existed sometimes combining it
with assembly language for fences and so
on in other places people just argued
that well it's okay if I just access
this one variable here without a mutex I
know everything works
actly the problem is by doing that
you're lying to the compiler and the
compiler will draw some conclusions from
that which will be long and we have no
idea what will actually happen when you
upgrade to the next compiler that that's
more aggressive about that sort of thing
the gains that people expected from
circumventing these rules were sometimes
real and and sometimes imagined we
London won there was an audio talk that
actually discussed some of the
trade-offs between using locks and
trying to program without locks but we
also won in the keynote it doesn't
actually matter people will use it
anyway because they think it's faster
even if it isn't so what we did in C++
11 is we and actually introduced a
notion of an atomic variable that of an
atomic object that actually can be
accessed concurrently outside a critical
section so these are these are objects
that are accessible without that can be
safely concurrently accessed and they
behave atomically so they it looks as
though if I update one of these in one
thread the it looks to the other thread
as though the update has either been
performed completely or not at all
they're indivisible in that sense
furthermore by default if I don't do
anything else they preserve a reasonably
nice programming model they still ensure
that I get interleaving based semantics
I can still view multiple threads as
being executed as though there was some
scheduler deciding at each point to
randomly pick an instruction from one of
those threads and executing it next
things still all look sane and that all
works reasonably
it's reasonably easy to read to deal
with these things in large part because
in fact these Atomics behave for most
purposes if I'm not concerned with
signal oriented up handlers they behave
exactly as though they were operations
that will in fact protected by a mutex
in fact they could be implemented that
way
so these Atomics are basically as easy
to use as a mutex provided all your
critical section only have a sections
only have a single operation on a single
variable in them then they're exactly
the same they do have the advantage that
they a bit faster than the mutex at
least in the the easy cases where you're
just replacing a critical section that
has a single operation inside the
critical section on the other hand
already with this kind of atomic you
have to be careful there's a large
research literature on basically
programming complicated algorithms in
lock-free ways using these kinds of
atomic operations and they take there is
state to take a more complicated
operation that in fact operates lots of
lots of distinct memory locations and
make it look as though all of those
operations together on a single atomic
operation even though they're
implemented in terms of many separate
atomic operations and that's in fact
quite tricky people who teetley get
those algorithms long doing this sort of
thing is something that you should be
really careful about if we take a closer
look at these sequentially consistent
atomic operations that c++ provides in
fact the implementation gives us a whole
bunch of properties for free essentially
just by using the Attar these atomic
operations clearly the implementation
has to guarantee that the operation
looks indivisible or nothing to other
threads but in all order to preserve the
saloon this interleaving execution based
illusion the fact is so that it still
looks like and let's just do one step at
a time in thread order we have to
provide some other guarantees which I
could sort of very roughly characterize
as I have on the slide I'm lying a
little bit here I actually need a little
bit more but these are the important
ones
so basically the guarantees I need that
I'll be relying on is if I execute an
atomic store if I update an atomic
variable I need to in fact make sure and
we'll see examples of this later on I
need to make sure that all pile memory
operations in fact become visible to
other thirds before that atomic store
there's no reordering sort of issue that
that goes on in on modern hardware with
modern compilers that's a significant
constraint similarly on the load side I
need to make sure and again we'll see
examples of this loads must complete
before subsequent memory accesses take
effect and that's another restriction
that that has to be enforced by the
implementation here it also turns out
for reasons we'll see in one of the
subsequent examples I need to make sure
that if I have an atomic store followed
by an atomic load those are actually not
reordered by the implementation and I
have on the slides here's some some
explanation of what these costs these
are actually not free on most
implementations so the indivisibility
generally is free on modern hardware the
store ordering is free on x86 but
generally it costs you a memory barrier
instruction on something like um v7
loads I basically have a similar
situation and then it turns out that on
both arm v7 and x86 ensuring the the
ordering of an atomic store and a
subsequent atomic load is expensive and
so to illustrate some of these things
let me give you some simple examples
here in terms of sort of plain atomic a
plain sequentially consistent C++
Atomics so this is sort of the most
canonical example that you can think of
in some sense it we'll revisit this
later as well it's often called the NP
or message passing example I'm using an
atomic variable just to communicate so
some one piece of information from one
pair to another so in this particular
case third one is initializing some data
and then setting a flag setting an
atomic flag
- - saying that the data is initialized
and it's now safe to access their two
leads the atomic flag and then checks
and then goes ahead and reads the data
if it's set because now it knows it's
been initialized
there's no way basically the atomic is
being used here as synchronization to
make sure that they can't be concurrent
accesses to X because said two is using
the atomic to wait until that one is
done with it so there's no data race on
X no concurrent access is on X and
everything works correctly the
implementation has to ensure that x
equals 17 in fact is visible to the
other thread before egg before the
assignment to X in it which might be a
constraint that's not immediately
apparent but if you put yourself in the
compilers position normally the two
assignments and thread 1 look completely
independent and it looks to the compiler
for example like those could be
reordered because they're changing
different locations but it's actually
important in this case that the compiler
not reorder those and make sure that the
assignment to X to X actually is visible
if the assignment to X in it is visible
to another thread and that's also too
bundled up in the implementation of the
Atomics so this actually among the
properties that I listed earlier this
needs the stall that I'm guarantee it
needs the s guaranty x equals 17 needs
to be visible before the assaulter x in
it
similarly the load from X needs to be
visible before the store it needs to be
visible after the load from X in it you
might think that that's automatic
because it's inside your conditional but
modern processors speculate they'll
guess the outcome of the conditional
that won't stop them so that's an
explicit guarantee it doesn't need this
total load ordering guarantee actually
so here's a slightly different example
that explains some of these again some
of the implementation requirements that
we have so what's going on here is Ted
one stalls to two acts initially
everything as false
and then checks if if the variable Y the
atomic variable Y is still false that
means there - hasn't done anything yet
then I can go ahead and turn the
east-west traffic light to green said -
goes ahead and stores - - into the other
variable and does this a complimentary
thing turning the north-south traffic
lights to green and the argument here is
that this is safe because one of those
because either fed 1 or 32 at least
we'll need will see a variable that's
already set to 2 and therefore they want
both tons on the green at once this
particular example also requires the
store load ordering guarantee I need to
make sure that the compiler it doesn't
rearrange the low instead 1 it doesn't
really load from Y with the store 2x
because if the store happens second then
in fact you can convince yourself pretty
easily that it's possible for both of
them to turn the lights green and things
will go on so this is an example where
we actually need that SL ordering
guarantee that I had before so this sort
of gets us to what does all of this cost
and why people want weakly ordered
Atomics so on a simple micro benchmark
'memory fences and often called a memory
barrier typically cost something between
2 and 200 cycles which is a pretty big
range and architectures really are very
variable but we I think we're sort of
generally converging on a dozen or two
dozen cycles on most on most of the
modern processors this is a cost this is
a cost that's negligible maybe almost
compared to a cache miss or memory
contention or something like that but
it's still a very expensive car
expensive cost compared to most
instruction execution and the problem is
if we went back a couple of slides here
in order to ensure these various
guarantees it turns out we need memory
fence instructions we need the or memory
barrier instructions in order to have
the hardware guarantee that these
auditing constraints are not violated at
the hardware level so in order to
implement Atomics I need to make sure
that the compiler doesn't the order of
the instructions and I need to make sure
that the hardware doesn't the order the
instructions typically make preventing
the hardware from the ordering the
instructions is actually the more
expensive part that requires these
memory instructions so basically so now
I have several tens of cycles of
overhead associated with each of these
sequentially consistent atomic
operations potentially it actually turns
out there's much less of an issue on x86
on x86 I only need knee defense for an
atomic store I don't need anything extra
for an atomic load so the cost is
actually more modest on something like
um v7 I generally need at least one
fence for every operation for an atomic
read-modify-write in atomic increment or
something like that I need to and for
armed v8 I have sort of costs that are a
little probably somewhere in between in
general because essentially the c++
sequentially consistent atomic semantics
are implemented in the hardware they are
sewing I I can do a little bit better
sorry there's actually something that's
worth observing about the the cost here
so in passing many people think of the
extra costs associated with these memory
fences to implement sequentially
consistent Atomics as being very high
and prohibited basically getting in the
way of scaling of your of your software
that's actually not the right quite the
right way to think of it and this
happens to be a gap of the performance
gap from a few years ago of a sieve of
eratosthenes examine this a simple sieve
of eratosthenes parallel program that
illustrates an effect which i think is
fairly typical actually so the blue line
here is the Citroen example running with
mutexes protecting concurrent accesses
the green line is essentially using
sequentially consistent Atomics and the
red line underneath you can think of
using relax using relaxed Atomics that
we'll talk about which just avoid
defenses which happens to be safe in
this case what you see here is that in
fact there's a significant difference in
performance at low passes accounts this
particular machine is very hardware very
memory bandwidth limited at high
processor counts basically you still
have all those over all that overhead
for the fences but in fact the overhead
is being overlapped between different
threads so it's not actually adding to
the total on time because the bottleneck
is still the memory so in fact it at
least on this particular machine at high
passes accounts it actually doesn't make
all that much difference the cost is
there but it says sort of in fact it may
make you scalability really lot better
because it slows down the mutex version
in some ways scales best because it has
the highest single processor account
cost and at 32 processors it doesn't
really make much difference anymore so
in order to address this cost
nonetheless because many people still
want to reduce the cost and in fact
there are many cases with actually is
spent official to reduce the the cost
C++ 11 and later also provide weekly
audit Atomics that relaxed the ordering
guarantees that are normally associated
with these sequentially consistent
sequentially consistent Atomics so it's
no longer these no longer preserve the
illusion that threads execute by
interleaving steps of individual threads
that we just always execute the next
step of one one thread or another things
can appear to be very visibly to be
reordered and to add to execute out of
order we no longer hide that and we no
longer hide that in order to improve
performance and to eliminate the these
memory fences that are needed to enforce
the stronger ordering primarily at all
they also end up giving the compiler
some more degrees of freedom which may
or may not help
so we basically give you the option for
atomic accesses to explicitly specify
memory ordering and I'll talk about
basically two kinds of relaxations of
memory ordering here in this talk so we
have memory order acquiring memory order
release and for lead modify operations
read modify write operations the
combination acquire release these
basically preserve the ordering that you
need most of the time or many times on
the other hand they they do away with
what I term the SL ordering they no
longer guarantee that a store followed
by a load can't be reordered that the
ordering is allowed the other
reorderings are still prohibited the
other one we'll talk about is memory
order relaxed which sacrifices basically
all the ordering guarantees leaving only
the indivisibility that's associated
with Atomics it still gives you one very
weak the ordering guarantee which is
that accesses to the same memory
location can't be reordered that would
be somewhat confusing and we decided not
to export an atomic type that doesn't at
least give you that guarantee so memory
order relax is much cheaper when you can
use it on something like especially
something like um v7 because it
basically avoids all the fences that are
normally associated with Atomics okay
and I'm going to insert lots of warnings
here because I don't really want to
encourage people to use the to use these
but there are cases when they're
appropriate and I'm trying to sort of
get the balance of cost here so the ugly
side of these weekly ordered Atomics is
that they're extremely complex and
difficult to understand so use them with
with extreme caution as I'll try to
convince you in a minute here the rules
are not at all obvious and in fact the
wolves are so bad that even the
committee doesn't really understand it
so if you look at if you look at the
definition of memory order relaxed this
in a lot of hand waving in the standard
the C++ 14 standard the C++ 11 standard
was more precise we decided it was wrong
so we threw that out and we decided we
didn't really know how to define it
precisely so we replaced the the offense
offending wording with hand waving in
C++ 14 this is not well liked by the
people trying to do formal program
verification who don't can't make heads
or tails out of it so you should be
aware of that issue if you can formally
verify a program don't use memory or a
relaxed I'm not talking about memory
order consumed here which was I think
it's in its current form I can safely
describe as a failed experiment it's in
the in C++ C 11 and 14 and in 17 will
basically have a disclaim don't use it
yet until we get it right so so follow
that advice the other point that I'm
only gonna mention in passing here but
that's actually important is that if
you're using these things if you think
that you can use weekly audit Atomics
inside and clever implementation inside
a library in that your client will not
will never be able to notice you're
probably long for for non-trivial uses
of these the general experience is that
it's very difficult to completely hide
these things inside a library the
semantics tend to show through to the
client so you should keep that in mind I
won't go into the details there so my
advice here that reiterating is first of
all if they work or use mutexes or
transactional memory if you have a
simple case where you basically all
accesses that are protected by a given
mutex only acts as a single variable and
the only access at once then it's easy
and straightforward to replace that by
sequentially consistent atomic you'll
get the same semantics it's no harder to
reason about and you'll get a little bit
of speed out of that if you need to
access variables from signal handlers
your first line of de general you don't
care that much about performance you
should generally use sequentially
consistent Atomics and use them very
carefully because very often you will
end up having to use more complicated
lochley algorithms which are very
difficult to get at I just it's
difficult because it's difficult to get
the right level of atomicity if you
measure and the you program really runs
too slowly you really want to use use
Atomics for performance you should
definitely should think twice but you
can consider using more complicated
lock-free algorithms with Atomics and
you should at that point you may also
want to consider using weekly audit
Atomics but remembering to be really
careful and that these things really are
back bug magnet and many of them end up
getting used incorrectly so what I'm
going to do for the rest of the talk
here is basically go through some of
some pitfalls to illustrate some of
these issues and then give you sort of
with give you a collection of recipes
that I've encountered for which weekly
audit Atomics actually do seem to make
sense and be relatively safe there may
be some at the end that are complicated
most of the ones that if I have enough
time the ones at the beginning of are
fairly simple and understandable the
ones at the end are more sort of
illustrations of how these things can
get really complicated and recipes that
might be useful to follow even without
sort of fully understanding why they
work so the the initial pitfall I want
to stop here is start with you is the
obvious one that applies to all Atomics
not just weekly audit Atomics which is
that you have to be careful about the
the the granularity of atomicity here so
in particular if you have an atomic
variable this is well illustrated by the
fact that x equals x plus 1 is not the
same as X plus X plus plus x equals x
plus 1 consists of two accesses to X it
consists of an atomic load in increment
in an atomic store if two threads do
this concurrently they may both end up
leading the value and writing back the
same value rather than actually
incrementing it twice X plus plus on an
atomic Tom
is really an atomic increment because
that's the way it's defined so it really
happens atomically has one indivisible
operation and you're guaranteed that if
two thirds do this concurrently X will
get incremented twice only the the
actual functions on atomic of T only a
single function call us is indivisible
not combinations of them you can hide
that with really clever algorithms
sometimes but that's way beyond this
talk so now getting more into weekly
audit Atomics the ordering constraints
you're really quite subtle this is sort
of a meta comment if we get around to
the examples at the end then you can see
exactly how how supple the ordering
constraints can be and this is as I said
earlier this is really a very common
source of errors so that's sort of the
the meta one now I'll get into some more
concrete pitfalls here some of which
have actually get turned into more of a
problem recently as as hardware and
compilers have gotten more sophisticated
so this particular example is an attempt
to take the example I have before which
required the store to load ordering and
to relax the memory ordering on that in
ways that I in Kanata not correct so
this example is not correct in this
particular case I've really only relaxed
the ordering on the stores store
operations the the load operations are
still sequentially consistent many
people seem to have this idea which
happens to hold on some architectures
and not others that because I use a
sequentially consistent operation in
here that somehow orders the other the
other weakly ordered operations that's
not in general - that's not the way
these things are actually defined so if
I relax the store operation to a memory
order release that gives away my SL
guarantee my store to load get order and
guarantee so what happens in that case
is the compiler now is no longer
required to actually preserve the
the compiler and the hardware no longer
required to preserve the order of the
store and the load so in fact the
compiler may choose to do the load first
in both both threads see that both of
them in fact are still false go ahead
and then execute the stall in both
threads and at that point both threads
will go ahead and turn the light to
green and I'll get the result shown on
the slide here interactions with other
synchronization mechanisms like locks
also not really what what people expect
I've seen some sort of some fairly
widely distributed code that basically
falls back that basically checks am i
compiling with C++ 11 do I have the
appropriate atomic fence operations
which I haven't really talked about here
yet if so use those to enforce ordering
otherwise I'll just use some critical
section I'll use a mutex to enforce
ordering between these different
operations because after all a critical
section is going to ensure that things
don't get me ordered around the critical
section that's really long so here's
sort of the canonical example here again
now I've changed both of the the store
and load operations to be relaxed and
I've tried to enforce ordering by
putting a critical section in the middle
there it turns out that's not sufficient
the easiest way to think of that not
being sufficient is a critical section
normally has in there if you look at the
details of the memory model a critical
section ensures that operations inside
the critical section don't move outside
the compiler can't move operations
outside the critical section since that
would unprotect them and clearly break
the code but it can move things into
critical sections so in particular the
memory model rules are written in such a
way that both the stall and the load
here can move into the intervening
critical section and pass each other so
that this in fact does not enforce
ordering between the store and the load
this one is particularly
nasty because it so happened that most
critical section implementations in the
past have in fact enforced this sort of
ordering we're starting to see some now
they don't so I expect some code to
break as we work this out and that's
precisely the the point here
traditionally compilers didn't to the
order cost synchronization they still
don't really and the synchronization
operations were implemented as Hardware
fences increasingly they're not
implemented as Hardware fences again if
you look at something like on v8 in fact
these synchronization operations have
are only one way the ordering barriers
you can move things into the critical
section just not out of them there were
some older architectures like Itanium
that also did that another pitfall here
many people think of one way to enforce
odd to enforce ordering between Atomics
is sort of data dependencies or control
dependencies in some way certainly if
one load is dependent on another they
have to be executed in order that
unfortunately that notion of dependency
turns out to be very hard to define
precisely and here's sort of an
illustration I've attempted to
illustrate why it's so difficult to
define and why in fact it doesn't hold
for any reasonable definition so what
I'm doing here is I'm first loading acts
and then I'm using the result of that
load of X to load a field of the
structure that X pointed to and you
might think that those in fact are
guaranteed to be executed in order and
in fact at some very low hardware level
if you translate it this naively to
assembly language actually amusingly
enough they would be executed in order
on modern hardware for reasons that have
to do with job are rather than see pause
pause but in fact if you compile this
code with the C++ compiler and look at
the result they're perfectly reasonable
and acceptable transformations that
violate that so what can happen here is
in this particular case I actually have
this other comparison of the result of
the load of X - pointer and only if that
comparison is - then I actually
dereference it the problem is that my
compiler might actually notice at that
point that well pointed and Y have to be
the same in that context so why don't I
just go ahead and dereference Y instead
of instead of pointer because I know
they're the same so I'll go ahead and do
that okay that's part of the problem the
other kinds of part of the conspiracy
here is that the hardware decides that
well I want to execute your code quickly
I don't want to really check to wait if
that condition is true or not I'll just
guess it's - and go ahead and then I'll
verify afterwards that it's correct and
if it wasn't I'll back up and routine
and modern processes do that routinely
so what happens in this case I can in
fact execute the second load off of Y
early on before knowing the truth of
that that condition and then go ahead
and execute the load of X in reverse
order and afterwards I may verify yeah
my guess was correct the condition was
satisfied
so therefore in fact it's fine I did
that I think but now the the second load
actually appeared to execute before the
first load ok so I'm now going to switch
gears from from pitfalls to how to
actually do this so the the general wall
here is for actually using these relax
operations is if I'm gonna use the
acquire release operations the only
guarantee I really get is that memory
operations preceding the release
operation preceding the released store
are visible after I do a load that sees
the result of that store other kinds of
operations other kinds of reasoning
about ordering are still invalid here so
that's tricky we'll look at some some
examples you
this actually works relax really only
ensures auditing for that operation for
operations on that particular memory
location and if I look at one of these
operations if I look at the result of a
relaxed operation I really can't
conclude anything from that about where
the rest of the program is or what the
rest of the program state looks like and
as I said consume we're going to avoid
yeah and again the disclaimer here that
if for general use of these you really
need to understand the memory model in
more detail than I'm presenting here so
these are the common cases so a common
case where I've seen relaxed used
successfully in at least in ways that
are not obviously broken is to update
single basically single location data
structures which are actually actually
surprisingly common in real code bases
as far as I can tell so these are
basically single worth data structures
that don't interact with other with
other data so I'm just accumulating
information in the parallel parts of the
program and then only looking at the
data structure after all the information
has been properly joined after I've
properly synchronized with the other
threads so that I know where they are so
the most common example of that is
clearly is clearly an atomic counter
there are better ways to employ their
ways to implement more scalable counters
but a really simple way to to correctly
implement a counter that's only read at
the end of the program is to use one of
these atomic update operations like
fetch add to increment the counter and
that operation can use memory order
relax because I don't really care about
the result of that value so I'm not
going to typically I'm just gonna throw
away the result that I get from the the
fetch add so I'm definitely not going to
make any decisions based on the value I
got back I just want the final correct
value at the end I might also accumulate
something in a bit set in a short bit
set by just oaring into it as I'm
running and only look at the final bits
set at the end after I've properly
synchronized with all the other threads
and I know all the other threads are
done with what they were doing
and those work correctly a warning sign
a place to be careful here if you're
looking at a codebase the place that
something that looks really suspicious
is actually looking at the result of
these things if you see a relaxed
obligation that looks at the result then
you really need to look closely another
common use case for memory or a relaxed
where it's entirely safe is if you're
performing an operation that just
doesn't affect program correctness
that's sort of important to making the
program run well efficiently but your
program is not doing a break basically
no matter what you get so a common
example of that as a compare exchange
loop what you're doing there initially
typically you load a value then you
compute a function here foo on the old
value and you try to you replace the old
value with foo of old
if it hasn't changed in the meantime now
it so happens that it doesn't really
matter if the value you loaded at the
beginning was long all that's going to
do is make the while loop go for one
more iteration so that's entirely fine
we're not relying on the we're not
relying on the result in any way another
common place to use relaxed accesses
actually is if you're accessing an
atomic variable in a context in which
you know that there's no racing access
so you actually didn't don't need the
the access to be atomic because you know
nobody else can be modifying it so a
very common example of this is is what's
called double check locking in this case
what I'm doing is I want to initialize
some variable X on demand only when I
need it to be only when I need to use it
so I'm gonna do it lazily but I'm going
to do it in a way that avoids locking on
the fast path so that avoids any lock
acquisition unless I'm actually going to
initialize the variable and we'll so if
we have time we'll see this and slightly
they find this
version of this and a few slides again
so what I do is I first check has X been
initialized and that's in this slide
that's a regular atomic access with
normal sequentially consistent
guarantees so I know that if X has been
in issue if I see X in it equal to two
then I can see all the initialization
work that's previously been done by
these SNL guarantees because the
previous initializing fed actually did
an atomic stall at the end and I now see
the result of that load so therefore I'm
guaranteed to also see X initialized so
if but if X was not if X in it is true
then I'm fine if X was not initialized
then I go ahead and acquire a lock
protecting the initialization I at that
point loaded again to make sure it
hasn't changed in the meantime nobody
came in and initialized it in the end of
them but now I'm holding a lock that
protects the initialization so at that
point I actually know that nobody else
is going to be changing X in it because
the only change to X in it is in this
critical section further down which
might be executed by another thread but
that other third would have to hold the
lock so using a memory auto relax
there's entirely fine if I still see
it's not it's not been initialized and I
go ahead and initialize X and set X in
it to two again X in it as an atomic so
at that point X has been initialized and
I do an atomic store which makes sure
that everybody who sees the load result
will see the initialization correctly so
okay so that that was the the last
example for memory auto relaxed let me
switch gears to acquire release here
briefly so the canonical use case for
quietly lease is this simple
communication idea that we saw earlier
on a slide already so this is the MP
example from at the beginning but I
noticed already at that point that I
don't need the SL guarantee I only need
to make sure really that if
in this case again we initializing
something this is the simplest scenario
that if I said if I see X admit having
been set to two by another third then
I'm also guaranteed that X is
initialized this is precisely the kind
of guarantee that I get out of a quiet
release so in this particular case I can
relax that example from the beginning
and use a quiet release ordering
specifying release on the store and
acquire on the load and I still preserve
the semantics correctly so that still
works and this is actually quite a quite
common case if you heard people talk
about I'll see you or something that's a
more complicated case that basically
based on this similar kind of setup we
can also use basically the same case
again already occurred in the double
check locking I warned you that you'd
see there you see it again so this is
double check locking they find further
so again when I check X in it here this
is basically the same thing as on the
last slide I really only need a choir
semantics at the beginning when I do the
initial check has it already been
initialized I need to make sure that I
can actually see the initialization if
it's happened so I need a quiet
semantics there but that's all I need I
don't need the default sequentially
consistent semantics and similarly when
I do the initialization I need to store
with the least semantics to make sure
that piyah the initialization actually
is visible to somebody seeing the seeing
X in the two with the with the acquire
load another fairly common use case for
a quiet release and you can think of
this as really the the place they got
the name in a sense is if I want to
implement simple locks and this is one
of several examples will you Lee sort of
90 99.9% of the time you should use
standard library facilities rather than
implementing your own but if you were
going to implement your own lock you
would do something like what I have on
the slide here in general for lock
acquisition I need to make sure that
operations don't move out of the
critical section so I need to make sure
I need to make sure that the load is not
the load pop of the the exchange there
is not Lee ordered with subsequent
memory operations
that's what acquired us and similarly
for the store I need to make neat to the
converse guarantee I need to make sure
that earlier operations become visible
before I actually complete the stall so
that those are not reordered so for for
lock entry I need a quiet semantics for
lock exit I need to release semantics
now your actual locking protocol might
be more complicated and involve multiple
Atomics in which case this becomes more
complicated but if we're really simple
dumb spin lock or something like that
basically acquire on entry and release
on exit is efficient okay we still have
a little bit of time so let me quickly
go through some some complex examples
which I suspect I won't be able to
explain in the time I have here
but sort of hopefully give you some some
insight into the the kinds of
complexities that you run into okay so
the first one which is sort of an
ongoing saga this is actually what I've
been spending a lot of my day job on
recently is reference counting a memory
ordering for reference counting so the
first observation again is please use
share Potter because then we have to fix
the bugs in only one place but having
said that the basically these reference
counting operate normally if you
implement reference counting it will be
implemented most likely be implemented
in with thread safety guarantee similar
to the standard share put which
basically which do not provide thread
safety for pointer assignment for ship
put their assignments themselves if you
concurrently access if you can currently
assigned to the same share put it in 2/3
very very bad things will happen never
do that on the other hand the share pata
implementation is thread safe in the
sense that if you can currently access
different share putters
because just because they happen to
refer to the same underlying object you
will not get a data race so there are no
data races invented as a result of the
reference counting which it turns out
means that the reference counting
operations underneath you're actually
incrementing the reference counting and
decrementing the reference counting have
to be atomic operations or they have to
be protected by a lot most
implementations use atomic operations so
the question is what memory ordering do
you need to get an T there and people
usually care because this reference
count operations tend to be very
performance critical it turns out if you
think about this long and hard what you
actually need is for the reference count
increment it turns out actually a
relaxed operation is sufficient because
the client op actually makes you has to
make sure that all of those operation of
the reference count increments have to
happen before the final get compensating
reference count decrement so those
already ordered with respect to the
reference count decrement the tricky
ordering guarantee is you need to make
sure that by the time that reference
count gets decremented at the last time
you're actually going to deallocate the
object at that point you can see all
pile operate all Piatt operations on the
object have been visible to you so that
no operations on this object become
visible after after you deallocate it so
the the general way the simple way of
doing this is when you do the the
reference count subtraction to do an
acquire release up perform an acquire
release operation so that in fact
there's a well-defined ordering among
all the reference count deck commands
and by the time it turns out the
visibility of also such such that by the
time you do the last decrement in fact
everything that happened to the object
is visible to you at that point so this
is sort of the pop we agree on it
actually turned out it came up in
earlier discussions today we don't
actually quite agree on what unique
should do and my theory is that it
should actually be it
in need it needs an acquire operation
but if you look at some of the
implementations that's actually not what
it does so something at least lip C++
currently doesn't do that I haven't
looked at the others so there's some
issues so actually getting the point
here is that getting this right is
really quite tricky and you should first
of all use somebody else's
implementation so you can blame them for
any bugs and if that fails please look
at what else is the advice that you find
on the web from the best implementations
and copy those rather than trying to
invent your own because this stuff is
really quite tricky actually a note on
reference counting if you actually
follow that advice and look on the web
web what people recommend they actually
recommend something else for reference
counter comment so I simplified somewhat
in fact you can get away get away with
using a using an explicit fence which
potentially enforces the acquire
ordering only on the last one this this
is rather tricky it's performance
benefit I think is increasingly dubious
on x86 and arm b8 I don't think it makes
a difference on some of the other
architectures this is probably still a
slight win so here's an another sort of
really complicated example I actually
published a paper in a research
conference about this so it gives you
some idea so the idea the issue here is
that we have data and this is this
occurs fairly commonly is useful fairly
commonly a semi commonly in the Linux
kernel for example that we have data
that's very rarely updated but very
frequently read if we actually protect
it by a mutex the problem with that is
or even if we protect it by some sort of
reader/writer lock shared mutex or the
like the problem is that just updating
the mutex either introduces contention
so every time
you read you're actually writing to the
mutex or the shared mutex which means
that you will go much more slowly
because you're fighting over the cache
line that's holding their shared mutex
so people have figured out very clever
techniques to avoid actually modifying
memory and readers and this is sort of
one of the earliest schemes
I'll see you I think it's somewhat later
scheme for doing us so the basic model
here is in the writer for we for the
purposes of these slides I'll assume
that right we I don't care about the
performance of right so I'll be sloppy
so I will have a an atomic version
counter the writer just requires a lock
to make sure that there's only one
writer before performing any operations
any modifications to the data at
increments a version counter and at the
end it increments the version counter
again so I know that when the version
counter is odd something fishy is going
on so if a reader ever sees an odd
version counter it knows that there's a
writer working on this and it should
leave it it should ignore it if the
version counter is even then I know that
things are things are okay if I read the
version counter at the beginning then
read the data and read the version
counter again and it's unchanged and
were and even both times then I know
that I got a consistent snapshot so
that's the basic idea here what the
reader does is it reads the version
number reads the data reads the version
number does no writes and then checks
that the version numbers were the same
if they want to tries it again because
right I got into the middle so that's
that's the basic algorithm so the
question is now how do we optimize this
with respect to memory ordering and
again I'm looking at only the reader
here so this is actually tricky in many
ways but the first observation is that
technically by C++ rules the way you
would like to write this actually
doesn't work because when I'm reading
the data it may be written by the writer
at the same time so therefore the read
even though it's protected by these
version tests in fact mayonnaise so that
invokes undefined behavior so the
consequence of that is that I in fact
can't do ordinary reads on the data the
data itself also has to has to consist
of some number of atomic atomic objects
so what my code is really going to look
like is the the data reads in the middle
will all be relaxed reads of Atomics and
I need to make sure that the version
reads are not be ordered with respect to
those it turns out I can easily ensure
the only order absence of the ordering
at the beginning by doing an acquire
load ensuring the absence of the
ordering at the begin at the end is
actually quite tricky and this is sort
of the most convincing use case maybe
the only use case I found really
convincing for actually using the C++ 11
memory fences so what I do there is I
actually introduce an atomic fed fence
of memory order acquire which it turns
out has roughly the effect of turning
the the olia relax loads in to acquire
loads but not preventing ordering among
the ordering among them so the data
loads can still be reordered among
themselves but they must happen before
they must not be reordered with the
later version load and I actually made
one mistake on the slide here the second
version load should also be memory order
relaxed because I'm enforcing its
ordering making it sequentially
consistent actually doesn't help so
conclusions
well weekly audit Atomics are hard
hopefully the last few slides convinced
you if not before the good news is that
there actually seemed to be a relatively
at least based on my recent experience a
relatively small number of use of
recipes that cover a reasonable fraction
of the the use cases for anything else
more complicated you really do need to
understand the memory model
their various explanations out there
there's probably room for more careful
more careful ones I think this is also
an incomplete list I think there are
also some there are probably some other
important ones that I've missed here so
there's an all WG 14 paper that sort of
gives a fairly high-level overview for
those of you who really you want to
understand the C++ memory model and have
the patience to do so in the
mathematical background I actually
recommend mark Paddy's thesis at chapter
3 and Mark Paddy's thesis which is a
very precise mathematical description
you can also try to read the standard
itself on the other hand and I think the
memory model I personally think the
memory model is actually sort of an
illustration of the abuse of standardise
this is not something that this is
something really that's much better
expressed in mathematics than it is and
under the constraints of the of an ISO
standard okay thank you any questions
hey um could you explain a little more
the need for the ordering requirements
when you were doing a reference counter
it seems to mean naively that if I'm
using a reference counter for a shared
object I should not have any
modifications to that shared object that
need to become visible when I release
the shared pointer or these shared um
reference count the problem is if I have
some if I had some independent uses of
an object in in two different pads and
then at the end it actually goes both
ways the easiest way to think of it
Publius in terms of a modification but
safe ed one still modifies the object
and then if it just did a relaxed
decrement of the reference count and
then thread two now decides it's done
with the object it does another
decrement of the reference count and
then D allocates it
the problem is at that point threads
what that one's modification may not be
visible yet and may still happen may
still appear to occur to threat on pair
2 after 32 as the allocated the object
so the two may put it back on the free
list and then at that point and reuse it
and at that point the modification from
thread one can become visible this also
I'm the the probably the others the
converse scenarios actually more likely
with third one is has read the object
and the load actually doesn't it doesn't
become in some sense doesn't occur until
after third to SD allocated it I have a
question about slide number six
she could just number six it says that
it's basically s stores become visible
to other threats after try memory
operations this is memory water
sequential basically three on x86 I was
always in the impression that it's a
full fans its exchange operations so I
elaborate on that okay so I it's a good
question
x86 is a little too key if you look at
the assembly code that's generated for
sequentially consistent Atomics on x86
you will invariably get confused the
problem is the the store actually
implicitly guarantees the s property
which is sort of the most important one
so you don't actually need a fence for
that because x86 naturally is fairly
strongly ordered it does not guarantee
SL the hardware by itself does not
guarantee SL so you actually need to use
a fence just to just in case there's an
atomic load following the atomic store
so the expensive fence is actually
issued for a very unlikely case the
common case is free thank you would you
say that using an atomic instead of
mutex when you need say a million of
mutex this is a reasonable thing to do
in atomic instead of a mutex say you
have a you need a million of mutexes
that would be using an atomic be a
reasonable thing yeah I mean it depends
on the case if it's really seguin it's a
really simple case in you're okay with
sequentially consistent Atomics or
something they're just an optimization
of sort of the single operation critical
section that's an easy case very often
in other cases where you want lots of
mutexes it's also reasonable to shout
the the mutexes so that you map a whole
bunch of objects to the same mutex which
is in fact what the atomic usually does
underneath in the implementation in the
case where the hardware doesn't support
the object size basically object
addresses will get mapped to it a
reasonable size collection of mutexes
and those will be used to protect those
objects thanks I have a question for
slide number 33 okay
right so you said it should be a acquire
fence and then relaxed load why not just
acquire lotor divvy - a good question
the problem is that actually it enforces
exactly the wrong ordering you can think
of an acquire load as ordering with
respect to subsequent memory operations
with you this is one of the few oddball
cases where you really want to load
ordered with respect to prior operations
and that's something that we don't
actually the Atomics themselves don't
really have good support for I've seen
in some source code bases where there's
some notion of release load which
actually if you look at the memory model
doesn't make much sense but that's sort
of used in precisely these cases these
oddball cases where you want in ordering
of the load with respect to 2 pi
operations in this case really prior
loads thank you
I have a question about the slide about
dependencies not enforcing the order in
between respect to threats I don't
remember the number sorry yeah this one
is it really possible to see reordering
in Hardware on modern architectures as
opposed to the laws being reordered by
the compiler
except for alpha maybe actually oh you
mean this sort of reordering it's not
the the hardware by itself will not
break things in this case again the the
reasoning for that is that all modern
architectures in fact at the assembly
level have some notion of dependency
that they enforce the problem is this
notion is doesn't really make sense at
the source level this is sort of one of
the obstacles we've been running into
with memory order consume as you can see
in this example so here it looks like at
the source level there's a dependence
but the compiler can compile it to
something that
read of the dependence thereby speeding
up the code but getting rid of the the
ordering guarantee dependency-based
ordering guarantees in the assembly code
so this really requires a conspiracy
between the the hardware and the
compiler thank you any other questions
ok thanks for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>