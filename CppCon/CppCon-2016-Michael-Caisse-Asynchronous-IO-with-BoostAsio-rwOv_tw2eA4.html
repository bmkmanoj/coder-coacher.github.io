<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Michael Caisse “Asynchronous IO with Boost.Asio&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Michael Caisse “Asynchronous IO with Boost.Asio&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Michael Caisse “Asynchronous IO with Boost.Asio&quot;</b></h2><h5 class="post__date">2016-10-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rwOv_tw2eA4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everybody so yesterday I
had the the nap session that's the one
that occurs right after lunch and I
think I'm in the middle of the hangover
sessions is the one where you couldn't
quite make it in the morning to get here
we seem unbalanced we're all shifted to
the side of the plane if you've ever
flown into Aspen there would actually
get you to stand up and partially move
on over to balance the plane out so we
could take care of that this morning you
know I'm not very far away but I was
tempted to take a cab they had a radio
dispatched one in the front and they had
a computer dispatched one but they
didn't have an azo dispatched cab and so
I just like seems like it might be bad
luck actually to do that so this morning
we're going to talk about boost azo
Chris the author of the of the library
calls it a Zeo I unfortunately have come
into the habit of calling it
Accio or something else and I'll try to
do my best year to not butcher the name
for him so the goal this morning is to
talk about the library from the
standpoint of how you might want to use
it and the documentation for the
libraries is really really good I'm not
going to spend time talking about a ton
of the functions I'm going to spend time
talking about the things that I find
users have problems with when using this
this program are excuse me this library
and I for some reason I really love IRC
in the boost IRC channel and so for the
last eight years or so I've been helping
people use this and and we'll see kind
of how we're going to have some problems
how we're going to result some of the
problems so it started off as an
asynchronous networking library but it
started supporting all kinds of
different things
serial ports timers file descriptors you
can write your own interface of sorts it
uses a pro actor model that's usually
where people get tripped up a little bit
it's extremely scalable far as
connections that can be supported
provides this portable abstract portable
networking layer for this abstraction so
that you can write on cross platform and
just just deal with the library as it is
so what is asynchronous i/o so some time
ago when my children your younger first
of all to be a child in our household by
age five you have to know how to use the
espresso machine it's just a requirement
because your parents want espresso and
that's your job in the household that's
why you're going to get fed and you know
have a place to sleep
so we daughter number one my eldest
daughter I would ask it please make me a
coffee and she'd say sure dad she
actually loved making coffee and so she
would run on over to the espresso
machine I would continue to work at my
office and she would come on back with a
cappuccino hand it to me and say here
dad here's your coffee and I'd say thank
you that was an asynchronous transaction
I would continue to do my work I
requested something to occur off it went
and then it came back daughter number
three was very enthusiastic about making
coffee in fact she would ask first do
you want a coffee and of course yes
please I would love a coffee and we
would both get up and walk to the
Machine and I would supervise and she
would push buttons and do the things you
have to do to make it go and I would
then walk with her back to my office sit
down at my desk and then she would hand
me my coffee and I think her and I drink
my coffee that was not an asynchronous
transaction it's how unfortunately a lot
of people though do IO still today and
we want to get away from that blocking
so you know you might have an interface
of some sort that reads a reads a file
it takes a filename and you pass it a
buffer and a handler redone that might
be an asynchronous type transaction so
why do we want to do this well if you
have a client connected to a server you
might write this in such a way where you
have a rethread and a right thread and
that that might be okay but it's not
going to scale very well it's amazing
the number of client offices that I show
up at and they have code that you know
looks like this basically they've got a
reader and a writer thread sometimes
they have a manager thread to deal with
what happens when the readers and
writers go away occasionally they have
like this third thread that having quite
figured out what it does but it's
associated with every client that's
connected and you just have swarms and
swarms of threads not very efficient
doesn't scale well at all instead we
should probably be able to do something
like this we've got lots of clients
still connect to the server but perhaps
a single thread for both the read and
the write because of the load that we
have or or the type of transactions that
were performing so we would like to get
to an asynchronous model but we'd like
to do it easily all right this is where
the house lights dim and we tell a story
it's a pro actress story or purple
slushies butlers and brain freeze mom
dad Johnny and Butler go to the beach
dad tells Butler to wait at the slushy
shack after some time dad and Johnny go
get a slushie dad brings his own cup he
is greeted by the owner I would like to
order a slushy here is my cup please
deliver it to Johnny when it's ready dad
heads off to explore the beach
Johnny builds a sandcastle owner begins
to make the slushy and Butler waits
owner starts the blender and goes back
to take the next customers order ding
slushie is ready and owner moves the cup
to the completion table where assistant
is waiting
the assistant gives the
blushy to butler or delivery to Johnny
Butler is happy to have something to do
Butler delivers the slushy to Johnny who
is happy to Butler returns to the slushy
shack and waits sometimes dad will order
multiple slushies one for Mom one for
Johnny that isn't a problem assistant
just gives the first one ready - Butler
Butler can only deliver one at a time
and returns for the second slushy other
families come to the beach and bring
their Butler's who also wait at the
slushy completion line this works well
because it helps keep assistant slushie
completion table empty assistants still
remembers that fateful day when no
Butler's came to the beach it was also
the time that each kid brought a butler
disaster no room at the shack too busy
yet nothing was getting done the
families agreed that two Butler's would
be plenty for all now they share
occasionally tragedy strikes johnny will
leave to chase waves without getting his
slushy Butler will die of exhaustion
trying to find him
or somebody will take their cup and go
home while the slushy is being made then
it gets poured on the floor yuck dad is
sometimes very generous Johnny would
like one orange and one purple slushy if
both slushies are done at the same time
and both Butler's are available then
Johnny gets two slushies at once this
confuses Johnny and
ozs brain freeze suzy is smarter and
doesn't mind both slushies at one time
but most often the dads are mickey
requests to the owner the assistant is
monitoring the table the kids are
building sandcastles and the Butler's
are waiting all right so what's the
purpose of this story well first of all
pro actor diagrams are incredibly boring
to look at the second part of it is I
create mental models or worldviews of
how things work and and I need those and
I suspect most of you also have one if
you don't understand the intricacies
behind the code you have some mental
model that you've created and you push
and you shove at it a little bit and it
breaks and then you adjust your
worldview and you do that over and over
again this little story came about long
ago in order to help me actually
understand how this stuff works so we
have some credits the initiator if
you're if you're familiar with a pro
actor model the initiator was being
played by dad the asynchronous operation
processor was the owner the pro actor
was the butler the asynchronous event
demultiplexer was the assistant
asynchronous operations
it's the blender making slushie the
completion event queue is the completion
table the completion handler is Johnny
we had some additional roles the
operating system played the blender was
played by the blender the memory to be
filled is the empty cup and data in
memory is a full cup if you are more
into diagrams like this there you go
that's what it really looks like so this
is that pro actor model now there are
some lessons that we can take away and
we can learn from this and if we take
those lessons from that little story and
we just kind of apply them to our usage
of the pro actor we pretty much will be
safe most of the time first of all all
threads of activity inside of the
slushie shack the actions that we're
going on slushie shack they stayed in
the slushy shack they never actually
exited
the butler delivered the results to the
completion handler the butler or the
handler thread was supplied by the
family or the application supplies the
thread that's going to be used for the
delivery the cup or memory it was also
supplied by the application and it's
owned by the application so there's a
major difference between a reactor and a
pro actor not all handlers Johnny liked
having multiple results delivered at the
same time you have code that doesn't
like to be reentrant it doesn't like to
be called that handler doesn't want to
be called by multiple threads some
handlers Susie didn't care it was okay
you might have code in which the
handlers can't be called by multiple
threads don't leave the beach or the
scope when a slushee is being made for
you it's very rude and the handler won't
be able to find you later
but I'm sorry the completion handler
will not be there and the thread will
not be able to find it so a few handlers
or threads butlers can service many
completion routines
you don't need sloughs of these things
so those are some of the take ways that
we can get um what does it look like
when we want to use one of these so we
start off by creating one of these io
service objects we're going to use
timers for a little bit just to kind of
wrap our hands around some of the basics
of how how the system is going to work
so we're going to create a deadline
timer the deadline timer is past the the
IO service as its first argument and
then it's provided with the in this case
the relative time that once it's
activated how long it will take until
the the timer expires we are going to
call timer async async' wait and at that
point that that's when the timer is
going to begin the timer is going to be
assuming the async wait is going to be
past a completion Handler this
completion handler is going to do
nothing except print out what time it is
and that the timer expired we're going
to print out calling run
and then service dot run so calling run
on the IO service that is the thread
that's going to service the completion
handlers that's equivalent to the butler
in the story so each time I call run on
each of those threads that is another
thread that can handle completions for
me so I do this then I get calling io
service run five seconds later the timer
expired and then we exit so make sense
right
how that might work now what's going on
we call async wait
well that request has come in it's
sitting there which is just a timer seen
there waiting to be done once it's done
it is placed on the completion cube the
next available thread will pick that up
and deliver it to the handler that once
at the completion handler let's do this
with a thread so we can go ahead and
create two different timers both are
going to be 5 seconds we're going to
start an async weight on each of them
and we will have one thread that we're
going to start off that is going to call
run on the IO service a timer expired
does nothing except for print out that
we've entered timer expired it waits for
3 seconds and then it prints that it's
exiting so we start these things off
they should each go up basically about
the same time right so there's no school
for 5 seconds so each of them will be
one team to each of them will produce
the completion they'll be stuck on the
completion queue there's only one thread
so the thread will have to handle one at
a time whichever one ends up in the
queue first will be handled and then the
next one so we're going to get this type
of an output timer one I ended up
hitting it first
and so we see enter and leave for timer
one notice that's in there for three
seconds and then right away timer two
because the thread has now finished
running for the handler it's gone back
there's something else in the completion
queue that
to pick up and then take care of that
that handler this is that idea of the
butler can only deliver one slushy at a
time yes
so the question Arthur is asking leading
questions here yeah it looks like
service dot run is expecting that there
is work to do is the basic comment and
yes it's looking that there is work to
do how does it know when it's all done
when there's when there's nothing left
inside of you can think of it as a
slushy shack when there's nothing left
inside the pro actor to do there's no
more work inside of the to be done work
there's nothing inside of the completion
queue then the run will return we won't
talk about it we don't have a lot of
time but what you want to do is you want
to create what's a work object you'll
instantiate this work object it doesn't
actually perform any work but you will
put it inside of inside of the IO
service and as long as that objects
alive the IO service will stay alive and
so that's the kind of the trick you
typically will use something like a
shared pointer pass it in and then later
on you might delete it that would be the
way to take care of that so alright how
about if we do this now with two threads
so the same set up we've got two timers
each of the timer's are going to go up
in five seconds we call async wait which
starts that off it delivers it delivers
it to be the work to be done we now have
the two threads that are running service
dot run we should now be able to pick up
both items off of the completion cube
and and run their handlers and indeed we
do and we get a garbled mess as we would
expect and um all right so we've got a
garbled mess because both handlers are
being executed simultaneously by two
different threads so that makes sense
all right so we can also just post work
we can just say this equivalent of of
the owner just placing items directly
there's no there's nothing to be done
there's no IO to fetch there's no timer
there's nothing associated with it we're
just going to take a handler and we're
going to stick it on the completion
queue for the next thread to pick up all
right so we are going to do this by
using post so in the i/o service we call
post dot
I'm sorry service dot post and then we
give it the work that we want it to do
eat drink and be merry we're going to
provide a thread to execute the run and
we get our eat drink and be merry so the
this in essence is a thread queue for us
right in fact a very common pattern when
you're writing code using asynchronous
i/o library is you will have a layer
that is your communication layer and it
has an i/o service and then you have
another IO service that will be taking
care of all the heavy lifting processing
and the two layers do nothing more than
pass off work between one another so as
communication comes in you can you can
delegate the number of threads you might
need for your communications and then
you pass that off to another i/o service
and and those items get posted and you
now have the work queue in which you can
provide as many threads as you need to
actually perform the work that you want
to get done all right so we have this
Johnny who could not handle two slushies
this is our same problem we've got both
5 seconds that five second timers that
are going off at the exact same time and
it created this garble mess right what
do we do when when we can't handle this
so there are a variety of different
things that we may want to do you know
we may want to like get out our handy
mutex we might want to do a lot of
different synchronization things that we
might know about don't do any of those
use the use the constructs that are
inside of the library and what's inside
the library is called a strand and so
the code now has changed slightly we've
created out of the iOS
service has this thing called a strand
we've taken and created a strand object
we pass it the service these are the key
bits here that we've added so we've got
a strand and now instead of just calling
async wait and giving it the completion
Handler what I want you to run when that
happens I actually take the Strand and I
wrap my completion Handler the
completion Handler excuse me the Strand
the strand will ensure that that there
is not that only one completion Handler
that is wrapped by the same strand will
run at the same time now we'll take care
of any of the threading issues that I
might have so I might have lots of
threads and I might have lots of things
on the completion queue items on the
completion queue that need to be called
but those handlers will not get called
if they are wrapped by the same strand
so let's take a look at that well first
we can take a look at this we have now
what looks like the serialized example
we had previously even though we have
two threads running timer one enters
first we have to wait three seconds
before we can actually see that timer
number two will then get called up let's
do something a little bit different here
let's add a third timer for six seconds
and when we start it's a sync wait we
will not wrap it inside the Strand so
timer number one a timer number two or
five seconds they're wrapped in the same
strand they're going to go off at the
same time timer number three is six
seconds it's not wrapped inside the
Strand and so what would we expect to
see well we expect to see that one and
two are serialized in whichever order
order is not is non-deterministic so
with whichever order it occurs first
inside the completion queue and they get
picked up and we would expect to see
because there's a another thread hanging
out that timer number three will go
ahead and be invoked and we do see that
so timer one goes off and then a second
later we can see that timer three we
need to enter timer one finally leaves
and timer two can begin and then our
timer three is finally leaving and then
timer two so if you have things that
need to be serialized for example in
oh we can't be going around writing to a
TCP socket from multiple threads at the
same time right that's a disaster so
right we would want to write wrap inside
of a strand we would want to make sure
that the access to that writing
processes is protected let's talk just
briefly about one more thing here that
will kind of get us going and the the
azo library in essence uses this concept
of views into data remember the the
concept of the cup in the story the
application owns that memory and so when
you pass something off all you're really
passing is a view internally don't don't
let the memory fall off the stack don't
delete the memory while it's in the
middle of being utilized by ezo because
you will you'll crash the memory will
just go away so the idea is buffers and
buffers mentally we can think about
buffers as as being modeled by a tuple
of a void star and some size and bytes
so mentally we think about it like that
of what's what we're passing off and
what's being stored there are mutable
buffers and there are constants and of
course we can convert mutable buffers
into conifers so they appear to be very
c++ see the way we like them so we have
a mutable buffer and a Const buffer the
the other thing that is supported is
scattered gather so if we have several
buffers that are stored inside of a
container we will then we will get a
behavior in which all of those appear to
like a DMA operation all of us will
appear to go on out if we have a
container like a vector of buffers those
can then utilize for reads again buffers
to not own their underlying data here's
an example in code so if we were to send
from a socket and this is just blocking
sins we could take in the FIR
example and use this function called
buffer which will take the argument and
then return a buffer type that can be
used so we're passing a data in size and
we're going to get one of these buffer
types or there are overloads for all
kinds of things so like a string I just
say buffer and then wrap personal
message inside of I'm Sue's me call
buffer with personal message the string
and we get a buffer back or a standard
array at the bottom so a zero has a lot
of overloads for the buffer cull and you
will then get an appropriate buffer back
that could just be passed off it does
nothing more than figures out where's
the pointer to the beginning and how
large is this thing that how many
elements and then that's what actually
ends up inside and that you're passing
around here's an example of the scatter
gather so pretend we have some protocol
in which we have some header that's
going to be going out the message and
then we have this additional data piece
I can create now a vector of these and
when I call send I can just pass the
vector of these buffers and those will
then be sent out efficiently alright
let's make sense
let's build a server and the way we're
going to go about this is we're going to
build it and I'll just kind of worry
about some of the problems that we often
have when we're building these things
the let's envision a chat server of some
sort there's some centralized server and
there are clients that are connecting to
it and when the client sends a message
then that message is broadcast to all
the different clients the the client on
the top it has initiated the connection
so a TCP connection has come in it
connects right on a port and then that
gets handed off to another port from
that point on the communication is on
some other port that's occurring right
now here's the question of the day who
owns this client handler if I have an
object that represents the connection
between the client and the server up
here it's called client handler who owns
that thing so there are lots of the lots
of ways to do this unfortunately seems
like a very common way is to say well
you know it's it's owned by some manager
and you know the manager keeps a list of
all the client handlers and occasionally
it goes through and it checks to see
whether they're still connected or not
if they've lost a connection and they
clean them up you know and so you've got
something over here doing some work to
make sure that all these client handlers
still need to be around if not it takes
care of of deleting them my my first job
out of college I had a manager
we were sitting inside of a code review
the manager had joined the code review
and probably unwittingly
he says about a piece of code he says
what does that manager doing managers
are not supposed to do work and you know
that was that was great we have tried to
hold it in for a little bit reviews were
coming up soon but you know if you are
designing something and you have
resource managers that are doing nothing
more than going around checking to see
whether or not the resource should be
alive anymore or not you probably don't
have your design right there's probably
something wrong in the design you
haven't thought about really ownership
and what the ownership model is long
enough
what's the ownership of the client
handler well when the client handler has
no more work to do
then the client handler should go away
well what's no more work in this case
it's it has nothing left to send or it
has the inability to read anymore the
client on the other side has gone away
so for us this connection has gone away
the client handler has no more
connection or
there's no more work internally so it
might be that the client handler
connection has gone away but internally
it has like a bunch of buffers and
things that it's in the middle of its
called completion handlers it's got to
clean all that but right those are those
are still things that have to happen
what knows best on whether it should
still be alive or not the client handler
right itself and so we're going to need
to come up with a mechanism in order to
keep the client handler alive that is
the client handler itself doing the work
all right and one way of doing this
chaining was probably a really bad word
up there we won't we're not really
changing completion handlers here this
is not what we're talking about what
we're talking about is when the
completion handler runs for example here
we have a redone so I've begun to read
um into the 8th I've called an async
read so I provided it the buffer that I
want it to read the data into and I've
given it the completion Handler when
you're done call this handler when that
runs I need to look to see whether
there's an error or not if there was not
an error then I will do some work and I
will queue another read I'll start the
read inside of the asynchronous and
another asynchronous read with a
completion handle again if there was an
error I won't start a read again right
I'll just kind of like fall out and if I
keep using this pattern by not queuing
up more work and another completion
Handler if I can now figure out how to
bind in my lifetime everything should be
fine so here's the goal we want to write
these two lines of code and we want to
end up with some server that's listening
on eight eight eight eight and we're
going to try to do it in some generic
way so we have this azo generic server
it is going to be instantiated with chat
handler and chat handler will basically
be the type of the different handlers or
the assuming the type of the handler
that will handle the connection
with the client all right so it's going
to instantiate a new one of those each
time that a client connects that's the
goal so what does the generic
server-side look like how do we get a
server up well we're going to start off
with saying that we let's just create a
type alias it's the shared handler
underscore T this is type alias and it's
nothing more than a shared pointer to
the type of the connection handler where
the connection handler is the type that
is going to be instantiate for each of
these connections what are we going to
have internally well we're going to
actually we're going to have a thread
count we may want some number of threads
to to deal with this connection so
that'll be our thread pool we're going
to of course have an i/o service the
server itself and we're going to need an
acceptor this is how we're going to get
the connections in for our TCP
connections what does the the
constructor look like the constructor is
going to take the number of threads that
we want the server to operate with and
so we've got thread count we're going to
initialize the acceptor with the i/o
service everything in a zo that has has
to do with services needs to know the
i/o service that they're working with
and so you can have multiple i/o
services in your program and they then
need to be associated with the service
itself excuse me the service itself
needs to be associated with this with
that we're going to have two main
methods the first method we already saw
was the start server where we passed the
port and the other is we're going to
have a handle new connection method and
as you can imagine this is as a new
connection comes in this is the handler
that's going to get called so let's take
a look what's inside of our start server
so start server the first thing we're
going to do inside of start server is
we're going to make shared I'm on the
type that is going to handle the
connection so we're going to get
a shared pointer object to the type
that's handling the connection we're
passing it the i/o service and now we
have that handler we're going to set up
the listen so we have an endpoint we're
going to use TCP b4 pass at the port
number we're going to open the acceptor
passing it the that endpoint protocol we
can set options such as reuse address
we're going to bind to the endpoint and
then we're going to begin to listen and
now here is the asynchronous part except
you're a sync except we provide it the
socket that we're going to that we're
going to transfer so as a as a client
connect it accepts and then it gets
transferred to the to the other socket
right the one that we're going to
actually do the accept on that's that as
you can see it's inside of the handler
handler contains its socket that's going
to going to continue to communicate on
and then the completion handler for the
async accept is this handle new
connection and then we're going to start
up our thread pool by however many
calling run on the i/o service however
many times four threads ww-want oh so
the question is is it does do we need to
use sed ref for the iOS service so you I
haven't told you what the what the
requirements are yet of our client so
that would be true if we receive the i/o
service by value then we need a
reference wrapper but in this case we're
actually gonna receive it by reference
so it'll be okay alright so what does a
handle new connection look like so
handle new connection it's going to take
the
handler so note notice here when we got
this thing going our handle new
connection passed in the handler and the
error code that's what it's getting
called with the handler and the error
code handler is what well handler is the
shared pointer of the thing that's going
to handle our connection and in doing
this by capturing by value we have taken
this this shared pointer object and we
bound it up and stuck it inside of the
IO service right so the lifetime of my
handler is even though we're going to
leave scope I'm not I'm not going to die
it's still alive so when I call this I
have the handler in the error code first
thing I want to do is I want to check to
see whether or not I had an error if
there's an error I just return I bail
now in returning that completion handler
is what had the handler bound to it
initially and so you can see I'm going
to start tearing down everything that no
longer has a reference count inside of
the i/o service will start falling apart
and I'm going to call handlers start I'm
going to create a new handler just like
before with make shared and then start
my my async except again and what is its
completion handler itself so now we once
we got things going then handle new
connection basically looks like get this
get the error code if there was an error
just bail if there's not continue on
what does continue on look like the
handler that's received the connection
start it create a new handler for the
next connection that's going to come in
call the async accept past that bye soon
as we capture that by value into the
completion Handler and then continue on
now this is this is going to take care
of all the accepts for me so what is the
actual chat handler look like this is
the object that's communicating now with
with the end bit
you get tension thank you all right so
the hand the chat handler this is the
thief the object that's handling the
communication / connection that comes in
the first thing you've noticed is that
it inherits from enabled shared from
this passing its own type so this is
this is CRT P you know you tell it right
away because you're passing the same
type as what you are inside of your
inherited type CRT P is curiously
recurring template pattern because we're
not really all that good at naming and
so we've got in essence a pattern that
allows us to do static polymorphism or
inject behaviors and what this one's
doing is its allowing us to inject
behavior enabled shared from this lets
the class be able to get what appears to
be a shared pointer to itself at any
particular time
why would something want a shared
pointer to itself to control its
lifetime that's why you would want one
so if it calls shared from this it's
going to get a shared pointer to what
appears to be its own object and it
could then bind that into something else
maybe stick it on a queue maybe pass it
around it could be completely hidden but
as long as that objects around there's a
reference count to that particular
instance of the object right so very
common pattern to bind up a shared from
this into something maybe it's a capture
in a lambda that's never actually used
but it's part of the capture that will
extend the lifetime of the object so
what else do we have inside here we've
got to have the service so we're going
to actually have the service by
reference we have a socket
the socket that the client wants to
communicate on we're going to have a
right strand because we can't do
multiple writes we don't know how many
threads we might be dealing with
and we can't from multiple threads do
write simultaneously that would just be
a disaster right we would get
interleaved data and something would be
upset with us we're going to use
something a little bit different so far
we've just seen these buffers a zero
also supports concepts of stream buffers
and so we're going to use a stream
buffer for this example and then for
sending data out so data coming in we're
going to use the stream buffer data
going out we're just going to take and
use a deck of strings to represent those
packets at the moment so you can imagine
the user of this thing wants to be
sending messages in our in our example
that we have the server is receiving
different packets or messages from these
different clients it's a chat program so
as it receives one it needs to package
that up and then send it out to every
client that's currently connected and so
it's just queuing these up so there
might be a bunch of them that are queued
up the deck is going to handle that for
us construction is very simple we're
going to receive the i/o service by
reference the socket needs to be
initialized with the service as well as
the strand socket did nothing more than
returned the socket by reference that
was the socket for the chat handler in
the server side it was using this in
order to figure out which socket the
client wanted to use this is how it got
that if you recall and then start what
it start do it was the server that
called start to get things going once
the handler was connected it just calls
read packet so the magic must be
somewhere else and it begins with read
package now this is a slightly different
read so we have this
read until some condition and we're
going to read until passing it the in
this case the socket the in packet and
in packet was a stream so it's basically
has a stream buffer that's going to read
into and then we're going to give it a
very simple condition a null Terminator
read until you see a null Terminator and
then when you see a null Terminator this
is what I want you to do so let's just
get rid of some of that cruft for a
moment or that the set up there
the callback handler itself is creating
a it's capturing shared from this and
internally inside the lambda that's
going to be called me the expression is
going to take the expression is being
set up to take two arguments the error
code and the bytes transferred that's
what a zo is going to call this with so
this completion handler signature looks
like it's going to take actually an
error code and the bytes transferred
that that's how it's been set up and
then internally I'm going to use that
shared pointer me to call read packet
done
passing it the error code and the bytes
transferred so what I've done is I've
bundled this thing up with a shared
pointer to myself by calling shared from
this I now have taken that lambda
expression becomes a closure object that
closure object now is getting put inside
of the i/o service as the completion
Handler and as long as that objects
inside this this object here this client
object is not going to to get deleted so
I have a reference count that is going
to stay alive as long as there's work to
be done inside of the i/o service this
is how I'm going to manage my lifetime
I'm going to manage my lifetime by
managing it myself by binding up this
shared from this or a shared pointer to
myself inside of the completion handlers
so what does read pack
it done look like well as you can
imagine if there's an error I just kind
of want to bail at the moment I'm not
going to think about what I'm going to
do there is there's going to bail the
idea here that I want you to see is
we're not going to queue more work
eventually all the references are going
to go away and the object will go the
reference camera the object will go to
zero and then the object will be cleaned
up automatically otherwise for the read
I'm going to take that stream sorry the
in packet I'm going to go ahead and
convert that into a string and then i
could do something smart with it at this
point typically I'd have a callback of
some sort in which that gets passed off
to an i/o service that's doing the work
and then I'm going to start repack it
again right I'm just going to call
repack it again initiate another read
start another read which will now get
the asynchronous read going again so as
long as reads are or working for me a
new read will constantly be getting
queued up queueing up a new read takes a
shared from this binds that up in the
completion Handler through the capture
and keeps the lifetime movie right so
the lifetime is always going to be fine
as long as there's a read all right so
now how about sins on the sins side yes
Arthur yeah so in this case it is just a
match so it's going to for the async
read until it's going to read until it
matches this in the stream and these can
be complicated and tricky you can write
your own expressions of what this might
mean you could use a red Jex
if you wanted to so I don't I don't
recommend doing this in production code
so maybe out thank you for asking in
production code this is great on a slide
it works wonderful for getting something
up right away but this is just waiting
for some annoying person to overflow all
the memory you have right so you want to
actually use
in my opinion you want to use a method
that gets whatever available now and
form your own buffers along with some
constraints about those like how many
bytes if I read so far does this pattern
make any sense yet no it doesn't make
any sense at all so that that's how we
use it we basically read what's
available if there's nothing available
you don't it won't return if there is
something available it's usually
whatever the packet size is or it's
whatever the chunk is that it's
currently received from the TCP layer so
I've got that data to work on it one at
a time so all right so let's just talk
about sind
what if sins look like so remember for
the send side I can't be writing to the
socket from multiple threads
simultaneously so I need to somehow take
care of that problem the reads that
wasn't a problem because I was managing
the reads and I wasn't going to start
another read operation until I was done
with the one that I was in the middle of
and then I would begin another read
operation so async read was getting cold
as I needed a new one on this side I
might have multiple threads calling the
sin and there would be lots of different
ways we could protect this but let's use
the mechanisms that are built in to do
that let's instead of creating some
other way using some other intrinsic
let's just use post so service post if
you remember is the same idea as taking
that that completion Handler we pass in
and just sticking it on the completion
queue right away so it's just queuing
work to get done and we're going to wrap
that inside the right strand so now I'm
guaranteed that multiple threads will
not be running anything that is wrapped
with my wraps wrap strand simultaneously
what am i what am i putting inside of
there again I'm capturing the shared
from this and I'm calling queue message
passing off the message that came in so
you know from from
the application side the applications
calling syn passing some data and those
are getting queued what does Q message
look like well we're going to look to
see if the packet Q send packet Q is the
deck that I have is it not empty if it's
not empty I must have a write in
progress now at this point you might go
like oh this is a standard container
those aren't thread safe I can't just go
looking at one of these well I can look
at one of these right because I've
wrapped all the access inside of the
strand so I'm guaranteed that only one
thread at a time is dealing with this
I'm going to push back the message that
came in and if there was not a right
currently in progress I'm going to start
packet send so what does packets in do
Pakistan is going to terminate whatever
was at the front so the front was a
string of some sort we're going to add
some null termination there and we're
going to call right giving it the socket
and then this is the buffer call that's
going to create an easy a buffer for me
giving it front I'm not removing the
item out of the deck I'm leaving the
item in the deck at this moment and
we're just going to pass it the memory
pointers in essence into azo as it's
sitting inside the deck I'm going to use
for the completion handler of course the
right strand because I want to keep
everything nice and protected for rights
again I'm going to pass in the shared
from this and so the shared from this is
going to keep me alive as long as
there's something to write and that will
require the error code in the size and
then I'm going to call packet send done
so the completion for sending the packet
will call packet done packet send done
check to see if there is an error if
there wasn't an error it can now pop the
front off the deck that was the message
that I just got done sending you can pop
that off and
if it's not empty I'll start another
packet send so if there's still if
there's something else in the deck go do
more work otherwise I don't have
anything to send so I'll just I'll hold
off right I've checked for error so if
it's not an error going to do that if
there was an error in this process right
now just baleen so if there was an error
in this process I'm just going to bail
by bailing I'm not taking a shared from
this binding it up into the completion
Handler and sticking it somewhere inside
the i/o service so this is managing my
life all right
um so some takeaways here you want to
use a layer design you want your
communication to be an i/o service
typically and then passing that off into
another i/o service to get the work done
and use your i/o services in essence as
executives so they're just going to be
working on doing work and then the the
i/o service that's doing the work will
then communicate back into the i/o
service that's actually doing the
communication you can add your own
services so we've written services that
talk everything from CANbus to weird
other mechanical bits and piece where
things writing your own service is super
easy and you can just look at this as
this is my centralized place to write
communication bits and pieces but more
often than not what we do in our company
is we actually we have a protocol that
we're already interested in and I've
given other talks on this you can
probably find online where we actually
have something that is called the Aussie
Oh spirit client Handler and if you've
not used spirit before it is both a
parser and a generator domain-specific
language so
you can pass into this the grammar of
what it means to take data so structs
and how you convert those struts into
byte streams and you also pass in the
grammar of how do I take byte streams
and convert those back into strokes so
when you instantiate one of these you
Stan she ate them with two grammars that
describe the by directions and then the
handlers that you're going to be
listening on in essence what's going to
probably receive the variant or maybe
it's already a visitor that's set up for
you that's how you want to that's one
way that you can use this and so you
write these layers of all the
communication that you want to use on
top of a zero you can also then use it
for your processing so as you can
imagine chat servers are easy other
things start becoming really complicated
and you've got a you've got to think
about how am I going to deal with
sloughs of completion handlers or what
not how am I going to think about this
problem so for some people
completion handlers are fine that's
that's how they want to think about it
there are I think yesterday was the
co-routine co-routine marathon right so
you may want to think about things as
co-routines
this works really well with covert teams
so there are there are different ways to
do Co routines they have you know
different benches one way or the other I
believe Gore has examples of why you
would want to use this with co-routines
that he has
there are Co routines that that Chris
has that deal with you can see how we do
with parsers so we've got a stream
parser that's coming in and it's
bouncing back and forth between the
middle of parsing and the middle of
trying to get the stream so co-routines
are another way maybe you are the type
of person who's more into state machines
we pair our completion handlers more
actually up with MSM meta state machine
from boost so we can write a state
machine
and it's a very efficient state machine
we've got a bunch of tables we can look
at and those events just become
transitions between between states it's
easy for us to think about so that's a
common way for us to deal with it and
then of course you can combine this with
spirit or whatnot
we'll have time for bonus slides but we
do have time for a couple questions so
yes yeah great question so the question
is basically testability how do you deal
with testability so there are a variety
of different things that we do for
testing in particular the the
communication streams we will replace
our sockets actually with stubs that
simulate the socket communication back
and forth the converting to void star
that's how you might want to think about
the problem that's inside of Accio
itself right so that's the buffer call
that's returning in essence the pointer
to beginning and how many bytes that it
wants to use internally so testing for
communications is always hard
we will stub out we won't have the
socket we will have another thing that
looks like the socket which works out
just to be streams of data going in and
out that we can proceed and then compare
to make sure whether or not the data is
right or not there is no there's no
facility that's already there given to
you built up I wish there was that would
be nice
yes yes right so the question is is
there in these slides the thread that
was doing the accepting was also doing
the work here is there a way to separate
that so you can create an i/o service
that is doing the accepting most of the
time accepting is doing mapping right
you're waiting for something to connect
once you start that connection off that
same thread can probably handle
communication but you don't want to do
work on that thread and so what you
would do is in these places where I have
you know exercise left to the student do
something with it let's do something
with it is probably take and put that
message somewhere else that is another
thread pool that will take care of it we
use i/o services to do that typically
it's another i/o service that has its
own set of threads just the i/o itself
yeah right yeah so you can you could do
one of two things so if the
communication is too high or the
accepting of connections or is too high
compared to the communication you want
to separate those you know your first
attempt might be I'm just going to add
more threads to the pool but if you want
to manage that separately those can be
separate i/o services you have a nice I
Oh service that's dealing with the
accepts and then your client right now
we passed in the i/o service right for
our clients and they're using the same
IO service which means that they have
the same thread pool that's dealing with
the completion hands we all have one
completion queue if we wanted we could
have actually used a completely
different I Oh service for all of our
client handlers with their own thread
pool so separating those would be as
simple as don't pass in the i/o service
that's the same one that's doing the
accept pass in a different one
and that would be the one that's
handling all the communication thanks
Nate
it's laughing so the general question is
there is there a more generalized
abstraction for file i/o there's file
i/o with this inside of windows is there
and they seem to be OS specific there is
a proposal that was in boost for a
sakuni's file i/o that is built on top
of the azo library or somewhat built on
top of it the it was not accepted in
this first round and Niall Douglas who's
here at the conference is doing more
work on it probably would love to talk
to you about it and what you might want
inside of it while we're while we're
here we might as well have another plug
beast if you were the Lightning talks
last night beast is also built on this
was our I think our beast guy was here
one point waving really on ok so if you
haven't seen beast yet it's basically
HTTP bits and pieces built on top of the
few if you need to do HTTP v-type
connections communication check that out
any of the ripple guys probably will be
able to talk to you about them all right
thank you very much our session is over
it was on I stab you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>