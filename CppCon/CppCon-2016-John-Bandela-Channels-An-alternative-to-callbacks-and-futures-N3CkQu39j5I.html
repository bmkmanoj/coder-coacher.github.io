<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: John Bandela “Channels - An alternative to callbacks and futures&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: John Bandela “Channels - An alternative to callbacks and futures&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: John Bandela “Channels - An alternative to callbacks and futures&quot;</b></h2><h5 class="post__date">2016-10-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N3CkQu39j5I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody my name is John Vendela
and thank you all for coming this
afternoon I'm going to talk about
channels as an alternative to callbacks
and futures I was originally scheduled
to give this talk on Monday but
unfortunately my flight got canceled at
Sunday night flight and after spending
more time I think at the airport than in
the air finally arrived here but looking
back on it you know it could have been
worse I could have been traveling with
Tom Hanks just a little bit of
background about me in the picture the
guy that you can't really see the face
that was me
just a little bit what we were doing
there is we were doing a awake
craniotomy - which is basically you put
the person to sleep open them up and
then once the skull is off and the brain
is exposed you wake them up and then you
like you can test different areas of the
brain so they had some people like with
cards the psychology neuropsychology
people they're asking for naming tasks
and stuff and then we would like zap
certain areas of the brain and if he all
Sun stopped in the middle of one of his
words we know hey you don't want to take
that part of the brain out after a while
of doing this I decided I wanted to do
something with a little bit less scary
undefined behavior so C++ it is nasal
demons are preferable to nasal brain
fluid now this was this for this
particular operation this this person
was having seizures that could not be
controlled by medication and the
seizures were coming from the speech air
of the brain so you had to know which
parts of the brain you could actually
take out and not to stop seizures all
right so today our main focus is going
to be on asynchronous programming is
everybody here familiar with
asynchronous programming raise your
hands
okay good so asynchronous programming
instead of asking you know hey are you
there yet are you there you are there
yet you just leave a note saying call me
and then you know at their work correct
time you get called so C++ knows about
two main models
first is callbacks and the other one is
futures that are in the standard so
callbacks is the oldest model when
something needs to happen it calls a
user-supplied function the synchronous
example is Q sort for the comparison and
an async an example even from C is like
signal
it happens asynchronously more recently
with ASIO
and the upcoming C++ networking TS we
have like a sync receive buffer where a
sync receive where you supply a buffer
and a callback and when you get that
data you receive the callback futures so
futures are this is the excerpt from
Anthony Williams simple HUS concurrency
in action basically it's a it's a model
for a one-off event that you can start
something up asynchronously and have it
let you know when it's done so there's
the problems that we the in the future
we've got two types the main stuff
that's in the standard now is the
blocking type you have a future string P
and you read from a file call F docket
for blocking and it blocks until then
and then you use it early on it was
identified that hey maybe blocking on
future die get is not such a great idea
so now we have the concurrent CTS those
published in January of 2016 that has
future dot then future dot when any and
future all so here's a you know an
example of future dot then where instead
of in the previous example we were
blocking instead you supply a dot then
and it'll it'll it'll let you know when
when the future is complete and in
addition we also have when any so if we
want to futures and we want to say hey
when this happens or that happens let me
know when either one happens we can also
do that via the when any so callback
versus future so callbacks are
conceptually simple
they're efficient but the downside is
they're difficult to compose like think
about how you do win any with just using
callbacks like you'd have to figure that
out every single time you did that
future on the other hand it is more
complicated because of that it can be
less efficient but the advantage is it's
easy to compose for example using one na
unfortunately the concurrency tss are
not widely implemented yet so for a lot
of these places where we use features
the closest I could find this in general
uses the boost futures so that's that's
what I'll be using in my examples so why
do we need anything else besides futures
and callbacks
so futures are basically designed for
one-off events however there are many
situations that don't produce one-off
events but that produce a stream of
related events and we want a way to deal
with the stream of asynchronous events
efficiently while still being able to
compose them and and we can call stream
of asynchronous events for this for this
purposes a channel and just as an
exercise will we'll talk about this
later I mean think about just using
futures how would you design something
like this where you had we took an input
that was serial for example you read
from something and you did like a
fan-out to process stuff in parallel and
then put it all back to output serial
we'll get back with this keep this in
the back of your mind so we're going to
talk a little bit about performance and
like I said these are for ballpark
numbers for doing this on my machine and
with that caveat let's look at some
stuff so first of all let's look at
callbacks so we're going to use the
boost executor loop executors so
basically that's an executor that you
can have control over when it executes
and we're just going to submit submit a
bunch of callbacks to it and run the
cute cute closures okay
and then this is future so it's a little
bit hard to use feature in a loop but
this is you know close enough proxy so
we have a promise we get the future from
the promise and then we set a future
that then that sets a done value equal
to true we set the value for the promise
and then we run the QED closures which
do you think is faster this one or this
one and how much faster is callbacks you
think so
it's about and my machine was like a
factor of like around six all right so
maybe just dealing with futures is you
know doing that kind of asynchronous
stuff is just inherently hard to do and
inefficient well let's take a look
anybody know what this is yeah that's
the Go gopher all right so let's just do
a simple things so we have a writer and
so this is not a go conference so I just
basically kind of learned enough go to
be dangerous for this but basically the
the declarations the function arguments
are kind of like backwards in terms of
that so C is actually a channel of
integers count as an integer and then we
fur up until reach the count we just
write to the channel and then we close
it and then for the reader we read from
the channel and so basically returns the
equivalent like a pair one is the value
which we're ignoring and the other one
is okay which returns false with the
channels closed so we return and then we
signal the outside thing saying hey
we're done and we do a timing of it and
here's what it looks like so it's pretty
close to call back and way better than
future so this is actually kind of
embarrassing because you know this is
what we have and you know we're stuck
there thankfully well we've got boost so
anybody know what came out in boost 1.6
to the beta go ahead
boost fiber yes in fact that was such a
big deal my local grocery store was
talking about it so anyway now with
fiber so let's see what we can do with
it so we're going to do the same thing
so you notice our code looks a lot
cleaner already and so basically one to
account we push to the channel and then
we close and then we read from the
channel and then if we get a status that
it was you know we popped from the
channel if we get channeled establish
with closed we return and this is what
it looks like so it's quite a bit better
than better than future but not as good
as go or the callback so in addition the
other thing that we lack that it doesn't
have is it doesn't have anything
equivalent to select which is like the
equivalent of when any for future when
any fur like channels so we can't say
can you read from this channel or that
channel and do that and it only works
with fibers so we're going to we're
going to in this session we're going to
write something and design something
that will help remedy that situation so
I'm a big believer in TDD and I think
that if you don't do TDD you'll never be
a good developer agree so thought driven
development so just to warm us up any
but can anybody figure out what C++
feature this is what is it no okay let
me arrange these in order
what is it who said that
all right y-yeah Jean Eric lambda okay
all righty all right so our goals is so
let's you know let's write a C++ channel
and this is going to have zero dynamic
memory allocations once channels are set
up are then required for moving we want
to have channel select and we want it to
be organic anyone to be adaptable for
use with other types of co-routines so
just a note on adaptability
so these channels that are developed in
this thing I've got on my github they
support actually the co-routine TS like
c oh wait I've tested it I think in the
implementation that's out in visual c++
2015 and I also wrote a library of stack
Lascaux routines that's a header only
macro free implementation of stack
Lascaux routines and this was originally
written as a demo for that library and
then this kind of grew up out of it so
as mentioned before we're going to be
using the co-routine TS so how is the
future implemented there's several ways
to do it but most of the ones that do
here basically have a future and a
promise and you have a shared state
that's dynamically allocated there's
been several talks about ways to make
more efficient futures I know back in
2013 I think Tony Venter gave a talk on
a non allocating future but the ones
they're out there the ones that are
implemented use some sort of shared
shared memory things so every time you
create a future you're making an
allocation so somebody comes to me and
says hey it hurts when I do this the
answer is don't do that right so this is
phew so we're going so here's here's an
example here so this is from gorse
talked before on co-routines
but this is a very trivial
um you know trivial thing that basically
makes a connection and reads from the
connection if you use future that then
it looks like this and then we have
co-routines
to the rescue which makes it again looks
like this but I think the underlying
problem with future is take a look at
the connection dot read so with future
we're reading that in a loop and we're
creating futures the connection that
read pretends that just about everybody
in the world will call it right however
you're probably calling it a loop and
it's probably going to be one thing
that's calling it you know it's kind of
like the difference between checking
into a hotel every night and actually
renting a place for a year like you can
establish a ongoing long-term
relationship and you can decrease
transaction costs so just a little bit
about how co-routines work so go to you
know gorse talks if you haven't gone if
you miss them watch them online but
basically co-routines simply our place
to store variables across curtain and
vacations and the ability to suspend and
resume so place to store variables so
basically we store the the parameters
and the locals and the temporaries and
the middle the middle stuff is actually
how go looks and the status co-routines
are on the right where it's actually not
part of the program stack so you can
have basically billions of these and
then we want the ability to suspend and
resume so we're going to talk with the
first goal how do you write a channel
with zero dynamic memory allocations
once channels are set up other than
required by moving any ideas alright so
we'll do this so here's the idea so we
have a reader co-routine and a writer
co-routine and what we do is we actually
store a node for an intrusive link list
as a temp inside the co-routine function
and then we have a channel the channel
is allocated like I said you know once
everything's set up that's when you
don't do any more allocations
and then we write it into an increase of
linked list of readers or writers and
then once we match up a reader and a
writer we do a move across the values
and because the co-routines
as long as you know as soon as we write
into that we go to we suspend the
co-routine we know the values are going
to be there until we need them so show
me the code
so this is our we're going to start with
a node base so basically this looks you
know like pretty much you know
bog-standard linked list node we have an
X in a previous and then where we're
going to store like we're going to store
a function pointer and then like a void
pointer to some data that can we that we
can use to that the function pointer can
use and then we'll pass this node base
to the function pointer and then we just
store a flag for closed all right and
then we have a node T that just derives
from that and just includes like the
value that you want so this is our this
is what our channel is going to look
like so we're going to have a node T
type and we're going to have basically
two doubly linked lists one is for read
and one is for write and then we're just
going to have a flag to tell us if we're
close and then a mutex that we lose and
like alias for the lock so we're just
going to look for what right does so
when you write to it we actually take a
pointer to a node and then just uh so
basically if it's closed we're going to
basically tell the writer it's close and
execute that function then we just bail
out early otherwise we take a lock and
then we basically detail remove is just
just your standard link list removed
from the end of the list actually from
the beginning of the list it removes the
read head from the read list and then if
we don't have a reader we add our cell
add that node that we had to the to the
right list otherwise so the elf
statement means we did find a reader so
we unlock
we move the value from the writer to the
reader and then we close it we sense
into the closest false so we don't
signal it because we closed and then we
call the writer funk in the reader funk
and then this is the one part I haven't
seen other people do when you separate
out where the note is stored so we have
a channel writer and the channel writer
actually stores the note and then we
this is what the right looks like so a
little bit how many of you been to any
of the co-routine talks oh good so a
significant amount of you so basically
when you want to write your own type of
co-routine thing the what you want is
you want to have some sort of a wader
struct and then you have three functions
that are used that the runtime system
will call a wait ready a wait suspend
and a wait resume so wait ready tells
hey are you ready already and we're
never ready a wait suspend
that's it passes a co-routine handle the
weekend you know used to eventually wake
us back up and a wait resume will once
we woke up we will call this and then
see your weight will return whatever
this is so in this case we're going to
skip a way to spend for now a wait
resume we just returned to or false
whether the note is closed or not and
then we return there later any questions
so far all right so this is what a wait
wait suspend looks like so we have the
from the previous thing we had the
pointer to this which is the pointer to
the outside channel writer and then we
set the note function so when we get the
note base we static cast down to you
know what would the drive type is and
then we access the data from there and
we and there's a the co-routine handle
has a static function that can return us
a co-routine handle from an void address
and then we call it to resume so that's
our function that'll get called whenever
we wake up and then we store the
co-routine address
in the no data and then we write write
the node to the channel so this is what
our channel writer code looks like
so go routine is basically the promise
type where basically we take in a
channel the first thing we do it is we
create a wait channel writer and then we
do the count to the right and then we
close it same with the reader you know
so if you look this code is you know
there's a lot of people that say oh C++
is very low-level hard to use this code
is pretty similar to what you would see
would go so it's pretty close to the
same level of expressiveness in age
library form and sorry then I ran this
code and what happened it crashed on me
any idea why so if you do stuff a
million times when you do the reader and
writer funk you end up growing you can
end up if you have the right combination
of calls you end up growing your stack
as each one keeps on reading and writing
and then you have a stack overflow
so how can what do we need to fix this
we need to have something in between any
ideas what we need we need this guy the
guy in the black mask
he's an executor and if you look in the
background you see all these ropes so
that's a multi threaded executor so the
trick is how do we make an executor
without allocating our whole thing okay
well now view now if you pass it to the
executor how are we going to do this
well what we do is we have the executor
that actually uses the nodes as a linked
list so we have the head and the tail as
a doubly linked list and then like I
said there there is no this is not
locked free this is just you know the
most naive implementation you could
think of for an executor so we just you
know add it you know being able to add
it and then a run while it's true pop
one you know pull
from the beginning of the list run it
and then you know if you don't have
anything away in a condition variable so
then we rewrite our success instead of X
unit directly we add to the executor any
questions so far about what we've done
all right
this is what a performance looks like so
if you see we're actually can everybody
will see this so right there laser
pointer see this one so right there is
when we have the channel with the
co-routine and right here is the
callback so it's about the same speed as
the callback now this wasn't the most
rigorous of performance testing and
everything so basically but you can see
the ballpark it's in the same it's in
the same range as a callback so we've
actually gotten a lot faster than future
and we're and we're basically in
callback territory but right now we're
not really doing much more than we could
with callbacks so let's move on anybody
know what this is so it turns out denial
is not just a river in Egypt it's also a
river in Sudan so this is where the
black Nile and the sorry the Blue Nile
and the white now come together so we're
combining two channels so our goal was
to do a channel select a when any four
channels so just an example of how you
do this
and go with built-in language support so
we have a writer so what we're going to
do is we're going to have two channels
now and for a count if it's odd we write
to one channel if it's even we write to
the other and then we close both and
then for the reader we do a select so we
don't know which channel we're going to
get the value on so we do a select that
comes from a referee from one channel or
the other depending on which one is
available so this is basically the
problem in the future when any so let's
so this is how to kind of you know
just for performance testing how to do
when any in a loop and it's convoluted
basically what we're doing is we're
using the loop executors and we're going
in a loop we're doing is selected as you
can see the selected when any and then
we set up with the selected looks like
and then we set the value to one or the
other and we run the queued closures
until we finish and and inside selected
selected then we're just careful because
we had to refresh our futures all over
again so if we use one up we can't reset
it we had to create a new one
put it back in anybody idea have an idea
of how this performs compared to our
regular future any guesses elephant yeah
so it looks like this so the middle one
you see the middle one that is future
select unfortunately you may not see it
the other iterations or a million this
one is 10,000 because when I ran it it
get an error saying I'm out of thread
resources I think when any may actually
spawn a thread so this is what it looks
like so so yeah so that's really bad so
I'm going to leave that out for now so
this was what the go select looked likes
about twice as worse as the go with
channels and like I said everything is
way way better than future select so
just on the side when we're designing
channels why can't we just use a stream
of futures right just have one where
channels basically a queue their returns
a future I played around with that a
couple years ago the problem is what
happens to the non-selective future
right you only want one thing you did a
select you got two futures back and you
didn't win any you got one now the other
one becomes available what do you do
with it is it lost you drop it you try
to push it back in that's a problem so
we're going to do a little bit of
thought driven development and here's
our idea for how we're going to do
select so inside the know
we're going to have an atomic pointer to
a atomic integer that's done and we
separate out what's what we call the
selector so whenever that triggers we'll
know you know that one of these ones in
our in our series was triggered and we
won't do with it anymore
it'll become more obvious in the code so
let's see here so this is our node base
that we had and so we just add an atomic
star atomic int star P done and then we
have some little stuff for handling P
done so basically we have this thing
said done so if it's null we return true
and otherwise so there's three states it
can be zero means we haven't done
anything with it ones means we succeeded
negative one means it's in dirt
indeterminate keep looping and we just
have little bit of helper functions
where you call set intermediate which
takes it from a zero to a negative one a
clear intermediate success between
succeeded take it from intermediate
negative one to one failure
take it from negative one to zero and
set success is from zero to one and
we'll see how we'll use this and then we
just have a place have a channel
selector that will hold the atomic
variable so so this one we're going to
do for reader so this is what we had
before doing anything so basically we
look if it's closed in the right head is
no pointer we unlock the lock and before
we just say reader closed equals true
and then add reader and return so now
instead we say can we say if it's set
successfully we do it which means that
nobody else has triggered the P done or
else P done was was no pointer and if
somebody else signaled it there's
nothing we need to do we can just remove
it we can just ignore it and otherwise
so then before then we found a matching
writer
so before we grab the one from the head
of the list and use that writer now we
we try to set intermediate on the writer
and that will fail if somebody else
already triggered that writer and so the
same as before if we don't find the
writer we add ourselves to the list this
is where we did successfully and we add
this code where we set success on
ourselves that may fail and if we're
able to do it if we can't do it then we
add ourselves to the list and we clear
the we add the writer back because we
you know we don't need that right or
anyone we add it back to the head and we
clear the intermediate to failure so
back to zero on the on the writer
pointer to done and then we return
otherwise we set the writer to success
that means so nobody else that was in
that select group can trigger the writer
we unlock and then we move our value
from the writer to the reader and then
we set the close to false and add it to
the executor so then we can so then we
can write a select range so there's we
can write there's one I wrote and select
range and there's one of very attics
will do the Select range we look at the
Select range because it's it's it's it's
less hidden by metaprogramming stuff so
select range basically takes a range and
then a function that we apply whenever
we were successfully read or write
something in that range when I one of
the channels gets triggers so we have in
a waiter and then we return their waiter
so this is what the away to looks like
so we have restore pointed to the range
a function a channel selector and we
store which note is selected and a mutex
so as we did we're never ready our
suspend so we take a lock at the
beginning and then we then we call read
passing in the selector on all the
channel readers and this is what read
looks like so we store the the
the co-routine handle address we said
the no data to the ax waiter and then we
set P done which was no pointer we set
it to the selector on variable and
inside the wake-up function we we
actually lock and unlock the mutex and
that prevents a race condition where
something can wake up before all the
other parts of the selections have done
the reads and then we set the selected
node and we wake up
that's the wakeup function the node func
and then we pass it to read await resume
basically we go through the range and
what's interesting is so we call remove
for the node on all the on every part
for we call no dot remove for the under
channel waiter so that removes it from
any channel that from the channel that
it was in and this is and that's why we
use a doubly linked list because we can
do remove in constant time so then we do
we check if the node that we are
currently on is a selected node other
what it is we call the function passing
the value in and we saw the selected
iterator to that and finally return a
bully a pair with a boolean that says
whether it was close or not and the
selected iterator sure
why we need unique lock so right here so
in a way to spend so we're doing all
these reads right so we don't want to
like be part of the reads because once
we hit once we hit once we start doing
the a to remove we want all the nodes
that were in there to be cleared out of
any channels because from then on we you
know the the co-routine may run and may
destruct and then we'd have dangling
pointers that make sense okay so remove
just you know we'll call remove reader
for its associated channel which is just
locks it and removes the ahead removes
from the end of the sorry moves from the
tail of the reader list and at that
point basically part of this it's not
they're basically sets all the previous
the next point is on the node to no
putter so that way we can there's no
there's no error in calling this even if
it's already been removed so this is
again what our go select writer looked
like and this is our channel select
writer so our so the only big difference
is we have these channel weight writers
and you know pretty much the same code
if we are an even we write to one and we
write to the other and the reader so we
do a select basically we pass in what
the channel reader is and then a lambda
in this case we're ignoring it so we're
not doing anything and now on the bottom
P force it will return whether the
channel that was selected was closed or
not if it is we break and here's our
code so our code is about twice the
twice as slow as the callback code and
about and about fifty maybe 50% faster
than the go select so then we you know
so some other features that we have we
can add we had buffer channels we won't
go over that now allow select with read
and write writers to be mixed and
channel readers and writers for threads
this is basically more proof-of-concept
stuff so there's still a lot of stuff we
can do an optimization there's been no
optimization other than
you know removing the allocations so
going back to our thing so channels can
be useful way to think about concurrency
this was our problem so you know we read
input serially we process it and we
output it serially as well let's make it
harder let's make the output in order
and let's recycle buffers so basically
we're thinking of is maybe hey we have
some sort of you know compression
algorithm or something you read in a
block the file you sent it off to send
it off to a multi-threaded processor
because it's CPU intensive but then you
want to write all the blocks that you
read compressed but you want to write
them back in order how do we go about
doing this I mean it'd be I think it'd
be difficult with channel width futures
but we're going to try to deal with
channels here anybody know what this is
right but we are going to use shan
parent style which is no synchronization
primitives right no Atomics no mutex is
uh no barriers so we're just going to
use the channels so here's how we do it
so let's just do some type aliases we're
going to have an array type that we're
going to read 32 Meg's say we're going
to read them in fixed block like that
size and we're going to Bank the payload
unique pointer and we're doing our
unique pointer on purpose for because
it's move only and then we do an order
payload which is an integer and a unique
pointer so this is what our reader looks
like so this is where you know the I
just did this on a simulation so
basically you know we wouldn't have
account we'd read from a file so
basically increments a integer and
basically right first of all it grabs it
reads a payload type from a pool reads
from the pool channel and then once it
gets that it you know maybe uses that to
read from disk or something and then we
write that moving it to the to the
output Channel any questions about this
so then the processor so this is
something I guess reading the go does
this a lot where you can have like a
make function that will create like the
channels that you need like
the output channel run the whatever
processor you have and then return the
output channel to you so this creates
the output channel for the channel to an
ordered payload and runs the processor
spawns up a new processor and returns
the out Channel so this is what the
process looks like we have an in channel
and an out channel and so we create the
reader and writer and then inside the
channel itself remember we're not doing
anything other than setup we make one of
the array types so we read from the
reader and then if the reader is closed
we close our out channel and then we're
done and then we're just going to
pretend to do some work so we're going
to you know sleep for a second but in
actuality we might be using so we've got
two buffers now the results dot second
dot second and the outer ray so we can
maybe you know copy from one to the
other do some sort of compression and
then we write the outer ray moving it to
the output channel and then we move the
the one that we read from the input
channel because we're done with it
because they're just the other stuff we
move it to be able to recycle it again
for the next next round
now the writer is the one that's most
complicated because we've got to reorder
everything so we actually have another
channel too
to being able to be felt to signal when
we're done and then we have a channel to
the pool because we want to return all
these payload types back to the pool so
we create create the writers and then we
create that we create a vector of
readers from the input channels and then
we create a vector of the ordered
payload type for a reorder buffer and
then we reserve however many readers we
have and then we start with zero and
then the first thing we do is if we need
to exit so the readers is empty so
there's no reader channels that were got
and there's nothing in the buffer then
we're done so we just write write true
to done and then we return and then
otherwise we do a select range so that's
what we
so we on the so we do a select on all
the readers and then we get the ordered
payload outright so what we do is we
then take it push it into the reorder
buffer moving it and then we use the
which is one of my favorite STL
functions push heap so which will you
know create like this one because of
order to create a mini heap so that
means that the smallest value is at
reorder buffer dot front and then and
then we also so that that takes care of
the call to select range and then we
check if if the channel that we actually
selected was we selected it because it
was closed so if it's closed we just use
that iterator that we got back in the
second part to remove it from the
readers and then we have a loop while
the reorder buffer is not empty and we
look to see if the first thing in the
reorder buffer that'll be that's that'll
be sorted that actually that'll be a min
heap for us if the first is current and
then we if that's the case we know it's
in order we just do something to output
it we increment current pop the heap and
and then write that buffer back write
that payload back back to the pool so we
recycle it back again for the reader to
use so once you've started everything
and allocated there's no more memory
allocations that happen this is what we
do from main we have a little thing
called channel runner that just was the
channel executor disk creates a new
thread and calls run on it so run
however many threads we want we create
our input channel our the order payload
type the pool and the done and then we
have the so what we do is then we call
the make processor and store them in the
ordered payload channels for the
processor which remember the out chain
that we got from the processor so all
the output channels when processor we
store in a vector and we pass that
vector to writer along with the dungeon
and then I actually this not on here but
it's an implement
tation I wrote like a sink channel
reader till you can use channels on a
regular thread so all you had to write
as a reader and a writer and you can use
the channels will interoperate between
fibers between co-routines
and between regular threads so in this
case we actually create a buffered we
create a buffered channel so if you look
back here where the pool it can take
like a buffer size so what it'll do is
when the right it'll non-block as long
as though you've got room in the buffer
so we you create a bunch of the array
types we write them in there and then we
wait for it to complete any questions on
this code so basically we don't have any
synchronization primitives using
channels we've written something that
can you know read from something process
in parallel and write it back serially
in order and you know not allocate any
memory while it's running so basically
we've gone and we've you know taking at
least go channels and try to bring them
to the dark side of the force right so
here's some more information the
concurrency TS system or there's been
some good talks on co-routines
and also there's a very nice article by
Dmitriev ukhov about how you know
something underlying go channels are
implemented if you want any code that's
here alright and I'll be happy to take
any questions and just remember like I
use these channels for that but like I
said if if you want if you don't if your
compiler doesn't support it but you have
C++ 14 I've got some you know that you
can either adapt this or else I've
gotten implementation for like I said
another implementation of library base
stack Lascaux routines thank you very
much
any questions yes I have not written any
articles on on this channels like this
was like I said it was mainly for this
presentation like you know you need you
know documentation more articles and
stuff like that for sure and better
packaging anything else yes go ahead so
the question is the go channels can do I
PC can do inter process communication I
believe at least unless they're
implemented you could the I believe it's
intra process as implemented but what
you can do is to do inter process
communication you could have another go
routine that sits between channels and
that basically does the inter process
communication and then reads it from you
does the inter process and writes back
the result to you so you are
communicating strictly with what you
think is just a local channel but the
other go routine can act as a proxy and
you're doing inter process communication
still using channels anything else
yes go ahead at this point the question
is can a channel be reopened once it's
closed
currently the answer is no but I've been
thinking about it and I think it is
possible to be able to do it you just
had to be careful how you do it but and
and there are some use cases where it
could be useful
go ahead this was all so I repeat the
questions so this was tested this stuff
was tested with visual C++ 2015 like the
highest optimized optimizer levels for
everything is on one program and I've
actually got this so it actually used
the the this one and let's see Channel
so like these first two or actually I
use with my the co-routine library just
to test it out and comparing with
futures and everything and though that
was tested with GCC 6.20 3 on Ubuntu
1604 and the results were roughly about
the same compared to relative to each
other any other questions comments
alright then thank you very much for
your attention
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>