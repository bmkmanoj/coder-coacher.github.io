<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: Matt P. Dziubinski &quot;Algorithmic Differentiation: C++ &amp; Extremum Estimation&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: Matt P. Dziubinski &quot;Algorithmic Differentiation: C++ &amp; Extremum Estimation&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: Matt P. Dziubinski &quot;Algorithmic Differentiation: C++ &amp; Extremum Estimation&quot;</b></h2><h5 class="post__date">2015-10-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3be3fznNiPk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to talk about I'll do it
with the differentiation C++ and extreme
estimation and I guess I should start
with the Y so what is it what do we need
it for and when do we need it so the
slide there's a few of them I'm just
going to cover the subset but the full
type is going to be available online so
first let's start with the Y the
scenario is is that we have a bunch of
data which is usually the bunch of
numbers that I'm going to call Y what we
would like to understand about this data
is to either something that explains how
they come about or something that is
going to tell us what the data is going
to be in the future and the tool we use
to do that is called a model which is
used just a bunch of mathematical
equations that explained us that what we
need is to have some kind of connection
between the model and the data and the
idea is that we try to feed the model to
the data so we would like to have
something that either makes the model as
close as possible to the data which is
maximizing the closeness or something
that at least tries to minimize the
error all models are wrong some models
are useful we might as well minimize the
destroyed and see there are different
names of four that is heavier
overloading houston numerical computing
sometimes it's called estimation
sometimes called calibration something
it's called training by Tori the same
thing now the problem is in general we
don't have a closed form solution that
is going to tell us the answer it would
be nice if the numbers are stowed prices
we would really love to have a simple
mathematical equation that tells you
well this stock price is going to be
exactly that many dollars in the future
most of the time we don't seem to have
something that works so the idea is to
a mathematical equation that's going to
tell us at least something reasonably
approximately close by minimization or
maximization in short by extremum and
that brings us to numerical optimization
because our model is a function that is
a computer program that's the function
that we would like to feed to the
numerical data so the way we can
optimize a function numerically to find
the parameter that makes our model fit
the data the most is by numerical
optimization algorithms there are
basically two classes of them the first
one is derivative free the second one is
gradient based so gradient based really
means that it's just going to need a
part of the riveters of a function and
the derivative of a function tells you
how much is the output of a function
changing as I change the input
parameters so you can think of the
function being for instance as pit of a
car or let's say let's roll with the
speed of a car and the argument being
the time now if you press the
accelerator the speed of a car is
changing right so the rate of change of
the speed of a car given the rate of
change in the time line that
acceleration and acceleration is a
derivative of a speed function with
respect to time so the rate of change of
the output given the rate of change in
the input that's basically a derivative
and you can imagine that's pretty useful
if you try to find the parameter that
gives you the best function value
because the relative directly answers
the question whether those tweaks you do
in the parameters actually have any
effect so the others that use
derivatives that are often better
they're often faster they're often more
accurate there's only one problem
we don't have formulas for derivatives
either for most practical applications
so that brings us to
another layer of indirection now we
would like to have a numerical algorithm
going to differentiate out function and
is going to tell us what is the
numerical derivative of the function and
where C++ programmers so preferably
would like something that is fast but
also something that is accurate it's
kind of like the zero overhead principle
we often like to have a no compromise
compromise and it would be also nice if
it's simple so that's essentially the
idea behind rhythmic differentiation
it's automatic it only is a chain rule
which I'm going to explain and also as
exact as it can be on a computer so when
I say exact that means there is probably
something that is not as exact so the
question is how does it compare to
different balance of methods and in
order to explain that it would be a nice
idea to look at how we can calculate
derivatives on a computer so a bunch of
approaches one of them is fine a
different thing I'm mostly going to talk
about order to differentiation ad
there's also symbolic differentiation
but they are related so starting with
finite differencing let's go back to the
example of a speed of a car and suppose
that we denote the speed of a car with a
letter F and the argument the letter X
is time now if you would like to have
some notion of acceleration of your car
which it could do is to see how much
time has passed how much has the speed
increased and get the ratio of these two
approximate the acceleration so that's
pretty much all there is to finite
differencing and that's the forward
difference approximation basically
approximate the derivative but the ratio
of the changes this is a very easy
method it's usually relatively fast if
you have a small number of arguments
it's usually not very accurate so
what we improve is two central
difference approximation which in a
sense is twice as accurate but it's also
twice as expensive so that's basically
the outlay we could analyze this with
some culturals and the bottom line is
that the smaller this x that is the
better the accuracy and that again kind
of makes intuitive sense if you think
about it because the step size is your
resolution so if you were rendering the
star graphically if you are trying to
have a visualization that animates the
motion of the car you can imagine that
if you increase your resolution if you
make those time steps smaller then the
animation is going to become smoother
because it's going to be more accurate
so that's essentially the idea behind
decreasing the step size as something
that makes the approximation better well
this idea is true in culturals calculus
is true for real numbers do we have real
numbers on computers we have something
similar so it's called floating point
numbers they are slightly different they
are not associative they are not
distributive here's an example you could
have a bunch of numbers number a number
B and number C and what we are going to
do is to add a plus B plus C when we are
going to add C plus B plus a and then we
are going to see if there is any
difference so what a plus B plus C that
C plus B plus a so for so root and of
the difference what mathematicians don't
like floating point numbers because they
are not really what they are used now
the representation and the reason for
floating point numbers this is not a C++
standard but most held were follows the
I Triple E 754 standard it's relatively
similar to scientific notation so if you
have ever seen something like 5 times 10
raised to power 4 that's pretty much
similar to how floating point numbers
are stories except that we have to rise
to some other power and the bottom line
is that we basically have two main types
float single precision and double double
precision and there is a way you can
measure their accuracy in a relative
sense
it's called machine Epsilon so you can
imagine yourself standing on the number
axis there is number one and then you
ask what's the next number after dart so
you can see that on real numbers you
should be able to get arbitrarily small
arbitrary close right there shouldn't be
any gaps between number one and the
number next after one while in floating
point numbers there are gaps because we
have only finite amount of memory on a
computer so we cannot store numbers to
infinite precision we have to stop the
precision at some point and that point
in terms of the floating point
arithmetic is made by machine Epsilon so
it tells you what is the gap around
number one and for single precision
that's going to be of size seven digits
after the decimal point roughly in
double precision that's going to be 16
digit after the decimal point
so the total accuracy around number one
that you can represent is just sixteen
digits and the reason those numbers are
called floating-point numbers is that
because this address is actually
changing so if you move to a higher
number you have less numbers after the
decimal
digits if you move close to zero
sometimes you use something crazy and
slow which is called subnormal don't
worry about that let's say relative
sense sixteen digits so what does it
mean for us well there is an implication
at some point those floating-point
numbers that we are going to subtract
are going to get close to each other so
remember that in this approximation what
we do is to subtract f of X from f of X
plus h if you get closer and closer and
closer at some point the result on the
computer is going to tell you that the
difference is zero even though on a
piece of paper it would not be zero so
the problem is as we make the step size
smaller
at some point there actually increases
you can represent this graphically it
looked like this so if this was just
real numbers on a piece of paper we
would only see the right half of this
feeder and we would have the error
decreasing as the step size grows
smaller however because we have
computers using finite amount of memory
with finite precision floating point
numbers at some point as we made the
step size smaller and smaller smaller
there're actually goes up so finite
differences not very accurate next ideas
to symbolic differentiation maybe we
could encode the known formulas for
derivative of every single expression
there is at least every single
expression that we're gonna use in our
program problem is symbolic
differentiation is mostly applicable to
algebraic expression something you to
write on a piece of paper and computer
programs are again a little bit
different but I happen to have those
things called variables which well vary
they also have program branches
sometimes you have a function that calls
a function that was another function
that maybe has a if statement that goes
to one place to another and this could
all be overwritten you
take that and iterate at a bunch of
times so that usually doesn't happen on
that piece of paper so are we back to
square one should we go back to find
differences well no we would like to
have our cake and eat it too
we would like to have something that is
automatic but also is going to be as
exact as it can be even comparable to
simple differentiation so that's all the
with me differentiation it's actually
been around since 1960s it just happened
to be called the back propagation
algorithm for machine learning in the
context of neural networks the idea is
very simple
it's especially appealing if you have
been accustomed to functional
programming you can think of your
function f or any numerical function as
a composition of the functions that are
called by that function may be those
functions also operators may be those
are other functions that inside have
operators but at the end it's basically
just a bunch of functions so if you
would like to obtain a derivative of
this function to just use the chain rule
and that's the only thing from calculus
you have to know to understand finite
differences it's just we understand
algorithm the differentiation does it
work well rigor says sometimes a
thousand volts I think I'm just
anesthesia the feeder there are two
parameters in my model that I'm
interested in parameter a and parameter
B so that's equation number three and
that's the only thing that matters here
let's see if this works I'm going to
simulate the data from this model and
I'm just going to give it some secret
values that I'm not going to share with
the numerical optimizer I'm just going
to teach it to myself that a should be
really 0.10 B should be 0.85 let's do it
a bunch of times so let's see if we can
recover them so this is the algorithmic
differentiation you can see that we have
parameter a parameter B 10,000
simulations and it's
imagine point now this is finite
differences this is a bit noisier and I
actually cheated what I did is to cheat
to my disadvantage and give finite
differences a chance by picking and
choosing to a better optimization
algorithm actually resulted in better
values for final differences so the
difference is basically ninety nine
point four nine percent successful
convergence for allele differentiation
and something like 3 percent or final
differences that's it there are more
slides more explanations best thing of
all if you would like to use this there
is a bunch of awesome open-source
libraries that also worked with linear
algebra libraries like I den which I
highly recommend so check it out thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>