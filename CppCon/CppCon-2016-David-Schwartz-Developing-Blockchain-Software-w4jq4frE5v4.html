<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: David Schwartz “Developing Blockchain Software” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: David Schwartz “Developing Blockchain Software” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: David Schwartz “Developing Blockchain Software”</b></h2><h5 class="post__date">2016-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w4jq4frE5v4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a pleasure to be here today thanks
to Brice and John thank you all for
coming how many of you know what a
blockchain is how many you've heard the
term knows something about it a couple
of hands anyone consider themselves a
blockchain expert think they know it
really well that's great because you're
all going to be by the end of this talk
really I'm not kidding I know you guys
are laughing that's not that's not funny
I do have one one thing to ask though if
any of you have looked at my slides
ahead of time please still laugh at my
jokes it's important for my self-esteem
so I'm the chief cryptographer a triple
and I was one of the original architects
of the rippled consensus ledger which is
a blockchain I'm known as joe katz
inaudible online communities the company
I work for is ripple the global leader
and distributed financial technology
we're working on allowing people to move
money the way they move information
today the internet of value in
distributed financial technology to
allow banks and financial institutions
to transact directly rather than going
through the correspondent banking system
and we have 135 team members 2/3
engineers a lot of experience in
different fields and main offices in San
Francisco offices New York City London
Sydney and Luxembourg we're developing
blockchain software and C plus boss have
been doing so since 2011 so we got a
vast array of customers and partners
working with major banks to move money
more efficiently so blockchains what are
they what are they good for block chains
records state and they record history
history past state state is modified by
transactions a transaction to command
some change in the state and the
important thing is that transactions are
eventually agreed on so if you transfer
a toget token that represents some value
everybody will eventually agree that
that value was transferred assets the
things that block chains manage funds
whatever they are are controlled by
identities which are represented by
public keys they prove their authority
by digital signatures so a transaction
is signed with a digital signature that
connects it to an identity the integrity
of the blockchain is protected by secure
hashes and what that means is we can
communicate a secure hash and then we
can follow a hash chain to find the
history of the blockchain
well boy that sounds an awful lot like
something you guys probably know a lot
about right I mean that's a database
database to store information they have
transactions that change their state is
a blockchain just a database well not
quite because the blockchain does
something that databases don't do and
that's managed a double spending problem
so if you're going to have some kind of
a payment system some kind of an asset
tracking system and Alice has an asset
let's say ten dollars she has to be able
to send it to Bob but we don't have a
useful asset transfer system and she has
to be able to send it to Charlie or
again we don't have a useful asset
transfer system but if she can transfer
the same asset to both Bob and Charlie
again we don't have a useful asset
transfer system so somehow if she sends
their ten dollars to Charlie that has to
prevent her from sending the same ten
dollars to Bob pretty basic stuff so how
do we usually solve this truly we've had
this problem before and certainly we
have and we solve it with a central
authority so if I write a check to
someone my bank is the central authority
that says that check is good they
prevent double spending by reconciling
against their ledger so they have a
record somewhere that shows how much
money everybody has and they say that
check is good that check is not good you
can also imagine doing it with secure
Hardware I could have some sort of box
that knew how much money I had signed a
transaction to transfer funds and then
it knew I had less money but someone has
to put the keys in that box and you wind
up needing a central authority it's very
difficult to design secure hardware that
we can trust without some central
authority to put the information in that
Hardware before a block chains we had a
lot of effort to produce these
decentralized asset transfer systems
Hashcash which came up with the idea
that you could use proof of work to
create something scarce computers can
only do so much work however much work
they can do they can't do more than that
so you can create a scarce resource by
attaching it to computing power B money
was the next step which came up with the
idea of essentially having some trust in
the servers to solve the transfer
problem so hash cash could regulate the
control of an asset but not a good way
to transfer it somehow you had to know
who held that asset B money had the idea
of trusting the servers Ryan Fuger
develop triple classic which had this
idea of instead of having one central
authority would have lots and lots of
authority so every asset could have its
own Authority and that way no one
authority ran the system just an asset
the breakthrough came
with Bitcoin which was the first
blockchain and it is literally a chain
of blocks and by that I mean that each
block has the secure hash of the chain
the block before it so you can follow
that chain from the current block
through the history of the system
transactions transfer a native token so
that is Bitcoin creates some sort of an
asset a unit a transaction transfers
that asset from one cryptographically
proven identity to another it has what
we call a UT EXO model this gets a
little technical but it's kind of
important to understand a UT EXO is an
unspent transaction output if you have a
Bitcoin you have a UT EXO a UT EXO is
essentially some bitcoins that have been
sent to someone but not yet been spent
so a payment creates a UT EXO if I send
you a Bitcoin I create a UT X so that
you have the private key to claim for
that Bitcoin and we assume the network
degrees on the set of UT EXO's we don't
actually directly check that but if they
agree on the transactions transactions
produce UT EXO's they should agree on
the set of UT EXO's and the amazing
thing is that bitcoins meet all the
requirements for currency so they're
scarce you have to have something scarce
in order for it to be a currency if we
can produce an unlimited quantity can
produce an unlimited quantity but if
there's something that regulates the
quantity anyway it's usable as a
currency so it has to be fungible that
means that the units look the same so I
don't particularly care what dollar bill
you give me it could be an old one it
can be a shiny one but I treat it as a
dollar
we don't use apples as currency because
there are nice apples there are not so
nice apples it has to be divisible if
you imagine we had only hundred dollar
bills that wouldn't be a very useful
currency how would you buy cheap things
with the bitcoins are divisible and
they're durable so you don't want
something that spoils it's a terrible
currency you want something it's durable
and you want something that's easily
transferred so we don't use blocks of
gold as currency typically directly
because they're difficult to transfer I
would have to give you the ball of gold
I would have to carry it around with me
so instead we virtualized it into an
asset that is transferable to fix that
problem so the amazing thing is is that
this native token on this blockchain
meets all the requirements to be a
currency which means that people want to
use it as a currency they can Bitcoin
solves the double spend problem with
mining it also solves the distribution
problem if you imagine that you created
a currency that had no central authority
how would you introduce it in the first
place there's nobody to give it away to
people there's nobody to sell
there's nobody to give it away so
Bitcoin does it through mining which is
a computational contest I used to joke
that it's like giving them a way to
whoever wastes the most electricity
that's frightening Lee close to the
truth the longest chain wins so that is
the chain that has the most
computational effort wins and miners are
incentivized to lengthen the longest
chain so if you sent me a block a
Bitcoin on the longest chain that chain
is going to get longer and longer and
longer the other chains are not because
nobody's paying to lengthen them
we have eventual consistency as soon as
the BOK chain that has your bitcoins on
it is sufficiently longer than the
others you can trust that it's going to
stay that way and that solves the double
spend problem in practice it's also
interesting to note that we have both a
currency and a payment system so we have
bitcoins in the sense that I can have a
Bitcoin and we also have Bitcoin which
is a blockchain that can transfer that
asset and you could imagine separating
those things that you could imagine a
bank that issued Bitcoin certificates of
deposit held peoples bitcoins for people
it can have its own payment system but
Bitcoin is both a currency and a payment
system because it's rooted in a
blockchain that also supports
transactions and the system regulates
the introduction of new currency the way
mining is controlled it creates bitcoins
at a predictable rate and the supply is
ultimately fixed so we actually have a
deflationary carnate currency some
people love deflationary currency some
people hate them but that's what we have
the rules are notionally set in stone by
which i mean we all have software that
controls what happens with a Bitcoin and
we can only change those rules by social
consensus so for example the Bitcoin
supply is limited to 21 million bitcoins
by the software but if everybody decided
to change the software so it produced
more bitcoins you don't have to change
your software but then you wouldn't
agree with the rest of the world and
because the world does have to agree it
requires eventual consistency everybody
has to agree on the rules and agreeing
on changing them which has been a bit of
a governance problem it has UT Excel
model and it uses mining to secure
transactions Ripple one of the next
block chains I have the worst elevator
pitch ever for ripple it's a platform
for issuing holding transferring and
trading arbitrary assets I'll unpack
that a bit I began in 2011 when Jed
McCaleb hired me to try to find another
way to solve the double spend problem
without a central authority we came up
with a distribute
agreement protocol which is kind of like
a room full of people agreeing on
something as long as they all agree on
which transactions to process and we can
tell what they agreed on that solves a
double spending problem - we replace
blocks with Ledger's and we allow to
arbitrary assets I'll go over that in a
second so we have Ledger's ledger is the
current state of the universe it's just
like the contents of a database so
instead of having unspent transaction
outputs that are sort of collected over
time we have the ledger that has
everything in it and that forms a secure
hash chain and ripple transaction sets
advance the ledger an advantage of that
is that prior Ledger's can be forgotten
if anyone's ever tried to run Bitcoin
you know it takes a long time for you to
synchronize the blockchain because you
need the every transaction that's ever
happened prior transactions can be
discarded and ripple you only keep them
if you want history so the ledger also
contains the transactions themselves and
what we call metadata so many debt is
what a transaction did so if I pay you
ten dollars the transaction presumably
if it's exceeded paid you 10 dollars for
simple transactions it's enough to know
the transaction succeeded if I write a
check to someone it's enough for
everyone to know that that cheque went
through but imagine if you had a more
complicated transaction say you had a
transaction that paid someone 10 euros
at the current rate and you were willing
to pay up to 12 dollars to do it well
just knowing that that transaction
succeeded isn't enough because it
doesn't tell you how much you paid it
doesn't tell you who provided the euros
and who took your dollars so a more
sophisticated system that tracks what a
transaction did can support more
sophisticated transactions it develops
eventual consistency by an algorithm we
call consensus which is a distributed
agreement algorithm and if you know what
pbft is good you'll find out soon enough
it doesn't require a hundred percent
agreement on the participants that was
what was interesting about Ripple prior
algorithms required everybody to agree
on the participants Ripple just require
a substantial agreement which means that
you can add and remove people without
having to agree on who's in and who's
out so it's ripples method of solving
the double spend problem transactions
are applied as a unit in groups it's
like a room full of people trying to
agree one of the reasons it's robust is
that an honest server just cares about
agreement so if you imagine we were all
trying to agree what to have for lunch
we all have different things that we
want some of us don't like gluten some
of us are on a low-carb diet some of us
are vegetarians it's hard for us to
agree because we all have different
interests but imagine if
all we cared about was agreeing all
everybody cared about is that we pick a
place to eat so someone says let's go to
the movie theater and it's like no
that's not a place to eat and someone
says let's go to Burger King in every
but it's like that's a place to eat it's
a much simpler problem when the
different participants have aligned
interests and they don't have unique
interest so if they all agree on just
getting the valid transactions in
agreement is much simpler
the other interesting thing to notice is
that you can solve the double spend
problem just by putting transactions in
order
why because whether a transaction is
valid or not as deterministic if you
want to spend money you have to have it
executing a transaction if two transfers
money you have it succeeds if it
transfers money you don't have it
doesn't succeed it's deterministic we
can all agree on it and transactions
either conflict or they don't if I try
to send the same money to two different
places we'll all agree that the second
transaction will fail but which is the
second transaction you might see one
payment first and then the other one
someone might see them in the other
order so transaction ordering is
sufficient to solve the double spend
problem and that's what validators do in
Ripple they propose sets of transactions
they Avalanche to consensus avalanche if
you imagine a snowball on the top of
roof if it starts to tip in one
direction everybody kind of pushes it in
that same direction until you reach
agreement very quickly and then they
sign the ledger so it turns out that
this is remarkably robust for us for a
set of reasons that make it seem a much
simpler problem than it than it
originally seen so one is all honest
participants will vote to include a
transaction if there's no reason not to
include it if I say I want to transfer
ten dollars to Bob and I have ten
dollars Bob's a valid account on the
system and everything looks fine every
honest participant will say yeah that's
fine that should go in and if a
transaction has any reason not to be
included maybe it's part of a double
spend whatever it's fine if it's not
included because if it's still valid
it'll be voted in in the next round and
the algorithm is biased to exclude
transactions to reduce overlap so what
that means is that if we can't agree if
there's a whole bunch of transactions
and there's just lots of disagreement we
can reduce the amount of disagreement by
eliminating transactions and not adding
new ones and that way the amount of
disagreement continuously drops it has
no rotating dictators so in Bitcoin a
miner chooses the transactions that go
in a block consensus doesn't have
rotating dictators so you can do things
like financial transactions where one
person controlled all of the
transactions they might abuse that to
their own financial benefit they might
take the juiciest offers they might
prevent people from moving funds and so
on and the past can't be rewritten
ripple is open source is C licensed
public ledger public transactions public
history equal access and it's written in
C++ by a team of developers who've been
writing high-quality modern C++ software
for several years now so how do you move
assets on a payment system that isn't
connected to any sort of financial
institution you might imagine if you
have a payment system like Visa or eBay
they have infrastructure that moves real
money and you might wonder how could you
have a payment system that wasn't
directly connected to infrastructure
that moves me at real money and the way
you do that is by virtualizing arbitrary
assets so an asset is identified by an
issuer and a currency let's say dollars
issued by Wells Fargo that seems like a
nice asset and because an asset can be
issued by anyone ripple only allows you
to hold an asset that you choose to hold
so I don't want my cousin Jeff to owe me
money that's not a very reliable
currency I'd rather have Wells Fargo owe
me money
I can choose assets have a counterparty
and what's interesting about that is
that means they can reflect a legal
obligation so if our CL says that Wells
Fargo is me $10
while that's while the ledger is
reporting that and ripple knows that
that's true there was a separate legal
obligation on the part of Wells Fargo to
owe me those ten dollars and presumably
even if something terrible happened to
Ripple that legal obligation would still
exist and you'll see in a minute what
that does so you have a bunch of
accounts their identities in the network
and they trust each other forming a
directed graph so this shows that alice
is willing to allow Carol to owe her
some money Alice is willing to allow
Edward to owe her some money of course
these would probably be financial
institutions but they could be people
and so you can have money by sending
money in the physical world so Alice can
actually send a hundred dollars to Bob
and Bob takes that hundred dollars and
pushes an electronic asset to out to
Alice now that electronic asset can be
in any system we want it doesn't matter
what's important is that bob boze alice
$100 and that's a real
the system is recording that fact but it
isn't that fact that fact is a fact
Alice sent Bob $100 bob boze Alice $100
so Alice now has this digital asset that
says Bob owes me $100
Alice can transfer that asset in any
system we want as long as Bob is willing
to honor it so if if Charlie also trusts
Bob and Alice sends those hundred
dollars to Charlie in any system at all
that Bob is willing to honor it doesn't
matter who runs it or owns it as long as
it doesn't mess up now
Charlie can redeem those hundred dollars
with Bob the only problem is if you
imagine what we all have different
people we trust them you get this sort
of spaghetti network where there is a
large number of participants and it's
not a very usable system so the solution
is to have a hub a hub would be like a
bank if there are a lot of people who
are willing to consider Wells Fargo
owing them $1 to be worth a dollar then
you'll consider it worth a dollar
because you know that other people will
consider it worth the dollar but now you
wind up with a whole bunch of islands
you wind up with Wells Fargo issuing
dollars you wind up with another company
issuing Euros or issuing bitcoins or you
honor all these different currencies so
how do you make a global payment system
out of an arbitrary asset system with
offers those things in the middle that
look like those double arrows those are
connectors they have an asset or an
account in two different systems and
they're willing to trade one asset for
another so if I hold dollars at Wells
Fargo and I'm willing to transfer them
for euros then someone who wants to make
a payment can give me the euros I want
and I transfer the dollars at Wells
Fargo and you can put together an
international payment from to domestic
payments so if I want to pay someone in
China and there's someone in China who's
willing to accept my dollars in the US
and supply the money in China
those two domestic payments from my
point of view act like one international
payment so arbitrary assets how do they
work money doesn't really move you
notice we never actually moved any money
what we did is we changed who owned the
money and that's just as good Alice
wants to pay Bob ten dollars what she
wants is Bob to have title to ten
dollars so he considers her to have paid
him so you don't actually have to move
any money you just have to swap the
ownership of the assets sender loses
custody of the asset they sent the
recipient gains
today payments ripple through the
intermediaries they gain and lose assets
to facilitate the payments they also
benefit now a crazy idea you can build
social credit out of that so typically
if you need money you borrow it from
someone typically a financial
institution I don't know how many people
here borrow money from their friends but
that's not really socially acceptable
but what you could do is instead of
borrowing money if I borrow $10 and
someone they don't have $10 and they're
out $10 and they might charge me
interest which makes sense because they
say take a disadvantage but suppose
instead of me borrowing $10 from them we
exchange IOU so I give them an IOU that
says I owe you $10 they give me an IOU
that says I owe them $10 those IOUs have
value those IOUs are something that
people will want if I have assets that
people want and you can build a system
like this someone has money at a gateway
they have a whole bunch of people who
trust them they can push balances to
those people now here's where things get
interesting so Alice notionally has a
$40 balance because the Gateway OS are
$100 but she owes $20 each of three
other people but the Gateway doesn't
know that she has those IOU she still
has that hundred dollars and you might
think that she can't spend that money
that she has to have a positive balance
that you wouldn't want to go negative
but if you think about it every asset
one party has an obligation some other
party has the system funds have to sum
to zero if Alice has $100 at the gateway
it's because the Gateway owes are $100
and has negative balance so a social
credit system would allow everybody who
wanted to to run negative or positive
balances and create value out of
borrowing works on Ripple today but it's
considered a pretty crazy idea
government money is not about to go away
there's a lot of people who think that
these things will take over the world
there's a multi trillion dollar market
in these legacy fiat currencies if you
will they're not going away anytime soon
so they are important you may have heard
of private blockchains these are block
chains that are not accessible to the
public they're becoming very popular now
for things like trade sell trade
financing settling securities and other
applications they're like public
blockchains except the participants are
controlled so you typically have a
cryptographic identity to use the system
at all
transactions can be private and they
don't need a native token they don't
have to worry about denial of service
attacks because you can cut off
participants you don't like attacks can
be mitigated because you know the
identity of the participants they can
react to a legal process so there's no
court you can go to to get your bitcoins
back if they're stolen there's no legal
process that you can apply to the
Bitcoin network because it's running on
thousands of people's computers and
there's no real management if any of you
are familiar with the block size
controversy there's no way to manage a
public blockchain really except to get
agreement a private blockchain can have
a formal management process and it can
be built into the software so they're
good for organizations of frenemies now
if you imagine banks have a cumulative
interest in making asset transfer
reliable but know what they don't want
anyone bank to run it Wells Fargo
doesn't want to use Chase's payment
system chase doesn't want to use HSBC's
payment system because their business
competitors you can use a private
blockchain to have a system where the
rules are enforced by software and
because the rules are enforced by
software you have redundancy if one
system doesn't follow the rules it just
get it gets ignored by the other and
they can be self-governing so that's
that that seems to be a way to save
money if traditionally if banks wanted
to set a payment system up between them
they would create an organization like
Swift it would have governance it would
have computers it would have offices it
would be very expensive they could solve
those same types of problems using a
private blockchain where software
enforces those rules no one person can
cheat the system or control the system
because the system works by agreement of
the remaining system of the participants
so a lot of Bitcoin enthusiasts seem to
think that there'll be one ledger to
rule them all we've beaten down that
path for a couple of years and the news
is there is not going to be one ledger
there is not going to be one payment
system there is not going to be one
blockchain there's not going to be just
like there isn't one web server just
like there isn't one payment system on
the internet there's one internet but
there's a whole bunch of different
systems on it and the reason for that is
that people like different things out of
their Ledger's and we want innovation
Ledger's are important they track assets
people want high-frequency trading they
want Ledger's that respond to government
subpoenas they want Ledger's that have
different rules some people want
Ledger's that are fully public some
people want Ledger's that are private
people will refuse to participate in the
system if North Korea has access to it
because they have regulatory
requirements there is not going to be
one payment system but like we don't
want Ledger's to be Islands either we
need a standard we need a way to make
payments across Ledger's we need a way
to build the system that consists of a
number of payment systems forming one
network just like the internet connected
a large number of private networks
together and we need a neutral standard
ripple has proposed Inter ledger as a
solution to connect payment systems to
each other and build a world where money
can move as easy as information it's
it's surprisingly simple so you have
Ledger's the track accounts and balances
but you have people who are on different
Ledger's a sender has a senders ledger
recipient on a different ledger these
could be financial institutions that
could be payment systems can be paypal
can be anything so we have a connector
that relays money connector has an
account in both systems and is willing
to accept the payment on one system and
make a payment on another so Alice wants
to pay Bob let's say Alice wants to send
100 widgets and deliver those widgets to
Chloe
whee Chloe receives a payment on Alice's
ledger Chloe makes a payment on Bob's
ledger and everything is good
except someone has to make that payment
first if Alice makes the payment first
she has to trust Chloe if Chloe makes
the payment first yes to trust Alice
there are tremendous advantages to being
able to use connectors we don't trust
because connectors can compete so money
would be lost if Chloe drops the payment
we can use escrow to fix that problem
and the ledgers provide the escrow I'll
show you why that's important in a
minute
so if Alice wants to send some money to
Bob through Chloe she puts the money
that she's going to send in escrow then
Chloe puts the money that she's going to
send in escrow and then the transfer is
executed from right to left so the
recipient gets the payment from Chloe
Chloe gets the payment from Alice and we
have an atomic transaction the
recipients receipt releases the funds to
the recipient to the recipient but also
the recipients receipt also releases the
file
guns to the market maker to the
connector to Chloe
so Chloe receives the funds from Alice
Bob receives the funds from Chloe and
the transaction is atomic so transfers
are executed from left to right their
escort from left to right executed from
right to left the ledger only has to
support two operations you have to be
able to put the funds in escrow that is
you have to lock them up so that
everybody knows they're going to be
there and you have to be able to release
them and as it happens just about every
ledger in the world can already do this
every banking ledger has a way to hold
funds while they're in transfer has a
way to clear the transfer so you can
make transactions across Ledger's
facilitated by connectors atomic just by
adding two operations to each ledger
operations that they can easily support
inter ledger has a set of crypto
conditions which is a standard that
specifies how to agree on whether or not
something happens so that we know that
Bob's receipt will release the funds
from the sending ledger the key is one
Ledger's receipt is another Ledger's
receipt condition so it's the receipt of
the funds to Bob that triggers the
release of the funds to Chloe leverages
the trust that already exists anyone who
has funds on a ledger trust that ledger
anyone willing to receive funds on a
ledger trust that ledger and you don't
have to trust the connectors so if you
look at it from Bob's point of view
let's say Alice Alice is sending Bob
money which is either going to create a
legal obligation for Bob maybe Bob's
going to send her some goods or maybe
Alice already owes Bob money Bob doesn't
want to give the receipt and not get the
money and he doesn't trust Alice or
Chloe Alice wants to send the money she
doesn't want to lose the money unless
bob gets it she has to trust her ledger
since it has her money
Chloe the connector doesn't want to pay
Bob and not get paid by Alice Chloe has
to connect trust both Ledger's already
because this Bob's ledger has her money
and Alice's ledger will have her money
she doesn't trust Alice or Bob she
doesn't want to pay Bob unless she gets
trust it gets paid by Alice and all of
those requirements are met all those
requirements are met by this system
because Alice cannot lose her funds
without the receipt that Bob get got
paid and Chloe knows that when she gives
that receipt to Alice's ledger she'll
get paid boy it just it seems so simple
I think I think some of you probably
know it's really not
that simple unfortunately so some
sometimes it is that simple sometimes it
is that simple
so why well when the sender puts funds
into escrow Alice's locks up $400 for
this payment and the release condition
Chloe gets that money when Chloe's pays
Bob seems good the problem is we need a
failure condition we need some way to
unwind this if it doesn't work why
wouldn't it work
maybe the connector can't meet the
payment terms maybe the connector loses
connectivity with the internet a ledger
break something can go wrong we need
some way to unwind this transaction if
something goes wrong now the sender from
Alice's point of view she doesn't trust
the connector she doesn't want the
transaction in limbo for an hour because
if Chloe doesn't make the payment she's
lost an hour of time the connectors held
her money but if the sender has to meet
a release time that's tight
but the connector is taking risk the
risk stems from the inability to get the
receipt to the ledger so the problem for
Chloe is what if Chloe pays Bob and then
something goes wrong Chloe can't get the
money from Alice the time expires and
Alice gets her money back in the
connectors out so for low value payments
it's not a problem you can use the
release time say 10 minutes and Chloe
can price in the risk of failure so if
there's a 1/10 of 1% chance of failure
she can charge 2/10 of 1% not a problem
for small payments not good for high
value payments for high value payments
you need to agree on whether a
transaction succeeded or failed you
can't have this situation where one
ledger thinks that transaction succeeded
and another ledger thinks it's failed
and you can't have a long lock time if I
want to make a million dollar payment to
someone I don't want someone having the
option to make that payment for an hour
without being committed to make it
because maybe they'll wait until the
financial markets move for their benefit
and they'll make the payment if and only
if they get an unusual fee so you need
proof that something didn't happen if
the payment doesn't happen you need
proof and simple schemes cannot provide
this proof of absence fortunately we do
have a scheme that can provide proof of
absence it's a fairly new technology and
it is definitely being underused it is
comes from the Byzantine generals
problem I feel they are familiar with
the business you generals problem anyone
know the Byzantine generals plumbing I'd
say probably ten fifteen percent so the
basic idea is you have two generals they
have two armies
and one of them realizes that if they
both attack a city the next day they'll
be able to take it but if they attack
alone either one of them alone will not
be able to take the city so the general
that realizes the plan is going to send
a messenger to the other general saying
let's both attack the city tomorrow the
problem is the messenger might not get
through but that's okay the receiving
general can send a messenger back saying
hey I got your message I'm attacking
tomorrow but what if that messenger
doesn't go go through does the sender
attack or not and the problem is you can
prove fairly simply that there is no
solution to this problem as stated even
with very simple requirements so simple
requirements would be if no messages go
through you can't attack because you
don't have a joint plan if all the
messages go through then both have to
attack otherwise you don't have a
solution even if nothing goes wrong and
then the other requirement would be the
number of messages has to be finite if
you have a message that requires an
infant a protocol that requires an
infinite number of messages before it
reaches a conclusion that's not a useful
protocol now it seems that those
requirements are simple and reasonable
but they're too many there's a very
simple formal proof that there are too
many I'll give you the formal proof just
in case anyone can follow it verbally
because it's cool and then I'll give you
an informal proof that's easier to
follow the formal proof is this if there
was some algorithm that guaranteed that
both generals attacked or neither
general attack it has to have a finite
number of steps let's call that n so we
have this algorithm that sends n
messengers and then if no messengers
fail they both attack now imagine if the
last message doesn't go through and no
messages after that go through well the
person who sent that message doesn't
know that message didn't go through so
they still attack and since the
algorithm presumably guarantees that
either neither of them attack or both
attack that means they both attack well
if they're both going to attack even if
the last message doesn't go through
what's the point in sending the last
message just don't send the last message
and the algorithm still works but we
have a problem we said that this was the
message with the shortest number this
was the protocol with the shortest
number of messages we just found a
protocol with one fewer message so we
have a contradiction if we have an
algorithm that works with n messages we
have an algorithm that works with n
minus 1 messages which means we don't
have any algorithm at all a more
informal proof is each general is only
going to commit if the other side
commits at some point some general has
to commit a rebek
some general has to say even if no more
messages go through I'm attacking if
nobody ever commits it never finishes
and someone has to commit irrevocably
first some general has to be the first
general to say okay I've gotten enough
messages I've sent enough messages I'm
attacking but we can't commit a
revocable until we know the other side
has committed arabica beliefs so neither
side can commit irrevocably first it's a
little more informal but a little easier
to follow and it also helps you to
identify Byzantine generals problems
when that argument applies you have a
Byzantine generals problem many of you
probably know that the TCP protocol does
not guarantee that either both sides
will see an error or neither side will
how many of you know that most you know
that doesn't provide that guarantee and
it can't provide that guarantee because
that's a business in generals problem so
we have this sort of you send these sort
of I'm done I'm done I'm done I hope you
got one of those messages maybe you
didn't but I'm not going to transmit
anymore but we do not guarantee that
either neither side will see an error or
both sides well and also creates an
interesting problem like you might think
it's a simple problem to say I'm sending
an email from one server to another and
I want the sender to stop sending the
email if and only if the receiving
machine got it again Byzantine generals
problem we run into the same
complication but we have an algorithm
that solves the Byzantine generals from
I know I just said there was no solution
it solves the problem by changing the
constraints a little bit but really it's
a realistic practical solution in fact
the pbn pbft stands for prom the stand
for practical I hope anyone know for
sure my misremembering I think it stands
for practical if not it could just as
well so it's a Byzantine agreement
protocol and it can tolerate some faulty
nodes it can tolerate drop messages so
long as enough of the nodes don't don't
fail and it combines beautifully with
crypto so as pbft was originally
specified it didn't use any cryptography
at all it combines nicely with crypto so
what you can do with crypto is since all
the participants are known you can give
them a cryptographic identity and so
they can't impersonate each other they
can't send one thing to one participant
another thing to another that makes pbft
run really nicely another advantage is
if you know what threshold signatures
are threshold signatures are cases where
you get cryptographic signatures from
say seven out of ten participants or
eight out of thirteen participants and
you can combine them into a single
signature so what you can do is if you
have a Byzantine Agreement protocol and
you have the correct quorum agreeing
let's say eight out of thirteen degree
you can
a single digital signature as a proof of
that agreement which is really really
nice
high-value payments and ILP is a BG
problem so we want the sent the payment
took Bob to happen if and only if the
payment Alice happens and what that
means is that the two Ledger's need to
either agree that the payment happened
or agree that it didn't consensus and
ripple is a BG problem we want the all
the service to agree on the transaction
ordering we don't want someone to pick a
particular ordering unless everybody
else does the double spend problem the
distributed ledger the distributed
database agreement problem these are all
BG problems in fact lots of problems or
BG problems and we now have a solution
to the Byzantine generals problem
essentially we gather in an army of
witnesses and we let the witnesses come
to a consensus on whether we're going to
do it or not they provide a digital
signature and then everybody knows from
that digital signature the thing that we
give up potentially is liveness if the
system fails that nobody knows may know
what happened until the system recovers
but that only happens if a quorum fails
when we can minimize the probability
that if there's not a single point of
failure we can control the number of
points of failure so it's easy to solve
with algorithms like pbft and we only
need those agreements to be private and
ephemeral you can also do that with
private blockchains private blockchains
use Byzantine Agreement protocols like
pbft so if you imagine a consortium of
banks that set up some payment system
they can set it up so that if eight of
the 13 banks agree on a transaction the
transaction goes through they can use
something like pbft to agree on that
transaction
okay now we're all blockchain experts
there is going to be some development in
C++ content I promise promise we're
getting there but had to make you all
blockchain experts first those are block
chains that's what they do so why are
they hard to develop what's so difficult
what are the unique software challenges
that we need to solve with C++ first of
all they have to be fortresses how many
of you are familiar with any Bitcoin
thefts there was a 70 million dollar
theft just recently you probably know
it's a huge huge huge problem and the
reason the problem is so big is a couple
of things but one of them is that we
don't have a financial system built into
the ledgers that can track the funds and
recover them easily once the bitcoins
are transferred they belong to another
cryptographic identity they can sit
silently for months or years and there's
no
to recover them there's no legal process
well I guess you could have a hard fork
and if you know about the etherium hard
fork you can sort of everyone have
everyone agree to wind the ledger back
but that didn't happen for a 70 million
dollar theft so and the code is public
so you can't hide your flaws you can't
put in a comment you know if anyone
figured this out we'd be screwed no you
might not put the comment in but someone
will figure it out so this is the
problem you got a multi-billion dollar
bug bounty so there's a tremendous
incentive for people to find the bugs
and because if they find bugs they could
steal potentially millions of dollars
and they're very unlikely to get caught
that stresses people's morals a little
bit I know if I found a bug in Cisco
software if I found a bug and you know
Apache I would probably do the right
thing and tell the community but if I
found a bug that I can make seventy
million dollars on a my probability of
getting caught is pretty low I like to
think that you know I would do the right
thing but I won't ask for a show of
hands who would do the right thing in
that circumstance because I don't think
you'll be honest but you know it's AB
it's a huge huge problem I can't I can't
stress how big a problem it is and the
other problem is even if it doesn't
allow a theft let's just say it corrupts
the blockchain let's just say creates an
invalid state on the blockchain you have
to get everybody to agree to rewind it
and that's not an easy thing we've had
very good luck Bitcoin had a problem
like that there was a block there was a
hard fork between two it was an
inadvertent mistake a small bug in the
software caused the two versions of the
software to have different rules for
what makes a block valid and so they
didn't agree on what the longest chain
was and there were double spins there
were spends on one chain that weren't on
the other chain and people lost money
it's impressive that the community was
able to come together and agree on how
to fix that but that is very very
painful very painful this makes
development slow if you're developing
software that everybody can see the code
to development is much slower I guess an
analogy would be imagine you're building
an operating system like let's say Linux
and you want to protect it from network
attacks that's not too bad now you want
to protect it from local attacks by
people who have user accounts that's a
lot harder because people have a lot
more access to the system they have a
lot more knowledge they have a lot more
control it's much more difficult to
protect the system against the local
user than a remote maybe makes
development as much as ten times slower
maybe not quite that bad maybe worse
depending very very paranoid the other
problem is when you're developing
mission critical software typically you
put out a new release and you tell
people if your software's
mission-critical don't upgrade to this
release just wait unless there's
something you really need just wait test
it out see how you like it block chains
have to have agreement by the entire
world so we can't do that we can't tell
people that they can run a new version
of the software unless they run it on a
separate network and one of the problems
is if any of you have released beta know
release betas and you try to get bug
requests you always get the bug reports
five minutes after the beta goes gold
that's always what happens people won't
test a beta you try to get them
hopefully big business isn't it means a
lot of money they will but they won't
because they have to build duplicates of
their systems that work on the beta it
won't replicate the real situation
because they're not going to build
everything on the test system so public
block chains must be fortresses and that
is one of the reasons that we like C++
and I'll show you why later we also have
to manage our resources we have to keep
up with the network we don't want to
fall behind people want to know that
they got their money immediately if
someone's making a payments us instant
satisfaction is extremely important one
of the big advantages in these systems
is that they're quick so we have to keep
up with the network we can't just fall
further and further behind some servers
have to respond to remote queries so
remote servers will ask them for
information they have to manage those
resources and not fall behind some of
them have to deal with local quarries so
the Bitcoin network stays stable because
people are willing to give information
to each other if I need history if I
shut my server up and turn it back on
other servers generously give me the
information that I missed they have to
be able to do that without overloading
themselves and because it's a public
network and there are no identities they
have to protect themselves against
attacks have to manage the resources
intelligently they have to cache you
have no choice you need binary formats
for data representation because you have
to sign transactions all kinds of
objects have to be hashed that requires
a single binary representation you have
to have unique binary representations
but humans don't like binary
representations very well that's why I'm
not giving this talk in ones and zeros
javascript likes them too javascript
likes json a lot and if you're not
careful you can wind up with a situation
where if you push something to jason and
then back to binary you don't get
exactly the same binary data back and
you need that because otherwise the
signatures and hashes won't check so you
need good binary form
and you need performance on everything
so some tasks are embarrassingly
parallel a good example is checking
digital signatures one of the biggest
things that blockchain software does
it's a significant fraction of its
computational effort is it checks the
digital signatures on transactions
because these use public/private keys
they don't use they don't they don't use
shared keys because no one's going to
share the key to controls their money
with anyone else
that's a lot of work checking those
those digital signatures but it
paralyzes beautifully once you have the
key the transaction and the signature
you can check whether the signature is
valid once and for all you don't need
any other data itself contains that
paralyzes beautifully but some tasks
don't paralyze at all so when you're
executing the transactions for real you
have to execute them one at a time
because you can't have two transactions
that conflict with each other and it is
all important you need super high
performance both for the things that
paralyze and the things that don't some
languages are really good at one some
languages are really good at the other
there aren't that many that give you a
lot of flexibility to do both and
blockchains do not scale horizontally we
can't make Bitcoin handle more
transactions by adding more servers that
doesn't work that way every server has
to see every transaction to know the
state of the network and so we're
limited by essentially the slowest
machine we're willing to support we do
not have horizontal scaling that
blocking it so performance is super
super important naive C++ code compared
to naive JavaScript code let's say to
solve the same problems about a factor
of five and performance so you really
you really need to be able to control
what you're making the system do and we
also need isolation because transactions
have to be deterministic some designs
fail catastrophically if you imagine
that I send out the same transaction to
two different machines and due to some
software bug it produces different
results on those two machines then we
won't have eventual agreement we'll have
disagreement and that would be very
catastrophic it's very easy to get
non-deterministic behavior by accident
you'd be surprised how easy it is to
accidentally write code that behaves in
some kind of non-deterministic way
unfortunately it is fairly easy to do
especially in smart contracts smart
contracts is essentially code that
manages intelligently the movement of
assets it's very easy to accidentally
make a smart contract that's behavior is
non-deterministic and if the system
crashes or fails or something horrible
happens when you have non-deterministic
that's a problem the way you solve that
is isolation you don't give the code
access to anything that's
non-deterministic so it cannot behave
non-deterministically so finally the C++
content so how do you meet these
challenges with C++ well that's what
we've been doing we've been writing
botching software for many years in C++
the Bitcoin project's primary
implementation is in C++ C++ seems to be
probably the primary language of choice
here so what does C++ do and a lot of
this stuff almost everything except one
thing is specific to C++ it's not in C
so if some of you guys are trying to
persuade people to switch from C to C
plus bus that you know they need to get
with the times some of these some of
these points may help because these are
things that are except for one of these
are things that are difficult to get in
C so one of them is move semantics move
semantics allow types to have value
semantics so by value semantics I mean a
type that behaves like an integer you
can pass it by reference you can pass it
by Const reference you can pass it by
value you can stash it in a lambda you
can store it in a container you do
whatever you want with it and you don't
have to worry about some weird
requirement that's imposed on it where
it doesn't really behave like a value so
move semantics mean that you only get
copies when you need them and one of the
big problems with value semantics is
that you get copies everywhere you just
wind up with code that has lots and lots
and lots of copies move semantics
eliminate a significant fraction of
those copies with minimal effort
sometimes none usually very very little
effort a huge huge performance boost C++
has lambdas which enable visitor
patterns and allow you to preserve
layering so what I mean by that is
imagine you have a container and you
want to perform some operation on that
container but it's complicated so you
need to acquire a lock on the container
perform the operation and then release
the lock well you don't want the
container to export its lock that's a
layering problem we want the container
to manage its own locking so with a
lambda we can package up the logic
that's going to apply on the container
and hand the logic to the container the
container can acquire the lock process
the logic and release the lock that
allows the clock to be managed by the
container lambdas also allow work to be
deferred in dispatch so we can package
up some work that has to go into a
lambda if that work needs to be done
later we can pick it up when we're ready
I don't know if many of you are familiar
with boost Co routines and I understand
the standard library there's some work
on ko routines for C++ 17 tremendously
valuable to be able to do some work and
then defer it until you have some other
information that you need and then
resume the work very very clean and
elegant you don't wind up having to
package all the information into an
object or doing other ugly things with
your state big one for us is compiled
time polymorphism this this has been
massive mostly because polymorphic code
fully optimizes so if you use runtime
polymorphism with virtual functions it's
very difficult the optimizer to optimize
through because the compile time it has
no idea what function is going to be
called compiled time polymorphism even
in lines and it allows you to respond
separate responsibilities cleanly so you
can put some of the code in the
polymorphic class and you can put some
of the code in the code that calls it
the responsibilities separate nicely and
you still get optimization all the way
across it tremendously valuable c++ has
type composition which means you write
code once and you get a nice API boost
optional is a good example gonna have
standard optional will see standard
shared pointer and weak pointer same
thing so I can take any class and I can
wrap it in boost optional or shared
point or a weak pointer and I know what
I'm getting I know I'm getting an
optional class I'm getting a class that
has strong and weak pointers I get a
very nice API and I don't have to tweak
it for the class or do anything fancy
and it's all optimizes through we have
code isolation features we have
namespaces we can import one namespace
into another which we don't have name
conflicts helps with large-scale
development and the other thing that has
also proven to be very valuable is that
a class can be a boundary between two
API so a class can have an API for its
use so somebody the public functions and
that can it can have an API for
derivation those would be its protected
functions so a class can be an API
boundary providing to clean API is one
on each side those can be documented
maintained have different
implementations different clients
extremely useful we have a lot of mature
tools we have at least three solid
compilers we have ICC we have GCC we
have clang we have tools for performance
analysis we have tools for finding
concurrency violations you name it we
have nice graphical ID es we have
libraries for just about everything it's
not a new language it's extremely mature
the tool chain is mature except
sometimes I think we've all had this
experience where you have an extra pound
sign or an extra semicolon and a header
file this was a 3k 32k error mess
which none of this code is the code that
had the error in it this is just a
randomly selected excerpt as some boost
code in here I don't know why any of you
guys can do anything about that that is
a a minor irritation and source of
amusement but I guess the source of
amusement is a good thing we all get
bored sometimes um this is also a si
feature this is the one C++ feature
that's not just C++ we have hand
optimized primitives so very little code
is worth hand optimizing you're not
going to write a major project in
assembly language you'd have to update
it for new CPUs and but there is some
code that benefits tremendously from
hand optimization digital signatures are
a good example digital signatures are
worth hand optimizing and one of the
amazing things about both C and C++ is
these calls are really cheap
they can even inline so you can have a
piece of hand optimized assembler that
will inline into your C++ code and the
optimizer will optimize across the
inlined assembly code that that's pretty
amazing that's pretty amazing stuff like
in other languages typically you have a
function that can be hand optimized but
then when you call in to the function
you might have to push stuff on the
stack when you return from the function
you have to pop stuff off the stack and
it becomes like a hard optimization
boundary everything in the registers
gets pushed to memory you don't have
that in C and C++ and we can leverage
work across projects so if someone makes
the digital signature implementation we
can use it in any number of projects we
have lots of mature libraries I did have
to include one bad thing which is the
slicing problem I think that's probably
the biggest problem in C++ right now at
least from the development that I've
done we like value semantics we like
classes that behave like integers and
polymorphism and value semantics mix
badly so if you have a reference to
something or if you have a pointer to it
you can't easily copy it you can't
necessarily stash it somewhere you can't
manage its lifetime that's a problem we
have a lot of solutions that sometimes
are at the right solution you can take a
raw pointer and at least then you can
pass it around you can have a unique
pointer at least then the lifetime is
managed for you but what if you need to
stash it in a collection you can use a
shared pointer
protip you can create a standard shared
pointer to T Const where the Const is
inside the parameter to the shared
pointer that way you're giving the
callers a Const shared but it's not the
shared point of itself that's constants
the T that's
which means they can't change its value
one of the problems with shared pointers
is if you pass them all around and
someone modifies the object at the point
of points - it modifies every instance
of the object you want to catch a
mistake like that at compile time make a
shared pointer to a T Const can't
manipulate the object you'll catch those
errors at compile time so the clone
idiom you can have a virtual function
that duplicates a class it's not a great
solution it doesn't wrap into standard
containers nicely we have boost pointer
vector but there are often situations
where one of these solutions is the
solution not a big deal if you can use
it compile time polymorphism is the best
solution for all the reasons that I
discussed before compiled time
polymorphism because of the optimization
to catch bugs at compile time it's
really nice a standard variant if it
gets into c++ 17 I think there were some
talks about standard variant standard
variant is a discriminated union so it
knows what type it is and it can say if
I'm holding this type we call this
function if I'm holding this other type
we call this other function you can get
compiled you can get polymorphic
behavior in a type that knows how to
copy itself without having to use a
clone idiom so let me give you some very
specific examples of wait that's lies
not sorry yeah much better sorry about
that
let me give you some specific examples
of things that we've done in C++ ways
that we've built up software capital
ways that we've eliminated technical
debt ways that we've used modern C++
idioms to produce high-quality
blockchain code so one of them is in our
caching we'll use strong and weak
pointers the cache holds strong and weak
pointers and this is how look someone's
on skype I meant to put it in airplane
mode and forgotten even know who Pedro
Moreno Sanchez is he's available of
anyway anyway so we use strong and weak
pointers in the cache the cache will
either hold a strong pointer to an
object or a weak pointer the strong
pointer keeps the object alive and the
weak pointer doesn't when you access an
object we promote the weak pointer to a
strong pointer so that the cache keeps
the object alive over time rather than
expiring the object from the cache we
throw away the strong pointer and keep
the weak pointer and what that means is
that if an object is in use if someone
has a strong pointer to it they pin it
in the cache and this has had tremendous
benefits like I said in the slide good
things happen for free so for example
suppose you have an a ledger or
some object that you're manipulating a
lot if you just share a strong pointer
to it you pinned it in the cache and all
your other code can find it so if you
have objects that are valuable that you
need to have around you have some class
that has a strong pointer to them you're
also making those objects available to
all of your other codes so frequently
used objects get pinned in the cache by
use worked out very well for us it also
resists algorithmic complexity attacks
an algorithmic complexity attack is when
an attacker who knows your algorithm
carefully craps his data to make your
algorithm give its worst-case
performance so if you imagine a sorting
algorithm that maybe it has its worst
case behavior if the data's already
sorted an attacker can give you data
that's already sorted you get the worst
case behavior you can't keep your code
secret so you have to protect yourself
against an attacker who carefully crafts
his data to make your code behave in its
worst case and the way we do it is we
index with salted hashes so a salted
hash we hash it right we use a random
salt the attacker doesn't know the salt
and so he cannot attack the hashing
algorithm new DB is it - nope all of our
code is open source all this is widely
available we've been promoting new DB
it's something we built to solve a
problem we have it's a key value store
stores fixed length keys and variable
length data and you retrieve only by key
we typically use it in a block chain
with the key being the hash of the data
so that you can follow hash chains very
efficiently
we have transactions we have hash trees
we have legislate entries all kinds of
things that we need to find by a key so
you probably know that there's a lot of
key value stores out there you've
probably heard of things like leveldb
and rocks DB and all kinds of other
things that are key value stores the
problem with all of them is that the
memory demand scales with the data size
or the performance drops as the data
size increases or both and that's
because they rely on caching for
performance so for example rocks DB is a
phenomenal key value store but you
either have to use bloom filters or you
don't if you use bloom filters the size
of the bloom filters is o of n double
the data yet twice as many bloom filters
takes twice as much space in memory if
you don't use bloom filters then the
performance is o of log n so as your
data gets larger at the performance
drops what we want is we wanted a key
value store that's memory usage was o of
1 and whose performance would level off
so once you exceed a certain data set
size we're caching doesn't work any more
rather than the performance continuing
to drop
memory usage increasing the performance
levels off and it levels off at a very
high level so when we designed it we
assume that caching was useless we just
said let's assume that we can't catch
our data is so large our memory is so
small that we can't cache at all we want
our performance to level off as the data
size increases we don't want it to keep
dropping unfortunately we can't get it
to keep increasing that'd be awesome but
I don't know any way to do that so the
performance will level off the
performance will ultimately be o of 1
and therefore there's no penalty for
massive databases and the memory use
only scales with the write rate we have
to buffer the writes in memory to
journal them so if you have a very high
write rate you have you have some memory
consumption but the memory consumption
is independent of the data set size so
we wondered what was the best that you
could do if we could make the best
possible key value store that had these
requirements how good could we get what
was what did we think was theoretically
the absolute best we can do and so we
figured if you're looking for data
that's not present maybe you could
magically know we're on disk the
information that it wasn't present was
you couldn't have it in memory because
we're assuming caching is useless you
have to go to the disk but the best we
thought what you can do is one i/o you
know where to look
you read that chunk and it says up the
data is not in the database so what
about data present what did we think is
the best you can do we thought to iOS
one i/o to go check to see if the data
is there and that would tell you where
the data was and then you'd have to one
more i/o to get the data so if data is
not present one i/o if data is present
to iOS and the performance limit would
be your SSD I ops so if you have an SSD
that has a hundred thousand iOS per
second you could retrieve 50 thousand
objects per second or you can determine
that a hundred thousand objects per
second we're not present new DB is very
very close to that like within a couple
of percent new DB is very very close to
what we thought was the optimum that you
could possibly do the data is append
only we have a data file that holds all
the data it also holds the keys so you
can recover the index in the data in
case it gets corrupt it uses either two
or three files so you have the data file
and an index file and then during an
update rights or journaled so you have
three files only during an update the
index consists of hash buckets and the
bucket count is dynamically increased so
when we have when we have more data and
our buckets are starting
get full and we hit a configured limit
we add another bucket we split a bucket
in half to add another bucket and we
keep the average number of objects per
bucket constant rights don't block reads
reads don't block reads reason don't
block writes nothing blocks anything
else so ultimately it is the SSDI opps
limit that is your limiting factor so
what does that have to do with C++
well it's header only which gives you
all of the advantages I talked about
before about optimization it uses a
templated visitor and compile time of
sorts with asserts which I'll show you
so this is a templated visitor it passes
the visitor function into the visit the
function that's called on each object
into the visiting function so that way
you don't have to export an API that
keeps track of where in the file you are
you just push the function that does the
visiting in it also templates on a codec
if you have any particular compression
algorithms if your data has in a use
case specific compression that's
possible for example if it has a
particular header you can optimize that
header out with a codec we also use the
static assert something very nice and
C++ is that you can catch lots of errors
at compile time if you're clever this
actually caught on someone went to and
compile new DB on a platform where the
size of a size type wasn't big enough to
hold the size of their hash we assume
that it is this static assert caught
that at compile time which is very nice
saves you a lot of time beast how many
of you whoops
so beast how many of you have gotten one
of these from Vinnie Falco anyone no one
Vinnie Falcon would like every single
one of you to have those beasts is a
library that we developed for HTTP and
WebSockets Vinnie's been pushing to try
to get it included in to boost maybe
we'll see maybe a little optimistic its
header only and it provides a boost like
API so it's an extension of boost for
HTTP and WebSockets that provides both
asynchronous and synchronous api's you
probably can't see this code but take my
word for it it's really clean elegant
simple code it actually does something
useful one side with on the left with
WebSockets on the right with HTTP it's
very very boost like another example is
polymorphic currency types and ripple if
we had it to do over again we probably
would use standard variants for this we
didn't at the time
ripple has both the native currency and
arbitrary assets some objects can hold
either type of currency some objects can
only hold one kind of currency we have a
mix of code some of which deals with
multiple assets some of which doesn't
virtual functions were not a good fit
slicing is one of the issue optimization
is another another one is that you have
all of these annoying runtime checks
like what if I got an object of the
wrong type and if you put a check for
that into your code you have to sort of
do something if you have an object of
the wrong type and since it's one of
those can't happen situations you put in
an assert but then you can't test it you
can't unit test code that's never
supposed to happen it's very ugly until
inelegant it's a way to build up
technical debt instead you can use
runtime polymorphism it knows the type
don't have that problem
templates give you concepts I think most
of you know what I mean by concept but I
mean essentially a set of rules that a
class has to follow in order for a
template to work and they can't slice
and they keep the common code simple and
easy to understand so some of the code
goes in the class that you're templating
on some of it goes in the template code
itself and it allows you to avoid having
to have all of these if it's this type
of s I do this but if it's this type of
s I do this other thing which gets to be
very very ugly and difficult to maintain
this is an example of one of the
advantages of using that type of
templating compile-time polymorphism so
this is a class that handles an offer of
someone willing to trade one asset for
another and you can see it's templated
on the type of assets and what's nice
about that is you see that boost
optional T out at the bottom you get
value semantics so that has value
semantics we can create a new object of
type whatever the output type is we can
wrap it in a boost optional very
difficult to do that with virtual
functions very difficult to do that with
inheritance but you can do that very
cleanly with compile time polymorphism
tremendously simplifies the code makes
it easier to maintain that's it
hopefully you all are now blockchain
experts
lots of time for questions if you want
if anyone wants to come to one of the
two microphones excuse me you said that
there was attack on worst-case behavior
but is there any sort of attack that can
happen on the best case behavior for
example let's say that your worst case
of behavior was n log n but your best
case behavior was limit linear time
could somebody create an attack on that
case where they're very giving you the
best T data that you that go through
your algorithm and just get the best
time yes but you would hope that that
would be an easier problem to solve
otherwise you pick the wrong algorithm
so if someone attacks your best case
behavior all of your normal congestion
control mechanisms would solve that so
for example if you have a public
blockchain like ripple or Bitcoin you
pay a transaction fee and those fees are
normally very low but if the network
gets congested the fee goes up so if you
want to keep the network congested you
have to keep sending transactions that
that saturate the network and you have
to pay a fee for every transaction
that's greater than I'm willing to pay
for my one legitimate transaction so as
long as you don't have an advantage over
me then I can compete on a level playing
field because I only have to get one or
two transactions free and you have to
keep the network saturated forever like
if you want to stop me from performing a
transaction you have to keep the network
saturated if you're willing to pay more
than me for a period of time yes you can
lock me out but what we really don't
want what we can't handle with the
conventional methods is someone who has
a computational advantage like if you
could submit one transaction and it cost
a hundred times as much as my
transactions now you can jam the network
at a low cost and I can't out-compete
you so so we designed the system to
tolerate the best-case behavior and then
we design it so that an attacker can't
force the worst-case behavior good um I
have a question we probably have seen
quantum computers are your algorithms
ready for that in Ripper for what
computers sorry quantum computers it's a
good question so we have to keep our eye
open we do have ways to evolve the
algorithm so we can swap out the
algorithms if we get to that point
we keep a very close eye on that
technology as everybody who does
encryption and digital signatures have
to I hope we still have about eight more
years before we really have to get
aggressive about it I could be wrong
about that but yeah we do we would you
follow the technology as everybody has
to everybody in cryptography everybody
you know who deals with the electronic
security these systems are dependent on
cryptography the dependent on secure
hashes they're dependent on digital
signatures and we have to make sure that
the algorithms that they use are
resistant against attackers who are
willing to potentially spend millions of
dollars to break the system so yeah we
will have to evolve the algorithms
definitely a lot of people will say well
why don't you change the algorithms now
like why don't you like ahead of time
it's kind of like turning the steering
wheel on your car when the curves still
50 miles ahead like you might do the
wrong thing it's hard to know like what
a quantum computer is going to look like
what algorithms are they going to break
and how badly what are gonna be the best
algorithms to resist them if we switch
too early we might find that we just
have to switch again so there's a matter
of finding the right the right time and
I think we still have a couple of years
but yet we have to be ready we
absolutely have to ready be ready we
have to follow the literature and we
also have to worry because we don't know
what an attacker might have that is not
in the literature so I'm sure the NSA is
a couple of years ahead of the of what
we know about and I'm sure foreign
governments might potentially interfere
with these systems as an amount of money
they hold goes into the billions of
dollars so yeah we have a huge bug
bounty we have a huge security bounty of
potentially millions of billions of
dollars so yes very concerned very
concerned about that on this side so for
a ledger system with large number of
participants what is a typical quorum
number how many you have to get replies
how many T's to get replies from if we
decide that it's an agreement and
consensus and you can move forward so it
depends on the type of system that
you're using in in Bitcoin it doesn't
use a quorum it uses proof of work so
it's just sufficient essentially
sufficient hashing power by anyone
ripple uses 80% of the active
participants but it identifies active
participants essentially by known
identities that someone has to convince
you to count them because if we just
counted anyone who is present then
attacker could produce thousands and
thousands of identities private
blockchains
typically they know all the participants
because they have some sort of an
organization that said like these ten
organs
patience or the participants and so they
typically will use a quorum of 3 n plus
1 where n is the number so if this 3 n
plus 1 participants n of them can be
missing and you can make forward
progress otherwise you don't make
forward progress some are a little more
aggressive than that where basically
they don't count a participant that
hasn't been active for a certain period
of time and they decide that that's an
acceptable risk
it's very use case specific though and
there isn't a perfect solution yet we
definitely do need more more work there
and in fact if you if anyone here is
looking for a computer science masters
with PhD I can give you the rundown of
what we really need research to figure
out exactly how we need to do that that
that would be that would be extremely
valuable but to some extent we're
slightly winging it now I mean we run
simulations and we stay very very
conservative so that we know we're safe
but it would be really nice to know
number one exactly where is the danger
zone like what where is the thing that
we have to stay away from and number two
what happens as we get close to it so
one of the things is we tested in
simulation where we took a network that
was very healthy and we took a network
that was unhealthy and we gradually
moved one to the other to say what would
happen as the network starts to break
and what's interesting is that it
doesn't like all of a sudden just blow
up and there's double spins and all
these problems what happens is the
network becomes unstable and slow so you
get non agreement you get extra rounds
and you get this sort of soft failure
that we hope would incentivize people to
say hey there's something wrong we
better fix it so that's good but we
really do want to know like what are the
exact theoretical constraints to be sure
that the system is reliable how do we
know where we are relative to those
constraints and that's definitely an
ongoing research area ok so that's why
you can use gateways right so that you
can reduce number of active participants
so that 80% will not be well and the
other advantage is if you use a system
like ripple where the system is tracking
a virtual asset if the system blows up
notionally you can constrain the problem
because the the Gateway still owes you
the money so if you give me $100 and I
give you an electronic balance on one of
these systems and the system blows up or
fails in some way I still owe you that
money right the system was just
recording the fact that that obligation
existed but for Bitcoin you can't do
that for Bitcoin if you have a Bitcoin
all you have is a number in the computer
and if something happens to that number
you're just out the money so a lot of
we'll say that like counterparties are
bad because the counterparty can fail
but counterparties have the advantage
that if the system fails in the
counterparty doesn't you still get your
money you so it's it's a trade off
so you mentioned how blockchain software
it needs to be like a fortress and I'm
curious do you deploy your software with
any sanitizers enabled or things like
that um we typically don't run too much
of that stuff on live like production
machines but one of the nice things it
would you have to you have two sets of
data flows so you have the data flow
that's the transactions and the Ledger's
and everybody has to follow that to
monitor the network and then you also
have like local queries people who want
to know information about what's going
on on the network so one of the nice
things is you can completely separate
those so the servers that are tracking
the sort of canonical reliable
information you can keep completely
private you know to give people access
to them and then the servers that are
sort of widely accessible to the world
and people can query they can't harm the
network they can only harm themselves we
do use we use all kinds of sanitizers we
use Val grind we use what is it called
tea Sanh address sanitizer we use a
variety of tools to analyze live data
the other nice thing is it's really only
failing on the transactions in the
ledger manipulation that's a problem and
we can replay that stuff from historical
data so we can replay everything that
happened over the past day on a machine
and we can watch that happening sort of
in simulated time rather than real time
so we don't have to we can test new code
on old data but it's but the problem is
still if someone figures out an attack
vector and they launch it live it won't
help that we figured out the attacker
that an address sanitizer has caught it
or some sort of pretty sick code has
caught it if it's already applied to the
ledger and we can't like manually screen
each transactions before applying it
because of the response time so that
does have some value and it's definitely
good for back testing but it's not as
useful for protecting live systems hi I
was wondering how sensitive your code
base is to compiler vendor optimization
flags versioning and how you mitigate
that so we we compile our code regularly
on GCC clang
in Visual Studio not too often on ICC we
get pretty good cogeneration on all of
them sometimes one will find a bug that
another one doesn't which is also nice I
has not been that sensitive to too much
optimizations do make a huge difference
any type of memory checking will just
put the performance in the toilet which
probably is an indication that our code
is still doing more copying than it
needs to but we've got really good
performance out of all of the major
compiler vendors and then we and I think
it's also very advantageous to compile
your code on multiple platforms and
multiple compilers because compilers
sometimes won't report warnings and
another one will compilers some
compilers are tolerant of code that you
really shouldn't be doing whereas
another one may not be building on a
variety of compilers is definitely
definitely helping but we haven't found
it to be particularly sensitive just yet
just like compiling a debug mode
particular with memory debugging is
horrible but everything else is pretty
good
lots of good application stuff lots of
good high-level programming stuff so you
must excuse me for asking a fairly
low-level question I had to back off and
attempt to ban slicing would you
encourage me to fight that fight again
it's painful so so yeah I mean it is it
it's it I think it is the biggest
problem with C++ today like in fact if I
could push my magic button and make one
problem go away
it would be slicing I don't know what
the solution is um I would love to see
people yourself like coming up with some
substandard variant I think is going to
be one one possible tool and we've used
all different tools on different things
but yeah we need some we need we would
benefit a lot I think I tried to press
that button but there's a lot of old
code out there my conjecture is that
essentially all bugs III I would have a
hard time disagreeing with Tweetie with
you on that yeah I will agree ACK to
some extent I'm sounding the alarm I
think that that's it's very surprising
behavior for every new C++ programmer
and it rears its ugly head sometimes
late and for people who are not you know
decades of C++ experience sometimes
you'll almost finish a piece of code
that you spent a long time on and
suddenly discover that slicing makes it
like means you do use the wrong design
from the very beginning and you're like
what crap I just spent you know three
days working on something and it's just
not going to work because of this C++
issue and so so yeah I'm not gonna it
doesn't ruin the language free by any
stretch of the imagination it is the
very worst thing though I think but
thank you that's good you mentioned that
you're salting your hashes with random
numbers given that it's open-source
software do you have to worry about the
seed that you use on the random number
generator and how are you getting the
entropy or whatever that you need there
I'm so glad you asked that so that's
actually a fairly well solved problem
except when every once in a while a
failure to do it exactly right rears its
ugly head but we know how to solve that
so the computers that we use in the real
world are not Turing machines they have
connections to the real world and they
get randomness from many sources and the
biggest one is on a modern x86 CPU they
have an instruction cycle counter and
the clock that generates that
instruction cycle counter beats against
the network clock and other clocks in
the system and it causes randomness in
the time at which data arrives and we
can mine that randomness every Linux
distribution does that already Windows
has some proprietary solution that we're
told does that I don't know if we
believe it but it seems to system save
randomness on shutdown and they reload
it on startup to have them to start from
an unknown condition and one of the cool
things is as long as you have enough
randomness at some point you can produce
an essentially unlimited supply of
random numbers so if you give me a
hundred and twenty eight random bits
unknown to an attacker I can produce a
gigabyte of unpredictable data or more
and it's at least computationally
unpredictable some people say well they
want true random numbers and they don't
want pseudo-random numbers I kind of
think that's like the difference between
holy water and water you know true
random numbers are like holy water like
some people think that it's a great
thing but you can't there's no test that
can prove the difference between the two
so that's kind of we use pseudo-random
numbers but we mind true randomness from
the system to make sure those numbers
are unpredictable now there
have been some well-known cases where
people have gotten that wrong there was
an abundance that had a bug in the end
right operating system I believe are in
one of the libraries they were using
that caused them not to produce random
keys and and that was a huge problem but
we know how to do it right we just have
to keep remembering to do it right one
of the cool things that makes that
easier though is if you take randomness
from a number of different sources and
you put it into a secure hash function
your output is secure as the strongest
input so you can take input from dozens
of sources even some that you don't
think are particularly strong like you
could get some from a server and maybe
you don't trust that server but but it
doesn't do you any harm because you're
as strong as the strong strongest
strongest source but yes it's very
important to get that right very
important to get that right there been
some spectacular failures all right
thanks one last question if that's okay
all right so you mentioned you use the
Clank Visual Studio in GCC right and you
mentioned that you have some select use
cases when you use inline assembly so my
question is do you have some good advice
to make it relatively portable by a
relatively portable I mean I only care
about x86 and x86 64 but even though I
only care about this I still have to
change the syntax say in GCC I have to
specify the constraints on the registers
on Visual Studio 32-bit I can use inline
assembly on Visual Studio 64-bit I have
to have it in a separate file so I'm
wondering it did you find some good
solution to keep it relatively portable
in your codebase unfortunately no there
I don't know of any magic solution to
that we most of that we use the same
method that OpenSSL uses um there isn't
really a good way to do it
unfortunately obviously inline assembly
is inherently not portable I would love
to see though some better cooperation
between the compiler vendors to
standardize that a little bit
particularly in 64-bit windows code
where it's just completely just I don't
I don't know why everybody seems to
think they have to do it there I guess
the good thing about standards is so
many to choose from as they say but it
does make code maintenance difficult on
the bright side you don't have to change
that code often and it's usually only a
small percentage of your code but it's
going it's going it's got to be painful
yeah it would be nice to have you know
there might be some people from
Microsoft in this room I'm thinking this
probably a couple right maybe some of
them could take that message back thank
you thank you so much for coming
appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>