<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Guy Somberg “Game Audio Programming in C++” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Guy Somberg “Game Audio Programming in C++” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Guy Somberg “Game Audio Programming in C++”</b></h2><h5 class="post__date">2017-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M8Bd7uHH4Yg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">First section of the
morning, so I appreciate it.
First, my name is Guy Somberg.
I am the client lead
programmer at Echtra Games.
A little bit about me, I've
been in games since 2002,
so my video game career
is just now getting
its driving learner's permit.
I've owned the audio engine
at just about every company
I've ever worked at, either
for the whole company
or just for the game I was working on,
and I've shipped lots and lots of games,
and lots and lots of audio engines,
everything from triple
A to small indie titles,
and kind of everything in between.
So today I'm gonna show you how we,
we work with audio in video games.
We'll start with some audio fundamentals,
give you an as if model
of what's happening
in an audio device.
This is not, this is kind
of close enough to the truth
to be, for the purposes of this talk,
and I'm gonna talk about what the current
state of the art is in
game audio programming
and move toward a standard
C++ audio library,
what that might look like.
Now the fundamentals are
kind of not necessarily
the things that we're
doing day in, day out.
Some of us are, but many of us are not,
but they are nevertheless things
that we're expected to have
background knowledge as
a game audio programmer.
Let's jump right into that.
Here's a speaker.
We have an electrical wire that
we send electrical signals down.
There's a coil that makes
a magnetic field that
moves a magnet that is attached to a cone.
The cone vibrates, moving
a, pushing the air.
The air travels.
The vibrations move
through the air to your ear
and that's how you hear
what's coming out the speaker.
Now if we graph the position
of that speaker cone over time
you get a curve, and if we
want to replicate the same
sound we just have to replicate
the curve out the speaker.
The problem with this curve
is that it's an analog signal.
We have to move the signal
a certain amount over time
but a computer is very much
a digital discrete entity.
So in order to, in order to work with
digital signals or to translate
this to a digital signal
we sample it, and when
you sample audio data
it's called pulse code modulation, or PCM.
So if you hear me referring
throughout this talk to PCM data
or uncompressed audio data that's this.
This is pulse code modulation.
Typical sampling rates
for video games are 48,000
samples per second,
sometimes you'll hear 44,100.
You'll also sometimes see lower numbers,
like 32,000 or there's a
little bit higher numbers
like 96, or even 192,000 in rare cases.
But either way it's going
to be in the mid to high
tens of thousands or low
hundreds of thousands
of samples per second.
So by sending this data
out to the speakers,
we can play one sound
which is not very exciting.
So let's see how we mix
together multiple sounds
and create a whole sound scape.
Let's say we have the
blue curve on the right,
and the kind of like puke
green curve on the left.
We want to play those guys together.
What is that gonna look like?
It turns out it's this orange curve.
We just sum up every
point along the curve,
sum them up together, and play that out,
and that is exactly the correct curve.
Write that out mathematically,
it's just like this.
The output signal is the sum
of the two input signals.
But because the output is itself a signal
we can now trivially convert that to this.
The output is simply the sum
of all the input signals.
So now we have a way
to play a single sound,
multiple sounds together.
How do we actually get it
out to the output device?
And here's an as if audio device.
I'm not an artist, so this is
the best I could do in PowerPoint.
We have on the right
hand side a, oh sorry,
that's the left, I realize.
On the left hand side, yes, a speaker,
and the middle is a digital
to analog converter.
This is your sound device,
your sound card, audio card.
It goes by various names.
And on the right, now I
have to reverse what I see
on my screen, on the right--
(audience member mumbling)
(audience laughing)
Thank you.
(audience laughing)
Not supposed to turn your
back to the audience.
So reverse everything I just said.
On the left, thank you,
on the left is the kind
of operating system's
representation of this.
It takes the form of a ring buffer.
Here I have, I didn't have enough time,
and I didn't have enough patience, really,
to draw it as a ring buffer.
So it's just a double buffer in this case,
which is really a ring buffer of size two,
if you think about it, and
so you're expected to be
writing into the red buffer
while the digital to analog
converter, the sound device,
is reading from the purple one.
So it reads from the purple
while you write to the red.
It switches and reads
from the purple again,
and the switch again.
Yay for my PowerPoint skills.
So, the thing about this
is that this happens
no matter what, whether
you're ready for it or not.
The number zero rule of
audio code, this call back
that is triggering, that
is asking for these buffers
waits for nothing.
And so in 2015 at CPPCon, Timur Doumler,
I hope I said that name
right, did this talk
C++ in the Audio Industry,
where he goes into
great detail about this very subject
and how, how to write code for this very,
very high performance
low latency environment.
And so I'm not gonna cover all of that,
but I do want to at the
very least I'll wait
for him to do his QR code scanning.
(laughing)
I'll very least
summarize that talk in kind of one slide.
So I encourage you to watch this,
but here's the one slide
summary of kind of what
he was saying.
In order to provide this, this call back
with its buffers and have no errors,
we have to guarantee
that, we have to provide
a number of guarantees,
and we have to provide
that the function will
return in some time less than
the length of the buffer
that it's trying to fill.
So if I have a 512 sample
buffer at 48,000 hertz
that gives me 10.6 milliseconds
to fill that buffer.
That's less than the length of
a 60 frames per second frame.
And 512 samples is luxurious.
A lot of times you'll
have 128 or even fewer.
So you have single digit
milliseconds to fill this buffer.
We have to guarantee
that we finish processing
the whole buffer.
We can't exit early and
then come back to it.
Once you finish, it's gone.
We have to guarantee the output
contains all valid audio data.
So typically this will be either
integer, signed integers,
or floating point numbers
in the range minus one to positive one,
and we have to guarantee
that there are no errors
or exceptions.
So to provide these guarantees
we have to do a few things.
First we have to run our
mixer thread at the highest
operating system priority
that is available to us.
We must also never ever
block the mixer thread
for any reason.
So that means you have to
make all your algorithms
either lock free or
ideally even wait free.
So to provide the no block
guarantee we can never
do any memory allocations,
no deallocations,
and no I O of any kind.
So these are some very strict
requirements on the properties
of this thread.
So by outputting, so this
is how we output the actual
mixed audio data out.
Now I've talked about how
we mix it and how we play
the audio data, but I
haven't really talked about
where audio data comes from.
So video games are a high
performance real time application
and so the trade offs of
memory size, CPU cost,
and disk I O are very important to us.
So we have to have a
number of different options
of how to represent, or
how to be able to get
our audio into our engines, excuse me.
The first option is just to
decompress the file in memory.
So we have a file on disk
usually and it's in some sort
of compressed format like
MP3, Vorbis, Flack, AVPCM,
the list goes on and on and on,
and we load that into memory.
We decompress it and
we end up with a buffer
of PCM audio data, and
when we load it this way
it's referred to loading it as a sample.
We can also rather than
decompress it we can store it
in memory as compressed data,
and what this means is that
we have to decompress the data
on the fly in that mixer thread.
So we have a lot of strict
requirements now about
which formats are available
to us because in order
to decompress it we have to
make, we have to provide all
those guarantees, the
hard real time guarantees,
no allocations, no deallocations, no I O.
Not every format can be
decompressed that way.
So we have to limit the number of formats,
and the advantage here though
is that if we are willing
to spend the CPU cost
which in general we are
then we have reduced our memory
requirements drastically.
Third option is if we have a
memory buffer that we've gotten
from some source, maybe we
loaded it from disk ourselves,
maybe we got it off the
network, maybe we got it
from a microphone, maybe we
got it from I don't know,
anywhere, a device of some
sort, we can take that
and copy it into our audio
buffer, into our audio engine,
or alternatively we can
point to it from outside
the audio engine, but
then we have to provide
some lifetime guarantees
about that buffer.
Three B, we could do the same
thing with the compressed buffer.
If the buffer's in Flack or
some other compressed format
we can point to it or copy
it in that compressed format
and treat it just like we did
if we'd loaded it from disk.
Option four, we could
stream the data off of disk.
So what this does is creates
a double buffer of audio data
and we, just like any double
buffer you're consuming one
side while you're writing
into the other side,
and the advantage here is that
now when a file gets large
enough, so it's larger than
the size of that double buffer,
now it can become more worthwhile to,
to have the expense of
the disk I O to exchange
memory for disk I O.
So you often see this with
large files, like music,
long ambiances, stuff like that.
A good rule of thumb for the
typical sizes and lengths
and compression ratios for
video games is that if a sound
is 10 seconds, give or take,
then it might be a good
candidate for streaming.
It's just a rule of thumb
and depends on the platform
and all the other settings.
The last option is a synthesizer.
You can actually fabricate
audio data from whole cloth
using an algorithm and maybe
some small amount of buffer
and we hook that directly
into the mixer thread.
So again the algorithm that's
generating the audio has to be
real time safe, but so long
as you can provide that
guarantee then you can generate
your audio data on the fly.
So if you look at the right
hand side of all those options
there's really only four
different things that you saw,
that we have a buffer of
wave data, either compressed
or uncompressed, or a
nonowning buffer of wave data.
We have a file stream
and we have some code.
That looks like, kind of like this.
Although you can actually use this.
This is a simplification,
but you get the idea.
This is the backing store for audio data.
Or something that you
know looks like this,
in some, in some vague way.
I wanna take a step back at
this point and ask where are we?
All right, so at this point
we can load audio data,
we can play as many
sounds as we want together
and output them to the sound card.
So the question is what now?
What's, what else is there to do?
Now the work begins.
I haven't even talked about
resampling and clipping.
Okay, I don't have time to do that today.
This is, you cannot even
start unless you implement
resampling and clipping
in your audio engine.
Video games are usually
three D constructs,
so we have to have some sort
of three D panning mechanism.
Historically that's been a
vector based amplitude panning
algorithm but these days
we're, with VR, we're moving
into a lot of ambisonics,
and stuff like that.
Three D attenuation, sounds
simple on the surface
but has a lot of subtleties.
Submixes are another important
feature where you can have
some audio data that is mixed
together with other audio data
and then creates a mix point
where you can apply effects
such as reverb or filters, and
then, and volume adjustments
and then send that off to another submix,
and onto another submix.
Effects, that I just
mentioned, low pass filter,
high pass filter, reverb, echo,
all these kinds of things,
there's a ton of work still to do.
Now if you're using middle
ware, some, but not all of this,
will be taken care of by that middle ware.
So there's just at this point
that's kind of the fundamentals.
I wanna switch gears now
and talk a little bit about
the current state of the art in audio.
Every operating system has
some way of communicating
with the sound device.
On Windows we have WASAPI
and ASIO, on iOS and Mac OS
we have CoreAudio, Linux has
PulseAudio, OSS and ALSA,
Android, the default is
OpenSL, just every platform
if you're on a console,
they'll have their own stuff.
Every platform has this.
But these really, really low level APIs.
These are providing you
with an opportunity to fill
that buffer yourself
and do your own mixing,
and so they're very important APIs to have
but video games, we don't
wanna live in that space.
We wanna live a much, typically.
So in video games middle ware is king.
FMOD Studio and Audiokinetic Wwise are
the big middle wares in
the Americas and Europe,
that kind of zone, and CRI
ADX2 is the big middle ware
kind of in China, Japan, Asia, that zone.
If you're using Unreal
Engine or Unity Engine
they'll have their own audio APIS,
and the nice thing about these
middle wares is not just that
they're providing a wrapper
for the different operating
systems and their low level
API, it's not just that they're
providing file I O and sample
decompression and streaming
and all these other wonderful things,
'cause they do provide those things.
The nice thing about these
middle wares is they're actually
providing tools to the sound
designers to allow them
to express themselves in
extraordinarily powerful ways.
This is FMOD Studio and
what you're looking at here
is a music event and this
music event has a low,
medium, and high intensity
section to the track.
This is a single wave file
that is being streamed in
with these different intensities.
It has different loop
sections for those intensities
and there's an intensity
parameter that automatically
as you move the parameter
it automatically transitions
on the next beat to the desired
intensity with a musical
stinger to hide any transition effects.
And all of this, this
all, all this complexity,
is exposed to the
programmer API for a single
play event call and an
exposed set parameter call.
All of that.
This is AudioKinetic Wwise.
What we're looking at here is
an event for a glass bottle
impacting a surface.
This event switches which
sound it's playing based on
the hardness of the target
surface, and then applies
a low pass filter, a pitch
adjustment, and a volume
adjustment based on the force
of the impact, and again,
all of this is exposed to the
gain through a single play
event call and a couple of
exposed parameters to tell it
which, what the hardness of the surface is
and the intensity of the impact.
This is CRI ADX2.
I am less familiar with this
tool, but I'm pretty sure
what we're looking at here
is a music track, again,
with parameters to adjust
volume, pitch, low pass filter,
stuff like that.
So given the fact that
we have these tools,
these extraordinarily
powerful and expressive tools
for the sound designers,
what is that we as game
audio programmers do all day?
There's, you know, are
we just hooking stuff up?
What are we doing?
So to answer that question we have to ask
what kind of audio programmer are you?
There's lots of different kinds.
Now this is actually a
relatively recent development
in game audio programming
that there was more than one
kind of game audio programmer.
These days there's a technical
sound designed who is
a sound designer.
This person is a content creator
but they have the ability
and the technical know how
to go in and create hooks
and implement features where they need to.
We have the audio engine
programmer who is kind of living
in this space between the
audio middle ware and the game
and creating the, and maintaining
the audio engine logic
and the tools surrounding that logic,
interacting with the middle ware code,
the game code, and working
closely with the sound designers
to make sure that the features
that they need are available
in the engine.
At a lower level we have what
we might call a DSP programmer
who is implementing some
custom effects, maybe some
custom mixing techniques
like frequency space mixing,
that kind of stuff, and this is just
the things I came up with
off the top of my head
that fit on one slide.
There's lots of other ways
to be an audio programmer.
So my perspective is this.
I am the audio engine programmer.
I have lived my entire
career in that space
where I'm providing services
to the sound designers
and creating that audio engine logic.
So the, the kinds of things
I'm doing are colored
by that perspective.
So here's the kinds of things we do.
We set up game hooks
for the sound designers,
just to have, like I
had, they have a sound,
they need to have some place
to put it into the game,
some hook to attach it to.
Number one bug that every audio
programmer ever has to deal
with is why are sounds not playing?
I actually fixed a bug, a why
is my sound not playing bug
last week, true story.
We have to understand the tools
that our sound designers are
using in depth because sometimes
they'll want to express
something very complicated
and we, they can't do it
themselves, so we have to be
able to go in and show them
how to use the tools to do that.
Sometimes we'll go and take
the role of a DSP programmer,
implement some custom effects,
maintaining the audio engine
logic as, you know, writing
and maintaining that,
and ultimately we're working closely
with the sound designers
to unlock their creativity.
And we do this using C++.
Here is some code from
my current audio engine,
and the, so this is code where we have
a playing event handle which is a wrapper
for a playing handle, or playing event,
and the idea is that the
playing event can get
garbage collected effectively,
and it's not actually
garbage collected, but
it can get removed out
from underneath, and so
the API surface is the same
between the handle and the
underlying object and we have
to guarantee that if the
object is valid that we forward
the function call,
otherwise it becomes no op.
And so this is the function that you see.
We're getting the playing
event, checking to see
if it's null, and then
forwarding on the function.
On this one slide I realized, I counted.
I have seven modern C++ features.
We've got variadic templates.
We have a using declaration.
We have stood result of.
We have forwarding references,
auto and null putter,
and stood forward.
Also this slide using smart pointers,
but they're not standard smart pointers.
So I can't really claim them
as a modern C++ feature.
Also in writing this slide I
discovered that stood result of
is actually deprecated in C++ 17,
which I did not know until
as I said making this slide.
It's in step deprecated in
favor of stood invoke result,
which has a little bit cleaner syntax,
and a little bit more power.
In my current code base I
only have access to C++ 14,
so once we can upgrade,
I'll upgrade the code.
Here's another example.
This one's straight out of Unreal Engine.
This is enqueuing this lambda to be called
in the mixer thread from
outside the mixer thread,
and this is a much simpler
example, but as you can see
we're kind of embracing C++
and using its functionality
and features.
So at this point, I wanna boot strap.
I wanna show you what a game
audio engine looks like.
I'm going to be using the
FMOD Studio low level API
and I'm going to start with a
minimal sound playback example
and I'm actually going
to be doing live code.
We'll see how well this works.
Here we go.
Visual Studio, with nothing in it.
The only thing I've done
here is set up the project
to link against FMOD Studio low level API
and set up the include direct,
or the directories for everything.
So here we have to include
FMOD.hpp in order to get started
and the first thing we need to
do is be able to communicate
with the audio device, which we do
by having a system object.
We allocate memory for the
system object by calling
system create,
and then we still have to
call system init for which
we'll pass in some default parameters.
Null putter, okay.
Now we have to wanna
load a file off of disk.
We do that by loading it
into an FMOD sound object
and we load that sound object
by calling system create sound.
We'll go ahead and pass it
the name of a file on disk.
If I could type, and it goes paren quote.
Pass it some
FMOD defaults,
and some other parameters
and we'll have it fill in
that sound object.
So there we go.
Now we've loaded the sound onto disk.
We play it then into a channel object.
Channel is holding the
data that we are loading
and we do that by calling, or
the data that we're playing.
This is actually the
currently playing sound.
We do that by calling
play sound, surprisingly.
We give it the sound object,
some default parameters,
and filling in the channel object.
The only last thing we have to do
is just wait for the
sound to finish loading,
or to finish playing, rather.
So we could say channel is playing
and although technically we don't need to
it is more correct to
update the system object.
So there we go.
What is that?
25 lines of code, and we'll update it,
or we'll compile this.
(upbeat chimes chiming)
(audience laughing)
So, yes?
- [Audience Member] What
if you need to update
the system object?
The system object, so in
this, in this example,
I didn't need to.
The system object updates
gives you call backs.
It gives you, it does some
processing for I think
for some three D positions and
if you read the documentation
on FMOD they actually describe
what the system update does.
So I tend to just do it because
it's good kind of practice
when using FMOD, but like
I said, in this example,
it would work just as
well without updating
the system object.
There's also, there's also
modes in FMOD that are
non real time modes, where
the mixing happens inside
of updates.
So if you wanna do offline processing
and stuff like that.
All right, let's see if I can
get this back to a working state,
and then I have to do that,
and, oops, that.
Yes, all right.
So, let's build an audio
engine, and I'm gonna go
about 99% light speed
because the purpose of this
is not to go through it
line by line but to show you
the shape of it, what it looks like.
Now of course I have no idea
how far through my talk I am
but that's all right.
That's what you're here for.
So here's the interface
for the audio engine.
We've got an init, update,
and shut down functions
for kind of static
initialization and update.
We have functions to
load and unload sounds,
set three D listener position,
some jukebox functions,
play, stop, pause, that kind of thing,
and some functions to
adjust channel parameters.
We're gonna use the pimpletium here.
Here's our implementation
and the backing store
is a map of sounds and channels.
Update function is more
interesting than the constructor
and the destructor.
We have, first we check to
see if the sound is finished
playing, add it to a vector of iterators.
We erase from the channel map
any channels that are finished
and then to update the system object.
Load sound and unload
sound are matched pairs.
First we check to see if
the sound is already loaded.
Then we either load or unload the sound,
and then we add or remove
it from our backing store.
Play sound, a little bit more
complicated, but not too bad.
We've got our, first we check
to see if the sound was loaded.
If not, we load the sound.
Then we, if we fail to load
because the file didn't exist
or there was some error in
loading it, it was in a bad
format or some corruption,
then we exit early.
Otherwise, we play the sound
and set up all of its parameters,
and I want to point this
particular chunk of code out
because we're gonna see it again,
and note just we're playing
sound, setting parameters.
That's all, just calling that section out.
The rest of the functions
all follow this pattern.
We check to see if the channel exists
and then we set a parameter on it.
They're all boring.
They all follow the same pattern.
All in all it's about 250 lines of code
and you've got an audio
engine that features
sound playback in three D, volume control,
and some simple jukebox functions,
plus whatever other kind of
parameters you wanna set.
But the problem with this
250 lines is that it's really
hard to add new features.
So to add new features we kind of need
to organize it into a state machine,
and I'm gonna show you one state machine
that has worked very, very well for me
across many, many games.
I've shipped lots of
games on the state machine
I'm about to show you, and
to build that state machine
I'm going to show you
three exemplar features,
fade outs, async loads,
and virtual sounds.
There's an asterisk next to virtual sounds
because unfortunately today
I don't have enough time
to talk about what virtual sounds are.
I'm including them here for completeness,
but unfortunately I'm just
gonna have to gloss over
their implementation.
So I'll cover them just very briefly.
If you think back at
that code that you saw
the audio engine,
or the state of a sound,
any given sound, is that
it's either it's playing
or it doesn't exist,
which makes this state machine for
that engine look like this.
This is the entire state machine.
So if we wanna fade outs
we're gonna add a state
to track the fade out time
and mark that we're no longer
playing this, and then mark it as stopped,
so that when this fade out is finished,
or when the sound has stopped playing,
we have the stopped state.
And that's it.
By adding these two states
we've implemented fade outs.
To implement async loads, now
we have to do a little bit
more because we have to now there's,
there may be some length
of time while we're waiting
for that sound to load.
So we have an initialized
state where we can do
kind of one time
initialization, a to play state
where we are actually trying
to play the sound if we can
and if we should, and if not, then we'll,
if the sound is not loaded,
we'll move it to the loading state.
When it's finished loading we'll come back
and then continue through
the state machine.
Note that on the left side, yes,
on the left side there's
that arrow straight from
to play to stopping because
now there's some non zero
length of time between when
the sound was requested to play
and when it is actually
playing, especially if the sound
was not loaded from disk,
and at that point we can
already get a stop request for that.
It might even happen on the same frame.
So we go jump straight
to the stopping state.
For the virtualization,
again, I'm gonna kind of gloss
over it.
Virtual sounds are sounds
that are important enough
to keep track of their
existence, but they're just not
important enough to actually
hear, and if you wanna hear
more about virtual sounds
come talk to me afterwards.
I'll be happy to wave my hands
and enthuse about them forever.
So this is the state machine.
You can ship triple A games,
you can ship tiny games
on this state machine.
So let's go ahead and
build the code for that.
I'm gonna go a little bit
faster, at 99.9% light speed
because again the purpose
is to show you the shape
of the code more than go
through it line by line.
Here's our interface.
It is almost the same as
before except we have this new
section here where we're
registering and unregistering
sounds that can be loaded.
The thing about this is that it's fake.
You wouldn't actually have these functions
that look like this in your code,
because in a real game
you'll have a spreadsheet,
listing all this information
or you'll have a database
of some sort, of some
form, where you'll know
which sounds are available
through some other mechanism.
But if you wanted to use
this you could ship on this.
This is, this is legit code.
It's also boring, so I'm
not gonna bother showing
any boring bits.
The rest of the interface is all the same
except notice here on
the stop channel call
we have a fade time and the update gets
a delta time seconds.
All right, I lied about not
showing any boring bits.
I did wanna show the load
sound call because this FMOD
nonblocking flag is the magic
that makes async loads work
in the FMOD Studio API.
This is our channel object
which is more complex
by virtue of the fact that
it exists in the first place.
In the previous audio engine
we just stored the FMOD
channel object.
Here we have an object
that holds a channel object
among other things.
Note here's our state machine.
The state is per channel.
We have our backing store
containing all the volume
and sound and state and
all these other things,
and some functions to
update and helper functions
for the update.
Play sound is now much simpler.
All we have to do is allocate memory
and the state machine takes care of moving
the sound through the state,
thorough its life cycle
of playing it and all that other stuff.
Stop channel is the same if
there's no fade time but if
there is a fade then we
just it as stop requested,
begin the fade, and the state machine
takes care of the rest.
All the other channel functions are
basically the same.
They're still boring.
But they're boring in
new and exciting ways
because now instead of
finding the channel object
and setting its value we're
just setting the parameter,
the storing, storing the value,
and the state machine will
take care of the rest.
So here's our update object,
or here's our update function
which is almost the same
except inside of the update,
or inside of this loop, we
are updating each channel
and checking to see if it stopped.
Or checking to see if it's
in the stopped state, rather.
If we introspect that
update function it takes
the form of a giant switch on state
in which a miracle occurs.
And if we introspect the
miracle we get a wall of code
for which I'm sorry but
I'm gonna go through it
step by step here.
First there's this initialized state,
which on this example just
has a single fall through
attribute statement.
There's nothing there.
It just falls through.
But this is a hook for
adding new features.
If you need to do any kind
of one time initialization
on your channel this
is the place to do it.
So if your sound designers
come and say I want to do
one time randomization
of the pan and the pitch
and the volume, that's where you do it.
So then we move on to the to play state
and here first thing we
need to do is check to see
if we should even bother
playing the sound at all.
First we check to see if it stopped.
We check to see if it should be virtual,
and we check to see if
the sound was loaded.
If it's not loaded, we
jump to the loading state.
Then we play the sound
and set its parameters.
Note, these are the same
functions as before.
The only difference is they
have all the state machine,
the cogs that lie around it.
And then we move on to the playing state.
So the loading state, very simple.
We just check to see if
the sound has loaded,
and if it is we can go
back to the to play state
which will do all the logic to
see what it should be doing.
During playing the sound
is already playing.
So we need to update
its channel parameters,
its volume, its three
D position, its pitch,
anything else that is
coming from the game engine
and then we just check
to see if it's stopped
and if it should be virtual.
It's already playing so we
don't need to do anything else
other than updating its parameters.
For the stopping state
we update the fade out
and we still need to update its parameters
because maybe you have
a very long fade out.
The sound can still be moving
around in three D space.
It can still be doing all
the other things that it does
during, while it's fading out to stop.
Other than that we just
check to see if it's stopped
and should be moved to the stopped state
which is the world's most boring state.
Here's the code for
the virtualizing stuff.
I'm not gonna go through
it but you can see it has
the same shape.
We do some updates.
We check to see if we
should be transitioning
to a new state and I'm only
showing it for completion.
All in all this is about 600 lines of code
and most of the difference
between the previous engine
and the current engine is that
it's the state machine logic.
But now, so we've added,
we've started with the same
features as before.
We've added those three exemplar features,
but now the most
important thing we've done
is we've added hooks for
adding more features.
So as the sound designers
come and say I wanna do this,
I wanna do that, I have this crazy idea,
now it's easy to add,
whereas before it wasn't.
Now, what we just saw is
mostly just game engine code
and abstracting the
operating system API is done
by the middle ware, by in this case,
which in this case was
FMOD Studio low level API.
Wouldn't it be nice if we could do this
using just standard facilities?
I think, and so the standard
can't really replace
FMOD, Wwise, and ADX2,
not only because of those
fancy tools that I showed you
the screenshots of earlier,
but because the richness and
the functionality of APIs
is not necessarily, of those
APIs, is not necessarily
something that we should be striving for,
because they're very targeted at games.
So it shouldn't, right?
The standard shouldn't be in that space.
But maybe the standard can
provide a way to communicate
with the audio device
for things that need it.
Before I embarked on this,
on this journey though
I decided to see what the
standard has to say about audio
and here's what I found.
(crickets chirping)
The standard has nothing
to say about audio.
It's crickets.
So I decided to move toward
a standard C++ audio library
and see, think about what
shape that would take.
But even again, before I started
there I decided to ask why?
Why should we even standardize this?
And to answer that I'm gonna point to P669
which is by Guy Davidson
and Michael D. McLoughlin,
Why We Should Standardize
Two D Graphics for C++.
This is a really good summary
of the two D graphics proposal
and starting on page
eight of that proposal
there's a justification
of why we should be doing
two D graphics in the C++ standard.
The thing is this paper
should be templated
because if you just replace
two D graphics with audio
all the same justifications
apply equally validly.
So these justifications
in the paper take the form
of responses to potential objections.
For example game devs won't use it.
If we provide this, no game in
existence is gonna use this.
Well, you know what?
Some games will.
Some games will have modest requirements
and just want to reach for the standard.
Some games have more serious requirements
and they want, but they're
comfortable building
on whatever feature set, so
long as it's powerful enough,
they're comfortable
building on the feature set
provided by a standard, and
then there will be eight
libraries that people will
build on top of standard
facilities that will be
powerful enough for games.
Also games are not the only customers.
Any device with a speaker wants
to communicate with audio.
Think about your Google
Home or Amazon Alexa device.
Think about your alarm
system at your home.
Think about a sensor that,
that is detecting things.
Think about any, any device.
There's tons of devices
that have speakers.
It's not just computers.
We also have an objection
that widely used libraries
already solve this.
We've got those three that I
showed, FMOD, Wwise, and ADX2.
We've got plenty of open
source libraries that do this.
Why do we need it in the standard?
And the answer to that
is all right the standard
is supposed to standardize
existing practice.
This is standard.
This is not anything new.
We know how to build these APIs.
The APIs for communicating
with these devices
have been stable for decades,
and we know how to do it.
We know how to do it
right, and as you've seen
new APIs like I saw that
Windows has a new one
called Audio Graph that follows
exactly the same patterns.
It's not exactly the
same API but it follows
all the same patterns that
we know and are standard
and are, that are in the shape
of the things that we should be doing.
So we should be doing this.
I believe we should be
having standard audio.
So I came up with a set of
abstractions that I think
are at the right layer for the standard,
and I'm gonna go through
all of these one at a time.
First one is the device.
This is the thing that is
wrapping that high priority
mixer thread,
and providing the actual
wave data to that driver,
filling the wave data,
resolving the DSP graph,
and creating, creating actual sound.
So believe it or not one
of the interesting things
about the API for this device
interface is that it has to,
one important part of the
API is that it has to provide
device selection.
Most PCs have more than output device.
If you've got a multi channel output
on the back of your
computer and a stereo plug
on the front for your headphones,
that's two separate devices.
If you've got an optical out
so you can plug in your PC
into a receiver, there's
another output device.
You plug in your Bluetooth headphones,
there's another output device.
If you, yeah, there's just, your piece,
and if you output through your video card,
through an HDMI cable,
there's another output device.
So PCs have tons of output devices,
and then some devices have no output.
So we need to provide a null driver
for computers without that audio output.
A voice is then a wrapper for
a currently playing sound.
All of this by the way is
subject to bike shedding.
I'm not married to any of these names.
A voice is a currently playing sound.
So it's gonna have functions
for things like volume
and pitch and panning.
I have there left right panning.
I was at the SG14 meeting last night.
People convinced me that
we really ought to be
supporting surround panning
and other interesting
panning mechanisms.
So pretend left to
right is gone from that.
And voices get their
audio data from sources.
Now a source is an abstract
base class that has
three built in implementations
from this standard,
a buffer, which is an in
memory buffer of audio data,
either owning or nonowning.
We have a file stream which
is a pointer to a file
that is streamed into
this double buffer that
I talked about earlier.
Now another thing from SG14
that I forgot to update
in my slide, I have there IF stream,
but there's no reason why that couldn't be
just an I stream and
get its audio data from,
do this double buffering
mechanism from any stream source.
And the last one is a synth
which is a synthesizer
where again it's another
abstract base class we were
expected to fill in and
override some virtual function
that implements your own,
your own algorithm for
generating audio data on the fly
and these are the boxes
that we saw from earlier.
These are the same boxes,
and that same shape,
and that's where these particular
implementations came from.
So just those three now, all of a sudden,
we have enough to load data
from disk, from anywhere,
play it out the sound device.
We also have an effect
and an effect instance.
So the effect is a thing that
can affect the playing audio,
for example a low pass filter,
an equalizer, or reverb.
There's tons upon tons
of literature on these.
They're often called, it's DSP processing,
digital signal processing, as
opposed to DSP processing is,
that's processing twice.
I just realized that.
Anyway, so we can make
whatever algorithm we want.
We implement it in an effect.
We override effect, override
our virtual function
and then when we apply
an effect to a voice
or a submix, which I'll
talk about in a moment,
then we get an effect
instance which allows us
to apply parameters to the effect.
The last interface is the submix,
which takes both voices
and submixes as inputs
and then mixes them together,
and then you can apply
volume changes or any of
those effects and filters
and then send it off to the next submix
or out to the sound device
and this creates a graph
that the audio thread has to
resolve kind of at run time.
And so these submixes are
very important for video games
because they provide hooks
for the sound designers to
create, to control the mix in
a very fine grained fashion.
If we put all these
things together we get,
so I'm gonna show some
code samples at this point
of what happens when we put
all these things together.
And this is my favorite,
what I hope will be standard,
C++ 10 liner.
Now there's a little
asterisk next to the 10.
That's because I had to add
those two using name space
declarations in order to
make it fit on the slide,
but you can pretend that all the functions
are qualified with the
appropriate standard name space
and then get rid of those two
lines and now it's 10 lines.
You can also adjust the brackets,
and it'll get rid of the return zero,
and it becomes less than 10 lines,
but you get the picture.
This is the same code I showed earlier
from FMOD Studio API except
using these standard facilities.
Another implementation, another example,
here's a low pass filter.
Don't use this code.
If you're implementing a low pass filter,
please don't use this code.
This is something that
I cribbed very quickly
off the internet.
It has, it is hard coded
to a single frequency.
It assumes a fixed sample
rate for your mixer
and it has pops and clicks.
So, please don't use this.
But I just wanna show again
the shape of what this is.
We have a low pass filter we're inheriting
from standard audio effect.
We're overriding this process function
and we have to fill the output
buffer with valid audio data.
Once we implement this, we
can add a low pass filter
by taking our audio, our voice.
This is our, this is
the same code as before,
and we add effect, low pass filter,
and now we play with the low pass filter.
Setting up submixes.
Here is our, a very simple submix chain.
We have a master bus, or a master submix.
We have sound effects, music,
ambiance, and voice submixes.
We assign those bottom four to the master
and now we can set parameters.
We can set the sound effects
bus to minus 24 decibels,
that's .0625.
We can apply a low pass
filter to the ambiance submix.
Now to play the sound through the submix
we start with the same code as before,
but we assign the voice
to the sound effect submix
and we get that volume reduction.
We can do the same thing,
apply it to the ambient submix
and now we get a low pass
filter as it passes through the,
through the chain.
Now lest you think that
all of this is slide ware,
here is, we can all see that, yes.
Here is our same code
that I had on the slide
and if I play this--
(upbeat chimes chiming)
That's demo number one.
Demo number two.
Here's our low pass filter.
This is a slight variation
on the code that I showed you
to remove some of the clicks
and again the same code.
Note here we are adding the
effect of a low pass filter.
Again we compile and run.
(upbeat chimes chiming)
Same sound with the filter applied.
The demo number three
is, we're gonna hear
the sound three times.
Here we're setting up our submixes.
The same code as on the slides.
We're gonna play the
sound once dry, that is,
without any effects, not going through
any particular submix.
We're gonna hear it again
going through the sound effects
submix with a volume reduction,
and we're gonna hear it
again going through the ambient submix
with the low pass filter.
(upbeat chimes chiming)
Yay.
All right so now I have to
do my same little rigamarole.
Let's see.
Extend, yes.
And then--
Thank you.
All right I want to finish up here
with a quick shameless plug for my book,
Game Audio Programming
Principles and Practices.
This is a gem style book.
It's an edited volume full of articles
from some of the best audio programmers
in the games industry
available from CRC Press,
and so go, I encourage
you to go check it out.
Unfortunately it's not at
the book store downstairs.
At this point I'm actually much earlier
than when I practiced this
because you always talk faster
during a speech when you're giving it
than when you practiced it.
So I have about I don't
know 14-ish minutes
for any questions, comments,
compliments, complaints.
I will be here afterwards also
for anyone who wants to talk,
and you can always reach
me at this email address.
Yes sir?
- [Audience Member] So, are
you actually going to put
together a proposal, or do you
have one for standard audio?
I don't have one yet.
I'd like to start the process.
I was at SG14 last night.
The SG14 meeting, and I got
a bunch of good feedback
from people.
Encouragement and good feedback.
A little bit of
discouragement, but you know,
people had strong opinions.
But I'd like to start the process,
and just get it, get the ball rolling.
I was told it's gonna be
a very long and painful
multi year process, so.
- [Audience Member] Something
that I just thought that
should be also included
is math utility libraries
for doing decibel--
Yes, for sure, like decibel conversion,
sonic to pitch frequency or
pitch percent conversions,
stuff like that is all very valuable.
I'd like to have that kind of stuff,
even if it, if there is a desire for it.
If I get guidance in that
direction I would even
think it'd be valuable to have some simple
math utilities for doing low pass filter,
for filtering and bi
quads, and some of the kind
of common things that are
relatively simple to implement
that are not too exotic.
I'd like to see just at least
in a math library available
in a numeric sense.
That, going that direction,
becomes even deeper
down the rabbit hole,
and I just wanna start
with you know showing kind of what I have
and even making sure that that works.
All the code that you
saw, the kind of demo,
I threw that together relatively quickly.
It's a wrapper for FMOD.
I actually wrote that as an FMOD,
just to make sure that the interface I had
kind of made sense.
But some of the, some of the
details of that interface
come from the fact that
the low level's FMOD,
and if I were to build it for real
we would have, I have to
change some of those interfaces
just because the things,
when you're building your own
DSP graph as opposed
to using someone else's
you're gonna end up with different
kind of design decisions.
Yes?
- [Audience Member] I was
actually curious about
why no input device support as source
and also just wondering about channel,
control over you know channel
normalization and stuff
when your audio files has four channels
and you're on a device that only has two,
that kind of stuff.
Excellent question, so
there's two questions there.
The one is about microphone input
and the second one is
about channel routing.
Microphone input is not
something I'm ignoring.
It is something that I am
omitting on purpose because
it's kind of orthogonal.
You can have an audio out
without having to deal
with audio in and audio in is important
and it's relevant and it's
part of this whole experience
but it's sufficiently
orthogonal that I wanted to kind
of tackle it one, one half at a time.
I fully think that microphone
input is very important.
A lot of games use it and I just,
it's a big enough topic
on its own, that you know,
start with, start with one.
I also had at SG14 there
were people who were
coming up to me.
You're the third person who
asked about microphone input.
So, I'm not, I'm not forgetting about it.
The second question was
about channel routing.
Or channel, are we
talking about down mixing
or are we talking about routing?
- [Audience Member] Oh just
all of, all the, yeah--
All of those things.
Down mixing.
I haven't delved, so I
haven't thought about
in the context of this
proposal exactly what
that would, what format that would take.
So I don't have an answer for down mixing.
For channel routing, though
I would actually implement
that as an effect, and you
would have an effect that
does the channel routing
from you know three four
to one two or you know
however you wanted to do it,
you know, maybe make it
configurable in some way.
Actually if you look at the
FMOD Studio low level API
they have one of these
and they implement it
in exactly the way I describe.
It's an effect that they
have like 32 channels
that you can route however you want.
You can do one one, you know,
left right goes to the same
channel four.
So that kind of routing.
Yeah.
Yes?
- [Audience Member] As
an expert of the industry
how would you recommend
studios, engineers,
guys out there who have
already been in the weeds
making their own audio
engines and stuff like that
to use the standard sound library?
Would you recommend them to
kind of just like table that,
start again, build a new
one, or somehow try and mix
the standard into what
they've already built up
for the past 10 years like you said?
That is an excellent question.
In case the microphone didn't pick it up,
the question is for people
who have already been,
have already got their audio
engines kind of stabilized
and standard do,
is the intent to kind of have them
start from scratch with this, or to
change or, is that, that's
kind of what you're asking?
That's an excellent question.
I don't actually have a very good answer
off the top of my head.
I think my vision is if
this came into existence
in its fully formed and
operational state tomorrow
I wouldn't use it in my games.
I might use it in my personal project.
But I wouldn't use it in
my games because I already
am using these middle ware products that
we've already got a license
for and we're using.
But as a middle ware vendor,
like if you're talking about
Unreal Engine, or Unity,
or you're talking about
a brand new game that didn't
already have some legacy,
some brand new game, I would use,
I would consider this if the functionality
is powerful enough and it's
providing enough flexibility
I might use this in a new game.
For an engine like Unreal
or Unity or any of those
other guys they would be, it
would behoove them to implement
a low level driver.
They already have some
abstraction layers on top
and so it's relatively
simple for them to say
well now we'll slot in standard
audio, so long as it has
enough functionality, which again,
assuming it came into existence tomorrow
in the vision that I have
in my head, it would.
So the standard audio
interface would in that case
become an option for people using,
using these middle wares.
Any other questions?
Cool, well, thank you
very much for coming,
and yeah, have a good
rest of the conference.
(audience applauding)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>