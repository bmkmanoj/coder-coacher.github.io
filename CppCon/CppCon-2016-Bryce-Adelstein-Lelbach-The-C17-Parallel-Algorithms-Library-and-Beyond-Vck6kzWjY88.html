<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Bryce Adelstein Lelbach “The C++17 Parallel Algorithms Library and Beyond&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Bryce Adelstein Lelbach “The C++17 Parallel Algorithms Library and Beyond&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Bryce Adelstein Lelbach “The C++17 Parallel Algorithms Library and Beyond&quot;</b></h2><h5 class="post__date">2016-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Vck6kzWjY88" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so welcome to CPP con I'm
Bryce it'll steam them all back on the
program chair I work at the computer
architecture group at Lawrence Berkeley
National Laboratory which is a US
Department of Energy Research Facility
in California and I developed parallel
programming frameworks and analyze
software performance and research next
generation hardware technologies I'm a
member of the stellar group and one of
the developers of the HP X parallel
runtime system some of you are in this
room the last talk
that was from Hartnett my colleague who
was also one HP X developers and so
today I'm going to talk to you about the
new parallel algorithms that will
tentatively be in the C++ 17 standard so
I'm not really gonna talk about how
particular compiler vendors might
implement the parallel algorithms the
reason that is the parallel rhythm
library was designed as a way for
programmers to request parallelization
and constraints in that parallelization
and the interface was designed to be
agnostic to the implementation so in my
talk today I'm going also be agnostic to
the implementation I'm going to focus on
how you use the parallel algorithms
library and it really focus on the
interface it's not going to talk too
much about sort of the details aside
from what the constraints mean if you're
interested in looking at particular
implementation I'd recommend hpx
I'm a little biased though so something
like thrust or boost compute or other
implementations the parallel rhythms
library has not yet been implemented in
any of the major C++ standard libraries
but we'll probably start to see
implementations in the next year or so
given that it's right now in the
committee draft for the 17 standard so
I'm first gonna talk about why we need
parallel algorithms in C++ so in 2012
Sean parent gave a keynote at C++ now
and during that talk he said without
addressing vectorization GPU computing
scalable parallelism standard C++ is
just a scripting system to get the other
99% of the machine through other
languages and libraries and he showed
this slide and so this this slide is his
intention was to demonstrate that prior
to see Faust
you can only utilize a small percentage
of your hardware with only standard C++
and nothing else
now just adding threading would get you
a little bit it's maybe a little hard to
see this pointer here but you can see
this little pink region right here
that's what you get from from just
threading but you lose all of this here
from not utilizing vectorization from
not utilizing any accelerators that you
might have so a lot of you may be
familiar with herb setters article free
lunches over from from a few years ago
which described the end of CPU frequency
growth and the rise of multi process
multi-core processors and how we needed
to adapt we needed to start thinking
about parallelism and software that
previously didn't need to have
parallelism because parallelism was the
new way that you would get additional
performance and we have adapted to that
but now we're reaching the end of CPU
core growth it's becoming harder and
harder to add additional course a
traditional cache coherent heavyweight
CPU designs so we're starting to move
towards lightweight cores simply
processing so GPUs type processing other
types of accelerators specialized
accelerators on package FPGAs on package
high bandwidth memory technologies like
MC dram and HMC and so what all this
means is that free lunch is over again
so now we have to sort of adapt to a
whole new set of things and this time
it's worse because the hardware is both
increasingly parallel and increasingly
diverse so now we've reached a point
where we're starting to move towards
specialization of we're gonna have this
this hardware that's going to be very
specialized for this type of task and so
it's very difficult to sort of write
portable software that can run on
multiple modern systems and really fully
utilize them today so what we really
need is vendor-neutral parallel
programming abstractions that can target
multiple different architectures and
systems without leaving performance on
the table so in C++ 11 and 14 we have
low level concurrency primitives so we
don't really have any higher level
generic abstractions for parallel
programming and that's what the parallel
algorithm cyber library adds but before
I get into that I first want to talk
about what it's based on which is the
standard algorithms
library so this is a question that I
came across is one of the first things I
was putting any of this slides was well
what our what our standard algorithms so
the standard says that the algorithms
library describes components that C++
programs may use to perform algorithmic
operations on containers and other
sequences so that sort of seems
fundamental that it's these these
algorithms that they operate on
sequences so what are they they're
generic operations on sequences but
there's a caveat that there's some
things that are in the algorithm header
that don't operate on sequences but
let's just ignore those for now just
standard algorithms or these generic
operations on sequences so there's three
types of these operations
so there's non modifying sequence
operations like for each and find
there's mutating sequence operations
like transform and copy
there's sorting and related operations
so things like sort and binary search so
the new parent logarithms library
provides paralyzed versions of these
sequence operations and it's going to
tension heavily be in C++ 17 as I said
it's right now it's in the committee
draft which which means that it's it's
very likely going to be in there there
may be some bug fixes that go in okay so
what do I mean by parallel so as I said
there's a lot of different types of
parallelism a lot of different types of
parallel hardware and most platforms
expose different types of parallelism
and multiple types of parallelism so
there's I like to think of five basic
levels of parallelism so the bit level
of parallelism is is parallelism exposed
by the word size that individual
hardware components operate on this is
this is implicit you almost never see
this directly in your software you don't
have to think about this frequently the
people who make hardware free think
about this for you instruction level
parallelism is that parallelism exposed
through the simultaneous execution of
instructions you mostly don't have to
think about this unless you're somebody
like me who has to sweat cycles very
frequently and has to think about how
does the branch predictor going to
function in this particular piece of
software but usually the idea is that
instruction level parallelism is also
hidden from you that magical Hardware
smarts will just sort of make it all
work under the hood the vector level
parallelism is parallelism which is
exposed by instructions and hardware
units that operate on multiple
words simultaneously so this is sort of
the middle ground you get a good amount
of vector pair of vector parallelism
from Otto vectorizing compilers but if
you really want to ensure that you've
got good vectorization you have to go
out of your way to address it in some
way explicitly maybe you just need to
give your compiler some hints so it's
not like fully explicit but just like
hint it like hey here are some
suggestions that will help you know that
you can vectorize this code then there's
task level parallelism which is peral's
and that you is exposed through the
simultaneous execution of tasks that are
communicating in some fashion and that
share a primary address space and then
finally there's process level
parallelism which is paralyzed I'm
exposed through the simultaneous
execution of different processes which
communicate via messages or shared
memory regions and don't have a shared
address space and so sort of at the
bottom here this is very explicit and at
the top here it's very inclusive so the
parallel algorithms library today
provides vector and task level
parallelism so in the future it might
provide process level parallelism like
distributed computing support right now
we're provided these two levels and
that's sort of what we need these top
two levels are not something that you
really need to explicitly address at all
in your software
so there's five main components to the
parallelograms library first of all
there's this concept of an execution
policy that I'm gonna get to in a minute
then there's three standard types that
that implement that concept then there
are the algorithms themselves so they
come in the ones that are just versions
of the existing algorithms they come in
the form of new overloads that take an
execution policy is their first
parameter and then there are some new
unordered algorithms that are based on
existing ordered algorithms basically
these algorithms are algorithms where
the wording of the serial version would
prohibit us from vectorize it from
paralyzing it so we've created a new
version that has slightly different
wording that relaxes some of the the
guarantees about the ordering of
operations and would allow us to
paralyze them and then we have fused
algorithms which are combinations of
other algorithms which wit by combining
them were able to create a more
efficient implementation than we would
otherwise have all right so let's start
off with execution policies so an
execution policy does
scribes how a generic algorithm maybe
paralyzed they allowed programmers to
request parallelism and to describe the
constraints on parallelism so what type
of parallelism what I like and I don't
mean that in terms of like saying like I
would like you to use my GPU what you
say is like I would like like these this
sort these sorts of operations are okay
like this you can do this sort of thing
to my code and it will be alright and
then whatever the implementation is
under the hood will figure out you know
oh I can use this or I can do this or I
can do this so there's three standard
ones that I mentioned
so there's stood seek stood par and
stood par on seek so stood seek is
mainly there for debugging it is a
policy that specifies that operations
will be indeterminately sequenced in the
calling thread so indeterminately
sequence just means that they will they
will be sequenced in some order we won't
tell you what but that that order will
hold and that it will only be in the
calling thread there will be no new
threads created so it's just serial
basically then there's stood par which
says operations are intent and
determinately sequenced with respect to
each other within the same thread so
stood par says paralyze but please do
not vectorize stood par on seek says
operations are uncie quenched with
respect to each other and possibly
interleaved in there's special magical
language there and that magical language
is what lets us have a library
functionality that supports
vectorization because the special
language says hey you can do some magic
and like inner leave these functions in
ways that normally you wouldn't be
allowed to do this with like just some
function object that somebody gave you
but it's all right here and so that's
saying I want both parallelism and I
want vectorization so both both task
parallelism and vector level parallelism
so let's look at an example of how these
two are different
so how Parsi poorer and poorer and
seeker different so suppose that we want
to use the following binary operation
this multiply here with a stood
transform so we've got some input here
and so basically what we want to do is
we want to do X I equals x I times y for
each iteration of these two different
sequences there may be vectors so let's
sort of compile this to sort of a pseudo
assembly here so what does this look
like well we're going to load X ID or
some scalar register
then we're gonna load weii to some
scalar register then we're gonna
multiply these two together and we're
gonna store the results in X I so with
stood par you get something like this
where you have we've got multiple
iterations and maybe each one is
executed on a different thread but they
will not be interleaved at all that each
one will will be just it'll be treated
like a regular function it'll be each
step it will be executed in sequence
what stood par what stood par and seeks
as is you're allowed to both enter leave
and sherwin these things and multiple
threads so like one thing we could do is
we could say hey let's move all the
loads of X up here to the top and all
the loads of y up here and all of them
multiplies here and all the stores down
here and then with vector instructions
we could reduce that down to something
like this where we say halo 2 vectors
worth of X is load of vectors worth of
Y's and then multiply those two vectors
of data together and then store the
results to this location thing and so
again this is possible because we've
allowed this this I'm leaving
interleaving here and okay the next
component is all these new overloads so
these are all of the existing algorithms
that have overloads the table of ones
that don't have parallel overloads is
probably more useful but unfortunately I
don't have it so most of them have it
there's a couple exceptions it for
example the order two algorithms that we
can't paralyze those don't have
overloads so like accumulate is missing
here for example inner product is
missing here for example and there
there's there's a couple others so like
there's obviously there's no parallel
manner max and I've highlighted sort of
the most useful ones
so for each sort and transform our
particular useful ones in a parallel
context so one thing to note about for
each is that it's a little different in
the parallel version the serial version
returns the urinary function argument
that's the input the parallel version
does not because that's not really to do
it would work for a parallel for each so
using these is pretty easy so like this
is parallel sort you just here's a
vector can you sort it first argument is
going to be an execution policy it will
be sorted in some
in parallel so for each here what we're
saying to say par and seek for this and
what I'm saying here with the with the
uncie cuz I'm sayin hey I'm giving you
this function and process and you can go
and like decompose it and interleave it
and do crazy stuff under the hood so
that you can vectorize it and that's
alright now if for whatever reason that
was gonna be a problem for me that was
gonna break my code I could just change
this to par and then it would be fine so
this is roughly equivalent to this
so pragma OMP parallel force md it just
looks much nicer and it's an actual c++
construct all right so then there's the
new unordered algorithms that are based
on ordered algorithms so should reduce
which is an unordered accumulate there
is stood inclusive scan and exclusive
scan which are unordered partial sums
and then there's transform reduce which
is an unordered inner product so these
are also some of the more useful ones
that pretty much all of the examples
will use these algorithms these are the
components of the pair algorithm is like
library you're most likely to use with
the exception of sort also being fairly
useful in this context alright so first
let's start with wizard reduce so as I
said it's it's a nun order to accumulate
the interface is the same so at first
you got the execution policy then you've
got some input sequence then you have
the initial value for the reduction
which is applied for the first element
and then you have this binary up and
that binary op is a callable that's
going to be applied to consecutive
elements and it's also going to be
applied to other invocations of the OP
so it's like it'll do like a tree
structure and by default that's that's
stood plus and the net value thereby is
by default just the default constructed
tea so the difference is between stood
accumulate instead of reduce is this so
stoat accumulates says hey first you
treat some temporary some value at ACC
of type T and initialize it with that
initial value then for every iterator in
this range in order go and apply this
operation here of applying the binary
operator to to this here and so you can
see here this is not you couldn't
paralyze this because every iteration
depends on
previous iteration so what stood reduce
says is it says this thing it says do
this G something of this binary up and
then in it and first so before we get
into what that is we have to step aside
and take a quick math review so the
reason for that is because the
difference between reduce and accumulate
is that with reduce you'll get a
non-deterministic result for a non
associative or non commutative like
floating point addition is not a not
commutative and not associative so that
you might get a non-deterministic result
here if you just did stood reduce with
just floats or doubles so this is the
quick math math review some utility
means that changing the order of
operations does not change the result
addition multiplication or commutative
subtraction is not associativity means
that the grouping of operations does not
change the result so again addition and
multiplication for integers is
associative and subtraction is not for
floating-point types they're not because
the order of operations may change may
determine how many you know bits get cut
off in certain places all right so let's
go back to that G something but first
I've got to define another weird
ambiguous operation which is this thing
so this is G n sum which stands for
generalized non-community of son so
we've got it takes two arguments really
it takes an op some operator and then a
sequence of thing needs and it's a
piecewise function so if the number of
things in the sequence is 1 then this
operation just returns the the thing in
the sequence that whatever the one
argument was but otherwise what it does
is it will go and it'll do two recursive
calls on some subsequence of the input
and the the partition point can be
arbitrary so that it could be just like
two elements could be like half the
elements could be whatever and then it
will call the operator on the result of
the two recursive calls here and then of
course those recursive calls could also
recur stem so they could have some tree
structure and again as I said that K
there is arbitrary so this is the
generalized non-community of
some so the other thing G some the
generalized some that is what we how we
specify reduce is very similar what it
basically just says is that it's the
same thing as the generalized non
commutative sum but it can reconfirm
mute the elements in the sequence so the
difference is that this here is
unspecified for non commutative obso GN
some is unspecified for non commutative
ops but it's not it's fine for
non-associative ops g some here is
unequal give you some unspecified result
for both non associate for
non-associative and non community ops so
this is why reduce gives you a
non-deterministic result for non
community of not associated by ops cuz
it's it's this is how its weights for
tit so what does this mean well so if
you've got this here if I've got some
vector and I want to accumulate it it's
very straightforward what actually
happens if we would just want them to
like inline this function in our head we
would just do this right so we got these
five elements here we're going to
accumulate them okay well we've got some
initial value here zero so that's what
the initial value is here and then for
each iteration we just sort of add on to
it straightforward okay so with reduce
this is one of the possible things that
could happen but there are many other
possible things that could happen so you
could get this but you could also get
this because it's allowed to permeate
the the per the order in which the
elements are are operated on could be
permuted so that it could go hey let's
deal with the second one and the first
one then the third one then the last one
then the fourth one and also it can do
this it can sort of split things off
into trees and do this recursive
partitioning of the workload so that you
could say hey go do this in one task
over here go do this and another task
over here and then come and combine them
together later and then also this this
this could also be so this is like this
is it ordered I'm looking throwing aside
okay
this is like this is ordered and there
could also be unorder in here as well
all right pretty straightforward
ish any questions on reduce okay right
there so the question was do you
guarantee that it will be one of the
possible orders plus one of the possible
associations I believe the answer to
that is yes because the result is its
unspecified but not undefined behavior
so it's on some unspecified things
happens but I might be wrong about that
pretty sure I'm not but it's something I
don't have to look up later okay all
right so next let's talk about inclusive
scan so inclusive scan is an unordered
partial sum so it has the same interface
as partial sum so again it's just it's
got the execution for a policy parameter
then it's got some input sequence
specified as a first and last iterator
then there's another sequence in an
output sequence and for the output
sequence we only have we only have one
iterator that's given and then it's just
assumed that that it's going to be the
same size as the input sequence and then
you've got an OP and that OP is a binary
callable it's going to be applied to
consecutive elements in the results of
other invitations and also possibly the
initial value and then we have an
initial value so the difference between
inclusive scan and partial sum is that
inclusive scan applies the off with
unspecified grouping so it gives a
non-deterministic result for
non-associative ups but it's fine for
noncommutative ups so it's it's it's a
GN sum is used in defining this not
ji-sung so slightly different from
reduced there so what this actually
looks like so this is like what partial
summer or inclusive scan looks like so
the first element of the output is just
equal to the first element of the input
the second is equal to the the first
element of the input plus the second
moment of the input and so on and so
forth so on so it's a accumulates the
sum of all the previous iterations
they're so exclusive scan is something
that we didn't previously have a
corresponding unordered version it's
just like inclusive scan but it excludes
the ice
in the ice some so what I mean by that
is that like the the first output is
just equal to the initial value here and
like the second output here is just
equal to the whatever the initial value
is plus the first element it does not
include the corresponding element so the
second element of the input in the
someday and it's particularly useful for
a number of these parallel programming
problems which is why it's added and
there's now a corresponding non
execution policy-based overload of this
exclusive scan too but it's sort of it's
very similar to a partial sum or
inclusive scan all right and so then we
have the fused algorithms and the last
of the fused at were the first this
views algorithms here transform reduce
is also one of these algorithms that
maps directly to an unordered algorithm
which is inner product
so there's transformer dudas there's
transform inclusive scan and transform
exclusive scan so transform reduce is by
far the most powerful of these in fact
this talk probably shouldn't be called
this it should probably just be called
transform reduce because that's pretty
much all that I do here is just show you
how to use transform reduce it's very
very very useful algorithm so it looks
like this okay that should not say
unordered transform reduce it should say
unordered inner product there my
apologies for that
alright so transform reduce it takes as
execution policy is the first parameter
here and then it takes some input
sequence so first last pair then it
takes a transformation operator a urn
Airy transformation operator and then it
takes some initial value and then a
reduction operator here now it's
important to note what the types are of
this transform operator this ternary
transform operator in this binary
reduction operator so here's what the
signatures look like so the transform
operator looks like this so it takes
some T the type of the input sequence
and it returns some type R of whatever
type it would like and then the
reduction operator takes two types are
and combines them into one type R so
what when Iced we say transform reduce
it's two if you want to think about what
it does it's very simple just transform
then reduce so what it does is it
applies the bind it applies the ER nary
transform up to each element in the
range first last then it reduces the
result with the reduction operators
transform then reduce now is of course a
little confusing because the way that
you see it encode is you like this
reduce and then transform if you sort of
are writing it out in your head but the
operate the order in which the
operations happens is the transformation
is applied to the input and then the
reduction is applied one important thing
to note is that usually when you think
about transform you think about it as a
mutating sort of thing to transform the
serial version right it has some output
sequence that that it writes to there is
no output sequence for the transform
here the value there's this transform
sequence that gets created but it's sort
of like a pseudo sequence it's sort of
created on the fly and it's immediately
consumed by the reduction op and so this
is where you get the the benefit from
confront from instead of just doing a
transform separately and a reduce
separately that there's no temporary
sequence created here all right so like
this is what it looks like so at the
base level the first thing you've done
is the transform off gets applied to the
input sequence and then the reduction
off gets used to combine multiple
transform values and then it also gets
used to combine multiple reduced values
and so the tree actually would look a
lot larger than this potentially all
right
so there is a again this slide should
say unordered stood transferred inner
product here so there's also a binary
version of transform reduced there's a
caveat here which is that the spec is
currently missing the binary version but
I think I'm going to be able to fix that
so I'm the rest of these slides assume
that the binary overload for transform
reduce is going to make it into the
standard I'm pretty confident that this
is something that we'll be able to fix I
think it's really just a naming issue
because they're actually it's specified
in the standard that there is a parallel
inner product and that doesn't make a
lot of sense because we can't paralyze
inner product so really what we really
just need to do is rename the parallel
and our product to transform to
transform reduce so the the I call this
the binary transform reduce so the
distinction between the version I just
showed you is that this takes a binary
transform operation and it works on two
input sequences so the binary transform
operation is applied two consecutive
pairs from the two and
sequences the first input sequence is
the first one last one sequence and then
the second input sequence is the
sequence that starts with first - and so
the transform op will be applied two
pairs of elements from those two
sequences and then the reduction
operation gets applied in the way that
we just I just showed you so all right
I'm gonna get some examples now so
suppose that we have some sequence X and
we want to compute its vector norm so
the vector norm is the square root of
the sum of the square of each element so
that's a little wordy but basically we
take each element we square it we sum
all those elements together and then we
take the square root of that so you
should be able to see sort of like how
this is an inner product because there's
there's - there's two different
operations here that we're doing so this
is we're gonna use an airy transform
reduce here and the reason for that
we've just got one input sequence X
we're going to do some urban area
transform on it and it's going to be
this so just a square function so that's
the middle of there right there sorry my
laser pointer does not seem to be
functioning all right so here's our
array and then the reduction officer is
going to be addition here so and so just
the this is going to reduce the left
value and the right value here and then
the initial reduction value is just
going to be zero so this is pretty
straightforward and then we just take
the square root of this whole thing so
this is peril vector norm with the
parallel algorithms library it's pretty
easy to read it's pretty straightforward
kind of like it all right so let's talk
these signs out of order slightly out of
order don't worry about that okay all
right so dot product so this is the sort
of canonical transform reduce example if
you are in Hartman stock I haven't
looked over Heartland slides but I
suspect there was probably a dot product
example in there I feel confident in
saying that there was a dot product
example in the last talk because that's
just an example that you use for
transform reduce so dot product is
you've got two different vectors and you
want to go and take for each consecutive
pair of elements you want to multiply
them together and then you want to take
the sum of all of those products so like
X 0 times y 0 plus X 1 times y 1 so
here's where we would use the binary
train
form reduce so we've got two input
sequences so we've got this left input
sequence here the X's and this right
input sequence here the Y's and then
this is our transform op so it's going
to multiply the X isn't the X and the y
almonds together from the consecutive
element pairs and then we've got just
about zero for our initial value for the
reduction and we have the reduction
hockey up here which again is just
addition note that you actually have to
write the reduction off in the
initialization here because they do not
have a default value in the current spec
I see some hands all right I'm going to
go here than here
so sister you're asking whether you can
whether you can do a binary transform
reduce on two input sequences of
different types I believe the answer to
that is no I would have to check the
wording but I'm pretty sure that the way
that reduction operators are normally
sort of worded it's that they have to
take they have to take arguments of the
same type because otherwise you you
could use it to reduce the elements but
you couldn't use it to reduce the
reductions themselves so it has to be
the same type otherwise you would need a
separate operation to be able to combine
the reductions so sort of make that a
little bit clearer let me go back a
couple slides so like here like if this
if pretend that this was some second
sequence here and that you want it like
this was complex this is the second
sequence this was a Y and this was
complex' and this was double then you
might want your reduce op here to take a
double on a complex but then this reduce
up doesn't work because this one is
expecting to the results of these two
computations there so that's why we
don't have that's why you have to have
the same input type for the sequence all
right
okay so there's another question over
here I feel I feel like there's probably
a requires clause that says that the Y
is to be larger than than the X and that
it you just get you B if you break that
requires Clause that that that would be
my guess off the top of my head but I
would have to check yeah I'm pretty sure
that it's just that it's just one of the
requirements that the one of the
preconditions of the function yeah if
it's greater than it will just only
process up for the for the the range of
the first yeah yeah
and this is the same way that the the
non parallel algorithms do things - yeah
so the comment was that he leaves as a
proposal to the committee in flight to
deprecate versions of algorithms that
specify some ranges some some sequences
as to iterators and some is just one and
make some assumptions yeah I think I've
seen that that's a good idea it's not
something that's in flight 417 since the
the train is shipped for 17 but that
would almost certainly be applied to
these parallel rooms anyways another
thing I should mention some of the
orders of our parameters for transform
reduce in particular doesn't really
match what the order is for inner
product so there's a chance that some of
the orders of pram of art of parameters
and the numbers might change in the
future for reasons like that as well all
right any other questions at this point
in time okay all right so we went
through this dot product ok so there's
another way to write dot product which
is to use this something like a boost
County underwriter so a boost Kenton
County innovators iterate over some
sequence of numbers so saying like boost
County generator 0 to X dot size says
like I want to iterate over all of the
indices in this range here and so the
idea here is that we want to write
something where instead of working with
the elements themselves and our
transform function we want to get the
index there's a number of reasons that
this might be useful it comes up a lot
in like scientific codes or maybe you
want to like apply a stencil and so this
is how you can do it it's a pretty cool
trick it comes up in a few places and
this is kind of nifty it lets you write
this dot product with the urinary
transform reduce so this is actually
also the more frequent example of or one
of the more frequented ways that you'd
see an example of a transformer is you
also might see like a zip iterator for
how you would use an urn Airy transform
reduce
with which to do dot product with an
ordinary transform reduce but the proper
way to do that is with the binary
version which it's I'm gonna try very
hard to make sure it gets into the 17
standard all right okay so next we're
gonna so I had sort of two lengthier
examples here and so if you get lost at
any point during the lengthy or examples
just raise your hand and okay the
question is are there requirements on
the iterator types there are
requirements in the iterator types there
are also lots of requirements on any so
there's these things called element
access functions element access
functions are any iterators basically
any user code that might be used by an
algorithm so any iterator operations
that might be used by an algorithm any
operations on the elements of the
sequences that are that are required by
the algorithm any of the user-defined
functions so like your reduction on ops
you transform ops your Optive to 4-h
those are we call those Allman access
functions and there are restrictions on
them basically the restrictions are you
you you can't do things that would cause
data races I am I good now all right
cool
so basically it's it's you can't do
things that cause data races I don't
actually have a slide with the
particular restrictions handy so I can't
really tell you what the exact
requirements are but you can't use
something like in like an iterator from
an i/o stream as an input to any of the
parallel algorithms because that would
break there's there would be leashed
loop carry dependence ease between each
different iteration but yes there are
there are those restrictions there and
it's certainly something to look up in
in the documentation before using any of
these algorithms any other questions
before we move on to this this example
okay so what we're gonna do is we're
gonna write a parallel word count using
of course transformer disk because
that's the only the only thing that you
would ever need so our goal here is to
count the number of word beginnings in
the input sequence alright so first we
need some way of figuring out what a
what a word beginning is so we're gonna
use this function here so what this
function does is it takes two
consecutive elements of an some sequence
and it returns true if the second
element is the beginning of a word and
so the test for that is if the left
character here is a space and the right
one is not then we've reached a new word
here all right so we've got this
function here we're gonna set it aside
for a moment so we go back here alright
so trivial case if the input string is
empty we don't have any words so we just
return zero and then because of the way
we wrote this we said that it's going to
detect the second element at the
beginning of the word we we have a
little problem if we're going to use
this which is that if the first
character and in the sequence is not a
space and it's the beginning of a word
and we're not going to pick up on it
because our function only tests whether
the second element the second of two
consecutive elements is a beginning of a
word and there's no element that's to
the left of the first element so we need
to count the Friedan to check the first
character separately and so we just see
is the first character of space if it if
it is if it's not a space then it's the
beginning of a word and so we count it
and if it is a space then we don't count
it all right so then as I said we're
going to use transform reduce here we're
going to use the binary transform reduce
because we've got this binary function
that we're going to apply the
transformation but we've only got one
input sequence so what we're going to do
is we're going to construct these two
sub sequences from the input here so the
left one is the element all of the
elements in s except for the last
element the right sequence is all of the
elements in askes except for the first
element and what this is going to give
us is what we wanted which is keys
element pairs of caen
native elements so in particular we're
gonna get like you can see it here s
we're gonna get s 0 and s 1 as the first
element pair and then we're going to get
s 1 and s 2 is the second element pair
now this this might look a lot like what
a reduction looks like because
reductions are also applied to
consecutive elements the difference here
is that there's overlap so this
operation that we're doing here
like our reduction would be like s 0 and
s 1 then s 2 and s 3 whereas this here
we have the overlap so like we're
applying a paste a stencil almost here
alright so we've got this and then is
word beginning is what we're going to
use and and so it returns 1 whenever we
hit a new word and it returns 0
otherwise
and then our reduction operator is again
just going to be addition and the
initial value is going to be 0 ok so
what does this actually sort of look
like in process so we've got some input
sequence here and then we've got this
I'm going to call it a pseudo sequence
because it's not something that's real
it doesn't actually have storage
anywhere it's built on the fly but we've
got this post transform pseudo sequence
that gets created by applying the
transformed function to these two sub
sequences that we've created note that
the first that this is one shorter than
the input here the first element we've
skipped over here ok so what do we have
in this post transform sequence well we
have zeros for all the things that are
not the beginning of words and we have
ones for all the things that are the
beginning of words and so then the
reduction goes and adds up all of these
ones here and hey we have one of these
for every word so now we've counted the
number of words that we have so this is
what it looks like and also one thing we
can do we can actually take this right
here and we can stick it down in the
initial value there because that's
really where it belongs
because it's that's the initial value
that gets added up to the sum here so
this is our parallel word count using
the C++ 17 parallel algorithms library
any questions on this okay all right how
are we doing on time I'm having trouble
reading that 15 minutes all right that's
good that's just about as much content
as we have left all right so our next
example is very similar we're gonna
create a sparse histogram so a sports
histogram is a mapping of all of the
unique elements in a sequence to the
number of times that the element appears
in the sequence so this is something
that's best understood through an
example so like I've got this input
sequence here a b c c a a b b b b e so
how many unique keys do we have our
unique element so we have well we have
four and so then what we want is an
output is we want that the list of
unique elements and then we want another
list of what how many times those unique
elements show up so it's it's called
sparse here as opposed to against a
dense histogram would have a D in this
list here because it's it's day and then
the count for D would be zero it is
dense through the entire min max range
of the keys and the input sequence all
right so this is one as far as histogram
function is going to look like it's
going to return a tuple of two vectors I
guess it could return a map but I really
like vectors because I program on
hardware that really likes struct of
array layouts as opposed to array of
structs so I don't and I don't want to
use a map here so we're going to return
a tuple of two vectors and those vectors
are going to be the first one is going
to be the the keys and the second one's
going to be the count here all right so
again first thing is just you know if
we're empty we're done we'll just return
these two things here and they will be
empty they will have nothing in them all
right so then what we need to do first
is we need to sort the input sequence so
that all the equal elements are together
we'll use the parallel algorithm library
again for this so just sort power on
seek pretty straightforward all right
then the next thing we need to do is
count the number of unique elements so
this is basically the same as counting
the number of words so we're going to
use transform reduce so this is what
we've got so again we're using the
binary version of transformer to use for
doing the same trick that we did before
where we're creating these two sub
sequences from this initial input
sequence here where one of them's got
everything but the last one of element
every one of them's got everything but
the first element and then we're going
to apply
transform up here which is very similar
to our is word beginning operator where
it's going to return 1 if the second
consecutive element is a new unique
element and because we've sorted it we
know that if the left is not equal to
the right and we found the next one
that's not it's not unique that is
unique rather sorry and so the reduction
is a little bit different here because
we know that we have at least one unique
key minimum and again just as with with
the word problem we would have not
counted that key because this transform
up here it's going to tell us whether
the right element is a new unique
element so it would miss the first one
because there's nothing to the left of
the first element so we need to count
that first one and since we know X isn't
empty we just assume that it's that it's
that okay and so then we're gonna
allocate our storage in our keys and
counts based on that number of unique
elements and then now we're now we're
driving off into the land of things that
are not in the standard yet but
hopefully we'll be which is we're gonna
use this thing called reduce by key now
I could write this without reduced by
key but I can't fit it on the slide
without reduced by key so what reduced
by key does is it's going to for every
range of consecutive values that share
the same key in some two set of key
value sequences it will do a reduction
on on that consecutive range so you've
got is two input sequences a key
sequence and a value sequence so it'll
go through and it'll find so all these
keys here are equal so then for the
corresponding value elements I'm going
to go and do the reduction on that and
then go find for the next keys how many
of these keys are equal and do the
reduction on that and so unlike a
regular reduce which just gives you back
one value this is going to potentially
reduce to multiple values because it's
just reducing on subsequences and so
instead of having just a return value it
needs to have an output sequence here
and the output sequence the output key
sequence is going to be this histogram
keys and the output count will be the
counts and again just lies with the word
count here we'll we're just
we're using this constant iterator thing
here for the input value and that's just
an iterator that always returns one so
because we want to count each one of the
input elements once so then what this
will give us as the output is exactly
what we need
alright any questions on this and this
is this is something that's in HP x and
thrust and boost compute if you want to
play around with this and there's a
whole bunch of other useful buy key
algorithms I think these are are some of
the most likely things to go into like
the next version of a parallelism TS
they're particularly useful if you're
dealing with if you want to if you want
to work with structs of array or a
struct of array or so instead of working
with array of struct was there a
question back there yeah that's so the
question was a driver version this
doesn't have so much false sharing
problems me talking about false sharing
would be a whole different talk so we
should probably take that offline but
yeah the answer is yes but we should we
should talk about that later
all right okay all right so now I get to
talk about the the part of the current
parity and stuff that I'm very directly
responsible for and I don't say that
necessarily in a good way which is the
parallel algorithm exception handling
so me and Jeff Bastion wrote a paper for
the last committee meeting called hotel
peril for Nia the the punchline is you
can throw any time you like but the
exceptions can never leave the reason
for this is that if we have exceptions
boiling out of our element access
functions it makes it very very very
difficult to vectorize them efficiently
because then we have to damask because
of the possibility of control flow so
this is very problematic one like on a
GPU basically the approach would have
been like you know like check the a
analyze the element access functions if
any of them could throw exceptions run
it on serial would have been the
implementation if we had gone with any
error reporting
that doesn't call terminate here so this
is I think a good solution for now which
is that if an if an element access
function throws an uncaught exception
terminate gets called you're just done
now it also could throw bad alec if
memory is needed for you know like a
thread pool or if it's temporary memory
resource I'm unit for the algorithm but
that's a much less common case all right
so there's that sorry or thanks
depending on your viewpoint or you're
welcome rather all right
okay so so the stuff in this slides
almost none of this was code that I
wrote it was almost all code that I
stole from other people and these are
the people and things that I stole it
from so they should get the credit for
that so there's a couple things I didn't
talk about which is future directions so
there's three big ones I sort of want to
briefly mention the first is
asynchronous versions of the parallel
algorithms so like right now one of the
issues is that if you're using multiple
parallel algorithms like I'm doing I'm
doing sort here I've got a transform
videos here I've got a reduced by key
here this is all fork joined parallelism
going hey you go split all these tasks
go do stuff then get then communicate
back together there's no way for us to
overlap computation with the current
parallel parallel algorithms it would be
very nice if we had versions which
returned which were asynchronous and it
would return a future that would be
ready when the algorithm had finished
executing yes question there you could I
just think you could but I think
realistically there's going to be
performance opportunities that will be
lost if we don't have the library
interfaces for this I'd have to talk
with Hartmut to determine whether or not
that's correct but I'm pretty sure that
you actually want the library to provide
you with the asynchronous version
instead of you just wrapping it in some
asynchrony prim primitive it's just like
with the fused algorithms I think I
believe there's going to be
opportunities there to optimize by
having by knowing that you do
that you you're joining this
asynchronously okay question over there
okay so the question was how does this
compare to Microsoft's parallel STL so I
should give the brief history of
parallel STL's which is that there's
been about a million of them they've all
kind of done similar things I haven't
looked at Microsoft's parallel STL in
particular but usually they've done very
similar sorts of things in that they
tried to paralyze sequence algorithms so
I I'm not I can't really answer your
question properly because I haven't
looked at it but I would guess it's
pretty close alright so the the other
two thing I wanted to briefly mention so
one of the thing that you may have
noticed is missing is that there's no
way to ask for vectorization without
paralyzation uh moms don't worry we have
top men working on that so there's a
proposal for this thing called for loop
and for this whole framework theoretical
framework for reasoning for in wording
just factorization for this for loop
construct it's very powerful it's a
wavefront model and Pablo is going to be
giving a talk about this later in the
week it's I can't remember the name but
it's something vectorization in C++ you
guys should go there it's really great
there's also some it's um so so there's
some algorithms that are like missing
like the bikey stuff there's a couple
others that I think we could really
benefit from having in the spec here so
that's sort of the future direction oh
and I forgot of course there's executor
z-- which are a whole talk in and of
themselves basically what I've shown you
here is a way to request parallelism but
I haven't shown you any way to sort of
like control how that parallelism may be
applied executor z-- will give you that
so executor z-- as a way of abstracting
execution resources like thread pools
for example and orb like a accelerator
that is attached to your device might be
abstracted as a
executor so right now the parallel
algorithms library basically has sort of
a default implied executor which is sort
of this unspecified thing that whatever
your vendor gives you is going to be so
like nvidia cuda compiler the default
executor is going to probably be you
know whatever GPUs on your system but
like what if you have two GPUs and you
want to say run on this GPU or that GPU
we're like with lib C++ the execution
the executor is gonna probably be
something that goes to live the Grand
Central Dispatch on Linux with Lipstadt
C++ so it'll probably be some thread
pool built on top of P threads with
something like HP X it's it's very
lightweight tasking and so executor czar
going to give us a way to add a lot of
the controls that you might want to
other parallel algorithms all right so I
think about a little bit of time for
questions so I think it would be easiest
if we maybe do a line on one of these
microphones here because it's very
difficult for me to see you as hands up
with the lights here okay all right deck
you door default policy can you actually
suggest that you want to use all all
sites of Hardware threads or Hardware
pools such as GPU CPUs FPGAs
simultaneously to perform on your
algorithm um so you're the right now
that the question was whether whether
you can request to use all hard all a
possible hardware available with the
default executor right now you can't
right now the executor is something that
the implement is not in the language of
the spec anywhere and it's just an
implied concept that it's completely up
to your implementation how how the
parallelism is provided so right now no
there's not a way to add that but we
have top guys on it and that's coming in
the future all right so did par unsick
used to be called par vac you
yes it did this is like if Const expert
so you've probably have heard about a
thing called concepts for if because
everybody in the committee has gotten
used to calling it concepts forever yeah
or static if but concepts brief is a
particularly bad one because the actual
thing is spelled in code if Const expert
and everybody calls it concepts for if
Yap are back it used to be a rapport and
CPUC and myself currency used to be
called par vaq it got renamed to be par
and seek because those the execution
policy identifiers are very short they
got like shedded many many many times
they were gonna live in a separate
namespace at one point seek was going to
be sequence at one point on they were
going to be longer they were going to be
shorter this is this is what we ended up
with but yes that that is how they they
mapped yeah last question about using
the beginning end of the destination
yeah and there's a paper on that but
since we're adding a transform reduce so
we have control over that now so why not
add that safety feature now because we
don't really have control over it
because the committee draft is has fixed
the only possible changes that could be
made to transform reduce is if their
committee decides that the current we
are the current missing transform reduce
overload should be there and that the
inner product overload for tradition
policies that is there shouldn't be
there then we might have some freedom
there but it's not something that we
could really fix in teleconference I'm
hoping to at least fix the order of
parameters and transform reduce but
maybe a little out there if you set up a
vectorized of execution policy but then
the operations that you sent to it are
not capable of being vectorized bad
stuff so does it cause a compiler error
or does it cause of runtime it does not
cause a compiler error okay causes it
does not cause a runtime error it causes
undefined babe yes all right any other
questions all right well I hope you guys
have a great conference we're really
glad to have you here and I'll see you
guys all round</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>