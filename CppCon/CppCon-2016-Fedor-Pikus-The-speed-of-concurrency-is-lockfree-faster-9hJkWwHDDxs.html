<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Fedor Pikus “The speed of concurrency (is lock-free faster?)&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Fedor Pikus “The speed of concurrency (is lock-free faster?)&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Fedor Pikus “The speed of concurrency (is lock-free faster?)&quot;</b></h2><h5 class="post__date">2016-10-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9hJkWwHDDxs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay well welcome to yet another talk on
concurrency file in people and let's get
started thank you for coming so we
continue with more concurrency and more
look free talks so we're going to talk
about basically why do we write log free
programs well of course and that's
obvious because they're faster well what
do you do when they're not faster that's
the real question and we're going to
analyze the costs of data sharing data
synchronization with some examples now I
prepared this slide before this morning
unfortunately if I did it after I would
have learned from dance talk who's here
dance talk yeah so the more data I'm
going to show you basically the less
you're likely to believe me then have
demonstrated that with scientific proof
so I'm going to kind of skim over the
data and blow eight more and then at the
end I hope to actually give you some
guidelines for when lock-free is faster
now
it's the focus of the talk is obviously
about writing efficient concurrent
program C++ is used as examples it's not
particularly specific to the language
it's very practical I'm going to skim
over some of the more theoretical or
more pedantic details I'll try and I'm
not going to like tell you any lies I
may conceal some of the details of the
truth and you can ask me questions and
I'll tell you more I know it's not a
class on lock free programming but
that's okay those of you who understand
more who know more details will have
some educated guesses about some of the
things I'm telling you those of you who
don't know anything it's enough for you
to know that lock reprogramming exists
oh I just told you that lock
reprogramming exists right to understand
of where what I'm driving at and then
how to actually do it well you can look
at my last year stalk you can actually
come to me and ask where
I'll be happy to tell you more about how
to do it in general like writing lottery
programs it is hard it's harder if you
want them to work correctly now in
practice the reason you start on this
path is because you want better
performance since we're going to talk
about performance like all the time
what's the rule number one of
performance never guess about
performance always measure it
there is also rule number two of
performance well that yeah there is rule
number one your measurements must be
relevant that's the one that gets you
after you learn rule number one and
we'll see some I'll talk more about that
okay so here is your real life practical
decision diagram for going with log free
programming do you want faster program
yes you try to put it log free it
doesn't work if you you slap a couple
locks around it and then it works now
this actually as depressing as it is it
makes some optimistic assumptions about
software engineering process
specifically two assumptions one that
you actually profiled before you decided
that's performance critical and two that
you actually test it to know that it's
correct if you didn't do it one of those
things you you don't know that it's
actually working okay so from the
beginning what determines the
performance of the concurrent programs
well that's I promise you a practical
talk and that's not a practical question
the practical question is I wrote a log
free program and it doesn't scale what
do I do now that's the practical
question so we're going to go that way
now several things there is a high level
and low level parts to it high level
parallelism basically how much work can
be done in parallel that's your
algorithm data partitioning these kinds
of things and if you have single
threaded parts how long they are that
some dowels long low level parallelism
what individual units of work can I do
in parallel how much data sharing can I
sustain and so on and that's the focus
of like talks on low-level concurrency
primitives such as log three now there
is some overlap the more different
things the smaller things you can do in
parallel the more different algorithm
suddenly become parallelizable so it's
not entirely independent a brief note on
the benchmark so there is a reference to
google benchmarks I used it for some of
my benchmarks I wrote some of my own
just to verify that the results make
sense the way I'm presenting results is
it's for the fixed amount of work so if
I have on one thread some amount of work
we'll call it one if it scales perfectly
on two threads
I'll get the answer it's done in half
the time that's just how how I report
the results so if it's perfectly scaling
the time will drop down as divided by
number of threads if it doesn't scale at
all if it's perfectly serial this line
will be flat which means two threads did
have the work each but one after the
other and if two threads did have the
work each and it took them longer than
one thread then I got some overhead
that's just so you all know how to
interpret these results so you could
have fixed amount of work per thread for
example then it would look different
this is how I do it okay several types
of concurrent programs that you might
have encountered there are weight free
programs which each thread will complete
its task in finite number of steps now
steps aren't exactly the same as time
nonetheless all the threads are making
some progress toward final results lock
free programs were at least one thread
is making progress at any time and in
log based programs it it's not
guaranteed that anything is making any
progress wait for your lock free doesn't
mean that it's not sharing any data it
just means that it does it in a way that
doesn't require locks okay so how much
does it cost you to share data sharing
data is like the bane of concurrency if
you didn't have to share data you would
have perfectly parallel was called
embarrassing
parallel programs life would be great
let's take a very simple example where
we have an atomic integer and we're
going to increment it on all of our
threads at once it's a weight free
program each thread is going to do
however many steps which I increment is
a step however many increments I wanted
to do that's how many steps okay the
blue line is sharing they're all
incrementing the same atomic the
whatever other line is the one that goes
down that's no sharing each of them has
its own atomic and they're incrementing
it and the dashed line which is there is
ideal sharing it's just the performance
at one thread divided by the number of
threads so basically without sharing you
see that it's embarrassing the parallel
programs are embarrassingly parallel
with sharing it's a weight free program
not only it doesn't scale it actually
gets worse with the number of threads so
just because it's weight free it works
in a fixed number of steps but not
necessarily in fixed amount of time time
gets longer something somewhere is
waiting for somebody this is probably
like the most you know theoretical bit
of it there are because it just because
it helped me to explain some of the
other things that are talking about
later in these multi CPU systems there
are two parts where multiple CPUs
fighting for the same location have to
deal with delays the first disk cache
coherence each CPU has its own caches if
you are writing into the same memory
location all the caches must be updated
but there is dedicated hardware there is
actually more of that Hardware on modern
microprocessors than any other hardware
that is doing that work and then when
you actually hit the same memory even
though it's log free there has to be
some arbitration you can't actually
write into the same location at the same
time you know deep inside it happens
what one at once at a time is just to
you it's at the same time so how can we
figure out what's taking all this time
well then there is interesting thing
about caches
while your microprocessor separates on
integer wards 4 bytes 8 bytes the caches
operate on cache lines and on x86 this
machine I'm using for testing there are
64 bytes long
so here is another test I added the
third line which is what's called false
sharing each thread has its own atomic
variable but they're sitting right next
to each other so that jacent ones share
the cache line now as you can see for
the first eight threads I'm basically
getting exactly the same cost as for the
shared variable so all of the cost of
the data sharing of updating the shared
variable comes from sharing the cache
line after eight threads it drops off
why yep I ran out of cache line eight
byte words 64 byte cache line after
eight threads it got faster because
they're now in groups of eight sitting
on different cache lines exactly right
now so at the hardware level there is
synchronization which it's important to
understand you know shared operation
itself doesn't scale it's not going to
scale don't don't bother don't even try
it's its purpose is to use your program
to get to enable your program to scale
so you want as fewer as few shared
variables accessed at the same time as
possible because that adds the cost now
we have three ways of synchronization
weight free not all algorithms can be
done weight free not all data structures
can be done well three log free it's may
be very hard it's usually possible and
locking well that's the easiest it's
rare for your entire program to be wait
for your lock free usually it's per data
structure per algorithm and that's fine
given how hard it is ok well let's
compare these three synchronization
mechanisms and we'll choose a very
simple computation which actually can be
done wait free log free and log based
and that's an increment we have some
data structure in the data structure
that lives an integer and we have a
pointer to it and I'm going to increment
that integer through this pointer
from all threads at the same time now
it's not the fastest way to compute the
sum but let's say I need it let's say I
actually need up-to-date count counter
of how many times I increment so mutex
compare-and-swap loop that's log free
and the atomic increment in C++ is
spelled fetch in AD who is surprised by
these results mutex takes way longer
compare and swap quite a bit faster and
atomic is even faster
for basically any number of threads
anybody surprised and anybody expected
anything different anybody thinks there
can be something different no all right
nobody has enough imagination to imagine
something on x86 okay I'm using relaxed
memory order on x86 it's not really
relaxed but yes in also everywhere I
mean basically nothing so the reason I'm
using relaxed memory order nothing else
depends on that value the difference
between relaxed and let's say release
would be if I'm going to read something
else indexed by that counter I want that
something else to be ready before I read
the counter here I'm not there is no
dependency I'm just reading that counter
nothing else so yeah I can reuse relaxed
memory order atomic increment on x86 is
not exactly relaxed but never mind that
while all mutexes are locks not all
locks are mutex s so let me throw in a
spin lock that I wrote myself that is
lockable it has the same interfaces as
the D mutex it has exactly the same
semantics if you've taken the lock
nobody else can go through and it gets
where it fits where the spinlock line
would fit on the plot on the previous
plot okay so somebody says it will be
better than the lock-free
no better than lock-free okay so or same
as lock-free what about wait free better
than wait free oh wow okay
better than wait for it to the gentleman
in the aisle sold it's a little more
complex than that it's but yes for most
number of threads it is now about yeah
okay the question about how I measure
overhead is divided by the number of is
the total work which is I want to
increment it let's say this number let's
say 1 million times so all of the
threads together have done 1 million
increments
if the work is evenly divided you know
two threads have done half a million H
four threads would do quarter of a
million H and so on
that's how its measured and this is the
total time so it it's never I'm
incrementing the same variable it's
never going to scale it's never going to
be faster on two threads than on one at
best it's going to build level because
I'm just measuring the cost of that
synchronization primitive the shared
access now after I'm done with my
sharing I'll go and do some work
hopefully on per thread memory that you
know is not shared and that's where my
scaling comes in but in order to do that
I have to figure out okay this thread
works on this element of the array this
thread works on that element of the
array that counter let's say index of
the array that's the shared one so we're
all going to increment it and each
thread is going to get it
or I'm going to advance the pointer in
the list and each thread is going to
work on the node the but advancing the
pointer the that's the shared pointer
that's where they get their work from so
I'm just measuring the cost of this one
share of this shared variable that
controls everything controls all the
axes okay well what does spin lock
actually do spin lock okay please don't
write this and you saw how great the
spin lock was please please don't
actually copy this and write in your
code this is not what's in that spin
lock please this is a small portion of
what's in that spin lock this is
horrible spin lock trust me but the
basic idea is there is a flag and we
exchanged this flag was zero and if what
we got back was zero it means somebody
exchanged it with zero before us and we
loop on it that's that's how they'll
work now in this case we're accessing
our shared variable through a pointer
which means we can actually do even a
little better we can grab the pointer
itself this way if we're atomically grab
the pointer nobody can even get to our
shared variable through that pointer we
just replaced it with let's say now
marginally better in this case now don't
just go and throw out all your lock free
code yet first of all there is
dependence on the number of threads you
can see if you have enough threads lock
starts to lose by the way this is a well
everything looks good at a small number
of threads so this is a large system
this system has 120 Hardware threads 60
60 physical cores this is a slightly
older system it has a little bit fewer
cores that's not really important but
it's an older the first one was an Ivy
Bridge Version three this is an older
Sandy Bridge system atomic is faster
here
now I was measuring real time which is
basically throughput what if I'm in a
low-power environment and this is Dawne
this light where I talk about different
metrics I'm not an expert on low power I
do I go for maximal performance but I
have to caution you remember
measurements must be relevant what if
you are interested in power and a good
proxy for power is CPU time consumed
well this is totally different result
this is where back to the first large
system ok what if I'm in a low latency
system and what I'm really interested is
long tale of latency I got those
measurements - again for this system
this is percentage so average latency
for mutexes of course much higher than
for log 3 so this is relative to that to
do whatever your average latency is this
is 90 is 95th percentile so the only 5%
of events take longer than than that so
remember your measurements must be
relevant whatever you are trying to
optimize that's what you measure
okay so mutexes are slow locks not
necessarily way tree implementation can
be simpler for for increment it is it's
usually faster if you work really hard
on your look so you can get even wait
for you give it a run for its money
compare-and-swap is actually can be
really slow although if you have enough
threads eventually lock-free start to
win you have to measure enough maybe
more than you ever care about i have
hundred twenty cores on that system if
you don't you may you may never get to
the region where a lot is your can
thread is your concern
okay let's put it in context of some
real data structures you know this is
just increment this is a model study
what if it's too simple oversimplified
let's put it in some context
we'll have typically a thread-safe data
structures divided into two fundamental
classes there are node based like lists
and staff they're very good from the you
know from the point of like nose not
filling them up there is no fixed chunk
of memory but you're hitting memory
allocator pretty heavily and that may
have a lock on it
then there are block allocated
containers nice cache locality you have
a fixed chunk of memory or everything is
running into it you have to worry about
what happens when you run out a fixed
chunk of memory so each has its own
problems well simplest not nodal
container singly linked list and in a
c++ known as forward list we have one an
STL has well you know the interface that
it has it's not thread safe it has your
normal steel turret safety guarantees
you can't mutate it from two threads at
the same time let's do the same thing
but thread safe and I'll start with push
front and pop front insert after an
erase after a very similar
implementation now for word list does
not have a find member function on the
premise that if I know details of its
implementation and how it synchronizes
is thread safety I can maybe do better I
will give mine a member function find of
course iterators okay push wrong push
front is pretty easy
we need a compare and swap loop we read
the head atomically we prepare a new
node don't put it into the list yet but
we use it at the head that we're ready
to set up the next pointer if head
hasn't changed which means we can
basically pretend that no other thread
existed for now we can atomically swap
our new head the new in in the place of
the old head and we're good
if head has changed we have to start all
over again
pop front same thing same way if the
head hasn't changed you deleted now here
is the catch okay who here knows what
the catch is why this doesn't work mmm
aha
okay well I'll go through it quickly so
don't worry I'm not going to leave you
behind
if you don't know here's a problem I'm
going to delete t one on the first read
they had the first element which means I
took the new I took the new head is
going to point to the element after the
one I'm deleting but this thread is kind
of slow now it's not in the good mood
today the second thread comes in and
actually deletes the one it's lock free
which means you know I haven't logged
the list deletes t2 now I've got a
really hard working thread because it's
also creates a new one it could it does
a push front and it got back the same
memory as was occupied by one of the
deleted elements now the second thread
finally wakes up and does a compare and
swap and guess what the heads are the
same compare and swap reports that
everything is fine nothing changed that
says compare and swap safely insert a
new element into the lists as compare
and swap and this is what you get you've
just disconnected the entire list and
your head points into nowhere yep
somebody mentioned they'd be a problem
this is the problem my head changed from
A to B back to a why because Mallory
uses memory pointers are not unique
identifiers if the memory is reused what
if the memory isn't reused well you'll
run out of memory furthermore good
memory allocators like to reuse memory
because the last memory that you just -
this probably still Houghton cash for
very for much less complex reason find
and pop front or not say if you have
fines running and I mean put the app up
front but from just deleted
all the elements from underneath your
pointers so the fundamental problem is
that we are deleting nodes to which we
are still pointing to either fine points
that more push front points them or pop
front has temporary pointers pointing to
them and somebody else is deleting them
there are many solutions to this problem
hazard pointers is one but we actually
happen to have a very simple one it's
just the thing we have the shared
pointer share pointer is designed to
solve precisely this problem it may be
okay if I had let's assume for a moment
that I had a shared pointer that is
thread safe herps that represented this
two years ago let's look at our push
front and pop front now same same
sequence one thread tries to delete the
head element it gets the old head and
then you had that it wants to be the new
head another thread comes in deletes the
first two elements we have shared
pointers there holding the two elements
alive just not in the main list we
create a new element we can't put it in
the memory of t1 because it's not freed
yet it's still in use malloc hasn't got
it yet okay now compare-and-swap has to
fail now the first thread actually
notices of compare and swap fails let's
go of its temporary variables for the
old head and you had that actually
deletes the nodes finally because there
are no more shared pointers pointing to
that man we start all over again okay so
that solves a be a problem that solves
the find problem for the same reason if
the temporary pointers are holding their
nodes alive in general if you didn't
have just push front and pop front you
would have all of your list pointers to
be shared pointers other than that there
is no difference okay what can we do
about getting the chair pointer
especially since somebody said it's not
necessarily lock-free well as the D
shared pointer looks basically like like
that with some more stuff it has a
pointer to the data and pointer to
reference count has other stuff that I'm
not interested right now I can do
another one for this specific case I can
have what's called intrusive pointer
where the reference count is located
right next to the data in my case I can
do it because I control
what's the list node I don't control the
tea but I control the list node which
has the reference count the next point
or whatever else I want to be so
intrusive SharePoint can't have weak
pointers if you use this design now for
all of these reference pointers the
reference count is going to be
incremented atomically and lock-free
using just fetch add so no problem there
the question is what happens when two
threads acts as the same shared pointer
at the same time not two shared pointers
that point to the same object that's
fine reference count is incremented
atomically but the same shared pointer
and why would that happen well because
they sit on the node and two threads can
be accessing the same node at the same
time okay I have several options in C++
11
you cannot say atomic of shared pointer
but you can say a Tomic load atomic
store atomic exchange of shared pointer
there are non member functions this is
in C++ 11 it works it doesn't have to be
log 3 but it is thread safe in C++ 17
ts1 there will be finally an atomic
shared pointer I didn't get that code I
downloaded the code from the proposal
that became that atomic shared pointer
so that's what I'm going to measure in
this talk finally if yours if you
abandon the SPD share put are all
together and go to the intrusive
SharePoint or you can actually do
something else you can do the pointer
lock that we have already seen which is
basically spin lock on the pointer okay
any guesses about yeah well I'll give
you him the next slide will show
performance graphs and it guesses about
what we'll see
well I don't know if any of them have
and the atomic clock doesn't have a
mutex the atomic load may or may not I
don't know how they did it the spin lock
doesn't have a mutex has atomic exchange
okay so which one of these three will be
fastest who says that the spin lock will
be the fastest who says that atomic
share pointer will be the fastest you
know it's in C++ 17 not almost on the
bottom is that spin lock the rest
okay that's do now again remember
measurements must be relevant this is
media referencing the pointer this is me
copying the pointer this is me doing
assignments on two threads
simultaneously in an opposite direction
the cross assignment which is actually
the this cross assignment is the whole
reason why you need a lock free or at
least the thread safe shared pointer the
rest was wasn't the problem it's you can
dereference the same shared pointer
multiple times it was out any it's
reading it it's the the cross assignment
where you're making it you're changing
the pointer while you are reading it
that's where you need thread safety not
looking good for low career right now I
have to tell you that okay the pointer
lock is not lock free now the comparison
was not entirely fair I have to confess
both of those shared putters do more
they have weak weak pointers they have
other features that my intrusive shared
reminders are shared pointer by an
amazing bit of foresight happens to
ideally fit the list and nothing else so
it's not an entirely fair comparison
extra stuff has overhead now if I befall
I want is the list then it's a perfectly
fair comparison all I'm ever going to
use it for is whatever I need for the
list I'm not interested in overhead list
itself is lock free as we have seen the
point
is not if you ever get a faster pointer
that is lock-free you can swap it in
your other concern is the possibility of
a deadlock don't worry about deadlock in
this case because the lock sits on just
the pointer swap there is no way for you
to inject user code into the critical
section which means you can't take
another lock while holding that one okay
so our atomic forward list we have will
have shared pointers all the way down
now one word of note if you want
iterators themselves to be thread safe
meaning two threads can access the same
iterator you need thread safe share
pointers into those into iterators as
well for this list that I wrote and
benchmarked I say no iterator has to be
held by one thread so what's inside the
iterator is just a plain SharePoint or
non thread safe one you can put a thread
safe share pointer in there if you want
a budget it will work okay push around
just the push around now how did how did
I do the benchmark of just the push
front was out running out of memory
well that 120 core machine also has
three terabytes of memory the benchmark
didn't run that long that's how I did it
lock free so what am i comparing
standard for word list was mutex gender
for Elizabeth's pin lock and my lock
free list lock free list kind of man
managed to bid the mutex not every time
push runt and pauper on that one
actually doesn't run out of memory same
thing again this is I remember
measurements must be relevant this is
not a fair comparison if all you want
from your list is push runt and prop
front this lock free list is a massive
overkill why is it a massive overkill
and specifically why where does the
extra cost well it's a massive overkill
because it supports all this other you
know inserts in the middle and all this
other stuff but why does it matter
spin lock sitting or guarding this
entire list has just one shared variable
that's the single point of contention
my log free shared lock free atomic list
shared variables all over the place
there is a reference counter that
they're all incrementing and
decrementing there are these pointers
inside the shared pointer that I'm
spinning on and there is one in every
node and one in the iterator and one in
the temporary shared pointer so there is
a lot of shared variable access and
that's the price that I don't need to
pay if all I want is push run to pop
fraud but I I want more than just push
front and back front now we're getting
somewhere interesting how would it you
find with a lock well you could put a
lock around STD forward list and called
find that would suddenly make your push
round have potentially order and
complexity if you happen to be waiting
on the find but it gets worse than that
find returns you narrator you can't
delete the know that the director is
pointing to you have to hold the lock
while that iterator is outstanding out
there which means you can't actually get
to iterators because the the first one
locks the list you could put a lock on
every node of the list you would
probably deadlock if you tried that you
would certainly deadlock if it was a
bi-directional list just traversing it
in opposite directions would get you
there now I'm not saying that block the
only way to build a thread safe list is
a lock free list there are other ways
you can use readwrite locks there are
other tricks the point is it's not as
simple as putting spin lock on an STD
for word list and if it's not as simple
lock release suddenly becomes a
contender assuming it performs does it
yeah for find it does for multiple
accessing of iterators pointing to all
the different elements of the list it
does they're all independent okay that's
enough for the list let's talk about
queues question okay now you can do
queue with a list just push run pop
front it will behave like a real like a
list we'll talk about a different kind
we'll talk about an array which is used
probably as a circular the ring buffer
array of your element
and you have some counters that point to
the head and the tail of the queue and
you will probably be doing atomic
increments on them there are many many
lock-free hue schemas and they have to
do with the fact why there are so many
well it's actually very hard to write a
generic completely generic lock freak
you following this without the list
actually on the block array and if you
do it probably not going to be
particularly fast so you take advantage
of every practical application specific
restriction that you can get for example
oh you just before me Anthony was
presenting he mentioned queues that on
which you stop putting new elements
after a while that's actually that's
very very important for log for queues
you don't have any more producers so you
you don't worry about discoverability of
the new elements you can just read them
all from the head of the queue one
producer thread multiple consumer head
threads
simplifies your life greatly other
things like that so there are a lot of
different queue strategies let's look at
several first of all let's look at how
the queue looks like to producers here's
our array of elements here is our atomic
counter and all the slots upto n are
filled that's no more the producers
concern what what does the producer do
atomically increments n now the slot of
that was the old value with index of the
O value belongs to that producer thread
and nobody else at least as far as
producers are concerned consumers are
different story producers so producer
can go and construct the element in that
slot at its leisure and no contention
anymore other producers keeps that keep
advancing n consumers have to somehow
know when producer is done constructing
them into the slot that's a different
story
ok let's look at consumers point of view
well first of all consumers have to know
which what is the next element to the
queue again we have M which is the front
of the queue consumers atomically
increment M
now that element belongs to that
consumer threat nobody else can get to
it
copy it out to the user at your leisure
no problem where the problems happen is
at the at the consumer producer boundary
you just atomically incremented em what
if that element isn't there could be not
there for two reasons one it's it's sort
of there but it hasn't producer hasn't
finished building it yet - it's actually
not there at all you overshot the head
overshot the tail you have to back off
consumer has to beg decrement the EM
can't just decrement it because another
consumer might have incremented again
you don't know how much decrement you
can run a compare and swap Lupo on em to
solve this problem how do you know which
elements are ready you can have a middle
atomic counter there the third one that
tells you the boundary between ready and
not ready elements incrementing that one
is non-trivial because producers can
finish building their elements in
arbitrary order so you can't always
advance it by one you can put a flag
into the element itself that you
initialize last atomically and when that
is initialized then the element is ready
cost you memory except when it doesn't
for example queue of pointers if pointer
is now it's not ready yet you atomically
change it to not now when what it points
to is ready and now it's ready so as I
said lots and lots of different schemas
specific to your unique special cases
nonetheless very few well not very few
but cool
not too many variants of kind of a
normal case so let's look at some and
how are we going to look at them what
I'm going to do is I'm going to actually
do something terrible I'm going to
benchmark code that is not correct I'm
going to profile I'm going to ask how
fast it is before I ask is it working
why here's why I have a normal case
synchronization protocol whatever that
is I have some shared atomic head atomic
tail whatever else I have and I have my
synchronization protocol that works in
the normal case the memory is not full
the queue is not empty consumer and
producer and running into each other I
want to know how fast that is that's
actually about 10% of your queue code
the rest is handling off all of those
special cases and that's where the box
will be before I go and make those bugs
I want to know if it's worth doing if
it's going to be faster so I'm going to
construct the benchmark that instead of
rare cases makes them never cases I'm
going to run a benchmark for just the
normal scenario which is handled by my
normal synchronization protocol and
going to measure how fast that is and
then I there is still non-trivial cost
to handling all the special cases or
maybe some cases it's trivial at least I
have to say if not special case may be
true they'll cost maybe not so I have to
profile it again but at least I'll know
if I should even try so here is our
simplest lock free q1 producer thread
one consumer thread this is actually
this is very simple this actually
protein this is pretty much it there
it's a very little special case gambling
you need in this case you actually do
the atomic increment producer does
atomic increment on n not before but
after it finished constructing the
object because there's only one producer
and advancing and signals to the
consumer that that slot is available
there is again only one consumer so it
can do its atomic increment of M after
it's finished copying out and it knows
that slot n is ready when you can get
this load L circularbuffer of course
size power of 2 so my module operation
is just a bit mask operation well works
for one consumer one producer thread I'm
actually going to run the measurements
for multiple threads which means I'm
returning elements that haven't been
fully initialized why am i doing
that well I'm doing that because I want
to know if I could somehow if I was much
more smart much smarter than I am and
could figure out a way to run multiple
threads on that synchronization protocol
would it be worth it
not so much
so for two threads it's similar to spin
lock you it may be a little faster
probably not worth the bother and I
don't know as I said how to get it
working for multiple threads well I do
know how to get for multiple threads a
synchronization schema with three shared
variables and some compare-and-swap
loops running on them and I did a bunch
of tests and the results are more or
less the same I am NOT going to show you
all of them I'll show just one okay I
said I tried queue number 2 3 4 so I
guess the next one will be 5 ok this one
goes back to atomic head atomic tail and
are ready for atomic ready flag in the
node this solves me the problem of
consumers reading nodes before they're
ready so there is a compare-and-swap
loop on the on M there is just atomic
increment on and and there is atomic
atomic really released or store as a
release barrier on the ready flag so the
store was the memory or the release
normal case is simple it's non-trivial
to deal with what depends on what you do
what you want to do when the memory
fills up do you want to lock and
reallocate do you want to refuse to in
queue new element I'm not worried about
any of that right now
I want to see if in the normal case
works now remember rule number two of
performance the measurements must be
relevant in my case the relevant
measurement is the support of the queue
I'm assuming that elements are flowing
through the queue at maximum possible
rate that's what I'm interested if your
queue is almost always empty the next
measurement is going to be totally
irrelevant to you
instead what you care about is how
quickly I can handle the special case of
empty queue most of the time
so remember measurements must be
relevant your look your simulated load
for benchmark must be relevant for your
actual case at least the throughput it's
not a competitive queue okay I have have
a convinced me by now that the lock-free
basically doesn't work now this is like
the darkest moment on the presentation
although some people say that things are
darkest just before they go pitch black
this is fortunately not quite not going
to be the case so the pro so what we
have seen is basically compare-and-swap
loop is just killing us on the other
hand spin lock is flying at least for
small workloads if it's guarding small
worker
wait a minute small workload ah what if
it's not small I'm going to include a
larger object takes me some time to
actually copy it into the queue now
mm-hmm things look very different now my
prototype q5 again it's a correct the
prototype is correct for the normal case
it looks pretty good it's similar to
spin lock for small number of threads
and it runs way ahead of the spin lock
for a larger number of threads so what's
going on here the log based queue is
just
STD queue with a spin lock where I
wrapped around it it copies the element
under the lock lock free queue copies
the element outside of it doesn't have a
lock but it has its equivalent of
critical section they're the place where
I'm actually touching the shared
variable and looping on the compare and
swap I don't have to
copy the element again after
compare-and-swap it so it copies outside
of the critical section that's why it's
faster so multiple threads in case of
Locker here doing their own copies all
at the same time the thing is actually
doesn't have to be this way it's just a
natural way to write code in each of the
paradigms so we have to be really
careful here some of the benefits of
flock free are actually not you to look
for itself it's due to the way you're
encouraged or forced to write code when
you're doing log free of course now that
I told you that the next question is
okay now that I know now the now that
I'm conscious of this can I borrow these
techniques for log based programming
sometimes you can for example this
notion that you want to copy outside of
the critical section gives rise to what
was popular around 2008 known as
minimally locked Hughes which probably
means they were known in 1960 is
forgotten after that basically you copy
the elements outside of the lock and
then you copy the pointers under the
lock works even better if you don't need
to copy the elements outside of the log
because the clients memory for those
elements is perfectly fine here is
another one
probably was also known in 1960 but I
haven't found the reference it's an
array of atomic pointers to single
threaded cues you don't lock on the
element you try lock if you haven't got
to go to the next one there is a
weirdness about this cue you give up
what's known as sequential consistency
the elements that came in in certain
order may come out out of order that's
sometimes it matters sometimes you
actually can't tell the difference like
why can't you tell the difference well
two elements on two different threats
came off in order but the return
statement on the first threats told so
the second one actually finished first
so you may not be able to tell that they
came in came out in order sometimes you
can tell so
this this isn't always equivalent
sometimes sequential consistency matters
if it doesn't matter let's see how fast
that thing is this is a trial log based
one and you can just have STD Q sitting
in that array or you can have ring
buffers and the ring buffers are faster
so again these this this is one of the
ways to have a smaller critical section
this basically shuffles pointers within
critical section all the copying is done
advancing the head and the tail is done
outside now this comparison of lock free
and log based performance is very useful
in ways of getting understanding that at
least to me wasn't immediately clear
when I started there are kind of two
sides to the overhead of synchronization
on one hand how long does it take you to
figure out whether or not you've got
access to data in the case of lock is
just how long does it take you to either
get the lock or fail to get the lock in
a case of log free how long does it take
you to read however many shared
variables you want to read do your
compare and swap and get back yes or no
if you don't get access how long do you
wait or how much computation do you lose
that's the other side these two together
that's how much you cost it costs you to
synchronize they don't necessarily scale
the same way so in general for example
for log based ones for if you have a
very efficient lock it's very cheap to
find out if you were granted access or
not it's just one to one atomic read or
atomic exchange then you have
wait for however long it takes somebody
else to finish it lock free programs
unless they're just as simple as wait
free programs on one or two shared
variables will take longer for you to
figure out if you are the one who should
for luxury it's not whether you should
be getting access but whether you're the
one who gets to commit the results
typically but it will take you longer to
figure out if you got access to the
shared data structure if you're if you
get to commit your results but once you
do you you'll lose less time if you get
denied that access the interplay of
these two determines what's faster now
once you understand that you can
actually try to get the best of both
worlds and sometimes you will discover
that if you look at your log based
program it didn't have to be written the
way you wrote it it just was a natural
thing to you put a spin lock on ask on
for word list well with a forward least
you don't have much option but if you
could get inside if you had your own
forward list and could get into it you
could for example pool copy outside of
the of the spin lock some of these
natural patterns are there for a reason
they're very hard to avoid some are just
you know the paradigm the way of
thinking also again remember the
measurements must be relevant which
metric are you going after so I said how
long does it take you to get access and
how long do you wait if you don't that's
real time if you after cpu time then the
question is how how much CPU time did it
if you're after power how much did how
many CPU instructions did it take you to
figure out if you got access or got
denied and then do you idle or do you
compute if you are denied totally
different interplay spinlock would a
good spin lock would pretty quickly idle
you and mutex would idle you're almost
right away lock compare-and-swap loop
would keep you computing all the time
so how can you combine the best of both
worlds well look reprogramming naturally
encourages the equivalent of a very
short critic I put it in quotes there
are no critical sections and LOC
reprogramming but the equivalent of it
what you do inside the loop now logb a
lock free programming also naturally
puts your shared variables close to what
you're accessing instead of having one
log guarding the entire forward list we
had our shared variables on every node
sitting right there guarding access to
that node if you try to emulate that one
with logs you run a high chance of
deadlock well if you can arrange a
locking scheme on them maybe you want
now if you can do a trial lock instead
of log you can do something useful
instead of waiting with lock tree you
naturally do that finally I haven't
talked about it at all I don't have time
to talk about it there is a whole other
family of lock tree algorithms RC use
and related which are basically
combination of the two you release data
by atomically updating a pointer and its
lock free so it's collect like a spin
lock which you don't spin on very few
shared variables very quick access and
it's log free don't have time to talk
about those things
the challenge there is memory
reclamation if you can solve that
they're incredibly fast
ok almost some time some guidelines when
do you have a chance of succeeding with
lock free code well first of all you
have to be sure that your performance is
critical second if you have data
structures that don't have like a
bottleneck for access like a stack
everything is channeled through one
fixed point guard it was a log hard to
beat that
distributed access the last slide
distributed access very good chance that
you will log free will win if you have a
ah threads you can probably get looks to
slow down enough could be more than you
will ever have or not measurements must
be relevant if your synchrony lock-free
synchronization protocol uses one or two
shared variables and especially if it's
weight free go for it it's probably very
simple how complex can you get was one
or two shared variables invisible with
wait for instructions it's going to be
simple it's gonna be fast go for it if
you have to wait can you do something
else useful instead if you can maybe a
totally not not log based not log free
based but a trial log which is a kind of
lock for a lock free approach will be
the fastest of the third will be the
fastest of the two well I started with
the you know decision diagram for how
people really do it so I have to show
you kind of a improved decision diagram
but basically shows what the conclusion
slide was and with one notice if you are
going for lock free code try to
benchmark your prototype your main
primary case synchronization schema try
to benchmark that as soon as possible as
soon as you can get it to like work more
or less correctly within the benchmark
so you can't hand a lot of memory case
restrict your benchmark so it doesn't
run out of memory just throttle it you
can't handle empty queue okay slow down
your consumer so they never drain it
fully and measure the performance of
your may of your main case
internalization schema or maybe if your
queue is normally empty then just
measure how long it takes you to figure
out that the queue is empty and so on
but do that as soon as possible hope
free doesn't guarantee the best
performance and writing correct lock
free code is hard be sure that you need
it so try to profile it as soon as
possible Oh after you finish writing it
profile it again
that's all thank you I'll take questions
okay the question was not just you I was
also shocked when I saw that atomic that
the spin lock was faster than a lock
free q the question was is it because in
order to support multiple consumers I
had to run a compare and swap elope well
as soon as I started to run
compare-and-swap elope I was like way
way off you know basically compares what
loop doesn't have to apply my weight
free program was depending on the
hardware a little bit faster or I've
shown you the weight free q just the one
consumer thread q on that on that piece
of hardware that I was using to
demonstrate it was had exact same
performance as a spin lock you on an
older piece of hardware it actually was
a little bit better probably not enough
to justify therefore of actually writing
it by the way one note I noticed
throughout doing this work is the newer
your hardware is the more likely it is
that the spin lock will win last time I
did this work was three years ago
Nehalem and West mayor course lock-free
was beating everything by a mile
once the on the latest Ivy Bridge I have
a bridge revision three this is what the
most of the results that I've shown you
came from there if I didn't say
especially spin lock is pretty good on
house well it's better you don't want to
know what happens on skylight
not exactly the Haswell supports the
question is about transactions and has
well ran app to support transactional
memory and then it kind of crashed and
burned it did support lock hardware lo
collision transactional memory is backed
by the way on the latest on the broad
well and on skylake I haven't tried
transactional memory yet I don't have
any hardware that does it yet I do have
hardware that does local Asian hardware
localization which is X acquired X
release the results are inconclusive at
this point so more research is required
you know I've been in the industry for
what almost twenty years since I left
academia I don't think I've ever
presented to the conference where I said
more research is required is this first
time from it when I was in academia I
mean not to not say more research is
required is like saying I don't want the
next grant but more research is required
yes you mean a deadlock avoidance you
mean the deadlock avoidance oh okay so
like a shared memory lock aynd of
situation work okay so the question is
there are systems where essentially you
have kind of out of process log like
shares to shared memories an example
distributed system basically the you
where essentially you can take a look
the process that took a log can die and
that doesn't bring down the whole
program like if one thread dumps core
the whole thing dumps core so not an
issue for normal mutex for shared memory
for example now for lock free first of
all it depends on the synchronization
protocol sometimes you still get stuck
sometimes you don't it depends on which
synchronization protocols you end up
with well you don't get stuck you lose
you lose you basically you will you will
never recover access to a portion of
your data structure depends on the
synchronous
schema there are you know mutexes that
have that like for example there is
shared memory mutex platform-specific on
some platforms there isn't some
platforms there isn't there are time
mutexes there are health monitors so yes
now lock-free
is in general becomes a lot easier you
know basically the complexity of log 3
versus the complexity of alternative
becomes kind of the gap becomes much
less and lock-free could become even
simpler if the synchronization protocol
itself is not tremendously complex but
there are log based alternatives which
basically have to do with either with
notifications or was monitoring or with
weights these are your three options yes
okay the question was what was the Numa
architecture of these systems and would
that make a difference this the big one
had eight Numa nodes I believe either 8
or 4 if you're interested I can log in
and check but it has either a 2 for Numa
nodes I tried some other ones that had
only 2 for just hammering on a single
shared variable doesn't make I mean they
all have to get there so for that
doesn't make a difference now once you
get access the memory that you get to
touch now in the privacy of your own
thread there's nobody else contains with
you for that it can make difference I
have I have seen less difference again
on the recent systems I first again
measured on a Westmere ax which had 8
Newman nodes on 128 physical cores it
made a lot of made like two and a half
times different worst case the best case
on these has walls and Ivy bridges it
you have to be really careful measuring
it there is some difference but it's
non-trivial to measure and it's you know
you have to accumulate an update
a--basically
to get noise low enough before you can
see it so there is difference it also
depends on your memory allocation and
use patterns so for example if you are
allocating memory on one thread and then
reflow it to other threads your memory
performance is slightly different than
if the thread that allocates it uses it
and releases it it depends on what your
kernel is doing like if you knew my
extensions are enabled and if they are
what what your kernel policy is if I've
seen both successes and spectacular
failures in kernels trying to be helpful
I've had a Red Hat kernel that was
actually decent to give me about 20%
perform performance bonus when I turned
on the new my extension I've had Sousa
kernel they fixed it since then that
like did exactly the probably the worst
possible thing and basically put all the
memory in the worst possible location so
turning on Numa extension made it about
40 percent slower so not so much for the
synchronization protocol itself but for
the access to the concurrent to the
concurrent data structures it makes it
makes a difference how much depends on
both the hardware and software yes yes
Michael
well by weight free I mean when do have
to run compare-and-swap loop or not if
you're not running compare-and-swap loop
you're going to your odds of actually
having superior performance in lock free
code are dramatically better
compare-and-swap loop is expensive
hazard Michael mention hazard pointers
hazard pointers are more complex but far
superior performance alternative to that
list that I was showing you Michael is
going to say the hazard pointers will be
presented on Friday they are harder to
get right it's an alternative way of
never deleting nodes that you shouldn't
be deleting it's a higher performing way
of doing it it's more complex
I haven't benchmarked the hazard pointer
implementation of for this so I don't
know now hazard porters themselves are
weight free well you know measurements
must be relevant but have to benchmark
on the same hardware to actually have
correct comparison but yeah if you have
some code that will be happy to you know
benchmark and compare with exactly it
was on the same machine just so we can
put it in you know in the hierarchy of
different numbers hazard pointers as
mostly weight free implementation well
there is still compare-and-swap loop on
the head but memory reclamation now
becomes weight free which Farzin
pointers are likely to be well unknown
not likely known to be much faster are
they fast enough we have to measure and
see any more questions oh yes
okay the question was how do you
actually prove correctness once you
decide that you're done testing broken
code and correctness actually starts to
matter okay I don't know if a way to
like prove prove it what I found to be
perhaps in terms of like you found bugs
well other than you know the obvious
explain your code to tube two to
somebody else other than that as far as
tooling teason in latest clang or GCC
the threat sanitizer
so threat sanitizer is basically race
condition detector but what it does the
interesting thing about that is it
detects not just the rest conditions
that happened but the race conditions
that might happen so if you read and
write the same location also you write
first and read later so nothing bad
happen but you didn't have a memory
barrier or a lot between them descent
will tell you that it's a potential race
condition even though it didn't actually
happen at track set so in this in this
regard it's not it's basically it less
dependent on luck you have to actually
do read and write it doesn't analyze the
program to figure out that you might do
a read you have to do a read and the
write of the same location but at least
you don't have to actually hit the race
condition so latest versions of design
understand Atomics and memory barriers
and I have actually you know I I have a
demo for a different okay where I have a
correct program I go and change memory
or the release to memory or relaxed and
ranty Sun and dishonor that still is not
going to catch abs yes so that is going
to catch like true race condition your
accessing memory that you are not sure
if it's finalized or not for ABA
well it may or may not catch abs it
depends on the synchronization protocol
that happened in the way you reclaim the
memory if for example if you have a
simple free list of your freed blocks
and you're pulling them on without
additional synchronization it will catch
it just because that block isn't even
guaranteed to be there so it will catch
that so it depends but if you hit a full
music if you go to malloc and you hit a
fuel full mutex there of course the
barrier has been acquired after that
everything is synchronized well if you
are not freeing memory at all you can't
have a BA okay so okay so that one is
actually alright so the question was on
a ring buffer if you're if two threads
are basically on different generations
of rotation one of them is full rotation
ahead of the other
it basically appears to you that the
queue is empty whereas in fact it's full
essentially that's a manifestation of
their that specific one I don't know of
any way to catch especially since most
implementations of a ring buffer will
not loop back they will let unsigned
long run forward all the way out and use
the bit mask so then sign Long's aren't
actually even equal just a bit mass
portion is you're not going to typically
to you know you're just going to heat it
with atomic increment over an hour
you're not going to bring the atomic
variable down you're just going to bit
basket okay they're saying that I have
to stop so that particular one I don't
know if any tool tool that will tell you
if you have any more session is over but
I'm telling you but if you have any more
questions I'll be happy to answer them
as long as you have them</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>