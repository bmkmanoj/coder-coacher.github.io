<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: D. Rodriguez-Losada Gonzalez “Faster Delivery of Large C/C++ Projects with...” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: D. Rodriguez-Losada Gonzalez “Faster Delivery of Large C/C++ Projects with...” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: D. Rodriguez-Losada Gonzalez “Faster Delivery of Large C/C++ Projects with...”</b></h2><h5 class="post__date">2017-10-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xA9yRX4Mdz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Good afternoon, thanks
very much for coming
to this insanity of talk about DevOps.
If you though that all the
templates were difficult,
try to prepare a talk about
DevOps with live demos.
So this talk is totally
undefined behavior.
If everything works, it will be by chance.
So I am Diego, I am one
of the Conan creators,
doing it with Luis, my colleague there.
And we have recently joined JFrog.
So I'm going to talk first about
the challenges that we face
when we want to do continuous integration
with C and C++ projects.
And then I will start
from the easiest things
to the more complex
programs that we can face,
and I will try to provide
a complete solution
to the problems that we want to solve.
So the first thing, is
that C and C++ projects,
because for me, it's basically the same
regarding continuous integration.
They are typically very large,
otherwise we wouldn't be here.
So one thing with large projects,
is that typically we want
to manage the binaries.
So we don't want to be building
again and again from sources,
so we have to manage our binaries,
we have to store them
somewhere and use them.
Our developers but also our
continuous integration systems.
And this is problematic,
because we have the
binary incompatibility.
So if we build a binary with
a certain compiler version,
probably it won't link with
a different compiler version.
Or with different settings,
or with a different standard library.
So we have to be careful
about our configuration file binaries.
And also, if we are doing
more than one platform,
and we are doing Linux and Windows,
our problems just increase in complexity.
And one of the challenging
things that we face
in C++ is the use of code inlining.
I think no other language has this issue,
it's basically this thing.
If we are developing here a
very simple static library,
like this one is just
adding two integer numbers,
and we build this static library,
we will see that the
Semver code is very simple.
This lea operation is just an optimal
version of the adding two numbers.
So this is my static
library, okay, that's fine.
But now, I'm going to
build another library,
another static library, Math3,
this is going to use Math2,
to add three numbers.
And it's going to code
twice the add function,
to add those three numbers.
Okay, so if I build
this library static too,
then I have this Semver code here.
And what I've got, I've got two calls,
two other functions that actually lives
in the other library.
So this is the call
and this is the return,
two times for each invocation.
Okay, but the call is actually not there.
So when I'm building
my finally executable,
then it will link both together
and the linker will do the thing.
So, if I go now to my math tool library,
and I fix a bug, or I develop something,
I improve something in my code.
In this case, I'm just adding a one there,
and then I rebuild my math tool library.
My code for the static library
Math2 will be different,
of course, taking into
account this new one number.
And if I rebuild my Math3 library,
the result in Semver,
will be exactly the same.
Why, because it's just
linking the other function,
that lives in Math2.
So everything's fine.
So if I rebuild my final application
with these two libraries, it will be fine.
Because it'll be taking the modification
I just did to Math2,
okay?
Everything good.
But what happens if, for example,
I build
Math3,
then I'm gonna share that library.
Then, the static library Math2,
will be actually embedded in the final,
because it's a variable,
it's an executable code.
So here, we can see
that our code for the add three function,
you can see the two calls.
The two calls are not calling
the static library anymore,
they are calling the function
that lives inside the static library.
And this is a problem, because if I go,
and I go to static library,
I modify and I rebuild,
and I rebuild my application,
it's good to use,
the code is inside the static library,
not the code that I have
rebuilt in the static library.
And
the thing is that it
also happens for headers.
And nowadays, everybody's
using headers with very, like,
inlining things is very
optimal for performance.
So in this case, well,
I didn't add the inline,
but it is actually
doing the inlining here.
So here, I just move the Math2
functionality to the header.
So when I
build my Math3
library,
then I can see that the Semver
is not calling at all the Math2 function.
It has in length everything,
so it is doing the operation there.
So what does this mean?
If I change the header,
and I reuse the static library,
it is using the old functionality
so I should rebuild.
And this is a problem,
we will see how can we build in the system
with continuous integration,
trying to build optimal.
Okay, so if we are doing
continuous integration,
and we have to manage the binaries,
I'm going to show you first how to create
a basic package for a certain library.
With Conan, of course.
So here, we have a library.
A library, and we have functions
that is going to print some
code to the standard output.
It's very simple, and also the CMake
file that is building this application.
This, sorry, this static
library, is also very simple.
So the code is there, I think
everybody understands it.
So if I want to create a package,
I cannot package with this library.
All I need is to add a conanfile.py
file,
to my repo, to the repo
where my codes live.
How is this Conan file?
It's something like this, okay?
So I'm going to briefly explain it
so we understand what is happening here.
The first important thing here,
is the declaration of settings.
This means, okay, if I change my compiler,
if I change my build type,
if I change my architecture,
I'm going to have a different binary.
And for every combination
of different configuration,
I'm going to have a
different binary, okay?
This is important, we will see it later.
Then I have a build, the
build method is very simple.
It's just calling my build system.
In this case, I'm using CMake,
because everybody understands it.
But you can just directly
call Visual Studio solution,
or any build system you want
to wrap, you can just wrap it,
and then create packages
from your build system.
Then you will have a package method,
that is just extracting the artifacts,
in this case, the header
and the static library.
All the code and
everything, is for Windows.
So I told this is going
to be undefined behavior,
I'm doing DevOps in Windows in live demo.
So this is going to be,
so I'm using here LIB extensions,
but you can use also A extension
if you're working in Linux.
So it will work in every platform
with very little modifications.
So this is the package method
that is extracting the
artifacts, header and libraries,
and putting them in the final package.
And finally, we have
the package information.
This is the information for our consumers.
So when consumers, they
are using this package,
they know that they have to
link with the Hello library.
For example, any pre-positioned
definitions we want to have.
Include directories, library directories,
library names, code binary flags
that we want our consumers to
use, we will put them there.
It will be equivalent to PC
files more or less, in Linux.
Okay, so with this file, I
can just create my packages.
I am going to show you
first in the Command Line.
So I have a repo here.
Okay, with the Conan file.
So I can show you that I can create
user/testing.
And it's going to build my package.
Okay, so now it's calling CMake.
It takes a while with a lot,
Visual Studio is kind of slow.
And finally, I have my package.
So I can inspect,
well, actually I can repeat this process
for any configuration I want.
So, in this case I just used my defaults,
that is Visual Studio 2017,
64-bit, release whatever,
but if I want to build
for another configuration,
I just tell this,
okay, I want to create a
package, but in this case,
I want to create a package for 32-bits.
So I would just pass the
architecture as an argument,
and then I will create
a package, for 32-bits.
If I inspect my Conan local cache,
I will see the library
here, allow package here.
If I inspect this package,
I will see that I have two binaries.
I have one binary here for 32-bits,
and I have another
binary here for 64-bits.
So right now, I could just,
okay, those binaries are cool,
I want to share them with my team,
so I could just do Conan upload.
Okay, and upload it to a server.
But I am going to do it
with continuous integration.
So what I just did, is I developed,
I created locally in my
machine, those two binaries.
So I have a package recipe
that is the Conan file,
and I have two binaries for
two different configurations.
Okay?
So those binaries, they
have an identifier.
So the identifier for each package binary,
is just the hash of all the settings,
and everything that can affect my binary,
we will hash inside one signature,
and that will be the
identifier of the packages.
We'll be using that to boost those
with a sort of path, for example.
So, in this case, if you
change the compiler version,
the static library, for example,
or you are building the
static above dynamic,
it will change and you will
get a different package
at the end.
Okay, so when you have
your binaries local,
and then the next step is
to upload them to a server.
The server, Conan also have a Conan server
that is also open source with MIT license,
it's fine for small and
medium-sized things.
But, it can also work with Artifactory.
It has integration.
In this case, in this talk,
I'm going to use Artifactory,
because Artifactory has a web UI,
so we can see the package there
in our cool web interface.
Okay, but you can use both.
So, then this upload
is what I'm going to
automate with Jenkins.
Okay, so I'm going to create
my package with Jenkins.
So if I want to, this package,
to be automatically created
in continuous integration,
so the easiest thing is
to add the Jenkins file
to my repertoire.
So I will have the Jenkins file,
and in this case I'm going to
use the new pipeline syntax,
so you know Jenkins used to to
use another,
but now they are promoting,
very heavily, to use pipelines.
But actually, they are
actually Groovy scripts, okay?
So a pipeline in Jenkins
is something like this.
It has some stages, the first is to,
one is very easy, just
check out the source code,
undefining that second state
for continuous integration
is create a package,
so just the line that I
wrote in the Command Line,
is going to be executed by Jenkins here.
Okay?
You can see that I'm running here,
client.run,
and the client.run,
client is a Conan client.
So I'm using here,
a plugin.
So Artifactory and Jenkins,
they have the Jenkins Artifactory plugin,
that will implement this
helper, this DSL for us.
But if you are using
any other build system,
just in your scripts for
your continuous integration,
just call Conan create, and
the package will be created.
And finally, in the last
stage of the pipeline,
Jenkins will upload what it just built
to the remote, to the server, okay?
Finally, if I want to, for example,
to build different
configurations at once, or,
but if I want parallel, for example,
I could use the parallel
features of the pipeline.
In this case, I want
to build both 32-bits,
64-bits
architecture in Jenkins,
so what I can do is, okay,
I can just launch two nodes
that will be identical
to this here,
okay, two signs, but the
first one will use the,
x86
architecture,
and the other one will be
using the 64-bits architecture.
So those ones are basically the same,
they are launching parallel
and they will create both packages.
So, we are going to try,
here we can see our Jenkins instance,
this is the LibA pipeline,
everything is empty now.
So what I am going to do,
is, okay, I have my library here,
I'm just going to introduce a fake commit.
So I'll sort...
Oh, sorry,
wrong repo.
Here...
Okay, so I just committed
some commit into my repo,
and now,
Jenkins will detect this.
It takes a while, it takes
on average half a minute
to detect that the changes
have been introduced,
because it's using cron jobs.
(audience member asking
a question off mic)
No, it's not necessary,
because I'm running
everything locally, yeah.
So
I will configure
a pipeline
for the next library, yeah.
So it detects that it has a new commit,
and it will eventually launch.
Yeah, the bad thing is that the pipelines,
they are running with a Cronan
script, with Cronan syntax,
and with Cronan, you
can down up to a minute.
So on average you have to wait 30 seconds
or something like that.
Come on.
I don't know why
it won't launch.
(audience member asking
a question off mic)
(all laughing)
No, especially in Windows.
Yeah, it worked like two minutes ago.
I'm just going to build,
to launch the build binary.
Now we launched to the--
(audience laughing)
Yeah.
The BMO effect, so it's
going to build twice,
both configurations.
So now it's building in
parallel both architectures,
32 and 64-bits, two times.
So actually in every row,
you are building both architectures.
Okay, so the only thing is
that it is building twice.
Okay, everything looks good.
Oh, I don't know why this is failing.
In any case, I'm going
to check in Artifactory.
Yeah, and here it looks good,
I have here my package,
my binary for 32-bits,
and here I have my
package for 64-bits, okay?
So now, I have set up
my coders,
they can just do Git pushes,
and Jenkins will fire a
job or a number of jobs,
and it will create packages
for all the architectures
and all the settings I want to configure
in my continuous integration.
Okay, so this is what I did.
Just some changes in
the code, going to keep,
will go into Git, Jenkins,
and then Jenkins finally
will upload to Artifactory,
my final packages.
So other users or other developers
for my continuous integration
system can use them.
Okay, but what happens to
packages that have dependencies?
Okay, let's go for a new one.
Library B, that depends on the library A
that we just created.
So the B
will be
including A,
and it will be calling A.
Okay, very simple.
So regarding the Conan file,
all I have to do is to
define the requirements.
So here, I will think, okay,
so I'm requiring LibA,
and I'm requiring LibA version 1.0.0.
Right, good.
Then also I need some
connection to the build system,
in this case, I'm telling Conan,
hey, I'm going to build with CMake.
Please generate a CMake file for me,
so my CMake scripts, they know
where my dependencies are.
So, the CMake list will
do something like this.
It will include the file
that Conan is generating,
that is typically full
of all the include paths,
library paths, library names, variables,
so it can be used into my build.
In this case, Conan will
be defining the targets
with some other CMake syntax.
So my B library can
just link with a target
defined for LibA.
Okay, the Jenkins file
is exactly the same.
So nothing new here.
So let's try to...
LibB,
okay,
and then I'm going to commit.
But the thing is that I
didn't define a pipeline yet,
so I wanted to show you
that actually the pipelines
in Jenkins, they are very easy to set up.
Because I all I have to do is to define,
okay, I'm going to define a new item.
This is going to be called LibB,
and I'm going to do a
multi-branch pipeline.
Okay.
So the defaults are,
all
are good,
or almost good.
So I'm just going to add my source repo.
So that's why it worked before,
because I'm just
doing here my local repo.
And in this case it
will fire automatically
because I just created the pipeline.
So when it works, and builds LibB,
what is happening here?
Okay, so Jenkins has decided
that it wants to build
LibB.
And it has the source code,
it has the build system,
so the first thing it will do,
it will be to retrieve the dependencies.
Okay, as B, requests A to
build, the first thing is,
okay Artifactory, please
hold the server, I need LibA.
So if we retrieve a binary for LibA,
so when you are building LibB
in continuous integration,
you don't need to build
LibA again from sources.
You are using the
binaries from Artifactory.
If we retrieve all the dependencies,
if there are positive dependencies,
it will retrieve all of them.
It will build a dependency graph,
and it will order things and output this
conanbuildinfo.cmake file.
This conanbuildinfo.cmake
file is the one that is loaded
by the CMake
file of the
LibB
project,
and it will be used to
build a LibB binary.
And once we have the LibB binary,
then we are uploading it to Artifactory.
Why, because we are using
Artifactory as our truth.
So all the artifacts are
always stored in Artifactory.
I know we could use another mechanism,
like ccache for example,
and build locally.
But this approach works
for every platform.
In Windows, you can just use Artifactory
as your source of truth
of binaries, all the time.
And downloading and
uploading is very fast.
So,
the thing also, that when this is working,
okay, it is returning dependencies.
Of course, it only
retrieves the dependencies,
the binaries that it needs.
So in this case, if it's building LibB
for 64-bits,
it will retrieve
the binary for 64-bits.
Okay?
This matters, because
sometimes you have up to
hundreds of binaries, depending
on what you are doing,
there are Conan users that they have
hundreds of binaries for the same library.
So if you are doing continuous,
you don't want to download 100 binaries
just to link with one of them.
Okay, so let's see,
I hope continuous
integration has finished.
And now I should have
two packages here, LibA and LibB.
And for LibB, I would also get the 86,
and 64-bits architectures.
Okay, so let me
launch while I speak.
I'm going to bootstrap
some more libraries.
Okay, looks good.
So now suppose that we have
a more complex application,
something like this.
Okay, so I have a full-dependency graph.
I have this typical diamond-structure,
they are important, because
they can lead to conflicts.
So if I go to a
certain version,
or on one branch of the diamond
and I get to a different
one, then I have a conflict,
and I have to solve it, okay?
So this would be an example
of an application project
I want to build.
The app code will be very simple.
In this case, it will
be calling just C and D,
if you check here,
we have C and D libraries
that depend on B, okay?
So my application will
be just calling C and D.
You can see here the main
function is calling C and D.
And my
CMake file
will just,
in this case, link with two targets.
The target for LibC and
the target for LibD.
The rest is exactly the
same as LibB, very simple.
So, to build this application,
I need also a Conan file.
So in this Conan file,
it's also very similar.
I only have to add in this case,
I want to depend on two
libraries, LibC and LibD,
so I declare both of them.
Okay, I'm adding also a line here,
in the build method, to run
the binary that I just built.
It's just for convenience.
So I'm going to build things,
and I want the output of
the running application.
And also the packaging,
but it's not important.
So let's check if this
has finished, looks good,
and everything has been
uploaded to Artifactory.
Okay so I'm going to show you
I'm moving to a clean
folder to build my project.
Okay, so what I'm doing here,
I'm not going to create a
package, I want just to develop.
So I'm going to use this Conan file,
just as a recipe to
retrieve my dependencies.
But I will be working locally,
I'm not creating a package anymore,
so I will be just Conan
installing my dependencies.
Okay, I called my dependencies,
and I'm going to execute Conan build.
This is just a local
build of my application.
It will be the same if
your run CMake manually,
it will be exactly the same.
Okay, so here I can see the output,
here of LibA is being called twice,
because of
any side
of the diamond,
okay?
So everything's working here.
So I could repeat the same thing,
I could just wipe
this
Conan
install,
for
32-bits,
Conan build,
and what I'm doing, is I'm just
retrieving my dependencies,
for 64-bits, for 32-bits.
And I'm calling one or the other, okay?
So for me this is amazing,
because actually I can have
a different dependency graph,
depending on my architecture,
or depending on my operating
system, or whatever.
So I can check here, for example,
my dependency graph,
or, sorry.
Two dashes.
So here you can see my project
here is depending on C and D,
which in turn depend on B.
So the graph that I
showed you in the slides,
is just what the tool has given me,
so you can check that this is real.
Okay, so, so far,
so good.
The challenge, the real challenge,
is when we start to
evolve our dependencies.
So what happens if I now go,
and I do an amazing
improvement to my code,
and I
generate these also message.
Okay, good functionality,
I'm going to release a new version.
I'm going to release version 1.0.1
for LibA.
So I go to my Conan file
and I bump the version,
and I create a package.
Sorry.
Release
1.0.1.
Okay, I got this, then I do my commit.
And I'm just going to find it manually,
so we don't have to wait.
And now it is building my new version.
It's building 1.0.1 for LibA,
that includes the new functionality.
So now in a while,
we should see
that for LibA, now we have two packages.
We have version
.0 and version .1.
Okay, and for each one,
I have two binaries.
Okay, for 32-bits and
32-bits architecture.
Okay, so what I have right now is this.
I have created two packages, sorry,
yeah, two different packages
for two different versions of LibA.
But if I go to my application
and I build my application,
is it still going to
show the previous code,
because my dependency graph is that.
Okay, so what could I do?
The typical thing,
and the thing that most users are doing,
is this.
Okay?
So it will be the manual
update of the dependencies.
Okay, so let's say that now I want to use
the new functionality from A,
okay, so I will go to B,
and I will bump here
my requirement here to LibA.
Okay, and then I will bump
my only dependency of LibB.
Okay?
Actually, typically you will also do
some source code changes,
but not always.
So you can just
bump
your version,
and fire your build,
and you will have this here.
Okay, so we are going
down the hierarchy here,
one by one,
it is good, it is another way to do it,
we will be testing everything,
but we will be slow, okay?
Is it possible to do
something so we can go faster?
So here the challenge is
how to update dependencies
when the source code doesn't change.
Actually in this example,
I haven't changed the
source code for B at all.
The source code for B is exactly the same.
So what can I do?
There are different ways update
your dependencies upstream.
The first one is to just basically
just bump the versions
manually one by one,
and keep running.
There are other ways, for example,
there is one that would be server-based,
or let's say defined
by the package creator.
You can have an alias.
It's basically like symlink,
so the package creator defines,
okay, I have this,
for example version 1.0
and this is a symlink,
this isn't an alias,
for the latest version
of the package actually.
So every time he created
the package,
he can update the alias.
And updating the alias,
it will make the consumers of that alias
to retrieve the latest
version of the package, okay?
So this is something to
retrieve the latest version
of a certain package,
but define it by the package creator.
Then we also have version ranges.
Version ranges is the typical expression,
you can use the approximate, approx,
of a node Semver, for example.
You can use and, or,
different conditions there
between the brackets.
Version ranges work nice,
but typically this is slow for example.
Because it has to check the server,
to check all the versions
that it can match,
and then every expression, okay?
So in this talk, I'm going
to use version overriding.
Version overriding is something like this.
So when I'm developing my app,
okay, so this is the
Conan file for my app,
it's the downstream final package
of the dependency graph, okay?
So I'm going to introduce
another requirement to LibA.
And I'm going to force LibA,
I want to depend on LibA version 1.0.1.
That is the one that I want to test
for my final application.
And then I in here, an
extra word is override.
Why is this word here?
Because if I don't add this override,
my dependency graph will change.
I will add an edge
between my app, node,
and my LibA node.
Okay, so,
it can matter for example,
it can affect the order of my libraries,
you know, when you link your
libraries have to be in order
to properly link.
So if you are doing these kind of things,
you are changing your
actual dependency graph.
And if you are doing some analysis of A,
why my app is actually dependent on A,
it is not dependent on A,
it is dependent on C and D.
So I don't want this edge
in my information here.
It won't help me to refactor,
it won't help me to move.
So that's the reason we
can specify override,
and if I prefer override,
it will only change upstream, okay?
This can also matter
because Conan has also
conditional dependencies.
It will have dependencies that can change
from operating system
from operating system.
Typical case, elective files in Linux.
But you don't have it in Windows.
So you have packages that will depend on
elective files on Linux
and nothing in Windows.
So your dependency graph actually changes,
from configuration to configuration.
And in that case, you definitely
don't want to introduce
a forced requirement upstream,
because you are just trying
to update the dependency.
So the override is, okay,
if you are upstream, define
dependencies to LibA,
use this one.
Use LibA 1.0.1.
So the override,
sorry.
Yeah, here.
Here the override works like this.
Just as I showed you.
So I'm going to update my app,
and this is Conan install,
you can see here that now I
am depending on LibA 1.0.1.
Good, so now we can see that we are using
the new method from LibA.
So my whole dependency graph,
is working with this new LibA.
And I didn't have to
rebuild any of the nodes,
I didn't have to rebuild B,
I didn't have to rebuild C or D, okay?
So everything linked
and everything worked.
But,
this might not be the case always.
For example, something that we do a lot.
Let's do something more amazing,
and let's do optimization.
So I'm going for a new version,
and I'm going to inline the message.
Okay?
As this is a
bigger change,
I'm going to bump my version to
1.2.0.
Okay, good.
Let's do it.
First I'm going to create
the package for
1.2.0.
Okay.
Let's check that it is here,
good, I have the three versions here.
So now, I can go to my project,
I can override and I want to say, okay,
I want to build with
1.2.0.
Let's try it.
Oops.
So something failed.
What is happening here?
So where did I get this link error?
And actually, I was very
lucky that I got this error.
Yeah, you know it, you know it.
Otherwise the debugging the opposite,
it happens a lot, is a nightmare.
So what is happening here?
I have three versions of LibA.
First two, they have in their native code
of the static library,
I have implementation for A.
Different, but I have
implementation for A, okay?
If I go to the third version 1.2,
I don't have an implementation for A.
Because the implementation
for A is in the header.
Okay?
And then for my library B,
static library that I built,
it had jumped code to static library A.
So when I link my final
application with LibA 1.0.0,
it links with it.
And it will call,
my application will call
the code from LibA 1.0.0.
And if I link with 1.0.1,
it will call the code from 1.0.1.
And everything works.
But, if I try to link
with the
third version,
it won't find the A code
and it will fail, it will.
So
why I say that we are lucky,
because if we just did changing
the code
in the inline
header,
my code will link,
but it will run the old
version of the code.
And then I will have to debug
and know what is having
that run time, good luck.
So what do I need?
What I do need is this.
If I want
to keep my
library B,
and I don't want to bump the version,
because I haven't changed the
code for B, the source code,
I need two binaries, okay?
So I need a binary, okay,
that will be able to link
with the first two versions.
And it will contain a jump
to the static library of A.
And then I need a different
binary for library B,
that has the code inlined
of the library A,
inline version.
So this, okay.
How can I manage to do this?
Okay, when I presented
the package ID,
I omitted the latest thing.
So one thing
that changes our binaries,
are requirements.
It's something that doesn't happen
in any other programming language, okay?
And (laughs) yeah.
So
how
do our dependencies affect our binaries?
We don't know (laughs).
So what we did in Conan, what it's doing,
is okay, it this ss1 Semver, by default,
okay so if you're bumping
the major version,
you will need a new binary.
But it happens that it
might not be good always.
So,
for that,
Conan allows to configure your binaries,
the binaries that you need.
In this case, for example, I
will explain with the settings.
We have here the default
package ID, for settings,
and the default behavior is, okay,
for every compiler version,
you will get a different binary, okay?
So for gcc/4.8, you will get a binary,
for gcc/4.9, you will get a binary,
for gcc/6.2 you'll get a different binary.
Okay?
And that's true, the compilers,
even if they are compatible sometimes,
you'll get a different binary.
Because probably they implemented
some optimizations or whatever,
so the final binary you get, is different.
But what happens if it's a real use case?
Some people, they are
packaging a C library, okay?
I say hey, my API will break,
I am fine.
I want to build my library
with gcc/4/8, for example,
and I want it to reuse that binary
for all my compatible versions.
And I want to do it, I don't
want ten different binaries.
I want one.
So in that case, with a
package ID you can do it.
You just override the default behavior,
I say, okay, so the information
is going to affect my hash,
my package ID will be
instead of using the version,
you use this string, use any.
So what you will get here,
you will get, okay, so
with this package ID,
you will get just one binary.
Of course, one binary for this,
you are going to Windows Visual Studio,
you will get a different binary.
But for this setup, you will get just one.
So the same can be
applied for requirements.
The default behavior for requirements is
Semver.
So every time you change
the major version, okay,
you will get a new binary.
What happens
that I think in C++ we
don't follow (laughs)
semantic versions at all.
Why, because if we did,
we would have version number
three thousand hundred,
that would be the major.
Every time we break the
API, I think, like that.
So we typically tend to bump our,
the major is typically our minor in C++.
So if we went, okay,
so I want this behavior.
I want anytime my dependencies
bump the minor version,
I want a new binary.
It's just what we build.
So you can specify in your Conan file,
you can specify that
with this syntax here.
This is just
a helper.
You can define your own
logic here if you want.
But what it means, is this.
Okay,
when it is evaluated,
if it is dependent on LibA 1.0.0,
or is it dependent on LibA
1.01,
then the patch is discarded.
So you get the same
result, you get LibA 1.0.Z.
And this always hashes
to the same package ID.
So for both cases,
for those both minor versions,
you will get the same package ID,
that will be the package ID of the binary
containing the jump that is able to link
with the static library.
And in the other path here,
if you are requiring library A 1.2,
it will be changed to library A 1.2.Z,
and it will hash to a different
package ID.
And this will be the
package, the package ID,
that has the implementation
inline from the header.
Okay?
So now, let's try to,
to do it.
The LibB...
I'm going to LibB,
I'm going to modify my Conan file.
Commit.
Lib Commit.
And I'm going to fire
LibB,
sorry, come on.
(audience member speaking off mic)
It doesn't matter.
It is the comment of the commit.
It will work.
So let me just,
build.
So what I am doing here, I'm just,
right now I'm overriding.
I introduced a change,
I submitted without bumping the version,
so I am rewriting my old binaries, okay?
This also can be done.
So now, it is,
it is rebuilding
my LibB.
And I'm going to try and
use it from my application.
Let me make sure that
everything is clean here.
Yeah, sorry, I forgot to,
to add the,
oh, no, the override is already 1.2.0,
so I'm going to install.
Yep.
And now I got a different message, okay?
It's still an error, but
this is a good error.
Because now it's telling me
that you don't have a binary for LibB.
Hey, I just created a binary
with the continuous integration.
Why, why that don't have a binary?
Because now, I'm trying
to use a binary of B,
that will link with LibA
1.2.0.
So I need that binary,
nobody built that binary.
So what I can do here,
is I can do myself.
Okay?
Just,
build that missing library.
Build that missing package.
Okay, and now it's working.
And the important thing here,
is that I haven't messed
up with my binaries.
If I want to
revert to my old 1.0.1,
I just do the override,
and it will work without
rebuilding anything at all.
It will manage all the binaries,
and we can check,
in our cache,
that for library B,
we have this.
We have two binaries, and you can check.
Here, we have a binary here
that is requiring LibA 1.0.0,
and I have a different binary for LibA
that is requiring library A 1.2.0.
Okay?
Yeah.
So now, I'm going,
this is the interesting part
regarding the C and C++,
is almost done.
So here I wanted to just do,
to do some proof that what can be done,
but this is mostly just Jenkins, okay?
So I'm going to do it quickly,
so I prefer to keep some
time for questions, okay?
So actually, if we want to scale this,
to brace that instead of
having five we have 50,
or 100 dependencies,
I will like something like this.
Okay, I want to test my whole application,
rebuilding with library A 1.2, okay?
But I don't want nodes like
LibA, LibE, for example,
that is not affected at all, I
don't want it to be launched,
I don't want it to be configured, okay?
I want to launch every
single node in a Jenkins job.
So I can visualize all
the hierarchy going, okay?
And I want to launch them in parallel.
Okay, so what can be done here,
Conan has a command that
is the Conan information,
Conan info,
will get the information
for the dependency graph.
In this case, you can ask Conan
what will be the build order,
you need to build such a dependency graph.
So you will tell, okay,
you have to start with A,
then you have to go to B,
there returns actually a list of lists.
Why?
Because a dependency graph
actually has directed graph.
So you can order it,
and you can order it by levels.
All the nodes that are in the same level,
they can be built in parallel,
because they are independent
between each other.
Okay, so in this case,
the algorithm detects
that C and D are not
dependent in each other,
so they can be built in parallel.
So with this,
just building a continuous
integration system
is very simple,
it will just use a recipe with some
tricky mechanism to define
the overrides I want,
and to compare them to the whole system,
but basically it's what you use also.
And we will be using a Jenkins job,
that will be firing all the children,
all the children for
every node of the graph.
So the script is like this,
don't think that these two have defined,
so it's something like this.
But if you check, most of it
is Groovy.
Comes from
Java and things it's,
I mean, it's verbose.
But here,
what you have is you are
just calling Conan info
to get the build order,
okay, it is being saved to a JSON file,
the JSON file has been
loaded, has been parsed,
and you get the build order as a variable.
And then,
it is just launching the child,
the children in parallel.
So it's doing nothing but
just launching things.
And the children,
it's always the same as we
saw for a single package.
It's just, the initial step
is a bit more complicated,
just to define an intermediate Conan file,
but the install and the
upload is basically the same.
Okay, so I,
in this talk, and
actually in the abstract,
I wanted to talk about
other things for example,
I wanted to talk about build requirements.
Because another way to speed
and to ease in our
continuous integrations,
is to define, for example, our tools.
We could do things like this,
we can depend, for example,
on the Catch testing framework,
we can do it as a build requirement.
That means, that it will
only be to retrieve,
and it will only be used
if you are building the
package from sources.
So if you already have the
binary for this certain package,
Catch will not be retrieved at
all, and it will not be used.
This is interesting,
because it can be used not
only for testing frameworks,
but also for tools.
So this is something that
typically our resources are doing,
imagine you want to use latest CMake, 39.
You have two options, you can ask your IT
to install CMake in the servers,
then wait for two months
or three (laughs).
You know what I'm talking about.
Or you can just take CMake 39,
build a Conan package with it,
upload CMake 39
to your remote server,
and then you can inject
this build requirement
in your profile for example,
or in your recipes.
So then, when you need
to build from sources,
the package, CMake 39 will be retrieved,
the path to CMake will be
injected to your recipe
while it's building,
so it will use CMake 39
from the Conan package.
So the result is that actually you can,
in a dependency graph,
you can actually build different nodes
with different CMake if you want.
And this can be, of course,
generalized for compiler,
there are some examples with mingw,
that's also very interesting.
So if you want just to
build with GCC in Windows,
you don't have to install it.
Just create a Conan package for it,
and it will be, it will use.
So, of course, very useful for developers,
if you don't want to be installing things.
And you want to run against
different GCC versions,
in Windows, for example, you can do that.
Okay, so my conclusions here,
the first part is automating
the creating of packages,
is very simple.
Just with 20 lines of a Conan file
that is practically always the same,
and 20 likes of the Jenkins
file that is always the same,
you can create packages when users do
Git push
to your repos.
And it will work for multi-platform,
the packages will end in the server,
so this is kind of easy.
For the challenging task,
there are ways to do it.
So if we define correctly
our versioning approach,
if we define correctly our package ID,
and if we define some updating mechanism,
for example, dependency overrides
or version renews, whatever,
we can make it work
that it will be optimal.
In the sense that only those packages
that really need to be
rebuilt will be rebuilt.
The rest, you can have a
hierarchy of 50 packages.
Only those that really need to be rebuilt
for compatibility and
for use in the right code
that it has to be used
will be rebuilt, okay?
And of course, Jenkins
offers a parallel decision
over the nodes that are
independent on everything,
so this can be actually pretty fast.
The only thing here is quite functional,
this is a proof of concept,
but it is quite functional,
is some usability.
So I had to use a couple of tricks
to actually provide uses of
all the overrides, for example,
so here I would love feedback.
So if anyone is here that is using Conan,
is using continuous integration,
please, please submit issues.
It's really something
that we are working on,
and the more feedback we have,
the best for the next versions.
And finally, I didn't want
to finish without saying
that we are hiring.
So please,
send your CVs,
come to talk with us at our booth,
by the way, we also have t-shirts,
we have these t-shirts at our booth.
So if you want one,
please come to our booth,
and we would love to give you one.
And yeah,
time for questions.
(audience applauding)
Any questions? (laughs)
- [Audience Member]
Trust me to pick the seat
furthest away from the microphone.
So I have a quick question
about the semantic versioning aspect.
You mentioned that the default was to use
semantic versioning.
- Yeah.
- [Audience Member] That
seems extremely dangerous
given how almost impossible it is
in C++ it is to get it right.
Given everything's, you know,
the propensities towards
putting everything in headers,
and the fact as you demonstrated,
that's exactly the kind of
thing you can get wrong.
So what was the motivation between
picking semantic versioning as the default
rather than making it the override?
- Yeah, it was basically
like being
purest, I mean, semantic
versioning says that,
we should do that.
And then we introduce it,
and we realized that probably
we should have gone for
a more restricted approach,
then there were thousands of packages
up there with the binaries,
and which, the thing with that,
if we change the default
that we are using for hashing
and completing the package binaries,
then all the binaries that are uploaded
to the central server,
they will be broken,
I mean you won't be able to find them,
because the package ID will change.
We are thinking probably for version 1.0,
that we might be able
to bring more things,
we might consider
doing a more restrictive
versioning approach.
- [Audience] Cool, I would, yeah,
strongly suggest doing that,
as 'cause nothing but the
debugging of that problem
is nearly impossible.
A second thing while I,
oh sorry I didn't realize
I have people behind me,
if you don't mind, I'm
gonna sneak one more in.
It looked like that the depending package
was the thing that determined
what the version was,
you know, whether it was using
semantic versioning or not.
Whereas it should be the package itself
that defines whether or not
people who depend on it,
should pick up on semantic versioning.
- Not really, because the
thing is, for example,
if you're the package
that is depending, okay,
it can say, okay, I'm going to build,
now I'm going to build a static library.
Then, I have to, every
single change in the version
of my upstream dependencies,
is going to affect me.
Or, I can build a static library,
and now,
minor patches,
they won't affect me.
So this actually the--
- [Audience Member] Only if you know
how the library was implemented yourself,
because the library itself knows
whether it has things in
headers that need to be,
I guess we should take
this offline, really,
I think I can probably
talk about this forever.
- Actually there are two sides.
First is
the package that this will depend on,
is following some versioning
approach that you understand,
and then, the package that
is using that package,
is the one to say, okay,
I'm a header-only library,
so my upstream doesn't
define how it's affecting me.
I do it, because I can,
for example, boost,
I can just do a header-only.
I will build a package
for a header-only boost,
or I will build boost for static,
or dynamic.
The way I build boost,
is the way that defines
how my upstream affects me.
- [Audience Member] Cool, I get it it,
maybe both packages should
be involved in that decision,
so, I'll defer to someone else now,
thank you.
- Thank you.
- [Audience Member] So
I have a question about,
your experience with this,
in terms of having codebases of scale,
so have you guys had any
experience using this
on a codebase that's
millions of lines of codes
and over a hundred packages
that are all highly changing all the time?
- We directly don't, but our users,
yeah, we have users that have more than
hundreds of packages.
So, the approaches,
I'll say every approach of
every company is different.
So at some of them,
they will try approaches
that will be very,
living on the edge,
and all the packages,
they move all together,
so for example, when you update LibA
and bump the version,
they will have a script that
it will bump the version
off all the dependencies
downstream, for example,
and they will move everything
automatically, for example.
Others, they are doing it manually,
they don't care that, I mean,
if we are upgrading this,
it can take one month,
but we want to make sure
that the changes upstream
are tested one by one down the hierarchy,
so I could say, I know
that it's being used.
Actually, when I said the
version ranges approach,
is a current issue.
Because some people that are having like,
one of them was having
like 70,000 artifacts
there.
I said, hey, my version range is low.
Yes, you have to search the
version that matches this
for several hundreds of packages.
So this normal utilization, for example,
and this is why we
developed the Conan alias.
So,
yes, I couldn't extract a general pattern
for larger-scale projects,
because everyone is doing
something different.
- [Audience Member]
Okay, one question about,
the fact you use exactly
strict version for your dependency,
like when you do require, you
specify exactly the version,
on that's the thing,
version superior to what,
one problem with that,
because your system is very similar
to Python packaging mechanism like Pip,
is generally when you
start with a very big tree
of dependencies, you get
at least like one library,
which is used with two or
three different versions.
Like, who do you under what?
- Sorry, you are talking about
versioning conflicts?
- [Audience Member] Exactly,
like in case of static
library like you have,
it's a bit of use, because
it's not going to compare,
in case you are using shell library,
you're gonna get a
really messy side effect,
when you override.
- Yeah, so I think with regarding that,
Conan is following the same,
quite a standard approach,
so it is using the typical
conflict detection and resolution,
actually, not only in versioning,
the thing is that you can have conflicts
based on configuration.
So you can have a package
that is being used,
hey, I want to link with
this package statically.
And then on the other side of the diamond,
I want to link with this dynamically.
So what, so that's a conflict,
and that has to be resolved.
So what Conan does, is, okay,
one plus of Conan is, it doesn't
try to do a lot of magic.
So if detects a conflict,
either of versioning,
or configuration, it's
going to tell the user.
And the user downstream
is the one that says,
okay, so I have a conflict
here, I want to use version,
I'm doing an override.
The override typically is a mechanism
for resolving conflicts.
So the same override you can do there,
you can do for optimize also.
And you can force downstream, okay,
you want static, you want dynamic,
I tell my dependency graph that you are
going to use dynamic.
There is also a different
kind of dependencies in Conan,
that are private.
So you can use private dependencies,
and so you can actually
depend on different versions,
in this independency graph,
but they will be different nodes.
You really want to make sure that the API
will across frontiers,
here and you will insert
objects of those libraries.
It's typically something
you don't want to do.
But some users, they really need to link
with the same library,
two different versions,
in the same dependency graph.
So it's something that can be done,
not something that would be recommended
for the general case.
- [Audience Member] Okay, thank you.
- Thank you.
- [Audience Member] So
we have a mixture of
C++ and Java code in our codebase,
we use Maven for building the Java,
is there integration with,
between the, sort of, the Maven artifacts
and the Conan artifacts?
- Okay, so, there is no
integration with Maven,
okay, but, Conan as you
have seen the recipes,
they are just very general.
So we have proof of concept
working for Python packaging,
also for Golang packaging,
there are also generators
for Rust from Conan packages,
you can just generate all the information
of certain dependencies,
and that will be used
from Rust to link with the artifacts
that you just created
with Conan.
So I guess, for Maven, you can do that.
It's possible to write a generator
that will output the information
of the dependency graph
and everything that Maven needs,
in Maven format,
it can be built in one day, for example,
one static generator.
- [Audience Member] Okay, cool, thanks.
- Hey Diego.
- Hi.
- [Audience Member] I noticed
that in your first example,
when you override the version,
Conan just failed instead of recompiling.
Does this mean that the default policy
is not to check the version
and recompile the dependency
as the graph changes?
- The default, when I did
the first override, you mean?
- [Audience Member] Yeah.
- Yeah, the first override was a minor,
actually it was a patch.
- [Audience Member] Ah,
and the default is to,
do not recompile, right?
- The default is Semver, so Semver,
if you're just bumping the patches,
Semver will tell you that
you don't need to rebuild.
- [Audience Member] Yeah,
yeah, I agree with that.
Okay.
Just to add to maybe a bit
response to what Matt said
just before we verse Semver
on the problems of versioning,
(laughs)
I actually am doing a
talk on Wednesday morning
about versioning in C++,
I will be very glad if you come
and you can give me your feedback,
if I can give you answers,
I will be very happy.
- Cool, thank you.
- [Audience Member] Looking
at the CMake use cases,
it seems that this new
thing of information
that it would already be available
through CMake's install process,
or the CMake modules that get generated
when you install a library.
Is there any way to have
Conan extract that information
and then repeat it the conanfile.py?
Things like, sorry, I should clarify.
Things like required
compiler flags, for example.
- No, I mean,
Conan is very automated to build systems,
so it's kind of transparent,
so you have to translate all the way
down to Cmake and also syncing from Cmake.
Of course, you can do many things,
for example, a typical use
case would be in CMake,
you have the install,
for example, utilities,
so instead of building, of
defining your package method
in the Conan file recipe,
you just call CMake install, for example.
And so we have the parameters,
like the package folder,
and you will say, okay, Cmake,
CMake install, prefix package folder,
so it will run the install
to the package folder.
So you can reuse that functionality.
So it basically is up to you,
there's nothing transparent or magic
in the way that we are
integrated with CMake.
Just, you call CMake to create packages,
you can pass CMake any flag you want,
but, your CMake
script,
it has to be prepared to use those flags.
- [Audience Member] Yeah, so
I meant the other way around,
so if your CMake for your library
is defining flags you need,
you know, downstream, the
typical way in CMake would be,
install generates a module file.
- Yeah, so basically,
imagine that your CMake is
requiring some pre-positioned
directives or something,
you have, right now, you
have to add them manually
in your package information.
So for your consumers,
Conan will not detect the
configuration from CMake,
you have to explicitly
define for your consumers
the configuration in
the package information.
So in that way you will
have some repetition.
But there is no way that you can do this,
very smartly,
I mean, extracting information for CMake
is not something that can
be done automatically.
In any case, it takes
you two minutes to do it.
If you know your build system,
building a package recipe is quite simple.
- [Audience Member] Okay, thank you.
- Okay, thank you.
Anyone else,
no?
So, thanks very much for coming.
(audience applauding)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>