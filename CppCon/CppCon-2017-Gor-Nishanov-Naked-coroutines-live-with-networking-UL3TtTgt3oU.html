<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Gor Nishanov “Naked coroutines live (with networking)” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Gor Nishanov “Naked coroutines live (with networking)” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Gor Nishanov “Naked coroutines live (with networking)”</b></h2><h5 class="post__date">2017-10-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UL3TtTgt3oU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Good afternoon.
Oh lovely, lovely, it works, it works.
So we had a bunch of
coroutine talks this week,
so who went to at least one of them?
Oh wonderful, wonderful.
So my name is Gor Nishanov.
I'm a developer in Visual C++ team
and one of the things I work on is
design and standardization
of C++ coroutines,
most efficient, most
scalable, most open coroutine
of any programming language in existence.
And that is not an accident
because we put a design principles
when we started working on that
that we want coroutines to scale
to billions of concurrent coroutines,
it means that overhead per
individual coroutine is
tiny, tiny, tiny, just a few bytes.
We also wanted coroutines
to be very efficient
so that an overhead of
suspend and resume is
equivalent to overhead of
a function call and return
and in those cases where
coroutine is inlined,
it disappears completely.
More over, we wanted
to put no restrictions
of what can be done to the
coroutine and from the coroutine
so from the outside coroutine
is a normal function.
You can store it in a suit function,
take an headers function pointer
pass to put it on the ABI boundary
and from the other side
there is no restriction
of what you can do in the coroutine.
Moreover coroutine can be
converted to a void star
so that you can marshall it through a CAPI
and then maybe like OS API
will call you back later,
you reconstitute the coroutine
from a void star and resume it.
Also we did not want to tie
the coroutine to any particular machinery
because different category of people
have different desires for
their asynchronous runtime
so coroutines are open ended,
they can hook up to anything you want.
And finally, to increase the
capability of coroutines,
we did not want to tie them to exceptions.
They will work nicely with exceptions
where you want to use them
and in environments where you cannot
or just impossible to use exceptions
you can use coroutine just fine.
So it's an old thing, a very
old thing, almost 60 years.
Invented by Melvin Conway and Knuth
gave this convenient definition
that coroutine is the
generalization of a function.
It has everything that a function
has like you can call it,
it can return back to you,
but in addition it gets
two more iterations,
suspend and resume.
And if the control flow in a
normal function looks like this
where the function will only return back
once reaches the end,
in a coroutine actually
a function can return
back its own suspend point
and then the guy who
called it or somebody else
can resume it, it'll continue
from the previous suspend point
and it'll keep going
until it reaches the end.
And in C++ let's look at the
dry language of the standard.
And it says quite nicely,
it just says a function is a coroutine
if it has yield, a wait, or coreturn.
And here's an example.
It's a simple hello world C++ 20 style
where we have a generator hello
which yields characters one at a time
and then your main in the loop spins
and pull them one at a time
and every time the range
base four in the main
pulls a character it actually
resumes the coroutine
that jumps immediately after yield,
it goes to the next iteration,
outputs another character,
and it yields back to main.
Or another pattern frequently used is
asynchronous task where you actually
await there waiting for
some event to happen
like in this case one millisecond passed.
And then coroutine is
resumed by say a thread pool
and it prints woke up
and returns for the two.
And this is like the
bigger example to show
how amazingly efficient coroutines are,
it's good bolted version
of the demo I showed at
the CPP Con 2016 so you see
all of those beautiful
coroutines composed together
sent to the standard algorithm,
the result is single constant.
Lovely, lovely, lovely.
Like C# try that, or Python.
Nobody, nobody can match that.
Maybe Swift soon but not yet.
So this was a intro to coroutines.
Now the main part starts.
This summer we had an amazing,
an amazing standardization
meeting in Toronto.
We got so many goodies,
C++ was shipped to some
three letter acronym
or two letter acronym,
Richard knows better,
it's DIS or something.
Anyway so it was almost
there, it was shipped.
A bunch of TS's were sent to publication,
ranges, coroutines, networking.
Modules went to the TS ballot.
Oh concepts, concepts emerged into
the working paper for C++
20 so many, many goodies.
But specifically we're going to talk about
two wonderful gifts we got from Toronto.
Namely, Coroutines TS and Networking TS.
Coroutines TS because it makes
asynchronous code beautiful
and Networking Ts because naturally
there is a lot of opportunities
to write asynchronous code there.
So let's see what's inside.
Wonderful, so Networking TS
brings us a lot of goodies
we have circuits, UDP
circuits, TCP circuits,
timers, executors, not the
scary one, the smaller ones.
And finally, the best of all,
I have context which
binds them all together.
I even have a slide about
IO_context so okay well not.
And many more nifty
things I want to mention
it's just I only listed a few.
So IO_context is the thing,
it's like a completion queue.
And all of the various
asynchronous sources
available in the networking TS,
they all send their completion
to a particular IO_context
and then on the consumption side
you decide how do you
want to execute them.
If you want you can just call IO_context
to run on your main thread
or if you'd like to be more scalable,
you will launch correct number of threads,
it's a difficult decision to
see how many threads to launch
and then you call run
from every one of them.
And those run will block
until there is any outstanding
asynchronous activity.
There are a few other calls, like stop,
which makes all of those runs to exit
for example you want to go away
and they will finish doing
whatever they're currently doing
and then they will exit without de queuing
the rest of the work which
is in the IO_context.
And there is a pull method
which you can use in your event loop.
It does not block if there is
no work currently available
to execute in your queue.
And it's not a global, you
can have many IO_context
for example if you have
a multi circuit machine
you can have IO_context for one CPU,
IO_context for another CPU and
you have affinitized threads
which essentially segregate your work
between those two CPUs
so there is no sharing.
And this is how an example of a code
written to Networking TS looks like
so we have an IO_context and two timers.
One will fire in 50 hours, well 15, sorry.
And another in one second
and the fast timer will
stop the IO_context
so how long this program will run?
- [Audience Member] One second?
Yes, yes, one second,
wonderful, thank you.
Excellent so this is an example
of a beautiful TCP server
and I am not being facetious,
it's beautiful because it is
written at a state machine
and state machines are beautiful.
Pyramid of doom, call back
doom, that is not beautiful.
This is good.
But we are going to make this
code even more beautiful live.
Now that's was Networking TS.
Let's look at the coroutines.
So what do we have?
Hey we get three keywords,
that's wonderful.
Networking TS did not
give us a single keyword.
Good, now.
(laughter)
Now let's see on the library
side, what are we getting?
Suspend_never, hmm, that's
kind of not very useful,
it's an awaitable type so
you can call await on it
but if you call await it does nothing,
it just does nothing exactly.
So what else?
Oh suspend_always.
Another very useful type.
If you call await on it,
your coroutine just suspends.
So hopefully you have a coroutine_handle
that you can later resume the coroutine
but if you don't, you are stuck.
And then it has this trait thing,
I don't know, we'll figure out what it is.
And that is all you get!
That's why it's called naked coroutines.
You open the box and there is nothing.
They're useless.
So together here we'll try to see
can we take those useless coroutines
and make that wonderful
beautiful networking TCP server
even more beautiful than it is?
Okay so what does the next slide says?
Oh yes, but it's just, we are
not going to read the specs,
show the explanations of how it works.
Now we will just jump to
experimentation with coroutines
and see what will happen.
Okay.
This is our hello world.
Let's see, can we use the first keyword
that we see there, co_return.
Okay, compile.
Nope, we cannot use co_return
in main, no problem.
Easy, easy, we'll call it F, right?
(laughter)
And then we'll call it from main.
Oh it complains about something well let's
maybe they want future.
Let's see.
Let's add include future and
let's make it a future of int.
Maybe that'll work.
Huh, still the same problem,
it says that compiler is looking for
some coroutine trait specialized
for the std future of int
and also it wants in that coroutine trait
a member promise_type.
Well let's give it to it.
Now you're not supposed to actually
specialize standard traits with types
which are only built ins are from standard
but for right now it's an
experimental TS, we can do it.
Now why compiler needs that thing?
So whenever compiler
discovers the coroutine,
it is helpless because coroutine TS
does not give any semantics to coroutine.
It wants you, the library designer,
to tell it how coroutine behaves.
And to do that it specializes
a template coroutine trait
with return type and
type of every parameter
that was done in that function.
So in this case,
let's teach compiler how
to deal with std future.
So we'll take that name R
and whole bunch of args,
in this case we don't take any
but let's just do the
complete implementation.
And we'll hey struct, std
experimental coroutine traits,
and the return type is the first one
so I will say it will take future of R
and whatever arguments that
were passed to the function.
(audience member speaking off microphone)
Ah, thank you.
That's wonderful look,
when I was practicing,
it was compiler who
constantly had to correct me.
Now with an audience it's so much easier,
thank you, thank you.
(laughter)
Okay promise_type and
yeah, that should work.
Make.
Okay now it wants initial_suspend,
so let's talk a little bit
about the coroutine promise.
So the promise_type here is a thing
that compiler will put on your behalf
as a local variable in the coroutine
that we have just written.
And you as a library developer
can put whatever you want there.
And then compiler will call
you to ask some questions.
So in this case it wants to know
do we want to have an initial suspend?
And coroutine allows you to
suspend on the first curly
before even entering
the body of a coroutine
and for this particular
case we will just say
no just go straight in, no
need to do absolutely anything,
so we'll use that useful
suspend, always or never? Never.
Initial_suspend.
Good, so why would you not want to
immediately jump into the coroutine,
for example in the generator case?
In that case, maybe it takes 15 hours
to get to the first value
so until somebody will actually start
pulling the value from the generator,
you don't want to enter the body.
Or you want to have some kind of actor
which always have to run
on the particular thread
associated with your actor
thus you suspend within your initial point
and then repost the
coroutine into some queue
and it will resume it to
run on the right thread.
And I will save you
from another compilation
because I know that also
compiler wants a final suspend
which is the same thing
but only at the last curly.
So at the last curly the compiler
gives you an opportunity to say
do you wanna suspend or you
want to do something else?
So if you have ref counted for
example handle to a coroutine
maybe you want to derive
it for your if count
and if it hit zero you will just
let the coroutine destroy itself
but if it's above zero it means
somebody else carries
a handle to a coroutine
and we just suspend there
and hope that it'll be
destroyed from the outside.
Okay hopefully this is enough.
Now it complains about co_return.
It does not know what
to do with co_return.
Well let's teach it.
And it says it wants to return value.
So in this case we
returned a future, right?
So we need somehow to,
in our return value,
and to speed up typing I just say int
because I know that I understand an int
but properly you put
template blah blah blah blah.
And here I kind of would like
to say something like this.
I want to have a promise
associated with the future returned
and I want to propagate the
value into that promise.
Where would we put that promise?
Hint, promise_type, that's
why it's called promise_type
because that's where we stick it.
Promise of R P, good.
Compile and see what happens.
Okay now it wants get return object.
It never ends.
So why it wants it?
Well coroutine can return to you
way before it reaches the end,
maybe even at the first
curly but coroutine in C++
doesn't change the signature
of a function or any way
or colon convention so
when somebody calls you
and then it return the future
even when the function suspends,
it needs to return a future.
So essentially compiler
asks, it is helpless.
It says hey I need to suspend,
give me what I have to
return to the caller
because he expects a future.
Easy, easy.
Get_return_object and
where do we get the future?
(audience member speaking off microphone)
Yay, yes from the promise.
Excellent, now we're done.
Now I know it says unhandled exception,
well exception cannot
leak from the coroutine
so there is a mandatory
member of the promise
which would tell us what
to do with an exception
if it occurred in the
body of the coroutine
so we will just say p.set_exception std,
yeah that's the one.
And let's just, to finish it off,
let's say percent D F .get.
Go.
(audience applauds)
This is our first coroutine
so we took naked coroutines
and make them work with std future.
And of course before the standard ships
we will have the proper
hook into the future
but I just wanted to show
how easy and quickly it is,
moreover, if you're on the Visual C++,
we already throw this thing in for free
because we're not very
compliant with the TS,
we do more and sometimes less.
And this is almost as good as
it will be in the standard,
the only tweaks I would make here is that
I would play with the
allocator of std promise
so that the shared state which
is allocated for that future
can be embedded into the coroutine itself,
thus a future return from the coroutine
is way better than a normal future.
Okay we are ready to proceed
to the most difficult task.
Namely, we will, wait, go,
you see I'm using PowerPoint
on Linux, that's crazy.
F5 I guess, no slide, start slideshow.
Now start the show, good,
excellent, and the clicker.
Go, go, go, go, go, go,
we talked about that,
talked about that, talked about that, yes,
so let me ask you,
there is a hard way to do
it and an easy way to do it.
Which should we start first?
(audience members speaking off microphone)
Hard, hard, easy, I hear easy,
well my slides are done
for the easy way first,
so let's start with the easy way.
So if you look inside of
any asynchronous API in the Networking TS,
you will notice that initial function
does absolutely nothing
because completion, sorry,
the call back that you see
that has to be passed
as the last parameter
doesn't have to be a call back,
it's actually a completion token
that is sent through a bunch
of transformation traits
which will tell a whole bunch of things
to ramp initial function,
it'll tell you what it should return,
what should be passed
as the call back to reference
implementation down below
and additionally what
executor to execute it on
and what allocator to use
for this function object
plus whatever else is needed
to perform the iteration
like overlapped on Windows
or some descriptor that you
register was a pull on Linux
and Networking TS
provides very useful thing
called use_future.
That tag type, if you pass
it as a last parameter,
it will make your API return a future.
But unfortunately, we don't have a .then.
So currently we cannot
really do async stuff
with std future as it stands today.
So temporarily just for a few minutes
I'm going to use boost future
because it's available and it has .then.
And to do that I will need to
define specialization for use_future
and you don't have to like
understand every line here,
I'm just showing you
how big this thing is,
it's not very big.
What it is doing, it is sticking
a promise, boost promise,
inside of a completion
handler, it's a call back
that you pass to the actual implementation
and you return back a future
connect through that promise.
Luckily for us, future
has .then, boost future.
So I can await on it.
So let's see what is next.
Oh, again, live, okay get ready
for me making tons of mistakes
and please correct me.
So what we're going to do is
we are going to take an easy way.
And right now easy is, this is a server
from this performance benchmarking thing
from boost a little slightly modified
and let me just show
you how it looks so we,
well let's go actually to the server.
So server starts listening on the circuit,
and it accepts connection,
whenever connection is accepted,
it starts a session that will
start reading and writing
from that TCP channel and
it starts accepting again
so it like in a loop it
reads, it accepts connection.
And the session looks
very similar, you start,
you set some options,
you start reading here,
you go then write, then you
read, and then you write,
so it keep read write until
the connection is closed
and at any error it'll
destroy the session.
So we are going to make
this thing a coroutine
but before we do that let's
make sure it compiles.
Well it compiles because
it already compiled before.
Let's start easy, so it started a server,
not a coroutine yet, and
I'll start the client,
and we get one, two, three, okay,
about 75 megabytes per second, good.
Let's see how much better
or worse it'll become
after we make it a coroutine.
So server doesn't really
care about the result
so how about we'll just say
that our coroutine will
be std future of void?
And that little example that
we did in the beginning,
how we taught compiler
to deal with futures,
I already put it in one of those headers
so we don't have to do it again.
So std future, session,
we'll take all args as
they are, all arguments.
In the beginning we will,
well we don't need to
deal with error codes
because now a coroutine actually can
propagate the errors automatically
via exceptions to the future
so we can use the version of set option
that doesn't take the error
so of all of the API's in Networking TS,
they have two flavors.
One was an error in the end,
it is not throwing version,
it'll put an error in the error.
And a throwing one where the
error will be like this one.
Okay then we will start region.
Okay let's just say circuit async_read
and we will replace this whole
lambda with use_boost_future.
Okay, we don't need this.
Now in the read what we are doing, we are,
okay I need to say and
actually use circuit S now
and I am saying async_write
from the circuit buffer,
now it's called N.
And use_boost_future.
We don't need destruction and
then after we write we read.
So we just need to say for.
And I think this is it.
Well not yet, we need to have,
let's see what are the variables here.
We need to grab this buffer.
Does it look good to you?
Okay, let's see.
(audience member speaking off microphone)
Yes, yes, yes, yes, of course, thank you.
Because that was done in the constructor
so I need to say block size.
What else, oh block size here
should be without underscore.
And now.
That's the trouble with live,
so I will just divine what
it is just by looking at it,
block size buffer, you know what,
I have the whole thing here
already done you know I can just--
(audience laughter)
I can just take it from here, so...
Ha! Yes, yes!
And actually we have this
wonderful attribute called
no discard and if that no
discard was on the boost_future
then we would have, and I
know that you'll complain
that I used a result of async_write
but we don't really care how
many bytes we have written.
Okay, now, go.
Oh, right, of course we
converted our state machine
to a coroutine now we need
to update the call site.
But it's pretty straightforward.
Here, instead of create a new session,
we will just launch the coroutine.
And it'll start all by itself.
And I think we're done.
Okay, okay, that's good, that's good.
Let's launch it.
Easy.
One, two, th-- Ooh.
I don't like that.
It's like 45% worse, moreover,
here we're doing a loop back.
If you do something simpler
like posting results
into an executor and using
a future as an intermediary.
It will not be good, it'll be
like 10 times worse or more.
So let me show you the hard way.
And the good part about the hard way,
the hard way not only has no overhead,
but it also applicable,
you can use it with any library you have,
any library to use as call backs,
any C library, lib UV, OS, whatever.
So the hard way,
let's start with the
result of the easy one.
Any questions before I go on?
No, okay, easy.
(audience member speaking off microphone)
Yes.
(audience member speaking off microphone)
Do I, okay, yes, yes I can.
(audience member speaking off microphone)
Yes sir, yes, the question
was why it is slower.
And you just triggered me,
you launched me into whole discussion
why future isn't an efficient type.
So future has a bunch
of inherent overhead,
it has this shared state
that has to be hyper
located it has to if counted
because there are two
ands that operate with it.
It has to have signaling when it completes
and then Mutex to protect the state
because sometimes your .then
can happen concurrently with set value.
Moreover, it has to
interact with a scheduler
because sometimes, well you cannot,
when you call set value
and you have .then,
you don't want to execute the call back
in .then in the thread of set value
otherwise you blow up the stack.
So what we have here, we have co_await,
which is want to suspend and
we have an API Networking TS
which knows how to accept call backs
and we are creating this big
intermediary to deal with that.
We don't need that.
And that's what we're going to do,
we're going to do row awaitables.
It's more work but it is rewarding.
(audience laughter)
So let's go hard.
Okay so hard starts from
the position of the easy.
So it's the same function
that we had before.
So in the hard way we
have to return awaitables.
The things that co_await natively wants.
An awaitable is an easy thing.
It's just a struct with three
functions, three members.
Await_ready, await_suspend,
and await_resume.
Await_ready will tell you
whether the thing you're
awaiting is already available.
Await_resume will unpack the result,
so result of co_await is
whatever await_resume returns.
And finally await_suspend
gives an opportunity
for us to subscribe to notification
that will tell us
that whatever we are waiting
for is already available.
So let's start with async_read_some.
So we'll make a function that
will be called await_read_some
that will take any socket that
will take a buffer sequence,
buffer sequence, return something.
Async_read_some, socket by reference,
and buffer sequence by
constant percent reference
even though we're
reading into the buffers,
buffer sequence is just a
descriptions of the buffer to read
that's why it is passed
by constant percent,
not by reference.
And here we create an awaitable.
That we then return from this function
and we would like that awaitable
to do all the work for us.
Thus we need to capture all the arguments
that were passed into the function
and stick it inside of the awaitable.
So we'll just grab this guy and this guy,
we'll put them here.
Okay now await_ready in this
case, it will be never ready
because we actually will
not launch an iteration
until somebody co_await on us
because that allows us to eliminate
a whole bunch of synchronization.
So that is future is doing
so we'll say await_ready,
return false.
Compiler sees that and says
okay it means I need to suspend.
So it prepares everything for suspension.
And when it call this function
coroutine is ready to be resumed.
So even though technically
coroutine is still running,
it is calling you await_resume,
the standard very graciously for us
declared that permanent
coroutine is considered suspended
thus it is safe to call a API
that potentially can call us back on this
or other thread and resume
or destroy the coroutine.
It's fine, it's legal.
So this is the place
where we're going to start
the asynchronous iteration.
So I will just say s.await,
sorry, not an await, async_read_some.
Here we go, async_read_some.
I pass buffer sequence
and I pass a call back
that will resume the coroutine.
So I will grab this.
I will grab H.
And the call back will
give me an error code
and how many bytes were transferred.
And actually I would like to store that
so that I can later access
them from the resume
so I will just say here STD
error code EC and size TN
so this is how many
bytes were transferred.
And you know smart people
or very detail oriented
would actually overload those two things
on top of each other
because socket and buffer are
only needed initially, right?
And the error code and and
are all we need and the and
but for simplicity we
just put all of them here.
So in the lambda we will
say well this EC equal EC
and this N, this N equal N, and h.resume.
It almost works unfortunately recently
we changed the specification
and we said that resume is not const.
So I have to put mutable here,
I hope we can return them back how it was
so I don't have to type mutable
every lambda which resumes.
And this is almost it, so now
what we did is that we said
when somebody co_awaits,
it simply calls into async_read_some,
compiler gives them the
handle to a coroutine
and whenever the thing completes,
we remember the result,
and we resume the coroutine.
The only thing left is
to unpack the result.
We want co_await in this
case to return size T,
namely N, whatever
number bytes transferred.
So I will say async_ready, no resume.
What happens on a resume?
I will return N and to be
nice I will handle error.
Is this what you said Richard?
(audience member speaking off microphone)
Thank you, so Richard has
unbelievable eye for errors,
it was await_resume, not, yes,
so we saved whole compilation cycle.
Okay if (ec), throw std::system_error(ec)
and so yeah this is our awaitable,
zero overhead awaitable
for async_read_some.
Let me, well let me first
verify that it compiles
and then I will do the second one.
And I will remove use_boost_future.
So now actually I'm calling an API
which looks very similar
to original async_read
but it's the one we just created.
Compiles.
Yeah, it compiles, good.
Let me just clone and make
another one for the async_write.
Async_write, the rest stays
the same, async_write.
Do I need to change anything else?
I don't think so.
And I think it gives an opportunity
for people who like to
template method programming
to make all of those things
generated automatically
from some kind of template
or policy or whatever.
So good, async_write
here, async_write here,
I remove boost_future, and let's compile.
Good, let's run.
Oh there is already a
server somewhere, yes.
Now we want a hard server
and let's start the client.
One, two, three--
wait a second, not
supposed, supposed to be 75,
what did I, what did we do wrong?
Okay I don't know what I did wrong
so I'll just copy the
good one that I know.
(audience laughter)
That is here.
No I'll try, I'll try.
So we already return
false, async_read_some,
hmm, looks so right.
Okay you know what I'll do twice,
maybe the second time it will work.
One, two, three, four, 75.
No.
I'll try hard2 just in case.
Okay hard2, one, two, three, four.
You know I think it's like it's a browser,
it's something consuming cycles.
It was 75 just a second ago.
And it should be 75.
(audience member speaking off microphone)
Huh, what if it's, it's faster?
(audience laughter)
You know I'll kill the browser.
(audience laughter)
One, two, three, four,
okay, good, good, good.
Even the old one works slow.
Thank you, thank you,
actually we did the old one, we got 60.
(audience laughter)
Okay let's get back to the slides.
I have no idea how to resume
it from this very slide so,
ah, we will just click
away, so what we did...
It's faster with the--
Okay the easy way,
okay so what we did we took
this beautiful TCP server
and we convert it into this.
So I just skip a step
and I also converted
for you the session, so.
Look, it actually took us about 15 minutes
if you discount the first attempt
where I used boost_future.
We created a wrapper for std future
and then we created the hard way,
the adapters that will do
zero overhead communication
with Networking TS.
And there is a whole, but
before we standardize,
we will figure out a way
how completion token trace
can then modify it
so that you will get similar
efficiency things for free.
Moreover, you will get them
for free for in your library
which use these transformation
traits plus coroutines.
Yeah, yeah, yeah, I already explained this
and somebody could say,
&quot;Well you know maybe completion traits
&quot;like they are today are not really bad,
&quot;maybe it's just the future which is bad.&quot;
And indeed if you play with coroutines
and use some very fancy coroutine
that will get inlined into the caller,
keep allocation alighted,
we can get rid of two out of
four of these inefficiencies.
We will still have to synchronize
between the starting iteration
and the subscription.
Because the, if you saw the example,
it starts iteration first
and then you subscribe for continuation
no matter what you do
whereas with our awaiters
we first prepare everything
for that iteration
and only then subscribe for continuation.
Huh, there are complications.
But they are, they are tolerable.
So complication number one,
let me show you this wonderful...
Oh by the way before I
go into complications,
any quick questions on the way?
No, excellent, excellent.
So this is a noisy clock.
When we start this coroutine
it'll start printing for
us tick tock tick tock.
Well you don't have to believe me
because we just see that sometimes one,
yeah, tick tock, excellent, it runs.
Now let's see if we can cancel it.
And to cancel it we will start,
like in the example at the very beginning,
we will start the fast timer
that will trigger in one second
and fast timer will go and async_wait
that will go and cancel the timer.
The one that is driving the noisy clock.
Let's compile and see how it goes.
Yeah, yeah, yeah.
Tick, excellent, excellent,
so what happened is that
we printed a tick here,
then the timer was canceled,
and probably at the second co_await,
Async when it was unpacking the result
discover that timer has
completed with an error code
and through an exception we code it,
we print it, we code,
so everything's great.
But in this case as your timer,
I individually canceled a particular timer
but what if you were to cancel everything
that is associated with the IO_context?
Well for that you can do
thing like IO_stop for example
now I'm stopping everything,
let's compile and see what's gonna...
Tick, done, context destroyed?
What happened?
Nobody threw any exception.
So and this is not coroutine specific,
this is a feature of Networking TS
which may get improved or not
by the time it is standardized is that
when you stop an IO_context
and then IO_context goes away,
none of the state machines
that were driven by those
asynchronous iterations
will get any notification.
So essentially the coroutine gets stuck
and if you are going, if your
process going away anyway,
who cares, maybe that's
one of the rational.
But if you have several
context and you destroy them,
recreate them while the program
runs, it is not very nice.
But we can solve it and
we can solve it by...
I have time.
By looking at this awaiter
that we just built.
Well, no, we didn't build
but we built the one for the async IO
and this is the one for the noisy clock.
Let's go with better async_wait.
And we'll call them better async_wait,
so we would like to get notification
when IO_context goes away
so that coroutine can actually complete.
So to do that we need to
go old fashioned style.
Namely, do not use lambdas.
So what happens here is that
this is the function object
that we pass to Networking TS.
And when the IO_context going away,
Networking TS will not resume resume it.
But it will destroy the function object.
So if we, instead of this function object,
and I will just copy it
because it's a lot to type,
I will make it into a call back,
hand crafted function object
we used to do before lambdas.
And then I will put it here,
replace lambda with that
particular object call back,
we'll pass this and cora,
okay, let's just see
what is going on here.
We captured me and cora just like before
like lambda was capturing.
In the operator call I will
now doubt the member variable
and then resume after I now doubt.
And in the destructor
I will check whether somebody resumed us
and if not I will send an
error to operation aborted
and resume the coroutine,
let's double check that it actually works.
One, excellent, we caught,
caught operation canceled.
Now it is not an ideal solution
because it happens during destruction
and it's all done by a single thread
so if you're scalable
and you want to cancel a lot of resources
you don't want to do
that on a single thread
but for right now until it's fixed,
you can use this technique
to deal with state
machines that just abandon.
Second complication is
that we never dealt,
we didn't deal with allocators
and we didn't deal with them
and performance was just fine
because the default recycling allocator
which is used by the
Networking TS is wonderful
so when your handler completes,
before it is dispatched,
it is copied to a local
variable and then it is freed
and put into recycling
block in the thread local.
So that if you launch
exactly one async iteration
from your completion it will
immediately consume that memory
so essentially for our entire TCP server
it would only use as many blocks of memory
as there were threads.
However, if you launch more than one,
then suddenly a memory
allocation would start
so you would want to change
your default allocator
and it's easy to do and
we almost ready to do
because now we have this call back
and we can simply add get_allocator.
It's one of those things that
the Networking TS understands
and also another wonderful
property of coroutines is that
awaiter is not going away anywhere
until the operation completes.
There are four,
we can just say std::array
of the appropriate size,
and then tells the allocator
here to use that array.
Done, so those are two complications
with those hand crafted awaitables.
Okay, beyond the TS.
So we have two possible
additions to coroutines
I want to share with you.
So last year I showed zero overhead future
which had none of those things
but it relied on the
property that Clang had
that at O2 optimization
level and if_await_suspend,
the last instruction is
resume of another coroutine,
it becomes a tail call.
But it is not guaranteed in any way.
So Richard, sitting right here,
came up with the brilliant idea
to guarantee the tail call,
what if we add symmetric control transfer?
So what if we'll have a
flavor of await_suspend
which returns a coroutine handle?
And it results in
suspension of this coroutine
and immediate resumption of
another coroutine in one go.
So with this particular thing
we can have zero overhead
future reliably in the standard
so it's only available in the Clang trunk,
not yet in the MSVC or Clang
5, not part of the TS yet.
Oh, another one, so another
addition we might get is that
people are creative.
We saw that get or hear an
object in coroutine promise
allows us to return a future, a generator,
whatever we want from the coroutine.
But some people have those
crazy coding conventions
where function must return an error.
And whatever you meant to return
have to be the last parameter,
out, star, whatever.
So and people figure out a
way to do it with coroutines.
Or people want to do things like
they want to pass an executor
so that in initial suspend
you suspend the coroutine
and then resume it on that executor.
And there are more flavors
of things people want to do
so essentially people want to be able to
look at coroutine
arguments from the promise
and the current workaround
they are doing are
they are overloading operator new
which is given all of the
arguments of a coroutine
so that it can sniff out the allocator
if it wants to use the allocator.
So what they would do, they
will find the thing they want,
stick it into the thread local
and then in the promise constructor
they will extract it from thread local.
Well it's wonderful but
well no it's not wonderful,
thread locals are slow
but we have amazing compilers
that optimize our heap,
that remove heap allocations.
So nobody will call your operator new
if compiler doesn't need it.
So we need a solution to that.
And solution is easy,
actually it was part
of the original design
but we stripped some of
that as not to scare people,
I mean those who looked at the
customization machinery of
coroutines initial reaction is
very very frightening so this is possibly
what we can do to enable those scenarios.
And at conclusion,
Networking and Coroutine TS
working wonderfully together,
it takes about 30 seconds to bootstrap,
no sorry, 30 minutes,
30 minutes to bootstrap,
at the moment for the best performance
you have to do it the hard way
hopefully we will take care of it
before it gets into the standard.
Coroutines are available
in MSVC with /await flag
and Clang by 5.0 with
those flags over there,
thank you Eric for doing
the library, where is Eric?
Eric wave, yes thank you.
Because it only available
with stdlib C++ and
Networking TS implementation,
I'm not going to read it
okay it's on the slide.
One of the things I want to mention that
it was already mentioned during this week
that there is wonderful
coroutine libraries
CPP coro by Lewis Baker
so you can look at it
and play around with it
and I promise I am going
to put the snippets
I used here at that
location, it's not there yet.
Any questions?
(audience applauds)
Well before you ask any
question I will show you
one little thing which I
skipped to speed up but...
I apologize, I apologize
but since I see we have,
we still have time,
so I wanted to compare the
overhead of the future,
use_boost_future like facility
versus a row awaitable,
so this is the, it's a very simple thing.
I keep reposting the same coroutine
to an executor in a loop.
And here I am doing it with
the handcrafted awaiter,
very simple one, and here
I am doing it with use--
Sorry, not this one, not,
where is it, where is over?
Over one, here we go.
Here I'm doing it with use_boost_future
and in no way I am saying
boost_future is not good.
It's future is not good,
not the boost_future.
So let's start, let's time
the one with use_boost_future,
okay, it's about one second.
Let's time the one with awaitable.
Ah, it's 077 so it's 10
times better or more.
Okay now I am done.
- [Audience Member] Hello,
I was wondering about
the future of TCP, the future of TCP's,
is there any work or consideration
for serializing the state of a coroutine?
Okay.
- [Audience Member] It
might be hard with the--
So far we cannot come up with an algorithm
that can serially copy the
state of the coroutine,
I think if you can play
around, the best you can do,
as long as you can deserialize it
at exactly the same
address, it might work,
but if you try to relocate it
do a different memory address,
imagine you have local variables,
some of them put in registers
and compilers don't sometimes use indexes
to navigate an array,
they actually use pointers
straight to the middle of the array
and when coroutines suspend,
you may have a pointer
in the middle of the
array in some register.
Now let's serialize that coroutine
and deserialize it elsewhere,
how do you know how to
adjust that register?
I mean maybe compiler can be super smart
but if compiler is that smart
we would not need code reconstructors.
It can just synthesize them for us
because it's the same problem,
you have some complicated class
with some invariance with some dependency
between the number of variables,
now force the compiler
to write a correct code reconstructor.
- [Audience Member] So
it's the same problem then
if I wanted to serialize the
state of a function call?
Well it's easy to serialize
the actual function call
for example people who are
doing actors with coroutines,
they are suspending at initial point
and coroutine nicely captures
the arguments to coroutine
and later can execute
them at a different thread
so problem of capturing
arguments is much simpler.
- [Audience Member] Okay.
As long as there is no
relationship between the arguments.
- [Audience Member] Yeah, okay, thank you.
- [Mike] Hi Gor.
Hi Mike.
- [Mike] Could you, I'm sorry,
I didn't quite follow what you were doing
for error handling and if
we need to use error codes
as opposed to I think
in most of your examples
you were throwing exceptions.
Yes, easy, easy, I mean
that's, we'll do it live.
- [Mike] Excellent, I'll
take my answer off the air.
Yes so let's just go to our hard way.
And how about we will,
instead of throwing a
system error here, if...
If there is an error, how
about I will return minus one?
Right, so you don't have to
throw, if you want, or actually,
if we had like std::expected,
I could simply return this
one and then the call site
you will get either an error or a value.
And you can do it in many different ways,
maybe there is an extra
argument of this function
and then it turns into the one
that gives you either an error or a value
so it's all in the awaitables,
you are right, up to you.
Did I answer your question Mike?
(audience member speaking off microphone)
Okay good.
- [Audience Member] In the easy way,
there was a function
which returns suspend_always
or suspend_never.
Why it's a function and not type dif?
Sorry, easy way.
- [Audience Member] Yeah.
Look, in an easy way I
didn't do any awaitables,
I just put boost_future
and it just worked.
- [Audience Member] Yeah I mean
in the exposition of
promise type for future.
Oh okay in our very first
coroutine we have everything.
This one?
- [Audience Member] Yes, line 11, 12.
Line 12, okay.
- [Audience Member] So it's,
this function already
defines what type it returns,
suspend_never,
can this function do
anything more useful or...
Oh absolutely, absolutely,
so for example imagine that
we have some external handle
pointing at our coroutine
and that external handle, and
here we have the ref count.
So we have like atomic int count.
Now in a final suspend
I will go and decrement.
Decrement the if count
and if it hits zero,
then I don't want to suspend.
And, actually let's say
bool done equal minus count
and then I will return an awaitable struct
which will say suspend_if
that will take a boolean
and it's await_ready,
it will return that boolean
then I will just have the same
and I'll keep the rest
and here I will just say
suspend_if not done, something like that.
- [Audience Member] Thanks.
Yes.
- [Man] From my understanding,
the when once the compiler encounters
one of the three queries
in a function body
it assumes that this is a coroutine
and after that it essentially makes
some sort of transformation
of the function body
into some sandwich can
which can which saves state
and then restores it from
some temporary storage
like of several entry points,
can you please give more
insight on what exactly is done?
Like when you do lambda
there is an anonymous struct
which calls all the free variables and--
Okay so I, I would like to
refer you to two presentations
which one is coroutines under the covers
and another you can look
at LVM dev, LVM coroutines
which go into details
but at a very high level
it transform the function into a struct.
Your initial call to a function,
it's a constructor of that struct
plus allocation memory from that struct.
- [Man] So basically there
is some call operator
and there is a giant switch
which enumerates in some sense?
Conceptually, yes.
But you probably saw this
wonderful breaking slide.
- [Man] I know.
This one right that all
of that switch goes away.
- [Man] It terminates with--
Yeah, yeah, but conceptually
yes there's a switch,
there is the location,
there is the destruction.
Yeah so it from a function
creates an object with
constructor, destructor, and move forward.
And move forward and the
destructor had the switch
because the state needs to
be destroyed differently
depending on which suspend point you are.
- [Man] And this switch is
literally a switch on some enum
or the state is more complex?
It's literally initially a
switch, then it compiled out.
- [Man] Okay thank you.
Sure.
I think we're officially aging
but I think we can keep going.
- [Audience Member] Is there
a reason there's no co throw?
I was writing some unit tests
validating that exceptions
worked in my case
and I wanted to write a function
that always threw that was a coroutine.
What would be the behavior differently
with throw than the co throw?
- [Audience Member] The
case that comes to mind is
a coroutine that returns void
like nominally runs off the end
but in some cases throws an exception
like you have co return, you
have co yield, or co await,
a function that contains
those keywords is a coroutine.
Yep.
- [Audience Member] But
you can't make a function
a coroutine if what you wanna do is throw.
Oh I see, I see, okay I got the question
so if the only thing that is
in your body of the function
is a throw you cannot make it a coroutine.
- [Audience Member] Right you need--
Well can you stick a
coroutine run somewhere?
- [Audience Member]
Well I faked it yes but.
Yeah because like I really
hated co return initially
when it was introduced
but I found a use for it.
It's to make anything a coroutine quickly.
I put co return and it becomes a coroutine
so maybe that will work, but no.
- [Audience Member] I mean I
was able to find a workaround
I just wondered if it was
an oversight or what it was.
No, what if there is nothing in the body,
how to make it a coroutine?
So we have limitations but
it was a design choice.
Other languages they change
the function signature,
they put some attributes
on the function itself
and thus they don't have to
have those special keywords.
For us we chose not to play
with the function signature,
it's just presence of those three things.
- [Man] Is it possible
once you have a co_await and a coroutine
you're kinda suspending your execution
usually is it possible to
start execution from there
more than once?
So in other words take your current stack
and make a copy of it and
then resume it multiple times?
As long as you make a copy it
must be at the same address.
- [Man] Must be at the same address?
It must be at exactly the
same address as original.
- [Man] Okay.
Just because we don't know
how to copy coroutines gracefully
- [Man] Okay.
Look at that we are really
done, thank you so much.
(audience applauds)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>