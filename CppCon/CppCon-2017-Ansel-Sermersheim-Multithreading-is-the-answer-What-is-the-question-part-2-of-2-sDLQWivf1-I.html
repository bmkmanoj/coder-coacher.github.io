<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Ansel Sermersheim “Multithreading is the answer. What is the question? (part 2 of 2)” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Ansel Sermersheim “Multithreading is the answer. What is the question? (part 2 of 2)” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Ansel Sermersheim “Multithreading is the answer. What is the question? (part 2 of 2)”</b></h2><h5 class="post__date">2017-10-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sDLQWivf1-I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- All right.
Well, thank you to all
of you for coming back
for round two of multithreading.
So, as I eluded to toward
the end of the first segment
of this presentation, there
are more questions now
than we started with.
So, what is the real question?
The question is how can we do
this without losing our minds?
So, I've gone over what
multithreading is and some
of the terminology involved in it
and when to use multithreading
and when to avoid it.
Now, let's talk a little
bit about how to review
multithreaded code because
this is extremely important.
Doing code reviews is an
awesome thing and if you do
code reviews and you work in
a multithreaded environment
and you don't know what to
look for you will miss things.
We first have to deal with
the case when you wanna
work with data that's
being read and modified
from different threads.
Out of order here.
(mumbling)
So, if you have two separate
classes, each of them
have data and in each of the
destructors of these classes
you need to update data
in the other object
and all of a sudden those
of you who have done
some multithreading will
go, oh, this doesn't sound
so happy because now I can
see deadlocks on the horizon.
I can see all sorts of places for this
to go horrifically wrong.
So, how do we handle this
with minimal blocking?
This is where I started
the design of libGuarded.
This is why libGuarded
exists, just to solve
this exact problem that we encountered.
Let's first look at the
sample of how we look
at shared resources in C++11
and many other languages.
Say we have a phone booth.
If you can actually find
one out there I'd be
very impressed, but if you
could find a phone booth,
well clearly only one person
can use the phone at once.
So, since we want only one
person to use the phone
at a time we make a rule.
We love rules.
We're programmers.
We live by rules.
We make a rule that in
order to use the phone
you must be holding the handle,
the door handle of the phone booth.
This works
because only one person
can hold the handle
of the phone booth.
When you're finished
with the call you let go
of the handle, the next person can come in
and use the phone booth.
So, we're using the
door handle as a mutex.
We're saying if my hand
is on this you can't use
this other object.
But this is kinda
confusing because the mutex
is a random variable name
that has nothing to do
with the data and shared
data is what we're trying
to manage, so we have a
couple of different mutexes
here or there.
There are different types of mutexes.
They mean different things.
Quick, what does my mutex2 protecting?
I have no idea.
What am I getting access
to when I lock it?
I don't know.
I can't read this code.
I don't know what it means.
Let's talk about how to
review multithreaded code
effectively and practices we can use
to make it reviewable.
Let's go back and look
at some nice, simple
single threaded code,
basic C++ code.
We have some method, it's
gonna create an object.
It's gonna instantiate a new one.
It's gonna do some
work, do some other work
and return it.
Does anybody have any problems with this?
Does this look like good modern C++ code?
Yes.
It's returning a raw pointer.
We don't like that in
our code review, do we?
This is not safe.
Any other comments on this code?
Exception safety.
I'll give you both credit for that.
Exception safety.
So, I know.
I can fix this code and
I can get it through
the code review.
What I can do is I can add a comment
above this code that says
do not mess up calling this code.
Never add any call inside this function
that could potentially throw an exception
and dear user, please
make sure to always delete
this object when you're done with it.
Does that solve the problem?
You happy in the code review?
I commented my function
with everything that you
need to know to use it
correctly, so it's usable.
No, we don't do things that way anymore.
We're C++11, 14, 17 programmers.
We have smart pointers.
We don't do this anymore.
What if we have
a slightly more complex example?
This is a multithreaded
example and I have a cache
and I've got the data structure.
It's just a regular std map,
got some information in
it and I have the ability
to insert data and look up data by key.
Any problems with this?
The object is never deleted, okay.
So, I've shown you lookup so far.
That's right, I don't have
an insert on this slide.
Yeah, so who's responsible
for deleting this object?
Absolutely.
Any other comments?
I'm sorry.
The thread annotation for
guarding the variable.
I'm sorry.
I didn't quite understand that context.
Okay, so you've got a question
about the locking here.
Okay.
Any other comments?
(person in audience responding)
So, your comment is about
simultaneous reading
and actually in this case
since I'm using a shared
time mutex and I'm doing a shared lock
then I do have the ability
to do simultaneous reads
on the data structure, so
my performance is fine.
This'll work.
I have multiple simultaneous reads.
Now we're getting to the meat of this.
There's a couple of problems here.
We have problems with
cleanup, because we've got
a raw pointer.
That could be fixable.
We could use a shared
pointer and things like that.
But, it's hard to see, it's
hard to review this code.
I'm using a shared lock.
Operator bracket is not const on a map.
If the key isn't present in the map
operator bracket inserts an empty entry.
This is a write.
We have a race condition.
I'm writing to the map
with a read lock held.
This is really bad.
This will corrupt the
internal data structures
and you will have a crash.
So, this is a problem
because this was hard to see.
This is a large room.
This is a good size code
review and many people took
some time to be able
to see this I will bet.
There's a better way.
We shouldn't have a mutex and an object
because they're not used
as one consistent thing.
We can encapsulate.
We have objects.
So, I'm gonna set up a
template called guarded.
Guarded is a container
for some data and a lock.
So, I'm gonna set up
this template and you
can construct a guarded
it just has a template,
a constructor, it passes
whatever arguments that you
have into the contained data
and have a function called lock.
This kinda looks like a mutex.
It has a function called lock,
but the difference is in the mutex
lock doesn't return anything.
Lock just says, yea, you locked it.
Well, what did it lock?
I don't know.
In this case lock actually
gives you a handle
to the data that's being
guarded by this object.
So this means that the
only way to access the data
being shared is to lock it
and when you relinquish
control of it because you're
handle goes out of scope
it's unlocked.
How do we do this?
Well, so we set up and
it also has the try lock
implementations that
you might need as well.
This is nothing more than the combination
of an object of type T
and a mutex of type M.
That's all this is, two
things stapled together.
And, all we have to do in
the constructor is just
forward the arguments.
That's a nice C++11 feature.
We have that.
And then in the lock
method, what do we do?
Well, we lock the mutex
and then we encapsulate the unique lock
which is a movable lock.
So, when we call lock, when
we create the variable lock
we're creating a unique lock which is a
reference to the original
mutex that can be moved around
to various points in the program.
And then, we set up this
handle which is a combination
of a pointer to the
object and the deleter,
which will release the
lock when the handle
goes out of scope.
And try lock does exactly the same thing
with modifications for
dealing with the fact that
it might fail, so I need
to return a null handle
which will act like a null pointer.
And the deleter itself
is nothing more than a
fairly ordinary unique
pointer deleter that also
maintains the unique lock
so that when the value
is no longer needed,
when the unique pointer goes out of scope
we can release the lock.
That's all I need.
Now I can go back to my restaurant
and I can implement something
a little more simply.
This is not a huge change,
but it makes the code
more reviewable because if you see,
when I lock the brick oven
I get an oven handle back.
This is the only way you can
get access to the brick oven
which means I now cannot have
a race condition in this code.
I can absolutely 100%
guarantee there is no race
condition in this code
accessing this shared resource
because this resource has been guarded.
There's no other way to get access to it.
This is a nice property
because I can now look
at the code and know
that it's correct
or at least it has no race conditions.
Not a huge difference,
one line.
That's not that significant.
But, this generalizes.
There are many other
forms of guarded variables
and these can be very useful.
So for example, shared guarded
is just a generalization
of guarded to read/write access.
You have the shared guarded class.
It has very similar
implementation to the previous one
but in this case our M,
our mutex is a shared mutex
and we have the ability
to do the standard lock
and try lock.
We also have the lock
shared and try lock shared.
Need to return shared handle.
What's the difference between a handle
and a shared handle?
Constinence, exactly.
So, if you acquire a shared
handle to a shared resource
it's const.
You can't modify the data.
And, a little more implementation here.
Again, it's just an object then a mutex.
This costs nothing.
This is a zero cost
abstraction because we needed
the mutex and we needed the object anyway.
This is just encapsulating
it to give us a better
syntax for dealing with shared resources.
Now, if I implement my cache example
and I put the map in a
shared guarded container
I can be guaranteed
that if I were to write
this code the old way
and I used operator
bracket, it won't compile
because operator bracket is not const.
It will correctly detect
the fact that I'm doing
a write access to something
that I promised to only read.
So, I figure that out.
I realize oh, I have to use find.
This is the non-mutating
way that you access a map
and I write the code to do the lookup,
check and see if a value was in there
and return it appropriately.
Any questions on that part?
Yes sir.
(person in audience responding)
Could I have put const on
lookup and mutable on mutex?
In this particular case
yes, that would have solved the problem.
In the original case if I had made lookup
a const method and I had
made the mutex mutable
that would solve the problem.
It would not solve the problem
in the general case however
because you're assuming
that there's for example,
only one piece of data in that class
and you only have one mutex
and so the constiness
of the method mirrors
the constiness of the data, yeah.
But yeah, that is one possible solution
that will solve that exact case.
This is much more general.
So, let's look at the insert.
Does this look good?
Does anybody have any
issues with this code?
Now, notice you're actually
looking at the code
and you're able to reason
about what this means.
I'm calling lock, which means
I'm getting a write handle
to this data
which means
I have the ability to write to it
in places non-const.
What don't you see here?
A separate mutex.
What don't you see here?
A separate variable that has no meaning
other than to guard access to
a particular piece of data.
This is a concise
representation of what we're
actually doing.
We wanna get access to a piece of data,
write access, modify it and we're done.
We can generalize this a little further
because what if we're thinking
in a very functional way
and instead of looking
at imperative programming
where we're gonna get
access to a piece of data
and do stuff with it, we
just want to be able to say,
this is the operation that
we'd like to do to mutate
this data.
We now have a way to phrase that
and this is a class
called ordered guarded.
Ordered guarded, instead
of having a lock method
has a modify method
where you pass in a functor,
that functor receives the shared data
with it already locked,
does the work and returns.
This is kind of interesting
because now if I have
a piece of code that uses ordered guarded
I have an interesting guarantee.
If I know the functor that
I'm passing in terminates
I know that that lock will
not be held indefinitely.
I know that that lock
will be unlocked once
the operation is done
because I've moved the
handling of the lock/unlock
into the guard.
Again, as questions
come up please feel free
to raise your hand as we go through this.
(person in audience responding)
So, the question was, does that
not happen with the handle?
And the answer is,
well, the handle is movable,
so you could acquire a handle,
move it to somebody else,
move it to somebody else,
move it to somebody else,
move it into some object
that's long lived, keep moving
it and you could hold it
for an arbitrary amount of time.
Now, I've said if I see
somebody using ordered guarded
in code review, I know
that all I need to see
in order to know that
this lock will be released
is the code inside this functor.
I don't need to examine
anything else and it's
the only thing I need to
know is, does this functor
ever return?
That's a much simpler thing to look for
than, did this value ever
escape this piece of code
into some other place that
I need to be aware of.
(person in audience responding)
This is a, it constrains the
cases by which you can have
the resources locked
indefinitely to those cases
where the functor never returns,
which we're not in the habit
usually of writing code
that never returns, so
it's much less common.
(person in audience responding)
Could you return a const handle and avoid
the movability of it?
You could certainly make it a non-movable,
non-copyable type, but that's
another direction to go there.
Non-copyable, non-movable
types are kind of annoying
to work with in this context.
The other reason to go this
direction is that we have
the same shared lockability
here because shared handles,
I don't care as much if they
escape, so I'm fine with that.
So, I have the same shared
locking so you can get
read access to this
value any way you'd like,
but you only can modify the value through
this very specific API.
So now, let's generalize
this a little further.
Let's first look at the
implementation of modify
which is just to lock the mutex
and then call the function.
So again, this is not complex
code, but by wrapping this
inside the functionality of
the class we can make more
guarantees about what is
possible from a threading
point of view.
So, if I redesign the cache
example using ordered guarded
this is what I now have.
And, it's slightly more
complex because I now have
to pass in a lambda, but I
can reason about the behavior
of this in a way that
I couldn't previously
because the locking is no
longer in imperative code.
The locking is a side
effect of the way I'm
modifying the data.
(person in audience responding)
You're saying this is
not quite correct because
emplace is not going to
change the map if key
was already in the map.
That's true.
That's a good point.
We can generalize this still further
because now that we have a
syntax we're talking about
the modifications to data.
Well, now we can say,
what if we want to defer
the modification, eventual consistency
can be very useful in certain scenarios
and I'll explain why.
But first, the deferred
guarded class itself looks
very similar to ordered guarded.
We have the modify.
It's now called modify
detach because the semantics
are, I want this modification to happen,
but it's going to happen
at some later time.
I don't know when.
So, it's detached from my processing.
There is also modify async
which like the standard
library async function
says takes some operation
to be processed and gives
me a future that I will
be able to query to see
when the task has been done.
It may happen immediately, it may not.
So, this is a great
way to get access to it
so you can mutate the
data, generate a new state,
return that from your
functor and then get access
to it in the calling
thread at some later time.
I said this has a really
interesting property.
Why is it so interesting?
In order to implement
this we need a mutex.
We also need a little more
information because we need
to keep track of whether
there's pending work to be done
because this data might be
locked for shared reading
at the time you try to
make a modification.
In that case, we'll just
defer the work til later,
so we need some housekeeping
information to keep track
of the modifications that are pending.
Now, I re-write this with modify detach.
It looks very similar to the
example just a moment ago,
but there's an interesting difference.
Since this is detached
it's eventual consistency.
I can look at this code
and without looking at
the implementation of this
functor, without looking
at any of the work that's
being done in this function
I can tell you with absolute certainty,
this code is deadlock free,
accessing the shared data
because we always have the
option to defer the work
until a time at which the
resource is available.
This is a really neat property to have
because I can now prove that
my code has no deadlocks
and no race conditions.
I like being able to prove
that my code is correct
in a code review
and I like being able to
see that other people's code
is correct in a code review.
Any questions on that?
(person in audience responding)
There's a long sequence of generalizations
of this concept you can build.
So, I've built guarded, shared
guarded, ordered guarded.
Most of these are available in C++11.
Since some of them need a
shared mutex they require
either C++14 or Boost
Thread or your favorite
implementation of a shared mutex.
Then we have deferred guarded
that I just went over.
There's also lr guarded
which uses the left/right
algorithm to obtain a
lockless implementation
of the guarded concept.
This uses more memory
because it has to maintain
two separate copies of
whatever it is you're guarding,
but on the other hand,
readers never block.
It's completely unblocking for readers
and readers never see data
older than one right ago.
So, you can never have more
than one pending right.
There's a bunch of very useful properties.
There's an entire menagerie
of guarded semantics
that you could develop
based on this concept.
So, this is awesome.
I've solved multithreading.
It's done.
There's one additional one, the
copy on write guarded system
which gives you shared
access without locks
because since it's copy on
write the data never changes
while you're looking at it.
This is also nice 'cause it has
some transactional semantics.
You can always cancel a pending write
because you just throw
away the copy that you
were working on and it
reverts to its original state.
That's very useful.
Like I said, multithreading, done.
Talks over, we can go home.
And then, we tried to solve a problem
again that I developed
libGuarded to solve.
In our CsSignal Library
we have a sender object
and a receiver object in
every signal connection.
So, I have a pushButton
connected to a window.
When the pushButton is clicked
I want the window to close.
The sender has a connection list.
It needs to know who to send signals to.
The receiver has the connection list.
It needs to know who to disconnect from
when it's destroyed.
So, each of these destructors
has to update the list
in the other side of the connection
and I tried to implement this using
the libGuarded technology
that I've showed you
and it didn't work and I
couldn't phrase this problem
in terms of the
abstractions that I'd built.
So, I took a step back and I
said, okay, is there another
way we can do this?
Is there something else we can
implement along these lines
that gives me the behavior I want?
So, first of all, what order should I lock
the containers in when I do this?
Well, I have to lock the senders list
and the receivers list and
the pushButton destructor
has to read its own
connection list and then lock
the senders receiver list.
And, the window destructor has
to lock its own sender list
and lock the senders connection list.
So, oh, I can't order
these in any direction
that will induce a correct
ordering for everyone
because I can't even find
the sender as the receiver
until I look in the connection list.
So, what do I do here?
Well, there's a lot of
algorithms for dealing
with deadlock avoidance
you can find in textbooks.
So, the first option is the
most popular, unfortunately
It's not a problem.
Deadlocks are rare.
It's no big deal.
And then,
deny responsibility is getting
harder now that we have
pretty much universal
access to source control,
so I don't really recommend that one.
You could always just wait,
lock, unlock, repeatedly.
Maybe that'll work.
That test is flaky, so
we're not gonna trust
that that test is correct.
We'll just pass CI anyway.
It's vitally important if
you have this situation
never to run thread sanitizer on your code
because it will find this problem
even if the deadlock does not occur.
It will warn you of the
incipient deadlock in your code.
So, just pick one of these
and we can stop there.
But, I'd like a solution
that actually works.
So, when we were designing
CsSignal, we were saying,
okay well, this is a
library that needs to be
multithreaded, but we're
delegating all responsibility
for threading to libGuarded.
We have a threading problem.
Therefore, it should not
be solved by the user
of the threading library.
It should be solved in
the threading library.
It's totally valid for
both of these destructors
to be active simultaneously.
We need a sane result.
So, we need a guarded
type that can handle this.
We didn't have one.
So, here's the properties we need.
We need a thread aware container,
not just a thread aware object.
We need a thread aware
container where the writers
don't block the readers
and readers don't block at all
and iterators are not invalidated
when a container is modified.
If I had these properties
and I don't care how
the implementation is.
Then, as the writer of the signal library
I can write my code and it will just work.
So, how do we do this?
How do we in libGuarded
provide this abstraction?
Two classes,
RCU guarded and RCU list.
Since we're implementing a container
these classes need to work in concert.
The RCU guarded is the outer wrapper
that enforces access
to the shared container
and then the container
itself has to be written
in such a way that it's
aware of the threading
that's going on.
So, what's RCU?
Well, it's an algorithm
used in many pieces of code
including the Linux kernel.
It's a well established algorithm
for managing linked lists
in a multithreaded way.
The classical RCI algorithm
is one writer at a time.
So, writers block writers.
That's okay.
We can deal with that requirement.
That's fine.
We have room for multiple
concurrent readers.
The readers take no locks at all
and readers never block writers.
If I can make this work
then I can make my solution work.
How do you do this in C++ user space?
The Linux implementation
of this is very different
from a user space C++ implementation
because it can depend on the
fact that it's the kernel
that knows how many threads
are active and it knows
when they're busy and when they're not.
We don't have any such
facility so we need to build
a slightly different system.
The basic bare bones of RCU is actually
fairly straightforward.
We use a link list of
notes and whenever we wanna
modify a node, instead
of modifying it in place
we read that node, we make a copy of it.
We update pointers, so
all subsequent readers
see the new copy and then later
we get rid of the old node.
Later.
What's later?
How do I know?
When?
Well, as I eluded to, the
kernel knows when later is
because it knows when all
threads that could potentially
have access to a
particular piece of memory
are no longer active.
We don't have access to that.
We don't have a grace
period during which we know
all threads will not be touching memory.
We know that users like to
hold references for a while.
They might have use for
this container for some
decent period of time,
so we don't wanna enforce
things like, you can't
sleep while you're holding
a reference to this.
That's hopeless in user space.
We don't know how many threads there are
so we can't know when they're all done
and we don't really wanna block the writer
until all the readers are finished
because we don't know how many there are.
It could be starvation.
So, what we do is we set
up the RCU guarded class.
It has a very simple API.
It has the ability to lock
the container for reading
and lock it for writing
and as you would expect from
the way that shared guarded
is implemented, the read
lock gives you a const handle
so you can't mutate the list.
You can traverse it, you
can't modify anything.
The non-const method
takes an exclusive lock
'cause remember, writers
block writers in this case.
So, it returns a right handle.
You have access to modify
the list as you see fit.
The RCU list itself
just acts like a container.
It's SDL compliant container.
It has a beginning and an
end and an insert, push back,
all the things you would be
familiar with with a link list.
The insert algorithm just
allocates the new node.
We initialize the pointers
in the new node to refer
to the adjacent locations in the list
and then we update the
next and the previous
and the adjacent nodes
to point to the new node
and this works because,
since we do all of these
updates atomically, since
we have atomic values
in C++ this is straightforward
to use the standard
library for.
As a reader is walking
concurrently it will either see
the new node or not.
We don't make any guarantees
about consistency,
just correctness
and the only guarantee
we have is if as a reader
you traverse the node,
if you traverse the list
from the beginning to
the end you will see all
of the nodes which were alive
during the entire period
of traversal.
Other nodes which were
created or deleted while
you were traversing the
list, you may or may not see,
but you will successfully
traverse from a beginning
to an end of the list.
Erase is a little more complex.
The first thing we do is
update the adjacent pointers
to take this node out of the list
so that subsequent
readers will not see it.
We mark the node as deleted
so that we know that this
is one that has expired
and is no longer needed.
And then we add it to the head of a list.
This list is called the zombie list
and it consists of nodes
which are no longer part
of the list if you were to
traverse it, but they may
still be referenced by some active reader.
Again, concurrent readers
might see the old node
or they might not.
It might be gone by the
time they get there.
And there are a few corner
cases to deal with when
you're dealing with the head
or the tail of the list,
but this is a piece of code
that just uses the intrinsic
C++ atomics and does this algorithm.
Now, we get to the heart of this system
which is the zombie list.
We need a way of knowing when
each node that has been erased
could no longer be referenced
by any active reader
and the way we do this is
by this ancillary singly
linked list structure.
Each one of these nodes
contains a pointer to the next
structure in a zombie list,
a pointer to the actual
zombie node that it is
protecting and a pointer
to essentially the iterator
that is active in reading
this current area.
When we lock the data
structure for reading we add
an entry to the zombie list.
There is some cost to a
read/lock in this implementation.
When the reader is finished
we walk from that point in the zombie list
forward seeing if any other
reader is still active.
If there is, we're not the oldest reader,
so we just remove ourselves from the list
and leave everything as it is.
If we get to the end of the
zombie list before we find
another active reader
then, all of the entries we
walked over along that path
could not be referenced
by any other reader.
We are the oldest reader
alive and all of these nodes
were deleted before we started reading.
Therefore, we can free them all.
So we do so and we free the
zombie list entries as well.
Any questions on that?
(person in audience responding)
What's your memory overhead?
The average CPU overhead.
It's a few bytes of memory and one malloc
which is unfortunate, but it's required
for this implementation
of the algorithm to work.
This is one place where a
very efficient threaderware
allocator is very useful
and many implementations
are quite good these days.
And, this is required
if we want the property
that writers don't have to
wait for all the readers
to finish because we want the,
essentially the last reader
who could potentially
have seen the data to do
the cleanup of that data.
Does that make sense?
(person in audience responding)
I'm sorry.
It's true.
There is no separate thread
that does the reclamation.
That is also a possibility.
There's many ways to
handle the reclamation,
but this algorithm allows
you to know deterministically
and precisely exactly when
every piece of deleted data
is no longer needed by any active reader.
(person in audience responding)
Why a zombie list and
not a ref counted node?
That's an excellent question.
And the reason why a ref
counted node doesn't work
is because you would have to ref count,
you would have to somehow
know how many readers
were active at the time
the node was deleted
and you would have to then
have each reader check in
and decrement the ref count
of every node that was deleted
while they were processing.
So, it could be potentially
a very expensive operation
if you have many deleted nodes.
At least this way it's a
single, singly linked list
traversal for each reader termination.
(person in audience responding)
He said, would it be the
same because you're just
compressing the ref count,
compressing the nodes
that have a ref count of zero.
If I'm understanding you correctly,
it's not quite the same
because the zombie list
only contains nodes
that have been deleted.
So, it's only proportional
to the delete band width
of your process.
Whereas, if you had a ref
counted implementation
I think there would be
places where you might have
to do work proportional
to the number of nodes
that were traversed
during the read process
which, it depends on your
particular situation.
It might be a win in some workloads.
But, it seems like it
would not be in most cases.
Does that answer your question?
Got a question?
(person in audience responding)
The question was something
about sequential consistency.
I didn't quite get the full context.
(person in audience responding)
If you had one thread writing
and you had another thread
holding onto a stale read,
then you would eventually
have ordering issues.
No, you wouldn't
because as nodes are inserted or deleted
the reader that's traversing the list
will either see the
nodes or not, but it will
always make forward progress in the list.
No node would ever move
positions from one place
to another in the lists and
since we don't de-allocate
a node until all readers
that could potentially
have seen it are finished reading
there's no possibility for the ABA problem
where you de-allocate a
node and then allocate
a new one at the same address.
That can't happen while
a reader is active.
Does that answer your question
or am I answering a different question?
(person in audience responding)
Okay,
definitely, talk to me afterwards
and we'll figure out,
we'll hash that one out.
Other questions?
(person in audience responding)
- [Person In Audience] Quick
comment on the question
that was asked before.
It's possible to do it
with a reference count
and there are some interesting
tradeoffs in performance
you can make.
The straightforward implementation
has all the problems
Ansel is talking about.
There's another implementation
that if you want to hear
about that, come to my talk
tomorrow, but you can get
a better performance for
readers at the expense
of the writers.
You can get better performance
for writers at the expense
of the readers.
There are a lot of different
tradeoffs you can do.
It's your whole complex mess
of these RCU algorithms.
- Thank you, Ed.
As identified, every one of
these algorithms has multiple
tradeoffs and which one
fits best in your scenario
is sometimes quite a
research project to discover
what's gonna have the
performance behavior you want.
So, some additional notes
about the RCU lists.
The read lock as you would
expect returns a read handle
to the RCU list and a
read handle can be used
to acquire an iterator.
And this iterator is valid
as long as the read handle
is in scope.
Normally, when you erase
elements out of a list
it invalidates iterators,
at least to that element.
But, this is a really
nice guarantee and it's
a guarantee that you
actually must have for safe
traversal of a concurrent
data structure in that no
operation on this list will
ever invalidate any iterator
that is valid at that time.
- [Person In Audience] Just for historical
purposes there actually was a
variance of the Linux kernel
that used something kind of like this.
They didn't have the snapshot capability,
This was C rather than C++ after all,
but it did use the trick where the unlock
would do the cleanup.
- Okay, good to know.
- [Person In Audience] It was specialized
for realtime systems,
fairly small CPU accounts
and it was the realtime
variant for a while
til I figured out a way to
make something like, to make
realtime print for large
use scale to thousands
of processors, so historical note.
I mean, it's a way that has been done
so I would consider that a
compliment in some sense.
- All right, thanks for the note.
Another item that's required
here is there's a very subtle
issue when you're traversing
an RCU list of this type
which is, if you say pick
a range out of the middle
of this list, say you grab a
iterator to some random element
and a iterator to some other
random element and you say,
I wanna traverse between these two points,
you may fall off the end of the list
because your end iterator
may refer to an element
that isn't in the list by
the time you get there.
So, you never stop.
This is a problem.
The fix for this is to
disallow comparing iterators.
If you disallow comparing iterators,
how do you ever know when you're done?
You have begin and end
return different data types
so, begin returns a normal
iterator, what we would
all think of as a standard iterator.
End returns just a
centinal value that denotes
the end of every RCU list.
So, the only thing you
can compare is an iterator
against end.
You can't compare two iterators together.
This unfortunately, means
that until you're in C++17
you can't use this RCU
list in a range based form
because this was, it was
not permitted to have
begin and end return different data types
until C++17 if you are
using range base for syntax.
Just a little minor note to be aware of.
You can still use it in a regular for loop
or a wild loop or whatever
other construct you want.
There's no synchronization
between readers, so modifying
an element, the data in
an element of this list
could result in a race
condition, therefore,
all iterators are const.
This will prevent you from
having race conditions
unless you have mutable data
in your elements
which is a bad idea.
So, in order to modify data in an RCU list
you wanna use insert and erase.
You shouldn't ever modify
the elements directly.
If you look at it and go,
oh, this iterator is const.
Let me just make my data
mutable so I can get around it
you will shoot yourself in the foot.
And, as I mentioned in my
first talk, it's gonna be more
content on this and many
other subjects on your YouTube
channel, new content every two weeks.
All our information is
available on github.
You can contact us via
email or get all of our
source binaries and documentation files
on our download website.
And, please do subscribe
to our YouTube channel.
We have more information
on multithreading, C++,
our libraries, data types and all manner
of things of interest to
the general C++ community.
Thank you very much.
Questions?
Wow, I answered the questions apparently.
- [Person In Audience] If no
one will ask a question I will.
So, you say multithreading
is a solved problem.
- I would say multithreading
is a solved problem
quite sarcastically.
I hope that came across.
- [Person In Audience]
What I want to ask is,
would it make sense to
combine this library
with sort of a executor based pattern.
I think they could inter
operate really well.
Do you have some high
level instruction on top
of a thread?
- Yeah, there definitely
is room to combine
something like this with executors.
This would be sort of a
natural building block
for working with executors,
thread pools, all kinds
of more high level
abstractions that deal with
synchronizing and sharing
data between threads.
- [Person In Audience] Would you consider
it a separate thing?
- I think I would.
I think I would consider
that a separate layer.
This is sort of,
libGuarded, in my opinion,
is kind of the atomic unit of shared data
if I can overload a term
in a really terrible way
to work with shared data
in a multithreaded program
correctly and like anything low level,
you can always build
higher level abstractions
on top of it that have
greater creature comforts,
more performance implications,
things like this.
I think something like this
would be very useful to have
a guarded data aware work queue
because it would give you the opportunity
to do things like scheduling
tasks, whose resources
are available, things like
that, that's well outside
the scope of this particular
talk, if not this library.
But, it would be I think, a
very useful thing to have.
Thank you.
Any questions?
Thank you all very much.
(audience clapping)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>