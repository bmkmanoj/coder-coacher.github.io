<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Scott Wardle “ EA’s Secret Weapon: Packages and Modules” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Scott Wardle “ EA’s Secret Weapon: Packages and Modules” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Scott Wardle “ EA’s Secret Weapon: Packages and Modules”</b></h2><h5 class="post__date">2017-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NlyDUQS8OcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- I'm Scott Wardle,
and I'm gonna talk to you
about EA's secret weapon.
Packages and Modules.
But before I do,
I have some kind of bad news.
Igor Maslov, who's a coworker of mine.
He ended up
dying recently.
He was kind of the go to guy at EA
for packages for quite some time.
He was reviewing this presentation,
that's the last time I saw him.
So hopefully I live up
to his expectations.
But who am I again?
As I said I'm Scott Wardle.
I'm a senior software engineer with EA.
I've been working on FIFA
for a long, long time
and been over 20 years in games.
Mostly I'm known for systems programming.
What I mean by systems programming
is making things technically better.
You know, best type of better.
So something measurable,
changing frame rates,
or effecting build times,
or
less memory, less space on disk.
Some kind of measurable
technical achievement.
Obviously then we have to
discuss with the team itself
and figure out what does
technically better mean,
and I write metrics for
that type of thing as well.
So memory monitoring
and that sort of stuff,
but that's not really
what this talk is about.
This talk is about the other
questions I get which is,
okay I want to make
things technically better
but I don't know how to measure it.
Things like,
I want to share more or engineering.
I want them to be faster
and more productive.
These things I really
don't know how to measure.
And if I talked about those things
at C++ con then everybody seems
to talk about modules here.
They sound like magic
and they're going to fix
all of the things everywhere
and so I wanted to look into them
and try to understand what they were.
Because I didn't really
understand them so.
One thing I did understand is they were
attempting to fix build times.
So this is nice, this is
easy to measure stuff.
You can just run it and
if it goes faster great.
As I said I'm not gonna talk
about that kind of thing
too much other people
have talked about it.
What was more interesting
to me is things like
bad interfaces.
Things that are hard to share
can this improve that
and how is that going to work?
I don't know, the other thing was that
everybody seems to really want this stuff.
As soon as possible from
Bijarne to Blizzard.
This is obviously very very popular.
But we sort of already had some technology
that really helped us with
a lot of these things.
It was kind of the secret weapon
I always sort of relied on VA.
I can take a package from
anywhere in our company
and I can build it and it just works.
Get this, get this.
This is the technology we have.
We take a bunch of C++ code
and we turn it into a library.
I know shocking.
You know it's a build system
and everybody hates build systems
and yeah most people in EA even hate art.
So you know, bad it's what it is.
But I really do think it gives us
an unmeasurable advantage.
Being able to take source code quickly
from any team and actually get it to work.
But modules aren't that.
So modules are about, the
new way of interfacing into
maybe a package or a
library of technology.
They're sort of an interface bit.
They're smaller unit
and so they're not quite the same thing
but at first I was not really so sure.
Anyways.
This takes a little bit for me
to get through this all talk.
It kind of goes like this,
you guys don't know packages
so I need to tell you well.
Most problem we were having
and when we invent them
then I'll talk about packages themselves.
Like you know,
how did the solution go.
What problems did we
have, that sort of thing.
Then I'll talk about modules
since I don't know how many
people know about modules
but probably not all of you do
and then I can finally
finally near the end sometime.
Talk about packages and modules
together and maybe how they
would fit if they would at all.
So packages.
The problem of packages
starts a really long time ago.
It's about 15 years ago,
PlayStation 2 is our default platform.
With 32mgs of RAM was a lot of memory
and even back then.
We had a lot of trouble sharing code.
We had a lot to code to share
and everybody was duplicating everywhere.
It's easier to see and measure today
so I took a look at some
current games we have
and ten millions line of code.
Yeah that's kind of about an
average size of game at EA now.
And we have maybe 20 games or so
200 million lines of code.
That would be hard,
we don't wanna be doing that.
We don't wanna be updating it so
we need to be sharing as much as possible.
This is kind of obvious
even if it's a little tricky to measure
all these things.
So games, maybe different than a lot of
other industries and the fact
that we don't really
control our platforms.
iPhone comes out and
great you're gonna have to
work on Macs and you know.
Actually that's kind of wrong,
PS2 came out and we had to work on Linux
for quite some time anyways.
Maybe PS3 we had to for a little bit too.
Then because of that kind of thing
we don't really have
control of what environment
we're gonna be on.
We may build a system that's
but it works, the way Semic does.
It's kind of like a generator
and so you can make and
order build system from it.
So how fast it runs
and how fast it builds.
It really depends on what
build system you spit out.
Most of the time,
we build Visual Studio projects
and for better or for worse
and so this this seeps
into our vocabulary.
Most of these platforms support
visual studio and so.
We call this step
as a legend for example.
So we also support a lot of compilers
whatever the manufacturer of the console.
The newest place to write your games on.
Green hills will come out
for Wii U or something
and you'll just have to deal with that.
To that end we wrote little configuration
packages as well.
And so we could share
these across with everybody
try and come up with.
A way of getting out as fast as we could
on our games on any particular platform
and making all of these
compilers as much as we can.
On the command line look kind of the same
so pre-compiled headers
and work the same way in all of them.
So when you're building games
generally speaking it
kind of works like this.
You download everything
off of source control
somehow or another.
Usually all the data, the tools,
and everything comes here
and then you have to build the game.
By building the game,
what I mean is you generate
a visual studio solution.
Or like the solution finals
plus all the VC projects
and everything to do with the package.
And then you just call
Visual Studio to build it
and you make some very large executable.
Usually only one, sometimes
we have some DOLs but.
And then after that we cook
and we make all of our data.
And the data is actually much
bigger than all the rest.
I'm not really gonna talk
too much about cooking today.
It's quite an interesting problem,
sometimes does it does
have to do with packages
on some teams. Most of the
time it doesn't these days so.
But it is necessary for this to be done
and that every time you sync,
you're gonna have all these things change.
And so you're gonna have
to check dependencies
on all of these things
and it just shows you some
of the scale of things
and how much we're gonna care
about timing on each one maybe.
You're gonna have to build more
than you're gonna have to
generate social for example.
So yes, 15 years ago.
Large amount of code being duplicated.
We actually had multiple build systems
even on a single game team.
We would have like one build library A
or library B or for pre-built.
You want to build all your
libraries then check them
into source control so that not everybody
had to build them for example.
You had a different systems for this
and each one would have
it's own build system.
That was enough trouble on its own but
even 15 years ago.
EA was a large company,
we had development going on
in Matton down in Florida.
We had our headquarters in California.
Up in Vancouver we were on FIFA
and lots of other teams.
The Dice studios were in Sweden
and we had the UK as studios in Gilferd.
So all around the world
we had all of this development going on
and not everybody knew what was going on
in all the different studios
and we could see this and so
we wanted to be able to start to share.
So at the time this sort of new hotness
was things like Ruby Gems and CSPAN
and those kind of things
and having a package server
and so we really thought this
was going to be important.
And so we wrote a system like that
and it was actually wildly successful.
Soon games were made out of
hundreds of these packages,
we have thousands of supported packages
on our servers and of course.
We run into another problem,
how to deal with many many packages
and so this was kind
of where we are today.
Basically I'm gonna talk
about the solution of packages
over a couple of questions here like.
How do build systems really help.
I'm gonna tell you about
something called masterconfig
which I think was a stroke of genius
that we had and if there's any idea
that maybe you could copy.
This is probably the one.
Then I'll talk about package servers and
co divergence and then I'll
finally start talking about
modules and things again.
So everybody here is a C++ programmer so
you kind of know what you need to do
to an interface in order
to make sharing better.
If you,
make things private
and hide data.
Then it's gonna be easy to share.
If your library is just a
everything all in lined all the time
it's gonna take longer to build
and be a little harder to collab
between various teams.
So our build system
kind of makes us think about this
and we have a concept for
most packages by default.
Will have a private
directory for the headers
and a public one
and so you only have
to read the public one
to understand its API
and the build system itself
does the same thing.
There's two main files
to our build system,
one is a global scope one
and one is a local scope
and affectively public and private.
So what do one of these
packages look like?
They're quite simple, they're
not really that complicated.
A good example here is the ESTL,
it's our ESTL library.
It's just a directory full of files so
we start off with the directory
which is the name of a package
and then inside that we'll have a version
and inside that we will have some
public included directories
and in that we usually specify
the name of the package.
Again which is a bit odd but
if you think about it a bit.
You wanna make sure that
your headers don't collide
with one another so.
At least naming it after the package
means that vector doesn't
collide with vector.
That's a good idea
and then you'll have your source code
and your private includes
and they'll go into a different directory
and that's pretty much it.
Now,
the version in the path is
sometimes controversial.
Why we do that is because
on a team you have many executables
and you want some decoupling to
allow yourself to have a different.
Each executable to be maybe on a slightly
different version of a library.
So you don't have to upgrade
everything all at once.
Now we don't necessarily
need the version information
in the path. There is
ways of flattening it
and there's good reasons
to flatten it as well.
Like if you're wanting to
use per fourth to integrate
in a whole bunch of packages all at once.
Maybe you need to do that.
But yes this is why we have
that version in the path.
So a little domain specific language
that goes along with this.
Local scope and global scope
are the two main files for it
and each scope has its own file
and this is an example of one.
You just have to specify all the headers
and all the CPPs and for us,
we usually just put star star in there
and just recourse up the subdirectories.
So it's kind of different
than say like C maker or something.
We don't really care about maybe how warm
your solution generally takes I guess
and resourcing up the
tree means we can just
cut and paste these things into
another directory and make a new package.
It's quite quick to do.
You have your private headers.
They're also specified
and then dependencies.
And dependencies are
the things that you need to use right.
So I want the includes or I
want the linkage from here.
I want the libraries from here
and linkage is transient if you understand
what I'm talking about there
and we don't do that with headers.
You have to specify
which ones you want so.
Now what you get when
you depend on something
is basically depends on
what's on its global scope
and say if you're dependent on LZMDMA
the compression library.
You would get it includes
and it's libs and that's it.
It's quite simple
not really needing to specify much more.
Now if you had some plug in system
or various other things.
Sometimes you need more
information for this
but generally speaking
that's all you need.
So you could see that
writing one of these things
doesn't taken you that long.
Soon we had hundreds of them
and it was a pain to update them all.
We ran into all sorts
of interesting problems
it didn't really turn out as
clean as my examples here.
For example,
when I talk about the names of packages
and depending on things.
We started with the syntax like this one
where you had to depend on a package
and then specify a whole
version of it, right in line.
And of course that didn't
work very well because
you upgrade Visual Studio once a year.
So then we started going how,
maybe we needed rearranges and
all that didn't work very well either.
It was all really repetitive.
This is just, this section here right
where right now I just list of packages
but previously I would have to
specify information like that.
So why is ti bad to have version
information in the path is
well everyone's gonna have a
package like this one maybe.
This is a library
it's a header only library
which detects the compiler features.
We take a look at the
version of the compiler
and then we'll know what level
and what's support of what features
we're allowed. It's called EABase
and so nearly every package in EA
depends on EABase.
So it's really highly used.
So if you put the version information
inside each package.
You're gonna have a problem,
you're not gonna wanna update this thing.
It goes on for quite sometime for this.
It's just crazy,
if you're finding or replacing
across the whole thing
and then have to check in all these files.
You're asking for trouble,
ever for a simple upgrade
let alone a complicated one.
So we removed that information
and we did what anybody would have done.
We put it in a separate file
we put it in one global file
that is hared across a whole team
and we called this the masterconfig.
All this file, what it has is very simple.
It's just the name of the package
and a version number. Name
of package version number
and one to one.
You're not allowed to
have more than one version
of the same package.
That's just not allowed.
You try in solution gem it doesn't work
and that meant at this point.
All we had to do now is
open up our masterconfig,
change your version number.
As long as it built, check it in
and you were done. That
was really easy now to do.
Simple version changes.
But package upgrades are rarely simple,
they often take time
and if everybody's changing them.
Even hundreds of packages
if you just sort of
think about it for a bit.
You're maybe doing one or two a day.
This file ended up being, like
every time you sync it
you'd have to resolve
conflicts with it all the time
and that was problematic.
So one thing we did to
help solve this problem was
we added some variables so
that when you were generating
solution you could go.
Okay if I want to switch to
the new Visual Studio 2017.
You just pass in another parameter.
Visual Studio 2017 is enabled.
Everybody else in the project
could still us the old default one.
The default one would be there 2015,
if you didn't pass anything
in and get that one.
So you don't have to affect anybody
but you could check in your changes
that you're going to do.
Roughly speaking I'm
gonna need these packages.
It could even be broken for you
but I'm gonna need a change
like this in this file
and checking in. So you could
get rid of the conflicts
as quickly as you can.
I think no it's kind of interesting
to sort of take a step back and see.
What we have now.
If we compare there
selves to Python or Ruby.
I'm not much of a Python
or Ruby expert actually but
I think this helps a little bit.
Just in case people in the audience
know this a little better.
We had a package server now
we could put all of
our content up on that.
It was private TA
which is unfortunate but.
It kind of works similar
to like Ruby Gems or PyPy
of something like that.
If you install the package
you would install that
and all of its children
and then you could use
it in the environment.
In Python or in Ruby
and that's really cool.
The one that we had is
a little bit different,
we can you'd have to have a
masterconfig somehow or another
and then you would generate solution.
And if it was a right type of masterconfig
then it would download
all the package necessary.
So if you generate solution
for a movie player lets say.
Then the movie player,
the file IO library,
the rendering library would all come down
and it would build you an
example movie player there.
And that was really good,
this is kind of similar.
The other thing that we could do
in a similar sort of way.
Lets say the movie player,
mostly what you use it for
is just playing movies back
and that kind of content.
So maybe it doesn't matter
that it's on a really
old version of DirectX.
It's running DirectX nine,
that's fine it doesn't matter.
You can just in your
masterconfig then say,
okay if I'm building the movie player.
DirectX equal version nine
but if I'm building the game.
I want it to run on a
DirectX 11 or DirectX 12
so I can specify that.
So that's like environments
and gem sets.
Now like I was even explaining
in the previous example.
You can kind of implement like
virtual environments with the same thing.
You have this masterconfig there
and you can just have a
switch there that goes.
Okay I wanna use the latest
version of Visual Studio
and you can switch between
platforms and things
in a sort of similar way.
So now that I look at it
and I understand this
stuff a little bit better.
I think what we've done
is we've implemented something
pretty close to log files.
Log files are usually generated thing
and ours are hard coded.
But it's maybe a bit simpler as well.
But they're quite similar
in feature set really.
I wanna talk to you a
little bit about an example
of how this ends up working
with the large scale
and what we're able to do here.
What happens a lot is you need to upgrade
some large piece of technology.
Say for example a new rendering engine.
So of course what you can do right away
is you can just change the masterconfig,
add some new variable in.
That goes okay well I'm
gonna need these packages.
It doesn't really build much yet.
But that you can set it up
so you could have a small team of people
working on this for six months
or a year or something.
Cause it's gonna take you
quite a while obviously
to put together a whole rendering engine.
You're gonna wanna make sure that you have
all the full feature
set of the previous one
and you're gonna wanna meet APIs and stuff
and have it be the same way.
The people that use the
old rendering library,
they're all fine.
They're all still using
the old rendering library.
They haven't had to change at all.
But you can stay in the same branch here,
you could stay all
working in the same branch
if you wanted to.
It's kind of like branching
on individual packages.
You're at submodules and git or something.
So this is a really powerful way
of thinking of things
and you can look at it at a high level
and see what's going on just by taking
a look at this masterconfig
and see what's changing in the whole game.
And you can do this is reverse as well.
So it's not just for new technology,
when you're inventing new technology
and you've got this
small group of engineers
working on that.
You can use it for older
technology or like.
The rendering guys they
haven't quite finished
writing their new particle system gooey
or something like that
but I need content for it.
Maybe we can just keep
that old render running
for a little bit and use
that gooey from there.
It'll all be good, we can make the content
and then write an exporter to the new.
And then new one's what we're
gonna ship to the customer
but we can use the old
tools to make the new data.
And that's okay and it
takes us a little while
but after that point
you can finally remove
these old libraries away.
Now,
I think this really only works
when the teams are kind of lopsided.
You've got a 150 people
on the default version
and you've got 10 people on
the new version of the code
or something like that. Then
it works pretty well because
you don't want everybody
to have to change the way
they work and to test both
environments all the time.
That's really difficult
and takes more time.
Really what you want for say
the new rendering library
is to communicate. Oh,
we need this new feature.
So they get broken a little bit,
it's okay. They're probably the experts
in writing this API anyways.
They need to have that conversation
and they need to know
about the fact that the,
team the main team needs
to write this new feature
and as far as they know.
They need to break the API here
and maybe they'll have a conversation
and they'll be able to get around it.
I don't know, that's the idea.
If the teams were exactly equal size,
probably I would branch.
But it is, and I say in between one.
So can packages actually help
or do they just get in your way?
If you have,
if your build system helps you
specify what is public
and what is private.
I think it can help you.
Also standardizing the way
you work across a company
the size of EA is really valuable stuff.
Masterconfig I think was kind
of a stroke of genius for us,
it really helped us deal
with the problem of libraries
at a higher level
and this was quite useful.
So now I'm gonna talk a
little bit about patch servers
and code diversions.
So package servers were
really invented to try and tell everybody
about a new package and
what was going on here
and automatically
download all these things.
We could see it in Ruby
and these other languages
that this was a really cool idea but
it's kind of a bit odd when it comes to
doing it on a whole game.
This doesn't really make any sense.
You're not gonna generate FIFA
and have ti download all
of the packages from FIFA.
At some point you have to stop,
it's not a versioning control system.
It's kind of an advertising system
for low level packages that
you think are curated and
good to share.
So if you only have one of
these across your whole company
you probably don't need to put it there.
It's a place to find and try
out low level packages and
test them. All we do to put
things up on our package server
is we just take that directory structure.
Zip it up and put it on the server,
there's nothing else.
Like that's the whole steps there.
Obviously you need a masterconfig
to go along with your tests
and so one thing we had to do is.
Duplicate this versioning
information again
but we didn't really wanna do that so.
What we did to handle that
particular problem was
we just all the library teams.
Roughly worked on one set of packages
and they just cut and
paste it across them all.
So they over specified how many packages
and dependencies they had.
But that seemed to work pretty well.
This didn't scale very well
for the game teams though.
They didn't post everything to the system
and there's other
interesting problems around
outsourcers and things.
But I'm not gonna talk
about them too much today
but you can just think of.
Don't put your latest version of Star Wars
on this public server with
10,000 people or something
before a movie comes out.
That would be a bad idea.
Version control systems.
This is really what we do
is we post everything we
can to our version control.
Tools, data, libraries, everything we can
and sometimes we're not allowed
to put some things there.
You can't export controls
across country boundaries
and things but and in those cases
when we accept and call
the Apache package.
There is ways but generally
speaking as much as we can
we'll post everything onto our servers
and we'll sync and we'll build.
And this worked great for game teams.
Our library teams though
have a really big problem here.
A problem is is every single game team
will just upgrade willy nilly.
They have their own reason to upgrade,
they need a new rendering engine.
They need a new audio thing,
they need new file IO performance.
But if you're writing a movie player
what are you gonna do?
You need to write to a standard API.
You need to know what's going on.
The on;y thing we could
figure out what to do
was to force everybody onto some version
and then track them.
Make sure that we knew
how many lines of code
they were divergence
and this worked pretty well that.
We'd also then find out about bug fixes
and cool ideas like that.
This changes actually
are really necessary or
have the conversation with them that.
Yeah oh no you don't
wanna be doing that way,
you could fix it this way instead.
Because the game teams
understand the problem
but the library teams understand
where the solution goes
and how to write the solution.
But this was only actually in sports.
Dragon Age and Mass Affect,
Need for Speed, Battlefield.
All of these kind of people
were solving this similar
kind of issue with Frostbite.
They built up a huge stack of technology
and then they just integrated that across
all the various studios
and that's how they
would solve this problem.
Today we've mixing the two together
and this actually one
thing that Igor worked
quite a bit on.
Is moving Frostbite to packages
and we're trying to
improve all this technology
as we go along now.
Package servers, really
useful for low level
leaf packages. Not really
used for whole games.
Divergence is a really hard problem,
the best that we know how to do
is just to have a stack of technology
and you can see this in all
of the gaming companies now.
In that nearly all of them
are basing on an engine
and they all have one version
to write all their code against.
Okay, finally we're starting
to talk about modules.
Excellent, alright so packages you can see
that they're this kind of library things.
Modules on the other hand.
They're trying to improve
build times right?
They're better separation
between interface
and implementation and really, hopefully.
There's some viable way of
moving forward with these
and for existing libraries.
And that's kind of why I
wanted to take a look at it
is to see how viable is it gonna be.
For us to move forward.
So of course everybody here
is a C++ programmer.
You guys all know CPP
files, make object files.
You probably know that
.h files make pch files.
This is the kind of difference though
is this idea of modules anyways.
There's kind of, a one
to one relationship here
for the modules and modules
are gonna be called.
.ixx and they're gonna spit
out and ifc and an object file
in the case of Visual Studio anyways.
The names of these things
are changing all the time,
client has its own naming convention.
It's kind of interesting,
I don't think the client one's actually
a good naming connection.
But anyways I'm gonna talk
about from Visual Studio's
point of view since that
was the farther ahead thing
for the module's TS.
So think of an example
of how modules are supposed to work.
I had to write a little tiny example
and that's what I'm gonna do here.
I have three files, I have main.cpp,
mimpl and a module.
So I just have a simple
class called MyInterface
and I implement it and I have the
interface to find in module.
The thing is though,
the command lines are not
necessarily stable yet
and there's a few
different ways of working
and so, I decided to implement them both.
Because I thought it
was quite interesting,
both of them really.
If you use these things called references,
you have to refer to individual files
and if you use search paths.
Then the name of your file
needs to be the same
as the content inside.
It'll be easier if I
just go along and think
so this is what a module looks like
and this module can have one of two names.
It can be mi.ixx or can
just be capital M.ixx
and you can write a bunch
of stuff for the top
and include things and this
is all private to the module
and won't be expanded to other things
and you can say you've got a module
and then you can export some stuff.
So I think these modules will be useful
because they can make
things private and public
and that's a good idea. We know this.
So this is what the command
line differences look like,
don't worry about all the stuff in gray.
That's just to make sure
you guys know how to get it
to actually work.
The main thing that I had to specify
that's different between
the two ways of working.
Between search paths and references
is just different output names.
So that's all I'm talking about here.
When you use one of these modules,
this is one of the
interesting stuff happens
on the command line.
Is you just import it
and then you have all of
the public interface there
and that's simple. It's just
like including a library
and then you can write your functions.
They've had to do with that name space
and then you're done.
That's pretty simple.
So here's how you use that
is on the command line you
will need to either specify
the reference to your ifc file.
This is the output that you built out.
You'll need a reference to that
and when the export happens.
It'll then find out the
inside this file there's an m.
So when you import an m it goes okay,
check all my references.
Ah there's my m, excellent and import
and if it doesn't find
one it gives you an error.
In the search path it's
kind of the same thing
except you just give a directory of stuff
and it goes and okays the file named m?
Okay yes there is,
now I look inside it and yes
there's a thing called m inside
excellent, then if that all works out
you don't get an error. Excellent.
So main.cpp, it's using it as well.
So the command line is the same,
there's no changes here.
Either have to reference your module
or you have to search for your module.
Excellent.
Linking, doesn't change at all.
You have object files, you
had object files before.
You have object files
now, nothing changed.
Okay, probably 45 minutes in
or something like that and
well finally talking about
packages and modules.
Excellent so what I would
actually think we would do here is
if we were using search paths
and we had our package technology.
We would probably just have
these search paths as being
just like our include paths.
So in our global scopes,
our public interfaces of our packages.
We would just list, okay these are the
new search paths for modules
that I would wanna use.
Well that would work out pretty well,
it's similar limitations
to header files today.
References are a bit more interesting
cause we don't really have
the equivalent of them.
So I could list out all of the,
I guess modules for this package.
Since there could be more than
one module per package but
I think this might lead
to an interesting problem.
So you see, modules can't
have circular references and
EA packages can. At least at link.
Headers themselves don't
have circular dependencies,
we already had to disconnect
all of these skins right?
But what I'm worried about if
one package equals one module
or basically the same meaning.
Would be one package
equals all of the modules
which is probably what
I would have to specify
in my build system.
This might be difficult
and would lead me to
an interesting problem.
This problem, just to
give you a feel of it.
Is the problem you run
into every once and awhile
when you're using a single
path linker like GCC.
And you're like oh yeah,
why do I need to specify
this library again
to fix this error?
You know it's just cause the
linker only links one direction
it's single paths and stuff like that.
Maybe you solved it like that
or maybe you did what we did which was.
We just put all of our
libraries in one big start group
end group and keep looping
around until you're dying.
Lovely.
Works great I guess,
it's the default in Visual Studio
how could it be wrong.
(laughing)
Yeah.
Anyway so the next one's if
you understand make files
it'll be clear and if not
you're gonna be a bit confused.
Basically in a make file you have output,
colon input and if you
change one of the input files
then you're supposed to build the output.
The line that you use is the line below,
so that's kind of what I'm
trying to explain here.
There's command line options
for the compiler to generate
dependencies and they
can generate little files
for you and stuff.
They don't have the
equivalence of these right now
for modules and therefore
I don't really know how
you're supposed to make this work.
The thing is here say it
had application.ixx here
and then it imports our m.
Now how do you know that
our m equals rendering.ixx.
Don't really understand how I'm
supposed to figure that out.
I guess I could make
some kind of global table
or something out of it.
I don't really quite understand,
I'm sure there's some good
thoughts involved in here
but I didn't know what they were.
I thought they were a bit odd
but if I use search
paths on the other hand.
I can tell, I would be importing in our m
and there would be an RM.ixx somewhere.
I could figure out what
directory it was from
and therefore I could figure out.
Okay probably the output would go here
and I could figure out how everything
was gonna work together
and that dependency.
Anyways so that's kind of interesting.
Modules at scale,
they have different rules
than .h files or object files
for naming. I mean object
files are kind of private
to a package anyway.
So they're kind of just a different beast
but .h files are a little similar.
.h files have paths
though and modules don't.
So that's a bit interesting,
I'm not sure why they made that choice.
EA has had trouble with
package naming in the past
and trying to get their
headers not to collide.
So if you have FIFA vector
and ESTL vector.
Then you depend on both of them,
you're in trouble if they.
Just to call vector right,
so probably if we were to use modules.
I would look at it and you would just go
package name. Module name
. ifc under dependencies.
You can kind of clear this out
as much as you can by
making sure that your packages.
Just don't have like one global list of
all of the headers that
they need for all packages.
All of your libraries right.
If this library only uses FIFA vector
then it only depends on
that, only gets those headers
and if this package only use ESTL vectors.
Well great it only uses that but
that only fix 80% of the problem.
You're still gonna have like
high level modules that try
to initialize everything
and they're gonna depend
on nearly everything.
So that's kind of a bit of a problem.
One other thing I noticed
in Visual Studio 2017 is
if I had like an in line
function in a module
and then I had,
another version of it in a CPP file.
Exactly the same content
or different content,
either way. Just the same function though,
same specification.
It wouldn't give me an error,
client did but.
Visual Studio would not
and so that was kind of interesting.
The in line one would
always have priorities
to the one in the module would always
win I don't know why.
So client has trouble with
legacy headers and back rows and things.
I don't know really too much about it,
I think there's other people
that have been talking about it
and I'm probably running
out of time anyways.
So let me just summarize
this whole talk up
one more last time.
So modules,
they're a better version
of a .h file right.
They're called .ixx or .cppm.
They could use someway of
creating dependencies.
So we could see maybe
how you're supposed to write
a build system for them.
That would be a bit interesting
just to prove that we can.
So an EA I think if we
started using this at scale,
we might run into a problem
around circular linkage.
I'm not really sure
but possibly. Name collision certainly
would be a small hindrance anyways.
But if we're careful,
it wouldn't be too bad.
Would I use modules in production?
Probably not, we have too many platforms
that just don't support this today.
It's kind a toy project
that we should be looking at
and just make sure that it's
going the right way for us.
So summarizing things up.
Build systems, are they useful?
Well if they get you
to hide the appropriate
things then yes.
If they standardize the way you work, yes.
Masterconfig? It was a really good idea.
Having one versioning files
across a bunch of executable.
One versioning for the whole team
was really valuable.
Package servers,
they are a really good idea but
they're really only
about low level packages.
You know if I was working
in a small company
or something like that.
I don't think I would,
think it's the first thing I would write.
But at a big company
like EA it's quite useful
because you can find
out the latest version
of each packages and those kind of things.
Finally,
you know mitigating
divergences between teams.
This is a very hard problem.
I guess you can just force it top down
and just say you're going
to use those version.
I don't know it's quite difficult.
And one last time for modules,
better version of headers.
There's two ways of using the
command lines at the moment
in Visual Studio 2017 by search path
and by reference.
I don't know which one's
going to be standard
so should both?
No way to do,
generate dependencies yet
and maybe we have some problems
with circular dependencies.
And that is everything thank you.
(clapping)
cool.
Guess there's a microphone over there
if anyone's got any question or anything.
Or try and repeat them or something.
Yeah probably better,
I mean I could try and
repeat them as well but.
I guess that's the professional
thing for me to be doing.
- So I have, one.
Well I have two part question I suppose.
Have you shared this with
the module's TS authors
and so forth? Like your
opinions on all this.
- Not really, this is kind of.
Hey surprise.
- So I've been somewhat
involved in this process and
the circular linkage one I think that one
is not super controversial
in the committee.
Like I think you're gonna lose that fight.
- Okay.
- But it's something we should bring up.
The other one that was more interesting,
it's actually a source of
some amount of controversy
at least historically.
Which is this whole idea of like,
how do I name these things.
How do I deal with name collisions,
what's the identifier.
So your like large scale
deployment experience
might actually be useful
in like helping settle some of this.
- Cool.
- So I'm not asking to write a paper,
that's a lot of work.
But at leas that would be good
if you could do that.
But at least sharing it with the people.
- Yeah I was hoping that this would cause
me some discussion here and I was hoping
to talk to some of the experts.
I don't know if any of them showed up.
- Do you know Gabby Dosreas at?
- I do know him a little bit,
I messaged him.
- You should corner him.
He's here somewhere.
- I will try to corner him again.
I courted him last time and so you know.
So it would be good to
try and talk to him again
so yeah thanks.
Anybody else?
- So I'm kind of coming from
other languages like Rust and ML and
I don't really get the
point of these modules.
Like they feel kind of
like pre-compiled headers.
- They are.
They're just about making
things build faster.
- I mean like why not just
use pre-compiled headers?
- Pre-compiled headers though at least
the relationship between
them is a bit different.
Right like there's a many
to one relationship there
and so you only have one
pre-compiled header per package
in our case right.
So this would be able to divide it up
in a slightly smaller unit.
I guess I mean whether that's
enough performance gain
to make it worthwhile I don't know.
- Like I look at languages like Rust
and Rust doesn't have the
best module system ever.
But it's just like,
it feels a lot more like
actually in the language
as opposed to this which
feels very much like...
Well a...
Pre-processor thing basically
and I just wanted to think
what's your opinion on that?
- I mean is modules just
a pre-processor thing.
Maybe it is,
I mean to me this is one reason
why I kind of did this talk
is because I felt like.
We weren't quite taking it far enough.
We want something more like
packages at EA I think.
- Yeah.
- Where I've had this for 15 years.
I don't see this as being
that technically difficult.
What's harder is to get everybody to agree
and if you look at Google
or something like that.
They have more than one build system
and they're really good engineers.
Like they're not, there's a reason
why they're probably doing that.
I don't know what it is
but I'm sure internally
they have this discussion all the time.
But yeah getting us all
to agree on one seems
to be quite difficult and I
don't know why. Not quite yet.
- Hi do you use these packer system
only for source based packages
or can you distribute
binary pre-built packages
across teams. For instance ESTL may be,
it would have already leap
the DLL files pre-built
and the user will have it linked
only without need to compile everything?
- So what you're asking for is do we just
use these for source level libraries
and the quick answer to that is.
Basically we only use them
for source level libraries.
We can sue them for binary information
but the complicated answer is.
We actually use them for
crazy amount of things.
We can use them for tools,
we use them for.
Data or something,
we can use them for all sorts of stuff.
So anything that you
could version in theory
you could put into one.
It's kind of like asking the question.
What could you use Git sub-modules for?
A lot of stuff actually.
Yeah.
- Thank you.
- So we use them for a lot. Okay.
- So my question actually
builds on top of that as well.
If you're using it for binary packages
then how do you get around the same.
The linkage and using the same flags
cause otherwise, it would not link.
- So how do we get around
the linkage problem.
- Right cause you kind
of glanced over that.
- Everybody has their own build options
and every team in EA has
their own build options
and this is one reason why
we're basically source level
compatible for the most part.
So there are some libraries that
we have to distribute like this.
Lets say the third party
libraries or something like that
and then we will actually
have libraries internally
and they're a pain in the ass.
They're hard and
somebody has to. Oh my gosh,
we've upgraded compilers.
Rebuild it and then distribute it
for everybody and it's kind of a pain.
Source level compatibility is
much easier for that sort of thing.
I guess that's where we are.
- And with the current
source level compatibility.
Do you specify the linkage flags
or the compiler flags on the very top.
Kind of like on the masterconfig
and then it kind of transcends into the
all of the packages?
- Yeah it's a little more
complicated than that,
we have a thing called
configuration package
and the configuration packages.
Specifies this
so each compiler kind of has their own one
and then on top of that.
We'll actually have some config
and then each team configures
their own on top of that.
But generally that's a
like it's one version of
all of that information.
So there's only one way to link usually
across a bunch of things.
We will specify options
on the command line to
change how we build.
- Okay thank you.
- Do you feel like,
that this is something
only valuable for like
the scale that EW works at or
like a smaller studio
that's only working on
one or two games?
Is there any value to doing
package management there?
- If I was in a smaller studio.
Would I do something like this?
I don't know if I would have
a package server right away.
I might avoid that but
having a standardized build system.
Certainly I would look at
if I could force the concept of
public and private headers.
I would try,
there is disadvantages
to doing that though
and I would like to see
if we could figure out
our number for them. Like think about it,
we have a 100 packages.
You have one dependency, 100 packages.
Now you have 100 search
paths for these things.
How expensive is that,
how good is the OS it's caching?
It's kind of an interesting question.
Seem like it's not so bad but,
I bet you there is some cost.
- Also you're masterconfig
once you start adding
parameters it sounds
a lot like a make file
at a certain point.
- No it just has packages and versions.
It doesn't end up like a make file at all,
it doesn't have any dependency information
or anything like that.
It's just,
if statements and packages and versions
and that's pretty much it.
- Two questions, so you
mentioned you have very large
number of packages. Right?
So just out of curiosity,
how many packages you
have which can parse XML.
That's the first question.
- Ah,
how many packages can parse XML?
Excellent question,
I have no idea. It's
gotta be at least seven.
Maybe ten?
Probably more, I don't know.
Like a lot.
And everybody uses their own in just FIFA
I'm sure I must have five.
- That's good yeah.
So second question to follow up on this.
EA is massive organization
so say I'm developing a package.
How do I broadcast new release.
I can't just email to everyone
that's too big.
- I could just put it on a server.
As I said, you could.
Any individual you see at EA
can just take a directory, zip it up
and post it to the server.
I don't think there's any
security on this thing at all.
Like you just post there,
that's how you post a package.
It's really trivial.
- It's not about security,
say I develop new package.
So again do is send an email
to everyone saying look
it's a great package.
This is what it does.
How do you promote it
and how EA deals with.
- How do you promote a new package?
I think it would depend on the package.
It's kind of like,
how do you market a new product.
If you have a good product and
you think it's worth selling
then you would have to find
the right channels to market it
depending on who you wanted to talk to.
You could send it to all
the programmers in EA,
there's kind of a mailing list for that.
But I think, if I could
target towards the people
that might actually care.
That would probably get you
more hits. That's
probably how I would do it
in an organization like FIFA.
We have lots of channels
for communication,
like Slack and channels.
Various mailing lists and things like that
and significant, like sig groups.
Significant interest groups.
Stuff like that so if you've
talked on those channels.
- So do you reward people internally
when they contribute to packages?
- Yes.
Do we reward people for
doing packages. Yes we do.
It's interesting problem
of whether you should.
It's like one of those questions.
How do you tell if a programmer
is a really good programmer?
How efficient are they?
If you tell every engineer
that they're supposed support
some major piece of technology
they will try very hard to support
a major piece of technology.
Whether they should or not,
it might be better that
there was only one of them.
But if you reward them
for it they will do it.
So it's,
mixed problem there.
- Thank you.
- High I have a question about
how you are going to support
multi platform because
like every game you have
different platform need to support
and each platform have
independent code for example.
Yes, so if you put it all
package it might be like.
Mess up but if you put in separate package
with like increase the
amount of package you have.
So how you treat this kind of situation?
- Generally speaking.
How do we treat
packages?
Are packages sort of
platform independent
or platform dependent?
It depends on the package.
Most packaged handle this themselves
and so you'll have a rendering engine
and it will hide the fact
that it's dealing with
multiple platforms with a code underneath.
So I showed an example
there that just had like,
source code start star CPP.
In there you can actually
put a bunch of if statements
and then you would go.
Ah if it is PS4, grab these files.
If it's Xbox, grab those
files kind of thing
and then the package itself
would change based on that
and they'll be. Like if you
need to do it within a file
they'll be if defs that
you could use across
things as well.
- Thank you.
- Okay alright.
That's it, thank you very much.
(clapping)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>