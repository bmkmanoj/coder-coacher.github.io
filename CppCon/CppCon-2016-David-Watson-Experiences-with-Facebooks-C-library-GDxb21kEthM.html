<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: David Watson “Experiences with Facebook's C++ library&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: David Watson “Experiences with Facebook's C++ library&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: David Watson “Experiences with Facebook's C++ library&quot;</b></h2><h5 class="post__date">2016-10-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GDxb21kEthM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is dave watson i am here to talk
to you about experiences with facebook
c++ library so this is not all of
Facebook C++ code base specifically
we're talking about something called
falling it's an acronym a very loose
acronym for Facebook's open source
library a very silly acronym so what is
falling I've been at Facebook for 6 7
years now
really long enough to see our transition
to C++ 11 long enough to see our
transition to C++ 14 and I hope a little
bit longer but really we started with a
whole bunch of code in a bunch of
different services well a lot of the
common code we tossed in a single
directory called common of course it was
a bunch of excellent code and a bunch of
very bad code all mixed together we
wanted to open-source some of our cool
projects and share them with you guys
you've probably heard of some of them
h/h p.m. hip hop Apache thrift proxy
Genma Crowder wangle there's a whole
bunch of open-source projects that you
wanted to share and to do that we had to
open-source all this common code so we
took a bunch of common code put it in
this open source library called poly
this code is trying to not reinvent the
wheel we try very hard only to fill in
the gaps between what is in the standard
what's in boost what's in various other
libraries that we use originally we had
a very extensive usage of boost it's
kind of waned over time as more and more
has gotten to the standard so in 2012 we
open-source this library called poly
we're gonna go over some of the pieces
in the library today there's like fifty
to a hundred different pieces in this
library so I kind of just have to pick
and choose it's a little bit of a
spastic talk jumping from one talk or
one topic to the next but it kind of has
to be to try and fit in all these
different pieces so a lot of things were
Boost moved into SCD as this has
happened we've tried to keep our library
up to date so moving to C++ 11 we of
course changed our entire code from
boost shared pointer to STD shared
planner we've done other things as well
similar going from 2 plus 11 to see plus
plus 14 in sequel plus 11 in lambda
captures you couldn't actually use
unique pointers so to work around this
we had lots of unique pointers lots of
lambda captures we made this thing
called move wrapper
which is really just a copying wrapper
around it that allows you to use the
unique pointer in the lamb to capture it
was terrible it's totally a hack we want
to get rid of it of course now we can
see foes +14 slowly updating all our
code to remove it so definitely a living
library as we move through the various
standards so let's talk about a bunch of
the different pieces you've probably
seen some of these already at other CPP
talks FB string there's an excellent
talk yesterday
really it is optimization on the
standard string that we'll talk a little
bit about FP vector small vector also
had another talk I believe that one was
yesterday conversion utility and some
synchronized locking FB string of small
value optimization if you have strings
under certain size it will keep them on
the stack small vector the exact same
thing small value optimisation if you
have under a certain number of things in
your vector it will keep them on the
stack FB vector is a little bit
different vectors when they resize they
have to go and allocate some new memory
they can do this in a bunch of different
ways for example FB vector actually
integrates with your allocator in our
case is je malloc it asks it hey do I
actually just have some extra space here
I can just resize into without actually
doing anything so it pulls that trick
quite often sometimes it's able to
coalesce some memory that's actually at
the end of the vector itself to just
grab some memory kind of like a realloc
and it's able to pull that trick as well
so these are just faster containers of
using various tricks we also have a
bunch of other little utility libraries
we have a conversion utility library
I've just called to as the name of the
function it takes a template parameter
of the type you want to convert from and
it uses the template grabber above the
type you want to convert to it's very
very similar to boost lexical cast if
you're familiar with that it's just a
standardized way to convert from one
type to another type so we could have
just used boost lexical casts and that
was actually what we started with we
were using that for a very long time
until someone noticed in perf like hey
this is actually taking quite a long
time especially with float types so we
went in up
dated a bunch of the different
conversion utility libraries wrote some
of them are self grab some other cool
you libraries from like v8 for fast
float conversion and we ended up with a
whole new conversion utility library
this turns out to be really really
useful if you're not doing this in your
own code if you're still using like a to
eye or something like that this is much
more uniform and much easier to read if
you end up changing your types you don't
actually have to go through and change
all your your conversion libraries as
well so I highly recommend using
something like this you guys have
probably heard of secret eyes too so I'm
just jumping through some of these
really fast that you've probably heard
of secret eyes is very similar to the
Java version we have lots of awesome
synchronization tools now in C++ 11 we
have STD mutex we have a condition
variables we have lock guards we have
unique locks we don't actually have
something like synchronized just yet
what is synchronized it actually ties
together your lock and the data you're
trying to protect so when you call push
back here in this case it's actually
grabbing the lock when you do reference
the synchronized object pushing back and
then releasing the lock so this right
away solves two of the main problems you
have with locking you can't forget to
lock the data you just you have to break
the abstraction to actually forget to
lock the data so that solved right away
the other whether to solved especially
with the are AAII resource acquisition
is initialization kind of lock guard and
unique lock is that you can hold it too
long and that happens quite often you'll
often see that show up in perf you
figure out hey what's going on I have a
bunch of lock contention here oh I need
to like add an extra little block here
to make it a shorter lock time and that
just happens as you go through and
modify your code you know someone's like
oh I need to do a little extra work here
in this function not realizing that you
actually have this Lockhart at the top
of the function so of course it
introduces another problem it can
actually be too short of a lock time now
so you actually have to open a new block
if you want to go and do multiple things
while holding the lock in practice that
turns out to not really be that big of
an issue why is that well in this
example we used a vector of a string but
in actuality most the use cases are some
sort of custom struct they would
probably just have a method on it so you
have a class
has another method that will actually
just do both things for you under the
lock so this is quite useful scope guard
a generalization of the ra íí-
interface I believe this is actually a
proposal for the standard we use this to
great effect in our code base
I believe the proposal is called scope
exit and boost I think also has some
macro versions of it essentially it's
the same thing but generalizing it to
anything you want when the scope exits
you can you can do something something
at all so this is almost an exact case
I've written before friends you want to
push something back in a vector and then
pop it after you do some other work if
you don't use this and you just put that
you know pop back at the very end of the
function it'll work fine most the time
the times it will really bite you is
when there's an exception and that's
actually I've written that exact bug
before like oops I forgot to take care
of this exceptional case and so you
don't want to wrap every single
exception with a try-catch so use a RAI
scope guard or scope exit and instead so
if you guys did watch the really awesome
string talk yesterday he talked about
you know we have these small value
optimizations we have copy-on-write
optimizations all sorts of things like
little tricks to try and make things go
faster it turns out you if you have an
ongoing string if you know the lifetime
of the string is going to exceed when
you want to use it using something like
a string piece just non owning string
type that's a standard proposal list
string view it's much faster it's faster
than all those you just have the pointer
to the start to the end and that's all
you really need it will still do the
range checking it could be a view into
some other data it can be whatever you
really want it to be we also have
generalizations of this based on any
container type and not just the string
generally people don't write this the
first time around in their code they go
through and say I want to just use STD
string or our version of FB string and
then write that throughout their code
and then they'll notice later like in a
file they'll go through and say hey this
string copying is actually hurting
performance a little bit let's go
through and write on these string pieces
in all the right places
exceptions Facebook is very pragmatic
about exceptions there's a lot of
different ways you can deal with
exceptions exceptions are very useful
you can throw and catch them without
having to muck up all your control flow
with a bunch of return values in various
places they have a couple different
problems though they're definitely
caused performance issues if you use
them in cases that are not exceptional
they're also sometimes harder to debug
than they should be you don't actually
know sometimes where the exception is
thrown from they could be redrawn and
caught and changed types so we have a
couple different tools in our library to
try and help us deal with these in our
code base so generally what we'll do is
we'll just go through and use exceptions
in the normal way that everyone has been
using them we have a bunch of different
exception types a hierarchy of types
they all inherit from STD exception and
let's say for example we have one that
is like a thrift exception for our PC
framework and then maybe you have one
that is like a timeout exception that
derives from that so we had several
different derivations of different
exception types then we're going along
and perform you notice hey this is
actually causing a performance issue in
our code we can see it actually the
throw and the catch taking up a lot of
CPU time what would you about that we
could of course just go through all of
our code and say we're gonna convert the
exception to a return type we're gonna
make an enum of all the return values
instead of you know timed out exception
and whatever else those will just be in
different numbers in the ITO that's not
a very pragmatic way to deal with it I
mean that's a lot of extra conversion
over that you don't actually have to do
I mean you can do that the first time
too but you have to handle all these
different return cases instead we have
one tool to deal with it one of many
exception wrapper essentially what this
does it's very similar to exception
pointer if you're familiar with that so
you have an exception pointer and you
can convert it to what you know and then
you get this pointer object you can
carry around with you that is kind of
like a return value and deal with it if
you had an exception pointer you would
then we throw the exception the catch
here is that the wreath Rowing of the
exception is really the expensive part
that's the part we want to try and avoid
instead we have this thing exception
wrapper that just takes whatever
exception type it is grabs it by the
dynamic type just like exception pointer
as opposed to trying to pass around like
STD exception and then instead of
restoring it we have a bunch of methods
on it that will be kind of like catch
statements based on the exception robber
so this is just a very pragmatic way of
dealing with these I'm hardly saying
this is the best way to do it but it
allows you to change the minimum amount
of code necessary to actually go through
and convert your exceptions into return
value types without actually having to
return value types so you don't have to
manage two different return types just
the small micro benchmark it turns out
throw and catch are extremely expensive
versus just passing around return values
something like 53 times more expensive
one of our other exception tools is
called exception tracer our platform is
Linux we run everything on Linux 64-bit
on our platform when you throw an
exception say you you don't actually
catch it anywhere in your program so you
finally get to main and exit main it'll
be like okay uncaught exception program
and maybe it'll print out the the what
of the exception I'll put on a little
description of the string of what the
exception did what it won't tell you is
where the exception was thrown from
which is very useful information to
debug your program
so what exception tracer does all it
does is whenever it throws it will grab
the stack trace attach it to the
exception it will also keep it in a
little stack of various exceptions that
you've thrown if you then catch that
exception it'll record the stack trace
if you throw out another exception it'll
record the stack trace so you have this
whole stack of stack traces that will
help you debug your trace of exceptions
as you go through your program so it's a
little bit of clever code it is makes
the exceptions more expensive versus
exception wrapper that is making them
cheaper and easier to use exception
tracer is actually making them more
expensive and just for debugging
purposes to help you debug
so we have some other utility libraries
you guys are probably familiar with bits
the the various bit of operations we
have in the standard library we have s T
V vector bool which sounds like might be
deprecated shortly we have bid sets I
think use a proposal for a various size
bit said that can be dynamically said
dynamic bit said I think it's called so
while a lot of the other talks that CVB
con have been about they have included
bid sets and like you want to modify
some bits they've mostly been about
trying to make your data smaller and
bits can definitely do that if you want
to store a bool it can only take up one
bit bits can also do a bunch of other
things our Intel processors have a bunch
of different hardware operations to deal
with this that's they can do fancy
things embed sets they can count how
many bits are set they can find the
first set bit if you have you know you
know a 64-bit word and a lot of other
processors have these instructions as
well it would be really awesome if
somehow in the standard we had access to
all these different instructions since
we don't we have a couple tools to use
these intrinsics for you GCC gives you
intrinsic s-- almost all the compilers
give you intrinsics to use these so
essentially we have a bit iterator that
will iterate over the bits and like all
the fine for set instruction so just as
an example of why this might be useful
like why do you want to find the first
set bit in a word like that doesn't make
any sense I just had to use it the last
month I was writing a timer wheel so
essentially you have a bunch of bits
that are set for when your timeout
should fire next you only want to set
the closest timeout through this system
timer you don't to have a whole bunch of
different timeouts you just want to call
the expensive system call once so to do
that you just find the closest timeout
and from there you can calculate what
the next timeout is so to find the next
timeout you have a bit set and you say
fine percent so it is essentially doing
some sort of search instead of a binary
search or min heap you're just doing
fine per set in the bit set if your bit
sets are fairly small if your sets are
small 64 bits it'll do it in one clock
cycle of course it will be linear on top
of that so if you have 256 bits it's
going to be four times that and
a little bit longer but if you haven't
used any sort of bit operations before
you should definitely look into them for
any sort of small set size they're gonna
be much faster than save trying to do a
binary search or some sort of heap
there's lots of other cool things you
can do with bits we don't have any sort
of atomic pizza in the standard there's
a Tomic operations but you'd have to do
by hand all the different bit operations
maybe you want to deal with endian and
flip the bits around the waffle so have
some operations for those highly used in
the networking code the primary place
that we're converting between the
different endian types so when quality
was released four years ago this is
probably the class they've made the most
waves we added some dynamic typing to
C++ people were really excited about
this what did we use it for things like
that since then we have a bunch of
different options that have come on the
scene there's various variant bearer
variant implementations so like boost
variant or I think there's an STD
variant there's optional types I think
there's an STD any type these are all
roughly trying to solve the same problem
dynamic goes a little bit further we
actually use it for a very specific use
case in our use case is parsing Jason
mainly Facebook is a web company we
interact with a lot of different dynamic
languages I mean it's PHP our primary
code and our website also Jason comes up
all the time so we're doing some
JavaScript do we want to pass the JSON
data back and forth dynamic is very
straightforward it's just a wide type in
terms of dynamic types it is larger than
one of the other ones and dynamic
languages so it's the union of all the
different types it could possibly be as
well as an enum of what the type
actually is and so you could think of
this in some way as just some sort of
variant with a predefined list of things
that it could actually be and what does
that predefined list it's exactly the
things that you can have in Jason so
just some examples you can convert you
can set it to any type you want you can
convert between various different types
you of course have the object type which
is really just a map of different things
you can put in it the same way you can
do in Jason you notice up here we have
the conversion you feel
these a string as int these actually
just used the the conversion library
that I was just explaining under the
covers so they're extremely fast and
they work extremely well that still do
range checking as well and then of
course the two most important methods
when you're going to use dynamic or
converting to and from jason so you're
gonna parse them jason give it a string
it'll convert it into these dynamic
types that you can deal with in the same
way you would in javascript and it'll
take anything that you've modified and
then it'll serialize it back out to a
string so in terms of our use case once
again only jason really we don't use
dynamic hardly anywhere else in our code
base we use variant quite a bit but only
for like several different types not
nearly as big as this dynamic typing
system all right we're gonna slow down a
little bit and dive into a little more
depth on some of these next ones i hope
you guys are all familiar with thread
locals thread local variables I love
thread-local variables they're one of my
favorites I probably use them too much
we're really spoiled for choice what are
you talking about thread locals we have
so many different choices for the ways
you can create them
pthread key create one of the most
portable ways it also has the most
flexibility with a bunch of different
ways you can create and use these P
threads
excuse me thread locals GCC has an
intrinsic underscore underscore thread
C++ 11 has some sort of a thread local
keyword boost has a thread specific
pointer we have a couple thread local
and thread local pointer it's a bunch of
different choices let's dive in and
figure out how they all work to make a
fast thread local it actually requires
dealing with compile time link time and
run time thread locals just like
everything is everywhere you kind of
have to dive in and work on them all the
same time to make them fast so if we're
going to make a very simple thread local
to try and explain how these work first
you want to know exactly where your
thread data is at so every thread has
some data associated with it for example
it has a name it has an ID it has a
bunch of thread locals it has other
things associated with it like probably
priorities
has a sleeping state door running state
so you grab your throat ID say we can
call the link system called get T ID
pthread has a similar call so we grab
our thread data somehow we index into
our thread local data I'm using a
pointer and then we're third local data
we have a bunch of different thread
locals how do we know which thread local
well really that's the the pthread key
create here what we're doing is you have
to call pthread key create before you
can actually go and use your thread
local so it will give you something
index essentially into your thread local
data it'll say hey pthread key create
you're the first one you're gonna get 0
so go use index 0 great so we can go
index into our probably a vector because
peterhead key create doesn't know how
many thread locals there's going to be
so it'll index into the first one so
there's a bunch of tricks we can play
here to make these faster if we're
compiling everything statically if we
know the whole world we know every
single thread local that we're ever
going to use in our program we can
actually lay out this array at compile
time we know exactly where all the
thread and locals are gonna be instead
of having to call pthread key create it
will just compile it in for you so
that's exactly what underscore
underscore thread is doing it's saying I
know the whole world I can actually just
at compile time figure out a number
that's going to go into your thread
local right away great so Facebook
actually does compile the world
statically makes a giant binary and
ships it out to all our machines this is
hardly the only reason we do that it
makes your thread locals fast it allows
some algorithms using thread locals that
you wouldn't have been able to use
before it's not the only reason though
also carries all your dependencies with
it so which is kind of nice but what
happens if you're going to dynamically
link things what happens if you're going
to be even more of a jerk to the system
and deal open things really you have no
idea how many thread locals are gonna be
so you're kind of back to square one
what actually happens is a function gets
called every time you access that thread
local call GUID pls data I think so if
you see something underscore underscore
TLS in your profiles or your profiles at
all that is what's happening is that you
have dynamically opened some library and
it doesn't know how many thread local
there are so it has to ask at runtime
where the thread-local data is so that
kind of sucks we could go even farther
if we actually want to get rid of this
trying to figure out what our thread
idea is if you can arrange for the
operating system to give you a little
support and say hey I know exactly what
your thread ID is every time we're gonna
preempt you we're gonna stick your
thread ID in a specific place so you can
figure it out that would be a little bit
more helpful we wouldnt have to make a
system call on x86 we actually can go
one step farther than that we actually
have hardware support for thread locals
way back before we had virtual memory we
had something called segmentation
registers we used to use segmentation
registers to index into specific
locations in memory and instead of
having a virtual memory system we don't
actually use these anymore they're
laying around empty and free so we
repurpose them for using thread locals
so what Linux actually does is it will
stick your pointer to your thread local
data in the segmentation register and
then when the assembly code is generated
it will just prefix the segmentation
register FX FX : FS :
and then whatever variable you're trying
to access in your thread local array at
the end of the day what this means is
you get extremely fast access to your
thread locals if you are statically
linking the world and you're on x86
probably other operating systems but I
don't use them so I don't know so it is
actually the same speed to access a
thread local using this scheme as it is
to access a regular variable exactly the
same speed cool let's go back for a
little chart here what does that
actually mean for our chart
pthread key create is the first one we
investigated if if you're doing
everything at runtime pthread key create
is the way to go it actually has some
extra hooks in there to help you out for
example when the thread is destructed
and you actually get a notification and
you could actually do some extra work if
you wanted to destroy your your pthread
key underscore underscore thread works
great for plain old data types anything
that doesn't have constructors and
destructors it will work excellent
thread a local thread underscore local
from C++ 11 if you have constructors and
destructors
that's the way to go that's probably
just the general recommendation and use
this standard why not it works great it
actually has a little bit tighter
constraints in terms of when you can
access it so if you will read to access
your throat local data before main or
after main you may actually end up
having to fall back to some of the
intrinsic sand the compilers so we have
these last three threads specific
pointer and thread local pointer what do
those do well so far we haven't talked
about member variables classes C++ has
classes surprise we have member
variables so we're gonna make a member
variable that is a thread local pointer
current currently you cannot do that in
the standard there is no way so that's
why boost has added thread specific
pointer folly has thread local pointer
for a motivating example for why you
would want to do this because I get that
a lot like this doesn't make any sense
why would you want to add one of those
we're just gonna invent some class here
that is a counter essentially we're
gonna keep statistics or something let's
say we want every time I function is
called to keep a counter of how many
times this function was called and then
we'll save it later well print it out to
a log message every minute or something
so if we just made this a global
variable with a mutex or even an atomic
that we're gonna atomically increment
that's gonna be a lot of contention on a
single place if it's called often we ran
into this in this case in our stats code
all the time it comes up very frequently
even on other cases they're less well
contended so what do we do we make it a
thread local pointer instead run local
instead so with a thread local int we
then just increment a thread local so
accessing a local variable a threat
local even is the same speed as the
local variable in commit at once great
then we want to get the whole value of
the whole thread local statistics
counter we use this access all threads
method and that's really the trick
that's pretty much the whole point of
why we have a version of thread local in
our library that no one else has really
implemented so you walk around all the
different thread local threads add up
the value and then you have gotten the
global value for whatever it would have
been if it was actually a global
so boost thread specific pointer you can
be used as a member variable it does not
have the access all threads method folly
thread local into our local planner do
have the access all thread method so is
the bunch of other use cases for this
and we're gonna jump into some of them
shortly Singleton's I hope you guys are
familiar with Singleton's they're pretty
easy right you have something you just
like to stick it on the stack like okay
here's the static we're gonna make it a
global it works pretty well it has a
couple issues if the singleton
references other Singleton's we could
run into problems there's a thing called
the static initialization order fiasco
this is gonna blow up all over if you
have to through excuse me to Singleton's
that are referencing each other from
different files you don't actually know
which ones constructor is going to run
first the linker is it gets to choose
which one is going to run so we can fix
that we can make a new version of
singleton we'll just make a method it
will go and make it a new thing and
return a new thing so if you want to use
a single thin great we don't have any
constructor order issues now if new
thing calls some other singleton in a
different file it'll work just fine
because they're all run at runtime and
it will be well-defined there's also no
destructors on this one we're just
leaking the memory everywhere which
which can be an issue that's great we
could solve that there's this thing
called the meyer singleton hopefully you
guys have all heard of the Myra
singleton you just have a static item in
your method and you return a reference
to it and this works great it works
great in the context of the C++ standard
if all you're going to do are things in
the standard you could just do this all
day and it will work just well for you
my only gripe with this it's just the
tiny little gripe is that it seems a
little bit magic you don't actually know
what's going to happen here this is what
actually happens there several pages of
assembly code taking locks that you
probably didn't expect it calls and
registers and add exit handler so it can
run the destructor but it all works just
fine
when it doesn't work and the case then
we have run into and actually using this
in our very large production codebase is
that it doesn't work in the case of
using some of the POSIX interfaces like
Fork so what does fork to you well it
goes and duplicates your program and
starts a new copy of it and continues
running court doesn't play very well
with multi threads it doesn't play very
well with open file descriptors you have
to worry about all these little subtle
things that can happen when your program
works if we were just talking about our
card base in our CTL + world we could
probably get around this and work around
fork will only call fork and like very
specific small instances unfortunately
we don't control where all our code runs
let's say we write ourselves a little
library we want to go and write some
singleton like a logger we'll make it a
thank you a synchronous logger so it can
like open up a thread in the background
it'll open up a file go and start
writing to the file let's say now that
we want to import our library that
happens to use a singleton that is a
logger and put it and use it from say a
different context like Python we want to
go and make this a Python extension the
problem with that is python calls work
all the time
python has a global interpreter lock and
of course then you want to go and use
multi-threading instead of threads so
you want to fork and make multiple
copies of your Python process you run
into all sorts of trouble this also
comes up in other context in other
languages but the primary motivation is
using your code as a library when you
don't really control how it's going to
be used so instead of the Myra singleton
we actually have something is just a
class it's called a singleton it takes
the template parameter of what the
actual singleton object is going to be
you could then call get on it just
giving it the type because of course
there's is a single tenth so there can
only be one of the type you could also
just grab it from the object itself if
you wanted and then the little bit of
like awesome automatic stuff is then you
can say I want to shut down all my
Singleton's and then I want to restart
them later so you shut down all your
Singleton's before you call for it and
you can start them back up
Fork isn't the only use case you can say
in my unit tests I want a clean world I
can just go and shed on all my
Singleton's where you start them
different things like that this is
really getting quite close to something
like dependency injection we're managing
all our dependencies in the world in
some way so you might have noticed here
that when you call singleton get it
actually is returning a shared pointer
so the reasoning for that is that you
actually want to make sure all your
Singleton's are in a quiescent state
when you go to shut them down before you
call fork or whatever you're going to do
so in the case of the logger you want to
make sure no one is actually using the
logger when you try and go and quiesce
the logger so way we do that is just use
a shared pointer when the shared pointer
count goes down to zero we know that no
one is actually using it anymore the
problem with that is that this is a
singleton having a bunch of people grab
a shared pointer increment the ref count
use it decrement the ref count the ref
count is all on a single cache line if
this is happening from a bunch of
different threads at the same time as a
might with like a logger singleton is
gonna cause quite a few issues so we
want to make this shared pointer access
a little bit faster so we resorted to
using our Cu and minding writing a
faster shared pointer so our version is
has roughly the same interface as atomic
shared pointer I believe we called it
read mostly shared pointer it what it
does is essentially shards the reference
count for the shared pointer into a
bunch of different cache lines it of
course can do this in many different
ways one way you could do this is same
using thread local now I just have a
thread local reference count to whatever
my singleton is so I'm using this the
thread local structure we saw before
then we want to go and shut down our
singleton at that point we could say hey
I'm gonna go and round up all this room
counts from all the different threads
using access all threads and then when
they go to zero then I can actually go
and quiesce my signal toons we're gonna
talk about some of the synchronization
primitives we have the standard library
gives us quite
you we have a city mutex st condition
variable quite a few others we have some
high-level tools built on top of that
these are really the ones that we've
found most useful for our large code
base so we have a multi producer multi
consumer queue these have definitely
been talked about elsewhere at CTP comm
they're extremely useful ours is a block
free version and a lock free ring buffer
so we know our hardware
we know we have Linux we use x86
something you may not know for x86 is
that there's actually a different
strength of atomic operations on x86
specifically but not most other
architectures atomic increment is
actually faster than the various caste
operations so the MP mcq takes advantage
of this fact by using it oh right and
you just basically grab a ticket and run
around the array for each different
insert or delete so you could just go on
the internet and say hey MP mcq you
don't have to use ours ours isn't like
significantly different than most of the
others the one thing that we actually
did find in production that was was very
useful is the ability to sleep so you
can either grab asynchronously the next
object say that queue is empty and you
want to go and say I want to I'm gonna
remove something from the queue you
could say that if it's empty I'm just
going to return right away or you might
want to sleep and wait until something
actually is inserted in the queue this
actually comes about because we actually
have the way our programs are set up so
our hardware is fairly small Intel
hardware we have 32 cores 64 cores but
our number of threads is much much
larger than that most of our programs
run with somewhere between 500 and a
thousand threads it seems like a
phenomenal amount of threads most of
them are sleeping demon' doing all or is
nothing the problem comes is sometimes
you run into these situations where
there are more threads and cores trying
to use this MP mtq or any queue or
grabbing any lock early and if you're
using spin locks if you're waiting in
cast loops you're going to be taking
much longer and run into very bad
situations because these cores are
spinning doing basically no work while
they could be sleeping instead so it's
one of the very few changes we've added
to MPN CQ that is different from the
average one you see the standard is also
a proposal for a shared mutex this is
basically just a reader/writer lock so
sdd mutex is a exclusive lock anyone
that grabs the lock has exclusive access
to the data shared mutex is a
reader/writer lock there's a reader
count in a writer account there can be
many different readers they can all be
accessed in the data simultaneously
there can be one writer so you can have
writer priority locks you'd have reader
priority locks that's that's really
beside the point what is necessary to
know is that all these readers at least
in the current implementations that I've
looked at are all contending on a single
cache line there is the reader count and
they're all just incrementing and
decrementing it is one location so
there's a few of scaling libraries
that's called a big reader lock
concurrency kit has one there's a bunch
of different papers on it I think
there's a cool talk on last year's CVP
con about the big reader lock
essentially what you're doing is you're
taking this single cache line here that
is happening on a single core and now
splitting it across multiple cores so
again we're sharding the cache line
across the charting the reader comm
across multiple cores you could of
course do this with drive vocals as well
the current implementation just sets it
up so there's one per core but it
doesn't make a huge difference either
way so one of the problems with the big
green lock is if we're taking this
reader count which you know is probably
a very small size we probably don't have
that many readers I mean it probably
fits into bytes as plenty even one might
be enough the problem is we now need a
whole cache line per core to store this
so we have an our Hardware 64 bytes
times 32 64 cores times the number of
shared mutexes we have so it really
becomes an unmanageably large lock and
it was completely impractical to do it
in that matter and we definitely don't
we actually this meter count here is not
actually a reader
it's actually just a linked list of the
readers that are there so this reader
slot can be stored it can be shared
among many different shared mutexes so
if you find someone who's already in
your reader slot you're just gonna chain
off him so what this does is it means
all those shared cache lines are actual
global data they're shared among all the
different shared mutexes that means
they're not actually part of the shared
mutex structure
it means our shared mutex implementation
is about four bytes which is very
reasonable for a mutex
by comparison a pthread mutex on linux
STD mutex is actually using the pthread
mutex under the covers that's about 40
bytes so a significantly bigger and it's
actually really surprising because you
could go and write just an exclusive
lock in Linux using Pugh tax system
calls using about two bits so using a
whole series of bytes is extreme
overkill one other optimization you're
stuck in here because of the way we have
actually sharded this because these
readers here are not actually a reader
count it's actually like a structure
like a linked list node that we're
sticking in here if your thread migrates
from one quarter to another
it can't go and remove your little
linked list known from the CPU it's on
it actually has to know which note it
had locked on so if between lock and
unlock your migrating CPUs we have to go
and you actually have to go and slowly
round up all of them instead of trying
to like search for your actual node you
could return a token from the lock
method so the lock method could say hey
you were actually locked on TPU one if
you're migrated somewhere else you can
just go and unhook from CPU one so
there's another little optimization
we've put in there so another secret is
the synchronization primitives we built
on top of the standard library is called
leaf OSM you guys are probably familiar
with condition variables so if you have
several different threads that are
waiting on a condition variable if you
have another thread that says wake one
wakes up one of those threads currently
it is undefined which of those threads
are going to wake up
it's probably gonna wake up to one that
is misleading the longest this is often
not the ideal thing to do you often want
to wake them up and the last in first
out order and let me give an example of
what that means so let's say we want to
go and write an executor this is going
to be a very simple thread pool we're
gonna spin up a bunch of different
threads that are going to do work so we
have two threads threads a and beep
sitting here waiting to do some work
thread C comes along it has finished its
work is now waiting the queue for
something to do someone says go do work
W go figure out something to do the last
thread that is has entered the queue in
this example thread C has finished its
work and it was the first one or it's
the last one into the queue it's going
to grab work W and it's going to be the
first one out of the queue so what this
really does is it means if you write
your implementation correctly that C
doesn't have to go through the scheduler
it may just be able to grab the work
right away and go on and do some work
it's cache is still gonna be hot it was
just doing work whatever it was doing is
still in cache maybe it had some thread
locals in cache if you were to wake up a
or B instead it would have to grab
different thread vocals from the cache
so that the cache is gonna be extremely
hot if you're using a last in first out
order just dropping this into our our
base thread pool implementation in a
couple other places in our code it ended
up with about like a 1% latency
reduction and a bunch of our different
services so I definitely recommend you
try out something like this in your code
it was actually a bit tricky to
implement to implement it you actually
need to choose somehow which thread
you're going to wake up route X on Linux
fast user space bu Texas has a very
large API one of which is that you can
choose using a bit sub which thread you
want to go and wake up so that's the way
ours is implemented you can of course
implement many other different ways you
could have each thread waiting on
specific different condition variables
and right at all and very portable bells
+11 code if you wanted it an end up
being much larger and using more memory
though so I'm a little bit of time we'll
talk about a couple more things futures
and executors this has been covered
pretty extend
that's EVP calm so I'm really just gonna
share our experiences using executors
and futures we've written our own future
we had started when we were still in C++
11 the standard future and C++ 11 is not
very full-featured it of course doesn't
have the then method which is the most
important method for all the futures
then is really just like asynchronously
chaining some work together so your
future instead of waiting for word to
finish for something to happen you're
just going to say after you've done that
work go do some other work and that
works great that's actually coming in 17
I believe but there's a bunch of other
very useful methods within when all
window reduce so if you want like a
sliding window of different chaining
items on your future when all these
different features are done then you can
go and do something else but really the
one that I haven't seen too much talk
about is the via implementation and what
this means is that we have these
executors there's been a lot of talk
about them too they do some work you
know in a thread pool or something else
like how do you use them together with
features it really this is the entire
power of features in my eyes like if you
just have futures and you want to chain
things together like you could use
callbacks and it wouldn't be that much
different but executors really take it
to the next level so a big example of
this going back to our asynchronous
logger example let's say that we want to
make a futures interface to our
asynchronous logger you say okay go log
something and let me know when you
finish logging it well the logger may
happen in a different thread so you say
ok future now go via over to the logger
thread try and log maybe this logger
could fail for some reason so we have
like a retrying method that will keep
retrying this future kept some errors
maybe with on error and then finally
we're gonna be a back to our current
thread and keep running so via is
extremely powerful tool it's it's just a
way to interact features and executors
together and make them jump between
different threads
all right so this is the last one I'm
going to talk about we have something
called indexed mempool this is a tool
for writing lock free data structures it
is a memory allocation thing so there's
a cool talk on Friday you guys should
all go watch talking about a safe memory
reclamation they're going to talk about
hazard pointers they're gonna talk about
RCU there's all sorts of different ways
to reclaim memory our choice has really
been to do none of them we don't reclaim
memory at all we just pre-allocated all
the beginning of the program and so the
only other thing you have to worry about
instead of safe memory reclamation is
you have to worry about ABA I probably
don't have time in this talk to discuss
ABA but I hope if you're interested in
lock free programming and watch one of
the others Hawks this week that you've
heard someone about the ABA problem one
of the ways that you could solve it is
just to tag your pointers so ABA means
that you have a pointer it changes
somewhere and then it changes back to
the original pointer and using a compare
and swap atomic instruction you don't
actually know that has changed in
between that there may be some things in
like the linked list that have changed
otherwise so to work around this problem
you tagged your pointers every time you
access this pointer with a cast you
increment the tag so now you have a 1 B
2 a 3 and the A's are no longer the same
anymore so the question really becomes
how BIG's of this tag have to be on x86
we current Hardware only has 40 bits of
virtuality
and virtual address space this might not
always be the case but it is today so we
have saved 16 extra bits for either we
can go and play with if your data is
aligned maybe you have some extra bits
at the bottom 8 by the line maybe you
have three extra bits maybe it's 16 by
the line you have 4 it turns out that is
not enough and not enough bits to solve
the ABA problem in fact I went and gave
it a try I took one of our lock free
data structures I solved the a be a
problem just using a tag I gave it 16
bits to play with it went in crashed
every six hours or so so instead what
you can do is just write a very simple
memory allocator
that manages your memory it doesn't even
have to be a real memory allocator it's
just giving you out fixed note objects
and all it's going to do is ensure that
all these things line up in such a way
that they only use 32 bits of address
space so you could just go and map a
large section of memory and hand out
different pieces of this memory as long
as the memory is is less than you know 4
gigabytes of space or so with the other
32 bits you can go and use them to add
your tag so this code is actually
extremely short extremely easy way to
solve any sort of a bas you're having in
log free programming it's all standards
compliant there's hardly anything that
you have to worry about it does use a
map but you could also use use malloc as
well and grab a piece from whatever
allocator you're using so go and compare
and contrast this code to what you might
learn on friday's talk about safe memory
reclamation so that was my short
overview of folly there's probably about
50 other utilities in there of varying
levels of usefulness
I'd suggest you guys all go check it out
it's on github</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>