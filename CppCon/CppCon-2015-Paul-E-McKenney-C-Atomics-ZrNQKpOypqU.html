<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: Paul E. McKenney “C++ Atomics...&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: Paul E. McKenney “C++ Atomics...&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: Paul E. McKenney “C++ Atomics...&quot;</b></h2><h5 class="post__date">2015-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZrNQKpOypqU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is kind of the third of a set we
had fader talking yesterday about how to
do atomic especially one on older
compilers which is an impressive feat
these days we had Michael talking about
the atomic in general the various memory
orders and so I'm coming drilling down
into the bottom of the depths with
memorator consume which has been a sort
of a sad story there's a few people
they've been involved with it through
its whole life but maybe we're coming
inside of a happy ending but let's see
we got here so I'm gonna talk a little
bit about why you would want to use it
the target workloads and why we'd have
it what the problems are with the
current definition of the standard and
some of you who are compiler writers may
know understand this better than I do or
feel it more sharply anyway we'll talk
about some before for prose resolutions
over the past year and a half there's
been a number of them put forward and it
looks like we've got something that
might be reasonable for two different
use cases and if we have time I mean
we've done double checked locking the
other two so I guess I got a double
check locking as well and show how you
do with member consume why not if we
have time we'll get their target
workloads so you'd use memory or to
consume four linked data structures it's
used basically what it's doing is
imposing order when you're following
pointers relying on the fact the
hardware does that for you in almost all
cases dec alpha being the exception that
proves the rule and if you're using this
there's other things you could do let's
face it if you don't care about
performance why don't you just write a
single-threaded program get on with your
life in fact why not use a scripting
language and make it so you don't have
to mess with compilers and all this
stuff right so if you're doing this
performance has to be something you care
deeply about and it has to be a
performance level you can't get with a
sequential process such as a single CPU
and this is not tongue-in-cheek
there's a number of cases where you do
optimization on special code it'll get
you orders of magnitude in contrast if
you're adding parallel processors it's
possible but hard to get a greater speed
up
than the number of processors you add
okay so again if you're boring handle
single-threaded do it single threaded
ignore this at the same time before
what's can't be the only concern if all
you care about is performance if you
want performance at any cost
no matter what my friend you're writing
hand-coded assembly you're not doing
this you know I used to see you're not
use C++ okay
so we're kind of in the sweet spot where
we care about performance we need
parallelism we need the scalability but
we're willing to give up a little bit so
let the compiler help us out and to deal
with that sort of things although quite
frankly it's quite possible the
compilers are better at generating code
than people are today but still if
you're drawing for that less little bit
you're down on the metal so in short we
want to maximize performance but we want
to maintain all the other nice things
you know portability maintainability
reasonable levels of productivity and
what we do here to get this to work you
know there's an old saying that I
disagree with actually this says that
weak memory requires strong lines and
well not really what it requires as good
api's if you have good api's you can get
the benefits the performance benefits of
the weak memory and member weak memory
isn't just the CPU it's also the
compiler doing things to you for you
against you something
and those api's allow that to happen
allow you to gain that performance
without having to twist your mind around
exactly older orderings that are going
to get and that's that's the goal here
so why never order consume specifically
I mean parallelism there's a lot of ways
to achieve it this is just one first was
a bit case so let's look at what we're
doing here so we got to have a diagram
with arrows and order is in dependencies
and I've left out the sequence before so
that Michael had earlier and so we've
got two threads the top three statements
are the first thread and running in
parallel is this other thread down here
with the bottom three statements and of
course there's sequence befores between
each pair of statements there now what
we're doing is we do a relaxed or 2x sum
related variable we then do a relaxed or
to an element within a structure pointed
to by P the pointer P and after that we
do a memory order release store of P a
local variable into some global okay so
we've taken and we've initialized some
unrelated variable we filled out a
element and a structure we're playing
with and we're taking a point of that
structure and publishing it in to this
variable GP that's when one thread is
doing because we do release there we
have some notion of ordering between
that last store and the previous two
stores all right
however as Fader pointed out in the last
turtle we call it memory barrier pairing
but however you call it you don't get
ordering just by ordering one thread you
have to have ordering in both threads if
you let either thread do things out of
order
you haven't got ordering they have to
both work together and so we have to
look at what's happening in the reading
thread down here on the last three lines
now what it does is it loads that GP
thing that global pointer that points to
that structure we filled out with its
single field a there and if we happen to
get that pointer we have a dependency
ordered before relationship between the
last statement in the first thread and
the first statement in the second thread
so there's a diagonal green arrow
represents that alright and the nice
thing about this is member to consume if
we had an efficient memory rotor consume
which we do not yet by the way that's
one of the things we're looking at here
if you have an inefficient one that
promotes ADD acquire and well life's
hard sometimes but and then so in Linux
world we still use balls with cast and
various other strange and odd horrible
tricks to get the effect but what
happens is that the second load is
loading a field pointed to by cue use
the local variable we load it into and
because those are dependent we had to
use the loaded value into queue to find
the address for the field cue arrow a
therefore there's a dependency between
these two statements
and that means that they have a
dependency carry between those two
statements as a result because we have
her sing closely for up there between
the store to field a up there this
middle statement in the first thread and
the store into the GP pointer we have a
confuse to ordering and we have a
carrying dependency from that
initialization of field a all the way
down to the use so we're guaranteed we
pick this up where you get the value we
store it up there the one we're not
going to get pre initialized garbage if
all we did was relax loads here we could
get pre initialized garbage because
again we have to have ordering in both
threads in order to see the ordering
over all this we load from GP into queue
and then we have a queue again here or
if I'm missing the question yeah yeah I
don't know I was trying to be more clear
perhaps as more confusing but life's
like that what's that oh okay you're
right that's that is wrong that should
be going away thank you okay however for
Rex
there's no dependency we load Q it has
nothing whatsoever to do with X there is
no dependence between those two
statements there is no ordering between
these two statements we have ordering
the first thread not in the second
thread and therefore this can pick up
whatever from X it can pick up something
before that initialization at the very
top of the slide so memory or a consume
is nice in that this can just be a
normal load on almost all architectures
so very simple instruction very fast nor
the fish will slow down but there's a
penalty there's no free lunch if you're
going to get the ordering you have to
have dependencies between that initial
load and the loads or stores that follow
it now it turns out that if you have a
linked data structure that happens very
naturally you go down at some pointer
and then you do dependent loads off of
that pointer so those dependency
happened very naturally
in linked data structures and so in
linked data structures this technique
works very well so we have to pay a
little bit of penalty for updating we
have to have that memory barrier or
something to make that release work on
the reader side we just issue normal
instructions and things work nicely for
us with the exception again of dekha
alpha but that's discontinued for a
while and we take care of that otherwise
so this is kind of the use case is to be
able to do blinked operation of linked
data structures that should read at
ensive ones using very simple very keep
instructions and getting very high
performance as a result well okay that's
great the reader went really fast but so
what there's got updaters to write
that's true you do the reason this would
become important compared to say 25
years ago when I was first starting to
do parallel programming is that the
machines have gotten a lot bigger and
more complex and they adapt themselves
to their environment if I plug a memory
stick what you saw I plugged the video
in this machine and the slight gesture
came on if I plug a memory stick into it
it would pop up and say oh you got a
memory stick if I were to try do that
say in the early 80s
I'd have to recompile the kernel to tell
about the new device because there
wasn't enough freaking memory to
represent the possibility there might be
a device or might not you had to
actually tell it I've got these devices
right now and if you're booted on a
system that didn't have those devices it
would probably panic at boot what that
means is inside this laptop there are
data structures they represent the
hardware connected to it the fact that
it Scott
doesn't have right now any memory fees
but it did a while ago and I saved my
presentation just in case and so those
structures almost never change I plugged
hardware into it occasionally but very
occasionally compared to how much I'm
using it but I could plug hardware in at
any time and that means that I have
things where I'm reading almost all the
time they always never change but they
might
and so this read mostly approach this
focus on reading
it's become increasingly important as
our hardware and software and operating
systems and applications have become
more able to adapt to a changing
environment so that's why it's getting
more important now that would have been
say in the 1980s of course you still
need updates and oddly enough you can
use memory order consume for updates and
some people do you can think of it as a
poor-man's garbage collector so the
tricks that theta are pointed out could
work with a garbage collector you can
use rc4 those as well and that does
happen people do use those in the linux
kernel for that reason okay but you know
we're talking about really small changes
here right we're talking on PowerPC you
get rid of an l a-- BSA or maybe and i
think these are just in the little
instructions on RN and i get rid of a
dsb by doing doing this or a DMD excuse
me on x86 your constraining the compiler
not allowing to do certain optimizations
so who cares why would any bake care
well a few months ago I received this
patch or a predecessor of it the initial
patch I received was technically correct
but gave false positives when various
tools ran on it okay so this is the
modified version and don't worry about
exactly what it is as much a weird
kernel code but the key point is that it
saves one store in one load both the
store in the loader non-atomic how they
have to be because this is pre c11 and
we don't have Atomics okay to the stack
not to a shared variable I initially
suggested to the submitter that there
wasn't a pointless patch and actually he
it was he surprisingly enough this is
visible at user mode this is a patch
deep in the kernel and the performance
of difference is visible at user mode
with certain benchmarks
it's not isn't anything and this is
these are two normal instructions these
not memory barriers not lock
constructions not Atomics not nothing
alright and the thing is is that some
people do care about performance and the
Linux kernel is one of many projects the
needs to accommodate their needs all
right
so developers who faced severe
performance requirements their echo
thank you if you go throwing extra
number offense instructions in their
code they won't appreciate that much and
they aren't going to thank you about
extra cache misses either or extra
atomic instructions and they're also not
going to thank you for unnecessarily
suppressing pilot optimizations they've
told me that rather directly several
times over the past decade or so it
don't take my word for it if you don't
want to but you know look at lkl if you
want in some cases lk mel is
insufficient to vent their ire so they
call me on the telephone and scream at
me so anyway what if C and C++ are going
to use supporting low-low development
and I believe they need to i mean c plus
c and c++ must serve a lot of different
needs still get me wrong it's not only
about low low element but the low level
development of synchronization
primitives one of the things we need to
be able to do in these languages we need
to support this kind of extreme
performance and scalability and that's
why we have a member order consume or
one reason it's intended to compile as I
said on a single load most CPUs with no
added croft and as a result it is quite
fast of course
one question is how the heck can use
this thing and as a fader pointed out
it's one thing to write concurrent code
there's another thing to write correct
concurrent code and we'll take a quick
look at that there's a guy used to work
with back in the early 90s
older guy at the time big bushy beard
you know long white hair didn't have a
wand sorry he had a he had a sign on his
cube though and that sign said only
those who have gone too far can possibly
tell you how far you can go so let's go
all the way here all right I mean let's
see what happens so we're gonna define
some primitives here or she read Locke
think of that kind of like a
reader/writer lock acquire acquisition
sort of kind of but not quite and we are
she read unlock and I do mean that
counseling to find are so read lock
Karen Karen new line that's the
primitive and I really do mean council I
define our she read lock parent parent
new line and there's this are so you do
reference things just remember order
consume if you look in the length
Colonel you see some fallible casts and
stuff like that because that's what we
have right now to use that with we're
kind of the same position cater is its
we have to support compilers that are
pre c11 I'm going to do a sign choir
which is a really storage you and you
might guess those two from that diagram
with the two threads I showed a few
slides back now until somebody tells me
otherwise I'm going to assert that if
you can use those four primitives there
can give you the best possible read side
performance scalability real-time
response weight freedom and energy
efficiency at least if you had a good
consume which we don't get now yeah I'm
an old guy you guys are young so you
guys might get to a challenge to beat
that for the first three of them you're
going to need especially the first two
you're gonna need some negative overhead
which could be a challenge but if you do
that it's really cool and like you tell
me how you do it all right of course
somebody may be objecting at this point
wait a minute these things aren't
affecting machine State how are you
synchronizing anything with them that's
a good question it's a reasonable
question and by the way I'm gonna go
through this really quickly there'll be
some reference to the slide set I'm
happy to talk to people later I'll be
here through mid Friday but
this is just to give you a flavor of it
you know and those of you have some
familiarity with it may gain some more
as well all right so we're talking about
linked data structures one of the things
we do with linked data structures in
insert new elements into the linked data
structure so let's take a really really
simple lake tenaya structure that has
just a pointer all right
so either has a point of this no lure
has a pointer of one thing hanging off
of it let's start simple we go from
there right so we have four different
states here as we're inserting this
element so we start off with a null
pointer and we have colors so red means
that a reader can look at any time now
remember our read lock doesn't do
anything I mean not only is it not
generate instructions the compiler
back-end doesn't even see it right the C
processor gets rid of it so we can't do
anything to stop readers there come
blasting through and check that pointer
no matter what we can't even tell about
it there so we color that red because
it's kind of dangerous if we do updates
we got to be careful of that variable
because readers can come in at any old
time they want to and access it green
stuff is stuff where we have the only
reference to it and so as long as your
memory allocator is sane and people
aren't using too many wild pointers you
can do ever you want to it without
worrying about interference or without
worrying about interfering with somebody
else yellow will get to on the next
slide so the first thing we do is you do
allocate a structure and so in the
second state here we got our structure
the pointer doesn't point to it
thankfully because this is got garbage
in it because this has been initialized
we got our little local pointer P to it
so we initialize it now it's got
definite fields they got values but we
still can't get in from outside and then
we use this our co-signed pointer which
is turns into a consume excuse me a
release store and I didn't want to try
to type a full release store into that
arrow which is why I'm using the Linux
kernel API rather than c11
and once we do that suddenly the readers
can get to it this does the release so
that a reader getting to it because this
using a consume load which is what our
CD reference does those two orderings
pair and therefore the readers see valid
data rather than pre initialized cruft
over here that way they would get if we
didn't have ordering but because we can
use the equivalent of consume load
they're just doing the normal load of
that pointer I mean they take a cache
miss the first time they load it because
we just stored the pointer but after
that full speed replicating Embry's
cache very fast so what this means is we
can safely insert data into a linked
data structure even though readers are
plowing through the data structure at
all times so for insertion we don't need
to exclude readers we don't need the
readers to pay the overhead of checking
where there's this writer
they can just go blasting through pick
their stuff up and get some valid data
so the key point is that our CEO of sign
pointer and our CD reference avoid load
and story tearing that means that the
our shidu reference is either get to the
old null value for the pointer or it's
gonna get a valued pointer to real data
it's not gonna get some bit wise mash up
of the two players so either way the
readers going to see something valid of
course if all we can do is add well we
either have a garbage collector we've
got a big memory leak and they're
actually our garbage collectors I would
try to complain to standardized and
always had problems but they're kind of
out there if you want to use them
otherwise we need some way to remove
something with this structure and that's
what we get to this diagram here and
this can be thought of how thank Mike
for this one shortage there's cat meets
Heisenberg's uncertainty principle we're
gonna up the ante a little bit here so
just having a single pointer with a
single element we're gonna use a linked
list all right try to get with the times
and this is animals we got a boil cat in
a canoe they're all red because it's all
hooked up their readers can see at any
of those elements at any time for any
length of time we can't stop them we
can't even tell they're there
but if we were to do a list LRC you and
that's just a macro inside the Linux
kernel but the essential point is the
rear take and store into the Boas next
pointer a pointer to canoe using an
atomic store so again we're gonna either
see the readers means the old value of
the cat or the new value point of the
canoe but it's not gonna see some smoosh
of the two pointers either way whether a
reader goes here or here it's gonna see
a valid linked list with real data in
them and won't be confused now the cat
is yellow because there might be readers
there that we reader is there for a
length of time but new readers can't get
to the cat anymore you only have to
worry about the old readers and a new
reader tries to get to it'll end up with
a canoe instead isn't tight now if we
had some magic operation that waited for
all the old readers remember we don't
have to wait for the new readers they
bypassed the cat so if we have some way
waiting for all the old readers all the
readers that already exist when we got
done waiting there wouldn't be any
readers look in the cat anymore
any new reader again bypasses the cat so
we only had the great for the old
readers once all the old readers are
waited for nobody's looking at the cab
except us at that point we can just free
it and be back down to two elements in
link list of course it'd be easier to do
this if the readers were actually
putting something in memory saying they
were there which they aren't
nevertheless
possible solve this problem in this case
and this is actually close to how things
are permitted in one form of RCU in the
linux kernel
it's a quiescent based so don't worry
about the QSB are for a moment anyway if
you build a server class Linux kernel in
other words config preempt equals end
for those that have built kernels you
end up with this type of approach and
the trick here is that our Co readers in
that environment are not permitted to
block and that's the same rule you have
for pure spin locks we had some
discussion on spin locks yesterday and
one of the key questions that came out
was wait a minute if you got the spin
lock in your block that's pure spin lock
that's gonna cause a problem and you're
right it is that's why in the Linux
kernel when you uphold a pure spin lock
you are not allowed to block every
period in this story forget it because
if you do you could have all the CPUs
now spinning waiting lock you hold
they're not gonna give up their CPU
until look at the lock yeah you're not
gonna give up a lock to get a CPU and
you have deadlock
therefore if you're holding a pure spin
lock in the Linux kernel you do not
block period if you try it you'll get a
nasty splat scheduling 12 atomic I think
it is we have the same rule for our C
readers once you've done our she read
lock you are not allowed to block until
you do the matching arcs you read unlock
now there are other forms of ours to you
for example user space we have other
tricks we use but in the kernel for
server class builds that's the rule well
what that means is that if if we have so
we got the CPU to remove the cat and
then go synchronize our C which means he
blocks right because thinking that
actually waits until the readers are
done that means that was a context
switch well if you have a context switch
you can't be in a reader because readers
aren't allowed to block so as soon as
the sakai context switches we know that
all the previous readers have to be done
kind of a dull bank-shot
synchronization if you want to think of
it that way and then later on CPU 0 he
does this big long reader up here our
she read lock a bunch of stuff our
to read the lock and then he blocks the
same reasoning he just blocked he's not
allowed to block inside Holly's to an RC
reader therefore all previous our
readers on CP 0 have to be done and we
applied the same reason to CPU 1 right
here at this point all three CPUs have
blocked therefore we know all the
readers in the system that were in
existence before we move the cat are
done
the only readers left are ones that
can't possibly get to the cat and
therefore at this point it is safe to
free the cat anyway again this is I have
a guest lecture I go through this like
one to two hours where I go through
violently this in detail but that's the
Cliff Notes version of how you can make
this work I can remove data from a link
structure even though the readers are
plowing through that structure at all
times any way they want while you're
doing it all right so let's go back to
the question we had earlier how the heck
can you synchronize when your
synchronization mechanism doesn't affect
machine state and the thing is they
don't have to affect machine state what
the infected is dead is a developer
because the developer is not allowed to
block between the time they did that
arse read lock ended in the Arish read
unlock all right so we have
synchronization affecting the developer
not the system what this means are she
was there for synchronization via social
engineering
has been for over 20 years thing is
though you know every other
synchronization mechanism every other
one has a social engineering component
you've got you've heard both fader and
Michael say you know no data races don't
do that well that's on the developer
that's a social engineering component
making this work if you're doing locking
you see things like look just don't mess
with this variable look at there anyway
unless you're holding a lock that's the
singer's date that's the social
engineering part there's also a
mechanical part and lock and also may
Atomics for the data races and also same
thing for transactional memory the weird
thing about our CEO isn't that involve
social engineering it's that some
implementations of it only have social
engineering but by doing that we can use
these very very lightweight instructions
in fact we're actually lock and are
should unlock no instructions whatsoever
in order to get real synchronization
work done and of course if you're not in
the kernel with a server class build
then I'm sorry our seaweed lock and our
CV that lock do have a little bit of
code it's very local but there's some
there has to do because you're in a
different environment in the same four
user space code although there is a
variant of user space RCU that also has
a zero cost are should be locking are so
you'll unlock how those are interested
there are some references in the back of
the slide set
so why are we doing this well this is a
inhaling machine and we've got different
operations down here on the rows compare
and swap with fair exchange instructions
you have locks cache misses and the top
five rows there are all within the same
cork different threads and same core
hydrothermal system and you notice when
you go off core things get expensive so
we're talking almost two orders of
magnitude more expensive than a clock
period okay so if you just take a cache
miss normal instruction takes a cache
miss you're talking to almost tours you
might be more expensive than a simple
register register instruction
communication is expensive and if you go
off socket way over tourism magnitude
really expensive you know it's kind of
like dr. hurts when I do this don't do
that that's why I wouldn't be up to the
top there and that's where our Cu tries
to be for the readers and the problem we
have with some synchronization
mechanisms there down here a lot and you
do need to do updates which narrowly
mean being down here but you know be
careful be smart about it if you're
gonna do heavy weight synchronization
get your money's worth out of it that's
what it comes down to so let's look at
what this is the hardware structure why
this would happen and so the speed of
light and the atomic nature of matter
that Mike will manage mentioned before
yep this right here this is the diagram
so speed of light goes about maybe that
far and back over and back in a clock
cycle for two gigahertz and that's you
know it's nice you know chips are only
this big right so that should be okay
right
well the problem is that speed of light
in a vacuum how many people how many
people's computers have contained vacuum
tubes yeah okay no and the other thing
is is that we're not talking about light
we're talking electrons and the best you
can say about electrons it might make
thirty percent of speed of light going
down a wire but you know you have to
have these
inconvenient things called transistors
buffers caches and all this other stuff
in between time those are like silicon
not wires and there if you're lucky you
might be doing three percent of those
fetal eyes which is pretty fast you're
trying to keep up with it running but
but it's pretty slow for these computers
and so you can see that you know it's
gonna take a while for stuff to
propagate back and forth to people see
if you use across that thing and going
back to Michael's thing what we can we
can name these things right you know we
can give these CPUs names you know like
Carl's Tom and Fred alright and so these
changes are kind of proc waiting out
through the system and so Carl and Tom
and Fred have a different idea of which
variable they are because the changes
are kind of sweeping across system in
waves and yeah we can make things
perfectly assessment if we didn't mind
having everything be perfectly slow
because we have to wait for all these
stupid electrons to make it run side of
the chip to the other all the time
so this is why we have these weak memory
operations and then remember we're
consumed being my favorite example of it
although not everybody's favorite
apparently hopefully we're fixing that
last part a little bit so just this is a
hash tape or excuse me a binary search
tree my percent of this last year that's
the lower one Isis improved it in
January 2015 got a little bit better
performance out of it by doing a little
bit of tuning on it so you can see what
you know this is super linear don't
worry about that happens sometimes 60
CPUs about 90 X but we are actually
getting some reasonable performance out
of this thing a reasonable scalability
at the same time this is a search tree
that's not necessarily your best choice
for a parallel algorithm I mean you have
the stupid root node which is a
bottleneck and that's usually a bad
thing so if you're doing concurrent data
structures a hash table is usually your
first choice although trees can be
useful used to pretty careful about how
you implement them and never order
consume helps because if you have
everybody piling through the root node
if they're doing it without touching
memory they go faster if they don't
actually modify the root node marry and
locks on it's not your reference on it
they just go flying through it they can
actually go fast while they're doing
that which is why we're getting that
kind of speed up but a hash table would
be about three at three times faster
okay
just depends what you're doing updates
slow things out a little bit and
actually 3% of the operation or full
tree scans which really slows it up but
as you can see we get reasonably good
speed-up as we add CPUs not perfect but
reasonably good this is a very toy in
our Satine home rotation but with full
speed readers it's about 20 lines of
code five of which might turn into
memory or kusuma someday and people
still insist that our C was complicated
not sure why this is the hash table
again showing a number of different
things are Cu hazard pointers are almost
as fast they have some other things
bucket locking does pretty well until
you get off socket and then it falls off
a cliff global locking we expect to be
bad it is but hazard fighters and RCS do
quite well in this environment and are
see you again likes memory order consume
now this speed comes at a price and this
comes back to the Iron Triangle that
Michael talked about earlier what we're
doing here is we're getting reasonable
productivity in the Linux kernel people
were used to RC you use it quite well
and quite quickly we have really good
performance as you saw we give up
generality if you're doing something
where it's read mostly and you can put
up with the fact different readers at
the same time might have different ideas
of what what's in the data structure
which is okay surprisingly often strange
though maybe our Steve works really
really well that's just great if you
need consistency it requires a little
more mechanism you have to add low-level
locks in various places but it still
works pretty well and that is using
lanes kernel if you're starting to write
a lot well there are parts of Linux
kernel is still good speed ups that way
but they're more complicated they're
harder and they don't get quite as good
as speed ups as you do if you're using
RSU for exactly what it's intended for
if you're mostly doing updates you
probably want to use something else
there are a couple exceptions one of
them is we're using our CEO as a
poor-man's garbage collector and the
other one is if you have real time
constraints on the readers even though
they don't happen very often they need
to happen really fast
in that case our so you still might be
useful and not
mostly thing but again if you're mostly
doing updates you probably did something
else so on the iron triangle we're
getting the performance we're getting
the productivity we're giving up
generality it's used a fair amount Linux
kernel back in 2002 late 2002 and it
first went in I was hoping that someday
Marcy would be a successful minister
kn'l as it wasn't dining speedy X in the
90s before that in other words it might
have a hundred uses eventually as you
can see I was somewhat pessimistic we
passed through 10,000 earlier this year
this by the way is not my work this is
the community's work this is showing the
power of a community that knows how to
make things happen and I'm proud and
humbled to be associate with Linux
kernel community they do amazing things
not perfect
while you guys probably know that better
than I do but this is their work so why
we want to use our see how is it helping
we get fastest scalable readers is a big
thing free is a very good price of
something identify with pretty heavily
and nothing is faster doing nothing and
we've had a number of cases we've got
orders of magnitude speed up by applying
our CEO in real code in the kernel and
elsewhere it also avoids many forms of
deadlock the first ever use of we didn't
call Garcia back then back in Dennis PTX
was mostly about software engineering
not about speed applying RCU in a
distributed lock manager allowed my
co-inventor of RC you to throw 16,000
lines a really horrible grotesque
parallel code on the floor along with
all the bugs it had so the thing is the
readers don't win anybody so they can't
deadlock and that's actually kind of
useful also none of those none of those
api is retry
you see ours to read lock your there you
say synchronize are actually you wait
you come back there's none of this oh
sorry try me again later stuff and that
means we aren't as prone to live locks
as other synchronization primitives
might be and these are also well suited
real-time programming it also eliminates
the ABA storage Ria's problem
and this is part of acting like a poor
man's a garbage collector that's a topic
that could take a talk in itself and the
nice thing is it also plays really well
with other synchronization primitives if
you want to you can be in a are series
that critical section and acquire a lock
or you couldn't atomically incrementing
or you do a compare and swap it's
compatible with them and you can use it
the use of combinations of the different
primitives in order to get something
works well across the range of read
mostly to update mostly and there's a
number the pathname translation and the
linux kernel is probably most impressive
in many ways example of having multiple
synchronization primitives including RC
working together nicely okay so I'm not
going to go through the readers too much
you you it's what you'd expect if you
saw it there the key point here is that
you can have this do something with
function and the dependency chain goes
into it so we picked up our seed
reference which is a memory or a consume
thing we put in pointer P and we pass
that pointer into this other function
and the programmers going to kind of
expect that the dependency chain goes
into that function with it and he also
expects that if we call that function
multiple times in multiple different
read side critical sections all the
defensive chains go in all the various
places and go into that function where
they need to so there's kind of fan in
it also needs a cross compilation unit
boundaries and we also have fan-out
there's no more places where there's
some function that's called that does an
RC do reference among other things this
is kind of a why you would do that but
this is a toy example showing that you
call some function it does an RCD
reference hand you back the pointer the
dependency chain needs to come out
through the return of that function for
this to work and does in the linux
kernel for updaters we wait for grace
period like you would expect looking at
that second diagram we showed with a red
yellow and green
so we acquire a lock we pick up the old
pointer we assign the new pointer to it
which we presumably allocated and filled
in before this we release the lock we do
a synchronized RSU to wait for all the
old readers once all the old readers are
done
we can just pay for either thing because
nobody's looking at it anymore I'll talk
a little bit colder pendency I mentioned
before that RCU plays nicely with other
synchronization mechanisms and so what
often times you'll do is in this case we
go to our see read lock we pick up a
pointer and then we see we're gonna have
to hold on to this pointer for a very
long time maybe we're in the networking
stack we say oops we got a like send
something over to some of the machine
and wait for the response and we don't
want to have a reset critical section
going over that whole thing so what we
do is if that happens we increment a
reference count and at that point - we
don't do this Linux kernel at this
moment but we might do a kill dependency
in order to flag the fact that there are
no dependencies going through that
variable anymore we don't need them
we've got the reference count okay and
so that's a to kill dependencies a way
of indicating that we've handed off from
I'm sorry oh okay I'm sorry okay great
so I'm sorry I thought an hour I
obviously I was confused this is the
current standard I've learned things
about people hating dependency chains
I'm gonna do is I'm gonna go through and
show the resolutions always make sure
you know how much time you have yes and
we have what we're suggesting is ways of
doing without annotations that basically
restrict the user to what the compilers
normally do and that means you don't do
things like white cancel out the
pointers subtract a pointer for herself
end up with zero because the pilot will
just say hey here's a zero and break
your dependency for you so you rely on
the fact that happens and allow for that
to happen and then that's something is
useful for something like a Linux kernel
it may not be nice longer term because
it requires a developer to be keeping
track what the compiler is doing it's a
longer term I'm thinking in terms of a
storage class type of a thing I've had a
couple of people say they like it
unfortunately those people were people
involved original proposal so who knows
but the idea is to mark the variables
the carry dependencies so the compiler
knows oh this variable
and see therefore I have to be careful
and it also documents for the developer
saying oh yeah okay here's old
appendices here they go in hard they
come out and everything's nice
that's the interactions we don't have
time for double-checked lock but they'll
be there maybe we have a P and I don't
know but my hope is we can use the
restricted dependency change essentially
documenting what optimization compiler
use for kernel code and other
pre-existing large code bases and use a
storage class for new projects and
possibly also migrate the old projects
to it anyway I apologize for running
over sure okay all right and apologies
to the next session thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>