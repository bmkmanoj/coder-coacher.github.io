<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Rian Quinn “A Test a Day Keeps Your Manager Away!” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Rian Quinn “A Test a Day Keeps Your Manager Away!” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Rian Quinn “A Test a Day Keeps Your Manager Away!”</b></h2><h5 class="post__date">2017-10-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KdJhQuycD78" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">- Alright, I think we're gonna get going.
It's nine o'clock, thanks for coming
to such an early
presentation on a Thursday.
It's been a long week.
Hope we still have people coming in.
Alright so my name is Rian Quinn,
I work for Assured Information Security
on our open source Bareflank hypervisor
which is basically the
first open source hypervisor
to be written in C++ and I'll be talking
about testing today.
One of the goals with our hypervisor
when we first started developing it
was we wanted to be able to use it in some
more esoteric type of applications,
some government, automotive, medical,
that kind of thing and there's a lot of
regulations associated with using
something like a hypervisor
in these environments
and so testing was incredibly important
and so we try to take it
as far as we possibly could
and since then I've actually been getting
quite a few questions both internally
within AIS but also
externally from other projects
as to how we actually did our
testing, how we pulled it off.
Also questions like what kind of an impact
does the testing have on your development
and then I've since then
actually had some questions on how can we
integrate some of this
stuff into our project,
we don't really know how to do it
from very experienced C++ developers too
so some of this technology is new enough
that it's kind of mystical
and so I kind of want to prevent that.
So I started by going to my management
and saying we should create a hello world
for continuous integration
because if I can create a simple project
that sets up continuous integration
for various different environments
and puts in as many tools
as I can possibly get
then I can always point
people to this project
and say this is how we set up
our continuous integration and all the
various different tests that we use
and if you want to be able to integrate it
into your project you can go here
and see a very simple
example and so we did that
so I provided the link.
I would love to get pull
requests from the C++ community
to further expand this project.
Everything that's in there
is very, very simple.
The idea is to keep the examples
as simple as possible
but there are a lot of other tools
that are out there that
we can apply to our code
that I'm probably missing.
As you'll see as we go along,
I've got quite a few but I'd love to
add as many as we can.
Another goal that like I said before
that I have with this is
to very much demystify
the idea of continuous integration.
I think people tend to,
I don't know, they see it as something
that's impossible for a large project.
I know when I wrote the topic,
that was probably one of
the biggest comments I got
was how does this apply
to my large project.
I mean we have hundreds
of people working on it
and it's you know, hundreds
of thousands of lines of code,
there's no way I could
possibly do this stuff
on a project of that size
and I've personally done it
on large and small projects.
I've worked on projects that
have zero testing at all.
Everything is done using manual
labor and test procedures.
I've worked on code like
Bareflank hypervisor
which is 100% automated
with 100% code coverage
so I understand both problems
and I know the positives
and negatives associated
with going in both direction
because believe me 100%, code coverage
does not have as much as
a return on investment
as you might think.
I'd also like to give a quick shout-out
to a couple projects.
I've been working with Jim from Astyle,
he's fantastic, I'm a big fan of Astyle
and so he helped out a lot to try
and get some of this together
as well as Coverity scan.
They actually made an exception
for this hello world project.
They don't normally allow
hello world projects
to go through their servers but they made
an exception for this one.
And also the GSL.
Neil is fantastic.
If you haven't used the GSL you should be
so go ahead, take a look at it
and IncludeOS just because
I love that project
and if you haven't seen
it, go take a look.
Alright so we're going to start off
by creating an open source project
so us as a group, we're
all gonna do this together.
We have this critical requirement
from our customer that we have to be able
to create this application
that can print hello world
and so we're gonna create a producer,
he spits out hello world, very simple.
We're then gonna create a consumer
that consumes this producer,
and then runs its print message program
and then finally we'll
create a main application
that creates the producer,
passes it to the consumer
and the consumer then does
its job on construction
and the program exits so
this is a fantastic program
we were even as a group we
were featured on Time magazine.
it's just awesome until
one day somebody decides
that they want to change the code
because they think that we could have done
a little better and
specifically they would like
to be able to change the
text from hello world
to anything they want and so they provide
the project with a pull request.
This is what the pull request looks like.
So as you can see the code's
changed quite dramatically
but the theme is there.
They're looking to allow you to create
a new define that can change
the text of the print message.
They're also interested
in adding or prepending
I guess message colon to
the output so that you,
it's explicit that this is a message.
Now you can see from this example that
the code itself won't even compile.
It's missing all sorts of things,
the style is very different than the
style that we had in there before.
It's hard to see on the projector
but there's added white
space and new lines
at the bottom that shouldn't be there
and this is certainly gonna have problems
with memory leaks and core
C++ Core Guideline compliance.
So what I'd like to do.
On that you've seen
this horrendous example
as I'd like to take code that at least
does compile and create a pull request
so we're going to go to this file.
Of course I don't have
internet connection.
Alright well I'll do it
later then if we have time
but at any rate what I was going to do
is create a simple pull request,
so we can get that kinda kicked off
and you can see what the
CI process looks like
but we can do that if
we've got time at the end.
Alright so without CI and we've seen this
on a lot of projects
that use mailing lists.
You're going to have a bunch
of basically project maintainers,
they're gonna see this this pull request
kinda come in and in the
case of a mailing list,
it's just going to be a patch
which is horrendous to review
and everyone's gonna
have to kind of chime in
and explain why that code is horrendous
and everyone's got different
ways of handling this.
Some people are gonna just slam the person
that put that code in there.
How dare you write such
horrendous code and all in yards.
Other people will be nice
and provide you know,
decent comments and actually
help guide the person
to a better piece of code but in the end,
the major theme here is that
this is a manual process.
everybody's got to sit down
as the project maintainers
and look at the code and try and find
all the different problems
that are wrong with it
and since they're project maintainers,
they're probably very
busy so the question is,
is this actually a good use of their time.
Should the project
maintainers be worrying about
how many spaces are between parentheses
or worrying if there's a memory leak
or white space or you know
should they be required
to take your patch, patch
the code then compile it
and make sure it still compiles,
make sure the code still runs
and that's a lot of time.
Even for a patch this
small, you could end up
with like an hours worth of work.
And you know for a lot
of people, that's a lot.
Another thing is what's the likelihood
that they miss something.
If we all stared at that
example for long enough,
do you think everybody would find
the same number of issues with the code.
Probably not right.
The likelihood that we miss
something is really high.
I can say in my experience even with some
very rigorous CI practices
with as many tests
that we can throw at it we
still end up missing stuff.
Now they're more structural,
architectural in nature
so it's not silly things like style
but it still ends up being a problem.
It's a lot of work to
do a good code review.
It's very hard and the
last thing you want to do
is be consumed with
these sort of you know,
issues that could have
been spotted by tools
that we already have today
and then the other comment I have is
what's the likelihood
that the author of the PR
ends up being offended by the comments
that come in from a piece of
code that looks like that?
Probably pretty high and you know,
especially for like an open-source project
but even when you're
working with your own peers
and on closed stuff at work,
your goal is to try and
foster a good environment
and when somebody who's a novice comes in
and writes a piece of code like that.
they can sometimes get
lambasted pretty quick
and so if there is a
set of automated tools
that could tell the person hey
you need to fix the following
before a project maintainer or some person
out there had a chance to review the code,
they're gonna have a
set of automated tools
telling them that they've
done something bad
and that's much easier to swallow
than an actual physical person
and so by the time the project maintainers
actually see the code, it should be things
that are more structural in nature
and likely to produce
a better debate online
or internally within your company.
So in case you're not familiar with what
continuous integration is,
it's simply the act of
integrating small changes
into your project on a continuous basis.
So every single time you make a patch
you're going to go through a very,
a series of various tests
and actually do integration
of that code to validate that
you haven't broken anything.
There's also other continuous type things
like continuous deployment
and that kind of stuff
but we're just gonna focus on CI today.
It can be considered part
of test-driven development
but it doesn't have to be.
The basic idea here is just that
whenever you're making changes
to your master code base
you're going to be
continuously making sure
that you haven't broken anything.
There are tools out there
that can help us do this
so for example things
like Travis CI AppVeyor,
GitLab, CircleCI, Jenkins.
There's a lot of tools out
there that you can use.
On this project we're just focusing
on Travis CI and AppVeyor
because that gets us Linux,
Windows, Cygwin and OSX,
but you you don't need to use
these tools so for example
if you can compile on
Linux you could just create
a Cron job that goes,
can kicks off every night
and runs your master branch,
make sure it compiles
make sure whatever tests you
have in place actually run
and then if there's happens to be an issue
you could send an email
to the project maintainers
and say, hey our you know
continuous integration
server found a problem and then you'd be
able to address that as
soon as the tests were done.
You could do that nightly,
you could do it weekly,
the idea is to do it as much as possible
and setting up a Cron
job with an email is,
should take like an hour
it's not that much work
so it really depends on
what kind of build system
you have but I can tell you like for us,
we have a project that
has very little tests
but it does build and we do
have a nightly build server
that goes and takes this thing,
this giant chunk of code,
builds it every single
night and then reports
if there's a build failure.
It takes roughly around
eight hours to do the build.
There's I mean hundreds
and hundreds of thousands
of lines of code and it will only build
in a very specific
environment but we still
have it automated so at least at minimum
every night we know if
the project compiles
which can be huge.
There are open source examples of using
continuous integration with a set
of various different types of tests
and the biggest return on investment
is gonna be could you know
can I get it to compile
and can I run a set of actual,
like you know unit
functional integration tests
and these projects that I've listed
are just a very small subset but I like it
because it kind of shows
various different degrees of how much
tests people are running in their
continuous integration
environment so for example
with Bareflank like I said before,
we have everything, 100% code coverage,
static dynamic analysis, everything down
to documentation and white space.
It's actually quite painful to try and get
a PR through the process,
can take as much as a day or two
to get even a small code change through
all the checks that need to go through
but this project has its
reasons for doing so.
The JSON library that I listed here
is very popular one for C++
and they leverage quite
a bit of the same process
so if you're looking for another example,
this JSON one is fantastic.
The GSL is a good example of a project
that uses continuous integration.
They use actually Travis
CI and AppVeyor as well
but they don't have as many of the tests
that I've listed here
built into their CI process
so for the most part they're building it
and they're running their unit tests
and that's about it so
this should give you
kind of an example of just
how far you can take this
versus you know how far do you want to go
based on your return on investment.
And then finally one note I wanted to add
is that if you happen to
be an open source project,
there are a lot of badges
that you can actually add
to your project that'll help show
that you're doing some of these tests
and I think from a C++
community perspective
it's probably a good
idea for us to use these
'cause it can really show how much rigor
you know we all put into our projects.
Alright so let's jump right into this.
So the the number one
largest return on investment
you can get for setting
up continuous integration
is hands down does it
compile and does it run
because that can actually
be quite a bit of work
depending on the project.
On a really small project,
the amount of stuff
that has to happen is
probably pretty minor
but on a larger project
that takes hours to build,
if you have a continuous
integration server
that's plugged into your
pull request process
then the project maintainers
never have to actually
pull down the code and do a compilation
or run the code themselves.
They'll get a report once
that's done by the automated,
you know tools and they
can be rest assured
that it compiles in different environments
they care about so it can
save quite a bit of time
for the developers and more importantly,
they won't click merge until
they know that it compiles
which from a development
perspective if you're
working with a lot of
people we've all been there,
you go and pull down
master and it doesn't build
and then you start
screaming across the office,
who broke the build system
or why is the code not built.
Well this can really help prevent that.
Still happens but it can help prevent it.
To do this, it is really simple,
if you look at this example.
I have several Travis CI scripts in here,
this is simply yml and so if you go to the
open source project you can see you know
how this kind of fits in
but here I've basically
defined a test that I
want to run in my CI.
The add-ons part is
simply telling Travis CI
what packages I need to install
and the environment variable at the top
just labels the tests so
I have a way of knowing
what test this actually is
when I see it on the server
and then the script part goes and simply
runs CMake against our code, it then runs,
make, make tests and then
actually executes the code,
so it's that simple.
Now once again like I
said everyone's projects
have different build
environments and different things
but once again if you have
the ability to build it,
you almost certainly can
get it to work in CI,
especially if you, if you have
very specific requirements
like we have on our larger project,
then you can do things like Jenkins
where you can create your own servers
and set them up however you want
and get them running
whenever you need them to so.
The next check here that has a really
large return on investment is simply
doing a white space check so this,
this is actually quite important because
it helps prevent contamination
of your git history.
So everybody's got different editors
and different ways they like to code
and some people will turn on things
in their editors that will
remove trailing white space
and in fact some editors
have it on by default.
Other editors don't and so
you'll end up with people
who are committing white
space to the project
and people who are then
removing that white space
when they make a code change
because their editor
detects it and removes it
and so you'll see PRs where
white space is being added
and removed you know
throughout the git history
and so lines are being changed
but that wasn't actually the line that
the person was trying to modify.
Instead it was just the editors adding
and removing white space and so having in
your CI process the ability
to check this white space
can help prevent that
type of contamination.
So I know like when I've
done a couple patches
to the Microsoft GSL I
have to because I don't
have this check in there
every time I do my patch,
the first thing I have
to do is go to my editor
and turn this setting off
and then I make my patch
and then I submit it
and I have to go through
the diff and make sure that I'm not adding
you know these lines that are accidentally
removing trailing white space.
As soon as I'm done making that patch,
I gotta go back to my editor
and turn that feature back on
because on other projects that I work on,
we have this check and
we don't want white space
being added so it's this constant like
back and forth, it's kind of annoying
so I think a simple test like
this can help prevent that.
In this particular case
we're using git check diff.
There's other tools that can do this too.
This is just a really simple one
if you happen to be using git.
If you run this command it'll tell you if
you have trailing white
space that's in your patch
or if you have, missing a
redundant end of file new lines
and in our Travis CI
script it's really simple.
We simply run this command
and if it outputs something
then we know that we've failed the test.
If that happens we tell
the user you fail the test
and then we actually run the command again
so that they can see
where in the actual patch
they're failing so where they actually
add in new lines and where they didn't,
and then of course we exit.
By doing the exit at minus one,
Travis CI will fail the test
and you'll get a nice big, you know X.
Question.
- [Audience Member] Is it
git diff dash check or--
- It is git different dash, dash check.
Thank you for correcting that
so I'll make that change
before I upload the slides
but yeah that's an error,
it should not be that
and you can see in my my screenshot
I have it the right way so I'm just,
dyslexic apparently.
(chuckles)
Thank you.
Alright so right off the bat
if we look at our example
of the person that made
this horrendous PR,
the git diff check will tell us that hey,
you have white space
and that test will fail
and so the author of this
PR will immediately get
some feedback to say hey
you need to make this change
and we'll end up with
the code on the right.
The next easy thing to
integrate into a project
whether its large or small
is source code formatting
so I've reviewed lots of source code.
There's nothing worse
than having to go through
the review process and say
please remove the following,
you know syntax or we
don't use spaces here
or we don't do that and the amount of time
it can take if the patch is
really big, it's just obnoxious.
But the problem is is that especially for
some of those projects we work on,
we don't really like
having this spaghetti code
of you know many different developers
and their coding styles
and so you can end up
with this project that that just has like
15 different coding styles throughout
and it's very hard to kind of follow
and read and where it's going.
So using a source code formatting checker,
you can alleviate these types of problems.
Now the the number one
issue with with these tools
and where you're going to
end up with a lot of debate
is that they're not perfect.
Astyle I love, Clang Format
is also another option.
The problem is is that for as good
as something like Astyle
is, it's not perfect
and you're gonna have to
compromise on something,
everybody will, I haven't found
somebody who has looked at
these source code formatters and said
it does everything that I want it to
and it meets my personal
preferences perfectly,
it's just never happened
so if you're going
to integrate one of these tools in,
the positive and minus to this
is that everybody compromises
so nobody's gonna get
everything they want.
But it's better to have a consistent way
to verify the style of your code than to
get the perfect source
formatting out of these tools.
In a sense the return on investment's huge
when everybody basically
just says I'm going
to put my hands up, we're
all gonna just use this
one coding format that
the tool is telling us
we should abide by and that's
it, the argument's over.
It's not perfect but
it's fantastic in a sense
that it removes the debate
and every single time
I put a code change in,
this tool will tell me
if I need to make a change or not.
Very easy to integrate into small projects
and it's actually relatively easy
to integrate into large projects.
For the large projects
these types of tools
will allow you to state
exactly which files
you want to do source formatting on
so in a large project
the easiest way to do it
is to look at you know say for example
your manager come down and says
we need to add the following feature
so I have to go do this piece of code
and I have to go in and
modify it to add that feature
so set up your CI with
the source formatting
and tell it to format that piece of code.
Everything else gets ignored
but that piece of code
that you touched, at least
at minimum you know now
it's following that standard and over time
you can keep adding to this until
you finally have the entire project.
The big issue that you also run into and I
remember last year at CppCon when
we discussed some of this stuff,
one of the comments that came up was
what do you do about git history
so let's say you've got
a medium-sized project
and everyone agrees let's just go through
and run this tool it's going to format
all our text and we're good to go.
Well the problem is your git history
is going to get just trashed and so,
there really is no like magic
solution to this problem.
I mean obviously if you're
gonna go through and format
all of your source code so that it
leverages one of these tools,
you're making code changes.
Those code changes are going
to show up in your git history.
There's no way to rewrite history
so the best solution that
we have found in practice
is that don't make a change like that
until you start your next release.
So you just got done delivering a product
and you're about to you know,
throw all the developers
now onto the next big task.
It's gonna start a new baseline
and you're gonna be making
a whole bunch of changes
over the next couple weeks or months
or whatever it is you're doing.
Make your source code change
right before you start,
that way your git history,
yeah it's modified
but you can always go back
to the previous release
and see everything that was done
and then all changes from that point on,
that the history is fine
so that that seems to be
the winning combination that
we've had at least in the past.
The other note I'll make is that,
and one reason I like
Astyle over Client Format
is that you really want to package up
your source formatter and
some of these tools in general
with the environment or
on a specific version
and with your build system,
the reason being is that if I take say
Astyle version two point X or whatever
and run it against my source code
and somebody else happens to
have Astyle three point X,
they're gonna find different things.
The same tool, same flags, everything
but the two tools since one's updated,
they may find bugs or things
that weren't being formatted
properly and they'll make
changes and so you'll end up
with somebody's source
format are making changes
and then somebody else
is putting them back
and then making changes
again and putting it back
each time codes being checked in.
You all need to use the
same version of these tools
and so just keep that in mind
when you're integrating these
things into your project
that there has to be a
way to specifically state
I want to use this tool this
version of this tool rather.
Excuse me
- [Audience Member] Why
couldn't you do that
with Clang Format?
- You can do it with Clang Format.
The problem is is that
if you want to package up
in your build system,
compiling Clang Format
is a pain whereas Astyle
takes about 30 seconds
so it's much easier to package up Astyle
into your build system and deploy it
with everything that
you're doing versus Clang.
Now you'll see I'm not a
big fan of Clang Format,
it's very hard to get
changes into the project
'cause they really only
service large projects
but I love Clang Tidy and
so we have this same problem
with Clang Tidy so I'll get into that.
Okay so I'm going to show this one,
I'm probably not gonna do the walkthroughs
for the other ones since
we don't have a ton of time
but I at least wanted to show
what it kind of looks like
within your build system
for setting this up
so if we look at our
Travis CI file to start,
it's really very simple.
There's some stuff in
here that won't make sense
unless you see a lot of the other tests
but in general we try to
update the build system
so that we have a newer version of CMake,
Travis CI uses Ubuntu
14.04 and so there's a lot
of things on there that are really old
so you'll see we're
always like updating GCC
and we put in a custom CMake here
and then from there each test just runs.
The way that this matrix
works is it basically says
every single test that
I have here for this
will spawn off a new
server that will then go
and download my code and compile and run
just the stuff that's in this block
and so every block that you
have is gonna be a new test
and so you'll see on
the output of Travis CI
a giant list of all the
tests that are running
and they'll have clocks
that are basically showing
you the progress each test is making
so if you happen to pay Travis CI
for exclusive access to their servers
like Microsoft does then
when you submit a PR
you're gonna see like
in this particular case
we have something like 20 different tests.
You're gonna see 20 server's
all start up simultaneously,
all running your project
to compile your code
and then run the specific instructions
listed here in each block
and so and about for a
project of this size,
in about two minutes you're
gonna have the results
for everything all in one
shot which is fantastic.
Within the CMake thing for Astyle,
you can see here we're
grabbing the Astyle code
and at the moment we're using Bareflank.
That's because Astyle didn't have like
a CMake build system built into it
so it was actually quite difficult to get
their build system integrated
into an existing CMake one
so I have a custom patch to
Astyle to integrate CMake
and I was actually working
with Jim to get this integrated
so I think in the future real soon here,
he'll end up actually
having CMake compatibility
so that would be pretty nice.
We can actually get
rid of our custom patch
and all this is doing, if
you're not familiar with CMake
the details here aren't important.
What this is doing is
it's downloading the code,
it's then gonna build it and install it
and this line right here just simply says,
please install it into my build directory
for my current project.
From there we're going
to create a custom target
called format so that
allows me to do something
like make format and that's
going to take the executable
that was compiled or
downloaded and compiled here
and it's going to run it
against these Astyle args
which is everything you see here
so this is where I actually define
all the various different formatting rules
that apply to this project and
then here's where I tell it
which files I want to
actually do Astyle on.
There's some details here
that are probably important
so if you're on a large
project there are two ways
you can do this, you can provide it with
just a source directory and say you know,
give me all the source code you know in
the following directory or what you can do
is which we do on Bareflank
is you can actually use git
to tell you all the files
that are in your repo
and then pass that list to Astyle and so
anything that's being managed
by your git repository
shows up in your AStyle so
there are different ways
of getting access to which
files should I be formatting
and that's really up to you guys
on just how much you want to actually do
and every single one of
these is very similar.
You're creating a target,
you're gonna integrate the tool
and then give it the parameters you want
and move from there so it's very simple
to kind of set this environment up.
Alright and so finally with
respect to AStyle format,
you can see we build the
project, we run make,
make is going to not
only build the project
but it's also going to build Astyle
based on our use of external
project add-in CMake
and then from there make format will work
so I can then run to make format.
If make format actually changes code
that means the test is
failed because it means that
there is source formatting
errors in the PR
in which case git diff will tell us
that hey there's been
changes made to the repo
therefore the test must have failed.
We'll tell the user and
then do git diff again
to show them which portion
of the file were formatted
so they know you know
where their mistakes were.
In general they're not going to care,
they're just going to run make format
and then resubmit their PR
but it sometimes helps to see.
So right off the bat if
the author of the PRCs
that those tests fail, they're
going to run make format
and the result is on the
right for our current code.
Now there is a bug in my screenshots.
At this point the code should
have been able to compile
and so you'll see auto buff equals new,
there should be new space char,
that code there makes no
sense so I apologize for that.
The next tool will
integrate a stack analysis.
I love these static analysis tools.
They are fantastic.
They make such a huge
difference in your projects.
If you're not using them
please find a way to do it.
Clang Tidy is my favorite but
I also like all the others
that I've listed CppCheck,
Codacy, Coverity Scan,
they're all fantastic and there's
a whole bunch of other ones.
in fact there's projects online
where they'll take all these tools
and run them against
your source code for you
so and they they have even
more than what we've listed
and so all you have to do
is you submit your code
to one online project or work with them
to do it privately and they'll
run all these static checkers
for you and validate if
you have a problem or not
and it's just fantastic.
These things do everything
from finding compile time bugs
to compliance testing for
example the C++ Core Guidelines
and good, bad and ugly
patterns so Clang Tidy
is fantastic for compliance.
They check for cert bugs, they check for,
I think they're adding in
5.0 High Integrity C++,
they have the C++ Core Guidelines
and if you happen to be
a Clang LLVM developer,
they also have checks
for your style guides.
There's also a static
analyzer that will do things
like compile time bug checks.
It's good, it's not the
best but to be honest
the other stuff, if you get
through the compliance checking
most of the time you're
not going to end up
with a lot of static
analysis issues in the end
because the the other
stuff will find issues
right off the bat.
CppCheck and Coverity
Scan tend to be better
at finding the compile time bugs.
CppCheck is very easy to set up.
Once again you can grab
a specific version,
compile it in like 30
seconds and then just use it.
Coverity Scan is a little
bit more complicated.
You have to submit your
source code to their website.
They then do a full
analysis and then give you
a very fancy web GUI to
parse through all the errors
that it's found and check them
but if you can get it set
up it's a very useful tool
and if you happen to be an
open source project that's free
so there's no reason really not to use it.
This is very easy to
setup on small projects,
harder to do on large projects.
It is, I mean hands down the best solution
is once again to break the
project into smaller parts
so let every single time
you make a code change
to a specific portion
of your large project,
integrate these tests for that small part
and over time keep doing that until
you've eventually got static
check working on everything.
And then once again if
you look in our CMake file
we added a check target for make
and once again it's very simple.
You compile the code, run, make, check.
That'll go through and run
in this particular case,
it runs CppCheck and if there
happens to be any issues,
it'll spit out what those problems are
and move on from there.
There's really nothing complicated
about getting this going.
Also make another note that Cpp or,
excuse me Codacy which does really
kind of regex type checks.
It'll look to see for common patterns
that are good, bad and
ugly and it'll tell you,
you probably shouldn't
do this because it's bad.
I don't know if it's been
here the whole time or not
but at least recently
I started noticing that
it's finding errors from CppCheck
and so if you use Codacy
I believe you'll end up
with CppCheck's along the way as well
in addition to the regex type stuff
that they're doing so
it's a fantastic tool
if you can use it and it
might save you some time
'cause you don't have
to go and do CppCheck
manually yourself in the CI.
So the static analysis tools
are gonna look at this code
and they're gonna find
all sorts of things.
Now what's what's cool about this
is that if you look at
what it's going to find.
If you were to fix all the things
the static analysis tool finds,
you're actually not going to end up
with any dynamic analysis
failures in this example
because it found all the
problems using static analysis
but because of that we
will not fix them all
and wait until we do the
dynamic analysis part of this
but you're gonna find
things like you should
have used a unique pointer
instead of new and delete,
that's a C++ Core Guideline check.
You're not allowed if
you're trying to comply
to those guidelines you're not allowed
to use new and delete manually.
Another one that it'll
find is you should be
using no pointer instead of zero or null
and that's a Clang Tidy
modernization check.
It's also going to notice that
that if statement is always true.
Reason being is that
right now since we're not
recompiling this program
with a different string,
hello world is a const char star
that always points to something
therefore it is never
null in this example.
This is due to the fact that
that is a runtime check instead
of a compiled time check
therefore the runtime
check will always be true
or it will always be false.
It's based on how the
code is actually compiled
and not how it runs and
the static analyzers
will actually find that and tell you
that this is always
true, it's useless code
and I believe Clang
Tidy's performance checks
will find that for you
but there are other ones
that'll do it too and so
that's gonna cause the person,
the author of this PR to kind of go hmm,
maybe I need to do
something different here
because I'm doing a runtime check
and that doesn't make any sense.
It's also going to tell you based on the
C++ Core Guidelines
that the use of sprintf
is an unsafe function.
It's unsafe for many reasons.
One, it's not the snprintf version.
Number two it's because it
uses variadic parameters
and that's not allowed once again
under the C++ Core Guidelines
so right off the bat the user's gonna be,
the author of this PR
is gonna have to rethink
what they're doing because they're doing
stuff that's just not allowed.
Dynamic analysis is
another one you can run
so when you think of dynamic analysis,
think of what can I find
after I execute the code
so static is done during compile time,
dynamic analysis done during runtime.
Valgrind traditionally was
the tool of choice for years.
It's a little heavy-handed.
The Google Sanitizers are somewhat newer
and they are fantastic, very easy
to integrate into a project and...
You know basically everybody should try
and compile these if you have them.
I'll be honest, I don't
know what Visual Studio
provides on here, I know they
have the static checkers.
I don't know if they
do the dynamic checkers
so you know if someone knows
how to set that stuff up,
please provide a PR to this project
and we'll get it integrated.
We're using AppVeyor and Visual Studio
so I'd love to get it in there.
So these tools are good
at finding resource leaks,
they're good at finding
memory corruption as well
so I ran over a buffer
or something like that
and they're also good at finding
race conditions and deadlocks.
Very easy to integrate small projects
and they're actually really easy to find
or to integrate into large projects.
Now I have the note here that you could
break it into sub projects
but I'll be quite honest
unless you're writing
really horrendous code,
the likelihood of Google
Sanitizers finding
a whole bunch of stuff wrong
with their code is minimal.
If it does find a whole bunch of stuff
in your large project you
probably need to fix it
because if you're finding
issues using dynamic analysis
they're probably really bad.
Static checks you know hey that's great,
it's gonna make our code a lot cleaner,
probably more efficient, blah blah blah.
Dynamic checks are gonna tell you hey,
your code is gonna like die so
you really need to probably
fix anything these tools find.
Alright so to integrate these in there,
especially so I basically
I don't like using Valgrind
because Valgrind intends to
have a lot of false negatives
and we do have it
integrated in this project
but like I said it can
find false negatives
and with simple things like just compiling
a CL hello world project it'll find stuff
that doesn't exist or it's finding issues
the libraries that really aren't issues
and things like that so it's a useful tool
if you can get it to work, great
but the Google sanitizers
tend to work really well.
I've really never had
any problems with them
but the Google Sanitizers
are broken up into different
types of tests and they're
based on compile flags
you provide Clang or GCC.
So you can't do them all at once like
you can with Valgrind so you're gonna
end up having multiple tests.
So you're gonna do one for
your address sanitizers
and you'll do one for your memory
and then you'll do one for undefined
and you'll do another test for thread
for thread sanitizer as well so here
you can see I basically enable,
add the address sanitizer,
and I compile the project and run test
and then I have three other
tests to do the same thing
but they turn on the undefine
checker or the thread checker.
And so the dynamic analysis tools here
will identify that that
new command is actually
leaking memory and that
if the string happens
to be larger than 64 bytes then you're
going to end up with a
memory corruption error
as well because we can define string
to be anything we want, there's a problem.
There's also an issue due to the fact
and the static checker found this
based on saying this
should be a null pointer
but because this is a
string that equals zero
it's perfectly valid from
a compile-time perspective
for string to equal an integer
and that's not what, we want
we want it to be a string
so I could put a lot of
really bad things in here
and end up with a bit of a problem.
I can also put in zero or I guess
it's not true since we're doing the check
but at any rate there are different types
that you can put in there that
that could cause problems.
Alright so it's all said and done after
you're doing the static
and dynamic checks.
There's no way the person's
gonna be able to get through
this PR without the code
coming out something like this.
There's another way you could do this too.
You could put in a
compile time static check,
think it's static assert.
That would do a type check on string
to make sure that is in
fact a const char star
therefore if the user were to put in
like a null pointer as
the you know define,
then it would fail to compile
because the static check would say hey,
we're looking for a const char star.
This code is a slick way
of doing the same thing.
This is in fact a compile time check
because you can't concatenate
a null pointer in here.
You can only concatenate a const char star
so the compiler itself is
doing that check for us
and this basically has the same effect.
Alright so I mean this is basically where
we're going to hold off on the example.
This is about as good as is it's gonna get
but there are other things we can do
and we have these all integrated into
the hello world project so
one of them is code coverage
so code coverage if you're not familiar,
when you write your
unit tests you're gonna,
the ideas essentially exercise the code
and what code coverage tools do is
they'll tell you which
lines have been exercising,
which ones haven't been.
It can be extremely helpful
if you're trying to make sure
that all of your source code
is being properly exercised
by your testing framework.
The downside here is that in my experience
there tends to be a lot of
code that adding unit tests for
provide little if any
return on investment.
In fact, could be argued
that it's like a negative
return on investment because
you're generating code
that really is pointless
so a good example is
you might have a piece of function
that's doing something very trivial
and the compiler or the
coverage tool's saying hey,
you know you didn't test
this and I can go alright,
I can write a test but it's doing nothing.
All I'm doing is writing this test
so I can get the coverage
tool to stop complaining.
We have a lot of that in Bareflank
so it you kind of have to pick
and choose your battles here.
The other thing I've
noticed with these tools
that they are, they're all
generally difficult to get set up
and there tends to be really
poor support inn C++ for these
so these coverage tools
work great with a lot
of other languages that have actual like
first-class support for testing which C++
doesn't really have so for example,
GCC will report when
you have partial tests
so for example if I say
like if I equals zero
or I equals one, do something,
or if I only have a
unit test that exercises
if I equals zero, that's
technically partial coverage
because you didn't test the other
test case which is I equals one.
The compilers like GCC and even Clang
tend to report false
negatives all over the place
for these types of partial supports
so you generally tend to
just have to turn them off.
Oh yeah so LLVM actually tends to be
a little better with that
and it'll actually show you,
I have different
instantiations of this template
and you didn't test this
version of the instantiation
which it's like mind-blowing
but it does matter,
there's a difference
between a double in it
and you can end up with weird problems
of doubles that you wouldn't see in it so.
- [Audience Member] You can
ignore certain lines of code?
- You can but if you've got a project
with a hundred thousand
lines and you're changing it
all the time, going into it
and saying ignore this line,
ignore this line, it's
gonna take you about
as long as it did to actually
go and write the test
so in my experience it's
been all or nothing.
You either test everything
or you don't test
or you just don't use these tools.
They can sometimes be helpful
if you want a quick glance to make sure
that code was actually
being tested but in general
what matters more is that you're meeting
your functional
requirements with your tests
so the idea of being able to validate that
the APIs are being properly
fuzzed and that you are
actually meeting all the
functional requirements
you have for a particular API set
as well as the the product itself
that return on investment
with the unit tests
and your functional tests
and integration tests
will be much higher if that's
what you're focusing on
than trying to obsess over
getting perfect coverage.
But once again sometimes your customers
are going to tell you you have
to do this and that's fine.
If you're like we have with Bareflank
so if you end up in that situation
these tools can be fantastic
at helping to make sure
that you have that and then
you can actually provide it
as a body of evidence for accreditation
or anything you might be using it for.
Documentation's also another thing
if you happen to be using
automated documentation tools
like Doxygen or Sphinx, you can
integrate these into your CI
as well and this hello
world has that integrated
and they're good at you
know finding all sorts
of various different issues like hey
you didn't document this function
or you're missing parameters
or return types or whatever
so if you happen to be
working on a project
where you have pretty
extensive API documentation
using this stuff,
getting this into your CI
is extremely helpful
because you don't end up
having to wait till the end of a release
and then go through all the documentation
and run your tools and fix all the docs.
You're doing it periodically
with each PR which is fantastic
and once again with this very
easy to do on a small project,
can be more difficult on a larger project
if you have errors galore but once again
if you're really that
picky about making sure
your API docs are done
right, maybe not a bad thing
to just turn on and get done.
Just depends on on how
important those docs
are to your project.
Alright so what's missing?
So API fuzzing is one that's not here,
API fuzzing really can't
be done in a CI environment
unless you're doing it
like say weekly or monthly.
API fuzzing takes a very long
time to run like days, weeks.
I've seen fuzzers that ran for two months
and then a minute later a failure is found
so it just depends.
There's also some other tools
that are starting to show up.
Things like finding
includes that are messed up
like you have too many or not
enough for that kind of thing.
We don't have that integrated,
into the hello world,
might not be a bad idea.
The other thing is is that git diff check
looks for trailing white space
and the new lines at the end of the file
but we've all done it,
we've checked in code
that had say like two
lines between two functions
somewhere in the middle of a file
instead of one line or something like that
so a tool that can really find
just excessive white space
throughout the code would be useful.
Might be like a cool said
script, write or something
I don't know, I'm not
good with the UNIX foo
and then other things could
be like comment styles
and camel case versus snack case.
All those kind of things once again
would be helpful 'cause they're all things
that would be great
that project maintainer
shouldn't have to actually write a comment
about a tool, should be able to tell you.
Other tools you might find useful
are things like Hipomocks for
mocking with the unit tests.
Unit tests libraries like say
catch.hpp and then the GSL.
If you want to try and
get C++ Core Guidelines
you're gonna need some
sort of support library.
Microsoft's is a great place to start
and then i will make a comment
for any committee members
that are watching this presentation.
It would be fantastic if
we had first-class support
for unit testing and mocking within C++.
If you've ever used these tools
it's very obvious that we don't.
A good example is if you
want to use a mocking library
you pretty much are required
to mark everything virtual.
Well that's horrible.
There's some macro tricks you
can play to get around this
which we have in our our
CI hello world project
where basically when
you're compiling for tests,
you have a macro that's uppercase virtual
and that compiles to
nothing under production
but it compiles to virtual under test
so that's one way to get around it.
That's probably the best
way I've seen but it's ugly
and then you know there's like
10 million different unit test
libraries that are out there,
catch.hpp is the best
one I've used so far.
It's a nice header only
implementation just like Hippomocks
but it's all macro based
and so if you're doing like
lambdas or commas and
stuff inside your checks
you get compile errors and your checks
or your your unit tests
end up just looking ugly.
Boy it'd be great if we
just had like non macro
base unit test libraries that
were part of the C++ language.
We spend so much time as a community
worrying about type safety and the core
C++ guidelines and all this stuff
because we claim that
that we have a, you know,
the most safe language to use but then
we don't have native
support for actually proving
that this stuff actually
works with unit testing
and mocking so I think it's
something we as a community
should probably put
more pressure on and say
let's finally get this done.
And on that note are there any questions?
(audience applause)
- [Audience Member] Hi, thank you,
this was a great presentation.
- Thank you.
- [Audience Member] I was
noticing in the the Travis CI,
you end up running Cmake and make
just to accomplish each
of the individual steps.
- Correct
- [Audience Member] Can you
comment on why you do it
that way as opposed to
doing all your checks
within the same Travis instance?
- Yes so there's a,
okay so the question is why
don't I run all of my checks
within one Travis instance
versus what I'm doing
in the example where I
do it one block at a time
and there's actually two
answers to your question.
The first one is that because
I have different tests
that I'm running, I have
to run CMake different ways
so for example I, with
the Google Sanitizers
I have to turn on one
sanitizer versus another
or I might be turning Clang Tidy on
or format or whatever
so that's one reason.
The second reason is that if
I run them in different tests
than I can leverage
the fact that Travis CI
will let me run five
servers for free at a time
and so for every single
check I can basically do,
you know five tests
simultaneously each time.
If you happen to pay
for their service then
you'll be able to run
them all simultaneously
so you can see that with Microsoft's GSL
you'll get their results
back in about three minutes
and that's all eight or 10 checks they do.
- [Audience Member] Kind
of a related question.
When exactly do you run all the CI?
Do you do it on every pull request
before it goes into master?
- That's a great question
so the question is
when do you run the CI
tests and so the answer is
all the above so there was a part
with the demo I was gonna show
but unfortunately
there's not a lot of time
but you're more than welcome to go
to the project and you can see
where we do this and you can
actually see how it works
but the answer is if you
can get it integrated
into your PR process then that's ideal
because that means that you don't have
to click merge before all the tests pass
and you can be rest assured that the code
you're merging into master
or your dead branch,
whatever you're doing,
actually is stable and reliable
and passes all your tests.
But sometimes like for example
if you're not using git,
git has nice hooks to be able
to integrate these things in.
If you don't happen to be using that
or you know maybe the tests
take too long to build
like Travis CI only has an hour
long or maybe two hours now
test cycle so that there might
be some constraints there.
Then what you can do is you
can do on say a nightly basis
or a weekly basis or whatever.
Continuous integration doesn't say it has
to be done at this specific time.
It just says that the idea
is you should be trying to
test as often as you possibly can
so that way you're identifying bugs
and integration issues right up front
quickly and you're fixing
them periodically over time.
- [Audience Member] Thank you.
- You're welcome.
- [Audience Member] Hi, thank
you for the presentation.
My question is about the
fuzzer and API fuzzer.
What tools are you using,
why are they running so long
and there isn't a recommendation
in Syria, thank you
- That's a great question so
why aren't we using API fuzzing?
So there are actually
different types of fuzzers,
depends on your project so for example
if you like IncludeOS and you want to fuzz
the network stack, you
gotta have to give it
lots of external packets
but maybe you're like
our hypervisor and you just have APIs
and you want to just fuzz the inputs
to those APIs to ensure that it can handle
all different types of inputs.
The problem with all of
these types of environments
is that they generally tend
to just throw random crap
at the APIs and so for the most part
if you've used these tools before
and I can't remember the
name, we don't use them a lot
so i can't remember the
name of the one that
we were using for a little while there
but it was, it was okay.
If you've used them, what
you'll notice is that
your code only ends up
being really exercised
by the first couple checks
because the large majority
of the junk that these
fuzzers are going to send
to your code is going to
be picked up really quickly
and you gotta wait until
like the randomizer
eventually hits a test case
where it can break through
all your checks and find that one spot
that you didn't do so it,
depending on how big your code is
and how many different
iterations it's gotta go through,
it could take very, very, very long time.
Now that being said I
know there's work to try
and integrate artificial
intelligence into API fuzzers
to be able to provide more useful fuzzing
so you're not just throwing
random stuff at it.
Once again I don't know
the progress of that
but you know if that if
there's good progress
being made someday then yeah
we can integrate these in
and it could be very helpful.
Thank you.
- [Audience Member] So a question about
check-in and formatting.
What's your opinion of this
when you check-in if
there's a formatting issue
like white space does change it as part
of the check-in versus this
warn the user to change it.
- Yes so we've actually seen that
and Neil from Microsoft
had a great answer to this
which I'm just gonna steal
which is that if you
integrate it into git itself
then everybody is forced to do that
and then there are some
cases where you might
want to click merge even if a test fails
so by doing it through CI you're forcing
or you're allowing the option
to allow a test to fail
just so you can get code in.
The other reason is that like for example
with styling if you know in the case,
of once again with the GSL
they use Clang Format
they don't use Astyle
and Clang Format's gonna be different
on every different dev
box and so if you want
to be able to check code
in and you make it part
of the actual git patch process
then they're all going to have to use
that exact same version of
Clang Format to get through that
so that's sort of dictating a development
environment and from their perspective,
they're, you know it's Microsoft
they're using Windows it's painful so
it's better to do it in the CI environment
because it's one consistent
environment doing the checks
and it's, you always
have the nuclear option
of clicking merge
regardless of what happened.
- [Audience Member] So in your example,
how many times do you
reckon compile your project
and other methods to
test the exact binaries
that will be shipped.
- Okay so the question is how many times
do I test the the hypervisor I'm assuming.
So we use the continuous
integration on each PR
so that's checking unit tests
and some functional tests,
mostly functional requirement testing.
So that's being done in every PR
and you can go to the site,
you can see everything
that I described here
we're actually doing.
We also have a Jenkins server
since it's a hypervisor,
we can't do the actual integration
and more complete functional
testing on Travis CI
because I'm not gonna let you
run in a nested environment
since Travis CI is already
running in a virtual environment
so to solve that we have a Jenkins server
that runs right now virtual
instances using VMware
where we can use nested virtualization
but then we also have physical
machines that do that too.
Those run nightly and they actually go
and run the actual code the
way the user would on their box
and so for example we
have to have a box set up
for Windows 8, Windows 10,
Ubuntu 16.10, all in yards.
It fires off each test every night
and makes sure that every
one of our environments,
everything that needs to
be tested is being tested
and so and every single
time we run into a bug
we find some way to integrate
it into our unit tests
or functional tests or even
integration tests so that we
never have to manually
track down that bug again.
It's always in our
infrastructure so it costs money,
it takes time and you're
gonna have to convince
your manager that yeah I can write
this piece of code in one day.
It's gonna take me three days to put
the testing infrastructure
in place to do this
but the reality is is that once you have
an infrastructure in
place you're gonna spend
less time as a developer
constantly reiterating
over the same bugs because you have
an automated framework to fix it.
- [Audience Member] So
for each test for instance
to run the sanitizer, to run Valgrind,
you recompile your
project for those tests.
- Yes, yep, every single time.
- [Audience Member] Thank you.
- You're welcome.
- [Audience Member] About the formatting,
would it be horrible to
just allow the formatter
to reformat the code and then
actually make that the patch?
- Yes so we could do that.
It can be kind of
complicated to integrate that
into the CI process because
what you end up having to do
is to say oh the you
know, so say for example,
Travis CI detects that there's
a problem with formatting,
it would then have to go
and check out the patch,
reformat it and check it
back in on behalf of the user
and it's kind of, you can
do it, it's kind of ugly
versus basically just
yelling at the person
saying oh by the way we have
a tool called make format
that'll fix this, use
it as you're developing.
- [Audience Member] Thank you.
- Any other questions?
Well I really appreciate your time.
thank you for seeing my presentation
(audience applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>