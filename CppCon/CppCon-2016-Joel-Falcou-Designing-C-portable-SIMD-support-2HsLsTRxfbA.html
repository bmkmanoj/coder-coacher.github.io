<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Joel Falcou “Designing C++ portable SIMD support&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Joel Falcou “Designing C++ portable SIMD support&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Joel Falcou “Designing C++ portable SIMD support&quot;</b></h2><h5 class="post__date">2016-10-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2HsLsTRxfbA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so thank you for being here I'm
George for ku I was thinking about
similar programming and how to actually
design meaningful C++ libraries that
deal with this kind of of programming
paradigms so I'm walking at them scale
I'm actually the CEO we have a small
French startup which which lies in to
bringing C++ library for performance to
to users into the objectives of actually
simplifying the use of modern out where
we are also pretty much involved into
C++ to ISO committee and our
contribution to different offenses
project and other community like
activities so let's think about what we
want to speak in there which is what is
Cindy programming so let's start with a
simple amount of questions so who
actually deals with performance issues
as a daily you know constraint in job
jobs around there okay pretty much
everybody cool so so we know about very
programming in this subset of people
pretty much okay and now that's let's
try to focus a lot we knows about Cindy
programming already ok so I probably
have to be careful what I will be saying
and so we actually use something like
like this kind of stuff in jobs ok and
actually was nightmare because of all
those yeah ok fine so let's so for for
some few people that are probably not
family I was in Froemming
it's seven actual small metaphor about
what we are dealing with so let's say
you have some amount of data to process
ok and what you have first is a single
simple no more CPU ok nothing fancy so
whenever you you you process
process the data through the CPU every
cycle you get the results okay so you
get some amount of cycles before getting
your results so it can be quite long
especially if the the processing is
rather regular could be cool if we could
actually process more of the same data
you know be faster so the idea is to get
a different version of your regular
operation that we would call a sim be
enable CPU okay it's a bit different it
looks like the first one that it's a bit
bigger probably a bit more cumbersome to
end all but every cycles well you get
more results so basically if you want to
get back to something a bit more
technical similar instruction set inside
CPUs are actually dedicated systems
inside the CPU that will process what we
call wide register that can store more
than a single value at a time and so we
get processed by a single instruction
every cycle so every data in the wide
register will be processed with the same
instructions and we end up with the same
amount of outputs in the end and usually
those special registers are processed
using special instructions that may or
may not have a human readable name most
of them don't and there is a kind of
general process I'm using the blitter
terms which is something about going
back into mm by what his name already
some guy from active AK which means that
you have data coming in in a regular way
you will apply this regular pattern of
instruction set on all your data and
same way in getting results so the idea
is as we process more than one data per
register per cycles we expect somehow to
get a speed-up which should be roughly
related to the width of the vector so we
want the speed of something that looks
like n if everything is perfect and
correct and whatnot and these additional
performance that you can actually
tract from basically every processor out
there may bring some additional times 2
times 4 or times 10 that you need to get
your application up to the correct speed
you need ok just having to resorting to
scaling out or getting more calls or
whatever and by using this specialized
instruction set you all we also be able
to maximize flops for what metrics which
is in some cases a very interesting of
the optimization so we are dealing with
some adware specific systems that
actually speed up your code by
processing more data per cycles and the
regular one sounds like what the GPU
does but it's not quite GPU that's
inside your processor and if I want to
go back to this very funny culinary
metaphors well if it's a si in the
expression set in your processor is this
very large knife well a GPU is more like
a huge potato slicer with a very thin
grid okay
this big and you cannot exploit all the
processes a power processing of your GPU
unless you have a perfectly sized
boleros okay
so similar computation is from data per
isms that also have an impact on oh you
can write low latency algorithms it's
not restricted to huge amount of data as
processing so the question is although
we actually what is actually available
that's that's a very small subset of
what's probably available right now we
have I don't know many x86 based
instruction set right now ranging from
the vendor it was a very very old MMX
64-bit systems up to the avx-512 most
recent 500 something bits extension set
with a lot of intermediate variants more
or less codependent or complimentary
across SSE 2 and ATX and so on that's a
bit less more a bit less such
institutions on PowerPC with mostly z vm
x vs x
systems of qpx that was the 512 bits
wide register for routing hue and of
course we have seen extrusion set into
armed mostly true neon and the 64 sis
room with versions of the ARM processors
that bring us double n 64 bit 64-bit
integers to the to the to the mix so I
guess that most people actually we
actually know about this actually focus
on on a subset of all of this the most
classical usage that we saw in the wire
these people using the SSE to to try to
speed up some codes some try to dabble
into using a BX well I don't know how
many people still try to use ActiveX
systems but probably a bunch in the
white somewhere and another very popular
one is obviously all the mobile
application using the neon insertion
sets so it's quite problematic if you
really really want to have an
application that goes noth-nothing
everywhere on this list but on on the
subset which is meaningful without
having to redo your code every times so
how can we actually take advantage of
those senior instruction sets so it's a
classical way of fitting this is to see
on one side what we call implicit
vectorization will be basically relying
on the compilers cap abilities of
finding subsets of code which are
candidate to vectorization and on the
other side explicit vectorization
through language extensions or libraries
and so on using the vector in 26 we get
to that at some point or li9 assembly
and just because if not probably won't
fit into one hour and i don't really
want to dabble into that let's all agree
that we say no to in my assembly anyway
yeah I got only so much sanity left so
okay so implicit vectorization well the
point is to have the compiler analyzes
whatever your loop nest is or random
code is you can actually try and F the
compilers by using ins that could be key
words like restrict we go back on that a
bit later or or pragmas or combination
of those that will able to compile you
know that yeah yeah yeah you should
really take into account that this loop
nests or this function is maybe
something you can vectorize but the
compiler is always as always have the
final word for that so if you decide
that the transformation is not safe in
terms of results or also constraints it
won't do anything so you have this rest
with keywords which is a way to test the
compilers that you put your right ends
up and you swear that this pointer a and
this point to be don't is so they have
no overlapping zone of data between a
and B so it's a compiler is free to
optimize as much as you can about the
access the memory access to these
pointers and this fragment is is an
inter specific program I would say yes
this is the there is dependence there
which are actually regular dependencies
in in vectorizable loop so you can
actually try to vectorize so the
compiler can find this int run a very
complex set of compile time analysis
that could be very simple up to stuff
like for your models that will turn your
loop nests into a multi dimensional
representation of your looping into a
polling or polygons and do fancy math
stuff on it and then generate the
correct code afterwards but at the end
if Z the compiler things that you
transforming this code into a vector
code is and say for some reason C's
won't do it this actually gets great
results in most of the to your cases and
some non that trivial cases around but
the problem is that you are tied to the
quality of your compilers the quality of
its let's say it's a vectorizable
patterns
and the fact that you really really
don't think that you you should not be
doing this so when it works it works
quite well and a large bunch of code can
actually be automatically vectorize
using this kind of stuff but when it
does not you're in the blue so on the
other side of the spectrum we speak
about explicit vectorization one of this
is for example what you can do with open
mp4 which is the latest open MP
standards
the idea is to the same way you flag
loops to misread them you can flag loop
to say a this loop you know I really
want you to make a CD version of that
operating before also supports
reductions like in this example you can
flag function as being something that if
you call it later you can also get there
and vectorize what's inside and so on
and so on but contrary to auto
vectorization you are in charge of
applying those fragments where it makes
sense so if you slap a primer OpenMP SMD
somewhere and to loop make no sense to
be vector right so compiler will
probably generate codes that make no
sense either okay and then again when
just when you are actually be able to
use that get a good quick vege of
performances on a subset of actually
interesting cases there is a lot of
stuff going on because you can actually
specify what kind of vector with you
want on a given a given loop nest
what kind of dependency you want to
carry on and so on and so on so if you
want you can actually do inside the year
of an imperial documentation there is a
lot of things you can do with this kind
of pragmas so that's another way to do
it
then again you need an opening before
compiling compilers and you will
basically gain at mercy of what the
compiler is about to do and the quality
of its implementation people also try to
wrap similar computation as a library
most systems provides intrinsics and
special types that maps to the Reg
assignee registers and the way they have
to be handled and you can actually just
wrap that into a properly named strap
and you spread overloading and so on and
so on and you can actually try to wrap
some very specific cindy items within
algorithms or other adapters and
abstractions and it's actually a flow
because you can actually improve
portability of cross compilers because
at the end you would just call whatever
is the lowest way of lowest level way of
accessing those registers there is a
bunch of them I'm just signing a bunch
there is the Agnes works x86 library you
have DC by magistrates Nova by King
Benjamin's GCD which is an effort made
by IBM and sign which is more or less
cindy library with gusoff focus on data
structures that's fancy
array of structure of RA adapters which
is quite cool and we want to see you
shall we be speaking about so and then
you can use in trenches okay sometimes
it's okay I mean this is how you make up
32-bit integer multiplication using neon
okay so you have two flavors of neon you
can use small and less small registers
basically by putting a queue or not okay
so I see we have two vectors and I just
remove them okay fine or you can do
something that looks like this using the
SEC for so the naming is different but
you still kinda soft guess what's going
on and well sometimes you know you want
this road and everything is fine and you
just get out of the road so so I'm a bit
pushing the examples there but because
normally if you use GCC you could
actually just use the multiplication
operators and you don't have to write
this kind of crap but just for the
example yes that's exactly what it looks
like it's the multiplication between two
32-bit integers okay I guess everybody
saw the algorithm which is used there
it's perfectly clear
well back in the day I had to write it
by n it took me like I don't know
probably a week or so to get it working
right but whatever so well and it's no
better if we go back to the good old
active AK it's a bit less country but
it's still a bunch of co2 right yeah so
I'm actually very pushing the envelope
there most of the time it's not that
that could be and you don't really want
to a piece of code like that in the
middle of nowhere in your code base
especially if you if you have to
maintain you know some kind of virginity
in in the complexity of your code base
you may are that you yeah we can slap
everything into a macro and call it a
day
yeah it would be okay for a bunch of the
end that quickly starts to be a big
cumbersome there's two cases where it's
actually able to go at zap them all but
normally you should not okay so yeah so
and as I say I won't speak about III
nessam Lee for a lot of reasons there's
another bunch of of tools you can use
the one I didn't speak about because
Nicaragua is mo is making making to talk
about is PC when tomorrow yeah so I knew
it would be probably better than me at
explaining all the cool things that is
PC can do so in a nutshell what we have
at one side is we can actually rely on
the compiler being able to implicitly
vectorize your code and sometimes I will
actually probably make an example later
where you can actually see that it's
getting good bunch of performance NPCs
vectorization is only as good as the
effort that has been put into the
compilers but they are getting there for
most of the trigger cases you see lab
issue is having non line function called
in the middle of a vectorized loop you
have a limited support for outer loop
vectorization it can be something which
is useful sometimes but when it works it
was okay and on the other side the
explicit
simply vectorization so you have to do
the walk of this loop actually victories
whatnot you don't have to need data
dependencies analyzes to figure out
what's going on
you can vectorize 99 function in some
cases and if you want to put some
reports you can actually also vectorize
outer loops and it's a bit more portable
across compiler in terms of performance
and I mean when something compiled with
an explicit vectorization scheme yeah
pretty much knows that something gets
vectorize relying on giotto
vectorization maybe to cause that may or
may not be vectorized for some reasons
so there was a funny old story at the
very beginning of october co-writer in
GCC where someone was actually
complaining because he has a loop which
was basically a file called b or 5 times
3 dot whatever of double they get
vectorizing 10 10 lines later in this
code he has something like a of by equal
a fi called c or 5 times 3 without the
dot and the compiler was saying ok you
multi-plane your double by an integer I
don't want to vectorize that and people
were stretching it for weeks until
someone say a what about putting your
doubles there but well now we are
hopefully far from that I mean I see C
for example as probably the most
impressive list of vector visible
patterns it's a very very nice piece of
compilers that do a very good but
sometimes you're stuck into a places
where the competitors are going to
vectorize and should totally who want to
write into in 6 or whatever and so we
need something to go further than that
so what we decided to do a bunch of
years now yeah big bunch of years
actually I was trying to find a way to
have a modern take on those let's make a
libraries that wrap intrinsic and so on
and so on we tried to do this get
somewhere some years ago and the state
of where we ended up was not that good
so we decided to scratch everything and
we think about the problem and we
finally found a better way to to
this is library and what we did actually
was yeah build this simply problem in
libraries that we called BC India which
is one of our closer to software which
is basically one of these explicit in
the library walks a bunch of
architectures everywhere x86 PVC arm and
we have some domina specific algorithms
which are pre pre optimized and what we
decide to do is actually take a bunch of
this closed source software and turn it
into an open source library which is now
Bruce Indy which and the open source
version is basically limited to its 86
and power and a bunch of standard like
algorithms so you can actually play with
that and try to see what you can do so I
will spend a bunch of time trying to
demonstrate what we can do with that and
we dig into the implementation tricks a
bit later to see how we can actually
manage to get there so what we decided
to do is to have a very simple
abstraction which is the what we call
the pack which is an abstraction around
redditor wide registers so basically
it's a regular value type you can use
like every other standard standard types
it drops a block of n elements of type T
which are continuous and depending on
TNN the storage of this pack of data
would be different and if you don't
really know what to put for n you can
just submit it and at compile time we
will pick up the optimal width of your
pack depending on your architectures so
T as currently needs to be a fundamental
types of integrals double what not
everything X a bull we have a special
tab which is called logic Hall that we
used to end up boolean values I would
get a bit over that why we need a
special type for boolean navigator and n
must be a power of two could be one not
very useful for the wide register
containing one element but it's actually
required for I mean your absence of the
of the
code and so what happens depending on
TNN well let's say we have C which is a
current add we're optimal register size
for a value of type T so let's say you
are on the SEC machine you are dealing
with float so C is equal to 4 so if you
if you make a pack of this size we just
directly use the natural register types
if it's bigger we will store an
aggregate of 2 packs of the same type
but ours is the size so the size may be
too big for the regular register and
we'll keep doing this aggregate of 2
down F we end up at this point and if
with your request is smaller than C
currently we fall back to an array of
scale value which have been proved to be
not that optimal so that we'd probably
be changing the next one is to use an
active register and use masking
operation whenever needed so basically
you easily Rob's a native register you
want or you will have a succession of
let's say intermediate presentation that
will end up aggregating the proper
number of native register having a wave
which is greater than the natural
Cardinal could be actually useful on
some systems because basically whenever
you do operation of large uns and
require required PAC the side effect of
doing this is basically we locally
enroll every operation for every
supplement of the pack which can
actually be useful to get some
performance back in some cases so but
when we choose Y where he's going on
depending on TNN we got something which
is as close as possible to the optimal
storage we need we take care of
arraignment and so on of course so
everything is is set up as it needs to
be so what can we do with a PAC well
first maybe we can try to see if we can
put some data into it we have a bunch of
constructors the classical Splatt
constructors that will fill the vectors
with n times the same value or you can
actually
calibration shall is it with a list of
scalars you can pass it a pointer and it
will perform a nonlinear load from these
memory addresses or you can pass a range
and it could be an actual arbitrary
range not necessarily a continuous one
and we use tag dispatching inside on the
generator category to know the best way
to actually fit our vectors with this
range so if we find out it's actually a
random access rather we do the proper
numbers of axis same for the other ones
whenever the c++ 17 continuous generator
concept would be in we probably have
something to say whenever you pass a
range of continuous integral integrator
you jump back to actually calling a load
version of the compilers why do we need
that actually we had a front side effect
with some of our users that wanted to
vectorize code that was processing data
from a steed list don't ask me why
okay so I add the list of whatever
double I think or something and they
wanted to vectorize whatever operations
that were doing on this list and well
the best way we find out was to actually
being about say okay give me three
raters over your list just load it into
a pack and do whatever competition you
need you also we also have explicit load
functions that can take a pointer and
enough set and we perform an N line or a
line load just as its memory pointers
with an offset the offset itself can be
a pack of integers and in this case when
we just perform a gazer so we pick up
data from different points in memory
starting from PTR and gather them up
into the vector and you can use a mask a
glow using the mask PTR type that's an
epitaph that you can actually build from
a pointer and a condition and you will
load only the part from the pointers
where the condition is true and you can
also pass an optional default value when
it's false and so on so it exists for a
load in the line load we have what we
call a misaligned nodes which is
basically aligned
it's a line load of course so is that a
line load with a static upset and that
means that your pointer is actually not
aligned but you know by all match it's
not aligned by one or two elements okay
something like this or minus one and
instead of using an aligned nodes that
could be costly on some system and try
to optimize a way to load this slightly
shifted memory access and we try to be
faster than just calling the airline
line node and same back we have a
sufficient for store again with offset
so we can do a scatter and with the mask
lines and unlined versions so we have
those explicit memory management we
wanted just to be explicit for a bunch
of reasons one is the main one being
that we don't really want to be going
back and forth between the registers and
memory without the users knowing it so
all the memory store and load except for
the web the constructor is something
which is actually explicit in its own
way as to be explicit so you actually
can manage whenever you want to stop
dealing with register and going back to
the memory okay so we put data into the
specs so we support density every
operators on on those facts also
standard language paraders we have a
bunch of extensions so for example you
are about you are able to call bitwise
and or or not on floating point values
because masking directly those floating
point value with a mask is something you
do a lot when you write in a Sindhi code
the only limitation that we don't do
conversion or promotion so if you have a
back of char one which is full of 255
and one full of one if you do the Sun
you end up with a back full of zeros
okay we've got two kind of comparison so
you have the operators that perform sin
decompression so you end up with the
vectors of boolean somehow but the
number of boolean is the result of this
comparison has to be a call to the
number of data you put into the first
one so you cannot just have a pack of
boo because you won't be able to know
all many Bulls you need to store so we
use this logical of
ctype which is basically saying yeah I'm
encapsulating a boolean value that is
the results of an operation on something
containing tea and we go back to the
correct width of the vector using the
tea instead of the logical tea we also
have what we call reductive comparison
so compare equal in kampala that
basically tell you if every elements in
two vectors are equal or if they are
correctly ordered with the with the
lexicographic order so usually you want
to do this in the middle of your
computation and if you want to stop when
if something is actually requiring Abood
you need to use the other ones back is
also a random access friends you can do
beginning hand on it and you will
iterate over the scanner element I'm not
advocating doing it too much cause it's
a bit a bit slow and you can app random
access to the value which we turn the
proxy type to access to the actual
register internal value so you can
actually write code like basically like
package the small array except it's
optimizing in storage in this operation
and you can do whatever whatever kind of
variation you want on that and whenever
it's possible you get the vector it's a
Sindhi version of this operation we also
have a bunch of functions 360 something
that cover all the basics form
arithmetic stuff bitwise operations some
interval I operations a bunch of
predicates we have a large selection of
trigonometric function exponential and
so on which are pretty much all ready to
use we have adventure productions so we
can ask if any or non elements in the
primary for predicates
computers some and so on and we have a
large subset of functions that deal with
what we call the swirl operation which
is manipulating the data inside the
vector including is a very helpful
shuffle function and stuff like group
split and combined slicer can slice or
aggregate vectors of a given size
to give you a bigger or smaller vectors
with keeping or not the output with so
combining slides basically you take a
pack of tea and you get a 2-pack of tea
and over 2 and we split what happens is
you have a pack of TN and you end up
with two pack of u n over 2 where u is
the type which is slightly bigger than
the one you had before so you go from a
pack of sixteen char to two packs of
eight shots so that's something you can
use to do casting and stuff oh yeah and
we have what we call splatter traditions
which is basically computing a reduction
but instead of giving you a scare you
get the same value everywhere in the
vector which can be able to optimize a
bunch of cases so what I say this is a
very funny operation is a shuffling
operation so reordering data inside the
pack or inside the register in Sindhi
mode is very useful because you can turn
your memory access and so your memory
bound code into something which is
actually turning to commutation somehow
so you can actually write Sindhi
transposition conversion between formats
and so on which is very useful so we
have support for what because the
specific permutation that you usually
map to a direct intrinsic so you can
flip your vector you can broadcast value
inside another one you can interleave
and dysentery values you can repeater
paths you can slide the value inside and
you can do what I call runtime new crap
so you can have a vector and vector of
index and you will at runtime we index
the vector but the most fun one is
actually the shuffle functions that
support arbitrary permutation and they
get optimally generated if the
permutation is known that compile time
so when that was a function that took
exists so let's say you have two packs
okay a and B and you can write stuff
like this so I want to shuffle a using
these permutation 0 0 3 3 which
basically say I'm returning a vector
that contain twice the first element of
a and twice the last one
for in-depth with one one four four you
can use a special value minus one to say
that yeah I want to zeros there so some
silly extension set has a special
intrinsic for this so I'm done so we
just wrap everything there and the
optimal code is generated so I'm taking
the first element of the a of a I put to
zero and I take the element 4 which is
basically the first element of B which
is 10 so you can actually put whatever
permutation you want there there is one
limitation of these syntax is that the
permutation is violet list of indicators
so you have to know how many elements
your pack contains which is good for
small toy example she's probably not
cool enough for actual code where you
want to write code which is independent
of your pack with so we provide two
other way to do this so the first one is
to actually write a meter function that
takes integral constants and return the
computation on this integral constant so
the first one I is the index of the
permutation you are computing right now
and C is the width of your vector so
this is basically reverse whenever you
got I and C I'm giving you C minus I
minus one and you can pass this type
directly as parameter to shuffle and you
get your chauffer god it's bit better
don't have to care about the size of
your of your shuffling it's still a bit
cumbersome because you have to write
this kind of non natural or native
functions so now the way to do this is
use a context where function starts to
be a bit better so you write your own
permutation function like like an actual
function and you just have to wrap it
into these pattern types and pass it to
shuffle and we basically do what say
compute whatever
indexes from there and shove it so why
do we need a pattern there you have to
send Visual Studio to that as I say we
should forth yeah
sorry II James anywhere no it's okay
now okay so as I say we support sighs
one pack even if it's loops range and
the problem is if you write shuffle of
zero a visual studio is confuse because
zero can be an integer or it can be the
new a new PTR conversion to a pointer to
this function so it doesn't know which
one to take
also a compiler speaks of correct one
which is picking the integral so we just
rub the things and everybody is happy so
what happens when we do is shuffle well
depending on the patterns you pass there
and your hardware we can recognize a
bunch of fermentations that actually
maps to something which is better than
just columns or whatever basic shuffle
functions so if you happens to actually
do this okay and you are on the system
where you have an interesting to flip
your your back we will recognize as a
reverse and you recall the correct in 26
if you if we fall through all the cracks
I've tried to recognize special patterns
then we call the basic shuffling
function and if we fall in cases where
it does not exist was the type you want
or for the size you want with just to
the permutation by n element by element
but you could actually use shuffle
everywhere and we map it to the correct
intrinsic
just beyond your back at compile time
okay so we also have this standard
library integration so we have a bunch
of algorithm transform reduce that can
take the choose generic frontal or
lambda to process both the scanner and
the same decide the size of the data we
have a locator so you can actually
allocate memory use a proper alignment
constraint and so on and we have a bunch
of range that take a range of contiguous
memory and turn it into something that
can be used directly to get access to
the data in a Sindhi way so the input
and onion are input range basically turn
a pair of pointers describe a range into
something that when you iterate over it
so they're referencing of this iterator
from coming from these frenchers give
you the part that was loaded from the
correct
memory port
location and then we have the segmented
range which actually take an arbitrary
range of continuous memory and it return
you a couple of treatments the first one
is a range from the scalar value that
you need to process between the stuff of
the data and the first locations of data
where vectorization is possible with
respect to size and alignment the second
part of the top wall is a range that
which is basically an ally in your range
or where all the data that can be
vectorized and the last part is whatever
scale of values is left after the end of
the last point you can vectorize and so
you can just take your pointers get into
a segmented range and you get your top
holes and you can decide what to do on
each of the sections so you can use the
algorithm exist so we use a polymorphic
lambdas there just to do these P times 2
over overall elements in this so
transform take care of knowing where to
call the lambda and scalar or Cindy mode
and those are the prologue and epilogue
and so on and you can do the same with
reduce so you have the simple reduce
like this whereas basically computing
the sum in cindy mode and sometimes you
need to pass a more complex operation to
the reductions so for example we compute
the sum of the squared value and in this
case sometimes you need to pass
additional information you need to
specify the identity element of your
operations which may be different from
the initial value and you need to pass a
function which is actually the function
we use to complete the reduction
obviously you cannot call a plus y
squared at the end of the reduction
because if you do this you square some
sub result one time to match so we need
to use plus bonus point every function
from Bruce Cheaney is actually a
function object and actually a
polymorphic function object so you can
just show them directly into algorithm
languages ok so how does all of this
work so
we take a bunch of time actually looking
at some details of the implementation
that I think are actually interesting on
ours in C++ 11 in 14 at the desk so
let's go back to to some piece of code
so as I said we we implement our
function actually as a function object
except for the very obvious template
function that needs to be template and
for doing so we use an internal library
which is called boost dispatch which is
a thin layer that was building function
object using tag dispatching inside so
we use that dispatching to select best
optimal optimized version of function
depending on the function argument types
classical tag dispatching stuff we also
pass a category that represents the
current out where specificity is so we
know at some point time that we are
compiling for something that has a
fundamental behavior like SSC or like a
BX or whatever and we pass another
information which is the fact that we
categorize the function itself so we
know that plus is an element-wise
function so we can use it in place where
you want to have a data per element
operation you know that sum is a
reduction such as plus as is binary
operator and 0 as is as is identity
element and so on and so on and this
defines the entry point to the function
and whenever you want we want to add a
specialization for a given architectures
or given bunch of types well we we have
a systems at best if you say ok this is
an overload 4 plus I'm walking on the
machine that supports some things that
looks like SEC 1 and my two parameters
are packs of single precision values
that are stored using the SSC
representation of vector registers and
well when I know that I know that I can
just call mmm at plus PS and Dom
and saying for if we go to I know what
is that a DX to so if I'm a navy x2
I got to in 64 elements coming in and
they are represented as a DX register
then we do this so it looks like the
redundant information to know which
architectures I'm compiling for and
which representation of register I use
into the pack in fact it's not because
you have a lot of senior extension sets
that as agent multiple representation
that's the case for example for neon or
in the case of a B X and avx2
you have a VX extensions that bring you
new operation on the SSE register so we
have some where I know we have a
overload for function on a VX machine
taking sec registers and using the
specialized a DX operation on sec
register and we have a pre build list of
you can see different up different
categories for all the basic types that
we can use to do this and this fact
dispatching I was to be able to find the
grain select what kind of column
generate depending on a lot of
operations and I spoke about the fact
that we we have this information about
the properties of the function so like
plus is an element-wise function some is
a reduction and whatnot and that we use
this information is what we call for
back over loads for example yep so I
said that for example you can write you
have two packs P and Q you can write P
plus Q but you could also write P plus 4
okay but we didn't want it to be able to
force to write every operation with the
scatter on this on one side on the other
side and so on so what we do is we say
whenever we have when we have a whenever
we have an element-wise function
whatever the functions and you call it
with a scalar and the back of whatever
types okay said what I do is I'm taking
the scalar I'm turning it into the
proper pack and I'm calling back myself
on this pair of back afterwards okay Q
Auto datatype black-and-blue do magic
stuff
but yeah basically we do this and you
can do that for I mean we do it for two
for three and so on and so on and so on
but we never care about the active
functions we also have a way to do
something which is a bit more complex
okay whenever you want okay okay well I
was thinking about being a bit more
complex than that so sometimes you end
up one thing to call the function on the
back but your architecture doesn't have
the function but you really really want
to call it so what we do is we're going
to a fallback mode where we say okay I
don't know how to do it using an
intrinsic so we just compute everything
pieces by pieces as a scale of values so
we end up with this elementwise overload
to say if if I ever a benevolent ID
functions that is used on CPU which is
the most highest level architectures
category which means you are just on the
CPU I don't know what to do and you have
a pack of whatever in whatever
quantities well I would just give your
right random job because it's quite a
bit of useful crowd but basically we
check that everybody has a correct size
and depending on the number of
parameters we just end up rebuilding the
pack containing the function applied on
every scale values up to some amount of
argument for me--for so this is way of
categorizing the function properties
help us doing this kind of global
filling up fallback functions so we also
have this yeah category a key things
going on for chuffing chuffing is
actually very complex if you really
really want to capture all the crank and
specialized shuffling so let's start
simple
if you have if you have a whiff of two
and you want to shuffle two vectors of
two and you want to take into account
minus one cases you end up with three as
a power of two shuffle cases that's nine
okay nine cases I can probably write all
of them
and specialize all of them now you get
up on the 4 element chauffeur so they
need to have five different value okay
and if you do a binary shuffle that
means that you have 5 ^ 8 cases it's
already too much I don't even know how
much it is and one of the most
interesting types usually is a bit shot
because that's the largest vector you
can have so you can have 30 to be 32
elements in vector child and a DX and if
you want to shuffle that
let's say it's only reshuffle you get 33
at the power 32 permutation and I'm not
gonna lie all of that okay I don't even
know how much is that then I actually
compute this I mean okay so I say what's
that at the power of this yeah far too
much but if we think about it there was
a lot of cases where we can actually
find an analytical way of finding what
we should be doing because for example
let's say you write a generic suffering
but which is actually in some cases for
back to being the identity pattern so
you show for everything in place 0 1 2 3
4 5 em you don't really want to call
shuffle on that you just want to return
a 0 directly or you if you find a way to
write a show for that happens to be a
broadcast you want to call the broadcast
intrinsic broadcast identity reverse
interleaved is enter leave a lot of this
actually follow a formula you can
actually compute for a given size and
you know if you are in these cases and
so what we did is actually use
metaprogramming to say okay let's
extract you find out if your permutation
is a broadcast it's a broadcast if all
the value of your permutation or the PS
are actually or equal to guesses so what
we do is that we've been an integral
list of PS and you check that everybody
is equal to the first one okay if this
is the case you are broadcast
and if I'm a broadcast well I know that
if I want to broadcast something I just
need to take my vector and close a
broadcast function with a proper index
and what we do is whenever we find out
that your pattern your key is actually
broadcast okay we find out which index
you need to broadcast over okay and when
we have this we'll just be going into
this broadcast shuffle cases where we do
the proper things and we do that for as
I say broadcast identity enter the
repeating reverse and sliding
that's all known to be about to be
written differently than a shuffle okay
so we all of this too much as a power or
far too much permutation we remove the
bunch okay but you see we still have
this a big Burke of them and in this
case what we use is what we call a
computer because the topology of the
shuffling patterns so topology of the
shopping patterns takes your patterns or
fermentation and try to know if it fits
into the constraint of your other
shuffle function which means that for
example I don't know I said what is
somebody there familiar with the sse2
shuffle functions so the shuffle
function in sse2
you take two vectors and you have to
shuffle them but the first part of the
output must come from the first vector
and the second part must come from the
second vector so what we do for example
in checking the topology of a
permutation if it's compatible with sse2
we want to make a function to say okay
I'm looking at your patterns and do you
fit into this and if the answer is yes
this is the value you need to pass
through your shuffle function basically
looks like something like this so yeah
so we have a four Prime into this
permutation and that's a we do the we
compute the fact that am I am I'm
talking is my permutation index
ordered correctly or are they older but
the n'zeer way around means that instead
of shuffling a and B I will be shuffling
BN a and if it is the case I will give
you this information about what to do
though you have a directly mutation or
you have an indirect one or you have a
regular patterns and what we do into the
shuffle where is it
it's there we say okay where is it you
know telling me you have a binary
shuffling to do
give me the topology of this shuffling
and as there is a direct one so we just
called mm so for with the proper mask or
if it's indirect I'm just called shuffle
with value family lead and the mask and
if not we fall back to the default
matcher so we use those also many
programming tricks using various
templates - OH - spread out all this
stuff is done and in the end we'll just
go there quite quite fast we end up with
something that we know generated proper
infant set a less funny stuff it just
before I go to something more
interesting is all we write reduced
using segmented iterator you want to
process a range of data in an assembly
way and you want to take care of the
prologue and the epilogue and so on and
so on well that's five line we start by
putting a zero in the back with segments
of range and you just have three for
range over the three sub topple and so
the first in the last one we deal with
the operation using increasing we're
using scalar and the middle one just use
a CD accumulator to accumulate inside
and we just return the sum of the scalar
plus the sum of whatever is in the back
seven nine I told I told you at the
beginning that this is a first.we right
of the old machine is all recently
reduce we don't have the range stuff and
whatnot was probably around eighty or
ninety lines and the probably get full
of bugs and this this range
faction is actually very interesting for
us because well what happens if we have
another kind of reduce dawn and
transform is a bit more complex this is
basically the same idea we segment the
input we process a bit and we know
what's going on so let's have a look
quite fast
I normally I did my homework yes I'm
just taking five minutes to show you
that so where is it
okay far away far away okay so let's
have a look at some look at where is it
shuffle so we all see shuffle where is
it test so basically we fill out some
vectors and we would call some shopping
on them the ID broadcast interleave and
some random shuffle even the name is
random actually so let's have a look at
our wits end up okay so well we filled
up we fill out the vectors this is a
place where basically is the first
shuffle which is actually no shuffle at
all so we just move a PS stuff at some
point I do a broadcast so where is it
yeah and the broadcast get in the case
of the floating point we get turned into
a V so for using the broadcast mask 5 5
which is using exact the same stuff and
for the short vector because a bit more
complex but basically shuffle with
pattern and then we should fall back to
the low and I paths so we get that and
then we have an inter leave that just
end up scoring and back directly ok and
the where is it's a random shuffle which
is called shuffled be directly with the
proper constants which is read from
these places so yeah we we are about to
change that and if you look at what
happens for
see we get a different set of entrance
eggs
so probably this is probably better as
it just okay um okay thank you
that's where we have food okay yeah
better so same thing identity shuffle we
zero five five for the broadcast again
and back and a different way to doing
see also shuffle calling the SSE shuffle
instead of zeros one but everything get
down to this okay so we are about to
generate these kind of things so I'm
going back to the slides just going over
a bunch very quickly a bunch of tests we
need of people did actually so I got the
comment that we do some benchmarks but
we've wrote both of the based version
and the busine U version so well I
decided to fetch some benchmark that was
not written by us so should be better so
let's have a look but first let's have a
look at something we wrote ourselves so
I spoke about having you know
Exponential's within a matrix and so on
this is a kind of performance we got so
it's computed the number of cycles to
compute a one value so for the assigned
e stuff if you want the actual latency
you have to multiply that by eight
but in the end whenever I want to
compute the next one n shows the cost
and cycler to compute one exponential in
to my back this might be his seventh
cycle and we spoke about the fact that
our function our object functions that
means that I can pass them as parameter
to other functions and we have a set of
stuff we call decorators like restricted
that you can apply on an object function
from busine D to change its behavior and
respect let's say basically your
function there as different behavior
depending on its input range and I want
you to use the smallest one possible and
if you get outside of these regions just
return am so if you want to compute a
cosiness the regular cosiness okay
you got Z speed okay and if you want to
compute a cosine s and you know it's
between like - speed on over four and
plus P over four can use a restricted
versions which go far faster because it
doesn't have to deal with all the other
code password endings the other values
so we can we can get at this level of
performance on this kind of operation
it's not perfect for some but it's
actually getting there so I obviously
have to have the stupid fractal example
into the slice if not you know like
people may think I'm not doing anything
in parallel so whatever yeah well it
could have been mattress mattress
product too but whatever so computing
this factor is actually funny because
it's largely compute bound so you can
actually see if you are doing things
right or not so the code is actually
rather simple you take your a and B
which is your complex part your your
company's value real and imaginary parts
we need to compute a number of iteration
in rest so we have a bunch of meter
function s is that you would type T and
you turn it into an integer and so we do
the computation and the problem is that
this competition as a why so vectorizing
you need some kind of tricks so what we
do is a user tricks of computing a mask
where all the value we computed and keep
iterating until everybody into the back
conversed masking the ones that already
converged and what we do is that we only
increment the iteration counter s if and
only if mask is true and we have this
earth computation art control function
like if Inc which are conditional
operation using a mask over back so this
is the new code okay funny things every
every Buscemi function also work on
scalar so you can actually take this
very template function and call it on
skater - and you have the scarab version
over that I'm quickly finishing so
instead of running my own benchmark
got a paper from Angus poor from
Timberland last year at 2:00 p.m. VP
that actually compared the bunch of
stuff including us
so you have sicko Sanji CBS HPC open em
PVC and whatnot and we are doing quite
well most of the time the funny thing is
that sometimes we we end up having a
better performance and actually using
the intrinsic by end because we let the
compiler actually change the way one to
allocate a register and whatnot so we
got to speed up off something around
three something on SSE four out of four
and six point nine out of eight on avx2
she's not bad mari Wilensky owns use boo
Cindy was boost on the end to vectorize
the audio resolution of Rossler systems
you can get all the details there
basically the code is largely unchanged
and we get three ton faster so this is
of course that do the stepping into the
audience and upstairs is a scaler called
and downstairs boosting because we're
Baz if you replace double by a pack of
double and we get treat and faster on
these kind of systems using an ATX
machine double precision so conclusion
because I'm already out of time well we
wanted to be able to show that you could
actually write C prosperous that looks
like modern c++ your templates we get
context per dot dot so we checked all
the boxes and we can get decent
performance the code generated is most
of the time
basically what you divide by n the ranch
and the iterators algorithms items are
very useful because you can actually
piggyback them into cindy and get a very
nice looking code so something changed
because i give some similar talk a lot
of time but we have a release like
yesterday it's also a pleasure to have
an actual documentation so everything in
terms is Gita now and as soon as I'm
going out I'm just trying to find Robert
Remy and probably my kick ass and try to
curse them to start the review as soon
as possible so someone on IRC channel of
boost wants make a joke about the fact
that 2016 probably is a year of Linux as
desktop OS and the date we're boosting
is actually released so yes it's done
so you're free to download that test you
get feedback we got a bunch of forums
issue tracker I really appreciate it if
you are dealing with a very specific
Domon of commutation that we do not know
and you've got strange bugs strange
performances and you want it to be
better I also want to thank a lot of
people also the BCD team from them scale
and the bunch of our earliest supporter
and adaptive management surgery at all
which is needed in Busan into PI R and
Mario Ford's walk on do Cindy and boo
study and some of our co-worker and
students that are actually testing boo
see me every day thank you and sorry for
having me over times so many questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>