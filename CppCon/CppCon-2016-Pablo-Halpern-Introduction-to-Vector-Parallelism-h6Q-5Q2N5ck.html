<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2016: Pablo Halpern “Introduction to Vector Parallelism&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2016: Pablo Halpern “Introduction to Vector Parallelism&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2016: Pablo Halpern “Introduction to Vector Parallelism&quot;</b></h2><h5 class="post__date">2016-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h6Q-5Q2N5ck" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay yes some line all right that didn't
work so I'm not happy about it okay all
right so how hot contagion like a
sandwich anybody have any ideas says
over it on this slide it doesn't really
say on the slide why it's like a
sandwich we're getting to that so here's
my analogy you're running a sandwich
shop and some of the orders a couple of
sandwiches so what do you do you can
have one employee make one sandwich then
make the other sandwich you can have one
employee make both sandwiches get two
employees each making a sandwich what's
the correct answer so I'm just gonna
take a quick poll how many people think
it's number one nobody okay number two
two employees a bunch number three one
employee
two sandwiches and a bunch of you is
abstained probably because the actually
correct answer is well it depends and
but this is you know surprisingly a
pretty good analogy at least I think so
for the different kinds of parallel
computation that you can do in a
computer so today so how many here were
at Joe Falco's talk on Monday about the
boost that Cindy nobody Oh tough few
people okay and what about Nicholas and
sorry I don't you know how to pronounce
Nicholas his last name given much talk
yesterday just a couple well alright
there's a little bit of overlap in this
talk but one of the big differences is
that Joel was talking about slicing
Salam bees in parallel and I'm talking
about making sandwiches so really
different subjects both food related you
have to have food in your talk so my
name is Pablo Halpern I'm an Intel
employee engineer I've given talks at
CPP Khan in the past about parallelism
and multi-core parallelism in the past
today I'm going to be talking about
vector parallelism which is actually the
most common parallelism in your computer
and the one that is probably least well
understood
so my goal is for you to understand why
it's important and how to give you a
head start on programming for sim D in
your C++ code this is a kind of a broad
talk it's not real deep dive but I
hopefully will give you enough to get
started I want you to hold your
questions to the end if you can unless
something is really unclear or you
really need to interject because I timed
this talk and it's really just right on
time to leave about five to ten minutes
for questions okay
the last thing that you need to know is
why is sandwich-making like a like
parallel programming vector programming
so what is vector programming well we
start with this notion that we have a
computation and I'm just kind of labeled
the steps ABCD and you have a bunch of
data that you need to process using
these four steps which are labeled 0 1 2
and 3 and we do the steps in parallel
using a single instruction executing in
this case in this diagram four pieces of
data at once so single instruction
multiple data Cindy throughout this talk
I'm going to be using sim D and vector
parallelism more or less as equivalent
terms they're not exactly equivalent in
addition to the short vector sim D which
we find in most modern CPUs there's also
long vector machines like the old CVC
supercomputers and connection machine
super computers they use the thing
called long vector and then there's also
pipelining architectures both in
software and hardware and they all use a
very common programming model so most of
what we talk about here would apply to
those but I'm really mostly talking
about the sim the architectures that are
in the CPUs today
our trunk suppose we have a computation
that's you know where we're trying to
speed up computations and it doesn't
really make it makes a lot of sense it's
speed up a computation on four data
items but if you have a million data
items or hundred million data items
that's when you really want to see some
speed up and of course our sim D
machines even if they can do more than
one thing at a time they don't do a
billion things at a time so we have to
chunk them and the trunking is very
straightforward in this case if assuming
our vector width is 4 that is that is
how many operations we can do at once
with a single instruction or how many
pieces of data sorry we can operate on
with single instruction we do four at a
time all the way through the body of our
loop are our hypothetical loop here and
then we move on to the next four pretty
very straightforward if if somebody was
giving you the sendi machine and said
here we can do these four at a time this
is probably we come up with is how to
get the entire loop done right now
what if it you're not having multiple
four items right well we're gonna get to
that you're gonna have to be patient
because that's kind of close to the end
of the talk but there's a thing called a
remainder loop that takes care of the
last few items that don't fit that are
less than a vectors width what's
happening in the hardware is that we
have our CPU and and we have our
instruction fetch and decode which is
stuff that you probably have heard of
before that that even has been around in
CPUs forever a bunch of scalar registers
memory subsystem but then the thing that
is new and I say new in quotes because
it's been around for quite a long time
like 40 years
are these sim D registers which are
these wide registers that horik hold
more than one data item at a time and
multiple floating-point units and
multiple RF medic and logic units to do
the arithmetic on more than one data
item at a time this is highly simplified
but is the basics of what we what we
care about
if we zoom in on the assembly register
it actually behaves like a whole bunch
of registers at once overlaid on top of
each other so a an SSE for register that
is the instruction set of previous
generations of Intel chips and which is
still supported in all of the new chips
has 127 128 bit Cindi registers and they
hold two doubles or two Long's or four
floats or four inch or eight shorts or
16 chars and of course all the integral
types can be signed or unsigned it
doesn't care so depending on what data
type you're computing on you can have
more parallelism with the shorter data
types and often there's a lot of
computations for which floats have
plenty of precision and so a lot of
times people modify the code to use
floats instead of doubles to really take
advantage of getting that you know 4x
speed up instead of 2x speed up using
sim D in the Navy X which is the new
generation the newer newer generations
are still not the newest the the
registers are 256 bits wide in the
avx-512 they're 512 bits wide so you
just quadruple numbers that we just
looked at it's really convenient to
think of a Sindhi computation as being 4
in this case I'm going to keep using 4
throughout this talk because it was made
it for really easy diagrams for if
you're doing four computations at once
it's clearly convenient to think of them
as four separate streams of computation
on four separate data items and we call
those streams to call those lanes but
that's really just a model because under
the covers it's really important to
remember that lanes don't really exist
what you have is Cindy registers and you
have these things happening in lockstep
and the most important thing that's
different between the Lane view and the
the real physical view is that this Lane
view can give you the missing
depression that you can have independent
computations going on when in fact they
can't they're not independent at all
they have to be in lockstep so because
they have to be in lockstep that causes
a problem called control flow divergence
or control flow divergence is not
necessarily a problem but it is for
Cindy what happens is if you have an if
statement and some of the lanes want to
follow down one path and some of the
lanes want to follow down the other path
if they're in lockstep how does that
work well ideally we have a mastic
execution and I say ideally because
there's a ways in which you can get
around it for older architectures where
we have a mask of bits that say which
lanes are active for this instruction
and which lanes are not active one bit
or a true a true value is for the ones
that are active a false value for the
ones are not active and so we start our
computation we assume that the mask is
set to all ones so the a computation
which as you can see is not inside the
if here takes place on all four lanes
then inside the if we compute I modulo
two in which for the odd numbers is
going to produce a true value and for
the even numbers of false value and
that's going to generate a mask of true
false values which is this zero one zero
one here so for the next instruction the
mask is going to be zero one zero one
and that means that for the then part of
the if statement the B to be instruction
the first and third lanes are masked out
as we say they do not perform the
computation and the in the else we
invert the mask so when the mask is
inverted now the opposite lanes are
active and see the C part takes over
okay and the see I mean say the C
instruction executes only on the first
and third and not on the second and
fourth lanes and then we set the mask to
all true again and execute D in
in all the lanes now I said ideally
because there are some architectures in
fact many architectures in the whole SSE
line that didn't have masked
instructions for all of the instructions
instead what you do is you do the
computations on all of the lanes at once
but you store the results of the been
part and the and the else part in
separate registers and then at the end
there's a blend instruction that says
using this mask take the appropriate
ones from this temporary register and
that temporary register and we combine
them together into a single register and
that's your your end result ok so that's
the way that's the way we did it before
a BX ok and on the Intel machines I'm
not really sure about the you know neon
and altivec and others but I assume that
the newest iterations of those probably
do masked execution so let's go back to
our standards analogy we we want to make
a sandwich efficiently with one person
with two hands now although we have two
hands most of us are not very good at
like doing two different things with our
hands who completely independent tasks
so I'm going to assume that we can't you
know like get arugula from over here and
less the same time from over there
because if I tried to do it I would mess
it up and have to do it three times and
it would take five times as long so so
we have one person now let's first let's
make two identical sandwiches right and
we're making a BLT a bacon lettuce and
tomato sandwich so we spread mayo on
both slices of bread and we add tomato
to both of them and we add bacon to both
of them and we add lettuce to both of
them and then we close them both and
we're done five steps and we've made two
sandwiches
pretty good if you're making only one
sandwich it would take about this long
right because you still to do the same
five steps okay but now we have some
control flow divergence we're making two
sandwiches but they're not exactly
identical one of them doesn't want
mayonnaise the other one wants a rubella
instead of lettuce
so now
we have a choice do we still make both
at the same time and the answer is yeah
I think that's the best way to do it but
where you have to make a little changes
in our procedure again either remember
the two hands can't be completely
independent so we're going to mask out
the steps that should be done for one
sandwich and not the other by saying
okay we're going to spread the mail on
the bread but instead of we're doing it
for both centers we didn't do it for
just the right one and then they're
gonna add tomato to both and then the
bacon to both and then lettuce to the
left one and an arugula to the right one
and then we're gonna close both
sandwiches anybody know the same thing
about the number of steps we used to
have five steps now we have six so the
procedure has gotten a little bit longer
because we've added this if-else that
says we've added arugula into the mix
but the interesting thing is if you were
to do make the sandwiches one at a time
it adds up to nine steps you can do the
math yourself but it's four steps for
one sandwich
and five steps for the other and so it's
it's nine steps so you make one sander
to the time nine steps here we're doing
it in six steps so we still have a 11.5
X speed-up over making the two
sandwiches completely one after the
other but before we had the control flow
divergence we had a 2x speed-up so this
is why the control flight of urgence is
the sandwich makers nightmare and is not
not too fun for Cindy either okay an
important thing to keep in mind so
control flow divergence cuts down on the
amount of parallelism you can get from
Cindy so let's compare Cindy more
directly with other forms of parallelism
in the multi-core world computations on
multiple iterations of a say of a
parallel loop are completely unordered
so they start at different times and
they proceed independently at their own
rate something like this okay we've got
ten iterations and notice that where it
froze the action iteration number five
is ahead of you
number zero and some of the iterations
haven't even started yet probably
because there aren't that many CPUs to
start them all concurrently so what's
going to probably happen is iteration
zero will complete and then the same pea
CPU is going to go ahead and do a
duration number one that's multi-core
and I'm going to use multi-core in
parallel more or less as synonym
synonymous terms even though parallel
could also mean vector parallel or other
kinds of parallel but if I just say if I
just say parallel I'm referring to
multi-core okay now let's compare this
to Cindy let's assume starting with with
two lanes and the iterations from
happened in lockstep the first two
iterations and then the next two
iterations in lockstep and then the next
two iterations in lockstep two at a time
notice that there's a left-to-right
progression kind of like serial and that
this is more predictable in fact it's
it's it's very predictable compared to
multi-core let's look at four lanes same
idea but now it's happening faster
because we're doing four lanes at a time
so we've done four iterations all at
once and then the next four iterations
get started if you look at the shape of
this you see that it kind of forms a
pattern which we call a wavefront vector
combination pattern where the
computation proceeds from upper left to
lower right that is from the first
iteration towards the last iteration and
from the first instruction towards the
last instruction along this rough
diagonal now it's a jagged dag normally
you know in reality but if you smooth
you know you you make the extend it to
infinity you'll get something that looks
more like a diagonal line and that's our
wavefront the computation goes along the
wavefront
so the wider your vector the closer the
horizontal the more the closer it looks
to up the straight from up top the
bottom and the narrower your vector so
that the that degenerate case is scalar
execution will go almost left to right
but you know of course it's still
scanning top to bottom and then left to
right like that
so interesting things to note about that
one is that scalar and vector are very
compatible because scalar you can take
any part of the vector computation and
if your hardware can't handle it can't
to do it as a vector computation the
compiler can just serialize or the
programmer can choose to serialize that
part of the computation and it falls
within the wavefront that the vector is
expecting it's say let's do these four
things at a time these four things at
time then these two things one at a time
one of them one at the time one of the
time and then we go back to vector next
for and it's still following that that
way from why is this important well
there's a very important dependencies
that a lot of loops have certain kinds
of loop carry dependence ease where
you're looking ahead or you're looking
behind at things that should already
have been computed or that you are sure
have not yet been computed when I say
not yet can be computed memory locations
that you must be sure have not yet been
modified so in this case we have our
hypothetical assembly loop and we are
looking ahead at the element from
logically the next iteration right if
you think about my iteration owns array
element a sub I this current iteration
owns L array element a sub I then array
element a sub I plus 1 is logically
owned by the next iteration so what is
the progress relationship between the
two well the wavefront
ordering says they may be these steps
may be occurring simultaneously but the
next iteration will never be ahead of
the current iteration this is really
important and I'll show you this in the
diagram later on when we get back and
get the actual programming of this stuff
so when we read X from this function
call on a sub I plus 1 we can rely on
the fact the ace of I plus 1 has not yet
been modified why because the step to
modify
a sub I plus one in the next iteration
is later in the sequence and cannot have
happened yet because the wavefront says
it'll never get ahead of me okay so this
is legal this is legal in vector code
and very fast the vector the vector code
just did not care as long as you you've
met the requirements you can't do this
with parallel or you kill it but it
would be so expensive to do the
synchronization and say wait I can't
look at this yet because you know this
guy I can't I can't modify this value
yet because my previous iteration hasn't
read it yet so you'd need all the sorts
of synchronization that's that it's
sometimes useful but most of the time
where people have tried this it's they
take long vectors first on one CPU and
then they try and gang up multiple CPUs
to get even longer vector quality and
they're that do across computation might
actually be profitable but so okay so I
think I've said everything I need to say
about this slide so let's look at what
we've gained here so there's less active
silicon why we don't have an instruction
decoder for every lane we only have one
we don't have a memory subsystem for
every lane we only have one so there's
less silicon in general and less silicon
that is active at any given time to do a
vector computation versus doing the same
computation on multiple cores where
you've got the instruction decoder is
working concurrently on all of them and
so on so that saves a lot of energy
quite a bit of energy especially the
wider the vector is the more energy
you're saving there's no scheduling this
is not an operating system operation
this is just part of the instruction set
there's no let's take this work and
divvy it up the divvying up is very
straightforward next iteration next
iteration next iteration there's no
overhead for sinker in synchronized at
the end when when the Sindhi operations
get to the end of loop you're done
you've done all
the work you don't have to send signals
back and forth saying okay we're all
done now you can continue and there's
almost no iteration no overhead for
communicating across lanes so that
shuffle operation they mentioned early
on where you can rearrange the order of
elements in the Cindy register is quite
fast much faster than if you had
concurrent threads that said you take
this value and I'll take that value and
very important and one of the things I
really want to hammer home in this talk
it can do this wavefront computation
efficiently but there are limitations we
mentioned control flow divergence that's
a big one if you have a lot of control
flow they're virgins
Cindy is not going to be profitable
there's limited synchronization you
can't just take threaded code and plop
it into into assume the registers if
there's mutexes or something like that
you'll deadlock why the mutex is
deadlock well when you get to a certain
to say a mutex you remember there's n
things happening in lockstep and say the
first one grabs the mutex and now the
second one wants to grab the same you
tanks so now you've deadlocked the
second one can't proceed and the first
one can never release that mutex because
it's waiting for all of the other lanes
to complete so no mutexes across lanes
in in cindy code there I'm not gonna say
something but I'll just confuse
everybody including myself again and
farther just think no mutexes all right
blocking operations like i/o kind of a
similar problem to mutexes they have to
be serialized we don't really have like
file i/o happening using sim to units
and but it can be done with multi-core
if you're careful and and then the other
thing last thing is that the the
microarchitecture determines how much
parallelism you're going to get you
can't just say well we'll just design
the motherboard with more
pockets that won't add Cindy registers
it'll add cores but it won't add Cindy
registers and now if you don't require
that wavefront dependencies and though
there's a lot of computations that don't
need them there's a lot to do but
there's a lot that don't then you can
combine Cindy with multi-core and get
the benefits of both
so in that case it does scale but within
you know the within the same the Cindy
world alone you can't scale it just by
adding more Hardware you have to have a
new generation of chip all right I'm not
an expert in GPUs so this is the only
slide where I actually mentioned them
but they definitely need to be mentioned
because I'm comparing different kinds of
parallelism GPUs are kind of in between
Cindy and CPUs the GPUs that are common
today have a lot of very small they're
like very small cores and there's like
thousands of them and they do some
things in lockstep and there's some
parts of you know there's chunks
operating lockstep and and then the
trunks themselves do not operate in
lockstep with each other so they're not
really designed to do these wave front
computations but they have some of the
benefits of Cindy they do save energy
for a certain number for you know per
gigaflop they tend to be more efficient
than than the core is and they you know
you can get this sort of massive
parallelism but there's all sorts of
trade-offs right that they tend to go
through a device interface so for really
small computations you don't want to use
a GPU but for very small computations
you can start getting benefits out of
Cindy okay so I said that we were going
to visit the sandwich shop one more time
here it is so now let's look at the
different kinds of parallelism and see
how related to that very first question
they mentioned now nobody said make them
two sandwiches one at a time even though
sometimes that really is the right
answer like when you don't have a second
employee you only have one right
- hands wet no okay so - so - employed
so so the multi-core approach Sanders
making two employees two hands right two
hands but they have to do the same thing
so if you're making two separate
sandwiches you're not getting really
that much benefit I mean you know we're
stretching the analogy a bit right so
two employees they each have their own
brain okay think of the brain is like
the instruction decoder and in the CPU
so you've got two of those you've got
two things it's it's expensive you got
pay now you're your filt you're filling
one order for two sandwiches now you're
paying to two employees to do that one
order
it requires coordination you do this
sandwich you this is this other sandwich
you can't just have them willy-nilly
decide to make a sandwich and decide
that discovered that they both made the
same sandwich when they're supposed to
be making different sandwiches and and
it requires coordination at the end when
they're done saying okay I'm done okay
I'm done let's put the sandwiches in the
same bag and give it to the customer
okay but with all that overhead it's
still the best approach if if what you
want is the fastest Philip fulfillment
of the order it's still the best
approach when they're making radically
different sandwiches all right if when
it's on why bread and the other is on
whole-wheat and one is a bacon lettuce
tomato sandwich and the other is a
turkey sandwich you'll get no benefit
from having one employee trying to make
two sandwiches simultaneously because
there won't be simultaneous at all
they'll be one after the other in which
case two employees each making one will
get the job done faster what about Cindy
okay we've already kind of seen gone
through this right now we have one
employee one brain one wage right it's
less expensive because there's only one
person there's no context switching or
coordination that's all really good but
if the sandwiches are not nearly
identical you don't really gain anything
you you slow down the process because
they effectively end up being Syria
where are these two radically different
sandwiches they're gonna end up making
one sandwich and then the other so it'll
be slower than two employees for that
okay so now we get into the C++ part of
this how are you actually going to
express this in C++ how are we doing on
time not great okay so I'm going to
speed up
where are my cards by the way oh you're
here okay okay oh that's right we're
doing I got the hour so we're just right
of almost at the 30 minute mark no I'm
doing great then I got my x I forgot we
were starting at 15 after the hour okay
so what kind of code can be vectorized
well data-parallel that's the thing you
should be thinking of it's if it's data
parallel then it's maybe a good
candidate for vectorization a whole
bunch of data items same computation
more or less on all of them that's what
you should be looking for now I have a
confession to make that was a talk
yesterday about know your hardware and
the question was asked why is this why
did the compiler produce faster code for
this than for that and the answer was
because it can vectorize it and I didn't
get it right away so that lets you know
limits of my my expertise because I'm
used to thinking of if I want to do this
computation how do i vectorize it I'm
not used to thinking of this computation
was vectorized how did it get there it
was kind of a backwards thinking anyway
so the kind of code that can be
vectorized data parallel no instructor
control flow like go to x' dependencies
that are consistent with wavefront and
no inter iteration synchronization
except for some things that we know to
be fast but no like mutexes so we're
going to revisit that wave front thing I
just mentioned right that we talked
about before and the computation on the
Left can be vectorized because we are
sure that that first computation
is going to occur with the purple arrow
okay that that first computation is
going to occur before they are the thing
it points to so that matches the serial
order of things and we always aim when
designing parallel systems whenever
possible we aim to have serial semantics
which means if you could execute this
completely serially if you actually get
that you execute this completely
serially you will get the same answer or
the same range of correct answers at
least as using whatever parallel system
whether it's vectorization or multi-core
or whatever so we call that serial
semantics and serial semantics are
preserved in the first case because that
that write to a sub I is guaranteed to
occur before the read from a sub I minus
one on the next iteration which is the
same value of the array really clear how
these two a sub these two elements of a
are the same element okay so they are
talking to the same element but they are
sequenced simply guarantees the sequence
vectorization in general guarantees that
sequence parallelization does not so we
see that diagonal line of the wavefront
we can be sure that at some point it's
going to have crossed the first the
first computation and will not have yet
have crossed the second okay the other
dependency here when we reverse the
statements cannot be true is not is not
dependable it's it's indeterminate in
fact we do not know at the time that we
are reading a sub I minus one over here
whether or not the previous computation
in the previous iteration has completed
and and it's but first of all it's bad
because it breaks the serial equivalence
but it's even worse because it's
actually indeterminate it you don't know
and you'll get different values over the
course of the loop depending on whether
these computations crossed a chunk
boundary or whether they were part of
the same chunk you'll get different
answers
okay so almost certainly not the correct
answer okay so that's the
another view of these wavefront
dependencies takes a little while to get
the hang of what is and is not a
wavefront dependency
fortunately loops tend to have fairly
straightforward wavefront dependencies
so the error rate is not all that bad
but it certainly does happen where
people say oh yeah this is vector and
it's really not so when you're
programming now that we know what kind
of loops can be vectorized how do you
get the vectorization from the compiler
operating system and whatnot well the
operating system is really not involved
vectorization is inside the CPU it's at
the user space level it's done by the
compiler so the compiler auto
vectorization is the oldest and still
the most common way to get vectorization
you rely on the compiler to do it all
for you so the compiler has to recognize
the pattern it has to decide whether
this is worth vectorizing so it does
this cost-benefit analysis based on how
much code it needs to generate how much
control divergence there is and things
like that which I'll get to in a minute
and does this this computation and
decides whether to vectorize it or not
it has to prove and I'm not sure which
order the compiler does it in whether it
does the proving first or the cost
analysis first it has to prove that this
is a valid vector computation now think
about that term proved right
anything that a suspect has to be thrown
out anything that could impede
vectorization has to assume that it will
impede it vectorization and the compiler
has to give up at that point so it
doesn't always succeed even when the
loop to a human observer or the
programmer who knows all of the inputs
and outputs can say yes this should be
vectorizable the compiler may fail to
vectorize it now then we keep getting
better at this I'm from Intel so I'm
kind of obligated to say the Intel
compiler is the best at this at least
for the Intel architecture but there are
some constructs that GCC will do better
at
for you know where ICC might give up and
say no I can't vectorize this in GCC
will actually see through it and and
vice versa
alright so you're right very very much
at the mercy of the compilers ability to
prove that this is vectorizable so what
hurts Auto vectorization well a lot of
things that hurt any kind of
vectorization but specifically for Auto
vectorization any kind of possible
aliasing that could in that that could
violate your wavefront rules so if you
have this computation on x and y which
are arrays going into the computation
and x and y might overlap well some of
the certain overlaps are okay other
overlaps would be a violation because
you would end up with say if Y came
before X I don't remember yeah I didn't
do the analysis but there there's a type
of overlap it's kind of like that
Mehmood versus mem copy problem with a
certain overlaps that mem copy can
handle and others that it can't if the
compiler doesn't know it has to assume
that you have a dangerous overlap and it
won't vectorize unknown alignment I'm
gonna mention the alignment later but it
may not be profitable to vectorize if we
don't know the whether the arrays are
aligned on these vector boundaries
opaque function calls if the compiler
cannot see through to the function
that's calling it can't vector eyes that
function in general there are notations
that will allow it to these to do
vectorization across opaque function
calls but in general if it can't see
that function then it can't vector eyes
and if you have too many opaque function
calls within the body of your loop it's
not profitable to vectorize if you have
one you may still be profitable you
vectorize everything up to the function
call then you call the same function for
8 or 16 times and then you keep going
back to vectors so it's not a death
knell but too many opaque function calls
and the auto vectorizer gives up a lot
of control flow divergence
especially like break and continue and
go to gathers and scatters again all of
these things impede the the they make a
lot of these things like the aliasing
actually could cause the compiler to
give up whereas most of the others cause
the cost function to overflow so it says
no this is too expensive to vectorize so
I'm not going to bother so to maximize
Auto vectorization you want to minimize
those things in the pre on the previous
slide in order to have the compiler have
the best chance of recognizing this as
the kind of loop it might even try to
vectorize you want to make your control
constructs as transparent and and boiler
like cookie cutter as possible so this
is a very straightforward for loop that
just counts from 0 to n minus 1 and ok
my life yes good ok so this the for loop
yeah straight forward any any compiler
should be able to recognize that as a
straight counting loop knows how many
iterations there are in advance can do
all sorts of computations on it this the
C restrict keyword this is from C and it
was added in the leave c 99 it is not
part of c++ but a lot of compiler
support to some degree or another and
they basically says x and y are
independent arrays they don't overlap so
that if you can use it with your
compiler or it might be spelled a little
different it might be underscore capital
are restrict because it's not standard
for c++ but if you if if you know the
compiler or set of compilers that you
are using support it you can use
restrict assume a line same thing this
is not standard but a number of
compilers support this or something like
it you might get these a macro for so
that depending on your compiler it's a
slightly different spelling and says
let's assume the X is aligned on 16
bytes for example which is good
sse4 and now the compiler has a lot of
information that it says oh I can
vectorize that not only can i vectorize
that come vectorize it very profitably
the alignment for example gives it a lot
of information on profitability and the
restrict gives it a lot of information
on whether it's legal to vectorize by
the way the compiler even without the
restrict the compiler could still
theoretically vectorize this but it
would have to generate two versions of
the loop a vectorized and an on
vectorized version then look at x and y
and understand the relationship with n
to say yes they don't overlap I can use
the vectorized version and again the
profitability is a big question there
yeah it would be a runtime decision as
to whether you use the vector version or
not now that may sound bad but remember
that modern processors also have branch
prediction and so if if it's always
valid to use the vector version after
the first time it's made that decision
it'll be lickety-split it'll just go
into the vector version every time until
unless and until that complication shows
different so it's not necessarily
impossible to use profitably and the
last thing I mentioned it's inlining so
that you can see you can vectorize
function calls the important thing to
remember about only anything having to
do with Auto vectorization just in order
to get the auto vectorizer to do what
you want it's a black art and it's
compiler specific and it's CPU specific
and it's they of the week specific I
mean its microarchitecture specific it
is profitable from a human standpoint to
do this if you are writing an
application for a very specific machine
or set of related similar machines and
you really really want to squeeze the
performance out of the machine and you
don't want to go to a summary language
and this that that happens often enough
believe it enough that are not that
people actually do this if you're
working in with particle accelerators
and you've got a specific supercomputer
that's made up a whole bunch of sim
decors you may spend quite a bit of time
with the compiler getting compiler
reports saying this loop vectorized this
one didn't end this is why and so on so
when it comes to vectorization quality
from compilers there's not only the
quality of how well there's auto
vectorize but how good are the reports
that help you figure out why it didn't
auto vectorize alright but how do
vectors i vectorization is really what
we want to leave behind if we can and
instead move to something more explicit
and the the best way in today's world to
do that is with openmp now one thing to
know is some people really have an
aversion to openmp and there's a couple
reasons for that some of them some
people just don't like pragmas for those
people I say get over yourself and and
some people are they don't like the
overhead the openmp puts in parallel
computations cans which can be
significant and doesn't scale well when
you have nested parallelism a lot of
those things have been corrected and
open to P recently but the most
important thing is that's a sim D part
of open a P is affected by none of that
except for the pragmas I mean it is
still affected by it still pragma syntax
but any overhead that the parallel
threading part of what OpenMP puts in
there and there are some weird parts of
that model where it exposes the threads
and things like that that are not very
pure
none of that affects the sim D portion
so you can go ahead and use OpenMP cindy
and not drag in a runtime library that
you don't want and things like that
so open and PMD is an it's it's a
guaranteed to the compiler that I have
thought this through and that I don't
have any aliasing that would violate the
dependency rules and I have I have a
standard form accountable loop if you
don't it's actually a syntax error and
that I don't have any loop carry
dependence ease that would violate our
our rules except for reductions and
inductions which have to be explicitly
called out and I'll get to those in a
minute
the other way that I'm going to mention
right now is a proposal that's in front
of the standards committee for getting
basically that same functionality into
C++ next not it's not not getting into
C++ 17 and part of why we didn't get
into C++ 17 is because actually writing
down the wavefront computation in
something that approaches standardise
was really hard and when I look at it
now it's like it's didn't look so hard
when you read the actual text now but
boy we went through iterations that were
horrible horrible so what this is is
using a syntax that looks like a
function call because it is a function
call it's a it's a function template for
loop which works kind of like the
parallel STL and how many people here
have been exposed to at least the basics
of how to parallel STL looks ok and
about half of you I'll quickly mention
for everybody else it's a set of
algorithms that most of which are also
present in the standard serial STL but
they have one extra parameter at the
front saying parallel lies this using
either normal parallelization or using
parallelization with vectorization and
we don't have one for vectorization only
right now it's in the proposal phase and
the reason we don't have that was
because of what I just said about being
really hard to specify the rules when
you combine parallelization and
vectorization you get the intersection
of the guarantees which means I don't
have to make a guarantee that I can't
describe ok and now that may sound funny
but it's still a useful concept so the
the vectorization without that wavefront
guarantee that's just called uncie
quenched execution so that's the uncie
quenched policy whereas we're proposing
a vector policy and we hope to get that
into c++ next so what it looks like here
is a for loop as an algorithm it's a
function call the first parameter says
do this function call using the
vectorization i as the caller as the as
the programmer and guaranteeing to you
I've met all the rules for something to
be vectorizable same as the OpenMP cindy
and we're going to iterate from 0 to n
well zero to n minus one and pass the
argument that value in as an argument to
our lambda here and then the computation
is the same computation we saw before
okay that's the Saxby computation all
right so it's just a different spelling
of like the open NP yeah yes so the
question is isn't it kind of ugly to
express something that is like a for
loop but using this library syntax this
was a big argument it it may come around
again but the direction that the
committee chose is to make new control
construct like things work like these
algorithms there are a few of us who are
interested in future language extensions
to make this prettier but this is this
that's the the status quo and it it
doesn't look quite as bad if I didn't
squeeze these into this narrow slide
either but it's still yeah it's still
not as nice looking as a for loop okay
you know you should remember in my my my
rhetorical this is you know assume a
four sim D this was actually proposed at
one point and it didn't pass because
largely because some of the parallelism
stuff is not it was not thought of as
being stable enough for standardizing a
new control structure that was part of
the language in C++ seventeen it was
like there was a thought that there are
too many knobs that you might want to
tweak and no way to express them once
you have settled on your keywords and
your syntax so we might stick with
library for a while until we have all
those knobs figured out and then we say
well but nineteen ninety percent of the
cases don't need all those knobs or they
need just this set of knobs and then
maybe we'll come up with language syntax
reduction in induction variables so a
reduction is when you have doing a
parallel computation and you are
accumulating the result into a single
value
this is this is a very common pattern in
any kind of parallel computation or
serial computation for that matter but
when you do something in parallel how do
you do this the wrong way to do it by
the way is with an atomic this is the
way people reach for an open MP all the
time and it is the wrong way to do it
Atomics are slow across threads they
really kind of serialize a chunk of your
code so the right way to do it is to use
a construct that the parallel system in
this case OpenMP on the left or the
proposal on the right gives you to do
reductions because what was actually
happening under the covers there's no
Atomics actually happening here instead
the reduction which is this first sum
here where we're melting things into an
accumulator and and there's different
kinds of reductions be tried besides
adding like you could order them
together or multiply them or whatever is
that it's telling the system that you
are that you are doing a reduction and
what it does is sums that suppose we
have four lanes it's gonna some things
four lanes at the time keep those four
partial results go on to the next chunk
keep adding into the same four partial
results and onward and at the very end
it sums those four partial results into
one final result no atomic operations no
barriers no nothing like that
please hold your questions because I'm
running out of time on the right is the
I'm not even going to go into much
detail but this is sort of what the
library solution is for this see how
we're adding clauses and this is back to
that syntax question as long as we have
not yet seen the last Clause to add
doing it in as a language extension give
people a mmm it makes people nervous
right but the library stuff it's really
easy just add another overload or
another interpretation of your very Adak
template arguments so we added this
reduction clause reduction plus a sum
and induction so the induction is the
other thing an induction is this thing
that happens linearly with our variable
it might it tracks the loop control
variable but it might go in increments
of two when the loop control variable is
going mean equipments have one or
something like that as we have here and
again the neck the iteration I sets a
value iteration I plus one needs to have
seen the change from the previous
iteration how do you do that in parallel
well because you know it's literate
linear you can pre-compute it for the
entire four lanes of your Sindhi
computation and then you jump instead of
incrementing by two you increment by
eight for the next chunk and then
pre-compute add plus two plus four plus
six for the next lanes the last thing
I'm going to mention this the way of
getting this into your C++ code is with
these class libraries so these are
libraries that mimic the sim deregister
they're kind of like a CPU agnostic or
in a portable way of expressing these
simony registers so you don't have to
use compiler intrinsics they're specific
to a CPU and your computations look a
little prettier they they try and give
you that really low-level access to the
Simbi without tying you through an
architectural microarchitecture so this
is what boosts in D this is the sax P
computation one more time using boost
Cindy we create a pack a pack means a
packed set effectively assembly
registers worth of floats okay and
there's ways to deal with multiple
architectures like I don't have time to
go into and you say well what how long
is that and what we're going to do is
Inc go through our loop and increment
not by one but by an entire Cindy
register is worth we're going to suck up
assembly register here's of data from X
and one from Y and then do our our
multiplication in addition here scalar
by vector multiplication means multiply
every value in the vector by that same
scalar and we're going to store the
result in y plus one and we're using
this aligned store thing which says use
the most
efficient way to store it which is
assumed that these things are aligned
okay
there's a lot to be said now there's a
proposal I mentioned it here P 0 to 1/4
it's kind of similar to boost Cindy has
some important differences and we did a
lot of work on it and it's progressing
forward through the standardization
process so we could use and word one of
the things we're going to try and do Z
and integrate it with the parallel STL
and get the get get the benefits of both
so you can you can mix the two
approaches pros and cons of the sim D
libraries
well eventually they might be integrated
with the parallel STL cons you can have
a bi problems if using different
architectures there's work we've done
with the proposal so that at least
you're likely to get a likely but not
guaranteed to get a link time error if
you do this mixing the one of the big
advantages very explicit you have total
control that's also a disadvantage too
much control too much rope to hang
yourself with too much work to get the
job done so I'm gonna very quickly skip
through this because I'm really running
out of time the ways to get the most out
of most performance I mentioned this
alignment in size before so I won't
mention it go too far too much into it
suffice to say you want your things
aligned you want if possible to have
exactly the same number you want to have
a multiple of vector with on your loops
sometimes it's not practical you have
exactly this much data ok so you'll have
to pay the cost for a remainder loop
that's not the worst thing in the world
ok the last thing here in the yellow if
possible let your compiler know that you
have created everything aligned and so
forth and then the compiler can optimize
accordingly but like I mentioned with a
branch prediction not always such a big
problem data layout this is a big topic
so I'm going to go mention it very
quickly ideally you want all of the
elements of the same type to be
contiguous unfortunately when you have
an array of structures they're not
contiguous so we've got double int
double into double int in this array
over here so if we just kind of turn
that inside out and instead of a
an array of structures we make it the
structure of arrays now we have all the
doubles together and all of the
instigator and that maids our our loads
and stores much more fact much much more
efficient ok so this is called AOS SOA
there is a there's a new product part of
the Intel 17-point o compiler that it's
called sim D data layout template sdlt
that will automatically do much of these
transforms for you it's very cool
you can combine thread and vector
parallelism the most efficient way to do
it is to make your outer loops parallel
and your inner loops vector you can't to
do it the other way around because you
can't really do parallel computations
inside of a single simply register so
that's a really good rule of thumb
that's about all I have time to say
about that so I have a few minutes for
questions and then I've got a conclusion
slides questions is there a microphone
no so just say your question and I will
repeat over here vector yes will except
for the plane STL algorithms will
compiler vectorize them often yes if
it's the kind of thing that can be that
has meets all the requirements and the
compiler can understand the the
dependencies and so on yes so a straight
like for each quite possibly the
compiler will Auto vector eyes so the
question is when you have multiple
microarchitectures is there any way to
get the best performance on the computer
that's actually running the code yeah it
depends on the compiler to some degree
but there are flags and sometimes you
could say generate just for say avx-512
which is the best that we've got right
in which case you can't run the code on
anything else or generate for for this
set of micro architectures that might
not include the oldest ones and then the
compiler will actually generate runtime
ifs and again branch
they don't cost very much there was
runtime ifs they cost code blue
but they don't cost much in runtime so
yeah any more one more so the question
is wouldn't for each have done the job
of this separate for loop not exactly
what we have considered it the big thing
that the for loop provides is that index
base threw out an iterator based in
order to go from iterator to index is
this ugly and somewhat silly backwards
computation where you have to have a
generator that generates a list of
integers and you have iterators it
iterates over the generator and we
actually don't have the right kinds of
iterator types yet to do that correctly
so that's one problem and the other
thing is that the that wave front
dependency we're is only really useful
when you've got whatnot it's not the
only way it's useful but it's often most
useful when you've got multiple arrays
when you're looking forward or backwards
which which from an iterator is kind of
an ugly thing to do oh well let me look
at the +1 element and then minus 1
element whereas you're using indexing
it's very clear what's going on right so
that's there's a proposal for the
iterator and like I said we don't have
the quite the correct iterator
categories yet for that proposal to go
forward but they're coming we hope we
think we hope the other thing about the
the for loop is that it has all these
extra clauses like inductions and
reductions that for each doesn't have
but we could add them to for each so
that would be the other way to do it so
yes
the question is if you if you if you
tinker with your code to try and get it
to auto vectorize is there any way to
find out whether it actually did yeah
aside from reading the assembly language
most compilers and certainly the Intel
compiler will produce a report for you
about what got vectorized and what
didn't and often why didn't get
vectorize it'll say there's a loop carry
dependence II on this variable or
something like that so I'll stay a
little bit longer we have a little bit
of time between this in the next session
I can stay longer for other questions
but let's wrap this up so use those
cinder units it is a little bit of a
black art that we're still catching up
with standardization and so on but for
data parallel code you can get these big
speed ups because that hardware is in
there and it's very energy efficient and
you don't want to leave that performance
on the floor you don't get the
scheduling overhead of other kinds of
parallelism so it works better for small
computations and but you have limits
right you have to minimize the control
flow depend dependencies
sorry minimal control flow depends ease
divergence control flow divergence but I
should have said but you can make a
great sandwich using Zim D so thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>