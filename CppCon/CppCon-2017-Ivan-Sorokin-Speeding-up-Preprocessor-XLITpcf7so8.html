<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2017: Ivan Sorokin “Speeding up Preprocessor” | Coder Coacher - Coaching Coders</title><meta content="CppCon 2017: Ivan Sorokin “Speeding up Preprocessor” - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2017: Ivan Sorokin “Speeding up Preprocessor”</b></h2><h5 class="post__date">2017-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XLITpcf7so8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">In IDEs, we need to
reparse files really fast.
Why?
One example of this is code completion.
It's when a user types a object name dot
and we want to show him a completion list.
And to do this we need to have an AST.
The problem is that any
editing invalidates the AST.
So if we want to show
the completion instantly,
we need to reparse files instantly.
That's why we employ different tricks
to speed up this process.
For example, we cache as
many thing as possible
in order to not recompute
them at each reparse.
Also, for some agents,
we update existing AST incrementally
without discarding the old one
and building a new one.
And also we defer some computations
in hope that we won't need them at all.
Unfortunately, sometimes
these tricks don't work
and we need to fall
back to a full reparse.
That's the process where preprocessor
took a significant share of time.
So let's consider simpler.
We thought about how can we
speed up the preprocesor.
Let's consider two macro, A
and B as shown on the left.
And every time A is expanded,
preprocessor notices that B is inside
and it expands B too.
The question is
can we compute the final replacement
for the macro A
and then apply it at each
expansion of micro A.
Surely B can be redefined
and that's why
the final replacement of
A needs to be updated.
We, using these ideas,
we made a prototype based on claim.
And at first expansion of each macro
we complete the final replacement for it
and we track dependencies
which macros depends on which
so we can invalidate them correctly.
This gave us a nice 20%
speed up on boost libraries.
A few months later
we were redesigning our own preprocessor
and we, not just that only
small number of macros
I expanded thousands of times.
Can you just implement them as built ins?
Surely we can.
We did this and
turned out that it was
surprisingly easier task.
We implemented six of the most common
macros in boost preprocessor.
And we got the same 20% speed up
on our own preprocessor.
This is the number of macro expansion
with and without this optimization.
The strength of this approach
is that it's much easier to implement
because implementation of each macro
is localized in one function.
We don't need any fancy data structures
to keep this graph of dependencies.
And as side effect,
we got better error messages.
For example, we can check that
the first argument of
boost_pp_if is a number.
The weakness of this approach is that
it only splits up boost
and the programs that use it.
How can we make this
more vitally applicable.
So my question is
what do you think is it feasible
to implement these
built-in major compilers?
If it is so, we can (mumbles) batches.
It's open question.
Should we implement them 100% compatible
with existing boost macros
or can we improve them?
So if you have any idea,
suggestions, comments
you can mail me
or you can find me at
jetbrains booth here.
Thank you.
(applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>