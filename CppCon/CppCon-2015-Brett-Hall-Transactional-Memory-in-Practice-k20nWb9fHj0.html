<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: Brett Hall “Transactional Memory in Practice&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: Brett Hall “Transactional Memory in Practice&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: Brett Hall “Transactional Memory in Practice&quot;</b></h2><h5 class="post__date">2015-10-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/k20nWb9fHj0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um so get going all right so may he live
an industry interesting times this is
generally talked about as an ancient
Chinese curse it's not actually the case
it was like a mistranslation by some
british diplomat who was in China in the
30s the Chinese proverb that he
mistranslated was like better to be a
dog and peaceful time than to be a man
in a chaotic period but regardless of
where it comes from it's an interesting
concept and it sort of applies to us now
in the multi-core era since processor
power is still going up Moore's law is
still in effect but to exploit that
power this mean
we need to use concurrency parallelism
and if you're lucky you don't have to
share any memory when you do that if
that's the case you don't need to listen
to me you're done you can go sit on a
beach but for the rest of us that need
to share memory we need to share it
safely and right now that generally
means either Atomics or mutexes now
Atomics are sort of too fiddly to do
anything with almost especially there's
no way you're going to structure a whole
program around Atomics it's gonna take
you a decade and you probably still
won't get it right they can work in this
programming in the small in limited
circumstances by people who know what
they're doing who still have a hard time
getting it right if you have a limited
subsystem you can contain your use of
Atomics they can work so we move up a
level in abstraction and we've got
mutexes they're not too fiddly for
programming in the small if you can keep
the whole subsystem you're working on in
your head at one time then you can get
them to work for programming in the
large so you get them to work in the
small subsystem you now want to compose
those subsystems to solve a bigger
problem you can't really do that very
well due to deadlock and does anyone
here not know what deadlock is okay good
I won't bother defining it then so
there's all sorts of strategies to avoid
deadlock and I asked people about this
in job interviews like how do you
deadlock and they give all sorts of
things and you can basically demolish
any way that someone is going to try and
avoid deadlock like you could run into
this situation you have some function
you lock a mutex you call some other
function is this safe who knows you got
to go look at the source code for some
other function and the source code for
all those functions if they if they take
locks this isn't safe you could have
deadlock now let's say you did that and
that worked but now you call some
library function you don't have the
source code for for some reason it's
closed source or it's just too gross to
go look at in this case you have to hope
that it's documented in that
documentation is a not lying to you and
be actually documents whether this thing
takes locks or not and maybe that'll
work for you but then this happens now
you're gonna take some function that
you're gonna call that function hasn't
been written yet you have no way of
knowing what its gonna do the best you
can do here is document your function
and ask nicely
don't take any locks but what these all
three of these sort of have in common is
they destroy your ability to locally
reason about your code you can't just
look at one function and decide if it's
safe or not and that's when I say that
locks don't compose that's what that
means you can't just put together small
solutions that use locks and hope that
they'll and expect them to just work and
really they're all using what I could
what could be termed the asking nicely
pattern and that doesn't scale to large
code bases so what are some alternatives
one thing you could do is use actors
which are also known as CSP or
concurrency control processes and that
goes back to the beginning where I said
if you don't share anything then you're
golden in with actors you share not you
don't share any memory and they
communicate via message queues this can
work if you're what you're working on if
the problem that you're trying to solve
is structured right and maps onto this
concurrency strategy for everything else
it's kind of inflexible and kind of a
pain to deal with so an alternative that
is transactional memory which turns out
to be a lot more flexible and before I
go any farther I should probably what it
is
since when I ask people this in job
interviews or just talking to people
generally they don't know so to begin
with transactional memory you have
transactions somehow you start one it
depends on what your system is I'll get
into details in a little bit but you
have transactions and these transactions
behave atomically
and in this case atomically means first
off when you're in a transaction it
behaves as though nothing else is
messing with memory so here I've got an
example in thread one we're accessing
ver a ver B and doing stuff with them
another transaction on another thread is
messing with ver a the first transaction
will behave as though that other
transaction isn't happening that's just
the transactional system ensures that
you see consistent view of memory in
your transaction and I'll get into some
ways that that's done in a minute
the second part of atomically is that
when you do rights the rights are not
visible to the rest of the system until
your transaction is done and if you can
see one of the trent if you can see one
of your rights or if another thread can
see one of your rights i can see all of
your rights so in this case we have a
transaction where we're setting the
value of a and then we do a lot of stuff
and while we're doing that stuff on
thread two there's another transaction
that runs it can't see what we've done
to it can't see the change we've done to
a yet and then we get to the and then in
our first transaction we didn't set the
value of B and we finish and on thread
to another transaction runs this
transaction can see both has to be able
to see both of those rights if there was
another transaction that ran in between
it might not see the it might see the
new rights that are done in that
transaction instead but it has to be
possible for it to have been able to see
again concurrency so the English gets
very convoluted it has to have been able
to see what you did so why did we use
transactional memory so I work for
company-wide technology we build these
instruments that are used in biotech and
pharmaceutical research and I started
there about nine years ago and I
inherited dynamics which was a program
that was actually originally developed
by a company that Wyatt partnered with
that company went out of business and
why it bought all their intellectual
property out of
courtesy court and it took about a year
to realize it but this is sort of what
the strip this is sort of the state
dynamics was in it was falling down
under the weight of the amount of data
was being tasked to handle and really it
was a it was an old program the original
architecture for the amount of data it
was supposed to handle was fine and for
the time it was probably what you would
do of course when the company was going
out of business they had hired some
contractors who were more consistent
probably more concerned with getting
paid than the future maintainability of
the code as well and so the main problem
with the architecture was it used
threads but it only used threads to
communicate with the instruments once
the intruments got date or once the
those threads got data from the
instrument it would Spacely send a
Windows message with that data purse and
a Miss a message through the windows
cute Windows message queue with that
data to the GUI thread once it arrived
in the GUI thread that's the only thread
that could touch that data and this
leads to problems because we if we have
a lot of data we have a lot of
calculations to do the calculations
themselves are really short but if you
got a thousand of them to do they can
take minutes and if you could only do
that on the GUI thread the program
stutters and locks up for two minutes it
was really embarrassing and it also
makes it impossible to paralyse the
calculations in a minute I'll show you
that we have a lot of ridiculously easy
to exploit parallelism in our data
structures so trying to fix it led to a
cascading series of changes that were
gonna meet need to be made I talked
about those contractors the code had a
horrible amount of coupling in it
changing something here meant changing
stuff everywhere and so when we realize
this we're under competitive pressure to
get certain features into the software
so we just had to kind of grit our teeth
and just go with the way things were
working for a couple of years it was
really embarrassing whatever I had to
meet customers that use dynamics so we
shelved making things thread safe and
parallelizing things for a couple of
years in the meantime I was playing with
Haskell and they're transactional memory
system and for kicks I said okay I want
to understand this better so I
implemented a transactional memory
system in C++
and in the meantime I was doing
improvements to the dynamics and
constantly finding places where hey this
would work great with transactional
memory and eventually I convinced my
boss let's try and use this
transactional memory system I wrote in
the program we'll use it in a limited
fashion in the data store or maybe
storing some experiment parameters just
the stuff we need for the calculations
so if it doesn't work we can just rip it
out and go back to using mutexes it
ended up working really well and kind of
took over the whole program I'll talk
about that in a bit so what does
dynamics look like so first off the
instruments that why it makes are called
light scattering instruments basically
you have some samples something usually
it's a protein you dissolve it in
solution you put in the instrument the
instrument shoots a laser into it and
this is dynamic light scattering so we
look at the light that gets scattered
out of it we look at how that fluctuates
and how that those fluctuations
correlate over time the faster the
correlation dies out the the faster the
correlation dies out the smaller the
particle is because the particles
getting bounced around by Brownian
motion and that graph up there is one of
the correlation functions so our data is
structured we have these things called
acquisitions which if you look over in
that tree the tree is full of
measurements and this is doing work no
there might um so the tree spool
measurements they're all called lysozyme
and a temperature because that's we were
measuring lysozyme which is a protein
and at that temperature but anyway so
each acquisition captures one of these
correlation functions we do a bunch of
horrible math on them and then we run
data filters and any acquisitions that
pass the data filters we averaged
together the correlation functions to
get the correlation function for the
measurement that eliminates noise from
the experiment and then we run those
calculations on the measurement now in
those all those acquisitions are
independent we can run it we can run the
calculations on those in parallel same
thing with the measurements of
measurement depends on its acquisitions
but it doesn't depend on other
measurements that's all the easily
exploitable parallelism that was really
a bummer not to be able to exploit for
years and just to give you a sense of
the scope of this the program is about
800 to 900 thousand lines of code so
it's not a huge program but it's not
trivial either all right so our our
system so this is what our transactional
memory system looks like to start a
transaction you call this function
atomically you pass that a function
object atomically in turn creates a
transaction object and passes it to that
function and then when your function is
done whatever it returns gets returned
by atomically atomically also takes some
other arguments that control some
optional stuff that I'll get to in a
little bit so I've got an example of a
transaction there this is a pretty
boring transaction all it does is
returns 0 so result will just be 0 to do
more interesting things you need
transactional variables and I'm gonna
throw a little jargon at you here our
system is explicit you have to
explicitly annotate what's going to be
transacted and you do that by storing
things in these transactional variables
then here I've created an int and a and
a standard string transactional variable
and in the example I'm setting the
standard string 1 or setting the string
variable and getting the value of the of
the intro of the integer variable and
you can note that we take constant
references and we turn constants is from
getting set basically everything is
either is copied into these objects and
get returns a reference to the stored
object and that reference is good for as
long as the transaction is active if
you're going to copy if you're going to
use that object outside of the
transaction that you call get in you
have to copy it we can't guarantee that
it will still be around after your
transaction ends because another
transaction could run and replace it ok
and our system uses what's called
optimistic concurrency and the way this
works is in transaction 1 we read we
have a transaction where we read a
couple of variables a and B and then we
do a bunch of stuff so our transaction
doesn't end for a while while we're
doing that stuff another transaction
comes in and writes to variable a and
and it finishes at the
our first transaction we go through all
we've we've journaled all our reads we
keep track of everything that's been
read if you try if you read the same
variable twice in a transaction you'll
always get the same value independent of
what's happening on other threats that's
the memory consistency I was talking
about earlier when you get to the end we
go through all that we go down that
journal and we make sure nothing changed
that you read while you were running
while your transaction was running if it
did change we throw away everything he
did and we repeat the we repeat your
transaction if not then we atomically
we've also been journaling the rights
and if you pass the validation then your
your rights get published to the rest of
the system in an atomic fashion one
thing to note is we don't validate the
rights if you wrote to variable and
didn't read from it we don't get we
figure you don't care about the initial
value of that variable we don't need to
maintain it we don't need to worry about
its consistency you just write to okay
so one thing is our system we've no way
of preventing you from doing whatever in
a transaction and one of those things
you could do is starting another
transaction in this case we have two
functions here not a child which takes a
transaction object and oh I forgot to
mention earlier but the only thing that
can create transactions is that
atomically function if something takes
an atomically argument in our
transaction argument and these
transaction objects aren't copyable
either then the only way to call not a
child is from within a transaction but
we also have this other function child
which starts a transaction and does
something and now we have a function on
here parent which starts a transaction
it calls not a child that's no big deal
that's just a function call it passes in
its transaction object nothing we were
just going on there but we also call
child and that will start another
transaction so the question is what
happens when the transact this new
transaction commits in this case we
don't publish any changes to the rest of
the system instead we merge all those
changes into the parent transaction and
they don't become visible to the rest of
the system until the parent transaction
commits there's pluses and minuses to
bowing nested transactions we allow them
because we have no way to stop it in our
system there are other says Haskell
system you can't actually do this there
are some problems you can run into you I
mean this does allow some more
composability in the c++ case but as
I'll show later there are some problems
you can run into you with this so okay
so that's the basics of our system
there's also sg5 it's not a reference to
the old stargate TV show it's actually
study group 5 of the iso c++ committee
and they're looking at adding
transactional memory to the c++ standard
there's a couple of papers there that
one of them is a sort of rationale and
overview of their system of the system
they're proposing and the other one is
sort of the first version of the system
it's a technical specification it was
voted out of committee at the last
meeting so that one's only worth looking
at if you like reading standardise so in
their case you start a transaction by
declaring one of these atomic blocks and
they have 3 flavors there's atomic no
except if you and the three flavors
differ based on if you throw a
transaction what happens if a
transaction comes out of the atomic
block in the no accept case if a
transaction it exits that atomic block
you get on the undefined behavior the
other two if you throw the right kind of
exception and actually sorry I think I
just said throwing transaction so you're
throwing exceptions in the other two you
if you throw an exception that's the
right type you don't get undefined
behavior and the right type is actually
relatively restricted you can look in
the the papers I don't remember exactly
they have to be defined they have to be
derived from certain standard exceptions
but anyway if you throw the right type
of transit exception out of atomic
cancel it cancels everything you've done
up to that point and just your it's like
your transaction didn't happen at all
the atomic commit if you if you throw an
exception out of that then it commits
everything you've done so far so yeah
no no normally you get to the end of oh
sorry the question was do you have to
throw a transaction to make your changes
visible in this system is that is that a
good paraphrase or throw an exception
sorry out of the out of the out of the
transaction to make a change is visible
and the answer is no in it if you get to
the end of the transaction without
throwing an exception then if all these
will commit and there are no
restrictions put on how you implement
this you can use optimistic concurrency
like we do you could have one global
mutex that you lock so you can only be
in only one transaction can run at a
time I don't think anyone would ever do
that that's basically a fence that's a
really convoluted way of writing single
threaded code and there's also
pessimistic concurrency which works sort
of like databases that do row locking
for their transactions you basically you
take a mutex whenever you access a
transactional variable you take a mutex
usually it's done by hashing on the
address of the variable and you have a
hash table these mutexes so you access a
variable in the transaction you take the
mutex you hold that mutex until the end
of your transaction so anyone else comes
in tries to access that variable they
have to wait until you're done
the problem is if you have two threads
and they have transactions they
implement or they access the same two
variables but in opposite directions
you have deadlock so somehow your system
has to detect that deadlock and tell one
of the transactions okay you have to
roll back and release all your locks so
this other one can make progress it that
works really similar to row walking and
databases everything I've seen the
optimistic concurrency wins out most of
the time as far as implementation
details go so in their case they don't
have something like our transactional
variable their system to throw more
jargon at us implicit as opposed to ours
which was explicit in an implicit system
anything you do inside a transaction is
transacted the one thing you have to be
careful with in this is so we're
incrementing I there if at the same time
you're using I outside of a transaction
that's a data race it's not any worse
than how
use mutexes to protect i if you own one
thread use a mutex and then access i in
the other thread you don't grab the
mutex you just exercise that's a data
race it's it's kind of the same thing
they have a notion of transaction safety
because they're modifying the standard
this is me implemented in a compiler the
compiler can use the type system to
enforce that you only do transaction
safe things in a transaction and that's
basically defined as you can have an
implicit lease safe function and that's
just the function that doesn't do
anything unsafe unsafe basically boils
down to you don't access anything that's
volatile which basically rules out most
I oh there's a few other technical
details you if you're interesting that
you can look in the paper
you can also explicitly declare a
function as transaction safe and if you
do something unsafe in that function
you'll get a compiler error and then an
unsafe function is just a function that
does something that's unsafe so then you
can see in the transaction calling the
implicitly safe and explicitly safe
functions those are both okay if you
call an unsafe function in transaction
you get a compiler error and the
compiler can statically figure out all
this stuff and stop you from making
mistakes um that is an advantage of
their system I don't know I couldn't
come up with a way to do this in a
library in Haskell to use mana using
bonnets and C++ it is really difficult
and we don't have pure function
restrictions so we just have to kind of
live with enforcing transaction safety
by policy they also allow nested
transactions mainly for reasons of being
able to compose things easier they could
have made like starting a transaction
unsafe but that leads to some other
design decisions that have to be
reconsidered so they allow Anessa
transactions they work effectively the
same as in a in the system we use it why
it alright so now the canonical
synchronization example when you learn
about mutexes you use bank accounts will
do the same thing here in this case
we're just doing a simple transfer on
the Left we have what it looks like in
white system on the
we have the sg5 technical specification
in both cases we have some variables
they're not very good bank accounts I
guess we store cents in they're not
dollars or we only store store whole
dollars and then we have a function that
does a transfer from account 1 to
account 2 in the white system you see we
just do a series of sets and gets
passing around the transaction object
and in the sg5 system it's even simpler
we just have an atomic block and we use
minus equals plus equals like normal so
the thing to remember about this is
there's no data races and there's no way
for another thread in the system to see
account 1 with the money deducted and
account 2 without seeing account two
having the money added to it and there's
also if this is implemented correctly
there's no chance of deadlocks so now
I'm going to stick to the white system
because I'm gonna talk about some
features that we have that aren't in the
technical specification yet um and
details are still being worked out on
that they're gonna be like a version 2
at some point that might include some of
this stuff but let's say we want to send
a notification when the transfer is done
now we could just so we have a function
here send email the thing to note about
sending email it's not transactional you
can't undo C sending an email I mean
they're all always times would like to
be able to do that but you can't so in
our system the way you handle that is
the transaction object has this method
called after and after you give it a
function object and when the transaction
commits it'll be called after all the
validation passes and we've made
everything visible to other threads you
could say hey instead of doing this T
after thing let's just do send email
after the transaction is done outside of
the call to atomically the problem is
transfer with notification here could be
in a nested transaction and then you're
back to doing the send email that isn't
transactional in a transaction when you
have a nested transaction we have this
list of things to do after the
transaction commits that gets merged
into the parents list and that happens
transitively until you get to the
top-level transaction and that commits
and then we do all the after stuff sg5
is looking at something similar they
call there's on commit action
they're still working out some kinks in
that so now let's say we want to wait
for our account one to have a certain
balance in our system what you do is you
just start your transaction like normal
you check the account balance by reading
it and then you call this function retry
and what retry does is it puts your
thread to sleep until another thread
runs the transaction that modifies one
of the variables that you've read and
then it goes back to the beginning and
starts your transaction again hopefully
eventually someone will trip will
deposit enough money on another thread
and our retry we won't we'll get our
condition there will be above the limit
and we won't hit retry and we'll do that
we'll do the transfer retry also you can
give it a timeout if it hits that
timeout it throws a exception that
hopefully you catch outside the
atomically block and also atomically
there were those extra arguments that it
could take one of them is specifying
sort of a global timeout for any retries
that are done within that transaction if
you specify both it takes whatever the
smaller one is so sg5 is also
considering something along these lines
but there are some issues to look out
for I'll show you an example in our
system in a bit when I get to pitfalls
which we're gonna get to now so
unfortunately it's not all unicorns and
rainbows it hasn't ever been the case
that it's been rainbows and unicorns
which according to urban dictionary is
really bad unicorns and rainbows is
really good so side effects so this is a
this is specific to our system since we
don't have statically checked
transaction safety here I've rewritten
our send email or our notification
example but I've taken out the tea after
and we're doing the send email in the
transaction so this will work send email
will be sent at least once but it could
be sent twice three times a thousand
times million times it depends on how
many conflicts this transaction run-ins
runs into and how many times it gets
restarted so that's why we have to have
our after functionality this couldn't
happen in the SD five system because
send email would be non transaction safe
this wouldn't compile
yeah um yeah so the question is could we
somehow pass the the transaction object
to the sent email and have that checked
for completion of the transaction to do
it the problem is you still need the
basically you're just kicking the can
down the road you'd still need the T
you'd still need to be able to call
after in the send email function to be
able to queue up sending an email when
the transaction commits or you'd have to
have some other thread that continually
watches to see if your transaction is
done notification notifying is somehow
one thing I didn't mention is
transactions and mutexes don't get along
very well in our system we we don't do
anything weird under the covers so they
just they don't play nicely but it's not
impossible we actually in the unit test
for the transaction memory system we use
me Texas a lot to set up various
conditions in the sg5 system if you lock
a mutex within a transaction it's
undefined behavior and that also pre
that also precludes using condition
variables since you can't locking you
ties
yeah so okay well okay so I wasn't
totally accurate so the question is it
sounds like we don't use meu Texas under
the covers to implement this and that's
not exactly true we use a reader rider
mutex there's one reader item you ticks
are our system has a really boneheaded
implementation that works great for a
small number of cores it's really fast
for a small number of the course but as
you scale it runs into contention and we
need to fix that I don't when when
there's core i5s that have like 8 to 16
cores we're really gonna need to fix our
system but right now we have plenty of
performance with our boneheaded
implementation and there are ways to
implement this in a mostly lock-free
I don't know how you do retry without
locks but the rest of it you can
generally do lock free for the most part
um ok all right
so sg5 the send email wouldn't work ok
so when we have a function that we know
it does side-effects and we don't want
it to be called in the transaction we
can't prevent that statically but we
have this thing called this macro called
no atomic which just declares an
argument with a default value when that
default value gets default constructed
it checks to see if we're in a
transaction if it is it throws a
transaction or it throws an exception
that no one can catch and crashes the
program we want to do that because if we
don't we're gonna get subtle bugs so we
want to fail fast it's somewhat
heavy-handed and a static check would be
a lot nicer but this generally has
worked for us and we catch the problems
which are pretty rare in testing so I'm
more general this is sort of something
that's general to all transactional
memory systems we don't get deadlock the
big sort of bugbear of transactional
memory is starvation and this happens
when we have say in this example we have
thread 1 which has a transaction it
reads some value and then it starts
doing a bunch of stuff generally you
want to keep your transaction short to
keep the chance of conflict down
but sometimes you need to do a bunch of
stuff in the transaction and while we're
doing all that stuff another thread has
started and all it does is continually
start a transaction increment that value
start transaction increment that value
so it's doing that really fast
so whenever we get to the end of the
transaction on thread 1 we're going to
have a conflict unless we get extremely
lucky and the thread scheduler in the OS
has completely messed with thread 2 and
not allowed it to run we're just gonna
get we're gonna get conflict after
conflict and thread 1 we'll never be
able to make progress last year I gave a
lightning talk on this and I said in
four years we've been using this I've
never seen this it was theoretical
unfortunately two months ago
what yeah I should not have said that
because now two months ago I saw this it
wasn't where it wasn't like a deadlock
where it got stuck forever what happened
was um in the GUI I showed you down in
the status bar we show how many things
that are left to be calculated and
normally that counts down to zero and
stops we were in counting situation
where it would start at 600 because it
was a big file it would count down to
zero and then it would go back up to 600
and do all the calculations the second
time and it was starvation there was a
long transaction it was trying to touch
every measurement in the system and
change something and while it was doing
that our calculation engine was going as
fast as it could cranking through these
calculations which took a lot less time
than touching every measurement in the
system and had something like 600 or 700
measurements which is a fairly large
file not humongous sometimes there's
thousands but so it kept pre-empting
basically the thing that was trying to
touch everything kept getting preempted
by the calculations and eventually the
calculations finish that's when we the
counter got down to zero so then the
thing that was gonna touch everything
would go back and touch everything and
re trigger all the calculations and we
go back up to 600 so the way we handle
this No okay I didn't decide for that
sorry umm the way we handle this is
another one of those arguments that you
can pass to atomically you can put a
limit on how many conflicts you're
willing to have this this transaction
get so like you could say if you get
five you can get up to five conflicts if
you hit that limit you have there's
another option that says what to do one
thing you can do is it throws it throws
an exception
I don't know somehow you you then remedy
the situation the other thing you can do
is what's called running locked and when
you run locked I mentioned we use this
reader/writer lock um
mutex under the covers you can actually
get you actually get what's called an
upgradable lock on that and when you get
that upgradable lock no one else can you
hold that for your whole transaction no
other transaction can commit you can
still do reads because read locks can
codes this with the upgrade of a lock
but no one can get a unique lock on that
pointer to do a commit until you release
your upgrade lock what you won't do
until you get to the end you upgrade it
to a to a unique lock you do your commit
and then you release it it's heavy
handed in this case it worked it was the
simplest thing to implement other things
you can do is basically you can assign
priorities to calculate two transactions
based on how many times they've had
conflicts and get priority to ones that
have had a lot of conflicts it's kind of
an exponential back-off sort of strategy
it's a lot harder to implement and like
I said we've run into this once in five
years and this solution worked for us
all right I there could be problems with
retry so here we've got what I'm going
to term a retry deadlock these so
basically what happens is you have one
transaction so we've got two boolean
transactional variables they both start
out false in thread one we set the value
one to true and then we check to see if
value two is true if it's not we retry
in thread two we do the opposite now
obviously if these interleave properly
then the value the setting true of the
values will never be seen by the other
thread so they'll be stuck retrying and
waiting forever if these are the only
two threads that are touching these
variables um and note I said these are
the only two threads touching these
variables with a with a mutex deadlock
if you're in deadlock and some other
thread comes along and tries to lock one
of those mutexes that you're deadlocked
on then that thread gets stuck to you in
our case if another thread comes along
and changes one of those variables then
we'll start making progress again so
it's kind of it's a softer form of
deadlock
it's still something could happen this
is not actually something that I've seen
in real life it's this code
kind of structure is sort of badly
structured and it's sort of basically
writing an infinite loop but we can fix
it by splitting this up into two
transactions when you commit the
transaction the other one will be able
to see the change to the variable that
it's going to wait for the problem is
eventually some bozo will come along and
do this they'll nest those functions
that we where we split up the
transactions into a transaction and now
all those all the things that we want it
to be in separate transactions are now
in one transaction again so this is kind
of I mentioned there can be problems
with nesting transactions this is the
sort of thing that happens in our case
let's see no I didn't put to solve this
I would basically mark the two the two
functions that do the setting and
waiting as being not allowed to be in a
transaction and arguably if you're going
to run a transaction and something else
is going to rely on the results of that
transaction or you're going to rely on
the results of that transaction later on
in your function that's not transaction
safe so it should be marked that way
so this is kind of probably peculiar to
our system and it happens really rarely
but so the example is we've got these
two values and let's say there's an
invariant on these values that value one
is always greater than value 2 and if we
rely on that you can see in transaction
in thread one we have a transaction we
read the value of the first variable we
read the value of the second variable
and then we allit that we create a
vector that's as big as the difference
of those two things well let's say
between reading the two values we do a
bunch of stuff again it's a bad idea you
want to keep your transaction as short
as possible but if we can't avoid it
we have thread two which comes along and
it modifies both of them it modify some
book consistently in that it maintains
the invariant value one is still greater
than value 2 the problem is we have
value one from the old values we have
value two from the new values when we
commit will the validation will say note
you saw inconsistent you have to go back
to the beginning the problem is we're
now going to try and allocate probably
four billion bytes or sorry for like
four gigabytes and it's gonna
um arguably allocating memory is a
side-effect we shouldn't be doing but
normally it's it impotant if you if you
use our aii the memory is going to get
released if you robot robach and it's
too hard to deal with any weather way um
this happened this has happened maybe
twice in five years and it's pretty easy
to recognize now the solution is we have
this function validate which we insert
right there and validate basically runs
the validation that we normally do at
the end of the transaction it runs the
validation then if your values up to
that point aren't consistent it sends
you back to the beginning to try again
we still have to do the validation again
at the end because more stuff could have
changed but it ensures that a is going
to be greater than B I've been trying to
come up with ways to make this automatic
so we don't have to insert the
validation one thing to do would be
every time you do a get you have to
validate but that kind of kills
performance especially if you get a
bigger transaction that's accessing a
lot of stuff so this is still kind of an
open problem but it hasn't bit us too
many times okay so this is more of a
design issue and it's kind of more
specific to our system to an X to an
explicit system but first off you have
to decide what are you going to transact
in an explicit system that's easy
anything that's going to be touched from
multiple threads the harder part is that
what level do you transact things you
can do things fine-grained where like
every member of the structure is in a
transactional variable you could also
coarse-grain it by not transacting
anything in the structure but when you
store one of these coarse grain
structures you store a shared pointer to
a Const coarse grained and so when you
use that you get the pointer to the
constant if you want to change it you
have to copy it change it and then shove
the new thing into the into the
transactional variable it all comes down
to access patterns what you choose if
you're going to be reading a lot and
barely ever writing the coarse grain
often works better because you only have
to make one transactional call to get
everything if you're going to be reading
and writing a lot of different stuff and
a lot of different threads that are
members of that structure you want to go
with the first thing
after a while of working with it kind of
become second nature in an implicit
system you don't have to make these
decisions how you access things within
the transactions kind of makes the
decisions for you and it's kind of
automatic yep yeah so all right so the
question is how come we don't see
consistent view of memory in this case
um the sort of the guarantee is that if
the transaction commits you saw you saw
a kind of gloss over this if the
transaction commits then you've seen a
consistent view of memory in this case
that transaction won't commit if for
some if we took out that memory
allocation that's gonna crash everything
and got to the end that transaction
won't commit it'll get rolled back and
you'll have to try again to get
consistent values and again this is
probably peculiar to our system and I'm
looking into ways to make it more
automatically catchable yep yeah okay so
the question is why wouldn't that
transaction commit the problem is so you
read a and then something else changed a
by the time you get to the end you're
gonna have the old value in your journal
the validation is gonna check that
against what's in a now it's gonna be
different so it's gonna say okay you
have a conflict you have to roll back
and try again it's because yeah that's
what causes the the conflicts if you
read something and it changes before you
get to the end of your transaction then
it's a conflict and you have to try
again yeah
yeah yeah the trip yeah the
transactional system doesn't know
anything about the invariant that's
something you have to enforce in your
own code it probably actually one
strategy now that I think of it would be
to somehow express those invariants in
the system so the system knows like when
you read this if this changed you have
to it's already a conflict that's
actually something gonna have to look at
what um well not necessarily it would
just be that yeah well it would just be
so we've read a when we read B if we've
expressed some sort of constraint
between them then we could then when we
read B when we read either of them it'll
check the call ready read a and then we
read B it would check the constraint and
if the constraint is violated it knows
okay something's wrong here roll back
and try again
that would be prevent you from having to
do a full validation I'm glad to give
this talk now I got an idea to pursue
here for this yeah yeah okay so let's
say okay so we've got five on of course
okay so week atomicity basically what
happens here is you've got this
transaction you pull something out in
this case you're flying down the trench
you pull out an exhaust port once you
have that outside of the transaction you
can do whatever you want with it you can
do stuff to it that is not transactional
if it's not structured properly in this
case we're gonna shoot a proton torpedo
at it that's not transactional so we
have a weakness in our case we handle
this by requiring that everything we
store in transactional variables is
either immutable or internally
transacted immutable is basically the
coarse-grain thing we saw before it's
you can't change it to update it you
have to get it copy it make your change
and then put the copy into the
transactional variable the other thing
to do is internally transacted and that
means any any mutable state in the
structure has to be transacted either
has to be in the transactional variable
or you access it through a transactional
function which will undermine that will
probably just be accessing transnational
variables this we
in force through code review I don't
have any way to enforce the statically
right now but it's generally good
sometimes we get new people who decide
to start cons casting things and that's
a problem but if they're consec asting
that's not gonna pass code with you
anyway okay so another pitfall is this
is more philosophical it can be really
invasive remember at the beginning I
said okay we're just going to apply this
to the data store and some parameters
and stuff reusing the calculations it
works so well it took over the whole
core of the program it was just it
worked really well and it's a lot easier
to interface transactional code with
transactional code interfacing
transactional code with
non-transactional code you have to use
the after stuff and deal with function
objects and think kind of it's
non-linear lee this is gonna be executed
at some point in the future so really
the stuff that isn't transactional argue
is written in MFC can pity me if you
want I would love to change that but I
don't have that much time so obviously
that's not transactional it calls into
the transactional code and starts
transactions like the master channel
just can start transaction everything
but it's not transactional itself so we
have to use afters to trigger things
there in fact one of the big side
effects that's usually really obvious
that
newbies make is they they pop up a
dialog box from within a transaction
that dialog box pops up maybe once maybe
twice maybe a thousand times and then we
say okay no you have to use after here
the other thing we have at the other end
of the program we have some some code
that has latency requirements mainly
getting data from the instrument we
don't get the data fast enough pull it
out of the socket fast enough the
instrument will say okay you're
disconnected you're not keeping up and
some of our data we're moving quite a
bit so the probably the last thing here
is no one's heard of it when I go to
when we go to hire people I'm
interviewing Salem have you heard of
transactional memory and most people
have not heard of it if they have heard
of it they've never used it it's not a
big deal generally you come up to speed
pretty fast it takes maybe a month
before they stop making dumb mistakes
our new Bheema so that's not fair Domo
said newbie mistakes they haven't ever
used it before it's not very common but
more importantly since no one's ever
there's no best practices out there
there's tons of academic papers on how
to implement transactional memory
there's nothing on how nothing out there
on how to use it so we're kind of
figuring as we go along it's worked out
pretty well for us and I think that's
just it's easier to use the mocks we
find cases where we have to figure
things out and that's actually kind of
fun because we're kind of working on the
frontier here but that's kind of it for
pitfalls unless you consider performance
a pitfall which some people do for
transactional memory and what I'll say
about performance just kind of as a
generalization it's kind of like this
car in the right conditions it's great
it's not going to be too dragster it's
not going to beat a Formula One car
those being new Texas or Atomics or
something in the conditions where those
things excel if you put a Formula One
car on a dirt track this cars gonna
destroy it so specific to our system so
first up dynamics it's not really a low
latency application
you know modulo the what I mentioned
before about instrument communication
where we don't use transaction memory
you know it's not a game it's not a
trading bot we don't have hard real-time
requirements for the most part the other
thing is we're really good at hiding our
Layton sees that we have we push as much
as possible in the backward background
threads and the transactional memory
makes that really easy the GUI as much
as possible we don't want it to lock up
if you're gonna do something that's
going to take more than ten milliseconds
or whatever put it into art we have a
calculation engine that you can put it
in as a high priority thing so it'll get
done as soon as possible
whatever threads for you to do it and
part of that is also we prioritize
things in our engine if you're looking
at a specific measurement any
calculations related to that get
priority in the calculation engine as
long as there's nothing really high
priority but the GUI needs done so even
when we have latency it's in there also
our calculations are almost ideally set
up for transactional memory and I'll
I'll get there's the next slide I'll get
into what that means as I mentioned some
internal communication is low latency or
needs to be low latency so it doesn't
use TM so with those caveats we get
plenty of performance and our
calculation structure schematically
looks like this we get some parameters
we get the data we do a bunch of
horrible math that takes a long time
relatively I mean it's a couple hundred
milliseconds maybe and then we store the
results somewhere the only parts that
need to be transacted is getting the
parameters getting the data storing the
result the calculations is basically to
pure function in the parlance of
functional programming it doesn't need
to be transacted and so the calculate
result that the amount of time that
takes totally Dwarfs any overhead we get
from the transactional system okay so
more generally in performance it's
probably never going to be as fast as a
hand tune using a mutex or atomic so
there's just more overhead you have to
validate in some way you're gonna have
to validate your journal or you're gonna
have to be dealing with these mutexes in
a pessimistic system and there's also a
bigger non-deterministic price to pay
when you do have contention so when you
have contention with mutexes you're
coming along one threads locked mutex it
can go on its way
another thread comes along it tries to
lock the mutex it has to sit there and
wait until the other one's done you pay
a little bit of penalty there waiting
for the mutex to be unlocked in our
system both threads will just blaze
right on through one of them will commit
the other one we'll have to go back and
start again and repeat everything it
just did so that's a bigger price to pay
unless you're using if you're using a
pessimistic system you'll have to wait
on locks and hopefully you don't get
deadlock and have to roll back and do
that um but there are cases where you
can have less contention here I've got
an example we've got two variables that
we're gonna use a and B and we're only
going to use them in a read-only fashion
to safely use a and B because other
threads might be writing to them we have
to lock a mutex in the mutex case and we
have to hold that while we're using a
and B in the so only one thread can be
running use a and B at a time in the
transactional case we read a and B we
aren't holding any mutexes and we have
to hold a mutex while we're reading them
but we've released it right away so
there's a lot more concurrency that's
available there any number of threads
could be running use a and B
concurrently this is a little artificial
you could say well let's use a
reader/writer lock in the in the mutex
case or you can restart
to not have to hold the walk the whole
time but notice I'm saying change this
change that it's automatic it
automatically happens in the
transactional system in the new text
system again non-local reasoning you
have to figure out how your mutex is all
relate to each other so in some ways you
can give better concurrency in some
cases and sort of in ciliary to to
performance whatever performance in
transactional memory comes up people
mentioned Hardware transactional memory
is like this is gonna save everything
and this actually exists today in the
form of Intel TSX it was in the Haswell
processors but there was a bug in it so
it got disabled it's currently in I
think the high-end broad Wells I don't
know if that means Xeon or the high-end
core i7s Intel's kind of Cape seems to
be kind of cagey about it I haven't
looked into it super deeply because it's
we can't really use it very right now
for our system but from what I have been
able to find and it's it's gonna be more
available in skylake processors so this
system first off it transacts cache
lines on what this means is say you have
two variables a and B that are on the
same cache line you could have a
transaction that reads a and you have
another transaction that writes to be
even though they've written and read
different variables that'll be a
conflict it's also implicit like the TS
system there's an assembly language
instruction that you emit that starts
the transaction from there on out
everything you do on that processor core
is transacted and then at the end of
your transaction you emit another
instruction to either commit or abort it
if you commit it it tells you ok you
committed ok or not and then the other
thing is this system is bounded and
first let me define what unbounded means
the TS system and or the sg5 ts system
and why it's system are both unbounded
you can do as much transactional stuff
as you want in a transaction if you do
too much it's a bad idea because you'll
probably get conflicts but um you can do
whatever you want up to the limits of
memory or address space or whatever and
it'll work in the tsx system basically
there's a right
a buffer and a right buffer that stores
these cash lines that you've accessed
transactionally they're of limited size
because you're on hardware if you spill
one of those buffers you can't commit
your transaction it's just done you
can't do anything
so you either a have to be working in a
domain where you know you're on this
Hardware you can arrange everything your
programming in the small you're looking
at the system you can arrange that you
know you're not going to spill those
buffers and if you do it's a bug you
have to go in and fix it and rearrange
things so that you don't spill the
buffers if you if you can't do that and
you want to use this then you have to
somehow fall back on a software system
when this happens and I suspect when we
get implementations to the TS they'll
probably I mean someone will probably
try and use TS x and create a hybrid
system where when they can they use the
tsx stuff and when they spill the
buffers they fall back on the software
system they're gonna need anyway when
they're running on a processor that
doesn't have TSS
as I said we currently I haven't really
I haven't tested this or try anything
this is all just based on reading
Intel's white papers the implicit kind
of causes us problems because our system
is explicit and we rely on that and the
bounded means just that it would be more
work to implement behind our system ok
so how did it work out the TLDR of the
talk which normally I should probably
should put at the beginning but we'll
put it here it worked great you just
have to keep in mind all the stuff I
told you about how our app is structured
what kind of stuff we do so that you can
understand the performance profile and
how it fits in new people get up to
speed quickly I mentioned no one knows
what knows about it but they come up to
speed pretty quickly it's it's
relatively easy to use it's much easier
to reason about them locks which you
know it's more fun than mutex it's
really new Texas are fun when you have a
small system and it's a puzzle to figure
out how to lock them and make sure that
everything works with that deadlock it's
not fun at the at the big name said we
have another team that works on another
application that's similar to that
because it's a little bigger it works
with different instruments they're using
new Texas because they then weren't in a
state where they were gonna rewrite the
core of the program and they're just you
know they're looking for dead locks all
the time and it's no fun um so really it
kind of optimizes for programmer time we
don't have to hunt down dead locks all
the time every once while there's a
weird issue but it's way less work
to track them down been dead box and
dealing with new Texas so before I open
up to questions last year of course the
first question was is this open source
can I play with this last year the
answer was no sorry
this year the answer is soon a couple of
weeks ago me and my boss convinced the
why it's the people who own the company
and run it
hey let's open source some stuff get
back to the community we use a lot of
open source stuff so we got permission
to do it so far all we've done is come
up with a license it's gonna be some
form of BSD license since I've been
preparing for this talk we're also
trying to get a big release out I
haven't had time to clean it up there's
some embarrassing code in there that I
don't want anyone outside the company to
see so you know open sourcing it's
already improving the quality of our
code it also it's it's it's got some way
to specific stuff in it dealing with
thread-local storage that needs to be
made cross-platform to you I think
there's also some other goodies in there
we have basically a transactional
version of futures not everything that's
in futures but something someone in
futures you've got multi cache channels
and queues and some other stuff to find
out when that happens you can watch my
blog which is that top link there it's
also if you go to my bio on the schedule
there's a link there if you want to know
more about why and what we do there's
why calm it you know you're starting a
biotech company you just got a ton of
funding and you want to buy a bunch of
instruments go there get in touch with
our sales staff mention that I sent you
so maybe I'll get a bigger bonus next
time if you want to know more about the
sg5 system there's papers there's n not
three nine one nine which is sort of
their motivation in the details of their
system and and 4514 which is what
actually got standardized there's some
differences between the two so if you
really want to know what's coming with
that you have to dig through the
standard ease and 4514 I also wrote a
paper for them 4438 that just details
our experience using transactional
memory what our how our system works and
what we found to be useful and not
useful it's a lot of what I talked about
here it goes into detail on more detail
and stuff I glossed over and their stuff
I didn't mention that's in there so if
you want to know more about our system
that's actually a good source my blog I
write stuff a little bit I was going to
write a lot on the blog but then this
Michael Wong who runs the sg5 group
asked me
right this paper form and that sucked up
all my time and I figure you can just go
look at the paper so there aren't any
implementations of the sg5 system yet
they're working on one and I know one is
in process on GCC I don't know what the
status on it is I think it's just they
just the person who's doing it just
started on it there's also talk of doing
one in clang but GCC has had an
experimental transactional memory system
in it since four point seven I'm not
sure exactly with it so all you have to
do to to enable it I don't know if you
have to compile GCC from the source and
give it a compiler flag or or if it's
just in there and you just turn it on
with a flag on the command line when you
compile your own code it's similar to
the sd5 system it's not exactly the same
and that's why GCC will probably have
the technical specification before
anyone else because it already has the
system that they can build on Intel had
an experimental compiler that did
transactional memory in a way similar to
the SD five the sg5 technical
specification but it's been retired it
was actually around when I first start
playing with this stuff but I it was
labeled very experimental and it's
experiment we didn't have source code
for so relying on that in our production
application would be a bad idea instead
we won with my systems since you know we
had the source code and we had the guy
who wrote it on the payroll but that's
I'm not sure if you can still get that
but that's something else you can look
at and with that I guess I'll open it up
to questions
oops okay over there
um so the question is we have in our
system it's explicit there's this notion
of fine grained versus coarse grained
and in the TS we don't have there isn't
that because it's implicit um like so I
kind of glossed over it but it kind of
how you access things is going to
determine if it does fine-grained of
course even if you're always like
grabbing a pointer a pointer to constant
object trance you know somehow
transferring that out of the transaction
and using that then you're basically
coarse graining if you're constantly in
the trends inside the transaction inside
the atomic block accessing the member
variables then you're fine grained it's
just it sort of happens automatically by
osmosis how it works it there isn't
anything different in that the system
does it isn't trying to detect how
you're doing this that I know of I mean
well none of this has been implemented
yet so I mean conceivably they could do
something like that but I don't think
they need to I think it's just how you
access things will determine how things
get transacted yes so the question is do
we have a handle on how much how many
CPU cycles we're wasting in abortive
transactions I don't I have some really
rudimentary profiling built into our
system but it's more based around on
figuring out what's conflicting with
other things I haven't really looked at
it I it hasn't been noticeable I'll give
you that
but again our system isn't super
performance heavy I mean we want the
calculations to go as fast as possible
but when we're talking about like you're
calculating a thousand things and each
thing can take half a second I mean it's
just it's a horrible math it takes that
long if the if the scientist who's
waiting for these results has to wait
thirty more seconds they aren't gonna
care it and the GUI is is going all the
time we hide these Layton sees if you're
looking at something it will get
calculated first you won't notice that
it's still calculating all this other
stuff most likely they're down the hall
getting coffee while this collection is
going on and things are being calculated
and they'll come back in a couple of
hours because doing the actual
collection takes a long time cuz usually
have to wait for temperatures to change
in the in the instrument and do all this
other stuff so yeah I don't have a good
answer
yes uh-huh
so all right so the question is what do
we do with more complicated structures
like vectors and maps so our one of our
main data types is called the
correlation function and it's basically
a vector of 512 doubles okay so this is
gonna be the last question um when we
store that we store a shared pointer to
a vector and most of the time you don't
change that never changes for the
measurements it does because we have to
average the acquisition correlation
functions and in that case we allocated
a new vector inside a store inside a
shared pointer and the share pointers to
a constant vector that's really
important you can't change the vector
once once you've put it into the
transactional variable in other cases
instead of using the standard map
there's a book by Chris Okazaki which is
about functional data structures where
you they're persistent but they're
immutable so basically copying them is
extremely cheap and then you when you
modify them you're basically copying
them but you're only changing the nodes
like in a tree you're only changing the
nodes that are affected by the thing
you're inserting and we use that I mean
mostly you know you want to use vectors
for everything and so use SharePoint is
two contactors a lot that is and for
more complicated things that might have
links inside again you're probably gonna
those things are gonna be Const so the
point and you're gonna store within them
you're either going to make those things
immutable by storing a pointer to a
Const object if it's allocate memory for
something or I mean these are within
your own structures that you're defining
not in like like STL stuff or those
things are going to be stored in
transactional variables within your
structure for the internal transacting
well maybe see me see me after you can
ask okay thanks everybody
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>