<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: Ben Deane “Testing Battle.net (before deploying to millions of players)&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: Ben Deane “Testing Battle.net (before deploying to millions of players)&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: Ben Deane “Testing Battle.net (before deploying to millions of players)&quot;</b></h2><h5 class="post__date">2015-10-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OPoZWnYIcP4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming my name is my name is
Ben and this talk is about testing
battlenet and it's I hope it's going to
have a bunch of things that can be
practical that you can take home maybe
to your own code bases and this about
the stuff that I developed when I was
working on the bound that servers and
the battlenet client libraries in C++ of
course so I'm gonna set the scene for
you guys a little bit before you get the
more interesting stuff first few slides
probably go fairly quickly so bound that
is about 325 and change thousand lines
of C++ which means it's it's not huge
you know there's plenty of people at
this conference who work on bigger code
bases but it's not that small there's
plenty of stuff in there that's kind of
varied and so the kind of stuff you
would expect to find in a code base
which has been worked on by you know a
couple of dozen engineers over the
course of maybe ten years and there's
lots of sort of different things in
there and the main few things that bound
that does is authentication social
services matchmaking which is pretty
much the only thing that's not a i/o
bound so much making is the kind of the
CPU intensive tasks that bowel med does
and then storing some stuff and as I was
saying it's pretty varied and it's
highly asynchronous to the point where
you know the mindset of Val net
programmer is that everything is
asynchronous because most things go
between machines in multiple hops for
any for any given function call you kind
of think that way and it's highly
configured which means you know a lot of
stuff gets read from configuration at
startup time and there's a lot of
telemetry that goes out so I found
myself you know maybe a few years ago in
this kind of familiar situation you know
I'm a games industry programmer I'm a
career game programmer I have very
little practice at unit testing it's not
part of that games industry culture and
Here I am on this fairly large project
that for me with many moving parts
that's for sure and as usual you know
it's made up of lots of components and
there are sort of mature lower level
libraries so there's stuff that was
written ten years
go hasn't been touched maybe it has unit
tests in there you know there's a
handful of unit tests when someone
thought it was a good idea but that code
isn't changing you know those unit tests
haven't changed in a few years and at
the same time you know this is constant
drive to add features so a team of half
a dozen to a dozen engineers just adding
stuff because we needed it yesterday so
you know this is the stuff that's well
tested in my code base and probably if
you're in a similar situation to where I
was
it's probably also well tested in your
code base so these are kind of easy mode
tests right these are things which like
I said you put them in the code you test
them once they never change
and this is kind of what you find in all
the unit testing literature you know
these are the examples they gave me
you're like well fine that's that's fine
sure I can write you a test for that but
I don't need to this is it stuff I need
tests for write large stuff stuff in the
lodge as it were high dependency stuff
in particular match making algorithms
which as I said are one of the more CPU
intensive things that the bound that
back-end does and and along with that is
this kind of queueing load bouncing
algorithms these are things that deal
with real world data maybe deal with a
lot of it and you know I can write them
on my machine and I can test them only
so far but then the production Hardware
scales up to you know millions of
players maybe I can't run millions of
players on my machine where do I start
testing these things so there's really
no magic bullet right and this is how I
started out I started out just you know
doing the legwork
I wrote tons of mocks I said that loads
of data structures for tests you know
and that worked that worked fine
it was a lot of code to take care of and
test code is code and like any other
code it needs debugging but then I
started to think and I started to kind
of figure out new ways to do things and
along the way of doing all this stuff
and learning this stuff I found some
better techniques and I found some
better code structure just a little bit
so my first problem which for some of
you may be a familiar problem is I'm
working in the legacy code base and
there's lots of classes here which are
kind of large and you know they've been
around a while they've grown so here's
an example an example I want to share
with you so this is a a present the
implementation of a presents channel
president's channel import is a class so
a presence channel is a channel that
delivers kind of up to the second
information about what your friends are
doing what games they're playing you
know kind of what they're doing presents
in the social sense so presents channel
in pulled arise from channel info which
itself derives from channel base so the
summary of there's an interface class
and an implementation class and then
channel base derives from this thing an
RPC implementer templated on the channel
protocol type so so right away you can
see just from the hierarchy of this
class there's a mixing of concerns we've
got RPC way down at the bottom and we've
got the channel interface and a fairly
sort of standard hierarchy sense and
then the input and then the presence
Channel so any logic that the channel is
doing in the present channel in pull is
you know mixed in the same class with
the RPC interface logic so if I want to
test this where do I start so let's look
at the constructor so the constructor
takes this thing called an RPC
dispatcher which is a fairly you know
large interface class itself and then it
takes a thing called a channel delegate
the delegate is the thing that kind of
manages permissions for the channel and
then that's a whole load of what might
be a whole load of configuration and
this constructor has what one two three
six arguments and at least three of them
there and maybe more have very wide
interfaces so you can see just but and
you know and in addition it derives from
this three for deep hierarchy so the
upshot is that if this is pretty hard to
mock as it stands here's another example
this time the achievement service
the class hierarchy not so deep here but
again I think the B net achievement
achievement service is something
protocol related and we have this thing
called a static data loader which
implies to me that on construction it
might do some kind of i/o and that's you
know that's gonna be something I need to
figure out and then the construct
arguments well it's only a two hour
constructor but one of those arguments
is a my sequel database that might be
quite a wide interface and the other
argument is this thing called a server
helper now server helper is interesting
because when we found we were writing
more and more of these service classes
naturally we thought well we don't want
to repeat stuff we gather together stuff
into a server server helper to help them
out server helper has a twelve-hour
constructor and it it I mean it works
but it's sort of rather than fixing the
problem it sort of just sort of put a
rubber stamp on the problem right so
said yes it's okay to do this and here's
how you do it
okay so these things suffer from
patterns which are enemies of testing
right there's there's no dependency
injection they could be doing a lot of
work in the constructors the
constructors are taking a lot of
arguments and the things that you passed
the constructor have wide interfaces so
this is a problem so I started thinking
about you know traditionally we have an
interface in pull split when I started
thinking well what do I really want to
do to separate this stuff rather than
having interface and implementation what
if I structure my class so the base
contains all of the logic that I can
test and then I just shunt all of the
interactions down into the drive class
and I thought about this for a while and
I decided to try it out so and and in an
intervening inheritance sense this is
actually quite similar to what it looks
like in a compositional pattern right
we're kind of used to this composite
compositional pattern in the games
industry where you would quite naturally
put logic in components and then your
entity class will tell the interactions
but it seems like we don't do it
in a hierarchical sense so I thought
maybe doing it in the hierarchical sense
might give me a step towards testing and
at least something to start with in
terms of segregating the logic so I'm
going to give you this example of how I
split up game queuing logic so the
Gamecube base is the thing that does all
of the game queuing and that means that
so we've got a limited amount of games
that we can run at any one time so
that's one thing right so when people
come in and look for games there's a
certain number of servers that can run
games we have to kind of control a how
many games are running in terms of
server capacity but also the rate that
games are started because even if you
can so even if you can you can run so
many games but the low profile of
running a game is different from the low
profile of starting a game so you might
be able to run you might have enough
server capacity to run a thousand games
that doesn't mean you can start the
thousand games at the very same second
in addition the queueing logic has to
take care of things like different data
centers maybe people from different
regions or different countries playing
together in some cases putting them in a
more advantageous data center for their
ping time so there's a moderate amount
of logic that goes into game queuing
anyway so I tried to split this up in
terms that we saw before logic in the
base interactions in the import so
here's the game Cubase the class stands
alone doesn't derive from anything no
RPC mixed in there no nothing so that's
a good sign
construct as five arguments which is a
little more than I normally like but
four of them are just callbacks which is
very you know one function interface and
the server pool interface is is just the
thing that gives the game queuing logic
information about the server load so
it's not a particularly large interface
and it's it that's the thing I want to
mock for my tests anyway so I want to be
able to say oh my servers are fully
loaded or they're not whatever so that's
a good start and and the the member
functions it has
and I should point out that when you see
three dots in the thing it's just
because I emitted it for the slide it
doesn't mean that varargs function the
member functions they're just kind of
what you would expect a cute a cuter to
do right push pop remove something from
a cue from the middle of a cue kind of
thing if players drop out and you know
polling them because it's on the server
site so that's good this looks like
something I can test now and it was a
fairly easy thing to separate that logic
and jumped it up into a base class and
now here's the info naturally it derives
from the base and it derives from the
protocol so it's going to be the thing
that handles all the interactions the
protocol we've got the protocol handler
functions where players come in and come
out because we're running on the server
we got some server events you know
there's some interactions on the process
level there's there's some config now
notice if I go back to game key base I
don't actually have a call here to to
configure it
well the only important things for test
is I decided that when I construct
something I shouldn't have to massively
configure it afterwards now although you
know the game cool does process some
config and it takes care of that
interaction and then we've got more kind
of queuing stuff that just take care of
ticking it around on the server so
here's another here's another example
this time in a compositional sense so
the game master hold the game master in
pull here is the thing that takes care
of once players get through that queue
then we actually have to make games
right so the game master is the kind of
thing that holds a protocol for making
games and how does it make games well it
delegates to a game Factory and so there
are factories for each kind of axis of
games that you could choose so there's
one factory when you play for example
Diablo 3 you say I want to play this
this act on this difficulty and there's
a game factory for that and if it were a
different difficulty a different act
it'd be a different game Factory if you
were playing you know hearthstone a
competitive matchmaking game versus
Diablo 3 a cooperative matchmaking game
again that's a different factory so the
factories the things that actually do
the work in terms of them
making logic so here's the factory again
constructor very small interface this
time even smaller than before so pretty
much just a version a program ID which
is what what game it relates to and just
an ID it does have a configure call but
it's separate from the constructor and
again if you construct one of these
things it's pretty much ready to go and
again pretty much what you'd expect in
terms of an interface to a factory so
you can register players to try and
match into a game they can drop out by
unregistering or if you know you know if
you want to join your friend in the game
that's when you say I know the game or
on their join because my friend told me
and that's join game now the Impala is
where all the interaction happens so
here we see you know all the hard stuff
this stuff separated out so I can test
this stuff is the stuff I shunted down
into the interactions so if you know if
we get disconnection this is a
instantiate factory is effectively the
configuration call that we saw before
and then the protocol level kind of
handlers that you know that can be
called from the outside world so this
turned out to be a pretty successful
pattern this this logic in the base
class interactions in the implementation
class it was fairly easy to apply to
monolithic legacy classes because all I
had to do was like shove stuff up shove
stuff down plum together some bits
wasn't too hard
dependency injection we all know is a
good thing for testing and it and it
makes things testable and at the end of
the day some testing beats no testing so
you know if you can apply this just to
one or two things you're already getting
some we're already getting traction so
you know that's that's kind of learning
number one constructors with small
argument small number of arguments using
narrow interfaces is it's pretty
important for testability
all right so let's get on to some more
interesting stuff that was kind of a
warm-up so here I am I'm let me go back
a slave Here I am
I'm I'm developing let's say matchmaking
and I'm wondering what's going to happen
you know I write this algorithm I've had
this data structure I'm wondering what's
gonna happen when a million people come
and start matchmaking into a game and
you know here I'm sitting on my machine
I don't I can't spin up a million people
easily although you know we do do load
test but that happens on the cadence
which is not like a compiled cadence I'm
a selfish engineer I want to be able to
see my code works quickly on my machine
you know in the kind of compile time
cadence sure we run overnight tests and
much more stuff that tests the system
level but I I want to know if my code
can scale so there's different solutions
for different sized data sets right so
if you're working when you do set it's
going to be like in the thousands it
almost doesn't matter what algorithms
you use it's pretty much all about the
performance on the machine it's about
being cash friendly it's about being
nice to the Machine and the algorithm
almost doesn't matter if on the other
hand you're at the other end the scale
if you're a Facebook or a Google and you
have to deal with billions data sets in
the billions
then then kind of algorithms are king
right and and you have to get things
right by construction or they just don't
work but but battlenet is in this kind
of million land where it's kind of
tantalizingly you can you can be lazy
and get things wrong and have things
blow up because you know a million
things in an array is not a large number
these machines can easily handle it and
performance is important right
performance is important okay it can get
you two orders of magnitude but it's
unlikely to get you five orders of
magnitude so even if you write something
which is really cash friendly if you get
the algorithm wrong and the million
people show up bad things might happen
so the idea I had here was you know
I realized that actually data structures
and algorithms were important too for
efficiency and not just that you know
cache friendliness and all that stuff
like I can actually measure on my
machine so what can I do well I can time
the tests and this is again this is easy
mode and and if I'm optimizing time
tests give me some idea of if I'm
optimizing if I'm if I'm doing well
right but fundamentally my machine is
different from the machine these things
are going to run on in production and
it's I can't tell therefore from a time
test a simple time test on my machine if
it's fast enough I can tell if I'm
making it faster and that's a good thing
so I use time test for that but fast
enough that's that's a different thing
so I I started thinking about this and
the other thing about efficiency is it's
really easy to lose so even if I write a
data structure right now rhythm and I
can be sure that and I've reasoned about
it and I've thought about it and I can
be sure that it's you know the dividing
line really is I want to be sublinear so
login or less right all the RAM is not
going to cut it with a million million
games open a million players on service
even if I do that today and I commit it
and I could review it and I check it in
you know next week everyone's still
hacking on features maybe someone needed
to change that algorithm up very easy to
accidentally put something in which
turns your nice algorithm into order in
so I thought well like you know what I
really need here is a way to test for
algorithmic efficiency and I can
recommend a long commute two hours a day
on the 405 helps you think and so I
thought I was thinking about this and I
came up with this really simple idea and
it seemed so simple that I wondered if
it could work and if it would work I
wonder why everyone wasn't doing it and
so so what does it really mean when
something is all there in right it means
that if we run it once on the input of
size n it takes some time call it t1 and
then we run it on the input side KN it
should type should take K times as much
time I mean to a first approximation if
we forget about the smaller term
I assume that the other end term
dominates that means that if we run it
twice on different sized inputs and then
divide through the divide through the
times we should be able to tell
independently of how long it actually
takes well they solder is at as four n
so there are very simple calculations
you could do for you know things I'm
interested in really so constant times
log in time order n time and so on and
like I say the dividing line really is I
want stuff to be sub linear in this
world so this idea seemed almost too
simple to be workable but I tried it out
of course timing is hard you know
wibbly-wobbly timey-wimey stuff if
you've been to Bryce's talk or
Chandler's talk my talk is much much
less scientific but you know it worked
for me and so I tried to do something
mitigating for you know things like
cache warming effects machine load and
kind of sensitive sensitivity to the
granularity of the time of function just
like you know many of us I'm sure I've
done profiling and and some like I say
very unscientific statistical mitigation
and I settled on you know so I want my
unit tests to run quickly so I can't
make these numbers too big because I
want it to run in the sort of timeframe
that I won't notice you know a second or
two at the end of compile kind of thing
so I settled on you know multiplier of
32 because we always pick powers of two
as computer scientists when an arbitrary
number is is needed and and there's 100
well I don't know take anyway so now I
have this testing framework which is
something like this
which you know runs the test once and my
run function takes how many how many
inputs it should have and then I run it
again and I divide through and the
testing framework there something like
that now the remaining sort of problem
here is how do I make these different
sized inputs so to start with I can just
build them into the test right that's
okay that works again it's a lot of test
code and it means that you know you've
got to put your timing inside the test
you've got to make sure your timing
doesn't account for your set-up time
inside the test you gonna do a bunch of
stuff
it ends up bloating your tests
nevertheless it it amazingly it did work
but my typical test for something for
something like this matchmaking would be
40 lines of boilerplate ten lines more
of the timing code only you know only
five or ten lines of actual code that
does real work so wasn't happy with that
entirely but nevertheless you know this
this approach actually worked enough and
well enough to give me some confidence
shipping and I think we did ship Diablo
3 when I was kind of in this mode so I
knew at that point from doing this that
matchmaking wouldn't blow up with a
million players and I lived with this
for a little while but I'm lazy and and
the code this has to be maintained I
didn't want to maintain it all the time
I want to be able to write more and not
keep bloating so the thing is in my
spare time I'm a student of Haskell and
Haskell has this thing called quick
check which is properly based testing
which generates test input automatically
for you it's kind of like magic in
Haskell it's less like magic in C++ as
you'll see but so the idea of property
based testing is that instead of saying
a individual test case instead of
writing in it thinking out an individual
test case you think of a property which
is true for every input and the testing
framework just generates random inputs
and feeds them to you and you say
whether the property is true so this is
why so I started with you know now in
the practice of unit testing I started
writing this wishful thinking code so I
have you know a fairly standard sort of
test framework which has you know a Mack
Road test thing like this you know the
test name is we
does something every time so what I want
is for it to you know I want to say well
this test is going to take an argument
and I'm gonna assert I'm gonna I'm gonna
test whether the property holds for this
argument and return it so like I say
wish driven development so what does
this actually mean how do we generate
arguments well the way we solve
everything in C++ with templates and you
know in Haskell it's called arbitrary so
in that should I call the arbitrary and
so how can we write this generate
function so for any type for any given
type we want to generate things of this
type so we're going to take a couple of
arguments one is some idea of the
complexity of the thing we're generating
the generation so you know generations
the idea is that we'll go from
generation 0 1 2 and the thing might get
gradually more complex is we sort of as
we go along and then just a random seeds
because we're generating random things
and we'd like to be able to reproduce
them so this is the base template and
then we then we just specialize it so
it's pretty easy to specialize this for
arithmetic types we can we can write a
small function to front-load the likely
edge cases of failure so 0 min and Max
otherwise a random uniform distribution
over the range so it looks something
like this for for integer like types
right so this generation parameter we're
using to front-load
the the inputs or the the things that
are generated so there's 0 min and Max
and otherwise if we're in any generation
after that we just generate a random
integer or you know you can see how the
be trivial to generate random pool or a
char I think I limited the printable
chars for char now I know when some of
you're looking at it slide you're
thinking that crazy guy is putting a
Mersenne twister on the stack every time
it's formatting for the slide in reality
I don't do that
maybe some of you aren't thinking that
but I work with people who immediately
would call me out in there so so that's
what generate looks like for integer
like types right and kind of arithmetic
types and so how would we do it for
compound types for our containers for
our structs well we can you know a
container contains something we already
know how to generate the thing it
contains so it's kind of like an
algebraic data type approach if we we
can write the arbitrary T generation in
terms of the underlying data types and
for a vector it looks something like
this
so imagine T in this case is a vector of
of whatever pull out the value type and
this time so the generation is going to
control the length of the vector or the
string or whatever whatever container it
is right so this is just a kind of
simple you know for the first hundred
generations it's going to be like 10 for
the next hundred it's gonna be like 20
it's a very trivial way to to control
that so the generation gives you some
idea of the complexity of the input and
then we just generate n values of the
contained type using our existing
template that's specialized for entity
types or whatever so specialized a
template in a very similar way for all
the kind of simple types and likewise
for all the compound types and then we
have a pretty good framework for
generating whatever we want so now how
do I now I can you know instantiate that
template tell this to generate things of
a certain type now I need to say so what
does this you know this back to my
wishful thinking code how can this work
so this is a macro and it has my my
parameter at the end here so how does it
expand well typically in the test
framework it might expand to something
like this it makes some struct with a
nonce type a nonce name you know and it
provides operator paren which takes the
parameter here
and then then we need to take apart the
the introspect the operator paren to
figure out the argument of the function
so this is we can do this with a very
simple function traits template so my my
struct has operator paren and so when
you put into function traits it triggers
triggers this instantiation which
derives from this one which in turn
derives from this this one and pulls out
my argument type so and there's a bunch
a meter from the slide is all the
constant volatile qualified things and
blah blah blah what have you but this is
the basics of it right so I can very
easily pull out the type of the first
argument of my struts operator paren so
having done that then I want to provide
so my here's my nan struct again a
different part of it now my test
framework knows how to run tests so
there's a there's a test interface and
it has a run function so my my property
struct my generated struct derives from
test and it's macro its macro provides
the operator paren and then this is a
common to all of them they just they
have a run function which creates a
property and returns property check so
I've discovered I know how to discover
the type I now to generate the type what
I need to do here is basically type
arrays type arrays the struct I've
created I know it has operator paren and
I know it can be run so I know and I
know what type it's operator paren takes
if I type arrays it then I can write the
machinery to generate that opera
generate that parameter and call it and
property is the thing that does the type
of Asia so it uses a fairly standard
type arrays idiom nothing particularly
out the ordinary here so the constructor
is a template which takes its argument
which an FF in this case is is my nan
struct with the operator friend so as a
callable object it Scrolls it away
inside this internal base which itself
is a template derived from intern sorry
internal is the template derived from
internal base internal basis provides
the check interface right and then my
property check ends up just calling
check through that virtual call so
that's that's a basic type erase pattern
and and all the magic happens inside the
internal which at this point has
captured the type of of the thing I
passed in right so when I call internal
check it know it gets the pram type as
before it takes it apart it takes its
template parameters were apart with the
function traits to discover the first
argument type it just decays it because
then it will pass it to arbitrary inside
the check function so and that's what
that's where the magic happens that's
what we call arbitrary generate and then
finally call MT here is the actual non
struck that I passed in and I'm gonna
call it with the correct parameter type
having generated it so that's that's the
basic machinery that I put together so
let's see that in action I do want to be
here so okay for example so I'm gonna
have my property take a string and I'm
just right now gonna print it out and
see okay okay can everyone see that so
if I now run so what we should see is
that that thing will so that machinery
will generate strings and call that
function a bunch I think a hundred by
default so we should see a bunch of
randoms
come out alright there they are and the
first one is actually the empty string
you can see that right right under the
right under the run line there so the
strings it front loads the empty string
which is usually a fairly good idea
alright great so far so good
can generate random things of pretty
much any type so now I can do that well
so here's so here's just a recap for you
guys so the macro takes it takes its
argument and expands to make an odd
struct
then the property type erases that and
internally does the deduction of the
argument type that the function traits
calls arbitrary of T generate to make it
call this operator paren and the rest is
plumbing with the random seed and then
and the number of checks I guess a 100
by default so so this is pretty nice now
I have probably based tests but this
before I go to you know using this to to
generate my input form because what I
want to do remember my ultimate goal is
to test the algorithmic complexity
before I before I moved on to that
there's one more thing that quick check
does and that all good property based
testing stuff does and that is that when
a check fails you want to find the
minimal failure case and so the other
thing that arbitrary does besides
generate is shrink so the idea is that
you've given your test some input and
it's failed right you want to kind of
shrink that input down in some way to
see if it still fails and in that way
get a minimal failure case so shrink in
some way has to return a vector of
shrunken or reduce T's in some way so
you know pretty easy way to do that for
example for Strings is just chop the
string in half so this is shrink for
strings and shrink for compound types
all compound types works pretty much
this way right now shrink for you know
in symbols is kind of more meaningless
but for compound types of what we often
need so as you can see here chops the
string in half just just
/ - and just returns a vector of the
first half on the second half that
that's all it does and then the
machinery the test running machinery
when it finds a fail it calls shrink it
calls the test again with you know the
things that shrink returns and it
basically follows the failure case down
the tree and so I'm going to show you a
little demo of that let's see okay
alright so here I have a test case that
will fail if the string contains an A
right so now there's always the danger
here that the random is random
generation it might not contain the day
weight in which case we'll just run it
again until until it does let's see now
okay might not contain they ah but that
would contain they okay so you can see
that shrink in action so that first
string maybe blow it up if you guys a
little bit so it failed on 8z curly and
a whatever chopped it in half the second
half succeeded so didn't bother
following that case but the first half
failed again you see how it chopped it
down the chop done chapter finally we
get the minimal failure case and you
know so these are the random seed
plumbing like I said before if I do the
same thing again and give it the random
seed then we should see exactly the same
output so that's you know that's also a
nice property I want for my unit tests
that they're repeatable okay great
this this is now getting close to what I
want so now that we can generate all
these things let's go back to the
algorithmic test because remember I have
all these 8200 line tests I just want
delete the code but there's a small
thing I need to take care of so remember
generate generate has this kind of fuzzy
idea of the thing it generates it used
this generation idea so I need a tighter
form of generate well that that's easy
I'll just call it generate n
and instead of doing this fuzzy idea of
a generation I'll use the actual value
of the generation that was passed in so
that you know because when I run my
algorithm ik tests kind of important but
I know the exact size of the input so
generate as you can see is exactly the
same generate and exactly same generate
only difference is that I don't do this
kind of fuzzy idea of the generation but
I just use it directly so now here's a
is a sample complexity test so what I do
in this case is again the parameters at
the end I just have this enum like
should it be old around should it be
other one should it be order log n
whatever and then I do something in a
test that's supposed to be whatever you
know order n in this case max element
that better be order n I think and so
the machinery runs the test you know a
bunch of times
multiplies the size of the input runs
test a bunch more times divides through
does minimal statistical outlier
smoothing like I said not very
scientific
compared to Bryce and Chandler but it
worked for me and then basically figures
out which which bucket the input should
be in because it knows ahead of time you
know if it takes if it if if the
multiplier is or if the you know if the
divide through factor is this then it's
all one if it's close to this it's login
it's and etc it's pretty simple code
just just figures out the kind of
closest thing that's on there and it if
it's you know like if it's for example
if I say it's all there in and it turns
out to be all the login well that's
still a test pass and in fact cause for
celebration perhaps the other nice thing
about this is of course that normally we
prop when we profile we profile in in
release mode without debugging but I can
be pretty sure that if I turn off
debugging the you know if I run in
release mode rather than debug
optimization actually is only gonna make
things better I hope it doesn't make
things worse and so if I if my
algorithms or the N or the log in in in
debug I can be pretty sure it's the same
in in release like and in the sense
I can be more sure in debo because the
compiler isn't like pulling stuff out of
my functions and doing weird things like
the code will execute more or less as I
wrote it so that's nice too so here's a
before and after though you're not
supposed to be able to read this this is
just for show up for for you know a
sense of it but this is this is an
example of a test I had this is I think
about 80 lines and you know if you
squint at this maybe you see there's
there's there's a bunch of stuff so
before is on the left after is on the
right so first of all I can pull out all
the code that generates there's just
there for generating stuff I can pull
out code that time stuff because now
that I'm not doing the generation in
there I don't need to shove the timing
in I can do the timing on the outside
because the tests only doing what it's
supposed to do and you know and there's
some other refactoring I need to get rid
of other stuff just because stuff got
simpler and so now what was 80 lines is
now about 20 lines and most of them are
actually doing work which is good so
there are war for good workers more
workers they say so this is kind of
where I am now there's some so as I've
learned dependency injection is is what
we need for testing you know that that
is already known but separating logic
from interaction in this kind of shove
logic in the base class way and shove
all the interactions down makes the base
class makes my logic much more testable
keeping my constructors small keeping my
constructors interfaces small very
important so I still have regular tests
for my like my normal cases where I can
identify what might go wrong I still
have time tests for when I'm optimizing
that give me some sense of am I making
things better but now I also have these
property based tests or invariants and
they take it to be honest they take a
little bit of practice to write because
it's you know it can be challenging to
figure out what's the actual invariants
but it can also be rewarding to figure
out you know what are the invariants of
my of what I'm doing and now I have
these you know now that I can generate
stuff I can have these algorithmic
complexity tests so I know that my
matchmaking algorithm will not blow up
beyond all reason when a million play
come on to the service so what's the
future what what's that well Alba tree
is kind of a lightweight in process
first testing you know it's nothing like
as robust as as as cautious lip buzzer
but but it's something and you know the
kind of important thing about all this
stuff is it runs fast it runs when every
time I compile it's you know by
engineers for engineers and a very
selfish sense is what I like about you
know being an engineer it struck me that
since I am kind of generating things the
way in which I generate them form some
kind of walk through the input space of
my function so there might be
alternative walk strategies that I could
use I haven't really looked into that a
lot yet but it's something I have on my
mind also it's been suggested that
because I have this algorithmic
complexity testing I can actually I can
see when certain inputs you know trigger
really bad run times and I can capture
that kind of 99 percentile sort of stuff
and I'm still lazy what was I gonna say
about the 99% off somebody somebody at
work mentioned to me that that was a
good a good use case I think so
that's that's in the main part I have
some bonus slides I can show you since
we got a little bit more time so people
see this and they say what they say so
sometimes I ride a unicycle around the
office where people see me riding a
unicycle what's the number one thing
they say oh can you juggle as well
and that and I get off the unicycle I
say why don't you try and then and then
they go away they see this and they say
oh that's cool can you do multiple
arguments so I didn't start with but it
turns out you can do multiple arguments
because we have var Arts in macros now
and so I thought looking at this so once
again you know here's my is my property
macro I've got virag so now it expands
to give me an operator friend with
multiple arguments so now how do i how
do i take that apart well now my
function traits can can look at the
tuple of arguments so so this is y do
now so I get the tuple of arguments
instead of instead of you know just the
one argument and then I have this apply
in unpack apply which is pretty much the
same as experimental apply you know am i
compiler I don't yet have experimental
apply but this is kind of trivial to
write and more or less the same so in
particular when you call apply it
creates the index sequence for the
parameter Peck and then you know unpack
apply it effectively calls the function
and expands that parameter pack for the
function arguments so doubtless if
you've been to one of the I don't know a
million presentations that mentioned
something like this you all seen this
and then so now now that I have this all
of my property tests effectively take
tuples as arguments and so now I need a
way to shrink tuples all that other
shrinking stuff will happen still on the
leaves of my kind of shrinking tree but
I need a way to to shrink the tuples so
I had already written you know shrinking
four basic types and integral types and
compound types
so before thing about tuples I thought
about pairs because they're kind of a
simple version of tuples I thought that
might be so I went back to my
implementation of shrink for pairs all
excited to do this and they saw a
comment that said to do
so so I went back on my commute it took
a few hours on the 405 and I thought
well I could shrink pair just by
shrinking the first item and shrinking
the second item and then I thought well
should I do a Cartesian product of the
vectors in the pair's or something like
that should I do something like
applicative for lists you know I'm I was
thinking haskell at the same time so
thinking quick check so so here's what I
came up with I came up with not doing
the Cartesian product but just but just
shrinking the first thing and then just
putting that together with the unshrunk
second thing and then doing the rust so
shrinking the second thing and putting
that together with the unshrunk first
thing I just returned that so that's the
way I shrink pairs make one half shrink
the other half the machinery takes care
of kind of getting me a minimal case so
now that I have pairs and thinking about
okay that's great how about two balls so
naturally I go to cpp reference comm to
see what's available for tuples and I
see make tuple no that's not the thing
Tigh well that's interesting but no for
this tuple no get sure but no tuple cap
wait a minute
tuple cat what can I do well what I
really want if I'm sticking together
first in a second well they really want
for tuple list is you know sticking
together head and tail kind of thing I
can shrink the head and I can shrink the
tail if I do it recursively because I'm
shrinking a tuple and the tail is tuple
so I have a coachable cat and what I
really want are tuple head or I have
that already that's get of zero tuple
tail which I don't have yet and tuple
cons which could be implemented in terms
of tuple cat but I kind of went for more
functional programmer purity elements
use cheap or cons so I thought well I
should pretend these things exist so I
can write to shrink for tuples and
shrink for tuples looks just the same as
shrink for pairs we just we shrink the
head and then we stick it weakens it
onto the the tails unshrunk we shrink
the tails and then we come
it onto the unshrunk head so this is
exactly the same as we do four pairs so
all we need to do is write tuple well
we've got two per head we just need to
write tuple tail and tuple cons what
tuple comes is is trivial to pecans is
very similar to to make a tuple I think
and it uses this pattern where you just
make the index sequence for the tuple
and you call through and you call make
tuple having unpacked the existing tuple
so two becomes very easy to pull tail
only a little only a little more
difficult but by now I was getting the
hang of using index sequence which I
think by the way is awesome and one of
the most amazing features of 0 plus 11
or 14 so to make a tuple tail what I
want to do is take an index sequence one
less than the length there tuple and
just shift it by one and that's what
this code is doing so you can see the
top function here makes a index sequence
which is one less than the tuple length
and then the bottom one expands it out
just adding one so instead of 0 through
I guess n minus 2 we get 1 through n
minus 1 which is exactly at the table
that we need so shrink works I take my
shrunken heads and I put them onto the
normal tail I take my shrunken tails and
I put them onto the normal head cons I'm
on and that's shrink for tuples just the
same as shrink for pairs and now that's
pretty much where I am and you can look
at the code at your leisure it's on
github
and thank you and I'll take questions if
you have any
yes I run them locally so do I run them
during the complexity test locally or in
the continuous integration system I do
run some on the continuous integration
system with complexity tests they tend
not to be I tried my hardest to make
them binary pass/fail but as you can see
there's randomness in there I just from
a sense of safety I tend not to run them
on the conditions to continuous
integration but I was running them
constantly as I was developing the
system you know because like I said it's
so easy to accidentally accidentally get
the wrong complexity in there that to be
honest they're pretty safe you can make
them pretty safe you can make them
pretty binary pass/fail I just really
didn't want to take the chance of of
blowing a unit test just owing to some
random randomness overnight yeah
or like collecting statistics on
performance is that a thing
Blizzard does and can you talk more
about that well we have a whole load of
telemetry that we do ok yes so you're
asking about if we collect statistics on
performance we do we do do a whole load
of telemetry we also have like I was
saying the top of the talk a whole
system this is my kind of selfish
engineer's test to my machine so I can
be sure about my code but we have a
whole team that does integration tests
and they they wrap things up with
c-sharp
in fact I've corporate github appliance
thinks that bound that is written in C
sharp because the number of lines of
c-sharp test code actually outweigh the
number of lines of C++ code but but yes
we do do a bunch more testing yeah in
the back I think this is probably just
answered my question I was wondering how
we would ensure that the test you had a
code that you were refactoring to make
testable on me how did you make sure
that how do you ensure that you retain
the functionality right so how do you
get started testing because you don't
have tests to ensure that your
refactorings still preserve the
functionality well I don't really know
the answer there except it's better
after you have tests let me put it this
way before you have tests everyone's
refactoring everything willy-nilly
anyway so your refactoring to put tests
in there's going to be no worse than any
other particular development that's
going on from day to day we all yes
thank you we also have testers and they
you know
the second line of defense off the unit
tests third line after unit test and
system test so that you know we have we
have QA personnel they they test it all
the time we have lots of players who
test it although we try not to make them
test it yeah
can you show some sample code about how
you set the expectations on your mocks
like how do you make it return different
values and stuff like that like you
basically said like here's how you do
your link time substitution or your
inheritance substitution to get rid of
Io based stuff but like how do you
within your tests like say you know this
network socket does this this and then
this okay
can you show a kite example either no I
can't like I couldn't say we really do a
whole lot of that in fact yeah yeah I
mock my MOOCs are more or less
hard-coded I don't use any particular
mocking framework I don't use anything
that introspects the classes or the
executable so is your test API when you
pass the screen as a parameter how do
you limit like size of the screen when
you generate a input food how do I limit
the size of this Jo that's so remember
the generation parameter that we hear
right so the jet so to start with when
you call the executable you can by
default it will generate 100 checks for
a retest and you can tell it how many
checks you want to generate and this
generation parameter G just
monotonically counts up from zero so the
length of the string in this case will
be ten for the first hundred generations
then twenty and thirty there's no
there's no code to to particularly limit
it to any maximum size because there
doesn't need to be I wouldn't you know
if I ran it with under millions and
millions of generations then I might get
some long strings but Oh
so there's not the kind of thing I would
do because that might take a long time
okay and from API perspective when
unified we define our test like if you
need to generate container with welcome
so you have to have two parameters I
guess one container itself and second to
define size of container so say your
test takes input STD vector and well you
need to define so 100 is a well each
it's basically everyone has a test
separate test input while I'm talking
about just input that just a test vector
with some number of elements I'm not
sure and that the driving point of your
question well the way understood luck
with STDs Trina when it takes s to D
string each element is a test input
right in your solution when you generate
a string if you have if your test takes
a string then a string is generated yes
so test input is entire string or test
input is Erlich the testing but we want
to if your test takes a string the test
input would be the entire string all
right okay okay okay thank you a
question I sometimes get is you know how
do I set this up for my own classes and
so it's a matter of specializing
arbitrary and implementing generate
which is typically not too difficult
an interesting thing arises if you have
a function that you know that you say
you're going to take an integer but you
know the integer is bounded
unfortunately there's no easy way to do
that just with an integer in C++ and so
sometimes I would I would wrap the
integer in the struct whose constructor
made sure that the integer was bounded
and they don't wrap it in the test or
something like that but go ahead with
your question
um so your tests actually test the
asymptotic complexity and then from that
you are guessing that at scale it would
still work because you know what's the
asymptotic complexity at smaller scale
right that is the idea so how do you
account for the constants you know
imagine that you have an algorithm that
you don't know
asymptotic complexity and then there the
real asymptotic complexity behind the
algorithm could be an plus let's say N
squared divided by a large constant and
then for smaller numbers that is where
it would never be significant factor but
at you know like when you deploy and
then you have business of users and then
s squared would become you know a
dominant right right so somehow that
works I try and set my tests up on my my
test multiplier up to to elucidate the
dominant term basically and I you know
good enough is good enough I try and
forget about the rest that any constant
multipliers come out in a division
anyway so phrase so on this slide you
know so if something is all there in
technically that means it's a n plus
some other terms smaller than n right
but I try and set up my test so that
those other terms won't matter and you
know it varies a little you know there
are tweaks I make here and there to my
test but it's something and something's
better than nothing and good enough is
good enough so if I'm sorry if that's an
unscientific
consensus I think that that's fine if
that other Fame is a constant but not if
that other thing is dependent on em like
and so imagine so if it was dependent on
n if it dominated it would come out as
an over N squared and the other end for
instance it would come out if there n is
large enough so imagine n class and
squared divided by C where C is let's
say a very large constant that is then
you're a catarrian question I see what
you say dominated by M because the C is
a large number so to combat that kind of
thing I guess I I still do code reviews
and you know because at the end the day
this doesn't replace the fact that I
still need to know everything my data
structures and algorithms are doing that
those those this is about really
catching unintended changes and those
kind of the kind of scenarios yours
you're talking about haven't really
occurred for me okay thanks cost yeah
yeah yeah I mean the codes on github
it's under an MIT license I'm not saying
it's anywhere near production-ready but
it works I'd be interested to take any
pull requests anyone has that's cool
there was lightly talked about rapid
check apparently I haven't looked at
rapid check but yeah this is I mean
right so this I think this is an idea
whose time has come maybe languages with
lots of other languages have property
based testing C++ is relatively late to
the game other languages tend to have be
ones that either have reflection I'll
have strong type systems like Haskell
but I think C++ is type system is good
enough as I've shown so so yeah I'm
excited to see this kind of new things
in in testing for C++ yeah Frank from
Spotify our colleague who words rapid
check libraries and in this talk but I'm
sure it would be really interesting to
talk to you about his library how you
implemented his and to compare with
yours because they are really similar I
think in design and how they work and
they are doing the same shrink of test
data and a lot of things like that so it
would be really interesting to compare
but diversity is good absolutely Thanks
I think our time for one more question
I've got the last question again so have
you noticed any change in the way
software is developed at your company
since you've done this as a result of
engineering culture is hard to change
and it's something I'm continually
working on I couldn't say I've noticed
any sea change overnight but you know
people are definitely interested in
testing their code so that's a good
thing all right thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>