<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CppCon 2015: Pedro Ramalhete “How to make your data structures wait-free for reads&quot; | Coder Coacher - Coaching Coders</title><meta content="CppCon 2015: Pedro Ramalhete “How to make your data structures wait-free for reads&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CppCon/">CppCon</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CppCon 2015: Pedro Ramalhete “How to make your data structures wait-free for reads&quot;</b></h2><h5 class="post__date">2015-10-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FtaD0maxwec" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Bill Drew I work for Cisco
in Switzerland and today I'm gonna show
how to make your data structures I hope
that's better yeah that's better
even I can hear now okay so I'm gonna
show you how you can make your data
structures with free for reads but
before we do that we need to understand
what is weight free what it's used for
what are its properties how it compares
to walk free and blocking and so we'll
take a little bit of time to to go over
that before we go into the really good
stuff so this is like the outline of
what I want to go through today talk a
little bit about with the writer Locke's
copy-on-write and then the left/right
pattern and how to use it
so progress conditions progress
conditions come in many flavors this is
a really nice schematic from Maurice
healthy and HIV if you don't know who
these guys are
those are the ones that wrote the heart
of multiprocessor programming and they
have this white paper I guess on the
nature of progress where they talk about
about this stuff and today I'm going to
talk a bit about some of this some of
the things like starvation free walk
free and the different types of weight
for you I'm not going to talk about the
other ones they're not relevant for
what's coming next but you can go and
read their paper if you want to learn
more about this so if you kind of
represent the non-blocking
conditions in progress of initiatives on
an event diagram it gets something more
or less like this and the interesting
part about this is that whatever weight
free whatever lock-free provides or I
have a kind of guarantees lock-free
provides which free will also provide
those guarantees and more and whatever
weight free unbounded in this case
provides where it rebounded will also
provide any more so that that's the idea
we're trying to pass here
and these are the definitions but we'll
go over each of them now the first thing
that most people get confused a little
bit about is that progress conditions
are not about time they're about the
number of steps it takes to complete
some kind of tests in that kind of work
and this is particularly confusing for
wait three because the word way it means
some implication with time is like
you're waiting for something or so if
it's wait free you are not waiting so if
there's like no time or something but
that's not directly what the definitions
talk about the definitions talk about
the number of steps needed to complete
some task so it we have to be careful
about that and another thing is that
when we think about these programs
conditions lock free we think of if it's
lock free then it's very scalable that
might be so but not necessarily what it
does have a lot of strong implications
is on latency how luxury and weight
through work and we'll see a little bit
just a tiny piece about that but this
has very strong implications about
latency so if you care about latency you
have to care about this so the most
simple that everybody knows is blocking
when something is blocking the web to
wait and you might wait indefinitely
now just because it's blocking doesn't
mean it won't scale in this particular
example we're doing you know some
computation that can be done like
parallel way with some data that belongs
just to this thread and and when we
finish this computation we put the
result in sum in this case we just add
so whatever but we keep that result
that's going to be shared now you can
think how how many threads without be
able to add to this and still scale
something and up to which point I would
be able to scale now if you think
blocking now it's blocking so it won't
scale or anything
well that's not true it really depends
on a thing called Amdahl's law and in
this particular case I don't know if
anybody wants to take a guess how much
this would scale you know you have five
times more the parallel side as you get
on the serial side which is you know the
lock so you could be it should be able
to scale up to five threads let's say
something like this I'm not gonna go
into anvils law but it's just to show
that just because it's blocking doesn't
mean it won't scale it will scale up to
a certain point and then it will reach a
plateau or maybe even start to go down
so who is one suppose you have two
methods they're very similar and the
only difference is that the part that's
nonzero why's the part that's not
protected by the mutex which is
completely independent take slightly
different times in proportion to the
time it takes to say that say that theta
so which one of these two methods is
going to scale better or let's say it's
going to scale up two more threats so
that means you can add more threats and
get more work done be everybody's Purvi
I guess okay so the answer is yes it's B
you add more threats
you can add more threads and still get
more work done now funny enough for a
and B you get the same kind of work done
which is because they have the same
bottleneck within this case which is the
mutex but but yes the answer is that so
just keep that in your mind when when
you think about blocking and scalability
at the same time what that implies
so now inside blocking there are several
properties the one that I'm more
interested about is starvation free
starvation free has strong implications
on less latency and the thing about
starvation free is it gives the
guarantee that progress is going to be
made as long as the other threads are
making some kind of progress as well so
if the other guys are making progress
the other item you can progress
eventually your thread is going to make
progress as well this is an example of
something that is not starvation free is
just a spin walk I think fed are shown
what showed a spin lock using exchange
this one uses comparing exchange but
something very simple and and the thing
is you have this variable that's being
changed from 0 to 1 as the walk is
acquired or released and yeah you can
have a thread that's waiting
indefinitely for the lock because
whenever it tries to see oh it's already
unlocked like me to the cast but then
some other thread comes in and does it
before and then I okay now I'm going to
yield and wait a little bit and then try
again and by the time it tries again
some other thread jumps in front and
this keeps happening theoretically
forever so this one is not starvation
free this is actually very prone to
starvation an example of a lock that may
or may not is here this is actually
called the ticket walk so for those who
know walks the answer to this one would
be very easy so basically we have two
variables two Atomics
and the first thing we do is do fetch an
add on the variable ticket this will
give you a ticket
that's a numerical value that to keep
and your the what will be yours once
once the grant becomes equal to the
ticket so it's two counters that are
monotonically increasing basically
that's that's what's going on
and then if it's not UART your time to
take a walk then you wait you reeled for
a little bit until it is anybody wants
to guess whether it's barking starvation
free or non blocking blocking starvation
free until one okay nobody wants take a
risk that's why so the thing is it is
starvation free at least on x86 because
the fetch and AB operation is a single
atomic instruction so there are no
retries you do it just once and because
you do it once now you have a ticket and
that ticket is the numerical value of
when is your time when is the time for
this current threat to go in the lock
and that means that no other threat can
pass in front of you eventually it will
get to your ticket and all the other
threads will be yielding waiting for you
to take the turn the end yet waiting for
that threat to take the turn and then
it's it so there is always a finite
number of steps which in which in this
case it's called the linear wait
starvation free because each thread has
to wait at most for entrance and minus
one dress and there are many examples of
starvation free locks
besides the ticket box CL h TDX which we
discovered and now you have lock free
now the definition of luxury is is it
written there so I'm gonna read it
because I don't know what my heart
method is lottery if it guarantees that
infinitely often some thread calling
this method finishes in a finite number
of steps now an easier way to prove that
something is lock free or to understand
which is a subset of what's written
there is to just say that if you have at
a thread calling method and it's and you
can guarantee that at least one is going
to complete in a finite number of steps
then it's not free it may not seem like
much but it is a really strong
conditioning very hard to guarantee in
practice now the thing about lot free
that maybe not most people don't realize
is that lock free implies the
possibility of starvation if you have a
method that's lock free it can be
starved now in practice if it's a good
algorithm walk free algorithm this will
only happen for very high contention
very high number of threads and cores
but it can happen so again keep that in
mind
especially when compared with starvation
free so it's like yes starvation tree is
blocking but it won't starve lock-free
is non blocking but it can starve so you
need to keep that in mind it has strong
implications and this little bit of code
is just the insertion in a queue or list
using an egg Michael and Michael Scott's
algorithm for a lock free so basically
you just have a linked list and you're
trying to put in the end than your note
and if it fails you do that with a
compare and swap and if it fails you try
again and you try again which argument
so radically it can go on forever but
it's luxury because the only reason why
it might fail is because some other
thread has put and you know there so
some thread is doing work that's okay
it's walk free
now wait for it goes to step beyond that
and gives you the guarantee that all
threats are always making some kind of
progress and in fact they're going to
finish that call the execution in a
finite number of steps now so far no
academic has proved that wait free
always gives better performance than
walk free and in fact under no
contention so just a single-threaded it
seems that the other way around is is
choose that lock free provides better
performance throughput
let's say - better throughput and weight
free but not on their.i contention for
sure so a thread making thread with
weight free program with a calling a
method that's weight if we will always
make progress if if the thread scheduler
gives its CPU time of course otherwise
no progress will be made and it will
never starve so there's no starvation
and like unlock freedom so one thing
when you think about great free that's
not obvious is that weight free is not
free from waiting like I said at the
beginning there's no time associated
with this so it even weight free methods
still depend on how much slice is get
from the thread schedule or how much
time the thread scheduler gives it to
execute if you have if you're running on
a low priority thread and you keep
getting preempted by higher priority
threads it doesn't matter that you're
running your weight free method you're
never gonna run so with it it's never
going to complete so you cannot
translate the weight free progress
condition directly into time unless you
have some kind of guarantee from the
direct scheduler that you're using so
that the task manager whatever you want
to call it so weight free and time is
not immediately the same thing and then
you have the best kind of weight 3 which
is population of weight oblivious which
means it's a finite number of steps and
it does not depend on
of threats examples of this can be given
where the tommix for example doing the
simple load is always weight free and
the simple star is also always weight
free population where it is doing an
exchange or fetch Annette is weight free
population oblivious on x86 because it's
a single atomic extra instruction is
exchanged and the other is X add or lock
and and compare an exchange strong is
also assuming you're not running it in a
loop of course if you're running in a
loop them to be long for you on other
architectures that depends you have to
check so another thing that people
usually say is that how this data
structure is lock free well if you say
that the data structure is lock free
then you're implying that all the
methods in that data structure must be
at least lock free because the progress
condition is something that you
associate with the method not with the
whole class even though you need to the
whole object I would think but it you
characterize it for a method so in this
case I artificially created some class
called cookie and you have three methods
and now I would like you to think about
what are the progress conditions of each
one of these methods let's start with
the easier one get cookie type which is
doing an atomic load on the variable
type which of these do you bet on the
weight free population oblivious any
other bets see okay so in fact it is
wait for your population oblivious
because the atomic clouds are always
weight free operation with oops sorry
already showed you things here so then
you get get total chocolate chips and
what it's doing is it's going over an
array of atomic integers and it's
reading each of them and now the thing
about this array is that it has the size
of the maximum number threads in your
application so it depends on the number
of threads so what is the progress
condition of
smothered very badly
that's right because it's bounded by the
number of threads if you increase the
number of threads you're gonna have to
do more work so it's a higher number of
steps to complete and then you have add
chocolate chips which is doing a load
over there on the chocolate chips
the first entry are in this array and
then just incrementing by one using a
comparing exchange yes that is correct
okay now let's take a very short look
and very light thing on how progress
conditions affect distributions of
latency this is just how the tests were
done but it's really not interesting so
suppose you have you have a lock and and
you do some work inside this lock you
know this work usually takes something
like three or four microseconds to
complete and then you release the lock
or at least the whole package like
acquiring the lock doing the work in
releasing the lock takes about this when
you're running in single threaded now
the thing is if you start adding more
threats to do the same thing obviously
the lock is going to serialize this so
they're going to take twice as much
because you know they're gonna have to
wait for each other only one of them at
a time to do so that means you increase
the latency so you have reached tiny
bits and if you add more it starts to go
three four times so it's more or less
linear if it works okay if the lock is
not too bad but as you can see it's not
just linear there are some other things
going on here and the problem is
sometimes the thread doesn't get it
let's say on its turn it's not a round
robin because if this is a spin lock for
example it's not necessarily
so it doesn't get it on this turn so
it's trying to get it on the next term
on the next one or the next one and you
can get you can have a really big weight
you have to so maybe I didn't explain
this plot very well but on the x-axis we
have time
so how long it takes for a certain task
to complete and on the vertical axis we
have normalized distribution so I used
to be a physicist so we like histograms
a lot so if you have more threats if you
keep on getting more distributed Peaks
and in fact these Peaks are going all
the way up to infinity or well not
infinity because the test so I only ran
the test for 30 seconds so that would be
30 seconds or something but you get a
fat tail in the distribution that's
usually how statisticians call this kind
of stuff and as you can see when you go
to 16 threads this is on a machine with
18 cars so we're still under the the
number of cars that the two team put you
know okay so there's a like less than 10
percent are around here or something but
most of them take a really long time to
complete because you know they're
starvation they're going on so if you
put all of this you see that no there's
a fat tail on this distribution which
means it's not good for latency you want
or you can see that the mark to the left
as possible now if you use the
starvation free lock you still see the
same kind of behavior where as you add
threads you have to wait more but
there's no such thing as these extra
Peaks this extra delay now with eight
threads you get some convolution that
makes it smear but and in this one you
don't see it because it's after the 15
microseconds but I've shortened this one
and you can see it's the orange Hill
there so you don't see the fat tail here
you just see the delay because now you
know that it's it's going to have to
wait for n minus 1 threads so if it
takes 3 microseconds for one thread to
complete you're gonna have to wait
n minus one times three so all of the
threads on average going to wait for
that so you got a peak on I n minus one
times three or four whatever it is so
how about lock-free everybody talks
about like feel of trees right well when
you start getting walk free
I don't know if you're seeing but there
are some little Peaks there
yeah because lock-free is star verbal
you can start with lock-free so what
you're gonna get is again the same kind
of behavior but this time you're not
sliding because it's locked so you can
get stuff done but the tail can go on
again for up to infinity over there to
infinity doesn't actually exist it's
just a mathematical construct but that's
another story
and then you've got weight freak or
weight free and sorry that sounded yes
bounded by the number of threads in this
case all of them are doing work so
because but because it's bounded by the
number of threads the more threads you
have the more work you're going to have
to do so again it takes longer in this
case it's linear with a number of
threads you can have a one that would be
not a one but a log and that would be
great but sometimes it could get o N
squared in depth with grow with that
order so you see the the advance but you
don't see the long tail wait three
doesn't have this this tail and of
course the best of all weight
repopulation oblivious which shouldn't
even slide to the right or anything and
this time it went back which is weird
but anyway it's still more it's always
more or less on the same place because
it's not supposed to take longer just
because there are more threads and there
are no tails as well so that's a really
good latency distribution if you go look
at the 99.999% there does nothing right
there's no
no events there are hunters all of them
are the 99.9% is very close to to the
median website so what this matter as a
kind of summary of that blocking in
lock-free have a fat tail on the latency
distribution wait free and starvation
free sometimes depends on how many cards
you have and reds and other stuff but
those generally don't have effective so
this is just again the same thing wait
three methods have what you can call it
cutoff a maximum latency beyond which
you know that there's you won't have to
wait more than that that's a very
powerful guarantee it's extremely
powerful and extremely important in
motivated scenarios of course okay so
apart from that maybe something I didn't
mention is that memory allocation and
deletion is usually blocking so if you
have an algorithm that is weight free
but it's allocating or deleting memory
well do you really call it weight for
you because it might do something that
is blocking it's a bit of a gray area
and the most desirable kind of blocking
I would say is starvation free because
of its latency guarantees and the most
desirable of all progress conditions is
weight free population oblivious okay
we're a bit behind so our safe questions
for later so how about now that we
talked about previous conditions let's
start to go into the code stuff so with
the writer locks are a way to protect
your data and at the same time they
allow for multiple read-only operations
to be ongoing so this is really cool for
data structures where you're doing
read mostly and and recently there has
been some today so we see you got Peter
ed with the writer lock boosters shelled
mutex C++ 17s shared mutex and 14 as
shared tiny mutex there are many
different kinds of locks we will read
the writer locks we won't go into that
there are some really cool new
algorithms like the cohort with the
rhetoric right preference which scales
extremely well for readers as long as
you're doing mostly read and this is
very recent and it's a paper by the
Oracle guys and we have some
implementations of that because we came
up with almost the same idea at the same
time now if you want to protect data
structure or whatever you want to
protect whether with the right lock you
know in if it's a P thread it would be
like this I didn't have time to put the
new one can you share the mutex think it
could be even easier and prettier than
this but basically you just protect if
it's read-only operation you protect
with a read write sorry with the read
block if it's modification like an
insertion or removal then you protect
with the write lock that's it very
simple so that is one way to make a
concurrent data structure even though
it's completely blocking but it's
concurrent you can access it safely from
multiple threads but there are other
ways one other ways copy-on-write
now most people I think call it read
copy update maybe I'm not sure what it
is is real data structure you have an
atomic pointer that's pointing to this
data structure and whenever you want to
modify the data structure you don't you
just make an entire new copy of it
in private you modify whatever you need
to modify and then you talk of the
pointer that's it and now you can you
have to take care of safely removing
this memory and that's the hard part
usually it's done with our see you but
you can use that pointers as well other
stuff
this scales really well for weeds as
long as you're doing only weeds but as
soon as you start doing a little bit of
Rights well you have to copy an entire
data structure and that's really
wasteful and slow extremely slow of
course and then you have to call RCV
synchronize and stuff like that it's
gonna take ages so how does this work
more or less well you've got a thread
that wants to do modification so it does
a copy and calls it instance two and
then atomic quick changes the reference
this is an atomic reference level or not
a reference urn and then the new readers
can go read the modification and the
right and another thread once in the
right doesn't makes another copy so
there are multiple instances you could
use could theoretically have multiple
instances when you use RC you actually
you can only have two instances at a
time but it it depends on how you decide
to manage your memory and the thing
about this is that it's it has a
property called linear ease ability
which won't go into much more detail if
we have time and it's the same kind of
property as a mutual exclusion walk or
with the write a lock so it provides the
same kind of stuff so this is the good
part
left right so this is a technique that
Andrea and I discovered some years ago
and the cool thing about this is that it
can safely be used in C++ because it has
its own memory reclamation it has some
nice properties one of them being
linearizable which is what everybody
likes and the idea is kind of like a
double buffering technique in computer
system computer graphics if you are
familiar with that
it's like on computer graphics you have
two drawings going with two buffers
going on and you're drawing it in one or
the other one is being shown then you
swap them well this is kind of the same
thing you have two instances of the
structure you're modifying one while the
other one is being accessed by the
readers and then you toggle them and the
readers going the other one and you want
to fight this one after a while that's
kind of the idea now if you want to
actually implement the algorithm it's
simple I'm going to talk a little bit
about that but you don't need to know
how to implement you can just use it if
you want to just copy place the code and
using that we'll see as well how to use
it it's really simple so basically these
are the components of of the algorithm
there's in your text there are two read
indicators which are like counters and
two atomic variables of version index
and a left drive and then two instances
of whatever you're trying to protect
typically it would be a data structure
but it can be anything you want
super data so let's see suppose you want
to protect a stood map so the first step
is making two stood maps and you place
them there I've represented them as
being these little guys here and now
suppose you want to modify well what you
do is you grab the mutex and then you go
on the left right and you see oh that
it's pointing to this side here right so
that means all the readers are coming
here because the readers look at this
variable so I'm gonna modify immediately
that other data structure once the data
structure is modified which is in this
example or it's better we inserted that
the element nine now we want to do the
same thing so that we have mirror of
each other so the writer comes back here
has to wait for the read indicator to go
to zero or go empty toggle that then
again and then can go and safely modify
and release the lock so what it does
it's really simple on the reader side
they don't require any lock they just go
in look at the version index that's an
atomic load they do arrive on that which
is usually just
an atomic store or a fetching ad then
they go and left right and see which
data structure should I go into and they
do whatever read on the operation they
need to do and at the end they go into
the part which is again an atomic store
or atomic Virgina so this is the first
part of the mechanism it's called
arrived apart and talk of machining
white but actually if you think if you
know are see you that what McKinney was
talking about yesterday this is actually
a kind of implementation of that so you
can replace it with our Co if you want
to that would work
okay I forgot about that now you could
the way to do that the way to to
actually use that well not use but how
to implement with lambdas would be with
something like this I'm sure there's a
better way to do that but I didn't have
time to research that
so the actual algorithm is the readers
when they want to apply a read-only
operation they do arrive which is kind
of like our seaweed lock let's say then
they look at the variable left drive and
they applied the mutation sorry they
applied the read-only operation based on
whatever the left right was and they do
the part which is like ours you read and
walk and that's it that's the only thing
they do as for doing a mutation so in
this case it would be like an insertion
in the stood map or a removal from on
the stood map then the first thing you
need to do is acquire a mutex call the
writers new text and then you look at
the left/right variable and you go and
modify the opposite one because the
readers are on the one that websites
pointing to so you can modify apply the
mutation there then call this thing
which is kind of like an RC you
synchronize okay
so that's how you would implement the
background but how would you actually
use it well again I'm pretty sure
there's there are many easier ways to do
this but basically you just need to pass
a lender that's doing whatever you want
to the data structure that you chose so
in this case it's a stood map that map's
integers to some user data class so you
just if you want to do a fine that's
concurrent you just pass mapped find of
the key in this case I want I want I
just want to billion I don't care about
the rest and if you want to do removal
you just called map arrays inside and
insertion is inserted and then you have
to pass this depending on whether or not
it's read-only operation you do apply
width or you to apply mutation now there
are a lot of variants of this algorithm
well yeah there are lower variance and
this one is the one that I like the most
is the classical one because we can
choose to read indicated that we want 45
okay so what is the real indicator I've
been talking about that but I didn't
explain so basically for the purpose of
this talk read indicators they have
three methods that's arrived apart and
is empty and they have to be
sequentially consistent or at least
linearizable and the simplest of them is
just an atomic counter where you to
fetch an ad on the arrived fetching ad
minus one on the depart and to see if
it's empty or not you just do a load so
basically this is what the readers are
going to call when they want to enter a
critical section they would be doing
arrive when they leave they call the
part and when the writer wants to know
where the there are a mean readers are
not it's going to call as empty but
there are on the left right there are to
read indicators so you can get it a
little bit more complex and but more
scalable because the problem with
go back the problem with this one is
that well if you have 10 cards and then
readers hammering the counter variable
all of them doing fetch a net it's gonna
kill your scalability
you don't want that right so how do you
do something that's scalable and by the
way this one is weight free population
oblivious for all operations on x86
applies so the trick here is to create
an array of counters and you put cache
line padding on each entry and then you
can either assign an entry to each
thread but that's hard to do in practice
an easier way is just to get the thread
ID and an hash hash it and distributed
in one randomly in one of these entries
and you need the cache line padding
because you don't want false sharing but
once you get that statistically you have
very little contention you can even
increase the size of the array to as
much as you want right it's much as
you're willing to spend in memory but
usually a few times the number of course
that's enough to to get good scalability
and then you can optimize reading the
array because of course you could do all
of them sequentially consistent loads
but instead you can can do some tricks
and do just one sequentially consistent
load and then read all the other entries
as relaxed and in the end you do some
atomic thread fences so that there's no
reordering so this case atomic thread
fence is preventing all the relax loads
from going below the fence and anything
from going above the fence so that's a
nice trick and it gives you great
performance but like I said before if
you can if you know how many threads you
have in your application and you can
assign an entry in the array for each of
them then okay even better then they
don't have to compete and they can just
do a regular store saying whether there
there's a reader there or not that's
even better um let's get that okay so
how do we know if it's correct and how
it works so this is the state machine
for the writer and the reader all right
miss look complex but it's not compared
to most synchronization mechanisms this
is really simple actually now the thing
about the reader state machine is you
know in this case for example you're
doing the transition if the version
index is zero you transition from A to B
but if it's one in transition from A to
C so okay but the interesting part about
this state machine is that there are no
loops in the state machine it's always
going forward let's say there's there's
no way to come back in instead on the
others on the other hand for the writer
there are loops because they have to
wait for something to happen in this
case for while it's empty so now I've
put the whole code back in again and in
quiz 4 what is the progress condition
for mutated operations on the left right
now I'm going to try to help as much as
I can before something so the first
thing you notice is there's a lock here
you get a stupid mutex and when you're
locking writers mutex and then there's
an atomic load there and then here
there's this method call which is doing
some loops moving so anybody wants to
guess which of these yes well it is
starvation free if you use the mutex
that is starvation free I don't think
that the fault one on one C++ is
starvation free but if you use one that
is starvation freedom that everything
else can be starvation free so that's I
think that's the right answer in this
case very good you're paying attention
and now the same question but for the
readers now the readers what they do is
they call arrive and the part which you
see here and you can think of this one
as being affectionate and that one is
also affectionate and then they do an
atomic load so basically what they do is
fetch a net atomic load such a man what
progress condition is that operation
oblivious yes that is correct
now you could think that it is
population oblivious immediately from
the state machine because the state
machine has no loop so that's well
weight free at least it would be
grateful for not having any loops and
and the thing about the writer is yes it
is starvation free because the writer
only has to wait for all the readers
even if new readers come in they are
going to go on the the other instance
and the writer doesn't have to wait for
those it just has to wait for the old
readers this is the typical RCU thing
okay so how about consistency now there
are a lot of different types of
consistency there was some talk
yesterday about that linearizable is one
of them sequential consistency is
another there's serialize ability there
are loads of relaxed stuff and and
there's even strict consistency which is
impossible for data structures based on
Angus kind of hardware so the thing
about concurrent data structures is that
if they're not at least sequentially
consistent it's very hard for the human
brain to understand how they work and
what kind of behavior to expect out of
them so yes you can make something
that's more relaxed than sequential
ecosystem something that's you know it's
going to give some word values but you
really don't know what to expect out of
it so even if you give it you know you
have this really pretty concurrent data
structure ultra-fast and you
to software developer and he might not
even know what sequentially consistent
is but he has some kind of expectation
on how it should behave in terms of how
consistent it is I mean what kind of
results it should give in certain
situations and that the thing that he
has in his mind is usually called
sequentially consistent well at least
that's how it should work
so linearizable is almost the same thing
as sequentially consistent but slightly
stronger it means that you can find at
least one instance in time where the
operation appears to become instantly
instantly effective now and and this is
the property that new taxes have that's
how mutex this is what new taxes give
the kind of consistency they give and we
the rods works so let's say left right
in this particular case the left/right
variable is pointing there you get two
readers going there but as soon as the
writer toggles the left-right then the
new readers are going to see the
modification that the writer is doing so
this is what is called the linearization
point from this moment on when the
atomic variable called left right is
toggled that means the modification is
now visible to new readers so that's the
linearization point from writers to
readers and okay so maybe some just
stating the obvious here these two data
structures they don't have any Atomics
inside those are playing single threaded
data structures just like you would put
on a reader/writer lock there's there's
no Atomics here you don't have to do
block free or weight free programming to
get this working more about linearize
ability so one of the advantages of the
left-right and similar techniques well
in this case copy-on-write and read the
writer locks also give this but not with
weight free progress of course is that
you can do linearizable iterations
weight linearizable
read-only iterations well imitative as
well but those are blocking so there are
a lot of lock free data structures well
somewhat ruthless Fester's out in the
world we talked about one like Michael
and Scott Q or list where it's just a
linked list where you have nodes in the
end and the thing about these data
structures is that if you want to do a
read-only iteration for example to count
how many nodes there are in the linked
list the result that's coming out of
that is not true linearizable it's not
doing the razzle for sure so what does
that really mean like let's see some
example so suppose we we have one thread
that's just reading and it's going over
a linked list that at this moment only
yes three nodes it's all that there is
and now there are two threads here the
animation is not complete but one of
them is going to add new notes at the
end keeps on happening and the other one
is like removing the back ones so it's
like oh the reader is count one two
three and then the other one thread
comes in and puts a new one and three is
like four five six whatever it's going
on forever now notice that there is no
moment in time where there are more than
three nodes in this data structure in
this linked list there are never more
than three it's either two or three
there's no other moment in time like
that and what is the reader going to say
in the end I was going to say that the
size of this list is 11 like really
that's how lock-free linked lists and
work nowadays
okay so that's not surprising to anyone
okay that's right so it's like however
how is this possible what's going on
it's like so what this means is that the
method that counts the number of nodes
in the linked list is not clean your
risible but if you do this with
left-right you're going to get the
answer two or three depending exactly
one what's going on
there's no 11 coming your way the same
thing if you is a reader/writer lock but
then it will be blocking the answer
but left side is going to give you a
weight free population oblivious answer
that's the advantage so let's compare
against reader/writer lock and against a
copy-on-write thank you
remember the copyright is the one where
you have to make an entire new data
structure whenever you have to make one
single modification to it it's like so
we have on the top four plots a stood
map with 100 items in it and on the
bottom four plots who have a student app
with a thousand items and this is x86
with 18 cores that's 636 hyper threats I
don't think you can read legend but the
top the blue one is reader/writer lock P
thread with the right a lock the red one
is left/right with this with atomic
counters and the green one is left/right
with an array of counters cache line
better to the cache lines and then
hashed on the thread ID and the
copy-on-write is the purple one magenta
plus RC you to save the memory so as you
can see here they're pretty much all the
same if you're doing just rights if
you're just doing modifications it's
kind of like doing a reader/writer lock
except well the reason rights lock is
slightly better for one thread or yeah
but as soon as you start doing more
reads then it starts to get really
better and if you're doing only reads
then it's nearly linear scalability
that's really something hard to get same
thing for a thousand items one thread
the reader/writer lock still wins but
everything else and over there with some
oversubscription I guess but as soon as
you start doing more reads well then the
left/right technique is going to get
better and for only reads you get again
linear scalability that was on x86 this
is on PowerPC the results are more or
less the same
so I'm going to keep that now the
interesting part is latency now this
table is a bit confusing but you can
read it for example if you look at the
first entry what this means is on a
reader writer when using a reader/writer
lock 99% of the calls to find finish in
51 microseconds or less so this is like
the latency of with a certain precision
or with a certain and it's a quintile
not to point out but so the lowest this
is the better on reader/writer lock yeah
you go 70 for TV whatever as you start
having more and on left right you know
it's always 2 3 7 you know it's like an
order of magnitude below so that's
really impressive
now the copy-on-write can also do the
same even though you can't really see it
there but when you do modifications well
when you do a modification you have to
copy the entire data structure and
copying the entire data structure means
it's going to take forever it's like
forget about latency
everything's so slow whatever and now
another thing ok ok so one thing that
you might think of is well you have two
data structures and you have to do the
same operation on both isn't that like
kind of slow yes if it's a long
operation but think of a stood map you
protected with a reader/writer lock for
example the time it takes to acquire the
lock can be so slow that the time it
takes to do them a favour modification
is kind of- legible compared to that so
in many cases you can say like the
mutation just takes this long but the
actual lock takes much longer so doing
twice the mutation well you're not going
to get it such a big slowdown as you
might
expect and in fact when you do one over
the other a ratio it's very close to one
sometimes it's even better
well there are fluctuations but it's
also because we're using students to the
mutex and reader/writer lock might not
be as efficient as that Peter Envy the
writer lock so this is a harder one
now another thing that is non-obvious is
when you have lock free data structure
doing any kind of operation requires
through versing some kind of node or
traversing data structure somehow and
usually to do that you have to do at the
very least acquire loads whenever you do
you go to another node or doing
something now on x86 you can see here
that doing an acquire load or doing a
relaxed load it's the same thing it's
just a move from memory there's really
no difference so there's no impact on
performance but other architectures like
PowerPC and arm well that could be a
pretty big difference because this one
is doing I think and that one is not
doing anything so when you do use reader
right when you use the left/right
technique to go through with a really
large light data structure and you're
going you know through all those notes
well you're it's kind of like doing a
relaxed load because it's just regular
they're just regular variables so you're
not doing any memory fence behind so
it's much faster to do any traversal so
that's an advantage usually for example
and if you if you think about for
example my clients Cutlass which is the
one we saw before on PowerPC it's going
to do n times an icing barrier where n
times is the number of nodes that are
being traversed while if you look at a
list with left-right it's going to do
three edge double sinks
because it's it's related to
so the variables left right and version
index so one is a fixed number the other
one is M so which one's gonna win I
think that's more or less obvious so the
blue one is the left right and these two
are the other lists with one word
sequentially consistent Atomics and the
other word sorry sequentially consistent
loads and the other would acquire loads
now on x86 the difference isn't that big
in fact the blue one which is left-right
loses a bit because it has the overhead
of calling oh heavens but if you take
into account that the other techniques
need some kind of memory reclamation
like hazard pointers and if you have to
inject other pointers that means every
node that you go through must have a
star for the hazard pointer then the
performance completely drops off and
left right doesn't need that it does its
own memory reclamation safely because it
has like an embedded RCU Tinky so you
get better performance so if suppose you
don't like this and you want to do you
think this is still too slow you know
you have to do the same work twice well
for a linked list there's an alternative
and the alternative is the following now
suppose you're just using left right as
usual and that means you have on a
linked list so there's actually two
linked lists they're pointing to the
same kids right
each there's the left instance where it
has depth and point it to that key in
the right instance pointing to that one
but in fact you can combine the two
lists and instead of and have one note
for each and the readers are up either
traversing on the next left next like
next left or they're traversing on the
next right next right next right and
they're never they're not going to touch
the other one
so you can apply the same principles as
left right but just you can't do this
with lambda you have to manually hard
code there and it's safe to use and it's
just one linked list and you just have
to do the work kind of once suppose for
example you in this case you want to
remove one note well then yeah you would
go over here and link this one wait for
the readers to toggle to the to toggle
to the new one so that they would see
the modification below then you to the
other modification and then you could
safely remove the note so this way you
don't have to like start from the
beginning of the linked list search for
the note and then remove that and go
back to the beginning wait for the
readers and then find out because it's
the same note so we just go over to the
note remove it wait for the readers and
then unlink the other part so an equal
part and the other so you don't do the
same job twice just once and obviously
this is much faster faster it's still
blocking for insertions and removals but
it's wait three four meters okay so some
people see that and they think left
right is like our see you know it's not
like our see you or see you doesn't even
have two instances but there are parts
of left right that can be replaced with
our sealed like I said before arrived
can be our see you read lock the part
with an lock and toggle version
synchronized RCU and this is the first
the last slide kinda so suppose you have
all the RCO algorithms and you can get
any of them and you can use the
copy-on-write technique that we talked
about that makes an entire new data
structure every time well then you can
get a single data structure by that
technique and what you get in the end is
a concurrent data structure safe to use
memory wise so that is weight free for
reads and blocking for mutations but
that's fine it's it's safe memory wise
the memory reclamation is done for you
in the data structure
very precious and of course you can do
the same thing are cu+
a concurrent data structure like Harry's
linked list or micron scarf linked list
and in the end you get a memory
reclamation concurrent data structure
now you can also get any of the
left-right algorithms you combine it
with two copies of the same single
friend data structure whatever seems
like that structure you want and again
you get the same result to get the data
structure that's safe to use in a
concurrent setting for memory in fact
you can interchange any of this with any
of these solutions and you get a
concurrent data structure safe to use
with memory reclamation so that's
extremely powerful that doesn't mean I'm
gonna skip depth so the last takeaways I
wanna I want you to take from this is
you get clean your visibility mutual
exclusion for any object or data
structure with left's right it's
interchangeable with RCU kind of the
things that you do there and you get
almost linear scalability if you're
doing mostly weeds and you can just copy
paste the code kind of put it in your
application and that's it and use some
nice lenses and its way through
population oblivious which is like the
best kind of way three there is so
that's it
so one thing I forgot to mention is even
though this presentation if there are
anywhere errors there my own but this is
based on work that I've done with Andre
Correa and we have a website called
concurrency freaks we put some of a
small part of the work we do we put it
there if you want to check it out yes I
think there's a microphone I think the
microphone is on if you want yes so the
people at home can hear the question
right hi so this is not a question but
just a comment regarding visualization
so when you show this table with
percentiles of your latency there is a
nice way to visualize this I think it's
from the finance guys there is this
exchange called El maths and what they
did is to have a high density range is
toad ram so you can go to github and
basically look for HDI histogram and it
basically visualizes the latency of
trading operation address the
percentiles so this way just from the
profile of this latency histogram shape
you can actually find very interesting
patterns in your trading Constitution
guarantees but that might be something
you might find interesting
thank you yeah any other question yeah
in the array paste counter
implementation where you have a thread
indexed array for reader counter you're
going to basically harvest the entire
array in the viewer in the writer how do
you know that after you went 4 elements
0 1 2 3 4
some reader doesn't bump up element 0
again so that's the thing about
sequential consistency and and the
algorithm itself let me just check that
think I know what you're saying yeah
that's the one so
the thing is you're only going to call
the maybe that's not the right one to
show okay see the thing is you're only
going to call and you're going to cover
in index to change which of the which of
the counters so which of the read
indicators is going to be changed it's
going to be arrived departed by the
readers and only then you call is empty
so if there's one thread that's going to
one reader thread that's going through
it can still increment like history
editing can increment the first entry of
the array that's fine because there is
no direct matching from the read
indicators to which instance is going to
be used what happens is you get an
happens before relationship which is if
the reader saw this version saw the new
version index then for sure it must see
the left right variable changing that's
all you need so what can happen is you
can get like false positives where the
writer is waiting for a reader that's
actually already on the new instance but
it marked the opposite read indicator
just like you said
it can happen in practice it's extremely
rare because this is like happens just
for a little while but that's that's the
reason why we need to wait twice one for
the opposite with indicator and then
again here for the other reason
indicator so this wait here is to
prevent the case that you just said
that's related to the algorithm or the
last write itself minor note when you
were doing the ticket look you go sir
overflows there they should be on silent
loans the Bandit will be speaking yes
that is a
any additions just wondered how do you
deal with favorite insert in the second
copy of the collection do real fast do
you undo the first one so let's say you
insert it successfully in the first
three but then fail to insert to the
second one they are mirror images of
each other so there's no reason for them
to fail say like issues involved if it's
madness it's pretty man
yes when allocation might be involved
out of memory then you return the
failure and pretty much you ever feel
like that
there is no undo in your case right
because say it could well you're saying
my story your strategies fail fast in
this case yes if you have that kind of
problem you have other things to worry
about right it's like you ran out of
memory yeah thank you one of your slides
you mentioned right that there were some
advancement made in the reader/writer
lock can you tell us something more
about that and however that changes any
of your results because you
compart well yeah I didn't I don't have
here the comparison I did with that I
think I did with some really long time
ago it's this one the numeral where we
the writer locks
I didn't compare that but the latency
isn't as good as left right obviously
because you know it's a really right to
work it's still blocking for readers so
you're not going to get the same latency
out of a reader/writer lock no matter
how good it is which is actually pretty
good that with the right lock but you're
not going to get the same kind of
latency out of it and even throughput in
the end depending on the exact workload
that you put whether you put more
readers no writers even the throughput
is is not as good but I don't have plots
to show you with that sorry
maybe the next time alright thank you
we available at time thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>