<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Web scraping in Python (Part 4): Exporting a CSV with pandas | Coder Coacher - Coaching Coders</title><meta content="Web scraping in Python (Part 4): Exporting a CSV with pandas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Data-School/">Data School</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Web scraping in Python (Part 4): Exporting a CSV with pandas</b></h2><h5 class="post__date">2017-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Zh2fkZ-uzBU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to part 4 of web
scraping with Python this is a four part
introductory tutorial in which you'll
use web scraping to build a data set
from a New York Times article about
President Trump if you'd like to follow
along at home you can download the
stupid or notebook from github and
there's a link to it in the description
below in the first three parts of this
tutorial
we built a structured data set from The
New York Times article using pythons
beautiful soup library in this video
we'll apply a tabular data structure to
our data set and then export it to a CSV
file at the end I'll wrap up with some
web scraping advice and resources
the last major step in our process is to
apply a tabular data structure to our
existing structure which is our list of
tuples which you can see here we're
going to do this using the pandas
library an incredibly popular Python
library for data analysis and
manipulation if you don't have it you
can search online for the pandas
installation instructions
the primary data structure in pandas is
the data frame which is suitable for
tabular data with columns of different
types similar to an Excel spreadsheet or
a sequel table we can convert our list
of tuples into a data frame by passing
it to the data frame constructor and
specifying the desired column names the
data frame includes a head method which
allows you to examine the top of the
data frame the numbers on the left side
of the data frame are known as the index
which act as identifiers for the rows
because we didn't specify an index it
was automatically assigned as the
integers 0 to 115 we can examine the
bottom of the data frame using the tail
method
did you notice that January was
abbreviated but July was not it's best
to format your data consistently and so
we're going to convert the date column
to panda's special date time format this
code converts the date column to date
time format and then overwrites the
existing date column notice that we did
not have to tell pandas that the column
was originally in month/day/year format
pandas just figured it out let's take a
look at the results here's the head
notice the change to the date column and
here's the tail not only is the date
column now consistently formatted but
pandas also provides a wealth of date
related functionality because it's in
day time format finally we'll use pandas
to export the data frame to a CSV file
or comma separated value file which is
the simplest and most common way to
store tabular data in a text file we set
the index parameter to false to tell
pandas that we don't need it to include
the index the integers 0 to 115 in the
CSV file you should be able to find this
file in your working directory and open
it in any text editor or spreadsheet
program in the future you can rebuild
this data frame by reading the CSV file
back into pandas and there you go if you
want to learn a lot more about the
pandas library you can watch my video
series easier data analysis in Python
with panda
or check out my top eight resources for
learning data analysis with pandas this
is all at my website data school dot IO
to summarize here are the 16 lines of
code that we use to scrape the web page
extract the relevant data convert it
into a tabular data set and export it to
a CSV file here is where we read the web
page here's where we parsed the HTML and
then extracted those 116 records here's
where we extracted the date ly
explanation and URL from each record and
then finally here's where we created a
data frame in order to give it a tabular
data structure converted the date column
to a date-time format and then exported
it to a CSV file okay briefly I want to
go through some advice for web scraping
first web scraping works best with
static well-structured web pages dynamic
or interactive content on a web page is
often not accessible through the HTML
source which makes scraping it much
harder second web scraping is a fragile
approach for building a data set the
HTML on a page you're scraping can
change at any time which may cause your
scraper to stop working third if you can
download the data you need from a
website or if the website provides an
API with data access those approaches
are preferable to scraping since they
are easier to implement and less likely
to break forth if you're scraping a lot
of pages from the same website in rapid
succession
it's best to insert delays in your code
so that you don't overwhelm the website
with requests if the website decides you
are causing a problem they can block
your IP address which may affect
everyone in your building and finally
before scraping a website you should
review its robots.txt file also known as
the robots exclusion standard to check
whether you are allowed to scrape their
website here's the robots.txt file for
the New York Times explaining how to
read this is slightly beyond the scope
of this tutorial but you can read online
about how to read a robots.txt file next
I have some recommended web scraping
resources for you I'm not going to read
each of these out if you want to click
through to these links you should
download this jupiter' notebook from the
link I have shown on the screen
github.com slash just Markum slash trump
- lies or there's a link to the notebook
in the description below this video
finally it's worth noting that
beautifulsoup
actually offers multiple ways to express
the same command I tend to use the most
for both option since I think it makes
the code readable but it's useful to be
able to recognize the alternative syntax
since you might see it used elsewhere
here's an example we can search for a
tag by name by saying first result dot
find and pass strong to it alternatively
you can search for a strong tag by
accessing it like an attribute so these
are identical you can also search for
multiple tags a few different ways
you might remember we use the find all
method on the soup object to find the
span tags where the class attribute had
a value of short desc
so this is how I wrote the code but if
you don't specify a method for example
if you don't specify find all it's
assumed to be find all so this code is
identical to that and even shorter you
can specify the attribute as if it's a
parameter you can say class underscore
equals short desc
now why you have to put an underscore
here is beyond the scope of this
tutorial you can read through it in the
beautiful soup documentation if you like
I always recommend the most verbose
options because again I think it makes
the code most readable ok that is it for
this video series I hope you learned a
lot please do check out my other videos
on YouTube about pandas machine learning
with scikit-learn version control with
git and other topics you can subscribe
to my channel if you'd like to be
notified about my future videos thank
you again for joining me and I hope to
see you again soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>