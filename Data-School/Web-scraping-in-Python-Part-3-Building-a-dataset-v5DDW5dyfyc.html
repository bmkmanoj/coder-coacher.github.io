<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Web scraping in Python (Part 3): Building a dataset | Coder Coacher - Coaching Coders</title><meta content="Web scraping in Python (Part 3): Building a dataset - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Data-School/">Data School</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Web scraping in Python (Part 3): Building a dataset</b></h2><h5 class="post__date">2017-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v5DDW5dyfyc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to part three of web
scraping with Python this is a four part
introductory tutorial in which you'll
use web scraping to build a data set
from a New York Times article about
President Trump if you'd like to follow
along at home you can download this
Jupiter a notebook from github and
there's a link to it in the description
below in parts 1 and 2 of this tutorial
we read the New York Times article into
Python using the request library and
then parse the HTML using the
beautifulsoup library the end result was
a collection of 116 records in this
video we'll separate these records into
their four components date lye
explanation and URL and then we'll write
a loop to create more structure in our
data set
let's start by extracting the date
recall that the results object contains
our 116 records but they're still in the
form of raw HTML like this now web
scraping is often an iterative process
in which you experiment with your code
until it works exactly as you desire to
simplify the experimentation we'll start
by only working with the first record in
the results object and then later we'll
modify our code to use a loop although
first result may look like a Python
string you'll notice that there are no
quote marks around it instead it's
another special beautifulsoup object
called a tag that has specific methods
and attributes in order to locate the
date we can use its find method to find
a single tag that matches a specific
pattern in contrast to the find all
method we used above to find all tags
that match a pattern here's where we
used find all to find all the span tags
with the short desc
as the value for the class attribute and
now we're just searching for the strong
tag so it's finding this tag right here
so the code searches first result for
the first instance of a strong tag and
again returns a beautiful soup tag
object not a string since we want to
extract the text between the opening and
closing tags we can access its text
attribute which does in fact
turn a regular Python string what's this
/x a zero well you don't actually need
to know this but it's called an escape
sequence that represents the nbsp
character we saw earlier in the HTML
source however you do need to know that
an escape sequence represents a single
character within a string so let's slice
it off from the end of the string
finally we're going to add the year
since we don't want our data set to
include ambiguous dates and there we go
next we're going to extract the lie
let's take another look at first result
our goal is to extract the two sentences
about Iraq unfortunately there isn't a
pair of opening and closing tags that
starts immediately before the lie and
ends immediately after the lie therefore
we're going to have to use a different
technique the first result tag has a
contents attribute which returns a
Python list containing its children what
are children they are the tags and
strings that are nested within a tag
here's the first child here's the second
child and here is the third child so
we can slice this list to extract the
second element these two sentences about
Iraq and finally we'll slice off the
curly quotation marks as well as this
space at the end and there you go
there's the lie based upon what you've
seen already you might have figured out
that we have at least two options for
how we extract the third component of
the record which is the writer's
explanation of why the president
statement was a lie the first option is
to slice the contents attribute like we
did when extracting the lie and here's
that explanation the second option is to
search for the surrounding tag like we
did when extracting the date and again
here's the explanation either way we can
access the text attribute and then slice
off the opening and closing parentheses
and there you go
finally we want to extract the URL of
the article that substantiates the
writers claimed that the president was
lying let's examine the a tag within
first result so far in this tutorial
we've been extracting text that is
between tags such as this in this case
the text we want to extract is located
within the tag itself specifically we
want to access the value of the H ref
attribute within the a tag beautifulsoup
treats tag attributes and their values
like key value pairs in the dictionary
you put the attribute name in brackets
like a dictionary key and you get back
the attribute value and there you go
before we finish building the data set I
want to summarize a few ways you can
interact with beautifulsoup objects you
can apply these two methods to either
the initial soup object or a tag object
such as first result
so find searches for the first matching
tag and returns a tag object find all
searches for all matching tags and
returns a result set object which you
can treat like a list of tags you can
extract information from a tag object
such as first result using these two
attributes text extracts the text of a
tag and returns a string and contents
extracts the children of a tag and
returns a list of tags and strings it's
important to keep track of whether
you're interacting with a tag result set
list or string because that affects
which methods and attributes you can
access and of course there are many more
methods and attributes available to you
which are described in the beautifulsoup
documentation now that we've figured out
how to extract the four components of
first result we can create a loop to
repeat this process on all 116 results
will store the output in a list of
tuples called records so we'll start
with our empty list records we'll loop
through results and for each singular
result will use the code we used
previously
so extract the date the lie the
explanation and the URL and then we'll
we'll take that as a tuple of length 4
and append that to the records list
since there were 116 results we should
have 116 records let's do a quick spot
check of the first 3 records here's our
first list element which is a tuple of
length 4 here's our second element and
here's our third element and each one
has the date the lie the explanation and
the URL so this looks good next time in
the final video in this series we'll
apply a tabular data structure to our
data set using the pandas library and
then export it to a CSV file then I'll
wrap up with some web scraping advice
and
if you have a question or a tip let me
know in the comment section below please
click Subscribe if you liked this video
thank you so much for joining me and I
will see you again soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>