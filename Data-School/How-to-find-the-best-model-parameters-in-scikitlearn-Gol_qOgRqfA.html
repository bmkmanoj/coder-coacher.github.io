<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to find the best model parameters in scikit-learn | Coder Coacher - Coaching Coders</title><meta content="How to find the best model parameters in scikit-learn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Data-School/">Data School</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to find the best model parameters in scikit-learn</b></h2><h5 class="post__date">2015-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Gol_qOgRqfA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back to my video series on
scikit-learn for machine learning in the
previous video we learned about k-fold
cross-validation a very popular model
evaluation technique and then applied it
to three different types of problems in
this video I'll be covering the
following
how can k-fold cross-validation be used
to search for an optimal tuning
parameter how can this process be made
more efficient how do you search for
multiple tuning parameters at once what
do you do with those tuning parameters
before making real predictions and how
can the computational expense of this
process be reduced let's just briefly
review the steps of k-fold
cross-validation as well as its benefits
and drawbacks first we choose a number
for K often 10 and split the data set
into k partitions or folds of equal size
the model is trained on all of the folds
except 1 then tested on the remaining
fold and evaluated using the chosen
evaluation metric that process is
repeated K minus 1 more times such that
each fold is the testing set once and
the training set all other times the
average testing performance also known
as the cross validated performance is
used as the estimate of the models
performance on out-of-sample data that
performance estimate is more reliable
than the estimate provided by trained
test split because cross-validation
reduces the variance associated with a
single trial of trained test splint and
as we saw in the last video
cross-validation can be used for
selecting tuning parameters choosing
between models and selecting features
however
cross-validation can be computationally
expensive especially when the dataset is
very large or the model is slow to Train
let's now review how we use the cross
valve
in the last video to select the best
tuning parameters it's important to
understand this code because it will
help you to understand the role of grid
search CV which I'll be covering in the
next section of this video in this case
we were trying to select the best value
for K for the KNN model when predicting
species on the iris data set we'll start
by importing the relevant functions and
classes and then creating the feature
matrix X and response vector Y next will
instantiate the K neighbors classifier
with a K value of 5 represented by the N
neighbors parameter the model object
which we called KN n is used as the
first parameter in cross valve or
parameters are the feature matrix x the
response vector Y the number of
cross-validation folds to use and the
evaluation metric of our choice we're
choosing to use 10-fold cross-validation
with classification accuracy as the
evaluation metric cross val score
returned ten scores namely the testing
accuracy for each of the ten folds used
during cross-validation remember that
cross val score takes care of the entire
cross validation process including
splitting x and y into the ten folds
which is why we passed it the entirety
of x and y and not x train and y train
the scores object is a numpy array and
so we can use its mean method in order
to calculate the mean which we use as
the estimated accuracy
see for the K equals five model on the
iris dataset
we want to repeat this process for a
variety of K values to choose the K
which will most likely generalize to
out-of-sample data and thus using a for
loop is a natural choice pay close
attention here because this is the code
that we're going to replace with grid
search CV we start by creating two lists
K range contains the integers 1 through
30 and indicates the values of K that we
want to try and K scores is an empty
list that will be used to store the
cross validated accuracy for each of the
30 iterations the for loop iterates
through K range and within the loop we
instantiate K neighbors classifier with
the chosen K value use cross-validation
and then append the mean of the 10
scores to the K scores list finally
we'll print the 30 cross validated
scores as you would expect the 5th score
in this list is 0.96 6 which we knew
from above would be the cross validated
score when n neighbors equals 5 finally
we plot the K values against the
accuracy
we see that the values 13:18 and 24k
appear to be best even though the code
above is not particularly difficult to
write you could imagine that this is
something you might want to do very
often and thus it would be nice if there
was a function that could automate this
process for you that is exactly why grid
search CV was created grid search CV
allows you to define a set of parameters
that you want to try with a given model
and it will automatically run
cross-validation using each of those
parameters keeping track of the
resulting scores essentially it replaces
the for loop above as well as providing
some additional functionality to get
started with grid search CV we first
import the class from SK learn good
search exactly like above we create a
Python list called K range that
specifies the K values that we would
like to search next we create what is
known as a parameter grid it's simply a
Python dictionary in which the key is
the parameter name and the value is a
list of values that should be searched
for that parameter as you can see this
dictionary has a single key value pair
in which the key is the string and
neighbors and the value is a list of the
numbers 1 through 30 next will
instantiate the grid you'll notice that
it has the same parameters as cross Val
score except that it doesn't include X
or Y but it does in
includ / am great you can think of the
grid object as follows it's an object
that is ready to do ten fold cross
validation on a KNN model using
classification accuracy as the
evaluation metric but in addition it has
been given this parameter grid so that
it knows that it should repeat the ten
fold cross validation process thirty
times and each time the N neighbors
parameter should be given a different
value from the list hopefully this helps
you to understand why the parameter grid
is specified using key value pairs we
can't just give grid search CV a list of
the numbers 1 through 30 because it
won't know what to do with those numbers
instead we need to specify which model
parameter in this case and neighbors
should take on the values 1 through 30
one final note about instantiating the
grid if your computer and operating
system support parallel processing you
can set the end jobs parameter to
negative 1 to instruct scikit-learn to
use all available processors finally we
fit the grid with data by just passing
it the x and y objects
this step may take quite a while
depending upon the model and the data
and the number of parameters being
searched remember that this is running
10-fold cross-validation 30 times and
thus the KNN model is being fit and
predictions are being made 300 times now
that grid search is done let's take a
look at the results which are stored in
the grid scores attribute this is
actually a list of 30 named tuples the
first tuple indicates that when the n
neighbors parameter was set to 1 the
mean cross validated accuracy was 0.96
and the standard deviation of the
accuracy scores was 0.05 while the mean
is usually what we pay attention to the
standard deviation can be useful to keep
in mind because if the standard
deviation is high that means that the
cross validated estimate of the accuracy
might not be as reliable anyway you can
see that there is one tuple for each of
the 30 trials of cross-validation
next I'll just show you how you can
examine the individual tuples just in
case you need to do so in the future I'm
going to slice the list to select the
first tuple using the bracket 0 notation
because it's a named tuple I can select
its elements using dot notation
parameters is simply a dictionary
containing the parameters that were used
CV validation scores is an array of the
ten accuracy scores that were generated
during 10-fold cross-validation using
that parameter and mean validation score
is of course the mean of those 10 scores
it's easy to collect the mean scores
across the 30 runs and plot them like we
did above so let's just quickly do that
we'll use a list comprehension to loop
through grid grid scores pulling out
only the mean score and when we plot the
results
we can see that the plot is identical to
the one we generated above now you might
be thinking that writing a list
comprehension and then making a plot
can't possibly be the most efficient way
to view the results of the grid search
you're exactly right once a grid has
been fit with data it exposes three
attributes that are quite useful best
score is the single best score achieved
across all of the parameters best params
is a dictionary containing the
parameters used to generate that score
and finally best estimator is the actual
model object fit with those best
parameters which conveniently shows you
all of the default parameters for that
model that you did not specify as you
might have noticed in the plot there are
two other values for K which also
produced a score of 0.98 I don't know
for sure how grid search decided to
choose K equals 13 as the best but some
limited testing indicates that it
probably just picks the first parameter
it tested that achieved that score it
might have occurred to you already that
sometimes you'll want to search multiple
different parameters simultaneously for
the same model for example let's pretend
you're using a decision tree classifier
which is a model we haven't yet covered
in the series two important tuning
parameters are max depth and min samples
leaf you could tune those parameters
independently meaning that you tried
different values for max depth while
leaving min samples leaf at its default
value
and then you try different values from
in samples leaf while leaving max depth
at its default value the problem with
that approach is that the best model
performance might be achieved when
neither of those parameters are at their
default values thus you need to search
those two parameters simultaneously
which is exactly what we're about to do
with grid search TV in the case of KNN
another parameter that might be worth
tuning other than K is the weights
parameter which controls how the K
nearest neighbors are weighted when
making a prediction the default option
is uniform which means that all points
in the neighborhood are weighted equally
but another option is distance which
weights closer neighbors more heavily
than further neighbors thus I'm going to
create a list of those options called
weight options in addition to the 30
integer options in K range again I
create a parameter grid and in this case
the dictionary has two key value pairs
one pair for each parameter to be
searched
now I'll instantiate the grid and fit it
with data just like before this is known
as exhaustive grid search because grid
search CV is trying every possible
combination of the N neighbors parameter
and the weights parameter because there
are 30 options for n neighbors and two
options for weights 10-fold
cross-validation is being performed 60
times
let's take a quick look at the results
you can see that there is a tuple for
every possible combination of n
neighbors and weights once again we'll
examine the best score and the best
parameters it turns out that the best
score did not improve and thus using n
neighbors equals 13 with the default
value for weights is still the best set
of parameters for this model next I want
to briefly remind you of what to do with
these optimal tuning parameters once you
found them before you make predictions
on out-of-sample data it is critical
that you train the model with the best
known parameters using all of your data
as you can see I instantiate K neighbors
classifier with the best parameters I
found above and then fit it with X and Y
not X train and Y train in other words
even if you use train test split earlier
in the model building process you should
train your model on x and y before
making predictions on new data otherwise
you will be throwing away potentially
valuable data that your model can learn
from here I've just made an example
prediction on a made-up data point
however grid search CV gives you a
shortcut to this process by default grid
search CV will refit the model using the
entire data set and the best parameters
it found that fitted model is stored
within the grid object and it exposes a
predict method to allow you to make
predictions using the fitted model
in other words the code in this cell
accomplishes the same thing as the code
in this cell obviously this is only
useful if you're running grid search CV
right before making predictions but it
can be quite handy at times finally
let's take a look at randomized search
cv which is a close cousin of grid
search cv the problem that randomized
search cv aims to solve is that when
you're performing an exhaustive search
of many different parameters at once the
search can quickly become
computationally infeasible for example
searching ten different parameter values
for each of four parameters will require
10,000 trials of cross-validation which
would equate to 100,000 model fits and
100,000 sets of predictions if 10-fold
cross-validation is being used
randomized search cv solves this problem
by searching only a random subset of the
provided parameters and allowing you to
explicitly control the number of
different parameter combinations that
are attempted as such you can
effectively decide how long you want it
to run for depending on the
computational time you have available as
with grid search cv we import the
randomized search cv class from SK
learned grid search with randomized
search cv you specify parameter
distributions rather than a parameter
grid for discrete parameters meaning
parameters for which you provide a list
of discrete values like integers or
strings
this specification looks exactly the
same as before in this case both of my
parameters are discrete and so per am
dist is no different from per AM grid
however if one of your tuning parameters
is continuous such as a regularization
parameter for a regression problem it's
important to specify a continuous
distribution rather than a list of
possible values so that randomized
search CV can perform a more
fine-grained search I don't have an
example to show you here
but some of the resources all mentioned
below include examples of this anyway we
can now instantiate the randomized
search fit it with data and take a look
at the scores you'll see that per AM
dist is used instead of Perham grid and
there are two new parameters an inner
which controls the number of random
combinations it will try and random
state which I've set for the purpose of
reproducibility
from the grid scores you can see that
ten random combinations of parameters
were tried and once again we can look at
the best score and the best parameters
we can see that even though it only
tried ten combinations of n neighbors
and weights it's still managed to find a
combination that has just as high a
score as the combination found by grid
search CV it's certainly possible that
randomized search CV will not find as
good a result as grid search CV but you
might be surprised how often it finds
the best result or something very close
in a fraction of the time that grid
search CV would have taken I've set up a
quick experiment here in which I run
randomize search CV with n it or equals
10 and I repeat this process 20 times
recording the best score each time as
you can see most of the time it does
result in a score of 0.98 and when it
doesn't find that score it's still close
to 0.98 in terms of practical
recommendations I would usually
recommend starting with grid search CV
but then switching to randomize search
CV if grid search CV is taking longer
than the time you have available when
using randomize search CV start with a
very small value of a knitter time how
long that takes and then do the math for
how large you can set the n hitter
parameter without exceeding the amount
of time you have available
as always the scikit-learn documentation
is helpful if you want to learn more the
first link is their user guide covering
the topic of grid search in general
followed by links to the detailed
documentation for grid search CV and
randomized search CV next is a
comparison of randomized search and grid
search also from the scikit-learn
documentation which provides some code
that demonstrates how much time a
randomized search can save you over an
exhaustive grid search next is a video
segment and ipython notebook on
randomized search by andreas Muller who
is one of the core contributors to
scikit-learn the relevant portion of the
video which I've linked to is just three
minutes long but the rest of the video
is worth watching if you want to learn
about the more advanced features of
scikit-learn the final resource is a
paper by
yoshua bengio famous for his research on
deep learning which argues in favor of
using a random search process for
parameter tuning as always I appreciate
you watching and look forward to your
comments and questions please subscribe
on youtube if you'd like to be notified
when my next video is released thanks
again and I'll see you soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>