<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to evaluate a classifier in scikit-learn | Coder Coacher - Coaching Coders</title><meta content="How to evaluate a classifier in scikit-learn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Data-School/">Data School</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to evaluate a classifier in scikit-learn</b></h2><h5 class="post__date">2015-10-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/85dtiMz9tSo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back to my video series on
scikit-learn for machine learning in the
previous video we learned how to search
for the optimal tuning parameters for a
model using scikit-learn x' grid search
and randomize search tools in this video
I'll be covering how to properly
evaluate classification models here's
the agenda what is the purpose of model
evaluation and what are some common
evaluation procedures what's the usage
of classification accuracy and what are
its limitations how does a confusion
matrix describe the performance of a
classifier what metrics can be computed
from a confusion matrix
how can you adjust classifier
performance by changing the
classification threshold what is the
purpose of an ROC curve and how does AUC
differ from classification accuracy
let's briefly review the goal of model
evaluation and the evaluation procedures
we have learned so far model evaluation
answers the question how do I choose
between different models regardless of
whether you are choosing between K
nearest neighbors and logistic
regression or selecting the optimal
tuning parameters or choosing between
different sets of features you need a
model evaluation procedure to help you
estimate how well a model will
generalize to out-of-sample data however
you also need an evaluation metric to
pair with your procedure so that you can
quantify model performance
we've talked in depth about different
model evaluation procedures starting
with training and testing on the same
data then train test split and finally
k-fold cross-validation training and
testing on the same data is a classic
cause of overfitting in which you build
an overly complex model that won't
generalize to new data and thus is not
actually useful train tests plet
provides a much better estimate of
out-of-sample performance and k-fold
cross-validation does even better by
systematically creating k train test
splits and averaging the results
together
however train tests split is still
preferable to cross-validation in many
cases due to its speed and simplicity
and is what we will use in this video
you always need an evaluation metric to
go along with your chosen procedure and
the choice of metric depends on the type
of problem you're addressing for
regression problems we've used mean
absolute error mean squared error and
root mean squared error as our
evaluation metrics for classification
problems all we have used so far is
classification accuracy there are many
other important evaluation metrics for
classification and those metrics are the
focus of today's video
before we learn any new evaluation
metrics let's review classification
accuracy and talk about its strengths
and weaknesses I've chosen the Pima
Indians diabetes dataset for this lesson
which includes the health data and
diabetes status of 768 patients I'm
going to load this data into a panda's
data frame using the read CSV function
and we'll explicitly specify the column
names since the CSV file does not
contain a header row let's print the
first five rows from the data frame
using the head method each row
represents one patient and the label
column indicates one if the patient has
diabetes and zero if they do not have
diabetes we are defining the
classification problem as follows can we
predict the diabetes status of a patient
given their health measurements we're
going to start by defining the feature
matrix X and response vector Y I'm going
to choose pregnant insulin BMI and age
as the features to create a panda's data
frame containing only those four columns
I put the names of the columns in a
Python list pass that list to the Pima
data frame using bracket notation and
store the result in X to create why I
use dot notation to select the Panda
series named label
now let's quickly walk through the rest
of the modeling process we're using
train tests split to split X&amp;amp;Y
into training and testing sets we'll
train a logistic regression model on the
training set which is a classification
model despite its name during the
fit-step the log reg model object is
learning the relationship between X
train and y train finally we'll make
class predictions for the testing set we
pass X test the feature matrix for the
testing set to the predict method for
the fitted model this outputs a class
prediction 1 or 0 for every observation
in the testing set which will store in
an object called Y pred class now that
we've made predictions for the testing
set we can calculate the classification
accuracy which is simply the percentage
of correct predictions we first import
the metrics module from SK learn and
then pass Y test and wipe read class to
the accuracy score function because Y
test contains the true response values
for the testing set the accuracy score
function can tell us what percentage of
the predictions in Y pred class were
correct the classification accuracy is
69%
which seems pretty good
however any time you use classification
accuracy as your valuation metric it's
important to compare it with null
accuracy which is the accuracy that
could be achieved by always predicting
the most frequent class in the testing
set let's calculate null accuracy for
this problem to see why this is a useful
comparison the why test object is a
panda's series object and so it has a
value counts method that counts how many
instances exist of each value in the
series in this case the zero value is
present 130 times and the one value is
present 62 times this is known as the
class distribution null accuracy answers
the question if my model was to predict
the predominant class 100% of the time
how often would it be correct let's
figure out how to calculate this because
why test only contains ones and zeros we
can calculate the percentage of ones by
simply taking the mean in this case 32%
of the values in Y test are ones if
you're confused as to why this
calculation worked pretend that you had
a vector of four zeros and six ones and
I asked you what percentage of the
values were ones
the answer is obviously 60% and you can
arrive at that answer by totaling all of
the values which is 6 and dividing by
the length of the vector which is 10
which is equivalent to taking the mean
of the vector anyway since there are
only two classes we can calculate the
percentage of zeros by taking one minus
the mean
why test the answer is 68% and since 68
is larger than 32 we would say that 68%
is the null accuracy for this problem in
other words a dumb model that always
predicts that a patient does not have
diabetes will be right 68 percent of the
time this is obviously not a useful
model but it provides a baseline against
which we might want to measure our
logistic regression model when we
compare the null accuracy of 68% with
the model accuracy of 69% suddenly our
model does not look very good this
demonstrates one weakness of
classification accuracy as a model
evaluation metric in that classification
accuracy does not tell us anything about
the underlying distribution of the
testing set before we move on I want to
show you how to calculate null accuracy
in a single line of code
which is simply to take the max of Y
test mean and 1 minus y test mean note
that this approach will only work for a
binary classification problem in which
the response value is coded as zeros and
ones for a problem with three or more
classes this is the code that I would
use though it will only work if Y test
is a panda's series
finally I want to show you one other
weakness of classification accuracy
let's take a look at the first 25 true
response values from Y test as well as
the corresponding 25 predictions from
our logistic regression model do you
notice any patterns you might have
noticed that when the true response
value is a zero the model almost always
correctly predicts a zero but when the
true response value is a one the model
rarely predicts a 1 in other words the
model is usually making certain types of
errors but not others but we would never
know that simply by examining the
accuracy this particular issue will be
addressed by the confusion matrix which
I'll discuss in the next section to
conclude this section I'll briefly
review what we've learned about
classification accuracy it is certainly
a useful metric in that it is the
easiest metric to understand however
accuracy does not tell you about the
underlying distribution of response
values which we examined by calculating
the null accuracy nor does it tell you
the types of errors your model is making
which is often useful to know in
real-world situations let's now take a
look at the confusion matrix which I
would loosely define as a table that
describes the performance of a
classification model the confusion
matrix function is also available in the
metrics module and in this case outputs
a 2x2 numpy
a the array is not labeled with any text
and so I've created a diagram to help
explain the output before we discuss the
diagram I want to emphasize the
importance when using the confusion
matrix function of passing the true
response values as the first argument
and the predicted values as the second
argument if you pass these arguments in
the opposite order the confusion matrix
will be reversed but no air will be
raised all metrics functions in
scikit-learn expect the true values to
be the first argument and so it's a good
habit to adopt that pattern even when
the order doesn't technically matter for
a particular function such as accuracy
score anyway let's talk about what we
can learn from the confusion matrix you
can think of it as a tally of the two
types of correct predictions that the
classifier can make as well as a tally
of the two types of incorrect
predictions
it can make thus every observation in
the testing set is represented in
exactly one box of the confusion matrix
sometimes the confusion matrix will be
explicitly labeled with the total number
of observations represented which is 192
in this case the size is 2 by 2 because
this is a binary classification problem
so if there were 5 possible response
classes this would be a 5 by 5 matrix
also note that the format shown here is
not universal in that sometimes the
positions of the true and predicted
values are reversed
and so it's critical that you pay
attention to the particular format when
interpreting a confusion matrix when a
confusion matrix is used for a binary
problem
each of these four boxes has a specific
name that is useful to memorize the
bottom right is called true positives
and indicates that in 15 cases the
classifier correctly predicted that a
patient has diabetes the upper left is
called true negatives and indicates that
in a hundred and 18 cases the classifier
correctly predicted that a patient does
not have diabetes the upper right is
called false positives and indicates
that in 12 cases the classifier
incorrectly predicted that a patient has
diabetes when in fact they do not the
bottom left is called false negatives
and indicates that in 47 cases the
classifier incorrectly predicted that a
patient does not have diabetes when in
fact they do have diabetes there are a
couple points I want to highlight before
we move on first it's conventional to
describe the class encoded as one as the
positive class and to describe the class
encoded as 0 as the negative class that
is why correctly predicting a 1 value is
known as a true positive and correctly
predicting a 0 value is known as a true
negative 2nd if you have any trouble
remembering the difference between a
false positive and a false negative
just think of false positives as cases
in which the classifier falsely
predicted positive and false negatives
as cases in which
it falsely predicted negative third
false positives are known in some fields
as type 1 errors and false negatives are
known as type 2 errors finally it's
important to note that these four
numbers are integer counts and are not
rates
we'll talk shortly about some of the
rates that can be calculated from a
confusion matrix I'm going to once again
print out the first 25 true and
predicted response values let's do a
quick quiz to make sure that the
terminology above is crystal clear I
want you to find examples of each of the
four cases I described above a true
positive a true negative a false
positive and a false negative go ahead
and pause the video and unpause it once
you're done okay so hopefully you were
able to find examples of each of those
you can actually find examples right
next to one another here's a true
positive here's a true negative here's a
false positive in which we falsely
predicted a positive or a 1 and here's a
false negative in which we falsely
predicted a negative or a zero
before we move on to the next section I
want to show you two things first I'm
going to save the confusion matrix as an
object called confusion and use numpy
z-- bracket notation to slice it into
four individual pieces that I can use in
the next section
second I've updated the diagram with the
terms described above and added row and
column totals which will also be a
useful reference during the next section
the confusion matrix is useful in
helping you to understand the
performance of your classifier but how
can it help you to choose between models
it's not a model valuation metric and so
you can't simply tell scikit-learn to
choose the model with the best confusion
matrix however there are many metrics
which can be calculated from a confusion
matrix and those can be directly used to
choose between models let's go through a
few of the popular metrics and then at
the end talk about how to choose which
metric to optimize as you may have
already figured out classification
accuracy can be calculated from the
confusion matrix you add the true
positives and true negatives and divide
that by the total number of observations
in Python 2 you need one of the numbers
to be a float so that it performs true
division instead of integer division as
we've already seen there's an accuracy
score function in the metrics module
which does the exact same thing
the next metric is classification error
also known as miss classification rate
it is equal to the false positives plus
the false negatives divided by the total
or one minus the accuracy score the next
metric is sensitivity which answers the
question when the actual value is
positive how often is the prediction
correct if we take a look at the sample
of true and predicted responses
we know that the sensitivity is going to
be low because in most cases when the
actual value is 1 the model incorrectly
predicts 0 looking out the confusion
matrix sensitivity is calculated by
dividing the true positives by the total
of the bottom row or 15 divided by 62
the bottom row is all that is relevant
for this calculation since we're only
considering cases in which the actual
response value is 1 to me the term
sensitivity makes intuitive sense
because it's a calculation of how
sensitive the classifier is to detecting
positive instances however it's also
known by the terms true positive rate
and recall which may make more sense to
you
which of these terms is most commonly
used is largely dependent on the field
of study
anyway let's actually calculate
sensitivity scikit-learn has a metrics
function called recall score that can do
this for us the next metric will discuss
is specificity which answers the
question when the actual value is
negative how often is the prediction
correct just like sensitivity
specificity is something you want to
maximize
examining this sample once again we know
that the specificity will be high
because in most cases when the actual
value is zero the model correctly
predicts zero in terms of the confusion
matrix specificity is calculated by
dividing the true negatives by the total
of the top row or 118 divided by 130
this time the top row is all that's
relevant if you want to memorize the
terms specificity think of it as
describing how specific or selective the
classifier is in predicting positive
instances anyway there's currently no
metrics function that can calculate
specificity for us and so we just have
to calculate it manually for both
sensitivity and specificity the best
possible value is 1 and so you would
describe our classifier as highly
specific but not highly sensitive let's
move on to false positive rate it
answers the question when the actual
value is negative how often is the
prediction incorrect
just like specificity only the top row
of the confusion matrix is relevant to
this calculation except this time it's
the false positives divided by the total
of the top row as you might have figured
out already false positive rate is
simply 1 minus specificity which is how
I remember it
finally let's calculate precision it
answers the question when a positive
value is predicted how often is the
prediction correct
this is our first metric in which the
denominator is a column instead of a row
and is calculated by dividing true
positives by the total of the right
column or 15 divided by 27
you can think of precision as describing
how precise the classifier is when
predicting a positive instance anyway
precision can be calculated using the
metrics function precision score note
that many other metrics can be
calculated from the confusion matrix
such as the f1 score and Matthews
correlation coefficient
to conclude this section I want to
advise you that you should always
examine the confusion matrix for your
classifier since it gives you a more
complete picture of how your classifier
is performing it also allows you to
compute various classification metrics
which can guide your model selection
process however you can't optimize your
model for every one of these metrics so
how do you choose between them the
choice of metric ultimately depends on
your business objective and the easiest
way for me to explain this is through
two examples in the first example we're
building a spam filter in which an
observation represents an email and the
positive class is spam in this case most
people would say that false negatives in
which spam goes to the inbox are more
acceptable than false positives in which
non spam is caught by the spam filter
thus our priority is to minimize false
positives and so we might choose to
optimize our model for precision or
specificity our second example is a
fraudulent transaction detector for a
website in which an observation
represents a transaction and the
positive class is fraud in this case the
website owner might judge that false
positives in which normal transactions
are flagged as possible fraud are more
acceptable than false negatives in which
fraudulent transactions are missed since
the former can often be resolved without
losing a sale while the latter likely
results in the loss of money thus our
priority is to minimize false negatives
and so we might choose to optimize our
model for sensitivity
hopefully this has given you a useful
framework for thinking about your own
machine learning problem I'd love to
hear from you in the comment section
about the classification problem you're
trying to solve and what metric you're
planning to focus on we're now going to
discuss how to modify the performance of
a classifier by adjusting the
classification threshold a term which
I'll define momentarily you'll see in a
few minutes how this topic relates to
the evaluation metrics above let's take
a look at the first ten predicted
response values by passing X test to the
predict method for the logistic
regression model and then slicing out
the first ten results it's a
one-dimensional array of zeros and ones
just like you would expect there's a
similar method for classification models
called predict prabha which outputs
predicted probabilities of class
membership what does that mean let's
take a look at the first ten predicted
probabilities to find out pause the
video for a moment while you examine the
output and unpause it once you've come
to a conclusion it turns out that each
row represents one observation and each
column represents a particular class
there are two columns because there are
two possible response classes 0 &amp;amp; 1 the
column on the Left known as column 0
shows us the predicted probability that
each observation is a member of class 0
the column on the right known as column
1 shows us the predicted probability
each observation is a member of class
one for each row these numbers add up to
one where do these numbers come from
explaining how logistic regression works
is beyond the scope of this video but
basically the model learns a coefficient
for each input feature during the model
training process and those coefficients
are used to calculate the likelihood of
each class for each observation in the
test set why might we care about these
predicted probabilities
since this model predicts the likelihood
of diabetes we might rank observations
by predicted probability of diabetes and
prioritize our patient preventive
outreach accordingly since it makes more
sense to contact someone with a 95%
chance of diabetes then a 55% chance
anyway it turns out that when you run
the predict method for a classification
model it first predicts the
probabilities for each class and then
chooses the class with the highest
probability as the predicted response
for a binary problem like this one
another way of thinking about it is that
there is a 0.5 classification threshold
and class 1 is predicted only if that
threshold is exceeded otherwise class 0
is predicted as you can see there are
only 2 instances shown in which the
probability of class 1 exceeds 0.5 and
those are the instances in which class 1
is predicted
let's now isolate the predicted
probabilities for class 1 since knowing
that alone enables you to calculate the
predicted probability for both classes
I use slicing notation here to tell
numpy that I wanted rows 0 through 9 of
the array but that I wanted all columns
of the array represented by the colon if
I just want column 1 I simply replace
the colon with the number 1 will store
these predicted probabilities in an
object called wipe read prob to
distinguish them from the predicted
classes that I stored in the wipe read
class object and of course we want the
predicted probabilities for all testing
set observations not just the first 10
and so we replace the 0 through 10 with
a colon
we're now going to plot a histogram of
these probabilities to help demonstrate
how adjusting the classification
threshold can impact the performance of
the model first I need to allow plots to
appear in the notebook and I'm also
overriding one of the default matplotlib
settings will use matplotlib to plot a
histogram of the predicted probabilities
of class 1 a histogram shows you the
distribution of a numerical variable we
can see by the height of this third bar
for example that about 45 of the
observations head values between point 2
and point 3 given the zero point 5
classification threshold that I
mentioned earlier we can see from the
histogram that class 1 is rarely
predicted since only a small minority of
the testing set observations had a
predicted probability above the
threshold think for a moment about what
might happen if we were to change the
threshold to a number other than 0.5 it
turns out that you can adjust both the
sensitivity and specificity of a
classifier simply by adjusting the
threshold
for example if we decrease the threshold
for predicting diabetes say two point
three we can increase the sensitivity of
the classifier that is like shifting the
threshold bar from here to here such
that all the observations with predicted
probabilities above 0.3 are now
predicted as class 1 this increases
sensitivity because the classifier is
now more sensitive to positive instances
if this is confusing the example of a
metal detector might be helpful it is
essentially a classifier which predicts
metal yes or no and a threshold is set
so that large metal objects set off the
detector but tiny ones do not how would
you increase the sensitivity of a metal
detector you would simply lower the
threshold amount of metal that is
required to set it off and thus it is
now more sensitive to metal and will
predict yes more often anyway let's now
lower the threshold for predicting
diabetes we can pass wipe read prob and
the threshold value of 0.3 to the
binarize function from SK learn
pre-processing it will return a 1 for
all values above 0.3 and a 0 otherwise
the results are in a 2-dimensional numpy
array and so we just slice out the first
dimension using the bracket notation and
save the results in the wipe read class
object
let's check that it worked
we'll print the first ten predicted
probabilities and then the first ten
predicted classes using the lower
threshold you can see that there are now
five instances in which class 1 is
predicted to see the impact of this
change across the entire testing set we
can print the previous confusion matrix
stored in the confusion object as well
as the new confusion matrix generated by
the confusion matrix function the row
totals have not changed since the rows
represent the actual response values so
there are still 130 observations in the
top row and 62 observations in the
bottom row but the column totals have
changed because a lot of the class 0
predictions have moved to class 1 you
can imagine observations from the left
column moving to the right column we can
recalculate sensitivity which is
increased from 0.24 to 0.74 as well as
specificity which is decreased from 0.91
to 0.62 why did specificity decrease
since observations move from the left
column to the right column that
guarantees that the number of false
positives will increase and true
negatives will decrease which decreases
specificity
as we've seen above a threshold of 0.5
is used by default to convert predicted
probabilities into class predictions
however you don't have to accept the
default threshold and can manually lower
it to increase sensitivity or raise it
to increase specificity depending on
your business objective however
sensitivity and specificity have an
inverse relationship so increasing one
will always decrease the other keep in
mind that adjusting the threshold is one
of the last steps you should take in the
model building process the majority of
your time should be focused on building
better models and then selecting the
best possible model during the previous
section you might have been thinking
that it seems incredibly inefficient to
search for an optimal threshold by
trying different threshold values one at
a time
wouldn't it be nice then if we could see
how sensitivity and specificity are
affected by various thresholds without
actually having to try each threshold it
turns out that there is a very simple
mechanism for doing this namely by
plotting the ROC curve let's take a look
at the ROC curve first you run the ROC
curve function from psych it learns
metrics module you pass it the true
values for the testing set stored in Y
tests and the predicted probability of
class 1 for each observation stored in y
pred prop it is critically important
that you use wipe read prob and not wipe
read class when creating the ROC curve
because using wipe read class will give
you incorrect
results without generating an error
anyway the ROC curve function returns
three objects and the next seven lines
of code simply creates a line plot with
the appropriate limits labels and so on
the ROC curve is a plot of the true
positive rate on the y axis against the
false positive rate on the x axis for
all possible classification thresholds
to use the terminology I've been
focusing on the y axis is sensitivity
and the x axis is 1 minus specificity
this plot tells you for example that if
you want to achieve a sensitivity of 0.9
you have to be willing to accept a
specificity of around point for the
optimal ROC curve hugs the upper left
corner of the plot since that would
represent a classifier with both high
sensitivity and high specificity in
summary the ROC curve can help you to
visually choose a threshold that
balances sensitivity and specificity in
a way that makes sense for your problem
unfortunately you can't actually see the
thresholds used to generate the ROC
curve on the curve itself however I've
written a small helper function called
evaluate threshold that allows you to
pass in a threshold value and see the
resulting sensitivity and specificity
when I pass in the value 0.5 I see our
original sensitivity and specificity of
0.24 and point 9 1 that threshold
generated the point right here on the
ROC curve when I pass in the value 0.3
I see our modified sensitivity and
specificity of 0.7 4 and point 6 2 which
can be found here
again the ROC curve is simply a plot of
sensitivity versus one - specificity for
all possible classification thresholds
from zero to one anyway given a
particular point on the ROC curve it
would be a simple trial and error
process to locate the threshold that
produce that point before we wrap up
this video there is one other term that
I want to introduce which is area under
the curve also known as a UC AUC is
quite literally the area under the ROC
curve meaning the percentage of this box
that is located under this curve because
an ideal classifier would hug the upper
left corner of this plot a higher AUC
value is indicative of a better overall
classifier as such a UC is often used as
a single number summary of the
performance of a classifier as an
alternative to classification accuracy
we can calculate the AUC for our model
using the ROC AUC score function from
the metrics module once again it's
important that we pass it the true
response values and then the predicted
probabilities not the predicted classes
are AUC is 0.72 whereas the
best-possible
AUC for any classifier is 1 it turns out
that AUC can also be interpreted as
follows if you've randomly chose one
positive observation and one negative
observation from your testing set AUC
represents the likelihood that your
classifier will assign a higher
predicted probability to the positive
observation it makes sense that this is
a useful goal because ultimately we want
the classifier to rank positive
observations higher than negative
observations in terms of predicted
probability finally it's worth noting
that AUC is a useful evaluation metric
even when there is high class imbalance
meaning that one of the classes
dominates if we were detecting
fraudulent transactions for example we
would expect that the vast majority of
transactions would not be fraudulent and
thus the null accuracy would be well
above 99% in this scenario AUC would be
a useful evaluation metric whereas
classification accuracy would not anyway
because AUC is useful as a metric for
choosing between models it is available
as a scoring function for cross Val
score as you can see here
in this video we've discussed many ways
to evaluate a classifier the confusion
matrix and the ROC curve are tools that
describe how your classifier is
performing and I would suggest that you
use both of them whenever possible the
main advantage of the confusion matrix
is that many evaluation metrics can be
calculated from it and you can focus on
the metrics that match your business
objectives as well it extends easily to
multi class problems in which there are
more than two response classes the main
advantage of ROC curves and AUC is that
they do not require you to choose a
classification threshold
unlike the confusion matrix as well they
are useful even when there is high class
imbalance
however they are less interpretable than
the confusion matrix for multi class
problems I've got many great resources
related to today's video so I'll
highlight a few first is my blog post on
confusion matrix terminology that may be
useful to you as a reference guide that
is followed by two excellent videos from
Rahul Patwari
which I highly recommend if you'd like
to get a better understanding of
sensitivity and specificity and IDI PO 2
gel a fellow data science instructor has
put together a great notebook explaining
how to calculate expected value from a
confusion matrix for a better
understanding of ROC curves I'd first
recommend that you read these excellent
lesson notes from the University of
Georgia which contain lots of real-life
examples as well check out my 14 minute
video explaining the ROC curve in more
depth using a nice interactive
visualization if you want even more
depth
the paper on our OSI analysis by Tom
Fossett is highly readable as usual
scikit-learn has excellent documentation
for learning more about model evaluation
I recently created a guide comparing
different model evaluation procedures
and metrics that summarizes a lot of
what we've learned in this video series
and finally if you want to learn about
sophisticated model evaluation in the
real world
check out this excellent video about how
stripe evaluates its fraud detection
model at least for now this is actually
going to be my final video in the site
kit learn video series I want to thank
you so much for joining me throughout
the series and also thanks for all your
kind comments if you'd like to join one
of my live online courses in which I
cover data science topics in more depth
please visit data school dot io / learn
thanks again and hope to see you soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>