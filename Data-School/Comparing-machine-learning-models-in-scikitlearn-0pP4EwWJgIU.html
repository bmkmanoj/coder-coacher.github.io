<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Comparing machine learning models in scikit-learn | Coder Coacher - Coaching Coders</title><meta content="Comparing machine learning models in scikit-learn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Data-School/">Data School</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Comparing machine learning models in scikit-learn</b></h2><h5 class="post__date">2015-05-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0pP4EwWJgIU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back to my video series on
scikit-learn for machine learning in the
previous video we learned about the K
nearest neighbors classification model
and the four key steps for model
training and prediction in scikit-learn
then we applied those steps to the iris
data set using three different models in
this video I'll be covering the
following how do I choose which model to
use for my supervised learning tasks how
do I choose the best tuning parameters
for that model and how do I estimate the
likely performance of my model on
out-of-sample data let's start by
reviewing where we ended up last time
our classification task was to predict
the species of an unknown iris we tried
using KN n with K equals 1 K and n with
K equals 5 and logistic regression and
receive three different sets of
predictions because this is
out-of-sample data we don't know the
true response values and thus we can't
actually say which model made the best
predictions however we still need to
choose between these three models the
goal of supervised learning is always to
build a model that generalizes to
out-of-sample data and that's what we
really need is a procedure that allows
us to estimate how well a given model is
likely to perform on out-of-sample data
this is known as a model evaluation
procedure if we can estimate the likely
performance of our three models then we
can use that performance estimate to
choose between the models there are many
possible model evaluation procedures but
in this video I'm going to focus on two
procedures
the first procedure is widely known but
it doesn't have an official name that
I'm aware of so I'm just going to call
it train and test on the entire dataset
the idea is simple we train our model on
the entire data set and then we test our
model by checking how well it performs
on that same data this appears to solve
our original problem which was that we
made some predictions but we couldn't
check whether those predictions were
correct by testing our model on a data
set for which we do actually know the
true response values we can check how
well our model is doing by comparing the
predicted response values with the true
response values let's start by reading
in the iris data and then creating our
feature matrix X and our response vector
Y
we'll try logistic regression first we
follow the usual pattern which is to
import the class instantiate the model
and fit the model with the training data
then we'll make our predictions by
passing the entire feature matrix X to
the predict method of the fitted model
and print out those predictions let's
store those predictions in an object
called wipe read as you can see it made
150 predictions which is one prediction
for each observation now we need a
numerical way to evaluate how well our
model performed the most obvious choice
would be classification accuracy which
is the proportion of correct predictions
this is known as our evaluation metric
there are many possible evaluation
metrics and we'll learn about other
evaluation metrics in future videos
anyway let's compute the classification
accuracy for our logistic regression
model I can think of at least three
different ways to do this but I'm going
to show you the one way I recommend
which is to use the metrics module from
scikit-learn first we import the metrics
module then we use the accuracy score
function and pass it the true response
values followed by the predicted
response values it returns a value of
zero point nine six that means that it
compared the 150 true responses with the
corresponding 150 predicted responses
and calculated that 96% of our
predictions were correct this is known
as our train
accuracy because we're testing the model
on the same data we used to train the
model will now try KNN using the value k
equals 5 we import the class instantiate
the model using the argument and
neighbors equals 5 fit it with the
training data make predictions on the
same data and calculate our
classification accuracy this time we get
0 point 9 6 7 which is slightly better
than logistic regression finally we'll
try KNN using the value k equals 1 this
time we get a score of one point zero
meaning 100% accuracy it performed even
better than the other two models and so
we would conclude that KN with k equals
1 is the best model to use with this
data or would we draw that conclusion
think about that for a second let's go
back for a minute to the previous video
in which I talked about how the KN model
actually works to make a prediction it
looks for the K observations in the
training data with the nearest feature
values it tallies the actual response
values of those nearest observations and
then whichever response value is most
popular is used as the predicted
response value for the unknown
observation with that in mind you could
figure out exactly why a KN model with K
equals 1 would always have 100% training
accuracy to make a prediction for any
observation in the training set
KN would search for the one nearest
observation in the training set and it
would find that exact
same observation in other words KNN has
memorized the training set and because
we're testing on the exact same data it
will always make correct predictions at
this point you might conclude that
training and testing your models on the
same data is not a useful procedure for
deciding which models to choose and you
would be correct remember that our goal
here is to estimate how well each model
is likely to perform on out-of-sample
data meaning future observations in
which we don't know the true response
values if what we try to maximize is
training accuracy then we're rewarding
overly complex models that won't
necessarily generalize to future cases
in other words models with a high
training accuracy may not actually do
well when making predictions on
out-of-sample data creating an
unnecessarily complex model is known as
overfitting models that overfit have
learned the noise in the data rather
than the signal in the case of KNN a
very low value of K creates a high
complexity model because it follows the
noise in the data this is a nice diagram
that I think explains overfitting quite
well each point represents an
observation the x and y locations
represent its feature values and the
color represents its response class for
a classification problem you want your
model to learn that generally speaking
points over here are red and points over
here
our blue in other words you want your
model to learn that this black line also
known as the decision boundary is a good
boundary for classifying future
observations as red or blue it doesn't
do a perfect job classifying the
training observations but it's likely to
do a great job classifying out-of-sample
data a model that instead learns the
green line as the decision boundary is
over fitting the data it does a perfect
job classifying the training
observations but it probably won't do as
well as the black line when classifying
out-of-sample data the green line has
learned the noise in the data whereas
the black line has learned the signal
since training and testing on the same
data is not an optimal model evaluation
procedure will need a better procedure
the procedure I'm now going to show you
is called train test split there are
other names for this approach such as
the test set approach or the validations
that approach but please note that those
terms sometimes refer to a slightly
different procedure from what I'm going
to demonstrate in this lesson anyway
here's how the procedure works first we
split the data into two pieces which we
call a training set and a testing set we
train the model on the training set and
then we test the model on the testing
set to evaluate how well we did that's
really all there is to it
the key idea here is that because we're
evaluating the model on data that was
not used to train the model we're more
accurately simulating how well a model
is likely to perform on out-of-sample
data
let's go ahead and apply this procedure
to our iris data before we apply the
first step let's remind ourselves of the
shapes of x and y X is our feature
matrix made up of 150 rows representing
the observations and four columns
representing the features Y is our
response vector which simply contains
our 150 response values to split the
data into training and testing sets
we're going to use psych it learns built
in trained tasks split function will
import it and then use this somewhat
cryptic command to split the x and y
objects into two pieces each it's
important to understand what's happening
and why so I've created a diagram to
explain the Train test split function
for the moment forget about the iris
data pretend that we have a data set
with five observations consisting of two
features and a response value the
response value is numeric meaning that
this is a regression problem our X
matrix is five rows by two columns and
our Y vector just has five values if you
ran the Train test split function on x
and y it would split X into X train and
X test which I've colored yellow and
blue and it would split Y into y train
and Y test which I've colored orange and
purple what was the point of this well I
now have a feature matrix X train that
is size three by two
and a response vector why train the
Desai's three and I can use those
objects to train the model then I can
make predictions on X test and compare
those predictions to the actual response
values in Y test in order to calculate
what is known as my testing accuracy
because I'm training and testing the
model on different sets of data the
resulting accuracy is a better estimate
of how well the model is likely to
perform on future data the obvious
question is how does train test split
decide which observations and how many
observations are assigned to the
training set versus the testing set this
optional test size parameter determines
the proportion of observations assigned
to the testing set in this case I've
assigned 40 percent of observations to
the testing set which means that 60
percent will be assigned to the training
set there's no general rule as to what
percentage is best but people generally
use between 20 and 40 percent of their
data for testing in terms of how the
observations are assigned it's actually
a random process you'll find that if you
run this function five different times
on the same set of data it will split
the data five different ways however if
you use an optional parameter called
random state and give it an integer
value it will split a given data set the
exact same way every single time I'm
going to use random state equals four
and if you use the same random stated
home your data will be split exactly the
same way anyway let's now check the
shape of these four objects to confirm
that it matches our expectations the
original X of size 150 by four has been
split into two pieces in which X train
is 90 by 4 and X test is 60 by 4 the
original Y of size 150 has also been
split into two pieces in which Y train
is size 90 and Y test is size 60 we're
now ready for step two which is to train
our model on the training set will
instantiate a logistic regression model
and fit it to X train and y train then
in step 3
we'll make predictions for the
observations in the testing set by
passing X test to the predict method and
store the results in wipe red because we
know the true response values for the
testing set we can compare the predicted
values with the actual values stored in
Y test
we see that this model achieved a
testing accuracy of 0.9 5
let's repeat steps 2 &amp;amp; 3 for our KNN
models again with K equals 5 and K
equals 1 for K equals 5 we achieve a
testing accuracy of 0.9 6 7 and for K
equals 1 we achieve a testing accuracy
of 0.9 5 we would therefore conclude
that out of these three models KN with K
equals 5 is likely to be the best model
for making predictions on out-of-sample
data naturally you might wonder whether
we can find an even better value for K
I've written a for loop to do exactly
that in which I try every value of K
from 1 through 25 and then record K ends
testing accuracy in this Python list
called scores
I'm then going to use matplotlib the
predominant Python library for
scientific plotting to plot the
relationship between the value of X and
the testing accuracy in general as the
value of K increases there appears to be
a rise in the testing accuracy and then
a fall this rise in fall is actually
quite typical when examining the
relationship between model complexity
and testing accuracy as we talked about
earlier training accuracy rises as model
complexity increases and the model
complexity for K and n is determined by
the value of K testing accuracy on the
other hand penalize a--'s models that
are too complex as well as models that
are not complex enough therefore you'll
see maximum testing accuracy when the
model has the right level of complexity
in this case we see the highest accuracy
from K equals 6 through K equals 17 and
we would tentatively conclude that a K
value in that range would be better than
K equals 5 however because this data set
is so small and because this is such an
easy classification task it's hard to
reliably say whether the behavior we're
seeing in this one plot will indeed
generalize regardless plotting testing
accuracy versus model complexity is a
very useful way to tune any parameters
that relate to model complexity
once you've chosen a model and it's
optimal parameters and are ready to make
predictions on out-of-sample data it's
important that you retrain your model on
all of the available training data
otherwise you'll be throwing away
valuable training data in this case
we'll choose a value of 11 for K since
that's in the middle of the K range with
the highest testing accuracy and we'll
call that our best model thus we
instantiate the KNN model with n
neighbors equals 11 we fit the model
with X&amp;amp;Y and we use the model to make a
prediction as we wrap up a natural
question might be whether there are any
downsides to the Train test split
procedure for model evaluation it turns
out that train tests split provides a
high variance estimate of out-of-sample
accuracy meaning that it can change a
lot depending upon which observations
happen to be in the training set versus
the testing set there's an alternative
model evaluation procedure called k-fold
cross-validation that largely overcomes
this limitation by repeating the Train
test split process multiple times in a
systematic way and averaging the results
we'll go over that procedure in a future
video
regardless train tests split remains a
useful procedure because of its
flexibility and speed and we will
continue to use it throughout this
series I've got a number of excellent
resources to share with you this week if
you want to go deeper into this material
first is a post from Chora that gives an
intuitive explanation for overfitting
in just a few short paragraphs next is a
video from hasty and tip shirani
the authors of an introduction to
statistical learning which quickly
covers many of the same concepts I went
through today after that is one of my
favorite educational articles for data
science it's called understanding the
bias-variance tradeoff and this is the
article you should read if you want to
really understand why testing accuracy
exhibits that upside-down u-shaped curve
when you very model parameters such as K
not only does it do a great job
explaining a difficult concept but it
also has this extremely cool interactive
visualization that will help you to
better understand K nearest neighbors
as a bonus I've also linked to some
guiding questions that I give to my data
science students that may help to focus
your own reading finally if you're still
struggling with the bias-variance
tradeoff after reading that article i've
linked to a great video from Cal Tech's
learning from data course that may help
you to visualize bias and variance so
far in this series we focused on
classification problems in which the
goal is to predict a categorical
response in the next video we'll expand
our scikit-learn
toolbox by learning about a machine
learning model for regression in which
the goal is to predict a continuous
response we'll also learn how to read a
data set in two pandas a very popular
library for data analysis and
exploration so that we can work with the
data in scikit-learn I'd love to hear
from you in the comment section below
what do you like about this series so
far and what could be better what would
you like to learn in the next few videos
and what questions do you have I value
your feedback and I read every single
comment thanks as always for watching
and I'll see you again soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>