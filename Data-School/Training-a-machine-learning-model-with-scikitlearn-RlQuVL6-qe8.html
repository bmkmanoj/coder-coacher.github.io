<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Training a machine learning model with scikit-learn | Coder Coacher - Coaching Coders</title><meta content="Training a machine learning model with scikit-learn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Data-School/">Data School</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Training a machine learning model with scikit-learn</b></h2><h5 class="post__date">2015-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RlQuVL6-qe8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back to my video series on
machine learning with scikit-learn in
the previous video we discussed the
famous iris data set and loaded it into
scikit-learn we learned some important
machine learning terminology such as
features response observations
regression and classification and
finally we discussed psych it learns
four key requirements for working with
data in this video I'll be covering the
following what is the K nearest
neighbors classification model what are
the four steps from model training and
prediction in scikit-learn and how can I
apply this pattern to other machine
learning models let's first do a quick
review of the iris data set that I
introduced last time since we'll be
using this data when training our model
there are a hundred and fifty
observations and each observation
represents an iris flower there are four
features represented by the first four
columns of data which are the sepal and
petal measurements for each iris and the
response variable is the species of each
iris shown in the fifth column because
our response variable is categorical
this is known as a classification
problem before I introduced the K
nearest neighbors classification model
let's first talk about how we as humans
might approach this task specifically
how might we predict the species of an
unknown iris given its measurements when
looking at the data we might notice that
the three iris species in the data set
appear to have somewhat dissimilar
measurements if that's the case we would
hypothesize that the species of an
unknown iris could be predicted by
looking for irises in the data with
similar measurements and assuming that
our
unknown iris is the same species as
those similar irises the process I just
described is similar to how the
k-nearest neighbors classification model
works the steps of K nearest neighbors
or KNN are as follows first you pick a
value for K such as 5 we'll talk in the
next video about how to choose this
value second the model searches for the
5 observations in the training data that
are nearest to the measurements of the
unknown iris in other words the model
calculates the numerical distance
between the unknown iris and each of the
150 known irises and selects the five
known irises with the smallest distance
to the unknown iris note that Euclidean
distance is often used as the distance
metric but other metrics can be used
instead
third the response values of the five
nearest neighbors are tallied and
whichever response value is the most
popular is used as the predicted
response value for the unknown iris
let's look at a few visualizations to
make this algorithm more clear first we
have some example training data which is
not the iris data set in this case you
can think of this as a data set with two
numerical features represented by the x
and y coordinates each point represents
an observation and the color of the
point represents its response class
which is red blue or green
next we have a KNN classification map in
which the K value is 1 the background of
the diagram has been colored red for all
areas in which the nearest neighbor is
red colored blue for all areas in which
the nearest neighbor is blue and colored
green for all areas in which the nearest
neighbor is green in other words the
background color tells you what the
predicted response value would be for a
new observation depending upon its X&amp;amp;Y
features if a new point was right here
for example its predicted response class
would be green because its nearest
neighbor is green finally we have a KNN
classification map in which the K value
is 5 you can see that the boundaries
between colors known as decision
boundaries have changed because more
neighbors are taken into account when
making predictions the predicted
response for a new observation here is
now blue instead of green because four
of its five nearest neighbors are blue
the white areas by the way our areas in
which KN can't make a clear decision
because there's a tie between two
classes as you can see KN is a very
simple machine learning model but it can
make highly accurate predictions if the
different classes in the data set have
very dissimilar feature values anyway
let's actually use KNN with the iris
dataset in scikit-learn
in the previous video we learned how to
load the iris data from the data sets
module of scikit-learn and then we
stored the features in X and the
response in Y let's quickly verify that
X and y have the appropriate shapes we
can see that X is a two dimensional
array
with a hundred and fifty rows and four
columns as expected Y is a one
dimensional array with length 150 since
there's one response value for each
observation when loading your own data
into scikit-learn make sure to meet the
four key requirements of input data that
I outlined in the previous video
now let's begin the actual machine
learning process scikit-learn provides a
uniform interface to machine learning
models and thus there's a common pattern
that can be reused across different
models the first step in this pattern is
to import the relevant class in this
case we import que neighbors classifier
from SK learn a burst
scikit-learn is carefully organized into
modules such as neighbors so that it's
easy to find the class you're looking
for the second step in the site kit
learn pattern is to instantiate the
estimator what does that mean well
scikit-learn refers to its models as
estimators because their primary role is
to estimate unknown quantities this
process is called instantiation because
we're creating an instance of the que
neighbors classifier class
we have now created an instance of the
que neighbors classifier class and
called it cannon in other words we now
have an object called KNN that knows how
to do K nearest neighbors classification
and it's just waiting for us to give it
some data before we move on there are
three important notes about
instantiating the estimator first it
doesn't matter what you name the
estimator object I tend to choose a name
that reflects the type of model it
represents though you might choose to
call it est short for estimator or CLF
short for classifier second notice that
I specified the argument and neighbors
equals one that is how we tell the can
and object that when it runs the K
nearest neighbors algorithm it should be
looking for the one nearest neighbor and
neighbors is known as a tuning parameter
or a hyper parameter which we'll talk
more about in the next video third note
that there are other parameters for
which I did not specify a value and thus
all of those parameters are set to their
default values by printing out the
estimator object we can see the default
values for all of those parameters
thankfully scikit-learn provides
sensible defaults for its models so that
you can get started with a new model
without researching the meaning of every
parameter
let's move on to the third step which is
to fit the model with data this is the
model training step in which the model
learns the relationship between the
features and the response though the
underlying mathematical process through
which this learning occurs varies by
model I simply use the fit method on the
KNN object and pass it two arguments the
feature matrix X and the response vector
Y this operation occurs in place which
is why I don't need to assign the
results to another object the fourth and
final step is to make predictions for
new observations in other words I'm
inputting the measurements for an
unknown iris and asking the fitted model
to predict the iris species based on
what it has learned in the previous step
I use the predict method on the KNN
object and pass it the features of the
unknown iris as a Python list it's
expecting a numpy array but it still
works with a list since numpy
automatically converts it to an array of
the appropriate shape unlike the fit
method the predict method does return an
object namely a numpy array with the
predicted response value in this case
the K nearest neighbors algorithm using
K equals 1 predicts a response value of
2 scikit-learn doesn't know what this
two represents so we need to keep track
of the fact that 2 was the encoding for
virginica and thus virginica is the
predicted species for the unknown iris
as you might expect this predict method
can be used on multiple observations at
once in this case
going to create a list of lists called
ex-new which contains two new
observations when I pass X new to the
predict method it again gets converted
to a numpy array this time with a shape
of two by four which is interpreted as
two observations with four features each
the predict method returns an umpire
array with values two and one which
means that the prediction for the first
unknown iris was a two and the
prediction for the second unknown iris
was a 1 let's say you wanted to try a
different value for K such as five this
is known as model tuning in which you're
varying the arguments that you pass to
the model note that you don't have to
import the class again you just
instantiate the model with the argument
and neighbors equals five fit the model
with the data and make predictions this
time
the model predicts the value one for
both unknown irises
one of the things I love about
scikit-learn is that its models have a
uniform interface which means that I can
use the same four step pattern on a
different model with relative ease for
example I might want to try logistic
regression which despite its name is
another model used for classification I
simply import logistic regression from
the linear model module instantiate the
model with all of the default parameters
fit the model with data and make
predictions this time the model predicts
a value of two for the first unknown
iris and a value of zero for the second
unknown iris of course you might be
wondering which model produce the
correct predictions for these two
unknown irises the answer is that we
don't know because these are
out-of-sample observations meaning that
we don't know the true response values
as we talked about in the first video
our goal with supervised learning is to
build models that generalize to new data
however we often aren't able to truly
measure how well our models will perform
on out-of-sample data does that mean
that we're forced to just guess how well
our models are likely to do thankfully
no in the next video we'll begin to
discuss model evaluation procedures
which allow us to estimate how well our
models are likely to perform on
out-of-sample data using our existing
labelled data these procedures will help
us to choose which value of K is best
for kN and to choose whether K and N or
logistic regression is a better choice
for our particular task
in the meantime I've linked to a few
resources that might be helpful to you
first is the nearest neighbor section of
the scikit-learn user guide it can help
you to understand the available nearest
neighbor algorithms and how to use them
effectively also worth reviewing is the
class documentation for que neighbors
classifier it's useful to become
familiar with the structure of the class
documentation since all classes are
documented in the same manner all of the
parameters and their default values are
listed at the top and described directly
below
this is often followed by usage notes
brief examples and the methods and their
parameters occasionally there are also
longer usage examples at the bottom of
the page I've also linked to the user
guide and class documentation for
logistic regression
finally I've linked to this post from my
blog in which I've linked to fifteen
hours of videos associated with the book
and introduction to statistical learning
my favorite book for introducing machine
learning this video from Chapter two
discusses K nearest neighbors and these
two videos from Chapter four introduce
logistic regression just be aware that
because logistic regression is an
extension of linear regression it may be
hard to follow these two videos if you
are not already familiar with linear
regression as always it's been a
pleasure sharing this lesson with you
and I look forward to your comments and
questions please subscribe on YouTube if
you'd like to keep up with this series
thanks for watching and I'll see you
again soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>