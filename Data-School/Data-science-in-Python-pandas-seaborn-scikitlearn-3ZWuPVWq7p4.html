<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data science in Python: pandas, seaborn, scikit-learn | Coder Coacher - Coaching Coders</title><meta content="Data science in Python: pandas, seaborn, scikit-learn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Data-School/">Data School</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data science in Python: pandas, seaborn, scikit-learn</b></h2><h5 class="post__date">2015-05-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3ZWuPVWq7p4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back to my video series on
machine learning in scikit-learn in the
previous video we learned how to
properly evaluate a model using the
Train test split procedure we were
focusing on classification models and
our evaluation metric was classification
accuracy in this video I'll be covering
the following how do I use the pandas
library to read data into Python how do
I use the Seabourn library to visualize
data what is linear regression and how
does it work
how do I train and interpret a linear
regression model in scikit-learn what
are some evaluation metrics for
regression problems and how do I choose
which features to include in my model so
far in this series we focused on
classification in which the goal is to
predict a categorical response in
contrast regression is a type of
supervised learning in which the goal is
to predict a continuous response
regression problems are the focus of
today's video
before we start talking about regression
we're going to choose a data set and
read it into Python using pandas an
extremely popular library for data
exploration manipulation and analysis if
you're using the Anaconda distribution
of Python pandas and its dependencies
are already installed otherwise I've
linked to the installation instructions
we'll start by importing pandas in the
conventional way which is to import
pandas as PD I've selected a data set
for today's lesson from the book and
introduction to statistical learning the
data set is posted online as a CSV file
which stands for comma separated values
CSV files are very common way to store
data in which each observation is a
single line in the file and the fields
are separated by commas
pandas has a function for reading in CSV
files called reads CSV and you simply
pass in the name of the file you can
read files from your local computer or
you can actually read files directly
from a URL which is what I'm doing here
I'm going to save the results as an
object called data and then run the head
method on that object to see the first
five rows of data the results are
displayed kind of like a spreadsheet
this object is called a panda's data
frame which consists of rows and columns
a single column in a data frame is known
as a pandas series anyway pandas has
figured out that the first row in the
CSV file contains the column headers
namely TV radio and so on however
it looks like there's an unnamed column
that contains sequential numbers
starting at one so those are probably
just the ID numbers for those
observations I'm going to take those
numbers and use them as the index which
is how pandas identifies the rows the
default index is sequential numbers
starting at zero shown on the left side
in bold to figure out how to set those
ID numbers as the index let's look at
the help for read CSV I'll click within
the parentheses and then hit shift tab
on my keyboard twice
you can see that read CSV has a ton of
parameters that allow you to control the
CSV reading process in great detail in
our case we need to use the index call
parameter
which allows us to set a specific column
as the index so we'll type index call
equals zero and then rerun this cell you
can see that the unnamed column has now
been set as the index data frames also
have a tail method which shows you the
last five rows because the highest index
number is 200 we would assume that the
data frame has 200 rows but we can
confirm this by printing out its shape
attribute the output tells us that there
are 200 rows by four columns now that
we've read in the data let's talk about
the data and how to structure it as a
supervised learning task the first
column TV shows the amount of money in
thousands of dollars spent on TV ads to
advertise a single product in a given
market or city so for example in market
200 about two hundred and thirty two
thousand dollars was spent on TV ads
similarly eight thousand six hundred
dollars was spent in market two hundred
on radio ads and eight thousand seven
hundred dollars was spent in market two
hundred on newspaper ads the sales
column represents the sales of the
product being advertised in that market
in thousands of items so in market two
hundred a quantity of 13 thousand four
hundred was sold
in this case let's attempt to predict
sales based on advertising dollars thus
we'll use TV radio and newspaper as the
features and we'll use sales as the
response because our response variable
is continuous this is a regression
problem and just to be clear our dataset
has 200 observations and each
observation represents a single market
before we start working through the
machine learning process let's visualize
our data to get a better feel for it I'm
going to use the Seabourn library a
python library for statistical data
visualization that is built on top of
matplotlib if you're using anaconda you
can run kondeh install Seabourn from the
command line to easily install it for
other Python users I've linked to the
installation instructions let's go ahead
and import Seabourn as SNS and also run
matplotlib in line known as a magic
command to allow plots to appear within
the notebook often the first
relationship you want to visualize is
the relationship between each of the
features and the response variable this
can easily be done using Seaborn's pair
plot function which produces pairs of
scatter plots for each x and y variable
that you specify these plots are a bit
small and so I'm going to change the
size and aspect ratio so that I can see
the data more easily
you can see that there are somewhat of a
linear relationship between TV and sales
meaning that is TV advertising increases
sales increases in a somewhat linear
fashion there appears to be a less
strong relationship between radio
advertising and sales and a weak
relationship between newspaper
advertising and sales we can actually
ask Seaborn to plot these relationships
by adding one more argument to pair plot
Seabourn has added a line of best fit as
well as a 95% confidence band because
there appears to be a linear
relationship between the features and
the response this is a great candidate
for the linear regression method linear
regression is quite a deep topic but I'm
just going to give you a brief
introduction before we implement it in
scikit-learn to start I want to make
clear the difference between regression
and linear regression regression as it
relates to machine learning is simply a
type of supervised learning problem in
which the response is continuous linear
regression is a particular machine
learning model that can be used for
regression problems and it just happens
to have the word regression in its name
anyway linear regression is a very
popular modeling technique for four main
reasons first it runs quickly which
becomes increasingly important as the
size of your data set increases second
there's no tuning required in the way
that we had to tune the value of K for K
and n which makes it easy to get started
third it's highly interpretive all
meaning that you can easily understand
the model and how it's making
predictions and finally it has been
studied for many years and is well
understood and thus there's a vast
amount of literature on the topic of how
to use linear regression properly in
terms of drawbacks the main drawback to
linear regression is that it's unlikely
to produce the best predictive accuracy
as compared to other models this is
because linear regression presumes a
linear relationship between the features
and the response if the relationship is
highly nonlinear as happens in many
real-world scenarios linear regression
will be unable to effectively model the
relationship and thus it's predictions
will not be very accurate let's take a
look at the functional form of linear
regression in order to gain an
understanding of how it works it can be
represented as follows y equals beta
naught plus beta 1 X 1 plus beta 2 X 2
all the way to beta n X n in which n is
the number of features let's briefly
discuss each of the model terms Y is
simply the response value each of the
features is represented by an X variable
and each feature has a coefficient in
this case we have three features TV
radio and newspaper and each feature has
a beta value beta 1 and beta 2 and beta
3 finally beta naught or beta 0 is
called the intercept which is the value
of y when all of the X values are 0
these beta values as well as the
intercept are learned during the model
fitting process using what is called the
least-squares criterion basically linear
regression seeks to find the line that
best fits the observed data as we can
see here it defines the best line as the
one that minimizes the sum of squared
errors which is really just the sum of
the squared vertical distances
between each point and the line once
this line of best fit has been learned
it can be used to make predictions for
sales given any set of feature values
before we start the modeling process
with scikit-learn we first have to
define the feature matrix X and the
response vector Y remember that
scikit-learn is expecting x and y to be
numpy arrays however we're lucky in that
pandas is built on top of numpy meaning
that there is a numpy array actually
storing the data frame data therefore
our X can be a panda's data frame and
our Y can be a pandas series and
scikit-learn will understand how to
access the underlying numpy arrays let's
start with X what we need to create is a
data frame that contains only our three
feature columns so first let's create a
Python list called feature calls that
contains the names of our feature
columns store to strings then we can say
data open bracket feature calls closed
bracket which tells pandas to select a
subset of the original data frame
columns you will often see these two
steps done in a single line as shown
here these double brackets can be
confusing so just remember that the
outer bracket is how you tell pandas
that you want to select a subset of data
frame columns and the inner bracket is
how you define a Python list
finally let's run the head method on X
to confirm that the operation worked you
can see that it's still a data frame but
now containing only our three feature
columns we can use pythons type function
to confirm that it's a data frame and we
can print the shape attribute to confirm
that it contains 200 rows and three
columns now let's move on to why our
response value is sales and we can
select the sale series from the data
frame using the bracket notation this
time however you only use one set of
brackets and you pass in a string
containing the name of the column
alternatively if there are no spaces in
the name of the column you can select
the sale series as an attribute of the
data frame will again run the head
method on the series to check the
results and again we can use the type
function to confirm that it's a series
and print the shape attribute to confirm
that it's a one-dimensional array with
length 200
our final step before using linear
regression is to split X&amp;amp;Y into training
and testing sets for proper model
evaluation as we saw in the last video
we use the Train test split function to
split X&amp;amp;Y
into two objects each will also print
the shapes of these objects we didn't
specify the size of the testing set and
so it defaults to using 25% of the data
for testing
finally let's actually build our linear
regression model using scikit-learn we
follow the same pattern we've used
previously which is to import the model
instantiate the model and then fit the
model to the training data every model
differs in terms of what occurs during
the fitting step in the case of que
neighbor's classifier all the model did
during fitting was to memorize the
training data so that it could later
calculate the distance between a new
observation and the existing
observations in the case of linear
regression the model is learning the
intercept and coefficients for the line
of best fit then it has an easy formula
for making predictions
during the predict step which we'll see
below
as I mentioned previously linear
regression is a highly interpretable
model let's see what that means by
printing out the intercept and the
coefficients and then interpreting them
the intercept and coefficients are
stored in separate attributes of the
linreg object which is why I use the dot
notation to access them you'll notice
they both have a trailing underscore in
their names which is scikit-learn z--
convention for any attributes that were
estimated from the data anyway the
coefficients are stored in the Co F
attribute in the same order as they were
stored in the feature matrix X it can be
hard to remember that order
so I like to use pythons zip function to
pair each of the feature names from the
feature calls list with the coefficients
I've plugged the intercept and
coefficients into the linear regression
formula this formula now allows us to
input the TV radio and newspaper ad
spending in a given market and output a
prediction for the amount of sales which
is the Y variable let's now interpret
the TV coefficient which is zero point
zero four six six it means that for a
given amount of radio and newspaper ads
spending a unit increase in TV ad
spending is associated with a point zero
four six six unit increase in sales that
makes sense when looking at the formula
because increasing TV by one would
mathematically increase the y response
variable by 0.046
six however keep in mind that a unit of
ad spending represents $1,000 any unit
of sales represents a thousand items
thus the more clear interpretation of
the TV coefficient is that an additional
thousand dollars spent on TV ads is
associated with an increase in sales of
46.6 items this same process could be
repeated to interpret the radio and
newspaper coefficients again this level
of interpretability
is why linear regression is such a
popular model for regression problems
before we move on to the prediction step
I want to add two important notes first
I was careful to use the phrase
associated with when interpreting the TV
coefficient because this is not a claim
of causation it's a very difficult
problem to determine causation because
that relies upon having access to every
possible factor that could have
influenced sales
whereas all we have is data on ad
spending this is why machine learning
tends to focus on Association rather
than causation second I want to make
clear that linear regression
coefficients can be negative for
instance if an increase in TV ad
spending was associated with a decrease
in sales the beta 1 coefficient would
have been negative
anyway let's use our fitted model to
make predictions on the testing set
which we store in wipe read previously
we've used classification accuracy as
our evaluation metric though that metric
is not relevant for regression problems
because regression problems have a
continuous response let's take a look at
some common evaluation metrics for
regression and then choose one to
evaluate our predictions we'll start by
creating some example numeric
predictions and then evaluate them using
a given metric so that we can get a feel
for how those metrics work in this
contrived example the true response
values are 150 30 and 20 and the
predicted values are 90 50 50 and 30
mean absolute error or ma e is the
simplest metric it's the mean of the
absolute value of the errors the error
is simply the difference between the
true and predicted values so if we
wanted to calculate our ma e by hand we
would add 10 + 0 + 20 + 10 and then
divide by 4 scikit-learn can also
calculate the mean absolute error for us
using a function from the metrics module
the ma e in this case is 10 which you
can just think of as the average error
mean squared error or MSE is very
similar to mean absolute error except
that each of the errors is squared so we
would calculate 10 squared plus 0
squared plus 20 squared plus 10 squared
and divide the total by 4 which would be
150 mean squared error is a bit harder
to interpret than mean absolute error
root mean squared error or RMS e is
identical to mean squared error except
that the square root is taken at the
very end of the calculation I use the
square root function inside the numpy
package to calculate the square root
though there are other ways to do this
our MSC is around 12 you might have
noticed that our our MSE is a bit larger
than our mean absolute error due to the
fact that our MSE squares the errors and
thus increases the weight of larger
errors let's briefly compare these
metrics mean absolute error is certainly
the easiest to understand mean squared
error is more popular than mean absolute
error since it punishes larger errors
and it tends to be the case in
real-world applications that minimizing
larger errors is more important than
minimizing smaller errors but root mean
squared error is even more popular than
mean squared error because our MSE is
interpretable in the Y units in other
words our MSE of 150 is a bit hard to
interpret
while our R
s-see of around 12 is easier to put into
context since it's in the same units as
our response variable as such let's go
ahead and choose our MSE as our
evaluation metric and then evaluate our
sales predictions will now compute our
our MSE and see that it's about 1.4 this
seems pretty good given that our sales
ranged from about 5 to 25 so what should
we do from here in the last video we saw
that train tests split could help us to
choose between different models and
different tuning parameters linear
regression doesn't have any tuning
parameters and we haven't yet learned
other models for aggression so those
options don't apply however note that
train tests split can also help us to
choose between features when we
visualize the data we saw that the
newspaper feature appeared to have a
very weak correlation with sales thus
let's try removing that from our model
and see how that affects the our MSE
this looks like a lot of code but it's
really just a repeat of the code we've
used previously except that newspaper
has been removed from the list of
feature calls in the first step thus our
feature matrix X only contains TV and
radio we again do a trained test split
fit the model make predictions and
compute the rmse the rmse this time is
about 1.3 9 a slight decrease from our
previous model unlike classification
accuracy in which higher numbers are
better error is something we want to
minimize and thus lower numbers for our
MSE are better thus our new model that
excludes newspaper is performing
slightly better than when it was
included indicating that the newspaper
feature should be left out of the model
you could repeat this process with
different combinations of features and
then select the combination with the
lowest our MSE as the best combination
to use for this particular problem I've
listed some great resources here for
going deeper into linear regression
pandas and Seabourn first is a longer
notebook on linear regression that I use
when teaching my data science class it
uses the same data as this notebook but
it also includes other topics such as
confidence intervals hypothesis testing
p-values and r-squared it also
demonstrates linear regression using
stats models which is a Python library
focused on statistics next is chapter 3
from an introduction to statistical
learning
which also uses the same data as this
notebook but goes into linear regression
in much more depth than either of my
notebooks after that is a quick
reference guide to applying and
interpreting linear regression which I
wrote to summarize many of the key
points from Chapter three
finally is an excellent introduction to
linear regression by Robert now of Duke
it's very detailed mathematically
thorough and includes lots of useful
advice if you want to make the most of
your linear regression models for an
introduction to pandas I really like
this three-part tutorial
written by Gregg Rita if you're trying
to read your own CSV files into pandas I
recommend that you read through the read
CSV documentation and if you're trying
to read a different type of tabular file
into pandas that is not comma delimited
you'll want to check out the read table
documentation if you'd like to learn
Seabourn the official tutorial is quite
good and the example gallery will give
you a quick look at most of its
functionality congratulations on getting
this far in the series I've got a lot
more content planned and your comments
have been helpful in shaping the series
so thank you one question I have for you
this week is whether you'd like me to
focus more on pandas or instead focus
exclusively on scikit-learn let me know
in the comment section below and then
I'll see you again soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>