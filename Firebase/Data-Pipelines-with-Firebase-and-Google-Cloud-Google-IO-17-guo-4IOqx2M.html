<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Pipelines with Firebase and Google Cloud (Google I/O '17) | Coder Coacher - Coaching Coders</title><meta content="Data Pipelines with Firebase and Google Cloud (Google I/O '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Firebase/">Firebase</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Pipelines with Firebase and Google Cloud (Google I/O '17)</b></h2><h5 class="post__date">2017-05-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/guo-4IOqx2M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to data pipelines with
firebase and Google cloud I am Alex
memory and I have Whittier meet today
Chris Rayner the both of us are software
engineers on firebase hosting we've
built and we've rebuilt data pipelines
based around firebase you may have seen
one to add your own domain to firebase
hosting using the custom domain
provisioning system we've also done live
user triggered data migrations from one
back-end to another our goal here today
is to introduce you to some concepts and
best practices that we've had to learn
the hard way so what is a data pipeline
well for the purpose of this talk we're
going to call a data pipeline a body of
work to be done not on the client
specifically and often as a result of
changes by a client we have some
examples to illustrate some of the
reasons why you might require a data
pipeline the first of which is trusted
code so this is the kind of code that
you don't want to run on clients the
examples of this include of loading
gamestate or running financial
transactions imagine every user of a
game allowed to upload their own score
and how quickly that would devolve into
chaos
second we have business logic this is
code you might not want to run on your
client because it's your secret sauce
you want to protect it you don't want
other people to be able to use it to
find it it might be a recommendation
engine or something else do you want to
change quickly and not wait for all of
your clients to update all of your users
update their clients the third example
is aggregation of data this is perhaps
data from many different clients you
don't want to make available for lots of
different every client this might be
things like average financial
transaction amounts or operational
metrics and finally you have complex or
asynchronous tasks
and these are tasks that might need
specialist software specialist hardware
to run or it might be too time-consuming
or or battery intensive things like
video transcoding or image thumbnailing
come to mind with that I'll hand back to
Alex to talk about the basis of our
examples Absolut 3000 you may have seen
the app shipped 3000 game out by the
firebase sandbox right over there this
is going to be the example they're going
to use for all of our data pipelines
that we have it's going to be the
overarching one that we've implemented
all of these into the game for those of
you who may not have played the game yet
I'm going to give a very brief
introduction of it very first thing that
you do is you sign into the companion
mobile web app this allows us to
associate the score that you get in the
game with your actual user profile we
give you the arcade sign-in code that
we've generated from your login to then
be able to log into the arcade controls
that we have at the game next you work
with the two other players that are
playing the game with you
cooperatively to try to steer the ship
and avoid asteroids two of the three of
you have to agree on which direction you
want to go before the ship actually
moves you lose fuel slowly over time as
you climb ah but you can regain this
fuel by answering various trivia
questions about firebase Android and
Google in general however the question
may not actually appear to you it may
appear to the plight one of the players
next to you and you may not have all the
answers so you have to shout out the
question and what answers you have so
that the three of you can communicate
and pick out which one is the best one
for a first example data pipeline in
this game that we're going to talk about
today is trusted code now in our example
we wanted to write a score supervisor
that takes the historical game opt is
completed that's uploaded to the real
time database and to verify that it is
indeed possible
now with this example we could fit this
is a very appropriate for a symbol a
single cloud function and so we take a
trigger from a real-time database we
write a cloud function and then when
that's finished we write back to the
real time database here you can see in
action so the first game here this
uploaded is verified as possible as you
can see it's got a score of 1250 with
the second game we've got a score of a
million and I don't know if you've
played it but that's not possible to do
this we write fairly short cloud
function we start with its basic
database trigger here we're listening
for the type sub note of the real-time
date of the game sub note in the
real-time database we verify that it is
indeed a historical blows and then we
fetch the entire game state in this case
I'm using the admin ref which is an
initialized database reference to the
location that triggered the database the
database trigger and I'm returning a
function so the cloud function here I'm
returning a promise so that the cloud
function here knows to wait for its
completion this is where I then do the
actual secret sauce of checking whether
the game's valid and if it's not I
simply mark the sidechain of promise and
mark the game as impossible if it is I
take the parent reference from before
and I do a deep update again chaining
promises of the type to verify it and
the score from as the reported store
one of the other things that we wanted
to add was an all-time leaderboard of
the highest scoring games across AIA so
this could be as simple as checking to
see if the score actually made it on
there or it could be very complicated
the very first iteration that we did we
just added the games into this ten play
our ten game list but we shortly found
out through our testing that one
particular team occupied like the first
five slots which is not at all what we
wanted to have happen so then we started
enforcing that a team could only have
one spot on the leaderboard at a time so
let's dive into a little bit of the code
here you can see that this starts out
very similarly to the example Chris just
gave we're also triggering on a game
right when it gets added however instead
of triggering on at a lower path down we
want the entire game object next we
check to see if the game score is
verified by Chris's function earlier if
not we just short-circuit right there
and in the function following that we
actually have to go grab the all-time
leader board so that we know what games
were played before and we can compare
those we use the admin SDK here to get a
privileged access to the database that
we instantiate this earlier on in the
code as you can see here using the
firebase admin SDK to have this
privileged access you have to use
certain API keys and service accounts
luckily since you're deploying these
functions in your project already it
knows your default credentials and can
supply those for you so you don't have
to paste in the giant JSON object going
back to our function the next step is we
actually insert the game in the
appropriate location or replace the
team's already existing game on the
leader board with this new updated high
score then we set this we set the
all-time leaderboard back at the same
task that we got
from finally we notify the team that got
bumped off of the leaderboard with a
push notification looking more
specifically at that we have to use the
admin messaging API to actually send
this message to them using the FCM
tokens that we gathered when they signed
into our mobile web app the important
part to note here is that this requires
the API keys and the service accounts
that we talked about before because you
wouldn't want to give any end-users this
privilege to just spam the rest of your
end-users with push notifications at any
point so extending this example a little
bit further we also wanted to have a
like more recent leaderboard so only the
games in the past hour would be shown on
this one you can still do this example
using just the firebase real-time
database and functions but it begins to
get more complicated you have to end up
structuring your data in tricky ways and
you have to make sure that you write
your queries correctly instead what
we're going to do is run a lien on cloud
dataflow help us deal with aggregating
all of these various games that are
coming in at random times so the first
step here is that we're going to still
trigger on the real-time database when
the game gets added but now in our cloud
function we're going to be sending a
pubsub notification ah chris is going to
talk more in depth about cloud pub/sub
later for right now all that we care
about is that this is our communication
layer with dataflow so very similar to
what we wrote before we're still looking
at the game's ref from the real-time
database but now we're dealing with
pub/sub looking more specifically at the
new code that we have here we have to
use the Google cloud pub/sub node.js
module and instantiate that this also
needs certain credentials that you have
from your project but again since cloud
functions is being run under your
project it already knows your default
credentials so we can just instantiate
that right away
next we have to have the pub subtopic
and we are going to push the entire game
object to that from there
cloud dataflow is going to be listening
to this pub subtopic and every time we
get a message in there it is going to
fully process this data flow is the
cloud product that helps you with doing
batch and stream processing over large
amounts of data or lots of different
inputs it allows you to do fully managed
resourcing that scales on demand so that
you don't have to worry about dealing
with any of other machines or boxes and
turning those up when needed the nice
thing about this is that we're going to
be getting a fairly random stream of
games being played because we don't know
how long each one's going to be taking
but coming out of data flow we're going
to be sending a very periodic update
every five minutes with the new updated
leaderboard so let's look at the cloud
dataflow code this is written in Java
and basically each one of these applies
EPS is just going to be a transformation
from one type of data into a slightly
different type type of data where we've
changed things around a little bit at
the very first step we're just listening
to the pub sub topic that we just wrote
to every message is going to come in
through this and then we're going to be
using Jaxon to change the JSON message
that's part of this pub sub notification
into a plain old Java object from there
we're going to be using a class that I
wrote myself the set key and time stamp
flap and that is just basically going to
be saying hey we only really care about
the team that each user's name that's on
that team that's how we're going to
uniquely identify that game and then
we're going to set the time stamp to be
when the game ended and got added to the
real time database at that point we're
going to be using a sliding window of
one hour so we only care about the data
that came in in the past hour and we're
throw away everything else and then it's
going to send out all of that data every
five minutes and it's going to process
through the rest of our pipeline then
we're going to deal with actually
finding what the maximum score for that
team was in that past hour if they
played five games we really only care
about the one game where they got their
best score then we select the top ten of
those users are those games out of that
big long list that we have that without
defaults there at the end that is just
saying that we're fine if we didn't have
any games played in that hour just give
us an empty list at that point and
finally we take that list of data that
we just got and we use Jackson again to
parse it back into JSON and send it back
in a pub sub message at that point odd
sorry at that point we're going to be
just sending that back off to a
different topic so looking back at all
of this each individual line of code we
have here is an individual class has an
easy to test function and all of this is
easy to compose the nice part about this
is we added a third leaderboard and we
were able to reuse most of that code so
we just wrote to a cloud pub/sub topic
we're now going to listen to that same
topic using cloud functions I'm going to
write back to the real time database so
that that leaderboard gets updated and
you can see that on your mobile web app
so this final function here is going to
look like this ah a little bit different
since we're now listening to a tub
subtopic odd this is the exact same
topic that we published in dataflow and
this will be triggered on every message
that we get and then we're going to just
be writing back to this leaderboard
using the same admin database reference
that we had from before so as an
overview this is all the steps that we
took
notice how it smeared right around
dataflow and we end up just popping
right back up the stack remember that
coming in from the real-time database is
going to be like a fairly random stream
of events of games getting added and
then coming back out we're going to be
having a very periodic 5-minute update
of the leaderboard so our final example
of a complex test in this case is
generating a flight image from the game
logs we create an image like this one do
we send out push notification to promote
re-engagement with the app and sharing
now just like Alex's last example this
is perhaps too big to fit in a single
cloud function so we use a dedicated
messaging system a dis example or use
cloud pub/sub as that messaging system
just like Alex did earlier and in Mike
in my code I wrote some go containers
running on container engines here with
pub/sub the topic is the main input for
messages and each topic can have a group
of subscriptions any number of
subscriptions a subscription can be seen
off as a queue of messages and it's
distributed between the workers
listening on the same topic in our case
we're going to use a single shared
subscription between all our workers so
that pub/sub does the work of
distributing that work for us
with this model you can see that it you
can chain many of these tasks together
so you could have a topic for a set of
workers but then pushed on to another
topic in an arbitrary amount at
so just like before we have a trivia
from the real-time database we write a
cloud function to bridge the gap onto
our messaging system and so we write the
code again like Alex did earlier we
start with a pub subtopic defined from
the Google Cloud SDK we have a database
reference this time linked to the score
verified sub node of the game so that
only games verified from the previous
the first example to trigger this
we do a final check to make sure that
we're adding a new node and not removing
it and then we push this on to pub sub
one alternative to the way we did it
here is if Alex and I shared a single
topic and then we defined two separate
subscriptions each of those acting as a
separate queue of messages distributed
between the two in the one case data
flow and in the other compute the
container engine so once the container
engine code the go code renders the
image it uploads it to cloud storage now
in this case we chose a bucket that we
had associated previously with firebase
to make it super easy to write a cloud
function to then send the push
notification we do this with this
function here we start this time with a
storage unchanged event this has changed
anytime any file in the storage bucket
changes we check to make sure this is a
new file the file is exists and then we
try to extract the the game ID from the
object name
returning if it doesn't exist now at
this point we said the FCM tokens that
our our PWA has been storing in the
database directly this is again a a
synchronous operation so we use a
promise here and then finally we send
them notifications just like Alex did
earlier so as an overview we used the
real-time database to trigger a cloud
function that then push the message on
to App this is the power messaging
system we use pub/sub to then distribute
work between our contacted containers
which then uploads our cloud storage and
finally we trigger a cloud function on
that storage bucket to send the push
notification ok as an overview we
started with the score supervisor and we
saw from there we can't necessarily
trust what our clients are sending to us
to be valid data so we have to check
that and we have to be able to verify
that on our own trusted machine next in
the all-time leader board we really need
to have an environment where we can
change the code quickly if we were using
an android or iOS app we would have to
push out an update and then wait for all
of the end users to update their app
before a change could go through using
cloud functions we were able to push
that update right away and see that
reflected next with the hourly
leaderboard
we had to deal with data that was coming
in at completely different times and we
had to aggregate across that and many
other sources of data so we leaned on
cloud dataflow to help simplify some of
that complexity finally with the flight
path image we began doing processing
that was much more complicated we need
more control over the CPU and the memory
and we needed our own boxes to deal with
this
so to help distribute this work
across these machines we use cloud
pub/sub to more easily and manage the
parallelism and the serialization that
we need across all of these different
tasks so thank you everyone for coming
and listening Chris and I will be right
over outside to answer questions right
after this I hope everyone enjoyed IO
2017</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>