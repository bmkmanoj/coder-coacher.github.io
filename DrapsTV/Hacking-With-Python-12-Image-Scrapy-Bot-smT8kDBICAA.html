<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hacking With Python #12 - Image Scrapy Bot | Coder Coacher - Coaching Coders</title><meta content="Hacking With Python #12 - Image Scrapy Bot - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/DrapsTV/">DrapsTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hacking With Python #12 - Image Scrapy Bot</b></h2><h5 class="post__date">2014-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/smT8kDBICAA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to hacking with Python 12 image
a scrappy bot in this video will be
looking at creating a web image scraping
bot with the Python 2 library scrappy
every video of all slideshows and
corrode available in the description
I'll be assuming you already have an
understanding of HTML also I assume you
have a decent understanding of Python
and object-oriented programming in
Python having knowledge in XPath will be
helpful so what is this bug going to do
our ultimate goal is to download all of
the images on the front page of image
comm this means we need our bot to visit
all of the front page gallery posts we
also want our bot to download all of the
image posts and their titles so what is
scrappy if you watch the last tutorial I
went over it briefly scrappy is a
low-level web scraping framework made in
pure Python it's lightweight and comes
packed with an abundance of features the
scrappy framework is really unique in
the fact that it structures its projects
in a package format forcing an
object-oriented approach ok so let's
have a quick look at the layout of a
scrappy project the hierarchy is fairly
simple to understand and manage first we
have the project name this project name
is the root folder and the sub folder of
the for the bot itself inside the root
folder we have the scrappy config this
config file rarely needs to be altered
because of the upcoming settings PI file
inside the project name subfolder each
file is important the inner PI file
doesn't have to be touched as it's there
to set our package up for Python it
interact with we then have the items PI
file this is used to hold our scrappy
item objects which we'll have a look at
a bit later we then have our pipelines
py file this file contains the pipeline
code we wish to use for our returned
items from bot then we have the settings
po file this Python file is where we can
set up any settings and config we need
for our scrappy spiders and pipelines
finally we have the spiders subfolder
inside this folder by default we'll just
have an inert file however this is where
we create our code files to write our
logic for our bot and the rules we in
place on it while it crawls the web ok
let's look at some keywords
items what are they item objects are
flexible storage containers that can
hold our scrapes data
next is spiders spiders are the logic
for airport you can think of it as a
spider decides how to spin and traverse
its web then finally the pipeline the
pipeline at least for me was one of the
toughest parts to grasp when learning
how to use scrappy the pipeline is the
stage that all of our scrape data is
passed back to before being output to
file or to console here we could filter
out duplicates or make our own custom
saving format or even override inbuilt
pipelines to change the functionality to
what we need we'll have a look at this
later okay now just quickly what is
XPath XPath is an expression base XML
format navigation system it allows first
define and grab information from
elements in an XML document HTML files
are actually in the format of an Excel
XML document so scrappy uses XPath to
easier deal with finding information
inside scraped pages on to our program
we are going to need to set up scrappy
first which is quite mature at the
moment as the version stored on the
advantage package list is a very very
very old version of scrappy so we need
to update our package lists and we need
to set the key that scrappy is signed
with then we'll jump over to writing our
bot okay so let's jump over to Ubuntu
and let's get scrappy installed so first
of all we need to set out our key that's
crap you signed with so sudo apt key adv
dash dash dash dash dash key server hkp
and severally one is the key server dot
ubuntu.com and the port is 80 and we
want to dash dash receive six
- 7 - 2 or e7 okay we run that it'll ask
us for a password we put in our password
up there we go and it'll go and we'll
request the key from the server and it
will grab it and now we need to update
our lists the app to get up.get searches
for so we're going to echo now we're
going to echo the string deb-deb
HTTP or - four - archive dot strappy dot
old /u bun - and we want the scrappy
main and then we're going to pipe this
so the vertical bar which is a shift
backslash we're going to pipe this into
pseudo t /e TC v / apt sources dot list
dot d forward slash scrappy dot list let
me run this and it'll echo it out into
our it should have a curator I cut it
out into our that's right assume it's
set up okay and then finally we can
install scrappy so sudo apt-get we're
going to update up to get and then and
then we're going to run the command sudo
apt-get install
scrappy - 0.24 which is the latest
version or the version at the time of
this video run that it will run through
and it will get all of the packages and
build all of the resources that we need
and this may take a little while if it's
your first install of scrappy okay so
that's done installing and it's ready to
go okay so find a folder that you want
to start your scrappy project in and
we're going to type scrappy and if we
hit enter we'll get a list of commands
we can use with the scrappy command and
the command we want is the start project
so we do scrappy start project and then
we give it a name so let's call it amigo
and we run that and it'll create our
project and it gives us a little
instructions on how you can get started
so okay we're going to get going to our
project file now so amigo and we can
type LS - have a look what's in here so
as you can see we've got the subfolder
Amiga and the scrappy config now we
don't need to do anything with a scrappy
config as the defaults are fine for our
use and we're going to jump into the bot
folder so inside the Amiga folder if we
Alice again we've got our init dope I
items pipelines settings and then our
spiders subfolder so let's set up our
item stop by first we want to create an
item that we can store everything in
okay so lets them items dot pi okay now
what we want is we want to set up all
the things that our amiga item holds so
to get rid of this pass and we can get
rid of the comments there as well okay
now we want to store the title of the
image that we scrape so we'll create a
variable called title or an attribute
called title and this is gonna be a
scrappy dot field okay
so we created a scrap it up field okay
so we need to add two more attributes
image underscore URLs equals scrappy
field now this this image URLs variable
is a special variable and scrappy will
recognize it as a a as an image URL
attribute and we'll use it to download
our images and because of this we also
need a variable to hold our images so
images equals scrappy field okay so
these are our three attributes for our
imager item okay
so let's save this and quit and next
we're going to edit our settings pi so
then settings pi now we need to set up a
few more settings for our our bot so we
can keep our spider modules and our new
spider module but we need to add the
item pipeline so item underscore pipe
lines equals and this is going to equal
a dictionary so each value in the
dictionary or each key in the dictionary
corresponds to a code file that holds a
class so we want to import the default
eye images pipeline from scrappy so we
want scrappy dot contrib dot pipeline
dot images dot and then we want the
images pipe line okay and then colon one
because we want to enable this pipeline
okay now we're going to need to set a
path to store all the images that we
download so images underscore store
pickles and this is just a stray
of a directory you want to store your
files so let's go with home and I'll do
my username so you bun - and I'm going
to store it into a folder called amiga
front alright and close quotes off okay
so this is our settings setup so we've
told scrappy that we're going to be
using an images pipeline and we've set
where we want it to store the images now
finally we can get on to writing our
spider so let's change directory to
spiders now inside the spiders directory
at the moment is just the inner PI file
now this is where we can create our own
file so let's move in and we'll call it
a amiga underscore spider dot pi okay
now inside our spider we're going to
need to import scrappy so it's import
scrappy now to make it make things a bit
easier let's use the from keyword and
from scrappy contrib dot spiders so
we're going to use an inbuilt spiders
classes so import we're going to import
the rule object and we're going to
import the crawl spider now the crawl
spider is basically a rule using spider
which will use rules to pick out
whereabouts it'll grab links from okay
so from scrappy dot contrib dot link
extractors we're going to import the
link extract the class okay now we've
got our all of our scrappy stuff
imported now the import our amiga item
that we created earlier so from amigo
which is our project name dot items so
the items file we're going to import the
amiga item that we created earlier okay
so that's it for our imports now we can
get onto writing a spider now a spider
is an a subclass of the crawl spider so
let's create a class and we'll call it
Emig amiga spider and inside the
brackets we're going to make it a
subclass of crawl spider okay now now
that we've created our crawl spider we
need to override some basic attributes
so first we must have a name so the name
attribute has to be unique to this bot
to this spider so we'll call it a media
because we're only going to have one
spider okay now we need to create a
attribute called allowed domains so
allowed underscore domains and this
equals a list of domains so we're going
to only have one domain which is the
imager com domain so we we don't want
our bot to leave the imager side okay
now we're going to have a start
underscore URLs so this is going to be
the pages that our bot starts on now
we're only going to start on one page
because we only want to extract the
front page so HTTP forward slash forward
slash wwm eager comm and that will get
the front page of amigo and make sure
that these attributes are in lists and
finally we're going to set up the rules
attribute now our rule is just going to
take a list of rules so open up our list
and we're going to create a rule object
so our rule is going to use a link
extractor and our link extractor is
going to allow any links allow any links
and we're going to use a list and we're
going to allow anything that it has a
forward slash gallery gallery forward
slash and then we're going to do a
little regular expression on the end
here which is a dot star so anything
after the gallery because amid Amiga
uses a hashing algorithm to decide on
what the name is of an image okay so
let's close that off so we're only going
to let allow links with gallery
something to be extracted and with those
links that we extract we're going to
give them to pass underscore amigo now
pass Amiga is going to be a method that
we create in just a second so we've
created our rules now let's define our
past Amiga method so def pass underscore
here we go and this will take a self and
a response
now response will be pass to it by a
scrappy so the response is whatever the
webpage gives back to us okay so now we
want to create our image item so our
Amiga item so call it image and that's
going to equal n can we go item and now
we're going to start setting some of the
values inside that item so let's set
image title and our title is going to
equal the response dot XPath we're going
to use an XPath expression to get to
extract some information from the
returned response so XPath and we'll
just bring it down a line so it's easier
to read and
we're going to use the XPath expression
for - forward slash for relative we're
going to get relative h2 because the
titles on the Amiga site are h2 which we
can have a look at if we go to the
imager site so if we jump into Firefox
McGary Amiga comm and we can see we got
this front page and it's got a bunch of
image posts on it and as you can see
down the bottom left we have the image
com4 - gallery forward slash and then
it's a random string of characters now
if we actually go to a post we get this
sort of layout here where it's got the
image the title and some comments so if
we right-click and we go inspect element
on the title you'll see that we get this
h2 ID is image title and if we inspect
the element of the actual image itself
we have an image tag and it has a source
address for the image okay so we can
close the rigor off now now we want to
get that h2 tag and from the h2 tag we
want to get the action the h2 tag that
has an attribute of ID equals and inside
here we're going to have an image -
title ok now I close off our square
brackets there so we're going to get a
h2 element that has an attribute with
the ID image title forward slash and
we're going to get the text from that
element so text and then we can close
off our expression and then once we've
closed off our XPath expression we want
to extract the values of what the data
from that expression okay now we need to
get the image and when we download
images with the scrappy we need to get
the URL and then store the URL and then
the scrappy pipeline the images pipeline
that we set up will download it for us
so the images as we saw just before as
stored with a relic
to address so let's grab the relative
address so rel equals response dot XPath
and then we're going to get the slash
slash image book that is to be in quotes
slash slash image forward slash and
we're going to get the at source so
we're going to get the source attribute
from the image tag okay so once we've
got that we can dot extract that
information so that will extract the
relative link and now we need to set up
our image URLs so our image image and
then the third image and the score URLs
is going to equal a list of HTTP colon
close quotes plus the rel the relative
link 0 okay so we can close off that
list now and then we can return image so
to return the image item that we created
and this is our spider logic completed
so let's save this and quit and let's
give it a run so to run our scrappy bot
we need to go back up to the root
directory so look CD back up we need to
do it one more time so we're in the root
directory and the root directory is one
with the scrappy config okay now it's a
run out spider to make it crawl we need
to type scrappy crawl and then the name
of our spider which is the name value
that we used inside of our spider logic
so that name attribute so we called it
amiga so we will hit run it'll start
running our scrappy bot and as you can
see it's going off and it's starting to
crawl pages and scrubbing images
crawling pages grabbing images
it will keep going I'll close this off
just for the moment now if we come over
to our actual home folder we have a
image up front which is where we
restored our images we can open it up
and store it in a folder called full and
as we can see we get the images on the
amiga front page so we can scroll
through and have a look at them in our
own time okay so that's pretty neat
let's jump back over to the slideshow
now currently our body is working at and
scraping images however at the moment of
image file names are being stored with
Scrappy's inbuilt images pipeline which
stores the images as a sha-1 hash hex
result of the images download URL so
because we want to store the images with
their filenames as the post title we
need to create a subclass of the images
pipeline class and override the methods
we need to set the filenames differently
okay so let's get on to writing a custom
images pipeline to save the images with
custom file names
okay so let's jump back over to Ubuntu
and we're going to need to edit our
pipelines so let's change directory into
our Amiga folder now LS we can see we
have our pipelines pi so we want to edit
that so then pipe lines dot PI now
inside about pipelines dot pipe will get
this sort of this structure already made
for us which is the Amiga pipeline
object and def process item etc etc now
because we're going to override a class
that's already inside the scrappy module
we need to import some things first so
import then import scrappy of course now
from scrappy dot contrib dot pipeline
dot images now this might be starting to
look a bit familiar
we're going to import the images pipe
line now you might remember this from
our settings part where we set our
dictionary to import this for us
well because we're going to override the
class itself we need to import it here
to override so inside here we're going
to create a subclass of images pipeline
so this images pipeline we're going to
create a sub crop subclass and start
overriding its methods okay
so we have the imager type line now
which is going to be our pipeline where
we do everything let's define a couple
of methods so we're going to define our
own method which we're going to call set
underscore file name now the file name
is going to take self as an or method
should and we're also going to take a
response okay now I'm just gonna put a
little comment here so in your own time
you can add a red regular expression to
check that the title is a valid a valid
title for a file name okay now let's
return so we're going to return the
filename so we're going to return full
that's the folder name forward slash and
we're going to use the format so you
want the zero zero with element dot jpg
for jpg and we're going to dot format
this I'm inside we're going to inside
the version replace the the zeroeth
element with the response dot meta and
the meta is a title that we're going to
pass in and it's going to be the zeroth
element in that meta okay
so this is our set by our name done now
we need to override some other methods
so we need to override the get media
method so get underscored media
underscore requests and this is going to
be take our self because we're
overriding and it's part of the class an
item and the info okay now what we need
to do for um each media request that we
get you need to have a folder of the
image underscore URL in item image
underscore URLs and in the for loop
we're going to yield and we're going to
yield the scrappy dot request object of
the image underscore URL and we're also
going to pass in the meta so we're going
to create metal equals a dictionary of
meta title and the title is going to
have the value item title okay so this
is getting a bit long we'll just break
it up a bit so we're going to have the
title and then we're going to close off
our dictionary and then close off our
brackets for the request okay now let's
define our get images function so def
get underscore images now this is
overriding the get images function or
method inside the images pipeline class
so this we need to pass in self we need
to pass in a response a request and the
info
okay so because we're overriding our
superclass what we're going to do is
we're going to call the superclass
function so that it will manage all of
the difficulties of getting the images
from the website so for key image and a
buffer so buff in super OOP
super amigo pipeline so the superclass
of Amiga pipeline so we're going to get
underscore images and we're going to
pass in the response it's getting a bit
long again we got to pass in the
response and we're going to pass in the
request and we're going to pass in the
info that we get okay so after that's
done it's going to go into our for loop
and we're going to use the key which is
the name of the file and we're going to
set that to self dot set underscore file
name and the file name is going to be
the response and then after a for loop
is done we're going to yield the key the
image and the buffer okay so this
completes our imager pipeline so
basically our get media requests will go
through all of our items and get the
images URL and make a request to get
that image then I'll get images method
that we overwrite we'll go through and
use the superclass get images method to
get all of the images and then we're
just going to tack on to the end and
change the key before it saves to two
file okay so we can now write quit an
exit out of this and because we changed
a pipeline we need to edit our settings
serve in settings doc pay now we need to
change the image of the item pipelines
and instead of it being scrappy country
pipeline we're going to remove it out
and we're going to get it from our
pipeline so amigo media dot pipelines so
the pipeline's
file and we're going to get the imager
pipeline now we need to set it to one
okay I'll be right quit and we exit out
of this and we CD back up to our root
directory we can run our robot so
scrappy crawl in makeup and we can also
use the dash capital L to set the
logging mode so as you saw before we
have a bunch of debug outputs and we may
not want debug so we can do info and
this will print out anything that is an
info and error or critical error so if
you run this you got an error it'll be
on my fault loop line okay so I'll just
edit this quickly I just didn't have my
colon there we go let's right-click C D
and let's pry running our body again
so I'll info and it'll only output the
infra needed okay
so that'll run and wait wait for it a
bit it's crawling pages but I'll cancel
it for the moment
keep this shorter and then go to our
amiga front page again and inside our
full and you'll see that we now have a
bunch of images that have the titles as
their names so you've still got the the
ones that we left in here from before
but we also have the ones that have
their names printed out as well so
cool so now in your own time you could
sit down and scroll through your images
and you can read the top for the title
of the image cool so this concludes our
image escaping but let me know in the
comments if you ever browse amigo in
your spare time if you have any
questions leave it in the comments and
I'll do my best to answer it as best as
possible
the next topic we'll be looking at is
searching images for metadata thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>