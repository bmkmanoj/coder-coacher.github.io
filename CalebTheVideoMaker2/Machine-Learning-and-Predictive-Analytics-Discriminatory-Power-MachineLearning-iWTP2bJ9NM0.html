<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning and Predictive Analytics - Discriminatory Power - #MachineLearning | Coder Coacher - Coaching Coders</title><meta content="Machine Learning and Predictive Analytics - Discriminatory Power - #MachineLearning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/CalebTheVideoMaker2/">CalebTheVideoMaker2</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning and Predictive Analytics - Discriminatory Power - #MachineLearning</b></h2><h5 class="post__date">2018-01-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iWTP2bJ9NM0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back everyone to your machine
learning tutorial series this video we
are going to be talking about decision
trees and specifically discriminatory
power which sounds completely
politically incorrect but we're just
gonna have to deal with that it's gonna
be fun so stay tuned this is gonna be a
good video for this video I want you to
pay special attention to this area right
here okay so we're looking at the root
node and the next step well how do we
decide to split our data by age and then
how do we decide to split our data by
history that is what this video is going
to be about my dog has fleas I think so
he's just like freaking out like rubbing
up against this cage we've done stuff so
it's not like we're just neglecting him
we've been trying to get that fixed
let's figure this out how do we choose
which attribute to put here to split our
data why did we put age first and not
wait because we're focusing up here I'm
going to erase this down here now one
more thing is I want you to pretend this
is an even simpler world than I've been
making it already and let's say anyone
under 50 in the past has always been
diabetes free anyone over 50 has always
had diabetes well then we don't even
need to use these next questions we can
just replace those with no diabetes and
diabetes so we've just jumped to the
leaf nodes right away in a perfect world
it'd be this simple you know you go into
the doctors hey doctor I think I might
have diabetes well how old are you 49
nope you don't have diabetes but give it
a year but it's not that simple and the
reality is it's going to be a lot more
complex than this but I'm gonna make it
really simple for you guys the thing
that we want to put here is going to
split our data based on target feature
values as close as possible into this
perfect world so for example if we have
some people we want to ask a question
that is best going to separate the
people with diabetes from the people who
do not have diabetes that's why it's
called discriminatory
our a descriptive feature has a level of
discriminatory power that can split the
data to get one group who has diabetes
and another group who does not have
diabetes so in this specific example we
could say we split this group and these
people do not have diabetes and these
people do have diabetes it's perfect a
more realistic example though would
split the data to where two of these
people do not have diabetes and two of
these people do have diabetes so in that
situation the majority have no diabetes
on this side and the majority have
diabetes on this side so you can see how
it's a spectrum so I kind of drew this
example for something else earlier in
this series imagine kind of like a gauge
or whatever that starts here going this
way and this is zero discriminatory
power and this is like maximum right
here and you can think of it like moving
from this end to this end all shoot so
in this perfect world this attribute
would have maximum discriminatory power
realistically though we're going to find
descriptive features that are only
pretty good at splitting our data so you
know they're discriminatory power might
be like here for example so when we're
making our tree the descriptive feature
that's going to go here first is the one
with the most discriminatory power I'm
going to erase this now and I'm going to
show you this visually here we have two
bar plots and I have two descriptive
features this one over here we have sex
and this one is age now let's look at
sex first and the percentage here is for
that specific group so if we look at
just males we could say 50% of them have
diabetes or no let's you know let's make
it a little higher so we can say 60% of
them have diabetes all right and then
let's say you know 53% of the females
have diabetes this percentage works
because it's just inside the group of
the male
so what I'm saying here is that of all
the males 60% of them have diabetes of
all the females 53% of them have
diabetes
this might show that males have a higher
chance of developing diabetes that's
because the majority of males end up
with diabetes it's just just a
correlation here we can't say for sure
you know there's a lot of different ways
we can represent this data and you know
we could probably warp it to make it
show what we wanted because because it's
really not that strong okay now let's
look over here and in our perfect world
anyone under 50 has no chance of
diabetes and anyone over 50 has a 100%
chance of diabetes and this situation
the discriminatory power of age is going
to be much higher than the
discriminatory power of sex
so the descriptive feature to split our
data on is always going to be the
descriptive feature with the highest
discriminatory power okay okay okay okay
okay
I'm gonna erase this one over here so I
can illustrate something once we split
on that data so we start with age and we
split into two groups of people what is
the next feature we're going to split on
it doesn't have to be the same for both
of the groups because we're going to
pick the descriptive feature that has
the highest discriminatory power for
that specific group so on this group you
know 100% chance if you're male you're
gonna have diabetes and 0% chance if
you're female in that situation you
would split your data on sex in this
situation you know we might have 50/50
for male and female but family history
of disease if if there is a family
history of disease and then 100 percent
chance they're gonna have diabetes so we
can have family history of disease over
here and you can see how it's very
dynamic now you can see this gets really
complicated
so we need an algorithm that's going to
build this tree for us and that's what
we talking about in the upcoming videos
so thank you guys for watching please be
sure to subscribe and I will see you in
the next video peace oh one more thing I
didn't really talk about it in this
video because I don't want to go into
all
the details of this but that
discriminatory power I was talking about
how do we go about figuring out a
features discriminatory power how much
power does it have you can actually
quantify this and if you want to know
how to do that look up Shannon's entropy
model and this is a formula that's going
to give a number to a descriptive
features ability to discriminate and
that's something we might get into in
upcoming videos but right now I just
want you to understand it conceptually
and then we'll figure out all the
details of actually doing it later when
that time comes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>