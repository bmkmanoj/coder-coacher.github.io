<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Testing in Production - Gel Goldsby | Coder Coacher - Coaching Coders</title><meta content="Testing in Production - Gel Goldsby - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Testing in Production - Gel Goldsby</b></h2><h5 class="post__date">2018-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aAsVbUyypfs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's Monday morning you've survived
your commute on the tube stress it's
sweaty strange bodies packed in next to
you and you've arrived at the office on
time you pour yourself a nice strong cup
of coffee you take a sip you start to
feel human again
you feel your mind starting to wake up
and you sit down at your desk and you
open your emails hmm got a ticket from a
user she's contacted you overnight and
she's found a bug in your system now
imagine if you were able to craft a fix
and roll that fix out for your user
before you'd finished your cup of coffee
you could send the user a nice reply
saying that it was fixed safe in the
knowledge that your system was working
as expected on production now imagine
that you could do that every day create
and roll out changes within an hour or
less now imagine that your whole team
could do that each of them rolling out
their own small changes as they went
along within an hour but first let's go
back to the beginning as all great
stories start a long long time ago in an
office far far away it's actually just
on the other side of town it became
apparent that we weren't happy with our
release for a process so we discussed
what it was that we didn't like about
our current system so I think this quote
sums it up perfectly how long would it
take your organization to deploy a
change that involves just one single
line of code and it became apparent to
us too long it's a common way to
organize deploys is to use a continuous
integration or CI system for example
Jenkins or Travis these queue up your
deploys and they control of all of the
steps that are needed to get your code
into production so this might include
deploying to a test system running tests
there and then actually deploying your
code out the problem with these systems
is that they work on a fire-and-forget
process so you commit your code and then
at some point in the future quite
possibly a little while later you
actually get your code deployed out to
production and you receive a
notification as to whether your deploy
worked or not that means that you might
have to end up doing a massive context
which you could be working on your new
projects and suddenly you're getting
notification that your last commit
didn't work and that's where we were and
we didn't like it so we came up with
some goals we wanted to be able to push
our code out to production faster in an
incremental fashion so in other words we
wanted to be able to release our release
sorry
increase our release cadence and we were
getting fed up of continuously switching
between things so we wanted to reduce
contact switching and we realized that
for us it was more important to be able
to deliver faster to be able to move
invited deliver value faster then it was
and that we were happy to accept an
increased risk as a result of moving
faster talking amongst ourselves we
realized that contact switching was
particularly the top bugbear of most
developers so could we change our
deployment process in such a way that we
could actually reduce this context
switching so today I'd like to share
with you the journey that we took some
of the systems that we put in place and
of course some of the problems that we
encountered because let's face it is
never plain sailing probably most
importantly I want to show you the
change in the way that we now think as a
result of making these changes to our
deploy system but first I should
probably introduce myself my name is
Kjell
I'm a senior developer working across
all of our dev teams to try and make
them all work better more efficiently be
more productive my job title is senior
developer across team which I always
think makes it sound like I'm on an
angry team or something and I work from
really so we are a company founded on
agile concepts from there is in our
blood we've always worked with agile
processors so we're all of our code 100%
of our production code is paired so that
means we've always got a second pair of
eyes on the code looking out for
mistakes and as you can see from these
beautiful photos we also do a bit of mob
programming this is particularly useful
when we're working on new
of functionality so it's really good to
get the whole team involved in the
discussions about the direction and the
model that we're going to use etc what
does unruly do so we're a video ad tech
company we run what's called an ad
exchange if you don't know what that is
it's a little bit like a dating agency
we bring together advertisers on one
side and publishers on the other side
and then we join them together and show
an advert on the end-users page as the
page loads we send out details to all of
ahead sorry
so as the page is loading we skip send
out details to all of our partners
asking if they would like to display the
ad on the page they all respond with a
bid saying if they would like to and how
much they're willing to pay we run a
real-time auction second-price much like
eBay and then we display the winning
advert on the page so all of that
process happens in less than 500
milliseconds preferably around 200
milliseconds so it's kind of more like a
speed dating agency then as the ad plays
it fires events back to us so that we
can monitor what interactions the users
made and how far they've watched so we
collect those events and then we use
them for reporting and billing we need
to be able to receive those events in
real time so we've got a pipeline which
streams those events and collects and
processes them and it handles about I
think 17 million events an hour at last
count so kind of medium medium sized
data we're not quite Netflix but we're
getting that so if we don't display an
ad quickly enough we won't make any
money if we don't display an ads on the
right kind of site at the right time to
the right kind of users we won't make
any money and if we don't receive
process all of those 70 million events
then we can't build people and we don't
make any money so all-in-all it's quite
important that we've got a relatively
low latency high throughput systems with
high uptime needed okay so let's talk
about our first goal to increase release
cadence so why would we as a business
want to release more regularly
especially if it comes with an increased
risk of issues as a result the way
software is released has fundamentally
changed in the past 25 years if you
think back not that long ago
burnt to a disk and then you shipped
maybe I don't know once a year twice a
year so you had to be sure that your
code was perfect it was bug free your
reputation depended on it we're not
there now but yet the idea that you
should release perfect software is is
it's still clings to the mindset of most
people in fact one of the hardest things
about making these changes was
persuading our developers that we as a
business really were happy to accept an
increased risk of bad deploys in order
to get our code out faster on to live
commercially releasing regularly has
some really great advantages you get
prom feedback on whether your customers
are happy with the changes that you've
just released in an iterative manner so
it allows you to see if they like the
direction you're going so rather than
waiting and deploying a massive change
that no one likes by which point you've
already done the code you've already
spent the time you can't roll it back
time wasted you can deploy out in an
iterative manner and get feedback as you
go along and with the right metrics you
can see where they're having problems on
your application on your site and you
can proactively release changes to help
them rather than waiting for them to
contact you so both of these leads are
really golden for any company that wants
to keep its user base sweet'n and stop
them moving on to competitors so how did
we go about being able to release more
regularly that what magic did we use to
make this happen
okay obviously the first thing you need
to do is to speed up your deploy process
pretty self-evident the faster you can
deploy the faster you can release but if
you think this three my mantra tends to
be hope for the best plan for the worst
so what happens if we make a mistake as
we're rolling out this code at speed so
we need the counterpoint
what's the counterpoint to fast deploys
it's fast under ploys so especially so
we need easy pain-free brain free
rollback process sort of a one-click
rollback process generally if you've
made a mistake and it's out on
production it's quite a stressful time
you don't want to be thinking about now
how do I rollback I have to do this I
have to do this now you just want one
button rollback done particularly if the
critical systems gone down so we can
deploy faster and we've got our
emergency
roll back process in place but how do we
know if we actually need to roll back
how do we know if anything has gone
wrong so to do that we need to be able
to verify that everything on production
is working as possible so those are our
three key ingredients so let's attack
the first one let's talk about faster
deploy process how did we go about doing
that so we had to think about what was
slowing down our deploys the most and we
realized we had a number of pain points
we had branches which added to confusion
when committing and deploying so we
decided scrap branches we were creating
large change sets between commits that
is the time between writing the first
line of code and releasing it was much
too long so we decided smaller change
sets with a single feature per release
we were deploying via a UAT system we
decided scrap that let's just deploy
straight to production and last but not
least our tests oh yes we were waiting
for all of our tests to run and pass
before deploying that's our cages so we
decided to run our tests on production
so we started to make some changes we
got rid of all of our branches so now we
check all of our code directly into
master everyone does no branches at all
and this makes me very happy
I'm sure you've all been there merge
hell I remember it very unfund lee half
a day or more trying to resolve
conflicts you've had number of different
people committing in while your branch
has been a long-running bla bla bla
two's recrimination it was horrible I
really don't miss that but committing to
master isn't all sunshine I should just
point out since everyone's committing to
the same place it forced us to get
really really strict about never
committing breaking code so every commit
has to be a self-contained unit of
working code and we found that we needed
to maintain much better code hygiene we
couldn't have unreleased codes sitting
in the repo it led to confusion
no one knows whether it had or hadn't
been released so we had to change the
way that we work when we commit code we
deploy that code no exceptions
simples you could say that by committing
codes you're saying that you're
committed
getting out onto production but now that
I've said that I realized that sounds
really twee so you might not want to so
by working this way with small change
sets where every commit is a deploy it
forced us to think in a lean way what is
the minimal amount of work that I can do
so that I can deploy the smallest slice
that I can work on not only where we
chopping up our epics into nice
vertically slice stories each with its
own business functionality but now we
needed to think about chopping up those
stories into smaller functional code
chunks where we could deploy each of
those individually for example we might
want to make a change and we might
deploy out the change to the API first
and then deploy out the UI as a second
separate deploy and it was the the
advantage of this of course is that you
get the advisor you can test the
functionality of the API before you've
actually rolled out the the front-facing
UI to your users with practice quite a
bit of practice we've gotten to the
point where commits might only touch one
or two files and might take maybe I
don't know five ten minutes to code but
initially we found it really really hard
to keep our change stats small so we
needed to be really disciplined about
how much we squeezed into a commit in
fact the moment you start thinking can I
squeeze X into your commits it's a sign
that you're trying to do too much as
soon as you've got a working piece of
code no matter how small how small the
change in functionality is you deploy it
smaller change sets do have advantages
for devs there's left code so there's
less likelihood of strange bugs creeping
in and if a bug does get in debugging it
is a lot easier you know which commit
cost your problem it's a small change
set so it's generally a lot easier to
work out what the bug is when we started
deploying like this we didn't
necessarily want to deploy out to all of
our users at the same time so we put all
of our new releases behind a future flag
this gave yeah this gave the visibility
of the new functionality to specific
users only
say for example we often deploy out to
our internal order adopts team-first who
are happy to test and report any bugs
for us if you've got an engaged user
base maybe like a beta testing user base
you could employ out and deploy out to
them first and we also started doing a/b
testing so we deployed to a small
percentage of our users first check that
it's all working and then deploy out to
the full user base afterwards but how is
that not chaos you might ask great
question I'm so glad you asked so we
split our codebase into separate
deployable modules that can be worked on
in different independently you might use
the word microservices if you appeal it
feeling particularly hip usually we've
got around two or three pairs working on
any one module at the same time so to
gate deploys we have what we call deploy
tokens for each project if you aren't
familiar with the concept you can think
of a deploy taken as a lock file and you
can only deploy if you're holding the
physical deploy taken so it stops the
situation where you've got two people
trying to deploy at the same time and
you get into a race condition and it
creates sequential rather than parallel
deploys for each project say we've got a
nice clear timeline of what code we've
got on production at any point so I'm a
big fan of physical deploy tokens as
opposed to virtual deploy tokens since
it forces you to actually talk to the
pair that's currently deploying you can
exchange information about what you're
both working on and you can check that
you have got any overlap and you can
catch up with people on the other side
of the office who you might not see that
regularly so our deploy tokens are
actually the CTOs kids toys presumably
they had actually finished playing with
them before he brought them into the
office and the bottle of wine is just a
bottle of wine that's not a deploy taken
because that would be cruel so it's a
usual sight to see someone carrying a
teddy around the office and I'm pretty
sure the rest of the company isn't
thoroughly mystified as to why
developers feel the need to take their
Teddy's with them when they walk around
the office we also have a slack releases
channel where you can see all of the
releases done across all of the teams
and all of the projects so that's quite
a good way for viewing the history
across everything so another change we
made was
deploy straight to production that's all
of our code goes straight to our live
servers no exceptions so we got rid of
any staging so as we got rid of any test
servers we got rid of any UAT
environments all of that was gone we've
just got our dev workstation where we're
doing our development and then our
production so this is quite
controversial for new starters because
they're used to the perceived safety
blanket of a test system but it works
for us since the business is happy for
us to balance the increased risk of
something going wrong against getting
faster and more regular deploys right
how are we doing our deploys so deploy
systems in general can be split into two
types
you've got asynchronous and synchronous
so conventional systems are asynchronous
those are the continuous integration
systems like Jenkins Traven's - Travis
teamcity you press the button and then
you fire and forget stuff happens and
then at some point in the future your
code is deployed to production but the
asynchronous nature means that you the
advantage of the asynchronous nature is
that you don't have to wait to see if
it's successful but it also means that
you don't know when it will deploy you
could have maybe lots of other things
banging up the pipeline so you're one
it's not going to be deployed for a
little bit or etc lots of different
reasons it might take a while the
problem with this I found is that you're
not going to want to contact switch to
fix issues you've already moved on
you're knee-deep in your new bit of code
you've got a nice nice head of flow on
you you don't want to then have to
switch back and work out what the heck
you were doing with your last commit and
what had gone wrong and etc etc I think
one of the reasons that most people
moved to CI systems was because they had
complicated slow tests with multiple
steps in their deploy process so they
used the the asynchronous nature of them
to get around the time lapse they didn't
have to wait for ages to do the deploy
but we've been speeding up our deploy
process so we decided to change all of
our systems to do synchronous deploys
what I mean by that is you press a
button and then you sit back and watch
as your code gets deployed out to
production there's no magic it happens
in front of your eyes you can see what's
happening
and you can keep an eye on what's
happening you can see the progress see
if there's any problems as you go along
and it's really important for us to have
that fast feedback loop so we can check
if it's successful still being context
of the code that we've just written and
if not we can instantly be able to work
out what had gone wrong and what the
bugs were when I say instantly obviously
that's that some bugs on as incident as
others in working out for the to deploy
out to production you've literally got
me two lines in advance of that I'm
about to tell you so so how so how did
we implement this we've got a sorry
we'll come to that
we've got a bash script to run our tests
and then it uploads our packages on to
production we update our packages on
production and then we restart all of
our relevant service so it's pretty easy
it's like I don't know six lines of bash
or something okay you're probably
thinking isn't that really flippin
annoying sitting there waiting for your
deploy process to go out so it's true
there's a bit of waiting but only takes
about five minutes for most of our
systems to deploy out to production so
not using a CI pipeline has motivated us
to make the improvements if it gets too
slow with an asynchronous pipeline you
might get into bad habits it's not that
important if your tests if your
deployment takes ages to do but for us
it is every every second counts because
otherwise people get too bored and don't
bother to wait and watch so one of the
advantages that I like so the five
minutes to deploy is that you get into a
bit of a break and I'm actually quite a
fan of breaks because I find that my
brain tends to work better when it's
kind of in that downtime relaxed state
and tends to come up with the best ideas
at that point and it's also a really
great time to grab a cup of coffee and
have a chat with your pair on where
you're going the direction you want to
take your ideas difficulties you'll
think you might encounter did anyone get
the picture it's a sink for synchronous
yeah
pun well received so feedback loop it's
defined as a system where the output of
the system becomes the input for the
next iteration of the system does anyone
recognize this probably most infamous of
feedback loops yeah so this is the TDD
or test-driven development loop so the
idea is that you create a failing test
that's the read you write the code to
make it pass that's the green and then
you refactor the code so why a feedback
loop is important to us why do we want
faster feedback loops well short
feedback loops allow you to see if
you're actually providing the expected
value to users you're more resilient to
change in direction due to market
vagaries you can mitigate risk by
releasing an MVP product and then
iterating on the functionality of it
rather than delivering the entirety in a
Big Bang process and the shorter your
feedback loop the quicker you can adopt
a change imagine what a customer would
think if they report an issue and they
see the fix implemented you know within
an hour they're likely to be alot more
loyal to your brand then to a competitor
who takes weeks to deploy fixes and
short feedback loops give us confidence
which tends to lead to less stress on
the job as well so after making all
these changes to improve our deploys
what remainders our biggest slowdown in
getting our code out to production I'm
sure no one be entirely surprised to
hear that it was the time our test took
to run we were waiting for our entire
test suite to pass before we deployed it
when we first started this could take up
to 30 40 minutes depending on which
project we were deploying obviously you
don't want to sit there for 30 minutes
watching your test pass so could we just
get rid of them do we need them what a
test for so I think there a sanity check
that I've not done something silly
I think existing tests are a really
great way to understand the existing
model and the purpose of the
corresponding code they're kind of like
a documentation they should show any
edge case
is any unhappy paths making expected
behavior in both cases really obvious to
you and then much like having a
parachutes they give you the confidence
to go ahead and make changes and deploy
mental well-being so let's do a super
quick recap on tests what different
types of tests do we have so the most
obvious one is automated testing
so that's your unit functional
acceptance and to end etc all of those
umbrella is an automated test then
you've got your manual tests so that's
where you've got a playbook and then
someone goes through all the steps
running manual tests visually checking
eyeballing that everything is looking
right then you've got graphs so you
might have for example a graph showing a
timeline of a certain metric in your
system maybe a number of user requests
over time and that's a really easy way
to see patterns and compare different
systems and you can also use graphs to
prove expectations for example you can
show if the number of expected users are
actually making use of your new feature
that you've just deployed so we try to
have graphs for each of our important
KPIs and that allows us to check that a
deploy has an inversely impacted many of
our important business features then
you've got alerting so that notifies you
to a change in your system as they
happen so you can respond straightaway
you've got load test so that's checking
the capacity of the system rather than
the functionality of the system and then
you've got a B testing so that's
deploying two different versions one may
be a you know five percent ten percent
and then the other one larger and then
you compare how they run and you can use
your monitoring that we talked about
earlier to do the comparison between the
two different between the a and the B
you can have ABCD testing as well if you
want if you go wild and then you've got
profiling so that's you can see how
certain actions are affecting your
systems and then my favorite one chaos
monkey so this is seeing how your team
or your systems
with random failures so remembering that
although computers and disks are getting
more and more reliable if something can
go wrong it will go wrong it's
especially true with the rise in
Microsoft's is where things like Network
latency and failure become ever more
harmful to your ecosystem so some
examples are the chaos monkey you might
take down a web server in an auto
scaling group or you might stop
connections to only database so in some
cases you would expect your system to
self-heal and in others you might expect
a human element and then see how they
cope with the failure so in the case of
the web server in an auto scaling group
presumably you would expect that a new
one would be spun up automatically in
the case of the database hopefully your
team is actually alerted gets their
alerts and is able to work out what's
wrong and fix it we do chaos monkey and
it's always a good idea to have a
retrospective afterwards to discuss what
what went well and what needs improving
and it can uncover some really
unexpected
snowball events as well where one thing
affects another thing affection etc if
you haven't tried it it's it's really
interesting but quite scary I'll be
honest but really really worth doing
right so we had a large number of tests
as part of our deploy system and they
were the main thing that was slowing our
process down the process that we're
trying to speed up so the real killers
were our functional and end-to-end
integration tests which I'm sure isn't a
surprise to anyone so first of all we
tried reducing the number of the tests
we tried deleting any tests that weren't
necessary but it turns out that's
actually not as easy as it sounds and we
didn't really get many gains so then we
tried speeding them up
we tried improving the performance and
we tried running them in in parallel and
in fact the parallel we had quite a lot
of success we did manage to speed up
quite a lot but for the amount of time
that we were spending working on our
tests we weren't seeing a major
improvement in our release frequency so
we did some brainstorming were we
looking at it all wrong instead of
trying to make the test go faster could
we just move the
at which we do testing so if you think
of a time line whoops sorry
conventionally testing is done before
the code hits the live servers so maybe
as you develop you do TDD you're running
your unit tests developing as you go
along maybe you run a comprehensive test
suite as you commit so you get your
commits on the fact that your tests have
to pass or maybe you run them against
staging or UAT as part of your deploy
pipeline maybe even have a number of
different platforms as part of your
deploy pipe moon pipeline and different
tests on each and you're getting your
deploy on the fact that your tests have
to pass the commonality between all of
these is that you have to run all your
tests before you release to production
so what if we shake that up a bit
what if we move the point when we deploy
to earlier in the pipeline so that is
run some tests before you deploy but
then run other tests after you deploy so
in essence we stop gating on all of our
tests having to pass in order to deploy
so how did we do this we move the slow
tests to post deploy so that's the
functional test the end-to-end
integration test etc but we kept the
unit tests before they super quick
anyway unit tests like seconds and this
really helped to speed up our release
process so like I think I said some of
our functional tests were taking around
30 minutes to run originally so suddenly
our deployments were taking 30 minutes
less to run which meant we could deploy
our code out in five minutes
win so we fundamentally changed the way
that we were thinking about testing and
its place in our process or as our next
speaker puts it testing don't end when
shit deployed which is a lot more
succinct now that we'd started making
fundamental changes to the way that we
ran our tests we noticed something else
really strange
we'd been running all of these tests
against our test environments why a test
environment by definition is different
to your live environment you don't have
real load pattern on your test
environment you don't have real users
doing
and unexpected things on your test
environment and you probably don't even
have the same infrastructure if you've
got hundreds of servers on your live
system you might have a lot less in your
test system so we made one other little
modification to the post deploy test we
started running them rather than against
a test instance we started running them
directly on our production servers so we
set up a parallel pipe line to run our
functional tests so that's parallel to
all of our other development systems
there's no dependencies independent of
any deploys that are running and they
alert us three Naji us if they fail
we've got a ton of functional tests so
initially when we tried running them in
one go into cages so we split them into
groups depending on the business service
that they were testing and that meant
that we could run all of the groups in
parallel and we were able to see any
failures sooner since we weren't waiting
for the entire massive monolith of tests
to pass and then if we did have any
failures we knew which business section
was failing without having to troll
through tons of logs and since deploys
since the code deploys are usually in
one business service anyway it made
sense to run the tests as separate
business services so each of these
groups of functional testers cron to run
once an hour and these become our
fire-and-forget tests they're always
running in the background checking that
nothing's changed on our live sites and
alerting us if they do so when we change
the way that we use single sign-on these
tests were actually the first
notification we had that we'd actually
introduced an issue but functional tests
in production aren't just about testing
your own systems they're also about
testing your third-party systems it's a
really great way to get feedback on
latency of third-party systems so for
example if it's an important metric you
could graph to see how much the latency
changes at different times of the day on
your third-party systems and then when
you need to have a conversation with
them about increase of decreasing their
latency you've got the evidence in front
of you great conversation starter but
when you're running against life you
need to think about
you don't want to pollute your live
business data with your test data for
example you probably don't want your
functional test to be making real
payments you definitely don't want your
test user to be able to rack up debts
that it can't pay another thing to be
aware of is that your analytics could be
misleading if you've got tests running
regularly against production that's
going to increase for example if you've
got tests that are pretending to be
visitors coming in to your website it's
going to increase the number of visitors
you've had which is slightly
embarrassing if business starts getting
really excited about all the new
visitors that you've generated and you
have to tell them they're actually only
test visitors so how do you special case
that well you need to tag the data in a
way that all your systems know to ignore
it so we created test users and then we
marked the test user IDs on all of our
systems to be special cased or ignored
say for example we marked our test
payments with a special flag which made
our consumers drop them rather than
actually importing them into our
database and we're able to remove test
data from graphs as well as a result of
this but to be truly end-to-end all of
your systems need to know about test
users and be able to special case them
which can be a problem if you've got
downstream systems there that are out of
your sphere of control for example the
third party systems so one option you
could have here is that you could have
two back-end systems and then
dynamically switch between them
depending on the user a little bit like
load balancing for a specific user so
when you're testing you can test what
you think is correct expected behavior
in your tests but it's only when you're
actually testing on production that you
see the real behavior of users and users
tend to do really unexpected things so
in order to get around some limitations
we had in one of our systems where users
had worked out a completely different
workflow than we'd ever expected it was
pretty ingenious to be honest but we
weren't testing for it because it wasn't
correct behavior so we started to test
for it since we knew they'd be upset if
that functionality stopped working
another problem with testing is keeping
your test data realistic can be a
full-time job data changes all the time
particularly
parties they changed their data and
possibly don't notify you an input data
that was valid when you wrote the test
soon goes out of date as processors
change it so unless you proactively
regularly check the data you input your
tests is likely to get out of date and
tests that run out of out over out of
date data are pretty useless so testing
in production means that you never need
to worry about this your test the input
your test is always up-to-date your on
production your using your your live
users and your live load so once we had
the bug for testing in production we
started to think hey what else could we
run on production and we decided to try
running load tests on production maybe
you're thinking why on earth would you
do that possibly you're thinking
goodness that sounds pretty dangerous so
why not run our load test against
staging what's the point in running them
on production by running against an on
live system you introduce points of
divergence the infrastructure is likely
different hopefully your staging systems
are set up as close to your live
production servers as possible with the
same OS in the same hardware but I think
I mentioned early you might have a
different spread of service if you've
got a hundred servers in your production
system you're unlikely to have a test
system with a hundred servers also you
can have different disk and memory
patterns say for example your live
servers might be running another process
in the background which you don't run on
your staging server for example you
might have a memory intensive backup
which runs once every night what would
happen if you had a really heavy load at
the exact point that your backup was
going off you wouldn't know if you
hadn't tested that on staging that's one
example but obviously there's tons of
different variables through things on
production that happen that you might
not be expecting by testing against
production you get the advantage that
you're using that real data that real
events and of course one of the really
great side effects of doing your testing
against live is that you don't have the
overhead of maintaining a separate
so what low do we use a naive attempt at
low testing would be just to repeatedly
fire an event at your service and see
how they cope but real life is a lot
more nuanced you've got events coming in
from different places at different
frequencies you've got various different
patterns you have daily load patterns
weekly patterns year patterns etc we
also have a really different load at the
weekend our load is much smaller at the
weekend which suggests that most people
tend to browse the web more during the
week when they're working we also have a
global user base our servers are
distributed around the world we use AWS
and we've got clusters in Singapore
Europe and the US and we do this to
reduce latency if a user in Japan loads
an ad they're routed to a Singapore
servers it's a much smaller round-trip
time and in terms of adverts every
milliseconds counts and each of these
regions obviously has its own load
pattern when people in Singapore are
awake and generating loads Europe has a
really low load because everyone's
asleep and vice versa likewise the
popularity of our products varies by
region so different systems receive
different different traffic patterns
from each region so we thought how could
we recreate these exact data patterns
for our load test and when the answer
came it was stunningly obvious we can
use our end users to generate the extra
load that's going to give us an
identical load pattern to what they're
already doing so how do we do this so if
each time an ad fires event we make it
fire one extra event we've already
doubled our load if we make it fire two
extra events we've tripled our load in
case you're worrying the user doesn't
notice the difference compared to the
the video file that's loading the ads
one or two it's a bits little pixels
makes no difference to the user at all
so make sure more realistic for testing
sorry the more realistic you're testing
the better job it can do of identifying
problems
obviously one of the main considerations
when you test loads test against life is
that you can't push it till it falls
over so we test up to a fresh hold we
need to be slightly more circumstan
circumspect so we started running our
tests manually and then slowly ramping
up the load so we run a test allow a
little bit of time see how it affected
our systems and then run the next one
with a slightly increased load etc and
we thought we were being SuperDuper
careful not to push it too hard but we'd
unfortunately forgotten a facet of our
load which then bit us in the ass so
with ads you tend to have a long tail
traffic pattern because people can load
an ad but they're not actually scroll
interview until a while later if they've
loaded it in a separate tab it could be
days before they actually watch the
advert and every time so that means that
the the events aren't being fired until
quite a time later so we were receiving
a heightened heightened load for much
longer than we'd actually expected to in
actually the first time we tried this
actually brought one of our live servers
down to a mostly unresponsive state so
we learnt our lesson and we altered the
load test to automatically stop if the
load got above a certain percentage so
now we push our load test until we start
to see any of our downstream systems
wobble wobble is a technical term how
can you see those wobbles well we have
really good monitoring on all of our
systems that will be affected by this
increased load so at the moment we're
load testing up to 40 times we happen to
know from our graphs that over Christmas
and Super Bowl which are our two busiest
times of the year we usually go sort of
1020 times so we're above that threshold
right so we'd created our faster deploys
we didn't have branches
we had single feature releases we were
deploying straight to production and we
were running our tests after releasing
and along the way we'd hopefully given
developers more freedom to experiment
and try out different things faster
deploy means faster feedback loop so our
next ingredient was making rolling back
easier since we had those deploy tokens
that we had the picture for earlier you
knew that you know that no one can be
deploying at the same time as you
so that makes rolling back dead simple
we just check out the previous git
commit and redeploy so remembering that
every commit is a deploy there's no
faffing around trying to work out what
version was on production before your
deploy you just know that it's head
minus one so it's really easy to write a
script to just do rollback and deploy
out takes a couple of minutes brilliant
so we've talked about speeding up the
deploy process which speaks about making
the rollback easier so now let's talk
about seeing if the deploy has been
successful or not so how do we know when
we make a deploy if the new code is
working as expected
we've got server logs right so you've
probably got your terminal openers you
deploy tailing application logs checking
for exceptions and errors and if you see
anything like that you roll back
straightaway we've also got graphs so
before we deploy we think about which
metrics we expect should change post
deploy and which metrics we expect to
say stay the same it's obviously
dependent on the functionality of the
code that we're deploying out at the
time as you can see are not very tidy
dev area has lots of big wall monitoring
screens so that's really easy visibility
if you've got a line plummeting on a
graph you know that's a bad sign the new
code isn't working you need to rollback
but we actually had quite an interesting
learning out of this so the idea of
verifying the post deploy code by
looking at graphs is really implicitly
baked into our culture but it turned out
that we had some new starters who
weren't aware of this verification step
they just see its deploy and then we
move on to the next thing and they
weren't aware that we were actually
doing this verification step
particularly I think if they'd come from
somewhere that had a build pipeline so
they would weren't used to the idea of
post deploy checks so this resulted in
us having a number of fires where we had
a couple of new starters who pushed
something out and hadn't done the check
to check that it actually worked
once again proving that the the
importance of good communication when
you're coding if you were to ask people
why they monitor I think they would
probably reply that it gives them
ongoing confidence in their systems
metrics can change at any time though
right it's not just straight after a
deploy so although our screens are
pretty in your face
we also automate a notification so that
sends us an alert if any of our metrics
for example with important KPIs have
been breached say for example if we had
a KPI to check that we're still making
money that would be bad for business if
we weren't so that needs to be addressed
straight away so we get a notification
so we recently noticed a really large
spike in our traffic we were receiving
warning alerts from various parts of our
systems and then after four days it went
away went back to normal we had no idea
what would caused it and our best guess
was that it had been related to a large
sale that our business had made but we
realized that we had no way of seeing
how the various business events were
affecting our systems say for example if
we onboard a big publisher we would
expect our traffic to go up since we're
displaying more ads and the higher
traffic to the site means that we've got
a higher load and therefore our metrics
would go up so we've been using deploy
lines for a while on our graphs that's a
vertical line each time you deploy so it
wasn't hard to start putting business
events onto our graphs so the yellow
vertical lines that actually aren't
particularly clear but anyway those are
our business event lines so when you
hover over it you get more information
the discreet at the top screen nearest
to me you can see this like little
square there that's showing that we'd on
boarded a large new site which we call a
whale for some reason so that was going
to increase our load and not only can we
correlate these business events to
fluctuations in metrics but also you can
correlate events across different
services to see how different services
are affected by the same business event
so let's talk tech for a second we've
got collecti that's pushing metrics into
graphite and then I mean graphics
for storing metrics but it's not got an
amazing interface so we use cro fauna
that's how all these graphs are done
it's got really nice transformations you
can do really pretty different things
and then we've got nudges to do our
alerting in a way our success was
actually our failure so everyone started
to create more and more metrics to keep
watch on the various different changing
systems and the number of metrics
steadily increased so we've got about
400,000 metrics and graphite I think
currently so one team spun up 40 new
servers all spinning out tons of metrics
and that actually managed to take our
monitoring server down yeah that was a
fun day
we'd actually we'd actually increased
our metrics to such an extent that we
ran out of capacity on earth on the
monitoring server so since monitoring
was something that we felt we needed to
improve on we had a hack day where the
entire dev team got together and took on
various projects of their choice to
improve our monitoring systems the
business events that I mentioned in the
last slide was actually one of them
overlaying that and increasing the
capacity of graphite was our other one
so another example a little while ago we
found that we had a problem with our
data syncing our systems weren't always
getting updated with the latest data we
didn't know why it was happening so we
were unable to gauge how serious a
problem it was it didn't happen
regularly and sometimes it just seemed
to fix itself which was super confusing
so we decided to use a monitoring
feedback loop so we created a new graph
to monitor improve the existence of the
problem in production from that graph we
were able to see that it was a recurring
problem and how often it recurred having
the historic data we were able to
correlate the problem with something
that our ops team had been doing so we
decided at that point to save work on
doing in any upfront work to fix it to
automate the fix so we deployed a manual
fix first and then we checked our graph
to check that the problem had gone away
with the manual fix which it had so then
we also added a nudges check to our
graph to proactive
alert us if our slack in our tonight
channel if anything happens and then the
problem did reoccur so at that point we
were able to automate the fix to
regularly update the data but so we
waited by using the graph as a feedback
loop we waited until we definitely knew
that we needed to make a proper fix we
just done the manual fix first so it
saved us time initially so does that
sound familiar
it's like code to the TDD feedback loop
I mentioned earlier so you've got the
read the graph shows you the issue the
green you fix the issue and you see your
graph coming up again to where you want
it to be and then you refactor either to
add another check to add a notification
check or to actually fix the underlying
issue so we've started to use this
feedback loop to show that the expected
business value is delivered with a story
so as part of a story we create a graph
or dashboard with the main KPIs that
will be affected by the story and we say
for example if we expect our change will
increase a certain type of user
interaction we would put a graph up to
prove this and then once we've released
we're able to check on the graph to see
that the the the code change has the
expected change in metrics our
assumptions were correct
so whereas monitoring's pull alerting is
push and Lessing is probably going to
make wake someone up in the middle of
the night as well right so is it really
needed and when is it needed what are we
going to alert on so let's get some
audience participation
I know it's post-lunch but if yo you got
armed army yoga so imagine that we've
got this incredibly simple system we've
got a back-end database we've got two
web servers and we've got a load
balancer okay so if we got very high
load on our database hands up if you
think it makes sense to send an alert
that could wake someone up in the middle
of the night before that couple okay
so it turns out most of our user queries
were running fine we had a high load on
the database but I didn't seem to be
affecting the queries that most of our
users were doing so one of our web
servers has stopped serving hands up if
we should alert on that couple okay so
we've got our load balancer hopefully
it's intelligent enough to transfer all
load to the responsive web server our
pages are taking much longer to load
then our KPI allows hands up if you
think we should alert on that one yeah
this is the main one right this is
actually the user being negatively
impacted the other ones your system
might be able to cope with this one your
users are going to be upset this is the
one that wakes you up in the middle of
the night that's a really great way to
avoid pager fatigue I don't know if you
guys have that but the more times you
get waking up at night the less likely
you are to really care about what's
happening so page on the symptoms not
the causes so one edge case that we've
had is that when you notice disk is
filling up it's probably better to
report on that during the day so that
someone can fix it straight away
otherwise it might result in someone
getting waking up in the middle of the
night which is a little bit cruel the
exciting thing isn't what's testing in
production is but rather what it enables
you to do as a result of the change in
your mindset about the way you release
you've got quicker feedback you move
faster you're doing lean deploys shorter
deploy cycles it gives you courage you
can see it working on production you get
more confidence that means you can
respond to your user needs faster I'm
quite a fan of feedback loops I'm sure
you're getting that so I think we need a
new kind of developer a new kind of
mindset to make this work you've
probably heard the phrase full stack
developer I prefer to think of a full
cycle developer so if you wanted you
could equate this to the wildly
polarizing and often misunderstood
DevOps movement so I define a full side
cycle developer as a developer who not
only develops code but also writes their
own tests and does their own deploys
builds and cares for their own
infrastructure be that bare metal cloud
service as your functions whatever and
they will say get user feedback and they
create new stories last but not least
they take it on to take turns to be on
cool
whoa wait a second did I say developers
should be on cool who's the best person
to fix an issue it's the person who
wrote the code who knows the inner
workings of the system that's the
developers so it makes sense to put us
on call in fact having a separate same
team for testing another to deploy
another to be on call introduces
multiple communication boundaries and as
we know failing community to communicate
all necessary details is a common cause
of failure in software and also having
separate teams introduces a bit of them
in US culture knowing that other teams
will go over your code test deploy and
look after it can kind of reduce that
feeling of accountability account of
responsibility oh it's their problem now
I don't if anyone followed the ramps
discussion by Michael Hill it's a little
while ago I rather like this make great
software by making great team so ramps
is rhythm autonomy mastery purpose and
safety
so he says teams lacking safety lack
ideas because having a new idea is scary
ideas are the fundamental unit of
software progress and also teams lacking
autonomy lack the freedom to make things
worse
but they also lack the freedom to make
them better so allowing us to make a
mistake and not be fearful of the
consequences
imagine how liberating that is to have
your boss say oh well you tried
something it didn't work this time but
whatever
no harm what are you going to try next
pretty liberating that outright
acceptance of mistakes is key to
building confidence and trying radical
new things that will hopefully drive
your business Ford I had to have an
obligatory cutesy cat picture right so
it's kind of a change of mindset for
developers the freedom to play a little
and experiment the satisfaction of
seeing your code being used by others
straightaway and being informed about
the success gives you the confidence to
work faster no one's nervous about
flowing or rolling back you know it's
all pretty straightforward right
so you think testing in Milwaukee
hopefully sounds interesting
how can you introduce it yourself at
work so first up I need to say it's not
for everyone some business sectors don't
lend themselves to working this way for
example if you write code for nuclear
deploy missiles
I would rather you forgot everything
you've heard today and you didn't test
in production but testing in production
works for us I'm really because we're
happy with the payoff of having less
rigor and less security and deploy which
allows us to deploy faster perhaps your
overall business model won't allow it if
you're in finance for instance but maybe
parts of your business could apply these
techniques so what can you do try
running your man you try manually
running your slowest test probably your
functional or your end-to-end against
production remembering to special case
and then once you're happy that it's all
working and you've got enough alerting
in case there's any errors then stop
running them as part of your deploy po
pipeline improve your monitoring I mean
more monitoring is always good right you
can't go wrong there and how quickly can
you roll back is it a complicated
process that requires document document
ation to be open as you go through
various steps try simplifying it the
quicker you're able to roll back the
less nervous your feel about deploying
it's no longer a process to worry about
but instead something that's really easy
to do if you've got a quick rollback
process it becomes commonplace it took
us a while to get where we are now I'll
be honest and in the process we learned
some valuable lessons mostly about how
our systems work and also what's about
what's important to us as developers and
as a business so I hope I've inspired
you to make a similar journey but
ultimately explore and have fun thank
you
I've got a couple of minutes for
questions if anyone had any yeah that's
a brilliant brilliant question yeah so
yeah we do that on development first so
we do check that's working and also as
part of if we're doing a release which
is a development schema
sorry a database schema update that's
the only thing in that release that one
schema so we've tested that on our test
servers and it is working before we put
it into production yeah
because although that's that's the one
scenario thank you for bringing it up I
should have mentioned that
anyone else yep
so roughly we've got probably about 15
pairs and I would say most pairs release
at least twice a day give or take that
is a really good question I guess we'd
probably have monitoring in place to
allow us to see but honestly no I guess
because we've been doing it for so long
now it's a really good question the the
unfortunate thing with working like this
is that the the thing that most people
see is when it when it goes wrong when
you have a failure to deployment and
everyone costs up that exactly how much
money you've lost as a part of having
their failure oh sorry
how much it we've saved to the company
by doing the entire process of the the
fast deploys
so usually you can see any problems
pretty much instantaneously when you're
tailing your application logs cause
you're going to have exceptions thrown
left right and center if anything it's
gone wrong but in some cases it can take
up to a day so you've got there
different patterns in user usage right
so maybe something happens every night
that's slightly different and that shows
up that your code is bad or some
everything every week and it shows up
that there's a problem with your code so
we can take a little bit longer but
generally I would say either tailing the
log straight away or just looking at the
graphs within half an hour yes
oh well no we we actually do deployments
pretty much straight straight away but
like I said each pair is deploying maybe
twice a day and if there's only a couple
of pairs working on the same codebase it
probably means that a code base is
deployed out four times a day or
something we've got quite small we break
in our code base into small micro
service e type modules we would deploy
if we're not 100%
okay so yeah so there are there are
cases if you find out that something's
gone wrong a little bit later that you
might end up having to roll back the
last two commits let's say that that's
definitely a scenario now I mean there's
there's always going to be edge cases in
anything right but the idea is that
we're we're willing to take the risk and
if something is wrong we're willing to
do a little bit of extra work to fix it
because we can deploy out faster that's
that's the key key hopefully successful
metric for our company oh maybe once a
month I would say Oh functional test
fail not that often surprisingly we
don't often make massive UI changes I
guess so probably once a month as well
yes yes yeah so if we literally know the
fixes one-line yeah why bother rolling
back one-line deploy out yeah absolutely
yeah we do that a lot to be honest if
the the problem on production isn't
something that's you know tons of money
going down the drain then it's easier to
just fix it and roll out the fix rather
than roll back the roll back is for
cases where you'll literally use it
losing money there's a fire everyone's
screaming that are those sort of
situations the stress the panic yeah
I'm sorry I don't think I understood the
question so we've got the the feature
I've gotten the word the feature toggles
so if there's something that isn't
really production ready we toggle it
behind a feature so that maybe only
internal maybe even just development can
see the changes on production and test
them and see how it's working so quite
often we will put a change out that
we're not sure how it will affect our
users so we do that we toggle it behind
a real feature so no one can see it yeah
that allows us to build up a number of
different commits check they're all
working before we then deploy out to the
user if necessary yep
generation of bad data rising the users
using our systems
so luckily we don't generally do
anything like that mm-hmm
so like I said it's not it's not
necessarily suitable for everyone if
you've got you know the the ability to
accidentally bill someone four million
pounds by mistake if you put your
decimal point in the wrong place you
might not want to use this system
because by the time you've done the four
million pounds you already you can't
roll it back right as money well unless
you can I don't know
I really sorry I couldn't entirely hear
that could you just okay Oh as in data
in the database data that's come in so
if your codes that you've released into
the production environment is generating
data and it's generating it erroneously
mm-hm
how do you then roll that yeah that's a
great scenario so we actually had that
about a month ago so we it was it was
actually the inverse of that we'd
actually stopped firing a pixel that we
needed so the data had stopped coming in
but we knew there should have been data
there so in that case we actually looked
through the previous day and saw what
data are coming the previous day and
then guesstimated what data should have
come in during the day and update to the
database
so yeah updating databases on production
brilliant</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>