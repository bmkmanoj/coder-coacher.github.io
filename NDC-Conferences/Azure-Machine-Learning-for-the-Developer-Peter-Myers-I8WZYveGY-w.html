<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Azure Machine Learning for the Developer - Peter Myers | Coder Coacher - Coaching Coders</title><meta content="Azure Machine Learning for the Developer - Peter Myers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Azure Machine Learning for the Developer - Peter Myers</b></h2><h5 class="post__date">2016-10-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/I8WZYveGY-w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's curious about what I could do with
my experiments and how I could integrate
and embed them into application
experiences and this is very early on
when Azure machine learning was
available in preview and there was very
little content that was going to tell me
how I could successfully integrate my
experiments IND application so the
emphasis on this is to explain how to
integrate into a dotnet application but
there's probably going to be some
requirement for you to understand well
what is Azure machine learning how do we
develop an experiment then how do we
deploy it then how do we integrate it so
the intention is an ambitious one for
this 60 minute session and is to go
through the entire lifecycle of how
Azure machine learning can be used to
develop predictive solutions and then
how to integrate them into an
application very briefly my name is
Peter Meyers I'm a Data Platform MVP of
ten years my specialty is really working
with sequel server analysis services
reporting services and anything that's
related and so Azure machine learning
comes in there as something to
supplement my skill set into cloud
services let's begin with describing
what machine learning is will then
introduce what Microsoft are offering is
a cloud-based machine learning service
with Azure machine learning I'll
demonstrate the full cycle from data
preparation right through to delivery in
an application and then we'll finish off
with some inspiration for you on how we
might use Azure machine learning and
other business scenarios so humans have
this interesting need to know what could
be alright and if we take a look
historically we've seen that you know
business people might consult the
services of a clairvoyant and I'm not
sure what you think of this approach to
predictions it's certainly not grounded
in good science but interestingly
historically we've seen examples of even
presidents consulting with clairvoyance
to understand you know or determine
military strategy we're not going to
suggest that another approach might be
referring to mollusks and there are
incredible ability with high degree of
accuracy to even predict the outcome of
World Cup matches but can you imagine
going to your IT manager and suggesting
that you install an aquarium of octopi
to help you with the predictive needs of
your business probably not a practical
one so let's talk about good data
science in the topic of
learning and let's begin them with a
definition that machine learning as a
subfield of computer science and
statistics that deals with the
construction and study of systems that
can learn from data rather than follow
only explicitly programmed instructions
let's put this into an example if you
need to respond to a question that is
what happens or what's the result of
adding two numbers together it's fairly
basic as you'd appreciate we could build
a function that passes in number one
number two and using a language like C
sharp we could produce a response and a
reusable function that's pretty basic
but let's consider this question a lot
more different I need to predict
customer profitability and you would
recognize up front the need for a
function whereby if we pass in customer
demographic state age gender marital
status and so on we would like it to
output some prediction of what profit
might be generated from this customer so
you begin with your favorite language
and I'll use c-sharp here and you
produce a method predict profit class
and you pass in all of those demographic
States what are you going to write as
fixed logic to take those inputs and
output a prediction and the fact is if
you're taking this path while I'm sorry
you're on the wrong path all right we're
going to suggest that you take some good
quality data that represents some
history have your customers defined in
terms of rich demographic values and
also some history of what profit they
have produced for you and then using
machine learning algorithms they will
sit through looking for patterns and
correlations between the different
demographic states and the profit and we
would then query this as a model and we
could query this real-time as I'll
demonstrate today so in fact this
introduces the demonstration that I'll
produce for you today producing a
machine learning solution that will
predict customer profitability where do
you think that might be useful in an
application what might we drive in an
application experience if we have an
understanding of what profit a
particular customer may deliver to us
financial about possibly I'm thinking
more generally in an e-commerce scenario
what might we do with that intelligence
recommendations yes right so how might
we inspire those that might deliver
higher profit or higher sales from us so
recommendations is one approach or
targeted advertising it's the the
approach I'll take today let's not just
randomly choose adverts let's choose
adverts that are more likely to attract
the different types of customers whether
they're low profit delivering customers
a high profit all right so when we
produce a machine learning project the
flow often follows this path with any
project you want to define quite clearly
what the objective is so the objective
here would be we need to determine with
a degree of accuracy what profit we're
likely to attract from a customer next
we look for some data what data do we
have available and if I had a CRM system
that might provide some richness in
detail of a customer if it's an
e-commerce application we're likely to
have a profile for the customer when
they registered to shop on our website
hopefully we collected some good
responses to a number of interesting
questions and that will help us out when
it comes to a machine learning task such
as this now we don't accept that data as
you viewed good to go we always deeply
suspicious of the quality of that data
all right so we spend good time and
effort preparing the data to ensure it
is in optimal and highest quality
possible and then we're ready to move
into the machine learning process which
involves training models note the plural
here we don't create just one model and
say you beauties great deploy and use in
fact there's two tasks that go hand in
hand
we define models in terms of an
algorithm inputs parameters to tune the
machine learning process and an
expectation of outputs all right and we
create lots of these there are so many
different variables and variations you
can work with ultimately you're going to
arrive at the one that best fits your
purpose typically in terms of
reliability and act
see and then we will deploy this as a
model and once published Azure machine
learning would deliver this as a web
service from a management point of view
we'd want to ensure that it's available
and scalable and then from an
application experience we can then
integrate this in some meaningful way in
an e-commerce application let's use it
to drive a real-time targeted
advertising all right so upfront a
discussion on what roles might interact
with machine learning so a little bit of
an experiment here I'm asking you to
tell me a little bit about who you are
this is completely optional could I just
ask you to jump to this URL either use
the QR code or the tiny URL I have here
there are five questions please be as
accurate as you can this is in fact a
power bi dashboard it's a real-time
delivery of your survey results up here
on the dashboard and we'll just do a
quick assessment of who you are and I'll
give you about a minute to submit your
responses
okay we had a Kiwi in the room
so as I'm expecting from an audience
like this that you're a majority
software developers some managers here
know likely likely to evaluate what
we've got on offer here I'm looking to
see whether there are any data
scientists in the room so far no
intermitting to it alright so thanks
very much 23 submissions by the way is a
plug for a session I'm delivering this
afternoon power bi for the developer
we're going to do a lot more detail on
how we can deliver real-time dashboards
programmatically alright so what I was
looking for was the submissions by role
so you can see the bar chart here
showing me that predominantly software
developers that expect at a conference
like this back on track then with the
discussion of roles so primary role that
you will hear with machine learning will
be the data scientist so with no data
scientists in the room usually I call
them out and the question I would ask
them is how much do you earn a year and
before they answer the embarrassment is
enough in what you're going to discover
is is a worldwide shortage of technical
skills to drive the needs of businesses
today and that lead is to deliver deep
intelligence and meaning from data sets
all right so the data professionals such
as me I'm a generalist business
intelligence I work with 11 reporting
but really we're looking for people that
are highly skilled in statistics
mathematics computer science and it can
solve complex data problems and that's
what the data scientist does now it's a
primary role associated with machine
learning but what I will emphasize in
this session is that machine learning is
becoming approachable for professionals
like myself and like yourself as
software developers so we're not just
focusing on the data scientists I would
suggest that those that have fundamental
and basic data skills whether that's
just defining relational database
schemas is a good start that is enough
to prepare you for some interesting
adventures in the machine learning world
the next one of course is the software
developer that may produce machine
learning solutions but are likely to
integrate machine learning solutions
into application experiences all right
when it comes to the challenges for
machine learning well skills or shortage
of skill sets
so traditionally machine-learning
they've been great barriers of entry
because of the requirements for hardware
software and the skills to run this this
is becoming more approachable by
services that I'm about to introduce
with Microsoft's Azure machine learning
so all that that's typically expensive
in terms of hardware the skills there
might be lots of different tools
required to deliver that entire workflow
that I've introduced you so far and
undoubtedly it is quite complex the
consequences of this are that if you
don't have it you're either not being as
efficient as you could in business or
you're relying on a lot of guessing and
estimations that may not just be so
accurate and what we could suggest to
you is that if you could improve the
predictions just by let's just suggest a
1% improvement by the relevance of
suggestions that you feedback to your
customers with the volume of hits and
browsers that take place on your website
that 1% improvement can translate to
tangible financial benefits that could
easily justify the cost of these types
of projects all right well let's now
introduce what Microsoft are doing in
this space so Azure machine learning is
a cloud-based predictive analytics
service it's designed for professionals
and not just data scientists as I hope
to convince you in this session but a
range of professionals including data
developers and software developers and
that armed with nothing but a browser
and good internet connectivity you have
a very powerful service available to you
to start experimenting with your data
and when you experiment and produce
something that you think it's valuable
you can easily deploy and operationalize
and as a web service you can then
integrate this as a predictive
capability into an application
experience the data that it can work
with so bearing in mind that is a
cloud-based service so cloud data stores
whether it's a just sickled databases as
a single data warehouse HD insiders
microsoft's big data platform you could
also access data in virtual machines if
you need to work with on-premises data
two approaches the first is you can push
the data up for storage and then access
by the service or in preview you have
the ability via a gateway
to allow sequel server on-prem data to
be available to Azure machine learning
how does it work well there's an azure
portal that allows you as your ops team
to provision a workspace in Azure
machine learning
there's ml studio that is a
browser-based design tool that will
allow you to experiment and publish and
then lastly you have an API service that
allows you as a software developer to
connect to and integrate into an
application experience so working
through the entire cycle I'm going to be
wearing the Hat just for the moment of
the azure Operations team and I come to
the azure portal and here under machine
learning I create a new workspace so
let's just call this the NDC Sydney and
they're only three data centers in the
world supporting this today and I will
create a new storage account also named
NDC Sydney assuming that no one else has
taken that southeast agent no it's not
I believe it's Singapore as simple as
this within 30 seconds we're going to
have a workspace the next step would be
the azure ops team would then add in
users so that within the workspace they
can collaborate on data sets and also
experiments so we'll come back to this
in a moment we'll move forward and then
start working through that entire flow
from defining the objective right
through to managing and integration so
within the context of my example and
that is looking to predict the
profitability of my customers so it's
very important to have a clear objective
and certainly in a machine learning
project you know unlike other data
related projects that I would work on
now
they're usually multi objectives it's
like we might use it for reporting on
sales and profitability and various
analytic requirements whereas in a
machine learning project it's extremely
narrow in focus it's going to do one
thing hopefully with good accuracy and
reliability it's going to predict the
profitability of my customers so back to
that question I need to predict customer
profitability but some more information
to deliver targeted display advertising
company ecommerce website to both
present relevant product suggestions to
also increase sales and profitability
with that in mind we then move on and
say well what data do we have to support
this and knowing that I want that
function passing customer demographic
details and pass out some prediction of
profit I would be looking internally
perhaps externally for valuable data
sets it could be a CRM system it could
be the data warehouse it could be the
OLTP system that drives the e-commerce
ultimately what I want is as much good
quality data that represents my
customers and their behavior so here in
demonstration I'm going to show you that
I had a very simple data set it is my
customers profit the CSV file that I'll
open up in Excel and here I have 10,000
rows of data so let me go ahead and
select and make this into an Excel table
and by the way this is office 2016 so
when I have an Excel table on my data
ribbon I have the ability to get and
transform and so this is a really cool
tool to allow you to both explore and
understand the data and maybe also
further prepare it in meaningful ways so
let's just take a look at the data we
have the unique ID is that going to be
valuable in predicting profit is there
any value in it whatsoever
yeah yes correct so we can link it to a
sales system but as far as asset that
would be used for machine learning at
this point is there much value in it so
you know data scientists wouldn't
necessarily dismiss it so quickly they
might suggest that a sequence might tell
us you know how old a customer is in the
absence of a join date we might actually
use that in some way but today will
suggest that it's not that useful what
about first and last name that someone's
first name is Peter or Paul would that
impact on profit No maybe so we might
just leave it open and say well let's
just see what happens let's just throw
it at an algorithm and see whether it
finds any relationship between name and
profit certainly in the absence of
which we in fact have you might use
libraries that know that these names are
male versus female so you might actually
use it to construct other attributes
that either you don't have or you have
missing data in them yearly income it's
been pre discretized into twenty five
thousand groups all right now that's
probably because in the e-commerce
website when they register as a new user
it's a drop-down list these are just the
values that were available to them
we didn't give them a text box and say
till the dollar and send tell us how
much you earn that's sort of a bit of an
imposition on privacy right now up until
here these are all demographic details
and here's where it's becoming really
interesting that we've joined to the
sales history and with their knowledge
of the total cost of sale we can produce
a profit and to be very very careful
here we've suggested that these are the
profit for a 12-month period you can't
just say the total profit over all time
but in the last twelve months for these
ten thousand customers this is the
profit that they have generated all
right does anyone know where the data
came from it's not real data it's one of
my favorite databases which is the
adventureworks
data warehouse all right so I have some
good quality data that is an absolute
prerequisite all right so the next
discussion would be about preparing this
data so significant amounts of effort
really need to be given to ensuring you
have that data that is representative in
the best possible quality that might
mean transforming the data to fix it to
cleanse it change data types standardize
maybe even isolate or flag of normal
data what would we do if we discovered
abnormal data flag it would we remove it
would we keep it you'd need to refer
back to your objective all right if my
objective was I'm looking for abnormal
data ie
fraud detection then you want to keep it
and in fact you might use statistical
techniques that would over sample it and
exaggerate it as part of your
preparation such that machine learning
algorithms would learn that
see that very clearly in this case we
might then determine okay anomalies they
skew the results we'd actually remove
them so depending on your objectives
will depend on what actions that you
take substitute missing values will come
to this one in a minute of course and
here's a word for you guys as developers
that when you're designing applications
today machine learning is probably not a
number-one priority for you but
certainly I'd asked me to bear in mind
that you should start capturing and
maintaining good quality data so that
machine learning is a viable option for
you in the future
for example in an e-commerce application
if you simply say when you register that
all we need is your email address and a
password there's not a lot that you can
machine learn from this yet if you came
in and had a profile that said right go
ahead and provide us details about you
know your nationality your gender etc
and you maintain this
have you noticed from time to time that
Amazon will ask you to go back and
review your profile so keeping a current
is also part of your responsibility for
ensuring you have quality datasets I
often use an example of the Afghanistani
accountant problem that you might come
across with datasets and I ask you to
think for a moment why that might be a
problem I've done Estonia accountants
you find that when analyzing your data
set you find in a number of them and
they stand out yeah so the software
developers decided let's just make life
easier for our new customers let's just
default to the first item in the list
all right and what your customers want
to do they're eager just to get to the
shopping cart
check it out pay and have it delivered
so if you provide defaults to the first
item in a drop-down list well you're
going to jeopardize the quality of your
data so Afghanistan is the first country
accountant is the first profession now
often what would happen at this stage is
that you would have such an abundance
and they would skew results that you
forced either to remove the entire
attribute country or profession or the
cases those individual customers would
simply need to be removed so something
as simple as that
ensuring that you have no defaults can
dramatically improve the quality and
therefore the volume of data and
therefore the quality of the machine
that you can perform alright so let's
see in demonstration that just taking
that raw data set the way that I might
prepare it first of all I'm gonna make
the decision that first name and last
name would be removed and this is an
important point certainly for some
customers in certain countries is it in
recognition that this data set is going
to be pushed up to the Azure machine
learning workspace and so in some
countries it's illegal to send
personally identifiable information to
storage up there so I'm just going to
use the functionality here to remove
these columns age looks good marital
status single or married gender male or
female discretized income next when I
look at the total children and in fact I
notice that there is some nulls here so
I've got some missing data and there
were just 10 so 10 out of 10,000 I can
live with this I can easily make the
assumption that they have no children
and then I'd have a chat to the software
developer and say could you please
ensure that new registrations don't
allow a null entry that they have to
choose some valid value now when we look
at the number of children at home and if
I group by and this is what I love about
this tool is it's just easy to
understand your data but I can see a
significant volume in fact almost 50% of
the number of children at home so I'm
going to make the decision that we don't
have enough quality I can't repair this
easily so I'm going to go ahead and
remove this column also and that's a bit
of a loss like with the nationality and
profession to remove that is going to
impact on the quality of what I can
achieve with machine learning the rest
comes down to education occupation do
they own a home how far do they travel
then we come to profit generated so with
a consistent or a continual numeric
value here I need to think carefully
about what I'm doing I'm looking to
predict the profitability of customers
and am I looking to predict to the
dollar and sent the profit they might
generate or am I more interested in a
generalization of what profit they will
deliver and the answer is I'm looking
for a generalization and I make a
decision that I'm going to categorize
each of these customers as either a high
medium low or
very low profit generator because when
we think about it from a marketing
perspective when we have targeted
advertising
we don't have adverts to the dollar and
cent they're really bucketed here are
the adverts we want to show a high
profit generating customer and here are
the adverts that we would show a very
low profit so I'm going to translate
this value into a label and that can be
achieved by adding a column and I'm
going to name this the what are we
calling a profit generated label and I'm
just going to use a language here does
anyone know what language this is
but if profit generated is less than 100
that I'm going to classify this as very
low no it's not VB power query and it
goes through an informal name it's a
single letter and it's not our no M
right so it is indeed M so using the
language of M I come with logic here
that says these are the labels that we
will assign now this is something you
want to play around with a little bit
and let's just do a quick group buy to
understand the distribution across the
ten thousand all right so I've got about
4200 low and I have about 1700 high
and so I've arrived at the end of the
data preparation phase so what I'm going
to do is just close and load and it
takes that transform query result and
loads it back to a new sheet within the
workbook here is my prepared data and
then I'm gonna go ahead and save this as
a CSV so let's just save it to my assets
and I'll call this the customer profit
prepared
and as a CSV file I can upload this to
the Azure machine learning workspace and
let me finish on that task so now that
the workspace has been provisioned the
ops team of added in users and then
those users can sign into ml studio and
so just simply by using a web browser
they can go ahead and start working with
data by uploading datasets and creating
experiments so the first task will be
here under datasets I'm going to create
a new one from a local file and that
gets pushed up now that's data
preparation happening on my desktop and
for the volumes of data that you might
be working with with machine learning
that could be terribly impractical Excel
certainly has limits to the number of
rows we can load into a worksheet
1,048,576 to be precise but we might be
working with you know excessive numbers
and big data stores so the other
approach is that you can also do data
preparation in the experiments
themselves all right so I would suggest
use the tool that makes the most sense
to you
if my data was in sequel server I might
use T sequel scripting integration
services whatever it takes or I might
choose to place raw data up there and do
the transformations in the experiments
themselves so let's switch across now
and create a new experiment within the
workspace I think the name here is ideal
it is an experiment and ultimately if it
proves to be valuable then you can
deploy it to become a manageable
production resource now when I create an
experiment just to show you that there's
a great way to get started with this
product is that there is a growing
gallery of different experiments that
you can work with for example if I just
put in a keyword fraud
and I know that I've got a fraud
detection problem to solve the best
approach might be well let's just learn
two other experts that are sharing both
their data sets and experiments for me
to learn from
here in this demo I'm just going to
start with a blank experiment and this
is an obvious but the name of your
experiment is here so let's provide a
better name for it which is to predict
customer profit and for anyone that
might be familiar with integration
services with sequel server is there
anyone in this room the data flow in
this product simply components
components to perform some type of
functionality and we connect them to
produce a flow and so I can show you
here under save data sets that I have
the uploaded data set I can also show
you that if I come to the data inputs
and outputs
I could import data from other sources
with these sources being supported all
right now if you were going to use these
sources well then data cleansing other
preparation would take place inside the
experiments so let's take a look at how
that could happen
first of all let me visualize the data
here so bear in mind this is just a web
browser there's no need to install any
software it just requires that you have
a supported browser which is all modern
browsers and Internet connectivity and
you don't even necessarily need an azure
subscription there's also a free
subscription to work with Azure machine
learning so let's just take a look at
the visualization of the data and there
are a couple of things that I need to do
first of all I'm going to narrow the
columns down even further the customer
ID I kept there and that's always handy
for you to go back to the source data if
you need it the other thing is that the
profit value that raw continual amount
should be removed all right I'm just
interested in the label and then lastly
I had some missing values for the total
children and I'd like to cleanse that by
substituting missing values with zero
it's a little tricky to do it when it's
not going to visualize my data for me I
did a run through this morning which let
me tell you is very wise because while
I'd presented this a number of times
Microsoft deployed changes from time to
time and there's nothing worse than
being in from an audience and things
have changed on you like they rename
components
go hunting for them so I did run through
it again this morning to make sure they
hadn't broken any of my demos and I had
this issue this morning it took about
two or three minutes to actually
visualize the data set so let's come
back to that and I'll move forward to
the next discussion and that is that
once we've prepared our data we're ready
to actually do the machine learning
itself and what that means is that we're
going to develop models and a model can
be defined as an object that is
processed by using an algorithm that
will sift through your carefully
prepared data and it will extract
patterns and it becomes a queryable
resource that we can then use for both
exploration and for prediction so the
emphasis here is on plural we don't just
create one we can create dozens we could
spend weeks determining what is the best
algorithm what parameters best tuned
what inputs and what quality of data to
get the output that we need so I'm going
to put hand-in-hand till we train models
and we evaluate them ultimately arriving
at the best fit model fit for our
purpose in terms of accuracy reliability
and usefulness ouch
okay so not a lot I can do until I can
have a look at what this data looks like
are there any questions while it's still
thinking about this
for uploading of data okay if you on the
free tier and Microsoft will allow you
to work with one gigabyte but otherwise
there's practically no limits the data
that you would upload would be stored in
Azure blob store and basically you'd be
constrained by it and so while it's
thinking about this how about I open up
the Azure machine learning pricing and
so to answer those types of questions
there are some great FAQ sites but also
when I scroll down here I'm sorry did I
say one gig you've got ten gig max
storage so potentially you could upload
ten gig of CSB data and mine from it if
you're prepared to pay for Azure machine
learning then it's basically an
unlimited there we go
that's the response I'm looking for
we're back on track so the visualization
that I've got up front it tells me that
there are 10,000 rows of data twelve
columns the selection of a column would
allow you to review some descriptive
statistics about that column we know
that customer ID should not be passed
into a machine learning algorithm it's
not a meaningful value that we expect it
to correlate with some profit we do
recognize that total children has ten
missing values and it's very very
important that I remove profit generated
because there's a direct correlation
between profit generated and the label
remember the formula in M that I used we
don't want it to be attempting to use it
because it will find a direct
correlation very quickly so two things
that I'm going to do the first is to
remove some of those twelve columns and
if I just search across all of the
modules by the word column I've got one
that's called select columns in data set
so it truly is drag dropped connect
configure type arrangement when I launch
the column selector I'm going to use a
rule that says start with all columns
but exclude these that is the customer
ID that is also the profit generated
next I want to clean up those ten with
missing values and simply by typing in
missing will find theirs
module dedicated to cleansing missing
data again drag-drop connect and in this
case the default is that it is going to
cleanse via custom substitution so which
column well let's go this time to no
columns but include by name
the total children so for that column if
it encounters a missing value the
default is that we'll substitute it with
zero but you'll see there are a full
range of different cleansing that it
might do Afghanistani accountants go
ahead and remove entire rows etc so
we're now at the point where we have
good quality representative data ready
for machine learning and I now need to
give consideration to the algorithms
that I'm going to use to train this data
so we will find them under the machine
learning grouping and we'll take a look
at initialized model and we have to
start thinking about what we're really
doing here so the four different
groupings of algorithms which is it that
I will be choosing from predicting
profitability in terms of a class either
very low low medium or high it's a
classification where as regression if I
was looking to predict the profit in
terms of dollars and cents there would
be a regression activity all right so we
look under classification and then we
say wow we've got this overwhelming list
of available algorithms and yet we
couldn't narrow this down very quickly
to a subset what criteria would I use to
narrow them down how many possible
classifications do I have for so is it
to class no a to class would be a binary
classification quite common does this
patient have the disease yes or no
whereas here there are four
possibilities so it's really the multi
classes that I'll focus on and then I
find that I have four and it's like more
I'm now confused which I should choose
and I'll draw you back to the object
that you're working on and it's an
experiment
if you don't know we'll try all
algorithms and then we'll have
scientific measures or tests
to evaluate what happened so using a
multi-class decision forest and if you
really want to know what it does when
the module is in focus you just click on
the more help here and the online
documentation will give you a general
idea about what it does but it will also
go into detail about how you may
configure and fine-tune it so when I
scroll down you'll see that there's a
full range of parameters they may not
mean enough to you as a software
developer or a database developer a data
scientist but it would probably come to
terms of this more quickly but again
part of your training is to experiment
make mistakes learn from those mistakes
so what happens if I change it from five
to fifteen does it improve the accuracy
and I'm about to show you the process
whereby we can test the accuracy of the
models that are trained so I'm not going
to change any of the parameters for the
algorithm we'll stick with the defaults
and having selected an algorithm next
thing is to go ahead and train a model
so we dragged in the module it needs two
things what is the algorithm and what is
the data now before we pass data end in
order to test the accuracy of a model
we're going to split the data into two
sets 70% random set for training the
models and the reserved 30 percent will
be used to test the accuracy and given
that that 30 percent knows what the
outcome is when it predicts it can
compare it to what actually happened and
it's a simple ratio of how often it gets
it right is a measure of the accuracy of
that model so let's take a look at the
split data module let's take the output
of that Cleansing pass into the split
data and use a fraction of 0.7 so the
left output will give me a 70% random
set of the data and then I can pass that
into here to train the model on what are
we training for I launch the column
selector and we're asking it to learn
how to predict the profit generated
label now the next thing is to test how
accurate it is
so this is machine learning and you
remember you as a student at school you
did a course or a class and you had
assessments and those assessments were
scored and therefore you were graded at
how well you had learned the objectives
of that class so the models go through
the same process they get scored
so using that train model with the
reserved 30% of data tell us how good it
is at predicting now that essentially is
the pattern for simplicity in this demo
I'm gonna copy and paste and produce
another train model and this time I'm
going to use a different classification
algorithm alright so here I'll use the
multi-class neural network and then
using the same data in for training and
the same data for scoring and then
lastly what I'll do is evaluate the two
models side-by-side and that can be
achieved through the evaluate model left
hand side what was the score for the
decision forest right hand side what was
the score for in the neural network and
now I click run by the way everything
I've done so far hasn't involved any
cloud service it's just been a designer
on my desktop and now I take a look in
the top right corner it's queued and
here it is running through the entire
flow sourcing the data from the data set
doing some preparation splitting in
parallel training two modules scoring
them and then an evaluation you could
so you want one experiment but you you
what we could do is persist the data and
in another experiment could pick it up
you could do it that way but when you
open up some of the gallery experiments
you know there's literally hundreds of
modules which is quite overwhelming by
the way but what you'll find is that
there simply we're using the logic
there's a pattern and there's a
variation on a theme and once your data
is prepared and split for a
classification project you'll just find
there's lots of these going on so don't
be overwhelmed by seeing this spiderweb
of connections you can quickly break it
down and understand the patterns all
right this is a good sign we have
success and when I right-click and
evaluate and visualize the result of
this we can see side by side left hand
side decision forest right hand side
neural network the overall accuracy the
simplicity in this short demo shows me
72 percent accurate on this side versus
77 percent accurate this side don't
assume that a neural network is always
going to be better it just happens to be
better for this configuration and this
set of data the confusion matrix down
here actually shows us with the dark
blue telling us where it got it right so
by the way this is unusual 78% is
extremely good prediction all right so
where it's was high it predicted high
78% of the time that's because this is
sampled data but let me just ask you
that if it came back and said in 30% of
the time I got it right is there any
value is this adding value beyond
otherwise a random guess and therefore a
random generation of targeted
advertising so 30% that would be good
I would agree anything above 25% is
random when you think of the four
classes if we had a four sided coin it
one such thing existed and you flipped
it well that's a random where is
anything above the random is what we
call lift that is the value that these
prediction models can deliver to
translated into the better experience
for your customers and hopefully
increased sales and profitability that's
where you start to justify these
projects a marginally increase above
guessing is going to translate to higher
sales well we're just talking in terms
of randomness at this stage a random
advert would get it right 25 percent
over the long term where is what we're
saying here is we've just proven that
with 30 percent of our data that we're
getting reliability is at 78 percent and
even 30 percent even 26 percent would be
better than random alright I've just
worked through the entire lifecycle then
of training and evaluating and I've made
the decision that the model that's been
trained by the neural network algorithm
is superior and I would like to deploy
this so we move on to the next part of
the demonstration which is to talk about
publication so as a cloud service the
way that we publish is via a web service
and so let me show you how this works in
demo the way that it works is you select
the train model that you want there
could be dozens in your experiment and
then we go ahead and we set up a
predictive web service watch closely as
it creates a new experiment that is a
predictive experiment Wow how cool is
that I think some developers in Redmond
have been working overtime but
personally I had a problem with what it
just did
what you saw was it sort of collapsed
those modules into the one that I
selected and what I don't like about
that is it leaves you with the distinct
impression that it has merged that
functionality to be honest it should
have just disappeared in the background
that decision forest model is irrelevant
to this predictive experiment it sort of
just disappeared and that's what should
have happened but let me describe to you
what has happened here at the dark blues
represent the inputs and outputs at a
web service remember that I'm looking
for a function there we pass customer
demographics in it we'll pass a
predicted profit label out what the
Blues represent a inputs and outputs
rather than call this input one I'm
going to call this customer profile
because that's what it is and this one
here is the output that I'm going to
rename to become predicted profit class
too easy what else has happened here so
you'll see that the CSV is still here
here we now have a trained model that
trained model you'll also find available
in your workspace for reuse across other
experiments all right we don't want to
retrain it every time we have a
prediction it's a trained model and what
this new experiment is doing is
performing a prediction by using the
trained model all right so what Elsa did
was any of the transformations I had
removing two of the columns replacing
missing values have been consolidated
into a reusable transform and that's
also available to us within the
workspace and so you can think of it
like a compiled transform that is going
to be much more efficient than an
interpreted one at prediction time now
this is when I started working with this
there was a lot a lot of documentation
but there's more work to be done than
just accepting this new experiment that
was created for me and the first thing
that I must do is come back to the
Select columns
remember how I eliminated two columns
earlier but I must eliminate one more
which is it yeah that profit generated
label should not be used in a predictive
experiment that's the output not the
input when we're training a model we
need it but when we're predicting from
it so simply by removing it now it
understands not to ask for it as the
input I can also break this connection
and say that the input should pass
either direct to the transform or in
this case I'm going to try and pass it
straight to the score
remember the transform is really
substituting missing values for total
children we know that we've fixed that
so during prediction stage I know that
it will not have to repair the number of
children being missing
so there's my input going straight to a
score that will use the trained model
and then the other thing I need to do is
to only output one column so I'll
introduce another select columns and
I'll basically say once you've scored go
ahead and remove all columns except so
let's say begin with no columns and only
include the scored labels very low low
medium or high and with that one column
you may output that as the output and
they're effectively is that function
that I was looking for passing customer
profit demography and then pass out a
single label very low low medium or high
we'll go ahead and run this and that
essentially validates that this
predictive experiment makes sense and
once it determines that it makes sense
we publish this and that essentially
becomes the web service
are there any questions while it's
processing this
so the question is yes a constant
reprocessing well yes you can so you
could have we could publish as a service
a training experiment whereby you could
just run this on a schedule every hour
ensuring that you know up-to-date data
is available so you'd probably want to
use a cloud source of data and yes you
could so you could use a service that
periodically retrains the model and you
could use another service that performs
predictions against it there was another
question on this side
so so what I'm hearing for you is yes
the prerequisites to really drive a
solution in machine learning requires
good background in statistics and
mathematics can I come back to that
point later I've got a slide that
addresses what skills you'll need and
you know perhaps how this technology can
be approachable to you as an audience
consisting largely of software
developers so I'll come back to that
point I'm just conscious one hour is
like really ambitious to get through
this demo so I want to then talk about
the last phase of the the project which
would be how would we manage this so the
next point would be we're going to
deploy a web service and that's easily
achieved down here go ahead and deploy
this as a web service and within a
matter of seconds we've got an endpoint
on the Internet we've got a key for
authentication there are in fact two
different approaches when using the
predictive service we have the request
response and the batch execution let me
just show you for the request response
you can do a simple test and here we can
see those demographic attributes that
must be entered and if I were to fill in
some values it would give me a
prediction back in the Asia portal the
batch execution service in contrast is
asynchronous so the first approach
requests responses like real-time just
shoot off get a response back integrated
the batch execution service is a
synchronous and it will allow you to
score typically you know I've got
hundreds of thousands of customers and
then output the scored results and a
synchronously I'll pick up those results
from blobstore later on all right from a
management point of view the azure ops
teams might come and manage the web
services for the workspace and so the
discussion here is really about
scalability and performance all right so
there's currently a single endpoint but
you could add in multiple to scale you
could monitor because essentially in a
production deployment you're being
charged in terms of compute time but
also the number of predictions so we
could monitor them here and then under
configure is where you can use the
elasticity of the cloud and say I need
200 compute units for this default
endpoint which Microsoft will guarantee
under service level agreement to deliver
up to 5,000 predictions per second
all right so it is designed to scale and
work at real time within applications
finally the topic that is of interest to
you is software developers and perhaps
your involvement in a project might be
simply integrating somebody else's
deployed web service so I've just
introduced that when a web service is
deployed there are available to you two
different methods the request response
for low latency highly scalable web
service versus the batch execution
service for high-volume yet asynchronous
the service can be called by any
programming language and any device you
simply need to satisfy four basic
requirements and that is that your app
needs internet connectivity it needs to
use SSL it needs the Oh data endpoint
and the API key for authentication and
then it needs the ability to work with
JSON where they use libraries or you
handcraft this through text manipulation
and so essentially it's open to any
application experience whether it's on a
mobile phone whether it's on a server so
let's now see through demonstration the
integration of this predictive service
in an asp.net web app all right
so I'm going to open up visual studio
and I have an asp.net web app so bear in
mind this is not my strength as an
asp.net developer
and let me just run it up front so you
can see what my test is for this of
course in a real world ecommerce
application you're not going to ask
people to input their details but this
helps us test you know what age are they
etcetera and then when I click Submit
it's going to call the predictive
service what it's going to display to us
then are some images so I have libraries
of images here and to keep it very
simple I just have eight for male and
female I have a very low medium and high
Edward and when I open up the web page
it's when I click Submit that it's going
to want to call the predicted service
all right so let's see the logic when we
click Submit it's going to validate that
there are no missing values for
simplicity I have a class called
customer profile and it then has
properties for all of the demographic
states so here it's simply assigning the
values from the controls into the
properties of my customer profile object
down here we get to the point that I
need some string that is the prediction
of whether it's a very low low medium or
high so there's some work to be done
here note that if the text error is
returned it's simply going if it's not
error excuse me it's simply going to
describe on the left hand or the right
hand pane of the webpage what the
predicted class was and then it's going
to choose the appropriate advert based
on the gender and the predicted class so
to integrate this when I deployed the
web service the responding page was this
here is the API key that you're going to
keep secure somewhere because it
authenticates with the service you're
going to make a decision about which
method and I'll use the request response
method and then here on this web pages
everything that you need is a software
developer to integrate this web service
including the endpoint including details
of what the request needs to look like
and essentially it's a JSON document as
the request body so take a look at this
there's one input named customer profile
it has these columns and your values
would need to be in here and note you
could submit more than one you could
come as separate and have multiple
submitted in the one request upon
success of processing that a response
will be returned and essentially it's
going to be a JSON document that looks
like this and we need to extract the
value from here so whether you use JSON
libraries to help you or somehow you
just extract from that location
there's the prediction for you now to
kickstart your development down here we
have sample code in c-sharp Python and R
and so I'm simply going to select the
c-sharp code copied to the clipboard
switch back to visual studio and add a
new
after the project
they do to a degree does it look like
I've lost Internet connectivity that's
going to be interesting all right so
control egg control V there is all of
that code now here's the problem it's
actually written as a console
application so it's not designed to be
integrated into asp.net also will notice
some references are missing here so
let's fix that the comments here tell me
that if I install this package tools and
you get a packet manager so this is
where it might fall down if I have no
internet an interesting challenge the
Ethernet connectivity in the room cut
just before I started to present to you
guys but we have success it's come back
on so collectively we cross our fingers
in the room
let me just reload a webpage and see
what happens
there's not a lot I can do at this point
yeah I'm on a different network and I
think what happened is it went to sleep
maybe because I'm not using it could you
what would you like me to do the the
problem is I'm working in a VM and it
doesn't like the underlying network to
change so is there something perhaps you
just look at your phone maybe it's just
failing this I might have a solution
that I can just open up and show you but
it's a little disappointing after all
the effort we went to so it looks like
we have lost it let me just see what
I've got is a backup so under solution
essentially what I had to do was get rid
of the main method as a console
application that makes sense but not in
an asp.net web app so I essentially just
transformed the main method into a
public static class here predict
customer proper class pass in a customer
profile object and return a string and
what the code gave me then and the only
change I needed to make then was to
actually pass my values to map into the
columns so to pass into the age column
use the age property of the profile use
the marital status to pass into the
marital status column next I had to
paste in the API key you'd probably have
an in config file somewhere and then you
have the base address that was already
in the sample code it knows the end
point for the web service but again that
should be in a config file next it
actually calls the web service at which
point in time we have a response object
and providing that that response has
returned a status 200 we can do
something if not it returns the text
error if it was successful what it needs
to do is get the JSON string that was
the document that was that response and
then somehow it needs to extract and let
me call this back to you it needed to
extract
this value from the document so I would
just copy that and paste it to create
some classes based on the JSON structure
and then programmatically I could
deserialize into the object and then
here is the path within the JSON
you've got a results object within it
the prediction within that the value
within it the values they translate to
the structure here and ultimately the
first column first row gives me the text
value and then I would output it and
when I return it I now have a method
which is that function passing
demographic States pass out prediction
and then I would come back to my default
dot aspx and I would say go ahead and
call that static method and then it
could render the image so just to finish
off with what it would look like and I
apologize for the lack of internet but
I'm prepared for this here is a
simulation of what could happen and it's
a shame because it really is real-time
but it's so slick and fast so they come
in here and say well the customer is a
middle-aged forty-five year old male
married highest possible income a couple
of kids number of kids at home not that
it matters because our model couldn't
use it highest education management owns
a home commutes five to ten kilometers
then it's going to be an advert like
this right to the mail that's going to
produce high profit we'll encourage you
with a Ferrari if they were professional
and owned learned less income less
education it's still read it has wheels
it's still a degree of sexiness but it's
going to appeal to someone that has
lower disposable income right and then
if we continue to work down a skilled
manual lower money
lower education and then what if they're
a partial high school low single manual
worker alright and that's really what
happens in targeted advertising you know
what's the best bang for this particular
customer do you need will change if it's
a female in retail analytics the biggest
discriminator is gender the patterns
between males and females are often very
different alright so apologies that that
demo didn't finish his plan but that's
essentially what would have happened and
I've got some links that will allow you
to watch a video of that in more detail
so it's want to wrap up with the
potential here and the message and
really in response to your question
earlier yes machine learning we often
think of data scientists as being the
primary role and yes that's true I mean
they develop the platform's the
algorithms and produce very advanced
solutions using these platforms yet
there's a message for you guys in sense
of applied machine learning so applied
machine learning is doing just what I
did then it's actually applying it I
didn't need to have a strong background
in statistics I needed the basics on how
to prepare data how to model it and how
to test the accuracy I didn't need to
have a masters or PhD in computer
science to do that so if you could do
the same and you trusted the outcomes
well then I believe that this is
approachable to you today as software
developers so just think about the
potential whether it's ad targeting
recommendations churn analysis to know
the customers that are likely to leave
you therefore what can we do to keep
them fraud detection and so on there's a
huge potential and I'll draw you back to
the galleries that will provide you a
kickstart with sample data sample
experiments and documentation
so in summary Azure machine learning as
a cloud-based service provides a portal
to engage with workspaces to a
collaborate to use a web driven studio
to access a range of cloud services and
also upload datasets you can then
publish and deploy your web services to
enable you to integrate predictive
analytics into a
full range of application experiences
alright so in summary Azure machine
learning is a cloud-based service fully
managed no need to install any software
integrated drag-drop connect
best-in-class algorithms used and
developed within Microsoft and some
industry standard algorithms as well so
Bing has been the major driving force
and that goodness is available to you
through a range and a growing range of
algorithms there's been some subjects on
our here this week you can use our
script and Python inside your
experiments also and I've just pointed
out you can deploy an operationalize
within minutes providing you have
internet connectivity key message to
your Ginoza software developers machine
learning is approachable to you guys
today a series of resources pricing is a
good one to look at it is quite
competitive and especially the free
pricing that allows you to experiment on
your own terms at no cost
not even a credit card is required to
sign up and work with it
here are a couple of links that will
allow you to watch the full video so I'm
sorry I didn't get to demo the final
stage of integration and lastly there is
a couple of books and a number of books
coming out these are good because they
talk to you from an entry level with no
assumption you have machine learning
background if you don't agree with
anything that I've said with machine
learning Paul the octopus has a
Wikipedia site here and you might engage
with him for your predictive
requirements beyond that we're out of
time that are welcome questions off mine
and otherwise thank you for your time
attendance and interest
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>