<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Coroutine Concurrency in Python 3 with asyncio - Robert Smallshire | Coder Coacher - Coaching Coders</title><meta content="Coroutine Concurrency in Python 3 with asyncio - Robert Smallshire - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Coroutine Concurrency in Python 3 with asyncio - Robert Smallshire</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c5wodlqGK-M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay can you guys hear me yeah good
right my name is Robert small Shire I
work for one part owned a small
consultancy based here in Oslo and in
Stavanger I'm part owner of sixty north
and we do work around the world in a
variety of technologies currently we're
doing working Puerto Rico we do a lot of
work in Oslo also work in London so we
get around quite a bit and although
we're not technology specific python is
a tool that we come back to again and
again and I'm not sure how many of you
here are familiar with the details of
Python a lot of this talk is very
accessible Python code is very readable
so even if you're not familiar with the
details of Python hopefully you can get
a good understanding from this talk of
what co-routines are and how they work
and how they facilitate concurrency not
just in Python 3 but in any language
which supports some more basic
primitives so although all the examples
I'm going to show you today are toy
examples for the purposes of
illustrating what co-routines are all
about and how concurrency works in
Python 3 I think it's important to point
out that a lot of what I've learned and
my exposure to co-routines and
asynchronous i/o in python is based on
industrial practice so I last year
worked on a system for a company in
London which does what's called demand
response management for the power grid
and where I was writing actually
embedded Python programs that run on
small Odroid single board computers
distributed on industrial sites to
control power flow to and from those
industrial sites based upon real-time
electricity pricing and UK more recently
I'm involved in a nascent startup here
in Oslo it's so new that it doesn't even
have a name yet where we're involved in
solar power prediction and monitoring
also using async i/o as one of the
technologies we use to build our
services
I'm also maintainer of a thing called
Pye cereal async aiyo you might think of
serial ports as being those all kind of
9 pin D connectors on the back of your
old computer going way back or even
further back C 25 pin B connectors and
you probably think that serial ports are
rarely used these days that's not the
case at all almost every device which
has sensors in it is going to have
serial ports onboard so serial ports are
a hugely important thing and we're
beginning to see use of async i/o
in Python to deal with serial input and
output so no one wants to help me make
this work on Windows and understands
Windows IO completion ports please get
in touch I'm also the author or
co-author I should say of three books so
for those of you who are maybe not
familiar at all or very familiar with
Python then please look into our books
and if you want a free copy of these
come and see me afterwards I'll see what
I can do so on to the structure of the
talk I'm going to talk about why we care
about asynchronous i/o why is this
something that's important to us why do
we need to solve this problem then I'm
going to move on to explaining what Co
routines are and we'll do that
independently of looking at input/output
we'll just try to explain Co routines
and in fact we're going to build
something in Python that is equivalent
to what the modern Python language gives
us but we're going to build it from the
ground up from first principles without
using async i/o we're just going to do
it from scratch as a learning exercise
then we'll move on and look at what the
language and library gives us in Python
and then only at the very end am I going
to get on to the actual IO aspect and
we'll plug all this together if we have
time at the end I'll finish with a quick
demo so what is asynchronous i io about
anyway well input output is high latency
compared to the certainly compared to
the speed of our
use these days and as such sequential
programs spend a lot of time waiting on
Io so you want to perform some IO and
external device a disk or a serial port
you spend a lot of time waiting for the
data to come back to you after you've
requested it so hit the - the
traditional response to this has been to
have multiple threads or multiple
processors more recently we've seen
things in no GS which use a similar
approach to what I'm going to talk about
today the problem with the threading and
multi processing approaches is there are
large resource overheads on linux every
thread is going to carry with it at
least eight megabytes of stack space
even if you don't use that and there's a
large cognitive overhead to synthesise
reasoning about shared mutable state
communicating between threads is
obviously where a lot of the complexity
comes into working with these kind of
systems and in python in particular we
have to remember that the entire Python
interpreter is a big pile of shared
mutable state that's what it is and that
shared mutable state in Python is
protected by the infamous now-infamous
of Gil or globe ring global interpreter
lock which really prevents Python making
use of the additional compute resources
that we give it okay but it turns out
that because programs from most of their
time hanging around waiting for IO take
the advantage of additional compute
resources isn't actually so important
for many applications what we want is
the IO to go quicker so I want to
establish a few definitions so many of
you this should be it's probably stuff
you know already but I just want to make
sure that the terminology I'm using is
are you during this talk is something we
can agree on
so concurrency versus parallelism
concurrency is about dealing with
multiple things at once without
necessarily doing the multiple things at
once so tasks which overlap in time
their start and end times overlap
although we're only progressing one of
those tasks at any given instant
and so co-routines which I'm going to
show today are very much about
concurrency on the other hand we have
parallelism parallelism where tasks are
actually running simultaneously and of
course to do this we need more than one
computer or more than one core with
multiple threads so I'm very much going
to be talking about concurrency today
another word that's going to come up is
asynchronous so essentially there's no
way no need to wait very results before
proceeding with other work and of course
when we can do this overall duration
becomes shorter which is a good thing
with less waiting on the latency of the
of the i/o versus sequential or
synchronous it's interesting that the
word synchronous which is obviously the
opposite of asynchronous and common day
usage synchronous means doing multiple
things at the same time but as computer
people have inverted that meaning to me
to mean doing exactly one thing at the
same time so it's kind of screwed about
linguistics a bit here but it
essentially means we must complete
something before proceeding of course
then the overall duration is longer
another way of thinking about
asynchronous and its opposite is
asynchronous versus non blocking so the
way we use it in what I'm going to show
today asynchronous period returns
immediately essentially with the promise
of some future and complete results
first it's non blocking which also
returns immediately but it may return
immediately with no results a partial
result or a complete results and
essentially we need to pull something in
order to figure out when the complete
result is finally available and then if
then we can think about pre-emptive
multitasking versus cooperative
multitasking so pre-emptive multitasking
is where the scheduler interrupts the
tasks that we're running concurrently
and the tasks have no say in when they
are interrupted which of course means
you are interrupt your tasks are
interrupted at the most inconvenient
moment which means that we have to
protect any shared mutable state with
locks so inconvenient contexts
and of course pre-emptive multitasking
requires some parallelism so even if you
have one core in your computer that is
hardware level parallelism going on
there are CPU counters Hardware counters
which are counting down ready to
generate an interrupt to force context
switch versus cooperative multitasking
where tasks have to deliberately yield
control back to the scheduler so this is
nice in many ways as a programmer
because then we get to choose when we're
interrupted okay so that can obviate the
need for locking on the other hand if we
forget to yield control to the scheduler
we can essentially hang the whole system
we can cause things to grind to a halt
because the scheduler has no means of
interrupting of pre-empting at a task so
with async i/o and co-routines we're
very much on the cooperative
multitasking side of the equation here
so the promise of async i/o and crew
routines in Python is that they allow us
to write asynchronous concurrent
cooperative tasks in a sequential style
which is great because it means the
reasoning about how these things are
working becomes much more
straightforward because we're
essentially thinking about synchronous
programs again which isn't you know what
we've been doing for years but at the
same time we get some of the performance
advantages of not having to wait on high
latency input/output
so let's set the scene if you like let's
move on to actually looking at how
co-routines facilitate concurrency and
in fact just what is the crew routine
let's try to answer that question as
well and in this part of the talk I'm
going to build something that is
functionally equivalent to a very
important part of pythons
async i/o but we're going to do it from
first principles from the ground up
without using async i/o at all and
essentially what i'm doing here is i'm
recapitulating the journey I went on
when I was first exposed to this stuff I
find it's very useful when I'm trying to
understand something to try and build
something simpler but which essentially
does the same thing and then by the time
I've solved that problem then I have a
really good understanding of what's
going on I think it's very important
when you're using a technology to be
able to have some appreciation of what's
going on one level of abstraction down
from the level at which you're writing
your program your application code
because when then when things stop
working or behave in odd ways you have
some insight into what's actually going
on and I find actually building simple
versions of things I want to use from
scratch is a very good way to do that
that's exactly what I'm going to go
through now before we get to that though
we need some code to actually do some
work we have a pact in our company that
we are not allowed to use the F word F
is Fibonacci and so in this example I'm
going to use a thing called the Lucas
sequence which is closely related to the
Fibonacci sequence but just has a
different initial starting condition and
so what you're seeing on the screen here
on the left is a Python generator
function and we know it's a generator
function because it has at least one
occurrence of the yield keyword within
it which you can see on the penultimate
line there in fact we have two
occurrences in this and Lucas just the
Lucas function just generates an
infinite sequence of numbers
on the right here I'm importing that
function I mean also importing a thing
called I slice which is like cake just
takes a certain number of items from a
sequence and produce it and converting
those into a list so I'm taking the
first ten Lucas numbers there we are
okay now
there's no concurrency or anything going
on here I just need some code that's
going to do some work that's the only
reason we have this here today the next
function I'm going to introduce is
search and search is also a very simple
Python function even if you don't know
Python I hope you can read this so
search except some if label series of
data and add another function a
predicate function and we test that
predicate against each item in the
series and if the predicate returns true
we return the matching item if we get to
the end of the for loop without having
found a matching item we raise an
exception called value error right this
is a very very straightforward Python
function again no concurrency here yet
so search is just a regular function and
here you can see on the right I'm asking
to the first Lucas number that has at
least six digits now I'm just converting
the Lucas number to a string checking
how long the string is and we can see
that the first lucas number with at
least six digits is 103 682 very very
simple python code and we can all
understand this
what we'd like to do is turn our search
function which is completely non
cooperative into a cooperative search so
that it can yield control to some other
piece of the program and all we need to
do to make that cooperative is to insert
a yield keyword so we're just going to
fit yield here inside the loop so that
we yield once through the loop and
notice that we're not yielding anything
this is a shorthand in Python for yield
non so non is like a null or nil object
so we're just we're not yielding
anything we're just yielding which will
interrupt the flow of this loop and to
remind us that this is now a generator
function and not a regular function I'm
going to rename it to async search so
let's see how to use async search I'm
going to import it and the Lucas
generator and I'm going to search again
for the first Lucas number understand
we're going to search for the first one
that is greater than 10 okay so not with
10 digits but actually just greater than
10 so what is G 1 we call G we get not
the results we get a thing called a
generator object okay so the generator
object encapsulate this the running
state of this function okay and the
generators in Python are also iterators
and so to it to to advance an iterator
in Python we pass the iterator to the
next function so we do next key every
time we call next year we're running one
more iteration of that loop until we get
to the yield again and we stop we pause
the suspend execution of the function
okay so we run one iteration now and we
can run another iteration and you'll see
that next isn't returning anything or
that's because yield isn't yielding
anything right so let's do another one
and so we can advance this search one
iteration at a time now it's not
immediately obvious when I'm doing this
but I can actually do more than one
thing that concurrently here while we're
our search function is run
I can start doing other steps right I
can print hello world so now we have
concurrency right using nothing more
than iterators
okay now I'm printing hello world and
running the search and we can keep
advancing the search with more calls to
next and then eventually next we'll
raise a stop iteration exception and
stop iteration is how python programs
signal that an iterator has reached the
end of the data raises an exception
called stop iteration what's interesting
about python is that that exception
carries a payload and here the payload
is the item that we found so that return
statement in async search actually
causes an exception to be raised by the
generator machinery in python and the
result gets wrapped up in the stop
iteration exception we see at the bottom
now on the right stop iteration 11
that's the result okay so we can use
generators generates functions in Python
to support concurrency and all our
co-routine is essentially is an
interruptible function like this as a
function where we can pause execution
and then resume execution at the point
we left off at some later time
and of course that's exactly what we're
doing here when we call next yeah so I
think that just says what I've just said
to you so let's make this a little bit
more sophisticated and a bit nicer to
use it's not very nice having to call
next and have this thing look like an
iterator we'd like it to look a bit more
like a task because conceptually that's
all we're building and thinking in terms
of iterators when conceptually we're
building tasks there's a bit of a mental
mismatch there so let's make let's make
a task all the task is is a generator
function called routine here and an
integer ID to identify that routine so
those of you don't know pythons dunder
init you can think of it as a
constructor the constructor accepts
routine and within the constructor I
just take the net
I D so next ID here is essentially a
static class attributes here so the task
is a very simple thing is an integer and
a routine occur routine so given task
which is a very straightforward thing we
can now build the scheduler for running
tasks right and I'm going to show you
the entire code for the scheduler it can
be a very very straightforward simple
thing so you see the constructor dunder
init there for the scheduler has a deck
of double ended queue of winnable tasks
and we have two dictionaries the things
of the curly braces which contain any
completed results on completed tasks or
any errors from tasks which have failed
while they're executing the next message
there ad allows us to add or schedule a
routine to be run by the scheduler and
you can see it as we just give it a
routine it wraps it in a task and it
appends it to the queue and then returns
the task ID for the new task that's just
the return an integer and then we have
this run to completion I can get down to
that so this function is the entirety of
the Co routine scheduling program so you
can see it's essentially a while loop
and it says while we have some runnable
tasks pop the next winnable task off the
queue call it task print the friendly
message and then try to pass the task
routine to next ok and capture whatever
result comes out of that into yielded
now when we call next several things can
happen first thing that can happen is it
can raise stop iteration which means the
task has completed so if it raises stop
iteration you can see that we extract
the result of that task from stop to
value and put it into the completed task
results dictionary against that tasks
d okay the other thing that can happen
is it can raise some other exception I
mean obviously it's a task it could be
doing anything anything could go wrong
if it's some other exception other than
stop iteration we capture that as e here
and put that into the dictionary of
failed task errors the else block here
that's a kind of strange Python feature
try else blocks are executed if there is
no exception
so basically if it says yes next
advanced without any exception being
raised we check that we get nothing back
with your search statement there and we
take the task that we've just taken
popped off the queue and executed and
pop it back onto the other end of the
queue so you can go around again
okay so we're just taking tasks off one
end of the queue giving them one chance
to run and putting them back on the
other end of the queue it's extremely
simple thing as you can see it's just a
while loop and this will keep running
while there are tasks in the queue so
let's see what this looks like when we
execute it we import the scheduler and
Stan she ate the scheduler and then I'm
going to add my async search tasks to
the scheduler you can see there we're
looking for the first Lucas number with
more than six digits and then I can run
to completion and you can see because I
might cure only has one task in it it's
just getting popped and pushed
repeatedly as we go around you can see
it's yielding every time eventually that
task will complete and there are no more
tasks left and we can pop the results
out of the results dictionary there and
get the answer great we've basically
written a while loop that calls next
repeatedly and so as long as it can and
it's not very exciting with one task but
this is completely sufficient for
running of many tasks as you like so
let's add two tasks here so here's find
the first Lucas number with more than
with at least seven digits and the first
Lucas number with at least nine digits
both of those tasks are added when they
went to completion and you can see it's
alternately running task 1 task to task
want us to
one of those will complete earlier
because it's a shorter problem to solve
then task 2 will continue running until
we get to the completed result it's a
very very simple straightforward thing
and we have concurrency right there's no
parallelism parallelism here but there's
there's definitely concurrency happening
we are dealing with more than one thing
at the same time yeah and of course we
can get the results out okay so
searching for Lucas numbers is all well
and good but it's the next part of the
demonstration we need to do a little
computational work something that will
take a bit more time so we're going to
search for prime numbers so here's my
primarily prime allottee testing
function is prime it's extremely naive I
wouldn't use this in production for
finding prime numbers suits my purposes
very well today it's very inefficient
and so it takes a long time and that's
exactly what I want for the
demonstration so you know finding
determining whether 12 or 13 is prime is
obviously very quick but if some of
those other calls they're like 2 to the
61 minus 1 you know that takes half an
hour to figure out whether that is prime
because Python for its wondrousness is
not a very fast language
I'm also going to introduce another
function now very similar to the search
function but rather than it another
co-routine in fact rather than it
returning the first match in a sequence
it's going to print every match it's
just going to keep iterating through the
'trouble and every item that matches
we're just going to print it out okay
and we can see it's a cool routine
because we have that yield in there so
let's use our scheduler and add the
async print matches on the Lucas numbers
which are prime ok so you can see now
that we're running and now and again the
tasks will print out a message which
gets interleaved with the kind of
logging messages which are coming out of
the scheduler itself okay so nothing to
expected unexpected there but you can
see the gaps between the prime Lucas
numbers are increasing which is a useful
property for the demo
now alongside us finding the prime Lucas
numbers I would like to do something
else completely unrelated I would just
like to print a message at fixed time
intervals okay so I have this other Co
routine here repetitive message I can
give it a message string and an interval
in seconds and every two seconds say
it's just going to print that message so
you can see we have a couple of loops
here we have the infinite while loop
which just runs forever
and the inner while loop which is
essentially a sleep routine in there
very simple function we need to make
this into a cool routine so we can run
it in our scheduler so we insert the
yield at the bottom there and as I've
done with my other functions we rename
it to async repetitive message just to
remind us that this is a KO routine
rather than just a regular Python
function there's a subtle bug here and
these things are subtle enough that they
they're quite easy to miss and you need
to you do need to sit down and think
about your code quite carefully and the
bug is that if interval seconds is
really short the KO routine will never
yield alright so you need to be very
careful about where you put yield
statements in your code through easy to
fix we just need to change the ordering
of things it doesn't really matter when
we yield in this routine in terms of how
it behaves other than the fact that with
small numbers it might never be
interrupted or will never yield control
back to the scheduler so it's important
to make sure that your co-routines
either complete is essentially
instantaneously they can just return the
result right now or they yield at least
once okay very important property of
these things otherwise you've built
something that can stall your whole
system
so let's plug these things into our a
little scheduler here so I'm going to
import my repetitive message co-routine
my print match is cout routine my Lucas
numbers generator and my priority tester
instantiate the scheduler well schedule
early with the repetitive message I
spent far too long in airports so this
is kind of imprinted in my my mind and
we'll do the Lucas numbers search and
then we'll rent a completion my clicker
works
okay stop it's great running it's
printed the message unattended baggage
going off you can see it's running task
zero and task one alternately but it
seems to have stalled right in spite of
the fact that these are co-routines
something's are stalled here and the
problem is is that even though async
print matches is non-blocking and we've
designed it that way is prime function
for small prime numbers it returns the
result effectively instantaneously but
the large prime numbers it takes a long
time to determine whether that number is
prime in our horribly inefficient Python
code so these prime function is now
essentially blocking progress of the
system and you can see it's getting hung
up on tasks zero and because it's
getting hung up on task zero task 1
which is supposed to be printing a
message every couple of seconds is not
getting an opportunity to run so the
guideline here which is that essentially
everything you call transitively
pharmaco routine needs to be non
blocking that doesn't necessarily NEET
mean it needs to be another Co routine
it could be non blocking in other ways
by using threading for example but it
mustn't block okay so essentially Co
routines and these kind of programs are
contagious to Cawley's if you are a Co
routine the things you call should
probably also be Co routines or at least
non blocking so we have a print matches
function print matches Co routine
calling a regular function we need to
make that regular function itself into a
cou routine so we take out is prime
function and we know how to make these
into pro routines now we just stick a
yield in there so we put a yield in the
in the loop there now a think is playing
as a generator it will return a
generator object on we call it because
it returns a generator object we need to
call it differently and the way in
Python 3 we yield from kind of nested
inner generators is by using yield from
the saves of having to write another for
loop in the calling function
to iterate the Cawley generator when we
do this it turns out that this bear with
bear yield which is a kind of term I've
invented here is no longer needed it's
no longer needed because the function
we're calling on the right has the bear
yield in it as well and that seems kind
of irrelevant at this point if we left
it in there async quick matches would
yield to the scheduler twice per
iteration rather than once that may or
may not be a problem depending on what
you're doing but the point is it's
unnecessary and in fact eliminating that
bear yield there is an important part of
a refactoring that's coming up quite
soon factors part of the refactoring
that's already started you just haven't
realized yet so the second rule is that
everything that calls transitively to
occur routine must iterate the generator
because the KO routines are packaged as
generator objects so co-routines are
also contagious to callers not just
colleagues so you can quickly see how
these two rules taken together cause
your whole program to be need to be
written in a particular style right so
if you start out with a sequential
synchronous program converting it to
this kind of programming style is not
necessarily straightforward because of
this property that it's contagious down
the call stack and contagious up the
call stack so this is one of the I would
say this is an architectural a
significant decision because changing
your mind about how you structure your
program is going to be quite costly
later on so it's something one of the
things you want to make your mind up
about earlier on in software development
rather than later
right so we're going to do a little
refactoring now so going back to our
async repetitive message we have the two
loops here and if you think about it the
inner loop here with the prelude to it
is really a sleep function that's what
you're doing sleep so we're going to do
an extract co-routine refactoring and
we're going to pull out a separate sleep
Co routine here okay so now repetitive
message now all it does is print the
message yields from the sleep KO routine
and the the inner loop is now moved out
into that overcoat routine so this is
also a very significant refactoring as
you'll see shortly async sleep has some
interesting properties it always yields
at least once async sleep Xero yields
exactly once and because async 0:18
sleep Xero yields exactly once any
occurrence of the bare yield anywhere
else in our program can be replaced by
yield form async sleep zero now then
that might not seem like any kind of win
at all because we've replaced one word
with three and a method call but it is
an important refactoring and it's one of
those interesting cases we're on the way
to simplify and code you have to write
more code and the code kind of blooms up
before kind of magically shrinking down
as all these things cancel out so that's
an important step there so then we can
take all of our other yield at bear
yields and replace them with yield form
async sleep zero and when we do that
it's important because now we have only
one bear yield in the entire program and
that is buried deep inside async sleep
right that's an important quality
so we have plenty of yield Franz but
only one yield so our other co-routines
are now relying on async sleep and of
course we know it's a refactoring in
spite of my total absence of tests here
because it behaves exactly the same when
we run it we schedule our repetitive
message just not refactoring from what
from the last demo video I showed you
because we're now fix the there is prime
function to be interrupts abour so now
you can see we're finding the prime
lucas numbers and we're printing the
messages and both of those tasks are
nicely interleave because all of our
functions are co-routines
and therefore they are non blocking now
in the 90 minute version of this talk I
make you wait for the next prime number
but we only have 60 minutes today so
we'll keep going okay so at this point I
have built something that is
functionally equivalent some very
important pieces of Python threes async
i/o without using any facing code
it's just generates functions and a
super simple scheduler that literally
fits on one slide well what I'm going to
do in the next part of the talk part 3
of the talk is take that code I've just
written and transform it actually
mechanically transform it into
legitimate Python 3 async i/o code and I
think it is important because it
demonstrates that there is no magic in
async i/o right you've seen you've just
seen all the magic could you know it's
like all of these things I'm sure open
the box understand what's inside it's
really really very simple so on this
slide I'm showing you I think all of the
code except for the scheduler which I've
created up to this point all the
functions there and I'm going to
mechanically refactor this
so the first step is every time I've
death a sink foo we are now going to
write a sink desk through okay so we're
using some key words that came into
Python 3.5 here so you can see all micro
routines and our async death the next
thing I'm going to do is just import a
sink i/o the reason I imported a sink
i/o is so that I can call a sink IOT's
sleep function rather than my sleep
function so I'm going to replace all the
calls to a sink underscore sleep with a
sink i/o dot sleep so that's that now
I've done that I'm not using my sleep
function so we could we can get out of
that and have less less code which is
great finally I'm going to replace all
occurrences of yields from with a wage
which is another new keyword in Python 3
point signs so now we have this okay
and this is a completely mechanical
refactoring obviously I did this by hand
I mean you could imagine writing a
program to do this it's very very
straightforward so let's see what
behavior we get now the indications here
a little bit different because now I'm
using some async i/o stuff so I I get I
get the async i/o event loop I no longer
need my own scheduler here and I now
called create task you can see it
returns a task object that's an async
i/o task object not one of my task
objects but it's essentially exactly the
same idea so now we've skipped their
scheduler I can run forever and you see
we get exactly the same behavior that we
just had except now we're using pythons
event loop and pythons tasks and pythons
futures and all these other things
rather than the very simple ones that I
showed you previously turns out that the
Python ones aren't actually very much
more complicated conceptually than what
I've showed you earlier
so let's not wait for the big prime
numbers so having understood the
concepts of how async IO and a sink and
a weight working Python it turns out
that async and await in Python are
implemented essentially using generator
functions we can move on a little
further and see what else a sink IO
library gives us for working with these
kind of objects so async IO in async
code co-routines a used to implement
tasks co-routines a wait other
co-routines
we have an event loop which schedules
concurrent tasks tasks must not block a
waiting facilitates context switches
between the tasks every time your wait
that's an opportunity to that's when you
yield control back to the event loop and
it may choose to continue running your
task or some other task but you can be
sure that eventually control will come
back to you and in async IO you're not
allowed to use the bare yield if you
just want to yield control to the
scheduler the way you do that is by a
waiting facing higher loop to 0 which as
I've shown is equivalent to that bare
yield just a note on terminology I feel
I find people aren't very precise when
they're writing about these things and
documentation which I can be very
confusing sometimes so the thing on the
left is a KO routine it's a bunch of
code and it's callable we call it the
thing on the right is a co routine
object which is the code plus its
execution stage it's executing code and
those things are awaited which is a new
concept in Python 3.5 so if you are
writing documentation about these things
please try to be precise because then I
can understand your documentation
so anything kayo gives us some other
tools like futures so a future
encapsulate the idea of a potential
results or potential error so here I
have a front row routine they're
monitored search which as you can see
just delegates with the weight to the
search I wrote earlier except that it it
also accepts a future object and the
future is descent essentially the future
we can either set it with the results to
say yes there is the potential result
has been realized or we can call set
exception and say an error occurred so
it's a signaling mechanism that we can
use to communicate between tasks I also
have another co-routine there monitor
future which will accept the same future
object and every interval seconds would
print out the state of that future so if
the future is not done so while future
not done print waiting okay and you'll
see at the the main program down at the
bottom there I have that call to create
future when I pass the same future
instance into monitored search and
monitor future so we're going to run
these two tasks simultaneously but
they're both using the same future
instance to essentially communicate lots
of notice when I run this on the right
there we get an error form async i/o the
event loop saying hang on when this
program finished there is a task destroy
I had to destroy a task but it was still
running it hasn't completed cleanly
we'll come back and fix that error later
when you're creating futures in async oh
you should have there's a very tempting
constructor which is the future
constructor you should really avoid
calling it the problem with calling the
constructor is that event loop
implementations and there are several
our loud to specialize the future
implementation if you call that
constricted directly you bypass that
specialization so you should always call
a factory function instead or
the event loop which Hall gives the
event loop the opportunity to give you a
different future so for example the way
futures the way the event loop is
implemented on Windows is completely
different from how its implemented on
UNIX so the implementations those
futures can then differ when we call
this factory function turns out in what
I view is a particularly dubious use of
inheritance which is not the first time
this has happened in Python land is that
a task is a future in async i/o so we
don't necessarily have to create futures
in order to monitor their progress we
can actually just directly monitor a
task so I've changed my program on the
right here so that it does no longer
creates the future we no longer need to
do that and where we monitor the future
down at the bottom I have loop create
tasks and monitor future we actually
pass the search tasking directly to
monitor it and we're allowed to do that
because the task is a future the reason
I say that this inheritance is dubious
is that calling things like set
exception on a task is something you
should never really need to do and it's
a pretty questionable API design
yes oh there we are tacky the future so
we can directly monitor the task rather
than the future when it comes to
creating few tasks again there's a very
tempting task constructor and for the
same reasons as futures you shouldn't
ever use this even though it's there in
the API because it's the same reasons it
prevents the event loops specializing
the implementation you should prefer to
call the create task factory function
even better and in a particularly
confusing way the correct way to create
a task in async IO is to call a thing
called ensure future right this is
obscure but it is the right way to do it
it's more general in that it accepts a
weight of balls not just co-routines and
it's also idempotent so you can it will
avoid wrapping the same task up in more
and more tasks if you happen to call it
more than once
so yeah in spite of its confusing name
it does actually return a task so I if
you get the sense that some of this just
kind of released to the world
prematurely but now we can never go back
and fix the API I would agree with that
so we are using ensure future to create
our tasks that's the right way to do it
I definitely need a new battery and my
clicker unexpended so I pointed out
earlier this bug if you like in my
program that one of our tasks is not
being correctly wound up that's because
our search task completes and if you
look at the bottom on my main program
there I'm doing run until complete
search tasks so as soon as the search
tasks we've done completely
exit but the monitor task is still
running so we would like to wind up the
monitor tasks gracefully rather than
just killing it so to do that we want to
wait oops to do that we want to wait for
more than one task and the way we do
that in async i/o is using this gather
function you can see at the bottom there
so gather allows us to combine multiple
futures into one essentially it says
give me one future that represents the
completion of all these other futures so
I've created a task I've created a
future called search in one of the
future by gathering the search tasks on
the monitor tasks and now when I do that
we get the graceful cleanup gather
accepts any number of away table objects
which includes futures tasks co-routines
okay we haven't talked yet a bit aiyoh
so if you look at the event loop api
it's huge it has all this stuff in it
starting and stopping the event loop
scheduling callbacks Factory as I've
told you you shouldn't use configuration
stuff you really need that's not even
the half of it
there's exception handling the
Diagnostics the signal handling we can
run blocking code in other threads or
processes there's low level socket
operations in there this is all one
class it's incredible well even more
there's protocol based SSL TCP socket
servers we can watch file descriptors we
can connect to pipes
this is a very substantial public API to
the abstract event loop the question is
what's all this i/o stuff doing in the
event loop so finally we get on to IO so
we've got this far talking about kuroh
teams without really talking about why
we care well the reason all this i/o
stuff is in the event loop is to do with
when tasks are run so the scheduler I
built earlier is super simple
round-robin scheduler it just clocks
around the tasks for an engaging turn
the tasks are in a very simple circular
order if instead we had an i/o aware
scheduler that could use a pole or
select on UNIX or IO completion ports on
Windows we can be a bit smarter about
when we schedule the tasks and we can
distinguish between tasks which are
ready to run or are waiting on i/o so
essentially that's why we have all this
i/o related stuff in the event loop API
it's because we have an eye
go away event loop which can be a bit
smarter about when it rink runs things
so it will skip a task which is waiting
on IO but then when the data becomes
available we can immediately go back and
run that before proceeding so it enables
us to be smart about scheduling which
enables us to reduce the latency okay so
the async i/o library in Python has this
pretty substantial Tower of abstractions
layered upon top of each other we've
seen at the bottom that we can build
resumable and suspend suspend herbal
functions co-routines using generators
on top of that we have tasks which in
conjunction with the i/o aware event
loop can await on sockets and file
descriptors and things on top of that
async i/o gives us transports and
channels which are a higher level
abstraction when I'm dealing with a
transport I don't need to care about
whether I'm dealing with a file or a
socket or a pipe it's kind of one level
of abstraction up from all of that on
top of that it layers a thing called
protocols which deals with the lifecycle
of connections so connected but a
connection created we're working with a
connection connection closed and on top
of that we have another co-routine based
api which is a much higher level use of
co-routines so we have 10 minutes left
let's see if we can quickly get through
some of these I'm going to show you the
protocol based api quickly without a
demo and then I'll show a stream based
API with a demo so this is a simple chat
room which is going to consist of two
classes this chatroom class and a
protocol class and you can see that the
chatroom is very simple as a name as a
port which we listen on an event loop
and some mapping of user names to
transport these async i/o transport
objects when we run the server with this
create server call to the event loop we
give it essentially a callback and it
will call this it will instantiate a
protocol instance for each new
connection to the chatroom and you'll
see this protocol class in just a moment
and then there are some functions in
here to deal with new connections which
worry register users and do registering
users when they leave the chatroom so
all this is on github so you can look at
it later in detail but it's a very
simple thing what's important here is
the protocol which I said protocols are
callback based API so here's the
protocol one of these is instantiated
for each new connection to the chatroom
you can see as a reference back to the
chatroom
and it corresponds to a particular
username and it has it will own a
transport which is the actual channel
that we can use to communicate with the
user here are the three functions that
will be called and the state machine
here so they think IO will call on your
protocol it will call connection made
exactly once they will call data
received as many times as data arrives
and it will call connection once lost
exactly once
when you finish the problem with working
with their this protocol based API an
async i/o is you have to do a lot of
work particularly dealing with text data
in doing things like finding ends of
lines and positioning a chunk of bytes
that arrives at you over a socket
basically this up to you to find the
frames of data within that so it's
relatively low-level I'm not going to go
through all this now I guess what's
important here as you saw on that note
that's just flown by is that because
only one of these proteins is running at
any given instant we don't have to look
any of the shared mutable state right
because we know that nobody else is
modifying that at any given instant so
we have shared mutable state between
tasks but we don't need to lock
okay so normally I would give a demo
here I'm a little short of time today
I'm not going to give you a callback
based demo but I'll show you the
stream-based demo I don't need to do
both demos because the user experience
of the demos is identical so that was
the callback based protocol Stefan async
i/o let's look at a co-routine based one
and the coyote and stuff is much nicer
to work with for this reason is that we
avoid these horrible we can avoid some
really horrible nested callbacks and
this kind of this pyramid of doom that
you get with this kind of nesting here
where we're awaiting tasks on callbacks
a wait allows us to transform that kind
of horrible code on the left into
something exactly equivalent and form on
the right so I hope you'll agree it's
much easier to read and reason about
what's going on and easier to get right
so what I just showed you the protocols
are those three callbacks connection
made lost and receive was the protocol
level API let's look at the stream based
API this is exactly the same program
chatroom and do it in one class now
what's important here is that start that
the run method here start server does
invoke a callback client connected and
that's a KO routine and that will be
that KO routine will be called once per
user connected to the chatroom let's
just look at that client connected
routine what's interesting is this
function can take a very long time to
run if you connect to the chatroom for a
week it takes a week to execute this
function write this function with
co-routine client active is the entire
life cycle of a particular users
connection to the chatroom okay so you
can see when they arrive we say welcome
then we just await user registration
we're going to ask them to register and
we just sit there and wait until they
register and we because we're using a
wait other connections have
opportunities to come and go
then when they're registered we tell
everybody else that your users arrived
and then we await user activity so other
people can be using the chatroom
and then eventually the connection will
be closed and they will leave that might
be a week later right might be a month
later okay then we do register the user
and then we're just some housekeeping at
the end we just drain our writer so that
we're living pending in outgoing vessel
and they leave okay so it's very
interesting that you can have these cool
routines which can take a very very long
time to execute the idea of you know we
have these little to fast computers
these days and you know the idea of
having a function that takes a week to
run which does almost nothing is it's
kind of fascinating so I will
demonstrate I think we have a demo you
know the rest of this is just
housekeeping I will use my last four
minutes to give you a demo and you see
the entire chatroom is a very simple
thing okay
so here I have my I've got the callback
based chatroom and the streams based
chatroom I'm going to run the streams
based demo for you here it here's just
the the class I just showed you
here's the client connected co-routine
that might run for a week I'm just going
to run this here servers now running
will just switch over to a couple of
terminals here and it's just just using
telnet lights nobody uses telnet these
days so I like to keep it in use running
on port 1 2 3 4 when we do this that
co-routine has started running and
remember pretty much the first thing it
did he says welcome to the chatroom
here's the message and asked me for my
name it's now a waiting user
registration so Who am I I am she
learned today who she has arrived
then we
telling us again over here and try mine
now and Jim and you can see that the
message has been sent to the other chat
room on reason say hi Sheila
so now both of these curry teams are
running simultaneously and that
alternately doing a weight user activity
so every time we try to type something
into one of these it was a Jim we do the
work of that co-routine eventually I
close the connection we do not tell Matt
like this when I quit telnet here you'll
see that that co-routine ended so one of
those co-routines is now finished
running the other ones still going and
you see picked up the message for Jim
departed so it's
Wow that's the Demerol very
straightforward I think I'm going to
stop there because we have like a minute
left I will remind you to come up and
get a free copy of my books and thank
you very much we have a minute or two
for questions if you're quick thank you
I am aware of see char async/await
and I have never used in anger and I've
done quite a lot c-sharp programming
what I'm not sure well relating with
async/await in c-sharp is what the
underlying mechanism is whether it works
this way or a different way
Oh
absolutely so I mean I mentioned at the
beginning that I am the maintainer of Pi
serial async i/o I recently started
using a thing called a i/o HTTP which is
the most horrible name to say but it's a
really nice HTTP server using async i/o
and very good performance and very easy
to use
on Tuesday IRAs using the async io
package for the casket API
okay so there's Det the community is
definitely stepping up and producing a
scene kyoto packages for basically
anything that deals with eighths and
kayo
so it's taking time but certainly over
the last year it's begun picking up
momentum any more questions
no okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>