<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Squeezing the Hardware to Make Performance Juice - Sasha Goldshtein | Coder Coacher - Coaching Coders</title><meta content="Squeezing the Hardware to Make Performance Juice - Sasha Goldshtein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Squeezing the Hardware to Make Performance Juice - Sasha Goldshtein</b></h2><h5 class="post__date">2017-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XBC0vDO4yCk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well thank you for coming everyone
welcome I know there is a tough
competition out there so really i
apprec--
you're coming I hope you're enjoying NBC
so far my name is Sasha and we'll be
talking about hardware performance how
to squeeze the most out of your CPU by
performing some rather low level
optimizations this is a talk for dotnet
developers generally most of my examples
are going to be in c-sharp and using
some libraries which have only recently
become available to dotnet developers
however the concepts described here are
relevant for everyone building software
and Intel CPUs
a lot of the examples are very general
and you can apply them in other
languages as well in fact the lower
level the language the better for
example in C or C++ it would be a lot
easier to apply a lot of these ideas but
again this is dotnet it's going to be
the examples are going to be using
dotnet so roughly what we're going to do
is talk about how modern CPUs are built
I'm going to spend a few minutes of this
because it's really hard to understand
what the optimizations are about if you
don't really know exactly how the CPU is
constructed internally with the
different pieces in the pipeline are
which will be taken advantage of we'll
see some basic examples based on you
know internet questions on these
different aspects of CPU architecture
and then we'll move to the actual
techniques applying what we sort of
learned about the CPU to optimizing
specific algorithms hopefully all the
examples are going to be fairly
real-world I'm not saying you know you
can take this code and apply it today in
each of your applications but it's not
complete pure theory which you never
will apply its actual algorithms used in
a lot of computational frameworks a lot
of libraries a lot of applications today
and towards the end this will maybe
touch a little on where I see the future
of optimizing this kind of low-level CPU
work on other platforms so this is a
modern processor inside there is two
processors here on the left is a pretty
standard desktop issue processor that a
lot of you probably have or had in the
past
it's from the AI series the Intel i5 i7
series it's not a very expensive
processor probably
it goes for 200 300 400 bucks and it's
really easy to see there's a some
structure here some well delineated
pieces so for example this processor has
four cores at each core has a little
piece of cash and there's also this
whole unit on the side which contains
the graphics and a bunch of other things
that are today built into processors and
there's a large piece here which is the
shared cache so a lot of parts clearly
visible and then on the right there is a
somewhat bigger chip a chip that's also
a lot more expensive this is impulse
Knights London which was released a few
months ago it's a it goes for a few
thousand dollars but it has up to 72
cores and this is this is today this is
a few months ago it's not like these you
know super computers that you heard
about in the movies which would cost
millions of dollars to build this is a
72 core chip which can run your OS in
the past Intel would make these chips
which couldn't run an operating system
you would have to attach them to an
existing computer that would already run
some other OS but as of knights landing
you can actually run your Windows or
Linux on this chip directly and use it
for computation it has a bunch of cache
it has a bunch of embedded fast memory
and it can attach to additional external
memory up to terabytes and terabytes of
additional RAM for this monster unit to
use 72 cores for a few thousand dollars
is a pretty good bargain I'd say and
people are starting to use these for
financial work for image processing for
video processing for a bunch of CPU
intensive tasks and then the challenge
is really to squeeze the most out of
this thing you paid a lot of money to
get and squeezing squeezing the maximum
out of this thing applies on each core
individually so you want the maximum
performance of each of these seventh two
cores but you also want parallelization
to work well you want these cores to
work together and distribute the load as
equally as you can between them so there
is a bunch of toxic could do on each of
these aspects today we're going to focus
more on extracting the most out of each
individual core
fertilization is a whole different topic
and hopefully you've heard some talks
about using parallel code in dotnet and
other languages in the past but getting
the most out of a single CPU is a bit
more challenging so a processor
organization just briefly I want to
touch on what happens inside Intel
processors for quite a while now
this general structure hasn't changed
since Nehalem which was released like
eight or nine years ago this is the
first generation Intel I series
processor so roughly there's two parts
here everything here on the left is the
front end of the processor and then this
thing here is the back end of the
processor and essentially the back end
is where instructions actually get
executed this is where you add numbers
and you multiply them and you write
stuff out to registers on to memory and
the front end is where roughly
instructions are being decoded processed
and rearranged so I'm not going to touch
on every single aspect of this just a
few highlights before the processor can
actually execute any instructions it has
to read them from memory so it all
starts with the instruction cache where
the most recently accessed instructions
are stored so if you have a loop for
example you would benefit from the fact
you already read these instructions from
memory so you can just reuse what you're
already read once you feed them from
memory but to read them from memory you
have to decode them you actually have to
convert them as well to an internal
format that the CPU uses called micro
ops view ops this is totally different
from what you see in Visual Studio when
you open the disassembly window the
instructions you see there are the
official public instruction set however
internally the CPU uses a slightly
different representation where each
assembly instruction is broken down into
much smaller pieces so for example an
instruction that reads the value from
memory and adds another number to it and
storage the result to some other
location might be broken into three
parts recent memory add a number and
then write the result somewhere and
these micro op are then passed forward
to the back end which actually executes
them and in the back end we have
multiple execution units or
execution ports which are capable of
doing things at once concurrently in
parallel on a single core right we're
talking about just one individual core
here so this is not multi-core this is
on a single core you have multiple ports
capable of doing things at once for
example and I'll show you some more
details soon you have a port for adding
numbers and you have a different port
for storing values into memory so if you
have two operations to perform if you
have to add numbers and store results to
memory and you can run them at the same
time they can actually execute
concurrently inside the processor so you
can get some more concurrency out of the
CPU on a single core by arranging stuff
such that all of these execution units
are busy at once so here's some of the
more important execution units currently
we have a total of seven ports some of
them are not really interesting they
just are reserved for reading from
memory or writing into memory or doing
calculations or addresses the ones that
have stuff inside are the ones that
perform actual computation so for
example we have port 0 which is
responsible for things such as branching
and arithmetic and string operations and
multiplication and even AES encryption
which is something Intel processors were
able to do for quite some time now for a
couple of releases back port number one
does arithmetic as well and also
multiplication and also encryption and
also floating-point operations and it
turns out that even the same kind of
thing like adding numbers or multiplying
numbers you can actually do concurrently
on the chip because you have multiple
ports that can do the same thing so for
example port 5 port 1 and port 0 are all
capable of adding integers together so
if you have a lot of integers that you
have to add together and you have a lot
of instructions just adding integers you
can actually do three of them at once
your core your single core can execute
three of these instructions at once and
this is very important for understanding
where like the best-case performance for
your code is and I'll show you a couple
of examples and how
can appreciate that in advance try to
figure out what the the best-case
performance can be for doing a certain
kind of thing and how to use this port
at once if we can now Intel have this
process where by understanding how the
processor works you can look at every
single CPU cycle so several billion
times per second you can look at what's
happening inside the CPU and try to put
it in four buckets what's happening
right now and it's just a very basic
flowchart so it starts from was an
additional micro operation allocated for
execution
so basically did the backend receive
another operation for execution they did
a an instruction start to execute now if
yes there are essentially two options
for how that happened how that happened
one is that an existing operation
completed successfully
it's called retirement in Intel parlance
so if you have an instruction retired
successfully it means everything is fine
you finished an instruction you're
starting another instruction this is the
bucket we want to be in or an operation
might not have retired even though a new
operation was beginning to execute and
this would happen if you tries to
predict what's necessary to execute but
in fact it was the wrong prediction it
was bad speculation and we'll see where
bad speculation comes in in the next
example I'm going to show you so this is
not very good
it means the CPU is trying to foresee
the future and fail them next if if a
micro app was not retired
then a sorry if a micro app was not
allocated then we asked why and then
there's two options is it the back end
stall or a front end stall so one thing
that could happen again going back to
this diagram is that your back end is
busy all ports are busy so you can't put
any additional operations in flight and
the other option is no the back end is
not busy but your front end isn't
capable of pushing more instructions
through the pipeline so for example you
might have cache misses on the
instruction cache you're trying to read
an instruction from memory and you're
just waiting for that instruction to
arrive so you have not
to do in the meantime so there's four
buckets that you could be in this is the
good bucket to be in when I'm retiring
completing instructions and if you're
not there all the time and no one is
there 100% of the time you can start
analyzing y-you can start analyzing why
I'm speculating badly or why is my
back-end so busy or why is my front-end
not capable of giving me more work to do
and this is the Intel top-down process
which the stress and all their tools
this is how you're supposed to analyze
performance methodically we don't have
time to do this kind of analysis for
most of my examples you're just going to
have to assume that we sort of did and
now we're just attacking the problem as
we identified it but this is the process
to do so let's start with a couple of
examples that illustrate the way the CPU
works these are just highlights and then
I'll have some examples of actual code
that we're going to look at look at it
in more detail so this is a stack
overflow question with an astounding 16
K uploads and almost 8,000 stars and I
bet some of you have seen this before
could you raise your hand if you did
okay awesome so why is it faster to
process a sorted array than an unsorted
array here's some code I don't
understand what's happening
indeed if you look at the two code
snippets this is what you see it's
essentially the same code which goes
over an array of numbers and adds
together all the numbers greater than or
equal to 128 now in the first case the
array is called sorted and the other
case is called unsorted and that
reflects reality the sort that one is
sorted and it turns out that going
through the sorted array takes 18
something units time units and the
unsorted array takes 121 this is not
from the stack overflow question this is
from an actual benchmark I executed at
the end of the talk I have a github repo
link where you can reproduce all of this
so it's it's nine times now thirty-six
times faster or even a little more to go
through the sort of the array than the
unsorted one
and there is just one explanation for
this which has to do with branch
prediction it has to do with the CPUs
ability to figure out what the next
instruction is going to be and whenever
you see a branch it's incredibly
important for the processor to make a
good prediction
whenever the processor sees a branch it
tries to guess what the result for the
outcome of that branch is going to be of
the conditional check and so in the
sorted array just think about it once we
start making a certain branch we're
never going to stop so we go through the
array once we reach 128 we're always
going to take the same branch and before
we reached 128 we were always taking the
same branch so there's just one
inflection point where we change your
mind but otherwise the CPU should be
fairly easily able to figure out the
pattern and just start guessing
correctly every single time so the CPU
can speculate and start executing
additional instructions even more than
one ahead and it never fails almost
never fails whereas in the unsorted case
we really have no idea what the pattern
is going to be the data could be totally
random numbers in any range and then we
could make a guess it would work it
wouldn't work there's really no way to
know and the processor keeps doing
unnecessary work never being able to
progress a lot of elements at once
before this information is known before
we actually know what the result of the
branch is going to be and this is an
incredible result this amazing
difference between going through the
source array in an unsorted one is
really only explained by this phenomenon
where the CPU tries to speculate and if
it speculates correctly you gain a lot
of performance and if it doesn't you
lose a lot of performance
so that's just one demonstration like I
said we'll have some more sophisticated
examples coming up later another example
which I wanted to share with you has to
do with this concept of using the CPU
execution units more effectively which I
talked about earlier so the top loop
over here is the very simple
implementation of min/max
so you have an array of numbers and you
want to find the minimum and the maximum
values at once going through the whole
thing just
once so this is really nothing to see
here and then some you know efficient
developer decided to optimize this thing
and so that we write the code so that
maybe more execution units in the CPU
would be able to do some work at the
same time now what happens here is that
essentially we go through the array two
elements at a time and we keep two
counters for the minimum values and two
counters for the maximum values so we
basically go over the even and odd
elements separately and then at the end
we just you know figure out which was
the biggest and which was the smallest
of these two minimums and maximums we
found and this is supposedly might be
better right maybe because in theory
there is more work for the CPU to do in
each iteration so there's potentially
fewer branches and better chances of
predicting what's supposed to happen
unfortunately it is slower the optimized
version and not by one or two percent it
is considerably slower and so you might
be saying okay well this is just all
garbage and you really can't figure out
what the CPU is doing and there's it's
worth less this whole thing let's go
home
and then you look at the actual
generated program at least what the
c-sharp compiler plus the dotnet
just-in-time compiler generated from
this actual benchmark and so if you see
the the simple loop the first one is
well it's assembly language but it's
very very short and simple to understand
it's basically comparing and then
branching comparing and branching to
find the minimum and maximum update them
and move on to the next iteration the
whole thing is pretty small and Intel
processors have a dedicated optimization
for loops that are particularly small
there is a thing called loop stream
detector which if it sees that you have
a loop fewer than a certain number of
micro ops some special sauce magic kicks
in and makes things even faster I know
this is not a very formal explanation
but this is sort of what happens
whereas the optimized version is one a
lot bigger and more difficult to
understand maybe which again sort of
prohibits the look stream detector from
taking place but also the red pieces
here arrange checks essentially the
just-in-time compiler could figure out
in this example that my array accesses
here here here and here are safe and
that I'm not stepping outside the bounds
of that array and since it wasn't able
it wasn't able to figure that out and
indeed these accesses might not be safe
the I plus ones might not be safe and
within the bounds of the array there are
range checks here which would throw an
exception if I step out of bounds but
obviously the checks have a cost even if
I'm not actually stepping out of bounds
and that's those things up so it's not
just the shorter code the shorter loops
it is also these range checks which
would happen in the optimized case so
this is just another anecdote of trying
to optimize for something you think
about the CPU and failing this is also
and there's also an important lesson to
be learned here a final example before
we actually dive into the more serious
optimization work this is a
vectorization so on top you see the same
min max algorithm which just you know
works on a single int at a time and then
a vectorized version which uses vector
instructions and I'll explain what
vector instructions are shortly and this
version that's the important result this
version works in 1/5 of the time so it's
5 times faster to do this and calculate
the minimum and maximum values than to
do this and this is on a single CPU this
is on a single core there is no
parallelization there's no multiple
threads it's just somehow magically
squeezing out more from the same
execution units we have on a single core
whereas similarly we could have just
paralyzed the whole thing
we could have taken the original loop
with the min and Max and we could have
used something like parallel four to
create multiple threads which would run
on multiple cores and that would also
speed things up hopefully and the lovely
thing is you can combine both effects I
do not have a benchmark here for this
but if you look at the results by using
vector instructions we speed things up
by roughly 5 and by using
parallelization without vector
instructions we speed things up by about
two and a half and hopefully if we apply
both vectorization here and
fertilization over here we could get a a
nice avalanche effect and we'll see in
indeed in some cases we will so that was
just the exposition different facets of
the processor we might be trying to take
advantage of so the branch prediction
mechanisms what happens inside the CPU
execution units trying to have them run
at once vectorization parallelization a
bunch of things we'll be looking at
concrete examples of in the next 3040
minutes all right
so we'll start with vectorization
because indeed it's one of the key
things Intel have been stressing for a
few years now trying to tell people you
understand parallelization you
understand how to write multi-core
programs this is all well this is all
fun but you really have to understand
what's happening in a single core and
you have to speed things up there before
you move to additional cores and
parallel life so essentially Intel
processors have had for many years since
the very beginning
classic instructions as I call them here
which operate on single values like a
single float or maybe two floats or
maybe two in that kind of thing some
examples this instruction here add adds
two integers together this instruction
here move copies a single integer to
another location
this instruction MO multiplies two
floating point numbers that's the kind
of thing you expect CPU instructions to
do right that's what you learn when you
learn assembly language
Sindhi or single instruction multiple
data are in store
actions that Intel have had for like 15
years now and again have been stressing
more recently because a lot of advances
have been made in last few years and
seeing the instructions operate on
multiple values at once multiple
elements multiple insert floats
what-have-you examples add PS is an
instruction that adds four floats to
another four floats and produces four
results so it does an element-wise add
of a total of four floats and another
couple of four floats producing a couple
of four results it's pretty nice for a
single instruction assuming it works as
fast as the standard ad which we'll see
later and then you have something like V
mo PS which is an instruction that
multiplies eight floats by eight floats
producing eight floating point results
and in very recent Intel CPUs like the
Knights landing I've shown you they also
have instructions operating on sixteen
floats and sixteen floats producing
sixteen floating point results if you
work with doubles it's obviously half
the size so for example there is a Remo
instruction that works on doubles and it
takes a total of four doubles and
another four doubles and produces four
products four double products so these
are pretty cool instructions and there's
hundreds of them here's a list of
categories it's definitely not a list of
instructions just some of the things you
have instructions for so obviously basic
operations add multiply shift operations
compare is pretty important and we'll
use that little later comparing large
regions of memory conversions
surprisingly converting into floats and
vice versa is a pretty expensive thing
to do I've seen code where 20 30 40 % of
the time is spent converting so if you
can speed that up it's pretty useful
cryptography including hashing as well
CRC check sums that kind of thing
various math functions like
exponentiation
frig that kind of thing memory
operations reading and writing to memory
and finally string operations as well
like searching in a string that's
something we're going to explore a
little later now there's really a great
variety of these instructions available
in modern processors and the question is
why is it even worth the effort so I
have the EDPs which operates on multiple
values at once is it faster than just
doing several of these classic
instructions one of the other
so hopefully you realize the answer is
yes but I still do have to convince you
in some way so when looking at
instruction performance we typically
look at two aspects one is latency which
is time from start to finish you start
executing the instruction how soon is
the result going to be available and the
other aspect is throughput and that's a
little trickier to explain so let's
start with latency here I'm looking at
two instructions ad which adds a pair of
integers and P a DDD which adds tuples
of four integers each so it does four
times the work
it adds four pairs of M's instead of
just one person pair of ins in terms of
latency start to finish both take one
cycle which is pretty cool you can do in
one cycle what you would need four
instructions in if you weren't using
Cindy throughput on the other hand is a
little trickier and throughput has to do
with the CPUs ability to execute
multiple instructions of the same kind
at once remember we have multiple
execution units if we have multiple
units capable of doing the same thing we
can actually issue multiple instructions
in a single cycle so what could happen
is that in the period of a single cycle
I could have multiple instructions
completing because I have multiple units
where they could be executing so if I
have a constant flow of add instructions
going into the processor because I have
a lot of data a lot of numbers to add I
can actually finish three ads in a
single cycle by multiplexing them and so
the throughput for add if you have a lot
of ADD
is three per cycle which is very
impressive the throughput for PA DDD is
just two per cycle but it does four
times more work so it's still worth it
to use the vector version rather than
the classic scalar one and this is the
the key lesson to learn about vector
operations they are usually as fast or
close to the non vector counterpart but
they do four times eight times and
sometimes 16 times as much work which
means you typically can squeeze out of a
single core four times eight times 16
times more operations if you use vector
instructions now here's an example of
how to actually apply this in dotnet in
c-sharp to a real practical interesting
problem I know it's a little theoretical
but it's an algorithm that's at the
heart of a lot of stuff a lot of
libraries image-processing a bunch of
machine learning algorithms lots and
lots of stuff uses matrix multiplication
and it's the classic example for vector
operations so this is the one I'm going
to start with but I have several more
interesting examples coming up later so
here's a just naive matrix
multiplication I have three of them a B
and C and assume for a moment they're
all the same size just to simplify so
what happens is I have three loops and
the core of the inner loop is is this it
multiplies two values from a and B and
put the result in C which is the output
matrix the vectorized version looks like
this
I still have three nested loops four
four four ika and J but instead of
multiplying and adding just floats I'm
working with a type called vector afloat
which is a new type that Microsoft
introduced with dot at four point six it
is part of a new gate package called
system numerix vectors which again it's
been available for over a year now but
it's still not as popular as I would
like it to be and it gives you from
c-sharp programs access to vectors
which was not available before you could
do this in C or C++ if you wanted to do
is from pure c-sharp
you couldn't you just have to do inter
up and talked to some native code so
basically this vector type represents
the biggest chunk of floats that your
processor is capable of operating on so
on some processors particularly older
ones vector afloat would only have room
for four floats but the newer processors
it might have room for eight floats and
in some very recent processors for
sixteen floats and maybe in some future
processor first thirty-two floats the
key part is you don't care you just work
with this vector of float type which
also has a size you can read here vector
afloat load count is a static property
that tells you just how many floats
would fit in a vector of float and so
all your operations are now going to be
using vector of floats so I read from a
and I read from B and I read from C and
then here's the actual work multiplying
and adding vectors rather than scalar
single floats and what happens in this
single line is that I'm multiplying and
adding four total floats or eight couple
floats again it all depends on my actual
processor at runtime when I write this
c-sharp program I do not commit to the
actual processor which is again pretty
cool because if you do use your C++ you
typically would compile your code in a
certain platform and you would get code
which would execute only on these
processors so with this system numeric
selectors support in.net you can
actually run code that would be sort of
flexible and work on different vector
sizes different processor generations is
it worth anything in terms of speed so
here at the benchmark results it's
roughly a five and a half times faster
on the core I measured it on which is
just a plain standard Intel i7 nothing
fancy definitely not a 72 core monster
it's a $300 or processor so this is
pretty impressive speeding up matrix
multiplication which is such a core
operation a lot of algorithms by just
using some slightly different API for
multiplying and adding numbers which are
pretty fundamental operations turns out
you can do them a lot faster if you have
a lot of numbers to multiply and to add
so all this code as I said is available
online which you can test later as well
in the meantime take a look at something
intel has apparently been working on so
this is from sort of rumors but
substantiate that rumors from online
Intel is supposedly going to add an
instruction with this lovely name v4 FMA
DDPs
in some future generation Intel
processor and it's going to have the
following so to say still tax where you
would give it something called zmm zero
which for example in our case could be
16 floats a total of 16 floats you would
also give it four tuples of 16 floats so
a total of 64 floats in here and 16
floats in there and you will give it a
memory location and then what it does is
describe here this is all just one
instruction this is what it does and I
mean don't really follow the whole thing
but look at what happens here it
multiplies something from here by
something read from memory and as the
result to this one here and then it goes
on and read from another memory location
and again multiplies something from here
by that value from memory and add that
to this one here and it repeats this
whole thing four times and this looks
suspiciously like matrix multiplication
like the core of that inner loop over
here which adds and then very multiplies
and then adds this is what this
instruction probably is intended for so
Intel is going to apparently add an
instruction with just one obvious design
goal to make matrix multiplication
faster because it is so important and in
this case it would make matrix
multiplication a lot faster because you
would be able to avoid certain memory
accesses as well to more efficient
memory accesses so this is potentially
going to speed up
it's multiplication even more so Intel
is working on their side of the problem
by adding even more clever instructions
and we have to work on our side which is
applying these instructions actually
using them for our work so I'm going to
show you a couple more examples of using
vectorization and then some examples
from different domains as well so here's
a problem which is again interesting in
certain areas it's sort of confusing
because the problem is vector
normalization and we are going to
vectorize vector normalization but
whatever what we're trying to do is we
have a collection of 3d vectors a
collection of points in
three-dimensional space and what we're
trying to do is normalize them by
dividing each component of the point the
x y&amp;amp;z by the norm of that vector so
basically this is called normalization
this is what we're doing and the
calculation is pretty simple you
multiply a bunch of things you take the
square root and then you divide the X
the Y and the Z by that norm this is
what you do in each iteration over a
bunch of points all right so here's the
vectorized version and this one actually
takes a little work because the way the
data is represented over here is
essentially tuples of XY and z but the
operations I want to vectorize are going
to go over all the X's and all the Y's
and all disease so it's actually going
to be easier if instead of an array of
points I had three arrays of X's YS and
Z's three arrays instead of an array of
structures that has three components
where each structure has three
components so once you do this
refactoring and you have X's Y's and Z's
instead of a single array you can
vectorize the whole thing pretty easily
you read the XS you read the Y Z we
disease and each of these XY and Z is
not a single float it's a bunch of
floats and then you can multiply them
like floats and you can extract the
square root like floats but this is all
vectors this is all tuple
of floats and then you can divide the X
the Y and the Z by that calculated
result which is still a vector these are
vectors and then you can copy the
results back to the arrays and this is
all roughly four times faster than the
original algorithm which is again sort
of worth the effort it's not 5% speed up
by refactoring and complicating the code
it's a four-fold speed-up which is
hopefully for most users worth the
effort even if I had sorry even if I had
to actually copy the data from the
original array to X's YS and Z's
separately and then copy all the results
back it might still be worth the effort
because we're gaining so much speed by
just doing vectorization here yes okay
right another example from the same
domain which I'm not going to mull over
for long it's just another application
of the same principle
this is the end body simulation you have
objects in three-dimensional space and
you have gravity forces acting on these
objects right they're applying forces to
each other so each particle or each
object in your space has a mass a
velocity acceleration a bunch of a
position obviously and you're trying to
simulate their movement which can be
useful for anything for computer games
or you know actual space simulations but
again the interesting thing is you
probably have an array of structures you
have an array where each component is a
particle or an object and to vectorize
you would probably be better suited if
you had a bunch of arrays with each
component laid out sequentially so
instead of having a huge array where you
have all the values for the first
particle and then all the values for the
second particle
you'd rather have an array with all the
X's and all the Y's and all disease and
all the velocity X's and so on so you
could operate on these arrays rather
than the whole particle I don't have the
to vectorize code here it's just another
application of the same principle of
converting from an array of structures
an array of particles here to a
structure of arrays to a bunch of arrays
with each structures component
individually laid out in memory so it's
slightly different memory layout which
lets you speed things up okay so I have
two more examples which are not so
mathematical or physical this could
actually be pretty useful in databases
for example full-text search queries
that kind of thing indexing the web here
we're trying to optimize STR STR which
for C developers is pretty is pretty
familiar
it's basically substring search so the C
short version you know no frills is just
take a string and coal contains string
has a method called contains which finds
if a string is a substring of that
original string so haystack contains
needle and then here's the vectorized
version which unfortunately we cannot
yet express in pure c-sharp so what I
did for the benchmark I wrote it in C
and then the benchmark actually T
invokes it does interrupt to call that
version written in C so we're paying for
interrupts and still even though we're
paying for interrupts it is nine times
faster then the string contains methods
which is hopefully implemented in
managed code so why is it so faster what
is this thing doing here what is this
code here with the really scary
intrinsics like CMP EQ and load USI 256
the basic idea is illustrated here I
have my string over here the quick black
fox jumped off and so on I put black
here because I wanted two K's there I
know it's brown and what I'm looking for
the string I'm looking for is quick okay
so this is the haystack the quake black
fox jumped and so on and I'm looking for
the substring quick so what I'm doing is
I take a vector of characters and I fill
it with the first letter of my needle
which is Q and I take another vector of
care
ters and i fill it with the last letter
of my needle which is K so I'm looking
for quick so Q and K and then what I do
is I simul taneous Li compare my
original string with this vector of Q's
and this vector of KS at an offset I
know the string I'm looking for is quick
so I offset the case by 5 from where the
Q start that's all I'm doing I'm just
comparing and then the result of that
comparison is potential indexes where my
substring could start wherever I found
the Q in the original string it could be
a potential match doesn't have to be but
it could be wherever I found the K in
the original string could also be a
potential match not necessarily it could
if both of these match if I found the Q
and the K at the right distance from
each other it still doesn't mean it's a
match but it's a more likely potential
match so once I have that kind of
candidate I can now check the actual
characters I can now compare the the U
and the I and the C to see if I have
what I needed but first we eliminated a
lot of potential places in the string by
doing this simultaneous comparison
across multiple places in the string so
this is the basic idea of this
vectorized version this is what it's
doing the cmp u eq epi8 is comparing
vectors of characters vectors of bytes
and finding the places where they're the
same and then there's some work on
extracting where exactly they are the
same and figuring out that we need to
take this quick here and compare it with
the actual substring we're looking for
the result again is pretty impressive
it's a nine-fold speed-up so think about
it if you had a database query and your
database could spit that up by 9 just by
using vectorization it would be pretty
nice so database is obviously would have
some use for this kind of thing
especially for full-text search here's
another example from a somewhat similar
domain yes
so you're asking like why aren't there
libraries that just use this under the
covers yeah so right so there's two
entrances one is that obviously for some
problems there's going to be a library
that does this internally and you
definitely should use that library
rather than using my code or trying to
come up with your own Intel in fact have
a nice library which which is written in
C that has a bunch of vectorized
algorithms inside so for example for
matrix multiplication you would
obviously reach to that rather than
implementing your row but some things
are just so domain-specific that it
would be unlikely to find a library that
does that or maybe there is a library
but it's for Linux and it uses a Perl
and it's interface and you you take the
idea from there and just implement it
yourself this is tr SD I think I have
not come up with this myself I just copy
it copied it from some C implementation
I found and it's really effective so in
some cases it's really worth being able
to write these things out yourself but
whenever there is a library yep
definitely reach out for so here's just
one final problem again from sort of a
database space you have two sorted lists
of numbers and you want to find the
intersection of these lists you want to
find all the elements that are in both
lists at once
so why is this relevant for anything
think about it suppose you have two
Cleary's to Google or some database two
strings you're looking for to search
keywords and then you have a list of
results containing the first keywords
and you have the list of results
containing the second keyword and now we
want to find all the overlaps you want
to find the terms you want to find sorry
the documents that contain both keywords
that contain both terms so you have two
sorted lists not necessarily of the same
length and you're trying to find all the
common elements
so this is a an interview kind
implementation it's a typical interview
question it's a loop basically that has
two pointers into the lists and it
advances the right one and it finds all
the overlapping elements think about it
nothing to see here
and then the vectorized version again
requires C because there's a particular
instruction here which is not wrapped by
the dotnet library yet might be in the
future not right now and this is the
instruction C and PE sPRM which is
essentially implementing set
intersection for us it takes two vectors
of bytes or characters whatever we
prefer shorts or in essentially any
length and it finds all the common ones
so in our case for example I support us
for the purpose of the benchmark I
assumed we're looking for short
so the actual data is short I have a
list of shorts here and list of shorts
there so I'm comparing you words to byte
elements and this instruction just finds
all the overlaps for me and then it's
just the work of extracting them
essentially there's some work here that
has to shuffle the vectors around to
find the actual overlapping elements
copy them over to the output and then
advance again it's not so interesting
but the result is interesting it's four
times as fast as the original naive
version which you might have written an
interview so again if we have a way of
taking to query results and finding the
intersecting elements four times faster
I'm sure you'd sign up for that so this
is again a pretty nice application of
vectorization to something that's closer
to the real world than matrices maybe or
vector normalization there's a bunch
more examples like this literally
hundreds of algorithms that have
vectorized versions today you definitely
don't have in most cases you don't have
to come up with your own original work
you might find something and then adapt
it to your needs or convert or refactor
the data you have so that you can apply
a vector
algorithm to that problem and in some
cases you might not get like the most
optimal result you will get the
theoretical maximum you're capable of
extracting from your cpu it get pretty
close you'd get 80% with 20% the effort
this is pretty respectable ok so these
were a lot of examples of vectorization
and I want to spend the next few minutes
talking about slightly different areas
in the CPU that we can be taken
advantage of and specifically I have a
few examples around the cache and memory
subsystem that are also worth reflecting
over so first this is a very general
description of what happens in terms of
the memory system and the cache when you
access memory from core this is a
supposedly a typical I 5 processor which
has three levels of cache so this thing
here the grey one is the CPU socket this
is a single unit which you pay for and
install on your system and then it has
two CPU cores inside each core has a
level 1 cache which is separate from the
other cores and the level 2 cache which
is also separate from the other course
and these two cache levels differ by
size and speed of access level 1 cache
can be accessed in 4 CPU cycles now
think about it modern processors have a
clock speed of say 3 gigahertz so force
cycles is like a nanosecond and a half
maybe a little less the level 2 cache
has a latency of 10 cycles so say 3
nanoseconds for nanoseconds something
like this and it's a lot bigger I sorry
I didn't say but level 1 cache is
typically 32k on Intel I serious
processors and level 2 is 256 K so it's
bigger but slower considerably slower
and finally level 3 cache in this design
is shared across both cores
it's a lot bigger it could be 16
megabytes even more on more
sophisticated CPUs but it's a lot slower
as well so 40 cycles for example and
finally main memory which as you know
can be really big you could have
terabytes of RAM
modern server that is accessible in more
than 100 cycles so it's really a lot
slower than all the different levels in
the cache hierarchy if you can get your
data to fit in level one cache and
repeatedly access the same information
repeatedly access the same elements you
could have up to 30 or 40 times better
performance then if you have to
constantly go to main memory this is the
main result and even if you're only able
to use level 3 cache repeatedly and you
don't have to go out to main memory you
could get a nice win of 3x or even more
depending on your main memory indeed now
if you have more sockets more actual CPU
packages you would have multiple level 3
caches and then there's some more logic
required to coordinate them which is
something I'll touch on shortly so
here's the first example where this is
relevant it's a pretty classic one going
back to matrix multiplication once more
this is one loop and this is another
loop which implements the same algorithm
exactly except the inner loops are
interchanged here I'm going over J and
then over K and here I'm going over K
and then over J so it might look like
just a simple renaming not nothing more
but it's not just renaming because it
changes the pattern of access in the
actual core of the inner loop what
happens in the first example is that one
of my matrices the matrix B is accessed
like this I'm going over a single column
of the matrix whereas in this version
that same matrix B is accessed like this
I'm going over a single row of that
matrix instead in fact in this version
I'm going over rows only I'm there's not
a single matrix where I'm going over a
single column in the inner in the inner
loop the result is not as impressive as
a lot of things we saw before but still
pretty impressive especially if you
consider the following in this example
where when I benchmarked just to make
the benchmarks run a little faster my
matrices were just 512 by
twelve matrices of floats so a single
matrix would be one megabyte three of
them together three megabytes and my
level three cache on the processor I
benchmarked this on my level three cache
is eight megabytes so all of this data
was able to fit in level three cache and
still by accessing memory in a better
pattern by accessing memory
consecutively we were able to use the
higher levels in the cache hierarchy
better so we didn't have to go to level
three all the time we were able to find
what we needed in level one or level two
in more cases and the result is almost a
2x speed-up another example which is
pretty similar so I'm not going to spend
a lot of time on is tiling in a lot of
cases when you have data that you need
to access in some random patterns it
might be better instead of accessing it
in random patterns to make blocks
smaller blocks and work on these blocks
because the smaller blocks might be able
to fit in cache and then you finish a
single block which fits in cache you
move on to the next block which fits in
cache and so on reducing again the
number of pool cache misses you have to
do we don't have a lot of time for this
so just take a look at this later the
performance difference again is pretty
considerable in this case and in this
case still all my data fit in level 3
cache so again it's not the worst case
result for for main memory ok so my
final thing I wanna touch on what it has
to do with memory and cache is where
this whole cache subsystem or the memory
subsystem can become an actual
bottleneck for your algorithm so the
first thing to notice is in a lot of
cases after you apply vectorization and
fertilization a lot of these
optimizations
you end up with something that's so fast
that memory becomes the limiting factor
and here's one example from an actual
project I was on what we wanted to
optimize was dot product a pretty simple
operation on numbers you have a vector
of floats you want to reduce it down to
a single float dot product operation and
what we had is multiple versions that I
benchmarked
in other than finding the fastest one
which we did pretty quickly using
vectorization we were able to on a
single core without fertilization to
reach speeds of around 23 gigabytes of
data read from memory so we had lots and
lots of these floating-point vectors and
we were working at the speed of 23
gigabytes per second now memory on that
actual system the the the channel
between memory and the CPU had
theoretical peak bandwidth of 25 point 3
gigabytes per second so if you're at 23
something that's pretty much the
theoretical limit of how fast you can
fetch stuff from memory before you just
stall and have to wait and so this is on
a single core imagine putting 4 cores to
work you'd very very quickly run into
the maximum theoretical limit that your
memory system is able to give you and
this is again exactly what the Intel
top-down method is for you would then
identify that you are typically back-end
bound your back-end isn't capable of
issuing more instructions and then you'd
see which component of the back-end is
busy and you see the memory system your
load instructions are stalling you're
not able to read from memory quickly
enough for your super optimized CPU
algorithm to process your data this
usually happens in simple operations
like adding numbers or doing dot
products multiplications if you have
more work if your algorithm is more
complex you would typically not run into
memory limits that quickly but it does
happen and it's an important thing to
keep in mind you might sometimes reach
run into problems when you optimize too
much because memory can't keep up with
your amazing optimizations
that's just another part of the system
and a final example I was saying caches
can sort of interview each other so my
final example has to do with cache
coherence or cache invalidation when you
have multiple caches and pretty much are
on every single system today you have
multiple caches on multiple processors
or multiple cores at least there's the
chance data in the
French caches will not match you could
have the same memory location in two
different caches having a different
value which would be potentially
disastrous so there is a very
sophisticated protocol between these
different caches designed to keep things
in sync so you don't have disagreements
between caches which value is correct
for a single memory location the way it
works is the following so suppose you
have some value in both cores caches on
your system that value is then marked as
shared once the core wants to modify
that value it marks it as exclusive and
it causes the other core to lose that
value this is called invalidation I had
the value in both caches and then core
zero invalidates the value in core 1
cache and then finally core zero can
write and actually modify that value the
result and go out to main memory and if
core number one leads that result it
will fetch it back into cache now what's
interesting here is that this whole
thing happens not on the scale of a
single integer or a single float this
happens on the level of cache lines a
single cache line is fixed before bytes
in modern Intel processors so whenever
you have to evict something from cache
you evict 64 bytes and we have whenever
you have to read something into cache
you read 64 bytes and then what happens
is you get full sharing you get
invalidation which was not intentional
it's not 2 cores accessing the same
memory location it's two cores accessing
adjacent memory locations which happened
to be on the same cache line and so this
very simple example again we're not
going to drill into all the details here
I have two versions of the same code one
I have a certain function run in the
background in update an element in a
shared array of doubles and this runs in
parallel on multiple
thread each thread is accessing a
different double each thread is updating
a different element in a single shared
double array second version almost the
same except each thread is working on a
local variable and only at the end it
updates the shared global double array
the speed difference is pretty crazy
especially when you add many threads to
the mix the key results to see here are
the red lines this is the first version
as you increase the number of threads so
you have one thread it runs and 54 units
you have two threads it runs in 53 units
you have four threads it runs in 55
units it's not getting faster as you add
more thread whereas the second version
speeds up linearly from 54 to 28 to 15
and it could probably speed up even more
the only reason for this difference is
that in this version you're accessing
adjacent memory locations from multiple
threads and in this version you're
accessing a totally different memory
location and only at the end you have to
sync the results up this thing is
causing lots and lots of cache
invalidations
whereas this version is not so this is a
pretty known phenomenon but you keep
running into this in a lot of different
areas it's really hard to predict at
times which things are going to be
adjacent in memory so which patterns of
use could end up colliding on the same
cache location and and cause this
performance problem for you so again
there are tools for this and this is my
concluding thought there are tools for
this I wanted to show you as many
examples as I could so I didn't drill
into each one and you know formally
showed you where the problem is exactly
before optimizing but Intel and AMD
which are the two major processor
vendors for us they have tools available
for benchmarking applications and
pointing out what the performance
problem is from the CPUs perspective in
fact Intel even have tools like the
trading advisor and the vector advisor
which will tell you so here's a loop you
could improve but you
vectorization and here's a loop where
you applied vectorization but you didn't
get the best performance out of this
loop you should work harder on this loop
and you will get even more so there's
tools available for this most of them
commercial but there's also some free
information you can get out of your
system looking into the future finally
we'll probably see even more complex
systems even more complex processors
that we have to optimize for for example
we might see systems where you have
hybrid course you have faster cores
designed for certain tasks and slower
cores which consume less energy which
would be dedicated to some what other
tasks so you'd have to think about
scheduling work across cores you have
external processing units for example
Intel the 72 core monsters they can be
added to a system as external PCI cards
so you could add multiple processors to
the system dynamically and then use them
as external processing units there's the
GPU which again we could spend the whole
talk talking about how to optimize for
the GPU which also has a bunch of course
available for you to use for
general-purpose computation so there's a
lot of interesting stuff to look into if
you're interested in low-level CPU
optimization optimizing algorithms like
this so all my examples are available
here and github and the slides if you
want them are over here on Dropbox if
you have any questions I'll be happy to
to answer but I do have to move off the
stage for the next speaker so really I
appreciate you coming thank you very
much and enjoy the rest of embassy thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>