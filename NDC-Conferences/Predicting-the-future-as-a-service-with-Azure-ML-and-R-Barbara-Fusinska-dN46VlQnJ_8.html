<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Predicting the future as a service with Azure ML and R - Barbara Fusinska | Coder Coacher - Coaching Coders</title><meta content="Predicting the future as a service with Azure ML and R - Barbara Fusinska - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Predicting the future as a service with Azure ML and R - Barbara Fusinska</b></h2><h5 class="post__date">2017-04-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dN46VlQnJ_8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I think we can start ain't good morning
everyone I don't know if you can say it
morning still but it's before noon so
let's start and thank you for joining my
session I know Jennifer's is very
interesting too happening in the in some
room next door today I will be talking
to you about predicting the future and
how to use our UML to do it and how to
actually use it in the form of a service
on Azure and Who am I to talk to you
about that I used to be a programmer I
switched the field and now I am data
solution architect I have a background
in machine learning
I was doing my p sorry I was doing my
PhD my master's degree in machine
learning
currently I am employed by Microsoft and
I'm talking to the clients about their
data solutions about databases how to
use predictive analytics on them today's
agenda we are surrounded mostly on
machine learning topic so give you a
brief introduction on what I understand
machine learning is because wherever you
go
in the internet nowadays there are
different definitions you probably have
your own
I'm not saying mine is correct I'm just
saying it's mine so I will tell you what
I think machine learning is and then I
will show you the capabilities of the
tool which is a machine learning portal
and I will show you how to create an
experiment so it will be heavily demo
driven part and then how to
productionize your solution once you
have your model built then I will talk
to you about how you can collaborate
your solutions or use the solutions
produced by other people and how to
collaborate with the community let's
start what is machine learning what do
you think we have quite a few people we
hear it in all those sci-fi movies and
books and basically it's a big hype at
the moment what do you think machine
learning
anyone yes
so one of the definitions is a piece of
software that improves basically learns
right and yes that's very very valid
definition anyone else yes
making judgments and based on data sets
this is actually how you use when you're
in your machine learns when your
algorithm learns then you're making
judgments or predictions for the future
data based on the historical data yes
very very well no one said anything
about artificial intelligence or robots
deliberately put it there you probably
know a lot on the topic so artificial
intelligence is something that is
broader and machine learning is one of
the fields inside of artificial
intelligence and we had the panel
yesterday on machine learning and I
think we covered it quite a bit on how
different word can change maybe not the
meaning but the perception of how people
feel about the field anyone else wants
to share what machine would they think
machine learning is yes optimization
very good I was actually waiting for
that answer which I almost never get and
so like back in the days like 10 years
ago and when you were doing for example
data mining or what was said artificial
intelligence was back then it was
solving optimization problems you had a
function and you were trying to maximize
it and minimize it in the most efficient
way because standard maps or standard
modeling didn't give you that answer
quickly so computation was very
expensive I will walk you through my
definition but before we go that I would
I would like to stress it that we are
living in a very exciting times exciting
times and I read this quote well
literally like 10 years
ago for the first time probably late to
the party and and then I started reading
Harry Potter and then I started watching
movies with Harry Potter and I had this
feeling that those guys especially
Hermione are like very good hackers they
really know all the spells which basic
Commons or the syntax all the
programming languages and they really
know how to use it and even though it
was about magic right for me it was
knowledge it was those people knowing
what they're doing there was nothing
magical about them especially when you
think of Hermione I'm sorry for not
Harry Potter fans because they were
learning how to use it they were
practicing how to use it and then if it
was advanced enough it appears as magic
and today if we see all the applications
of artificial intelligence cognitive
services machine learning it really
sometimes resembles that this is the
example I encountered several books and
articles and it's about movie types I'm
not pronouncing this word because I
can't I tried believe me that's why I
will be using types instead and it's
supposed to show you how does machine
learning algorithm work and I think it
doesn't tell you that and I will I'll
point that out while I'm explaining it
that's why I took this example
deliberately to show you how much in
don't thing or actually that machines
are not thinking at all it's just down
deep mouth sometimes very complicated
math that's just math equations numbers
the example is as follows we have a list
of movies we have a title and someone
went for the f-word watch all those
movies encountered how many times people
were kicked and how many times people
kissed those are not real numbers I
actually didn't go for those movies
indeed and count that I just make those
numbers up but let's imagine someone did
and those numbers are
and then based off so also on the
observation someone attached the movie
type as the separate column so the
machine learning task would be based on
those data if a new example new movie
comes up and you have the information
about the kisses and kicks assign the
movie type and when I saw this example I
was thinking wait a minute
well I have a new movie and I can assign
the type just straight away based on
title most of the times if I watch the
movie I really didn't count it those
kisses and clicks even if someone
provides me with this information me as
a human being I don't use this
information but if you think of how
machines are thinking they say it like
this for them it's numbers for them
those titles are meaningless they don't
have any emotions they are not actually
watching movies even though nowadays all
those cognitive services all those
available API are trying to convince you
that they do understand the written text
and they can get the information about
the movie by watching it they get the
numbers they are not getting a feeling
they are not understand the movie they
are not actually watching the movie they
are gathering information they're
gathering numbers so in general if you
have a table of data and you have some
rows feature one and feature two you
have a Class A and B again you as a
human not the Machine new example comes
in feature one is thirty one feature two
is four which class would you assign the
new example to yeah B
and how did you do it what made you
decide that those features belong to
Cosby they're similar so as humans we
are constantly looking for patterns if
you try to make machine find the class
you need to tell them what they're
looking for either we tell them find me
similar examples and then you need to
define what similar means because the
Machine won't understand that or when I
got in the answer like people only look
at the feature 1 and on it to example
huge feature one where of the class B so
some people didn't even look at the
feature - this is how you present the
solution to the machine but this is not
how the machines are thinking again so
what if you have let's say not five but
five thousand examples and yeah let's
say five thousand examples what would
you do to figure out the new class the
class of the new example any ideas
because it wouldn't be this easy right
to look through the numbers for five
thousand examples and actually find the
pattern unless you are natural pattern
finder which as people sometimes we are
but I just gave you five thousand what
about 50 millions any idea how to
that's the solution I will show you yes
that's a very good point so we we can
find the line that separates one class
of the other to find the line as humans
what would we do
sorry
yes find some statistics right so the
the the approach we usually do and data
science as we try to visualize stuff and
if you can you can say like those are
the numbers that belong to the class aid
those are the the dots that belong to
the class beam this is the line that I
just heard and we could say well
everything on the left side of this line
is of a cross a so whenever the new
example comes in for example like here
it will be of the class a everything on
the right will be of the class B and
this is again as humans what if we have
not to feature but 50 features or 500
features how would we visualize that how
would we find a hyper hyper area to do
it it would be much harder because me
personally again want to offend anyone I
only seen three dimensions so we
realizing more than three dimensions it
would be would be a challenge for me not
to mention I wouldn't really actually
have the tools to present it to you know
business people for example so
visualization is great but it you can
only take like two free variables
features at the time but with machines
this is where they are great they are
they are not visualizing the I mean they
do visualization but it's for us but
they don't think oh I cannot think in
more than three dimensions because they
can machines take dimensions and just
pour just put calculations on that so if
you have
tons of data if you have a lot of
dimensions machines are bloody good and
calculating stuff the problem is while
we see the line line is meaningless to
machines you need to say this is the
equation this is the how you calculate
stuff and from now on use this equation
and try to fit the coefficients of this
equation some parameters but I'm
basically telling my machine what's the
solution I just not don't know the shape
of this solution this line can be very
simple it can be very complex can be
like a roof system but it's me telling
the machine to find this particular
shape of solution and the machine is
calculating the coefficients nothing
really magical although very often it's
very very complicated maths this slide I
think a little bit unfairly was and
yesterday's pop quiz I didn't realize it
will be yesterday and my talk is today
I'm not breaking code of conduct and
this is a very profitable job if you
search it online you can get like 40k if
you can do it so what's the problem and
why am I talking about that so I found
an example to kind of show you that
machine learning algorithms based on the
historical data in some magical way are
learning about the future about the new
examples and this is the example of how
human brain works no one knows how human
brain works but we can see sometimes how
does it apply to the life to the
production so chick sexing if you're
producing eggs or meat products from
chickens and there is this problem when
they are small no one knows about the
gender or maybe put it differently the
gender matters in that case because the
chicks are separate
and have completely different faiths as
you can imagine so some are producing
eczema are producing eggs in a different
way or are just killed and used for meat
the problem is nobody knows was the
gender of the chick that's actually not
true
some people do know but they cannot
explain it to you they just I don't know
if they look or feel I don't want to
know but they have the capability of
knowing the gender the problem is the
this person that knows cannot explain it
to the other person but there are
schools that teach you that so how do
they teach you if they cannot explain it
to you the student sits with the teacher
the student starts to guess the gender
of the chick and the teacher is telling
him you're right you're right you're
wrong you're right you're wrong based on
that apparently after few weeks the
brain of the student rewires somehow
magically and the student after few
weeks is capable of chick sexing this is
what we call supervised learning so some
like old sources we're calling it
learning with the supervisor or learning
with the teacher and it's not actually
true the supervised part is that you
have the right answer do you have the
labels or you have the values of the
historical data so for example if we we
have for example classification problem
image with the cat and the dog let's say
we have a system that it's task is to
classify if the pictures of the cat or
of the dog and we feed our system with
those pictures and we labeled them so
while learning the other machine the
computer the algorithm knows oh this is
a picture of a dog this is a picture of
the shape of the cat like with chick
sexing there was a teacher and was
putting the right labels
after a while the algorithm is
optimizing the coefficients and based on
those labels it rewires it calculates
the parameters and learns how to
classify new examples
the problem is with overfitting which is
a glorified name for learning examples
by a heart and very often this line is
too exact so you get very good accuracy
on the training test and when a training
set and when you fill your algorithm
with new examples it totally crushes it
really does not perform well so to
prevent it usually you do split of the
training and testing set another branch
of learning is unsupervised learning and
this is the one that is considered more
cool because you don't have the right
answers you don't have the labels you're
trying to find out something about the
data but it's very hard to actually
evaluate york evaluate how your
algorithm is performing because no one
knows right answer usually the market
validates your answer so if you're
applying it to like clustering and try
to find the markets of people that are
similar to you and you you do it wrong
you just want and money but it's not a
very scientific method right so this is
where when people are really thinking
usually when they think about machine
land it is finding patterns finding
rules
finding associations part of machine
learning so when I was doing my PhD I
had two problems and one of them was
that if we're doing machine learning I
was doing classification problem so
supervised learning I had this nice
workflow so you got historical data with
your labels you need to prepare them
because they usually are not well usable
basically you need to clean them you
need to say
of which features are significant which
features you need to get rid of which
are redundant or correlated so maybe you
can get rid of some for the computation
savings and then once you have them as I
said you're splitting the data on a
training and testing data set and then
finally you have this training set and
the fun begins but this part especially
the cleaning and data preparation can
take you up to 90% of your time and then
you have this 10% or 20% for the fun
stuff and then this is this is where
your other data science is a fun yellow
yada yada and then you have your trained
model it's performing very well now you
don't know yet how is it performing
first you need to publish it somehow in
to score it and then you need to
evaluate it and for me as a data
scientist all those other parts were
kind of operational DevOps very boring
they are necessary because otherwise you
don't know how your algorithmic if you
don't clean your data who don't have any
data to work on but I was like missing
some components are ready to use
libraries which weren't available as
easily back then as as how it is now and
I was I was craving for the tool that
could do those stuff for me or automated
somehow so I don't write the code by
myself because you know very often this
code could be just wrong and I could
just reassuring myself that I'm doing a
good job but the main reason was I
wanted to do this fun part and I wanted
to push the boring work as far as
possible so this was one reason why I
got frustrated when I was doing machine
learning for the first time and so as I
said so a lot of those non machine
learning components in machine learning
process that are necessary but not that
exciting
and another thing was I built the model
I have something that is predicting if
it's a cat or a dog let's say and I want
to use it in my system in my web app in
dotnet up for example or I don't know
Java whatever whichever you are writing
your application and usually when you
think of machine learning and people
that are creating the model and people
that are using the model too so
completely different teams they very
often don't speak the same language or
don't want to speak the same they are
both technology people but they don't
like to do it so developers like to
speak code api's DevOps and data
scientist just like to do machine
learning you want to talk about
infrastructure in my opinion they should
talk better but they are they are having
constant fights and an easy way to
publish your model wasn't there how
people were doing it let's imagine we
have this line we have this perfect
those perfect coefficients and we can
distinguish between two classes so data
science people were serializing this
model somewhere developers were picking
up this model somewhere and then there
was this thing well okay I have this
line but I'm not a data scientist how do
I use this line in my web application
and then you need to talk to those data
science guys again and it's all over
again so two things automating not
exciting tasks in machine learning and
publishing it and what I'm saying is
other ml is fixing it for me and I will
show you in a second in the demo how do
I do it but before I do that I'm warning
you not that I will come and kill you if
you don't listen to me but there are so
many articles initiatives
that are just telling you you need to be
this tall to be a data scientist that
everything is out there just for you to
reach everything is out there for you to
reach and you can use it no one will
forbid you because almost everything is
open sourced the thing is you really
need to know your stuff
to use it properly you can use it
somehow it will produce some results but
if you don't know what you're doing you
will just reassure yourself that it's
working well I encourage you to write
your own machine learning algorithm but
not to use your own machine learning
algorithm fermentation productions
because there were people producing all
those libraries all those components for
you to use spending years on that
probably did it better than the project
you would do during the weekend unless
you're one of those people then of
course use your own algorithm then then
this doesn't apply so everything I will
show you I hope will look simple it's
not that easy if you don't have a
background so what is our machine
learning service first of all it's an
drop and drag tool so you have all those
components it will kind of resemble the
machine learning workflow I just shown
you and second of all once you have your
model once you train it you just push a
button and it publishes API and on the
only thing your developers need to do
now is just call your API with the new
prediction with the new data and the
Appel will just create a response with
the prediction and that's simple
it's taking care of caching it's taking
care of authorization authentication and
data science people don't even have to
know about that
how does it work inside so I'll show you
some demo showing how machine learning
works
it's a weird resolution but let's hope
it will work so if you go to studio our
amount of nine
by the way everything I'm showing you
now is for free just go and use it sorry
so you can see I have in a moment I have
quite a few experiments because I'm
giving this talk for some time and if
you want to create a new experiment you
just click new and you can see that you
have like a visualization of the last
experiment I've created let's create a
new experiment blank experiment if you
don't want to use blank experiment just
want to see it some samples they are
available here too but I will show you
everything from scratch and right from
the beginning you get the tool and that
shows you like kind of us were to drag
items
unfortunately this disappears after I
drag the first item unfortunately
unfortunately because I know I used to
be a software developer how software
developers hate to I can drag drop tools
and but bear with me it's an amazing
tool I was reluctant from the beginning
but then it solved so many of my
problems so remember this machine
learning workflow I was showing you
first I need to get my historical data
and this is a problem of its kind so if
you're in an organization you usually
have some data if you want to learn
machine learning there are a lot of
public datasets available usually
cleaned and ready for you to use in the
same the same thing is with Azure
machine learning studio you have a lot
of datasets available for you to just
use so you can see so many samples some
of them are just taken from UCI which is
very recognizable data set repository
what I will show you now it will be a
binary classification problem so all try
to predict the income this is the data
set and let's let's have a look how does
it look like
fortunately the network is not the
greatest but should be fine so you can
already see that it has 32 K rows it has
15 columns so it's not that easy to
visualize
all of them you can see the column names
and even the preview of several rows so
we have stuff like age education
occupation relationship race gender etc
and what we are trying to predict the
task is we're trying to predict if the
person that has all those features will
gain income above or below 50k
so we have two labels either is above or
others below and it's called binary
classification classification because we
only have two labels so if you remember
from the workflow first thing we want to
do once we have our data cleaned and
those data are quite clean because they
prepared it for us is to split your data
test for the training in testing data
set you can look through all the
components there's so many of them so
you can see and try to search which one
do you want but I know names so I'll
just put split data and I'm just
connecting the dots now as you can see
the data set only has the output port
while every other component usually has
at least two at least one input and at
least one output in this case we have
two outputs why because we are splitting
the data for the training and testing
data set so whatever comes on the Left
will be well if you say it would be like
test data set but you can revert the
order whatever you want and on the right
there will be the rest of the data set
how do you do it you just set up the
fraction so you can see I need to click
so you can see that there is a fraction
the industry standards is saying start
from your point eight for the data for
the training data set so yeah let's
start with zero point eight so 80% of my
data will train my algorithm 20% are we
used to test if it's
performing well on the new examples once
we have our data split now we need to
train our algorithm and then I will use
train model component I'll make it a
little bit smaller so I'm connecting my
train model with my data and it still
has a problem it has a problem because
it doesn't know how to train it I'm I
just told it train it but I didn't put
the proper algorithm I didn't said to my
to my experiment train it finding the
line train it finding a curve or train
it finding decision tree so it doesn't
know which algorithm to use in there
plenty of algorithms used for different
kinds of problems this is a
classification problem and I found an
algorithm for you to show it's called to
class and then if you can see there are
plenty of other algorithm could use for
to class go to Azure machine learning
studio and play yourself so I will use
boosted decision tree because it has
nice visualization capabilities so I'm
saying train mint is train this model
based on this data and this algorithm
it's still not happy it's still not
happy because machines are dumb I
shouldn't say that because if they are
actually becoming our overlords they
will remember anyway they don't know
what's my label they don't know what I
try to predict this train model is
treating my income column like any other
column with data right so I need to tell
set up the task find me the income
now it's happy so I could stop right
there but I wouldn't know if it's any
good so I could say like trimming the
the motto I would find the motto and I'm
happy but I don't know if if my data
isn't good if my algorithm if I put a
proper parameters on my boosted decision
tree because every algorithm has some
hyper parameters so what I want to do is
now I want to tell my algorithm once
it's learned what once it learned well
now try to predict the income for the
test data we'll see how good you are
so it's called scoring right
model so I'm saying score me this data
this time I'm using test data using this
trend model and then I will have another
column that will be well the income
that's my Algirdas after it was trained
and what do I do to find out if my
algorithm is performing any any good
once I have real labels and predicted
labels sorry yeah I'm actually you're
comparing the real labels and you're
comparing the predicted labels and you
see how many times were right this is
like the simplest thing you could do
sorry in 29 minutes you know I have the
solution for that
ya Hossein come on come on ya did it
back then didn't work another one
another one yeah just ask yes okay
no cancel anyway we have our models card
and now we want to evaluate our model so
I don't want to just take do ifs and
calculate how many times I was read
because there should be something that
does it for me there should be a
component and there is evaluate model so
I'm using evaluate model it's the sizing
thing is the list of my favorite but
it's the only thing I don't like so now
we have the whole experiment and let's
run it so it's a very small simple data
set quite a quick algorithm so it will
take like less than 10 seconds but this
is the most consuming time consuming
part of machine learning you're trying
to learn your algorithm depending on how
much data you have depending on how
complicated data set you have like how
many dimensions because then if you go
deep down to some algorithms they are
based on matrixes and if and if you
multiply them etc so how the computation
cost could be huge but very often you
only have to do it once like well that's
actually not true if you try to tune
your parameters you're doing it over and
over again so you want to do it several
times until you sure so let's see how
good our algorithm is let's see how our
twin model look like and remember I
chose the decision tree for the purpose
of showing you that some algorithms
of the form that you can actually
explain to the human being once it
decides to render so it's a decision
tree if you know how does it work it
just takes the dimension and makes the
decision go right or left based on some
cut through the values and if you have
this in a final stage you can easily
translate it to even non-technical
people that this is the final decision
tree and this is how it works internally
the very recent decision trees weren't
very accurate but many companies were
using them for this exact reason because
how they work was very transparent and
understandable and they were just
sacrificing the accuracy because it was
it was easier to have the buy-in from
the business and so let's see how our
model was scored and if we visualize it
it will really resemble the data set
itself
yay so as you can remember it it had all
those columns it it has less rows
because we only take 20% of a testing
set to actually score our model and the
data set just ended at the income and we
have two additional columns after
scoring and the way it works is first
there is this probability or a number
from 0 to 1 assigned to your example and
if the number is less than 0.5 the
classification goes and say this is the
the example of the Class A and if the
number of probabilities above 0.5 the
classification goes this is a Class B
the 0.5 is it's only a parameter can
also set it up like how the decision is
made because sometimes you need like 80
or 85 percent certainty and even only
then you can decide of a different class
so we can see how our labels were scored
so so far so good some inconsistencies
here and here and then we should go
through all of them check if they match
and count how many times the new
prediction was right but as I told you
we don't have to because we have all the
statistics about that here in evaluate
model so there are some pretty nice
graphs and tons of numbers and as you
can see accuracy is just one of them and
very often it's very poor indication of
course if accuracy is very low your
algorithm is definitely not performing
well but it's just the beginning of what
you should look at mmm
imagine a data set that is not uniformly
distributed in terms of labels imagine a
cancer to a cancer research only if a
few percentage of people actually have
cancer so if your classification is just
assigning the label that person doesn't
have cancer it will have 90 something
percent of accuracy doesn't mean your
classification is any good if it's just
assigning the label healthy to anything
so there are different indicators to
show you if your classification in good
and remember I told you about the 0.5
threshold you can change it here because
you want more we want to be more certain
or changing bugs depending you know
which label was a and which label was B
and all those numbers are changing and
you can see how those statistics are
recalculated without you doing actual
calculations so this is very simple this
is how this has 1 2 3 4 5 6 7 elements
and this is never how it looks like in a
real life right let's have a look 7 6 I
don't I don't know how to count that's
why I need machines
it's very simple and I hopefully shown
you that in a simple way but I knew what
I was doing I knew that all I knew on
staff on machine learning I knew it
algorithms which was I knew all those
components and what they are doing and
I've seen her I know what it's expecting
from me like any tool it needs to be
taught or needs it requires learning but
it really really isn't stuff especially
when you're prototyping when you're
trying to find out oh is this or this
algorithm for this specific problem but
like with every ready to use component
you sometimes need to customize and if
you want to well customize to stuff that
are not there in the toolbox what what
we do what do we do in I don't know
development work we write some code
right you can do it here it's not C
sharp or Java it's either our Python so
I don't use our module this was our
model try to find it I'm looking for
yeah our script the names also are
changing so I'm not really attached to
them and I'll try to make it bigger
that's not the best idea you you would
have it's just a placeholder if you have
your our script somewhere develop it
there and then move it here because this
is not an ID this is a placeholder that
you use your script and it just shows
you some ways how you could port other
components that go through your code so
you use the MA ml map input port 1 2 and
you can also have an output if you have
your coat the same is with Python if you
have your call
and some package you can bring the whole
package there and just use it inside
there and so there is well I wouldn't
say the easiest way of customizing your
coat but it is very widely used I I was
working with the clients that were using
Azure and Mel only two to write our
scripts there and you would ask why
would anyone use this to lunch there are
scripts that are probably better way so
they did this for the second purpose I
told you that machine learning is
sometimes hard because they wanted an
easy way to productionize their models
so let's go back to our model and let's
imagine we are data scientist and we
want to give it to our developers so
they can do the predictions and how do
we publish this model there is a button
here that we can set up the web service
I think I need to run it again first
because I was moving stuff around it
needs to run again and we can set up the
web service so it says either I'm
sending a predictive web service or I'm
setting up the web service that will
retrain my model if my data changes but
let's set up the predicted web service
and what it does it will do some magic
now mine is just moving stuff around
looks pretty and so what it does it adds
two components you can see there is web
service input web service output and it
removed some stuff and put something
like this here so this cube here is
actually our experiment
and taken the built model serialized and
put here so that no one has to pick it
up and know how to use it because it's
all done here and it uses score model
because this is actually when something
adds the label to the prediction and it
looks like it's using the data set but
it's not because it's great right so
it's not actually using the data set
it's using the schema of the data set so
it needs to know what are the input
permit input values and then after
scoring it it just adds the additional
two columns to the prediction and the
thing with machine learning is it's
sometimes very confusing the separate
thing is when you're training your model
and you have it and another thing is
using this model so if you if we go back
to this predicting income if you're
training the model you're using every
column including income because this is
our label when you're predicting the new
values you want to find out about the
income the income is not your input
parameter so this is not doing the whole
job for you if you want a very nice web
service here you would need to go and
work on those columns because you want
to get rid of the income as an input
parameter I don't know if I'm explaining
it correctly for the output parameters
the default behavior is take everything
that was on the input and add those two
columns of the scored component but
maybe you just want the label and that's
it
you don't want to see the input values
maybe you do that's the choice so you
can tweak this or you can see how does
it look when it's just default lis setup
so let's see how does it look we first
need to run it
again few seconds yeah so now we have
web service running somewhere and now we
can deploy it actually now we have built
web service and now we have to run it
somewhere this is different tab as you
can see we were here at experiment now
we are at Web Services and this is
something that doesn't look very
self-explanatory but if you just go to a
request and response you have the whole
web page how to use it so you just can
just use curl or any rest client you
don't have to use Azure dotnet well just
curl it and it will work but you need to
build a proper request so you have the
shape of the request here you have
information how all the response look
like what are the data types and if you
still don't want to do it just go down
down down and you have a cost in c-sharp
Python and R on how to run this
particular web service and in terms of
authentication you have an
authentication key so only people who
know it will be able to run your web
service so you only give it to the
people that are out right so basically
your developers or even not developers
to your CI server so I don't think it
can be made any easier if you go back to
web service you can always check and see
how does it look like so as I told you
it took all of the schema including the
income at the moment because I didn't
tweak it I just left it as it was on
default and I won't run it now because I
don't know the prior the actual values
but if you run it it just caused this
Web API and the answer just shows here
oh I did there you go
yeah this was just about the experiment
created but if I test it now and don't
put any value some of the vows are
expected not to be no no no it actually
returns something so if you go to the
details yeah you can see about the
values but not all of the values I bet
because there are some missing one but
you probably could see it like in
developers tools but it's not that you
know it's not the production way of
doing it it's just for you to check if
your experiment if your web service
actually worked was deployed etc if
you're working with the business people
you might be interested in Excel stuff
it's like excel sheet already
pre-configured to use this web service
but but but as I told you it's just the
REST API call so you can do it from
anywhere and anyone can do it and I
think we have just a few minutes so just
tell you about my another demo so just
skip the slides if you don't want to
start from scratch there plenty of
people collaborating you go to Cortana
intelligence suite in the Cortana
intelligence gallery and if you're
looking for something my example is
Twitter sentiment you can see other
people creating solutions like this
sometimes they are even publishing their
web services so you can copy their web
service we've already trained experiment
and use it in your system if you trust
the data because if you think of Twitter
sentiment analysis this is something you
can train on any Twitter feed right but
it's rarely the case usually for example
you're doing predictive maintenance your
machines your devices will generate
different data than some people doing
predictive analysis on the Cortana
gallery so you need to train your
algorithm of your data but with Twitter
well text is taxed right unless you
really want to feel the sentiment
of some specific group then you need
your specific tweets and then you need
to go through them and label them which
is not a funny task so if you do
something like this I want to see how
does it work you don't even have to go
this one is actually very very well
documented but you know from your life
that it's not wearable the case if you
just want to see how it's done and seen
see free and it takes sometimes because
it takes their experiment and copies it
to my workspace and there there it is
there it is
I have my experiment in my workspace and
I can use it this one is actually much
more complicated it has stuff like
feature hashing feature selection
because working with text only recently
became a thing that we know how to
approach for years it was very very hard
task the same is with images natural
language etc but if you start learning
machine learning it usually is based on
numbers and labels so very tabular data
not the text data but as you can see
this is something that shows that a
dremel can also deal with text data it
has the capability has the components so
the last thing I want to tell you if I
go my slides last thing is about the
future of a dremel and I just checked
this week and it looks like it's not the
future it's the current state because I
was giving this talk a few times and
there are some videos flying around and
you see their studio and your studio if
you go there or the studio I've just
shown it looks completely different
because it's constant it's constantly
changing their new components
added some of them are deprecated new
features added so you're creating your
experiment and you want to troubleshoot
and see some stuff it's improved
massively
so now the error message is much are
much more meaningful and actually can
point you to what you did wrong but
again these are like syntax errors so
it's like you plumb those components
wrongly it won't tell you that well
you're trying to do regression and using
classification algorithm it won't tell
you it's it's just wrong in terms of
machine learning itself and history in
version control so for the very like
recent few few weeks back like two weeks
back the only way of you seeing what did
you do what how did you change the
experiment was to look at the history
because there is like a log file that's
showing what you did while creating and
changing the experiment now you can you
know download your experiment in a JSON
file and put it in any version control
you want and have it in your CI process
or just to collaborate with other people
or just to you know control the version
of your experiment the same is with web
service it's also serialized and you can
put it in a JSON file it's still not
great because you need to use some
PowerShell scripts but it's getting
there because everyone is asking for
that and if you don't like drag and drop
if you're a coder you can use Jupiter
notebooks so you can use are you can use
Python and its color I think I only used
for use it for R in Python so you write
your code and using any library you want
keep key to learn or I don't know PI
spark some machine learning libraries
tons of them out there and then you just
publish it as a web service
and this is what the client of our was
doing they were just putting their our
code there and using publishing web
service capability because it was so
easy for them to productionize what the
data scientists were doing it goes back
to imagine you're producing your R or
Python code somewhere completely
different not in a dream oh not in a
dream l Jupiter notebooks somewhere L on
your computer and you want to publish
this as a web service there are there
are packages which is publishing is all
you need to have you need to have a
workspace obviously because this is how
it's hosted and then you say publish
this code in this workspace so then you
have your model built on your laptop
productionize so to summarize we talked
a little bit about machine learning and
chick sexing I shown you my two concern
about how machine learning can be more
exciting and then I shown you the table
and I encourage you to go learn try it
out and collaborate and Cortana
intelligence gallery this is all thank
you so much for joining me today and if
you have any questions I'll be around</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>