<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Easier AI allows everyone to build smart apps - David Burela &amp; Azadeh Khojandi | Coder Coacher - Coaching Coders</title><meta content="Easier AI allows everyone to build smart apps - David Burela &amp; Azadeh Khojandi - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Easier AI allows everyone to build smart apps - David Burela &amp; Azadeh Khojandi</b></h2><h5 class="post__date">2018-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MxnGYtFVEzM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good day thanks for joining
attention my name is Earth and I work in
commercial software engineering team at
Microsoft I have more than 10 years
experience in developing software
I'm coordinator of girls geek Sydney and
girls dotnet and this is David Wright so
I'm also in the commercial software
engineering at Microsoft Australia as a
day and I are both in the same team so
today what we want to talk about is how
is your a I can allow anyone to build
smart applications so so far at NCC
we've seen a number of AI based up
sessions we've had Barbara Barbara
talked about tensorflow and showed how
you can go and build deep machine
learning models with tensorflow
that went way over my head it's way too
deep for me then there was a session by
tests and tested a great session for an
intro machine learning for a software
developer and I was able to pick up a
lot of things from there I picked up a
lot more about what regression test
linear regressions are all about and
then this morning the keynote we had
Scott Guthrie talked a little bit about
Microsoft's AI cognitive services things
like that and so our session today we
wanted to show that there is a whole
landscape in AI but we want to show that
you can go and just use somebody
services straightaway with that and
having to know too much about what's
happening under the covers so our
session goals for today is we just want
to show that super easy to add machine
intelligence to apps straight away with
and we want to show you some examples
that we've built and explain the
architecture that we use to build these
little applications and so our agenda
for today we're going to start off by
looking at the continuum of AR services
that are available to put some
perspective on which bit we're going to
be focusing in on we're going to be
focusing on the azure cognitive services
just on this piece for the entire
session which is what Scott talked about
this morning and then we're going to go
and show some sample up some
applications that we built and walk you
through the architecture of them now
we've got an entire spectrum of
different things which can be split up
into three broad categories
we've got the AI services AI tools and
IA our infrastructure so yesterday when
Barbara was talking about tensorflow
that's for people who have some type of
idea of how machine learning works they
know what that they want to build up
their layers and tensor flow models so
all those tools are great for people who
know how to do machine learning that's
not me
you've got also got AI infrastructure
and this is so these are the tools so
you can build a higher level constructs
down here is more of the lower level
building blocks so we've got a place
that we can store all of that raw data
either in databases and data warehouses
we've also got the really low level
compute services I to the pet Apache
spark batch a I service the new IOT edge
where you can run AI at the edge and as
your container services we can take any
machine learning module throw it into
and aqus like docker container then
throw it up to be scared up massively
now and all of these can be run on
either CPUs GPUs or field programmable
gate arrays so we've got a bunch of
articles showing how you can go and
accelerate your machine learning with
our virtual machines which have GPUs
baked into them but we've also got the
field programmable gate arrays which are
in preview at the moment which will
allow you to massively accelerate the
training of the AI your AI models now
again these ones for great for people to
know if they're doing for me I focus on
this area because I don't know how to
create my own models and so what
Microsoft has done is these AR services
we're trying to democratize the use of
AI to every developer so what I've done
is I've taken some machine learning
modules that they've created they've
gone and pre-packaging them up and
they're giving them out as a service C
with a single API call you can go and
call into these now the way that them
live being split up you can kind of
think about it in the same way that a
human will try to understand the world
themselves so a human but me if I'm thi
to figure out what's happening here in
this room I'm gonna be doing things
roughly with vision to try see and make
sense of what's happening here in the
room with speech so the what I'm doing
right now but you guys are having to try
to understand my speech
we've got language knowledge search so
I'll dive into each one each of these a
little bit more so the vision services
again is about trying to have a computer
makes sense what's happening in either a
picture or a video feed and you score
Scott go through show off some of these
this morning the idea these really
quickly are things so computer vision
making sense of what's in an image face
a face and ap emotion ap is to single
out people's faces and see the age
gender and emotion
we've got speech so whereas vision was
taking in an image speech is taking in
an audio stream and with this audio
stream it's used to try and figure out
who is saying something and what they're
saying and converting it all into text
once we have some text we can go and
look at the language services and these
language services are taking in text
either from that speech service or
taking it in from a chat bot and so a
lot of the hacks that as an i working
with customers where we're building chat
bots we use a lot of these services to
try to understand what a customer's
saying I've been typing in so we've got
Luis which is trying to derive in an
intent from what the user has said they
say can I please book a flight we try
and figure out that their intent is to
book a flight we've got text analytics
to try to get sentiment out text
translation to translate between
languages etc so the first three were
trying to make sense what's currently in
the world or try to understand what
people are trying to talk to you about
the next category knowledge is all
around trying to draw on your wealth of
knowledge to go and answer a query or
question that someone has asked you so
we've got a Q&amp;amp;A maker which is kind of
like a giant if
q database you can throw in all your
questions and answers and then you can
just attach a chatbot to it that way you
don't have to have the exact question
that the person will ask but we use a
whole bunch of phazon fuzzy matching and
train up our models so that we can try
to understand what a user is querying
and asking and bring them back the
correct answers
we've got recommendations API which is
just like with Netflix we've seen that
you've you watched these type of movies
so you'll probably enjoy this type of
movie as well so test gave a great
example yesterday of clustering so a
zombie movies people who like zombie
movies will probably like these movies
people who like Brad Pitt movies
probably like these Brad Pitt movies and
there's be some people who because they
like both of them will like what was it
World War Z but had Brad Pitt in zombie
movies and we've also got intelligent
search intelligent search where we help
people be able to search and find things
better either through images web
searches custom entity searches so being
able to do intelligent fuzzy matching to
try and help users search now what is an
I wanted to do next is we just tried to
give you a whirlwind overview of what
the Cod what type of cognitive services
are available and as we've said there
are five broad categories with lots of
services under each one of them what we
wanted to do is just show how to use one
of them from first principles just to
set the scene and level set and then all
of our demos from there we just assumed
that you know how to do that first bit
so what I'm going to do is jump online
now the scenario I want to do for this
one photos tacos so my team down in
Melbourne just as a team bonding
exercise because we're never in the
office together we've set up her thing
so every Tuesday we go down and get
cheap tacos for Taco Tuesday we took
this selfie
and although I'm pretty sure we're all
enjoying tacos I want to make sure that
they're not faking it so I want to try
and use some emotion analysis to figure
out if they're all truly enjoying their
targets
so the first step I would do I would
jump up onto the azure cognitive
services website and you can see all the
different categories there that I was
just talking about if we jump into the
vision ap is the one that we want to use
is the emotion API so we can click on it
and we can see all of that you can just
we've got a demo app here you can just
do it live on the website just to try it
out and see how it works
but we're devs so we actually want to
dive in so we can go into the API
now all of these cognitive services are
or just a simple rest service so again
going back to what gets super easy for
you guys to be able to just take these
services and just drop them into your
application they're all just there by
arrest so we can see what we need to do
is just do a single post to that URL
pass in your subscription key passing it
the image that's it
now that's okay but what we need here is
a subscription key we've got two ways of
doing it so for a lot the bulk of these
services we've got free keys that you
can use so if you jump onto as you slash
try cognitive services you can go select
any of them and go and get get an API
key and you can see we're really
generous with them you can do 20 calls
per minute for 30,000 transactions so
that's one way of doing it another way
is if we jump up into the azure portal
so again I only want to show this once
to level sets make sure for those people
who haven't seen the azure portal so if
we lock onto the azure portal we can
click create scroll down to AI and
cognitive services we go and have a look
at all of them so you can see we've got
all of the other things up here that I
showed and that be
slide we've got Azure machine learning
labs bot services Dawn's we care about
other cognitive services and you can see
we've got all of them available here so
you select one emotion API accept the
legal notices give it a name in DC
London emotion - and you go so we've got
the pricing so and we've already used
our our free one them in our
subscription so which you can see which
the free one gives you 30,000 calls a
month or for a couple of cents here we
go it's about 13 cents for every
thousand calls that you do to the
service we'll just select that one
create a new one in DC London I have
read that and click create so for each
of the services that you use you're
going to go and have to create an API
key so that you can go and call them
that's just that we can charge you them
but one good tip that has a day just
showed me was what she did she went
through run one free she actually went
through each of the services and created
each one of them sending them to the
free tier so whenever she needs to she
can go in here click on one and grab out
the keys that she needs
so whenever she's playing around doing
her demos doing her around sample code
she just jumps back into here she who
uses these keys I'm up until our
customers who requires their own key and
they need to do more than 20 calls a
minute and they go and didn't just pay
for the 13 cents four thousand a calls
for the emotion API so if we look back
up here we can see that that provision
my key I can go to it
throw it away it shows you all of the
links so how you can go in find more
documentation look at the api's but what
we care about is we click overview we
can see the endpoint URL and the keys so
if I copy that key we're going to use
postman the quick show of hands how many
people here have are currently using
postman awesome like 90% that all we're
going to do we're going to a pre pasted
that URL in there we go to headers
subscription key paste in my new key
paste where are we body binary Taco
Tuesday hit Send
that's it so again wrist wrist call we
pass in the URL passing our subscription
key pass in the binary clicks and boom
now let's have a look at this I'm in the
photo we've got one two three four five
faces if we have a look through here we
can see we've got the first face so it
gives us the the cordons for the face so
you know a square where the face was so
we pass back where the face was and all
the details about them how they were
feeling so we've got you go the first
person 99.9% happy that's what we'd
expect second person 99.5% happy third
person is 99.9999 and this one is only
97% so we're gonna have to force-feed
that guy a few more tacos to puppies the
happiness by like three more percent but
that's what Bob would quickly want to
show so again the point of that demo was
to very quickly show you with a single
rest call we could go and get some
analytics out of an image straight away
and just again want to level set to show
how to get a subscription key and how to
call it raw in post map now what I'm
going to do is
so as she's going to demo a few of the
other services yeah I'm going to show
you two please ah so the the question
was what's the difference between the
face API and the emotion API the face
API will give you the faces and try to
give you some demographics about about
that person so age gender the motion API
is more around so and the face API it
returns back the location of your for
example your eyes your lips so if you
want to do more analytic so if you use
it in unity you can use and upload the
photo of the person and you can know the
location of leads if you want to do more
things for exam all right so I'm going
to show you two more API that I'm really
excited about it and you might say oh
that's that's boring but I'm excited
about it what is describing the image so
imagine that you have an image that you
can give it to the computer and computer
can describe what it sees in the image
and why do you think it's important how
many of you have been the website
website almost everyone and we know that
how important it is in an image tag they
have alt text we know that it's
important for text readers that we have
alt text everyone knows it content
editors that are using CMS they know
about it it's you know it's everyone
knows it but I saw so many shocking
websites that it passed accessibility
test but the alt text is image one this
is an image this is shocking and we
cannot solve this problem easily by
using one of the cognitive services so
if I go for example select this image
hang on I want to open is me this is my
dog cookie I really like cookie and
cookie has Instagram account and
surprise surprise he has more followers
on Instagram than me on Twitter I don't
know why but anyway so this is my dog so
if I go and select my dog
the API so it's a custom vision API and
it year it uses describe and it says
that okay the maximum current candidate
so give me two different description of
the photo so Dave showed us how we can
add the subscription key so I skipped up
and I'll go and select my son's photo
here and I send and what do you think it
would say this is the photo of dog or a
dog wearing a costume a brown and white
dog wearing a costume so you can see
that how accurate it is it's very
awesome so you can use it so I give you
an idea for the plugin that you can add
a sidecar on Brock of WordPress or
whatever CMS that you are using you can
write a plug-in that automatically
suggests the alt text for your media
library and this is not only the API we
are using it internally so if i go to
photoshop a powerpoint and if i go here
and you find in there and i'm going to
select one of the common bears that you
can see in australia oh now i'm going to
select this is the mascot of australia
dotnet website so let's see what what
powerpoint adds it so if i select a
photo you can see it automatically adds
the description for the photo a kangaroo
and standing in front of the body of
water so we are using the custom vision
API is internally not products so you
can be confident when you use this
product that these products are tested
we are using it internally and also
another API custom vision API it
analyzed the photo and it gives
the location of the main object and it
can gives you the design idea so it
gives you that ok what's the dominant
color of the photo and you can see that
it provides some design idea that you
can use in your Photoshop so if you
don't have design taste there is no
excuse now we can have a better slides
alright so let's go back to
demonstration another thing that I like
to show is how many of you use search or
indexing stuff like plastic
Lusine so you know that searching is
really so searching text is easy
what about searching images you know you
rely on user to provide some information
about the image but what if if you could
draw kind of like analyze the image and
see that what exactly it's in an image
not the description tags so here you can
use analyze and you can use visual
feature tags and categories and this if
I select another photo so I am going to
select third and I send it
so it it categorizes animal bird so you
can see the taxonomy in the
documentation that's how it does the
taxonomy and it returns about tags so it
returns back its seagrass confidence
highly confident it know it's outdoor it
know its animal it know its paired and
center field so you can see that it's
returned with a confidence level and the
name of the tag this API can understand
around two thousand different objects so
you can see that how you can analyze
your photo and use it for your indexing
so in your search when you have search
functionality that users search for the
image search for something so not only
you can search the text you can also
return back some images based on what's
on the image good question I don't know
you have to test yeah yeah yeah so
probably it returns so you have to read
the documentation about the taxonomy and
tags and you can look into that what it
returns by I'm confident that it returns
but I haven't tested it you have to test
another thing that I like to show is if
I go to the caption vision and I choose
the bear photo again nice and safe it
only says a bear standing on top of the
grass cover the field but it doesn't say
what type of beared so later we are
going to Dave and I we are going to show
you how you can be more specific what
type of bird is the parrot or is it
something else so you can you can be
more specific about it
how many of you have been in G not
everyone all right so I'm going quickly
to show you about the sentiment so you
saw the house God you showed you how you
can use sentiment and on the sea to
analyze tweets so you can use sentiment
analysis to analyze reviews so I realize
that when you have app or when you have
some sort of providing the feedback or
testimonial on a product some people and
then you provide most of the time you
provide to to select between one to five
and the description some people although
they provide a positive sentiment they
choose one have you read the body did
you know what it because they want to
make sure that when you provide one and
it's very terrible people read that
description because I didn't wanna know
what's why it's bad but they write a
good stuff about that product or that
hotel review so if you want to only
wanna show and show the sort the
products based on the reviews sometimes
it's not fair so I thought that it's
good idea to show you that how you can
use and solve this problem so you can
use sentiment analysis to analyze the
text if it's a positive or it's negative
and what it retains it returns a number
between 0 to 1 if it's terrible it sends
close to 0 if it's very good it returns
close to 1 and also it can analyze a
sentiment in different languages because
sometimes so in pass we have to
translate from English so if the text is
in for example in Chinese you have to
translate it in English and take this in
and get the sentiment based on the
English but sometimes the sentiment
might lost in the translation so it's
good to trance get a sentiment based on
the language and you can do the bulk
sentiment so you can provide multiple
multiple sentences so in the sample I
have one Texas that I had a wonderful
experience the room were wonderful and
the staff was helpful so and it's
probably no it's positive and then I
have the Chinese equivalent of same
sentiments and I have one sentiment in
Farsi and let's see that how it goes
so if ice
hit Send so it returns back the result
quickly so we know that the first one
was good the second one was I had a
terrible time at the hotel the staff was
rude and the food was awful
I'm sorry for that guy but anyway so you
can see that this is really terrible
sentiment and exactly the same so the
Chinese version exactly same thing and
then you can see that the returns error
for the fifth one because our service
doesn't support Farsi and returns right
oh you only support these 18 languages
so you can use these 18 languages or you
can translate it to English and get the
sentiment based on that all right I hand
it over to Dave it returns back to score
so it returns back a score between 0 to
1 so if your score is close to 1 it
means that it's a positive if it is now
it's so the people came the question was
that the 10th that's coming in any able
to see if it's positive or not no so we
go and rate the text that comes in on a
scale 0 is very negative 1 is very
positive point 5 is neutral and so the
school that comes back it's it's on that
scale
okay now what we've shown so far were
the Royal cognitive services and how can
you use rest to go and call them just
very directly and as showed briefly how
we are using it within our own products
of Microsoft inside of PowerPoint for
alt text and a I'd slide design
suggestions we also use it in PowerPoint
for live live translation now the
problem is you're probably laughing at
how bad the text captioning is at the
moment it's because the microphone is
all the way over here on this laptop and
because I'm too far away the microphones
not picking it up it's so good but one
cool thing about this is there's a URL
here translate it / and then this is the
session code so what we do it some other
when we present overseas in other
countries if you bring that up we will
go into live translation into a bunch of
different languages so I can speak in
English I've done I'm in China a couple
of times we're speaking English and all
the attendees can have the app open and
they're reading the live translation in
Chinese again we just have to get a
Bluetooth microphone for next time for
this one now they'll be shown how all of
them used what we're going to show now
is two applications where we've gone and
taken those individual cognitive
services and then mash them together to
create some sample applications the
first one we've built is called menu
Explorer now as an I go and travel
around a lot and when we travel we like
to go and try the local cuisine we like
to be adventurous try some of the local
places something that happens a lot
though is we be looking at the menu so
even if it's somewhere here in England
for example I look at the menu and I
don't know what a local dish is like a
Yorkshire pudding don't know what that
is and so we wanted to build a server so
when you're travelling around you can
look at a menu and try and figure out
what is on the local menu
so keeping with a whole we're from
Australia theme there's a famous
restaurant in in Sydney which is there
overlooking the Sydney Opera House
and of God's really fancy models cuisine
it's so fancy that I don't know what
some of the dishes are and they're so we
thought would staff there but analyzing
one of these models did menus so we
created an application called menu
explorer I hit browse go to the desktop
we bring up the menu so using a number
of different services here we're first
one using is OCR so it's going to once
the conference Wi-Fi picks up we're
going to go through it's going to detect
all the text that's on the menu and then
we can go and try and figure out what
what we want to figure out now looks it
all looks fancy we've got southern squid
because it's different from north and
squid but we've also got a bunch of
desserts and I have no idea what a snow
egg is now what we've done is you can
click on it it goes and it knows so
again we're using OCR to detect whether
all the text is clicking on it to pull
it out and then we're using what of the
the cognitive service for the image
search to go and bring back examples of
what of snow Eggers so that's pretty
cool I'm looking at the menu now don't
have to try and that'd be embarrassed by
asking the waiter this fancy restaurant
what is a snow egg another one is so
we've got this one OCA
I save in a hat pronounce that and I
click on it and it's bringing back
pictures of like goose and some Veggie
so what we've done is if the first
couple of images don't bring back what
you would expect you can just click on
the text and we'll go and launch both a
being and a being image search and a
Google image search to try and give you
a bit more of an idea so looking at
being it looks like it looks like a type
of vegetable and Google Image Search I
think that's a goose but but I get an
idea that is probably a type of
vegetable so again I don't have to
embarrass myself
okay now that's nice and easy we're just
doing simple OCR with image to text and
we're doing an image search so that's
two services so far so we want to throw
in a third one now when we travel to an
english-speaking country this is okay
but because we're in Asia Pacific we
travel to a lot of Asian countries so
China Japan Korea etc where we can't
even begin to understand these menus so
if we click browse I can go and I can
load up a Chinese menu we give that a
few seconds for the conference Wi-Fi to
pick up but we'll do the same thing
it'll go through it'll go and do OCR but
we support OCR in multiple different
written languages we do it our printed
font to text we also supported on
handwritten text so if you have a bunch
of notes from someone who's taking
conference notes you could do OCR over
that as well
now we've gone through well we've
figured out all this and I have no idea
now if I click on one of these click
scroll down we do the literal text
translation fish head with chop to feel
it Chili Peppers but we've got images
that come through now one important
thing that we're doing here is we are
doing the English translation to try to
give you an idea of what the dish is but
a lot of dishes a literal translation
will not work at all so what we're doing
was we're doing an image search based
off the original text and this was
something that whenever I was overseas I
could never do because I don't know how
to type in these characters I wouldn't
be able to go and do a search for what
what is this dish so we can click on
some other random ones radium eggplant
we've got this one sesame taro so if I
just saw that the two translations test
me taro I wouldn't know what it is but
looking at these images they go oh yes I
know that dessert so it's actually my
wife's favorite damn dessert over in
China so we know what that is so we
thought of this was I'm an important
to show that this was just three ref
service calls bein by embedding it into
your own application can go unlock
things really really simple and to show
how this was built I'm gonna hand over
to as is gonna deep dive into the
architecture thanks David so the source
code of the menu explorer it's available
on github you can have it look and play
with it as you can see it contains two
project are you want to take photo after
we share the link later after the
presentation so as you can see it it
contains two project the first project
is a client-side project it's basic
node.js web application content
javascript jquery and html5 and the
backend part is Asha function and it
contains three function for those who
are not familiar with your function as
your function is Microsoft service
offering and what it's really good about
either function or any service is you
care about the function that you care
most all right in strong in English by
any way you get it so you write a
function and you define what event where
it runs the function and you set so you
don't need to worry about infrastructure
scaling or you don't need to worry about
the whole web project to write it you
only focus on your function here I have
three functions or and they're all hash
TDP trigger means that if any HTTP calls
raise it runs that function so the OCR
one it gets a user uploads the photo the
menu Explorer a client application
handles all of the user interaction gets
the photo senses to the OCR as audio see
our agile function sends it to the
computer vision API computer vision API
analyze the text the image extracts the
text and the location of the text and
also it returns back the language of the
text if it's not in English because the
translate function and a translate
function translate
the text into the English and if you
want to know more about the menu item we
call being image search and we return
back top six relevant image the reason I
use Azure function because I wanted to
protect my subscription key you might
say oh you might you could have called
it's API I could have called it here in
menu explorer and yeah you could you
kind of expose your subscription key and
I didn't want to do that in the next
demo I'll show you that how you can use
and call the API directly in the app if
you like so if I go and deep dive to the
code
so get the client-side but there is
nothing very interesting happen about it
it's one single HTML page and I only
reference jQuery JavaScript put a strap
and the reason that I didn't use any
JavaScript framework because I wanted to
make sure anyone with any skill set can
understand and use the code and the
whole thing that happens in analyze
image so in analyze image when when user
uploads the photo I call OCR function
and as you can see in OSI are calling
the agile function HTTP trigger it's
exactly same as calling normal API
there's no difference so it's normal
post code normal postcard and if you can
see that as I have my URL here what I
really excited about as your function is
you can run and debug them locally so
you can see that I hope I hooked my
local environment
to use my localizer function so if I
call it if I run it I can start if I go
to my add your function app so you can
see I have three different functions and
they all do the same so what I really
love another thing I like about
cognitive services is they are
consistent so there are different teams
but there they are exactly doing the
same thing so you you need only to have
a path and you need to have a key so
that's all you need to do so this is my
search function my OCR function so I put
a break for so you can see how easy it
is to call it but as you can see it gets
the path and I add substitution to
header and that's it the only difference
is in translation API that I call the
API and I get it but there it doesn't
return JSON and it turns XML so the
extra step that I do I pair our parse
XML
returns back and returned back to Jason
so if I run the agile function
application so nice logo and as you can
see I will have three endpoints for each
of my HTTP three gear functions so if I
run my menu externally locally and if I
load any photo any menu photo and boom
stopped here and I I have all of the
debugger functionality that visual
studio provides for C normalcy shot code
there's no difference it's very powerful
so you can debug it you can step through
you can look there and you can see and
in the background yeah that was too
quick so in the background it calls the
translator API and then sure adopt all
of the English menu here so if you do if
you get dizzy for seeing the different
language so you can see the English
material and it over to David
now one of the beginning demos when as
was shown cognitive services I shall
order the photo of Australia native bird
and as she explained it only came back
with its a bird now that's okay that's
what we wanted to do is show that you
can use the cognitive services vision
API to go and get generic information
back so such as it's a bird but we want
to show that there's another service
that you can use custom custom vision
service where if you have a domain
specific set of images that you want to
identify such as a bunch of different
birds how you'd go about that so we
built another demo for this
I'm gonna check it the bird from the
background okay now what we decided to
do was we wanted to show that it's
really easy to use these services and
consume them in anything so the first
demo ash that we showed was how to add a
website and just to show that it's the
same everywhere we decided to do this
next demo in ionic so we've built an
Android application which is using ionic
which is going to call one eighty one of
these cognitive services they go select
an image pick one of these birds it's
going to come back and it's gonna come
back and say with a hundred percent
confidence that it's a rainbow lorikeets
which it is and we can go through and
select a few of these other images so
we've got a sulphur crested cockatoo
that one's a little bit lower can try
this and we can see that it's a it's a
gala now I want to show some of the
basics of how this technology works and
then I'll hand it over to as after to
show the architecture of house you built
this ionic application if we jump back
into the onto the web we'll go back to
Cognos services we're going to go into
vision and we can see that there's one
day out here custom vision service so
the point of custom vision service is to
go and train it up again in your own
domain specific area of images and get
to understand your your subset of what
you care about so if we go over to here
so if you log into our custom visions on
AI and you've got all of your custom
vision projects so we could create a new
one but here's birdlife the one that
we've created before if we load it up so
the whole way you use custom vision is
you click add images we'll see that in a
second and you add some tags so we've
gone through we've uploaded a thousand
images of 38 different Australian native
bird
if we look through we can see we've got
some some gala photos it's so the exact
one that I just uploaded we didn't use
for the training data now well when you
here you can click train etc but I want
to test another bird so if I go browse
go back we've got an Ibis now if you can
see the name of the files called our bin
Chuck
so in Australia in Australia we've got
some this bird called our Ibis now one
thing unfortunately with Ibis is we've
been in Australia a lot of the places
where they would normally live we've
been building houses and things like
that and so they've been forced to live
in more of the urban areas and they just
go and rummage around in our bins and go
and eat everything out of it and so
locally we just call them bin chickens
now let's go and see if cognitive
services has any idea what have been
been chicken ears so they come back and
it has absolutely no idea what have been
chicken is but we can fix that so to
show you how we would add another one in
here we could click add images click
browse I'll select all of them except
for the one that I want to test call an
Ibis add the tag so you could add
multiple tags in here if it's more than
one thing can upload we just wait a
second that was quick we hit train it's
then gonna go off and train up that's
that model based on all again those
thousand and or one thousand and eight
images could we just uploaded uploaded
eight more just take a few moments to do
that and then what we're going to do is
based on the new model that it trains up
we're going to just quick test it again
and see if it detected it there we go
you can get the stats for each of the
birds quick test upload my bin chicken
again
well something has gone wrong demo
that's normally would see that the
probability has gone up but later on as
we'll show how if you were to train
something like that so I only uploaded
eight images and as we show you later
how we could go and improve the
probability detection of it so a half
pass over to as we show the architecture
thank you
so it's a very basic app that I wrote it
in ionic I wanted to be adventurous and
learn new stuff so that's why I chose
ionic so the architecture very basic in
I have one an ionic application and I
use directly custom vision and REST API
need so if I want to show you the code
so here is my ironic application in in
source in home page I have classify
method and here I call it's very basic
it's it's only copying the HDD post I
add the prediction key the header and I
call the method that's it and I show in
a home page I showed top five prediction
so you can see here your top five
prediction it's very cool if you haven't
used iron ik before its I really highly
recommend to use it and you can export
it to Android and so you build the
mobile application with the language you
know sign I already know angular and
typescript and JavaScript so I built the
application by using ionic framework and
I exported to Android and web
application so it's very powerful all
right
you might have 1,000 images but did you
get the thousand images of the birds so
the whole story started at me and they
wanted to classify Australian native
snakes and I started brought in the web
page after the unloading few or two
images of snakes I realized that I'm too
scared I couldn't see them yes
no offense to snakes but they were after
anyway so I said all right so I I cannot
do it manually they looking at the image
of the scared scared me see so I wrote
the image downloaded that I used email
being image search and Google Image
Search to download all of the photos
that I want so I wrote an application
and again it's available on github
report a repository that you can put a
give the CSV file and it goes through
the CSV file and download 30 image for
everything and then you only that you
know the only thing that you need to do
is you go to the folders and manually
check that it's the correct photos and
then upload it to custom vision api's
but even if the uploading the process of
a snakes I realized that now I'm not
enjoying doing it and me and they've we
did a quick brainstorm and sir okay
let's classify Australian native animals
that it's cute and doesn't kill you so
unfortunately it rules out kangaroos and
Carlos and it left us with pets and we
started classifying the parrots so
you're not story now so Australia has
eight states and then if you see that hi
so how the different parrots are visible
or you can see that in different states
so gala Laura Kate and cockatoos are
very it's very common to see so I wanted
to test my model but I wanted to make
sure that people that are using my app
definitely they take for the top three
or top four bears so I want to make sure
that my model works perfectly but those
four and then for the rest I obviously
can be more lenient so this is my
confusion matrix for those who are not
familiar with confusion matrix it's not
very difficult
it's an XY coordinate through label
predicted label and what
you need to do is if you have a diagonal
it means that your mother is good if you
see that your mother is confused so you
can see the dots so for example here it
doesn't very good for a stand
grandparents so you see that it couldn't
even recognize it well so that's my
confusion matrix now you all have the
bad vibe hot dog or not yes so I thought
that it's very good to build a quick
classifier for hot dog or sausage that
we me and David we upload all of the
parrot photos manually so you can
imagine how long will it takes to upload
all of a thousand images to the tool but
I realize that there is an API that all
of the AP are custom vision is using is
available so I can literally upload the
photos automatically so we build an
application that you can feel a hot dog
or sausage dog in few minutes
so all I need to do is clone there so if
you want to so custom vision generator
source code is available on github you
can you can clone it and all you need to
do is you pass your keys I need to close
the other one
and in the tags come on computer you can
do it
so in attack I added hot dog and - hand
and I hit run and wait for the magic so
what it does in the background it
creates a project in custom vision tool
for me it tries to download the images
and it starts adding the tags and it
uploads to the custom vision tool and it
trains the model for me and it tests the
model see that how good the model is it
returns back the confusion matrix for me
so you can see that the that's what we
want it's a diagonal so I had three -
hands and it's all predicted - hand
which is correct and I had three hot
dogs it's predicted the hot dog you
might say as you might all faked it it
and you just show some random text for
me so I am going to show you in the tool
so if I go to my custom vision you can
see it's created the model for me the
custom vision API it's very powerful for
building a image classifier you only
need to provide two tags and five images
what it does in a background it uses the
transfer learning of the other API that
you saw earlier the vision API so the
model of the vision API it's exactly the
same model as you're using for the
custom vision but it uses transfer
learning to make sure that it
Classifieds your images so let's test
the model
I see so if I go here let's copy the
easy one is it a hot dog yes let's see
that what mother says yeah hundred
percent
seriously let's not 1 minute you have a
classifier hot dog or sausage dog how
cool is that
so you can go with the cat or koala or
London Bridge or Harbour Bridge or you
can have anything so is it a sausage dog
yeah 38 person sure definitely not hot
dog what about this one well we don't
know it's a business decision it's not
hot dog or it's not sausage dog but you
can make a decision so you can train the
model to see that oh I want to have it
both so it's definitely a sausage dog
and it's a hot dog so you can do you can
train it as you want so the but even so
to show you that we have time or no we
don't have time now we'll probably jump
to the next yeah
everything we've shown so far is law
assuming you've got a connection to the
cloud and all of them have been rest
call to go up into Asia and then we
return back the results to you and
that's fine and when you've got fast
internet connections but there's many
many scenarios where you want to go and
do that ái processing go towards the
edge which was way now marks off we
talked a lot about the intelligent cloud
and the intelligent edge so we've got a
number of ways you can go and run the
these AI models just like these AI
models so we've got on the first one
which is called IOT edge this is our
service - for IOT devices which are at
the edge are being able to push down
machine learning models down to these so
down to raspberry PI's down to your
embedded controllers so when we're it's
really close to the data so you're able
to process a lot more data locally to it
rather than having to stream all the
data up into the cloud another one is
the custom vision service that has just
showed we can actually go and export it
out to a core ml or attempted flow model
so you can run it on mobile phone
devices so again doing it locally but I
mean disconnected States so we get as to
show how to do that all right so I
quickly show you how you can export the
model
into ten Sephirot so what you need to do
is you need to click on the card and
change the domain to compact and Save
Changes
alright common internet and then I go to
performance and training so it tries a
new model based on the compact version
that you want to use on your phone
because probably you don't have internet
all the time or you don't want to call
the internet so boom done you create a
you exported and you can export it to
core email or telephone model I go with
the tensorflow model
download
so this is my download if I copy the
model and put it in my tensor flow
sample and paste it here
so do you remember so which one do we
test so we test with that - a lot so if
I say a Python Tesla hot dog
all right taste with the hard I try turn
clear see if I can type
and to 1.jpg so it runs a Python script
for me based on the model that I
provided so it reads the label and it
classifies the model as a hot dog 100%
positive so you can see how powerful it
is and how quickly you can create a
model you can export it and use it in
sensor flow or you can use it in caramel
I think the batches on the things gone
okay so before we wrap up I just want to
go over the session goals what we're
trying to show you today is that a super
easy to go and embed these services or
to your applications without having to
know too much about regression linear
regressions or clustering and things
like that
you've got services you call a REST API
and you can use them now one thing we
want to close off on was one video so
we'll show one two minute video and then
we'll bring up a slide with them a link
to our link to our github repo with
links to everything now
everything that we showed was showing
we've got local services that you can
run in the cloud and we wanted to show
what could happen if you combine all
these together into a real real world
application so if we'd rather video
I'm Sachi Shaikh I lost my sight when I
was 7 and shortly after that I went to a
school for the blind and that's where it
was introduced to you talking computers
and that really opened up a whole new
world of opportunities I joined
Microsoft 10 years ago as a software
engineer I love making things which
improve people's lives and one of the
things I've always dreamt of since I was
at university was this idea of something
that could tell you at any moment what's
going on around you
I think it's a man jumping in the air
doing a trick on a skateboard
I teamed up with like-minded engineers
to make an app which lets you know who
and what is around you it's based on top
of the Microsoft intelligence api's
which makes it so much easy to make this
kind of thing the app runs are on
smartphones but also on the pivothead
smart glasses when you're talking to a
bigger group sometimes you can talk and
talk and there's no response and you
think it's everyone listening really
well or are they half asleep and II
never they I see two faces 40-year old
man with a beard looking surprised
twenty year old woman looking happy the
app can describe the general age and
gender of the people around me and what
their emotions are which is incredible
one of the things that's most useful
about the app is the ability to read our
text thank you
I can use the app on my phone to take a
picture of the menu and it's going to
guide me on how to take that correct
photo move camera to the bottom right
and away from the document and then
they'll recognize the text read me the
headings I see a appetizers salads
paninis
pizzas pastures years ago this was
science fiction I never thought it would
be something that you could actually do
but artificial intelligence is improving
at an ever-faster rate and I'm really
excited to see where we can take
as engineers we're always standing on
the shoulders of giants building on top
of what went before and in this case
we've taken years of research from
Microsoft Research to pull this off I
think it's a young girl throwing an
orange frisbee in the park for me it's
about taking that far off dream and
building it one step at a time I think
this is just the beginning so that
application is actually available on
iPhones so you can use an iPhone to help
you with now we're going to close off
there there's a URL to a github repo
which has links to all of the samples
and I put in demos that as well showing
off and well that will be around for a
few minutes if you want to ask questions
after thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>