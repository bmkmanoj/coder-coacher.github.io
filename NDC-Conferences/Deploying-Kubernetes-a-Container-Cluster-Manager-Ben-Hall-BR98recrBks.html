<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deploying Kubernetes, a Container Cluster Manager - Ben Hall | Coder Coacher - Coaching Coders</title><meta content="Deploying Kubernetes, a Container Cluster Manager - Ben Hall - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deploying Kubernetes, a Container Cluster Manager - Ben Hall</b></h2><h5 class="post__date">2016-09-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BR98recrBks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">very good cool well thank you very much
for coming so obviously we have darker
and darker with order age and everyone
is loving deploying containers into
production however Adam some of you may
be aware once you hit a certain size of
container and certain scale of
containers you start to run into some
interesting problems and that's where
you need some additional support so that
your containers don't end up looking a
bit like this and this is where kuba
Nettie's comes in humanities is designed
to help you run containers at scale and
that's what we're going to be talking
about today so my name is Ben Horne I'm
a founder of ocelot up war we're
basically do a lot of work in docker
containers helping companies that dot am
and my grave monoliths applications into
containers and make more beautiful and
nice and like maintainable and we also
build around products so one of which is
cut coda cata coda is an interactive
learning platform for software
developers you medically get a
step-by-step tutorial together with an
interactive environment and you can
start learning docker you can start
learning Cuban it is you can start
learning DCOs and measures and all of
the cool things which we're going to be
talking about today or for free all
interactive all all online what I want
to go into today is kind of introduce
Cuba natives and introduce the problem
Richards holes and how you can actually
get started and how you can start with a
name production like our Monday
basically I'm and I highlighted the kind
of key point which I think are important
and which I've kind of my clients and
people which I work with have
experienced and the problems which they
went into a comment basis so how do you
actually deploy Cuba natives how do you
launch it how do you then deploy
applications on top of them what does
that look like how can you then start
taking advantage of actually having a
proper orchestration and a proper
container is system for like managing
secret Sam scaling and I all the
important stuff and then actually where
does Cooper letters fit into the entire
ecosystem how do they compare to nomad
how did compared to dsos and swarm the
other offerings and so that's kind of
the overview but my aim is to get you to
understand where Cuba netted fits and
how you can start
siding whether it's important for you or
not and how you make that judgment call
so kuba natives with bit weird it's a
bit of a weird name it's not what you
would expect and any thought that
managed to containers but actually let's
take a step back and lets you look at
what containers are in the first place
so docker like for last two years has
all you've been reading about in hack
news it's been the latest technology
it's the best way to get you to the top
and your blog post retweeted everywhere
but like what container it's like why
why is everyone so enthusiastic about
this if we look at the real world if we
look at how items and products and parts
of physical infrastructure is sent all
around the world they're all done in a
huge big massive metal shipping
containers and the shipping containers
have a certain very interesting key
properties they are Wi-Fi there why
height and and actually they have
special things like they have doors in
certain locations and every single
corner they have a hook and if this hook
which allows them to be connected and
allows them to interconnect with each
other container and that's how you get
this very strong coherent unit even if
the sea is slightly unfriendly like in
the pitted pitch at the beginning and if
this standardization that they are all
the same side toward got the same hooks
in order some places the doors and a
label that all in a third place that
make them work together it makes them
work at the connected coherent unit
without that standardization it wouldn't
work what's actually inside the
container doesn't really matter at that
point it can be cars it could be
t-shirts it could be toilet paper it's
just irrelevant it's all about that
standardization and making them connect
together seamlessly Dave important and
that's the same mind that when you look
at containers of the software world with
actually on the inside doesn't really
matter it could be the JVM running java
it could be done there and course ALR
could be mono it could just be a goal on
binary with missing else that doesn't
really matter what matters it watch on
the outside what matters is how you
start on how you build them and how they
all connect and talk to each other from
look at the container they look and feel
very similar to what we were if we were
winning a hypervisor so generally with
merengue yes
so then we'd have a further I bare metal
be an easy to instance or something
which we actually own in a physical back
on top of that we'd have a host
operating system which whether we're
comfortable be a red hair and boo to
Santos and then normally would have a
hypervisor which manages to
communication between your third
machines and the underlying host
operating system and the hardware this
is what doctor is fundamentally
different docker is Colonel derivation
so instead of you having a guest
operating system which takes over head
and it have its own kernel is completely
isolated docker gets all of it benefits
but being able to take advantage of the
host operating system itself it's being
able to take advantage of what's already
been booted was there and what's
performing and on top of that you have
containers each container starts at an
image the image is a pre-baked package
more or less and which have got
everything you which your application
needs in order to be able to win so
it'll any rent I'm dependencies which
you need for example the underlying
virtual machine like the JVM or course
healer it's got the processes and it's
got any dependencies which it needs I
nuget packages have all been downloaded
and compiled into this container image
and it's got any configuration
everything your application needs to be
able to start and that's all packaged up
are they image when South starch turns
into a container and all of these
containers are completely isolated
they're winning completely independent
and completely separate to each other
and so when you ask is container what
winning it will only be the things which
you actually told it to be winning be it
you're done let potus be it your ASP
done application be it a bash pumped
whatever you told it to be winning that
is what the container thinks is winning
on a host and nothing else everything
else is completely isolated it
completely invisible which means that
the sandbox and means that they can
interact with things which they
shouldn't be allowed to do which is
great from a security point of view and
they kind of have other interesting
properties so for me these are probably
some of the most important ones so as I
was saying that from the viewpoint of
the container is going on process space
if the only thing running on the box
from the viewpoint of
and so that means that it can ruin
whatever you want but it's all the
sandbox it's all isolated it's all
secure it goes on network interface and
so if you it's got IP address so you can
send Network packets you can open ports
you can interact with it how you would
normally if it were the process or a
virtual machine it's good to unroot
directress I'm from the viewpoint of the
container it's got everything which the
operating system needs it's got all the
light things like XC XC x XTC directory
it's got I'm far I've got user it got
home directors it's got everything which
you would expect from our operating
system but if you delete everything and
you completely destroy the home
directory it's not going to interact i'm
not going to cover up the hosts file
instead it's completely isolated and
protected which is great familiar kind
of like testing point of view because if
your test scripts actually accidentally
clean up a little bit too much and like
maybe delete one directly too high it
doesn't take out the entire host it just
takes out the container and you can if
we start and come back to normal and
this is a wholesome boxed and you so you
can think of the properties and some of
the mechanisms are a little bit like
virtual machines but they're very
different they have a lot of different
key properties which are important
because their containers the key
notarization is what makes them happen
it what makes them important and it also
is what makes storm interesting from a
performance point of view because you
don't have this middleware layer you
don't have this hypervisor managing the
processes you also don't have any of the
overhead which you're generally used to
if you don't have any bottlenecks in
terms of CPU you get access to the
native memory and most importantly you
can access to the native IL and so if
you even gonna like heavy data
processing jobs for example winning
databases or equal fervor you don't have
any bottleneck which you generally used
to with hyper fighters because if or
talking directly to the host is all
having me access to the performance
because you also don't have any access
to because you don't need that guest
operating system in the way you also
don't have to do any pre allocation you
don't have to give the operating system
a gig of ram just in order to win your
little and 100 magno process instead
wherever your process of the using
is how much resource it will take and so
you can get much better return on
investment from your infrastructure in
your hardware bucovina can actually
start taking advantage about like using
all of the possible wham you don't have
to give it pre-allocated just for the
operating system in order to be happy
and every thought everything takes
milliseconds to launch because the
important aspect of the colonel and
that's already been booted by the host
operating system the processes just
launched like they would normal
processes been said that contained the
sandbox deshielded inside of container
win time and would you don't get without
him and this containers them tales have
been around for a long time containers
as a concept were introduced into in
freebsd in the 1980s because the freebsd
crater needed to be able to work on
freebsd future updates at the same time
is running freebsd so we needed to be
able to learn two things like by sight
and this is where the concept of
containers were introduced and then this
with extended with things like the
lowest zones we had other concepts which
were introduced in Red Hat and then
Google and a lot of other people
introduced them into the Linux kernel at
part of the alex c feature set and then
doctor came along which kind of made it
accessible and it made all of this
technology which was already in the
linux kernel to begin with but it was
very difficult and very hard to actually
take advantage of and actually manage
darker made it accessible and that's why
people love docker they have built an
amazing tool set and how you can take
advantage of the container technology in
a sane manageable way which you don't
have to be linux kernel experts in order
to be able to take advantage of and for
me like is not just a toolset it's also
that they've actually got us to agree on
something before containers everyone was
trying to reinvent the wheel they all
add different ways of describing at
different ways of launching their
different ways of actually like
structure in everything and building
images dhaka aboard all of that together
they created some standards which have
you been watching Twitter with an
interesting conversation but I've
created an
ecosystem and a way of describing what a
container she looks like and this is why
we now can start innovating and pushing
things forward we think like Cuba net is
because the foundation is there we don't
have to keep trying to reinvent that
wheel but as I saying at the beginning
containers are great but when she start
winning like a hundred containers things
started to get a little bit more
difficult one server is like an a simple
thing to manage but a thousand servers
is a very different problem and a very
different beast how do you know which
container is winning on which hosts how
do you manage when a host goes down and
he's still want that workload to be
redistributed acosta system and then how
do you cannot manage scaling maintenance
modes all of this normal production
winning activities inside a container
ecosystem and this is where cuban actors
commence in cuban it is just one of many
different orchestration systems which
our available in a moment so he's got
dr. swarm which is the lovely whales
holding containers and 50 fish Dockers
built in a kind of default option for
managing containers on multiple
different hosts it's got cuba netted
which will be talking about more we've
got D cos I'm mesosphere which is
inherently from the Twitter architecture
worth crated internally at Twitter and
then we've got hash corpse nomads and
they all have different problems they
also open it slightly different ways but
my personal favorite is clipping edit
and Cuban entity is also gaining
traction in the community at the hole
and so this is some stats about what
people are looking at and what people
are investigating and obviously dr.
swarm is the default because it's darker
it's the one which doc or pushing that's
what people are naturally gravitate them
before towards but what I have generally
found is that people start implementing
it and they hit a certain point and they
need some additional functionality which
didn't exist and that's where people are
then going to Cuba Nettie's and from a
viewpoint about who owns it everything
is open source Cuba natives was created
inside Google more or less and but
Google have released it to the entire
ecosystem is part of the cloud native
foundation and which works and operates
very similar to the Linux Foundation and
as such other people the company
investing and committing to the actual
source code until we've got google is
the main competitor but we've got Red
Hat we've got meteor we've got caress we
live the music mesosphere who even have
their own orchestration but they're
still helping communities become the
best that it can possibly be and be only
cool people lots of people outside of
Google if not just Google it's an open
ecosystem which is being supported
constantly and we can kind of see that
from stack overflow and how many people
are actually jumping on board asking
questions adopt an eight winning it in
production and Cooper netted is only two
years old but it gained all of this
traction and all of this birds because
it's kind of pretty cool however it's
not without problems so one of the
things which you'll find and as I talk
through what different orchestration
attitude is it is quite the boss and
while i'm showing demos and what I'm
explaining it bear with me when I say it
the bowls because it does make sense
well it looks like there's lots of
duplication it's actually giving you the
options and customization and the
configuration which you need for winning
different types of posters and different
workloads until my general feeling with
kuba letters is it got kind of quite
steep learning curve but kind of once
you get over this initial hurdle which
is what my aim is for the rest of the
talk it's kind of like it just all hangs
together and it kind of works quite
nicely and for that the aim so let's
break it down and let's talk a little
bit more in detail into what this is so
Cooper native is open source or scanner
and it's designed for automating
deployment scaling and management of
container applications and this crew is
kind of born inside Google through
google have been running containers for
the last 10 years and as such they have
learned and got a lot of experience
about how you've in containers at scale
and one let's go scale I mean proper
scale like billions of container the
launch per week inside can google and
they've run everything different
workloads big data the search engines
databases everything it's containerized
and they basically issue workloads into
that cluster it gets managed it gets
organized and
stuff comes out the other end and they
thought all of their experience in a
paper and based on or the experience and
system which they built called Borg
would say summarized into a white paper
the white papers aren't very accessible
they're not very easy for people to take
and deploy into their own ecosystem
which is then they took all of that
experience I made it open source and
built Cuban edges but the white paper is
interesting in if you want more details
then it's available for you could to go
read and their mindset is how they run
container that Google been an open
source manageable way that people can
vinit themselves so they can take all of
the containers which you need to learn
in your system you put them in turku
benetti's and kuba netted manages which
hosts which server which data center
they need to be winning on to ensure
maximum performance high availability
and all the goodness which we would
expect and alongside that it's got some
really cool features which generally you
can get away with not having but when
they're there they just make life so
much easier for example horizontal
scaling when you're in a significant
load self-healing so when things go down
it just automatically knows how to
repair itself and bring it back to life
things like low ballentine are just part
of the features outbox you don't have to
think that house manager it's just there
ready and waiting for you to work with
and things like managing canary releases
and roll out and all of this course off
which historically you generally had to
write complex automated scripts in order
to be able to do it's just there I part
the accumulated feet of that and
document isn't the only game in town so
ducker is just one container
implementation it's got the mindset it's
the most popular but other companies and
as a people who adopt in different ways
so chorus of relief rocket which is got
some really nice a different way of
looking at the problem of how do you
manage containers very security focused
we've got hyper which again is kind of
taking a virtual machine approach I'm
wrapping a virtual machine around a
container to ensure maximum security and
then we also have windows containers
coming which I'll be talking about this
afternoon so how do you actually store
and how do you actually deploy this
introduction and as
certain key core component which I think
are important to understand how the
moving parts actually fit together so
everything all the stay in all of the
information is stored in X ed X ed it's
very similar to zoo keeper or console it
basically highly available distributed
value store and that store that
everything which you need about your
system and so it will even know if your
master goes down all of the information
is still replicated across multiple data
centers and it's still there and
maintaining it also uses xcd for
maintaining service discovery and things
like dns and I which container the
winning on which hosts and how do you
find where things are managed and so
exited the underlying base of what cuban
actors are building on on top of that we
have an API which would expected every
application has an API now and you have
a master the master is responsible for
understanding what workloads need to be
deployed and where they need to be
deployed to so it has a viewpoint over
everything was happening in your system
and in your data centers along so that
we have accumulated proxy that while I'm
not trying not to follow stage for the
proxy wins on every single node in your
cluster so everything will node which
you're going to want containers to be
deployed to and containers running will
have a proxy server and if we'll manage
deploying because containers actually
launched in the processes and having
them winning on the individual host it
will also manage networking and
communication and IP tables and all of
that walls and all of that management an
individual host the master would then
communicate with all of these different
boxes on your cluster and manage which
ones need to be deployed where and
alongside that we have the cube CT out
this is a command line interface so you
can actually see and manage your
workload so you can see the health of
the cluster you can see what know join
your system you can see what containers
are winning it on your host and where
they're winning but not everyone likes
our command lines and some something
like nice more digitalization for
monitors and a sporting and so they were
the dashboard which represents the
health de status and everything across
your entire datacenter so these are key
components it seems complex when he
first
get into it but actually when you break
it down into quite a nice logical
simplified view of how architecture
works so how do you actually learn and
how do you start deploying stuff so one
of the simplest ways is to reuse cube
TTL you faker eight you give it file I
yamel structure and then that will go to
the master it will execute where within
the yellow file and that will get
deployed on to your cluster the Emma
file details all the internal aspects of
what you want to be deployed and it's
got certain key interesting properties
like it's got the name it's got how many
replicas or how many instances of that
container you want winning and things
which would expect like the container
image into a portent need to be opened
the things which will expect and what
your would normally be running on docker
it's just been defined in a young
approach and I kind of think of it as an
idoc a compose but for production I'm if
you use the duck on compose so let's
actually look at how this approach if
the Wi-Fi is friendly
because it it's not nighttime here it's
just nighttime in the UK so if we go and
Vern as our cubes etl so I'm just using
catch coda because it's easy and we've
got help script which basically launches
the master launches AG CD instance it
launched at the proxy and download coupe
TTL and gives us a instance of Cuba debt
is winning and if we do presentation
mode you can see and so you can now do
like kuti tailgate nodes and we've got
localhost ready and waiting for
container word i'll just be issued so
then we can use a cube CTL and we can do
oopsy die then so what this command
executed was a fairly similar how doc
oven works but we basically said told
the cuban native master to run a service
which we'd given friendly name of HTTP
use this particular image and launch one
of done in the prescott and so what we
can Daryl do is ask for and how many
deployments you have and it will say
like we thought he's asked earth to
deploy HTTP you've requested that they
have one desired state there is
currently one available because
everything's healthy and everything is
up to date and it gives you the overview
of what applications are available and
if we wanted some more information we
can kind of go describe and it gives you
some more in-depth information about
what winning on system and actually
where web particular containers are
where the hosts what the information is
and if we are running 10 containers we'd
have 10 things in our instance and when
we do thief so now we've got a container
winning by default very similar to with
docker everything in sandbox and
everything is shielded so you need to
expose and tau eliminated what ports you
want to be opened and what services you
want to be made available so in this
case we simply just telling computer to
explode the deployment called HTTP and
we've make it available on the external
IP address on this port a map
so instead of doing things like duh
govern hyphen p we're just separated our
process we've got our content of winning
and then we add an additional thing to
learn our service and make it available
in the recover that's what Cooper netted
during the master is talking to the
proxy the poxy goes are neither
container being made available and it's
updating the IP tables and updating or
the networking balls in order to make
that happen and then we can issue a coke
arrest and we've now got kubernetes is
running our containers for us in the
recovers everything is still docker so
it can be duck appears we've got our
Cuban NC the instant winning as
containers and then we also have our
HTTP server winning at the top we could
fundamentally crew beneath you that just
managing these darker containers for us
it's not doing anything special it's not
adding it's not changing how dock works
is not changing how the container the
bill or how the image works just making
it much easier to manage at scale and
fundamentally that is creepy letters
like in a very simplified
straightforward way if you just do cube
detail win instead of document which is
also now how does it look when you
actually burn like real applications not
just a single HTTP container so Cuba
netted introduces certain concepts to
make managing containers easier at scale
and the first concept is called a pod a
party basically multiple containers or
logically grouped together and it means
that you can have multiple different
processes which all share the same IP
address they all share the same network
interface they all look and feel like
they're deployed at one single container
but for management purposes and for
making them easier they've been
separated into multiple different
containers and this allows you to do
various interesting different concepts
and you can do this concept with and
dock itself and so you like I the
concept of a potted basically you have a
long run in container like which is
called pause and that's responsible for
creating the network interfaces
responsible for creating any
volumes any additional data and all of
the containers then connect and share to
that one container namespace and so
that's why you have that's how he can
take advantage of the network interface
it can take advantage of sharing volumes
and having everything look and feel like
one single container Bert them being
separated isolated it's more useful for
other described in this example you've
got an application and then you want to
enhance it with maybe a lager or maybe
some backup scripts or additional
functionality which your application
needs and because of all sharing the
same namespace they all are maximizing
performance because I don't have any
additional jumps to make in terms of
different interfaces and different
approaches you can maximize how much
three port your containers Bochy dealing
with and so it's a nice way to architect
it and this is how kuba net is just did
things by default you don't have to
think about it like you do with docker
it's just automatically giving you the
best practices and structuring your
applications and you can think of pods
as like how you scare all your
applications and so pods are scaled
together so if you scale a pod which
I've got your web application and a
logger it will when you scaled out
you'll have to web applications and two
loggers running side by side when you
have 10 of them ill scared together and
so you don't have your web application
any database in the same pod because you
want to be able to scale and split them
independently and so that's held pods a
light logically strokes it together just
like nice little logical connected
resources these are all managed by
controllers all of the cumulative
managers everything other controller so
the third controller and one which is
probably the most are widely used is the
replication controller and this has
managed it how many instances of the
pods are winning on the system at any
one point in time so here we've heard
that did the desired count is 2 &amp;amp; so
kubin entities will make sure that there
are two pods always running in the
system if one of the pods are want to
container crushes kuba natives will
instantly detect that and automatically
restart it for you if the pod or if the
host goes down again Cuba natives will
detect that the pot
no longer there it will detect that you
wanted two of them and it will find
another host where that container needs
to be running and as 12 different
control is always possible for only in
different parts of your infrastructure
and solving different problems so one
which I really like is the demon set
this basically tell you tell google to
launch a container the demon set and
when new host get added into your
cluster it will automatically deploy
whatever container the needs on to that
host and so if you're doing things like
log aggregation backup scripts like
visualization any debugging tools it
will automatically be deployed so you
don't have to think about how that whole
sketch configured kuba netted will just
manage it for you and then as you update
it it will just be automatically updated
and rolled out across the system for ya
replication controls are always ensure
that everything is in the desired stain
is always up to date and always working
across the entire system and so that's
where you'll see people running most
commonly and this is what we ran in the
first example you can see at the top we
say that the kind of application
controller and that defines the type of
the type of method that Cuba Nettie's
will be using in order to manage what
your containers actually doing so if we
changed from being a replication
controller to a demon sir Cuba netted
will treat it differently and it'll tree
that life cycle in a different way and
then we are now we've got certain
properties for example the replicas
which manages how many should be
actually be running and you can keep
detail which I described that get angry
thing can say create this will then
start launching the pod in the de cover
which will be winning Redis we can say
get pods and if we're good actually give
us a state of what's happening so
instead of it just being like this it's
raining or not winning Cuban attitude
actually aware of like that more than
offer on in a life cycle of an
application and so for example I would
say that a containers creating which
means that it's downloading the image
and it's setting up all the networking
and it's actually configuring all the
fat hosts and then that will just switch
to winning what if a is in a in a stable
live state and then the cover that we
described before everything for an inlet
Nina's for your jealousy like your
reddit container wings along with the
pores container which is managing the
network some multiple different things
can connect to that where this instance
and it works for everything so if we say
deploy a front-end application in this
case we're saying Vern will often end
have three instances always available so
we'll have three individual separated
pods but when we say get the replication
controllers it gives us an overview and
an aggregation of what's actually
winning on a system so I'll say the
desired state and a current state this
is interesting especially when you get
into things like scaling because your
desired state maybe a thousand but the
current state may be on 500 and that's
because you don't have enough resources
available in your cluster in order to be
able to winning at a scale which you
actually request and so that's why it's
nice be able to digitalize and see the
difference between the divide and the
current state which I described here and
other thing the y mo is all
infrastructure is cold it's your
infrastructure and it's how the current
state of your application should be
deployed at scale in third cluster and
so when people instead of manually
winning duck burn or manually updating
things when new versions need to be
rolled out for example a new doc image
the Yama file will be updated and any
send that or dated llamo to the Cuban
latest master and then that will
understand and that will understand what
needs to be changed and how that needs
to be rolled out across your system and
in that Camille version committed and
Thor's controlled it can then be have
its own Jenkin tasks which managed Cuba
netted swim process and everything can
be structured in a more sane way than
its random ad hoc given at his win
scripts so let's take a look at how that
actually looks at scale
so the guest book is kind of like Cuba
netted version of hello world it's it's
got a reddish master and then it's got a
Redis too eager it's got venoth master
and it got a ready Flav for replication
and scaling purposes and then it got a
PHP application just sitting at the
front end aspect and we can look out
what this looks like solicitor our
various replication controller and we
saying that we need two replicas of it
for availability and performance reasons
we give it everything which would expect
like name the image but we also can give
it some detailed tour and service
discovery so by default kuba Nettie's
has dns service discovery built into the
box so you don't have to worry about it
when we define and we launch container
we can't a in order to discover what the
containers of winning on the network use
DNS and you can switch to use an
environment variables which will set IP
addresses but by default here we then
use DNS and so that means our very slave
will be able to ping other containers
for example cord where this master which
we define here and they'll be able to do
that over dns and without actually
knowing each other and we've also got
replication controllers for a front end
and it all works in the same way we just
defy a different image we specify
there's a number or what you would
expect but all defined at the Amal files
and then beyond that we've got our
services so previously in the previous
demo we had cube GT ow we told it to
expose a certain container but we can
also configure how contains the exposed
using gamma files and using these this
way of defining our infrastructures
configuration and we just set the
service kind option we say what poured
the containers vinegar and what poor we
want to be exposed I made the container
available and that's how things get
deployed and so we can kind of live
through we can create our master where
you can create the service and so we can
get through seas and it'll tell us what
container exposed what IP addresses
they're winning on which ports are
available in a nice structured way
and we can listen through and deploy
everything do and now when we look at
actually what's on the system you can
see we've got off for an end we've got
our various master we've got a slave all
of these replication controllers which
we've told out how to deploy our system
we can create our service to make it
available so now we've got different and
running on port 80 we've got reddish
master I've got a better slave and this
is in this example one host but it could
be many hosts I need to give this diff
nice aggregated view and then we can go
to the dashboard you I
while I do a little dance just why the
Wi-Fi is loading up and this dashboard
would then represent basically
everything which clubs et al is showing
us been a nice pretty away and I we kind
of like really dashboards as monitoring
visualization of what the state of the
cluster with actually winning so now we
can see that we've got a better slave
we've got red its master we got for an
end and how many pods are winning so
with a nice way to visualize everything
in a nice way for the dashboard and if
things start going wrong and for example
if when its master disappears you will
actually be able to visualize and see
that along with your map monitoring
metrics and so that's kind of how
containers are all all accessed and so
web contagion replication controllers
are the heart of Cuba natives for me
personally they manage what's running
and how many things should be winning at
in the first place but containers also
need to be accessed so we exposed up for
an end using a known port we said when
our containers make our festival on port
thirty thousand eighty and but Cuba net
is buddy for had different ways that you
may want to work and deal with them and
this is where the services come in so
you've got web contagion controllers
running things you've got services which
make them accessible and this is where
could beat CTL manager de proxies
ensures the data is going to the correct
place and the correct host and so you
can send traffic to any particular tree
benetti's nodes and that will know how
to read traffic to the correct container
on the correct host even if it's not the
one ethan traffic to in the first place
and so in your dns settings you can
basically use dns round-robin send
traffic to any other nodes and then it
will discover and send traffic to where
it actually needs to go into the covers
and so again it's all about maintaining
high availability and ensuring that if a
no goes down you don't lose traffic or
you don't lose reliability in terms of
our application controllers everything
they've managed via labels and so if if
where it kind of get the bowls but it
kind of makes sense and so it's got the
name which is for an end and we also
give it labels and neither hell
containers are grouped together and
logically and so in this case we say the
label is called for an end and
obviously we have three vectors in our
service we can say expose all of these
containers winning with the name for
intend on port 80 and group everything
based on this front end name and so
that's how it can manage load balancing
for us because we can take any pod
winning on any different host and group
them together based on its name and
based on its label and with the approach
of no port it basically defining the
port and making it making that
application available on that port on
any particular hosts so this is really
good if you have applications running
for example databases and they are
always winning on 3389 and you wonder
poor to be consistent and available for
all applications across your cluster so
they don't have to keep going and keep
looking up which port the applicable
actual application is winning on and
that's where no pork comes in and you
basically in the ml file which is the
hard to lothian you just define which
port right you want to be available and
that's how Cuba netted managers and
reserves that port for that particular
container but it also smart and it's
particularly good at when it's running
in the cloud because it's appreciated of
all the cloud api's and what's at
available and possible so if you're
running in Google compute engine a jaw
or et tu an AWS it will know how to
configure and how to communicate with
all of the load balancers and how to set
up and how to configure them to update
and serve traffic and to your individual
particular pods and so it's aware and it
will use that and take advantage of it
instead of you having to reinvent wheel
and having another layer of indirection
and when you start the containers in
your service instead of exposing a node
port and defining that you simply say
the type of load balancer and then it
google and cleaver natives will not be
kuba lettuce will manage that and they
will communicate an update everything
I've it needs to the other way is
external IP addresses and so if you have
a subnet for your data center and on
your networking and everything is
configured properly you can give
containers individual IP addresses and
now will it be accessible via the IP
address across your network and again
when you start the container inside the
service you lift the IP addresses which
you want it to be made available on this
will be updated when it launches the
coup proxy will update all of the IP
addresses and the IP tables and then
traffic will be any traffic sent to 10
to 50 for 13 will go to that individual
pod and if you can start doing sac I bit
of static IP addresses even with
container system and now what a new
thing is is if you are villing in your
data center or you don't particularly
want to use a cloud load balancer then
now an English load balancer and it
works in a very similar way you just
have complete control of it and so you
can specify rules and approaches about
how you want traffic to be managed and
dispute across your system and so it's
all rule-based for example here is
defined in a vault in anything going to
our mud cloud service payments goes in
actually to all of the pods which I have
got the label called payments and so if
we've got 20 different payments
microservices winning traffic will be
load balanced and shared across those
based on the host header and then you
just have a list of rules which you
manage and update as part of a yama file
and then you send it to cute detail
they'll dispute across your data center
and everything will just be updated in a
nice consistent way without dropping
traffic if you want even more control
you can jump in and start update and
load balancing rules so if you want to
introduce like sticky sessions in engine
X if you want to change the how the SSL
is configured it's all available at all
possible with all there and could be
netted just the same defaults beefy one
that flexibility this is where you have
the English controller and be made
available and as I say and everything
it's done by DNS so even in our mobile
intervals we've got kind of like the
veins which is basically doing a loop
over everything and then for every
single thing which we define in our
paths we say it's accessible and proxy
the request to a particular container
and you'll find that container
based on Dennis so everything it managed
by Dennis and I makes it discoverable
and easier to find which I defined here
but this is okay and this is like DNS is
great and dnf is a great way to find
that like container is winning on this
particular host on this particular IP
address but there's also the problem I
like how do you actually send network
traffic to that particular host and this
is where overlay and software-defined
networks come in and so if you're used
to winning containers you may be
familiar with tooling like we've or
calico or now docker itself and to
manage this networking for you and
managing how container 11 hosts talks to
a container on another host without
expose in any particular port kuba
Netta's do this for you it manages all
of that communication out of the box so
you don't have to think about it again
there isn't a ducker the whole batteries
included but removable if you want to
use we've if you want to use calico
everyone health co-op Cuba netted
support and it's very easy to drop in
and drop out and different ways are
doing networking for you about how the
box Cuba net is is managing ridding
containers at scale and so you don't
have to worry about it so this is kind
of like the key main points so we've got
controllers which are winning and
showing the lifecycle and application
and how different things are managed
we've got services which are managing
how containers talk to each other what
poor exposed and how different things
communicate whether it be low balances
or external IP addresses DNS is
supported at the box to make things
discoverable by nice friendly names
which we're familiar with and then with
the overlay networks which help
different hosts talk to each of them and
this is the peak or components of human
lettuce but there's more important thing
it's not just at like winning that scale
and winning them at auction we need more
support for things like how do we keep
containers 'life and how do we keep them
updated so when we're defining our
container and when we're defining our
replication controller we can add
additional options one of the options
which are you very frequently is the
live mr. basically health checking built
into Cuba netters and check into
where the container is a live or not
like it's great having a container
winning but that doesn't necessarily
mean that your application is operating
as it should be it could be alive but
just sending 500 rests which you don't
really want you had been syntech until
we can do with our liveness probe in
this case we're doing HTTP gap beacon to
do tcp connection g thumping requests
any health check you'd expect from
nagios or other monitoring solutions we
say which part to keep hitting along
that we turn in a nice to enjoy request
everything's happy if it doesn't and
every turns a 500 Cuban answers will
detect this it will detect and healthy
or move it from the load balancer and
update or the iptables subtending
network traffic to it and flag it as not
winning but in healthy and so we just
define it when we define in our
reputation controllers these are really
nice thing is readiness so this is
particularly useful if you're winning
your containers of running tomcat or a
heavy java process or has got some
initial warmup time which needs to be
performed and so you can have a a probe
which checks you say like you may be
winning but are you actually ready to
start receiving traffic and so it's just
if noise health checks and separating
how your application works and being
appreciated that applications aren't
just in a state of winning or not
willing there's different life cycles
which it goes through and kuba netted
appreciate that and it's already fine
when you're defining your application
controllers it also manages updating and
so one of the key things is rolling
updates so when we have got version one
of our container and we want to go to
version two we don't necessarily want to
go like let's stop everything and let's
like just start version 2 because that
would stop live in traffic pigley if we
got throw boot up time sort of warm-up
time we want to be able to ensure that
we're still maintaining high
availability and we're still serving our
capacity without actually taking down
and any loss of service and so other
orchestration system to have got they're
prone to just stopping everything and
starting everything again Cuba net is
instead is rolling updates and so you
generally you'll update your llamo file
you'll go from 30 to two thousand three
for the image you'll say rolling update
apply and that will then update
everything in a nice seamless fashion
and it will be appreciated of health
checks and will be appreciated of
readiness and so little it will stop one
container it will start a new container
until that containers ready and healthy
it won't continue rolling out the update
until you get this nice staged approach
you can then take this abode for canary
release it and you can say I want fifty
percent of my traffic going to container
a and fifty percent going to container
be once I'm happy with that release
continue to roll out if you start seeing
things are going wrong and not as you
would expect then you just say roll back
and Cuba netted will know how to go back
to the previous state and undo any
deployments which you actually put in
place and we can feel it in the log
files and so add things are going
through we can say at the top with an
update engine X and it will just slowly
roll out it will always ensure that the
containers running but everything's
being updated and bring them one
container down and we flashing and
bringing it back up in terms of Canary
releases again if you thin flexures and
labels are the way to manage and know
which ones machinarium which ones most
stable release and which containers
should I send traffic to and which one
should I lights and traffic to and
Felicity how you can start separating
your service and separating your load in
a knife seamless way by just having
selectors if those are cool and
appreciated of things like maintenance
like performing security updates on a
host operating system 82 needing to
reboot the actual virtual machine which
it's running on and so you can say drain
a particular note keep Nettie's will
then stop all the containers moved on
and restart them on a different host and
then it will be the node will be paying
to an unhealthy state add new workloads
are executed and distributed to Cuba
netted it will not load them onto that
particular host because you performing
maintenance and so we will find other
hosts which were available when you're
finished went to hold the node of
updated and it's happy then you can say
bring it back in
keep it until then instantly start we're
deploying reissuing workloads on to that
particular host for you this is
particularly useful in terms of data
centers and so you can start winning
this app data center scale anything like
I we need to perform maintenance on a
network path or an f5 load balancer you
can take out all of the nodes in that
low data center drain them all and with
all the traffic we shift to a different
data center and can bring it back up a
kind of like a datacenter scale instead
of going to each individual node which
could be problematic if it's got
thousands of them so this is called um
and then there isn't managers and other
things which you need to be appreciative
when you're running in production which
containers it's so cleon great art if
I'm honest and one of them a secrets
management and reason which I'm not
particularly happy with containers and
secret management is they have default
to use environment variables which while
environment variables are very
convenient they're not the most secure
way of managing database secrets and so
things applications generally have a
nice habit of like the environment
variable could be potentially useful for
deeper things so let's lie plug that in
a secure in location for example
pagerduty has got a system in place
which will detect and potentially the
mobile environment variables and remove
them from their log files just in case
anything actually happens no Jay s and M
p.m. are particularly happy to like
share the environment variable with
other things running on the system and
so you don't necessarily want your
passwords be in this place and so crib
annette is managed of it as volumes
instead so when you start your container
you can flag the secrets which you need
access to and then v secret emanage has
files on disk instead there are physical
files they're just a volume which cuban
exited managing for you and so you can
say output my database westford which
lifts up my DB password and that's how
your application will death where your
application will read from when it's
booting up and how it gets access to the
secret but it means that if an attacker
does get access to the environment
variables it can't actually see what the
secrets are in the first place and for
Cuba net is managing it and ever
incubi density camel and so that's what
we'd expect our secrets to be managed by
two and four here is simply defining the
mount we're defining where the secret
should be stored in this case ax ed
secret volume and then we've pointing it
to what secrets store we're actually
looking for and the same with API keys
with same with everything and so that's
just a nicer way I'm in a different way
just our box are managing how we do this
in terms of scaling and we can start
being restrictive about which hosts and
which nodes containers should or
shouldn't be running on particularly
useful for databases so you may have the
hosts and nodes which are more suited to
winning 13 types of workloads for
example they have a SSD disk or they
have high memory or they have high CPU
and so when we define our application we
can say like always win this can
particular container on a node which
have got disc type SSD and so Cuba
netted will realize and appreciate that
and so when it got workloads I'm when
you're scaling up it will only scale it
up onto those particular nodes nodes are
then just added and defined labels and
selectors and so you can simply play the
master this is the label of type SSD
disk and it though containers can be
deployed on to appropriately in terms of
scaling again we can affect auto scaling
and at the moment it's based on cpu
usage he simply they often end
application which is our PHP application
also scale up to five maximum containers
and basis on a threshold of eighty
percent CPU until Q benetti's will
automatically monitor it it will look at
what a cpu usage is across the
containers if it does peaking and spoken
to eighty percent it will start
launching more on different nodes and it
launches dormi will update the load
balancer and the IP tables and we'll
start spreading and sharing that traffic
across everything so if we look at what
you have done we don't have time if you
want it all on Cuba natives it's all on
katakana and for multi house but we'll
move on to cover the interesting topic
so auto scaling is great um but
cumulative
planning to go a step further and that's
with bin packing so because Cuba natives
knows how your application starts he's
also monitoring how your application
actually operates and the processes and
the behaviors of what the application
actually looks like so for example it
knows whether it's a high memory
application or high memory usage
application or well his very bound to
CPU or i/o it will then use this metrics
and these judgments to judge what other
hosts and other containers that
particular container should be running
on so if you've got a very high memory
and low CPU container willing it should
be paired with the container which is
needs a lot to CPU but very low memory
and this is how you can maximize and
take advantage of your entire
infrastructure without introducing
bottlenecks or contingent or containers
and applications fighting for the same
reason and this is how Google operates
at scale they fit how Google can have
hundreds of thousands of servers or
winning at eighty percent because each
application is not competing and not
constraining the fighting for the same
resources because it's monitoring what
the application actually needs and if
the concept is going to be introduced
into Cuba netis its concept called
pinback bin packing and for me I think
would be like playing Tetris with your
applications like you benetti's will
know which applications are running wear
and wear when you say deploy something
it will know where to fit that into your
entire deployment approach and so you
won't have to do capacity planning you
won't have to do resource management
you'll basically have everything
structured and dim for you we then comes
up with strict Nick so we're in a team
level we can start saying that certain
teams are only allowed to deploy certain
amount of resources into our datacenters
on into our clusters and particularly
for protesting you could have testing
which takes twenty percent and it can
only ever take twenty percent even if it
goes a little bit crazy and you know
you're not potentially constraining and
potentially impact in your production
usage and it's just a royal way to
manage how everything is I'm shared and
everything is not fighting for the same
one and if you've all done by namespace
so every container can be issued into a
namespace but the full communities
within cube system but it's like your
professional services team could have
their own namespace and so that's
ring-fenced and separate from everything
else going on system when you do things
like coupe GT I'll get pods you can say
get pods for all needed particularly
namespace and throw your teams don't
have to worry and digitalize what else
is happening on the cluster you can just
care about what's actually in your
interest and so it's a nice way to
manage ring a particular scale and you
can then list what's happening just by
using coupe CTR once you get to a
certain particular scale you can then
start winning Cuba natives at high
availability so you want to learn
multiple Martin masters to ensure that
if Walmart to get shot in the head or
kind of like losing network
infrastructure you can still start
deploying containers and you can still
manage your host in a multiple way and
this is all part of like an approach
called Federation or uber lettuce and
which is allowing you to win containers
and rinku benetti's across multiple
different sage centers and ruin them as
scale and be data center aware so it
will know whether it's winning in region
1 or region 2 and it will make sure that
when containers are operating in it will
work and communicate with things which
are nearby to ensure that it's got like
less latency in network traffic's and
Google because it's open source it's
also very appreciative of being open and
transparent about it approaches it so
you'll see lots of proposals and lots of
detailed architecture detailed design
documents and decisions in the gate hood
bleep repo and so if you're actually
interested in what's happening you can
go down you can find out more
information and this will then auto
scale across clusters and so it won't
just be it would it be pods which can be
scaled it will also then start detective
and your hosts have got reach the third
and capacity and start spinning up
additional house for you and so you can
start scaling act like a bigger larger
level
so with that I'm will move on okay let
me just um I'm running out of time okay
if we'll aren't skip the bear bad plug
so um like ribbon ideas I've got all of
these problems a lousy train container
that scale and allows you to win a
minute cloud and win them across
different things but like where did that
fit into the entire ecosystem and so
swarm is great I'm swarm kit is the new
floor molded particularly interesting
and that's available and I would say
that that's probably people first
logical approach um where they're trying
to scale out to multiple different hosts
because it's a simpler learning curve
and you don't have to deal and it's just
a natural wave winning Zing's DCOs is
kind of dedicated and optimized for
winning different types of workloads um
and so if not just associated with
winning containers it can also burn
Hadoop job it can rinse park and come on
ETL processes and so that's where you
kind of need when you want to take
advantage of your entire cluster and
when different types listed by dcs fits
in terms of nomads and it's kind of like
it's just a scheduler it didn't do
frequent management it doesn't do auto
scaling and it didn't have some of the
features which are discussed instead
it's just a pure scheduler but at the
same time it's probably the purest and
cleanest and neatest one of them all
because it did one job and it did it
very well and not only did it can do
containers we can do VMs it can do jar
files and it can manage it winning
processes and so if that's what you need
then no matter is actually quite a good
alternative without all of the
additional complexity which comes with
Cuban edges but connected about the
traction it's got the mindset if
definitely got the most people
proactively working on it in terms of
the community and as I said that I it
got powerful features to help you in and
containers out scale and it's got even
more coming like the role map is
transparent it visible but it's also
aggressive like constantly they're
releasing new updates which you actually
need and would you actually want when
you win and contain the touch scale so
with that let's summarize
the other thing google it is it's open
source it's designed for winning and
automating containers at scale
originally backed by Google is now cloud
native foundation and so it's all open
source multiple different companies are
working on it and once you hit that
point in your container architecture you
need 13 particular features and that's
where kuba net is fits in when you need
auto scaling when you need better health
checks and low balancing and winning
things this is where communities and
you'll start to hear that point and it's
an Isis simple way of managing multiple
different hosts and where your workloads
are actually winning it's got a lot of
traction it's got a lot of interest it's
definitely progressing nicely and it's
definitely a project to watch if you're
planning come to deploy containers into
the future I gave you a quick demo of
Cuban it is on catch coda if you want to
learn more than that's obviously a great
place because that's my baby but it's
all there it's all free and so if you
want to play with Cuba neta to yourself
then feel free and if you have any
questions then to please reach out and
contact me with that thank you for your
time and I hope to see some of you in
the windows contain a session this
afternoon thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>