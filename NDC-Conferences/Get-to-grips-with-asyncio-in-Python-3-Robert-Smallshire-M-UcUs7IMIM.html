<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Get to grips with asyncio in Python 3 - Robert Smallshire | Coder Coacher - Coaching Coders</title><meta content="Get to grips with asyncio in Python 3 - Robert Smallshire - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Get to grips with asyncio in Python 3 - Robert Smallshire</b></h2><h5 class="post__date">2017-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M-UcUs7IMIM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so there we go right so getting to grips
with async IO or a sink y'all as I want
to call it forever
in Python 3 ok so you may be interested
to know that one of the reasons the
lights in London stay on these days is
custom of some async IO code that I
wrote about this time last year which is
currently deployed on to hundreds of
single board computers running in
industrial facilities around London
monitoring power in industrial
facilities and turning those things on
and off as the electricity price goes up
and down so what I'm going to talk about
today is founded on real-world
experience of using this library and I'm
essentially going to replay the journey
of learning that I went through when I
was first getting to grips with this
library in order to understand it of
course if the lights go out and the
power goes out in the middle of this
talk we now know exactly who to blame
those of you are new to Python or
learning Python you might be interested
in our books early the first one is
published at the moment Python
apprentice it's targeted at people who
can program but not necessarily in
Python my co-author Austin bingum is
sitting here in the front row and we
have a discount code just for you that
URL at the bottom there so the structure
of the talk today I you know what is
async i/o why do we care about it why do
we need it why is it a thing I'll start
with that then we'll move on to
explaining co-routines
and understanding what co-routines are
they are at the foundation of how async
i/o works and I'm actually going to
introduce them without async i/o and in
fact we are going to build something
that is largely equivalent to async i/o
without using gates in Cairo as a way of
understanding how it works from first
principles I'm a big believer in the
idea that although abstractions are
great and what allow us to to build
fantastic systems with much less effort
you should at least understand
how things are actually working one
abstraction level down from where you're
working day to day because then when
things don't work quite as you expect
you have some well-founded intuition
about why they are working the way they
are then we are going to actually
introduce async i/o itself and move on
and see some of the things that async
layer layers on top of that very basic
implementation but not until the very
end are we going to actually begin
talking about IO in the context of async
IO
so why do we need a sink IO well IO is
high latency I'm sure you're all aware
that your computer can do a gazillion
things while it's waiting for the next
bite from well even from RAM these days
sequential programs waste resources
waiting on IO operations and the
traditional solutions for ease of
multi-threading or multi processing
carry large resource overheads every
thread on linux is at least 8 megabytes
of virtual memory stack space and large
cognitive overheads it's hard to reason
about these things when there are
multiple things happening at the same
time and then we get go down the rabbit
hole of locking and all these
difficulties that arise and ultimately
the Python interpreter itself is shared
mutable state protected by this famous
global interpreter lock so if we use
threads in Python we are softened by
what Kepler Denny would have us called
the global interpreter bottleneck so
let's just establish three definitions
to make sure everyone's on the same page
here concurrency is about tasks which
overlap in time but in terms of the
start and end times overlap although
we're not necessarily progressing each
task at any given or all tasks at any
given instant whereas parallelism is
about actually doing things at the same
time so this is dealing with multiple
things at once versus doing multiple
things at once ok so it's important to
understand the distinction here so async
i/o is about concurrency
whereas threads and multi processes for
true parallelism require at least more
than one core in your computer so a
synchronous versus sequential versus
synchronous so asynchronous means that
we don't need to wait before proceeding
we can get on with something else while
we're waiting for a task to complete
whereas synchronous or sequential means
that we must complete a unit work before
we can move on to the next thing okay so
of course the overall duration of work
in the screen shal situation tends to be
longer it's interesting that I find it
interesting that the word synchronous in
this use is completely contradictory to
the word synchronous in everyday
languages if something is synchronous in
the rest of the world that means at the
same time and we use it with opposite
senses asynchronous versus non blocking
this is not really an official
definition if you like but it's a useful
distinction to make
so asynchronous tasks return immediately
often with a promise of some later
results called the future we'll come on
to those later
whereas non blocking functions return
immediately but they may come back with
no results a partial result or all the
result yeah so the styles of programming
you tend to work with in these
situations are different so with
asynchronous programming we generally
get a callback or a future with
non-blocking we generally have to pull
something until we actually get all of
what we want okay so this is an
important distinction and then down at
the kind of operating system level we
can think of different ways of arranging
for things to happen
we have pre-emptive multitasking where
the scheduler interrupts tasks this
requires that the scheduler is running
continuously so you need real hardware
level parallelism for this to work the
way operating systems do it is by using
Hardware timers in the CPU to trigger
interrupts all those cooperative
multitasking we're now the tasks are in
control and they must yield control back
to the scheduler but if they don't yield
control back to the scheduler the entire
system
ank's okay so important distinctions so
async IO is about cooperative
multitasking yeah so async IO is allows
us to write asynchronous concurrent
cooperative tasks but the great thing
about it is it allows us to it in a
sequential style we don't have to think
too much about the fact that multiple
things are happening concurrently it
allows us to do quite neatly sidestep
all the problems that are usually
associated with concurrency okay so in
the next part of the talk we are going
to introduce co-routines I'll explain
what co-routines are I'll explain
demonstrate how they work with Python
from now on we've had no code so far
from now on pretty much everything is
going to be code ok I'll show how Co
routines are implemented in Python and
we're going to build something very
similar to async i/o without using
Gatien tayo and that will reveal to you
that there is no magic and it really is
very straightforward so to do this we
need to have some computations we can do
it's forbidden for me to use the f-word
in any of the presentations I gave
otherwise someone will sound the
Fibonacci klaxon on Twitter so I prefer
to use the leek lucas sequence which is
closely related to the fibonacci
sequence so here we have a simple
generator function Lucas which generates
the Lucas sequence and here I'm just
using it Atul's I slice to pull off the
first 10 numbers and the Lucas sequence
ok we need to use I slice because the
lucas sequence is infinite and this
thing will happily loop forever as you
can see by the wild true in here it will
never never exit so we slice off the
first 10 so you're going to see me use
lucas calls to lucas threw out a big
chunk of his talk as a way of just
having some computation to do
now we're going to introduce a simple
linear search function this is pretty
much as simple as Python gets to have a
search function which takes an iterable
and some predicate function for every
item in iterable we test the predicate
against the item and return the item if
we get to the end of vegetable without
having found something that matches the
predicate we raise a value error we've
not found everybody with me
does everybody hear programming Python
or P is it on here new to Python okay so
it's worth me taking the effort to
explain some of the Python here so we
can call search on the Lucas sequence
and we can pass our predicate which
basically says give me the first number
in the Lucas sequence which has six or
more digits all right we take the number
convert it to a string measure the
length of the string so cheap way so
search is just a regular function right
there's no no concurrency happening here
if we call search it either directly
returns the result or it raises an
exception
let's take search and make it into a
cooperative linear search rather than a
blocking search so we'd like to search
to periodically yield control to a
scheduler which you will see soon this
is very easy all we need to do is insert
the word yield here we're not going to
yield any particular result we're just
going to yield okay
now search is not a regular function I'm
also going to rename it to async search
to remind us that it's now quite a
different animal so let's import async
search in Lucas and we will invoke async
search just as we did previously I'm
using a slightly different predicate
here so we'd actually run it for
gazillions of iterations we just win it
for a few so what is G right G is not
the thing we're looking for with the
search right G is a generator object
right there's an object which
encapsulates
function and the running state of that
function we haven't stuck yet started
iterating that thing but the generator
encapsulate the code and the state of
that particular invocation so this is
called now called the generator function
and to make it run and up until the
first yield we need to iterate it and
where we iterate in Python is by Q can
be built in next function so do next G
and nothing happens we link anything
back what's happened is this function as
I run up to the yield yielded nothing
and we get nothing back here
okay so just we've just made it run one
iteration of that function okay now we
can make it run the next iteration of
that function so you send we call next
with iterating one one more loop on that
search loop yeah so what's interesting
and notable about this is that while
we're progressing this function over
here we can do other stuff at the same
time right I can print hello world now
while that function is running then I
can go back to running advancing my
search for next next next
eventually we fall off the end of the
fall so eventually we get to return item
we find the item now normally in
generator functions in Python we have
yield some value and return nothing
right we've flipped that around here
we're yielding nothing and we're
returning an item and when you return a
value from a generator function
generator functions that return raise
what's called a stop iteration exception
if you return a value Python packages up
the return value in the payload of that
stop iteration exception object which is
kind of unexpected and weird and very
rarely used in Python but we've just
used it to actually quite good effect
and make a concurrent search
well as you can see this is this is
pretty awkward to work with isn't it
this is quite messy but in theory we
have or in practice we have a concurrent
search so async search is a generator
function calling async search always
returns a generator object we progress
the search by invoking next on the
generator object and the final result is
quote is is returned and I use the scare
quotes for real here is returned as an
exception payload everybody with me okay
because if you lost at this point
right the rest of the talk is really
going to be a waste of your time okay so
because that's quite hard to work with
let's wrap it up a bit into a task so a
task is a very this is the entirety of
the codes task right it is a class with
an initializer which accepts a routine
like the generator function we just had
we passed the Co routine to it and it
has an ID and we ought the ID we have a
keep a class ID here which we increment
each time and we just assign the next ID
to each task so we get task 0 it has
born test - it's just a very thin
wrapper around the routine now we need a
scheduler to run this routine for us
because doing all this next stop
iteration stuff by hand is is quite
fiddly so our scheduler is very simple
the scheduler contains a queue of
runnable tasks and it contains two
dictionaries which contained results
from completed tasks or exceptions which
have been raised by tasks which are
failed
we have a function ad for a routine
which is essentially a wrapper
it actually wraps the routine in a task
and adds the task to the queue
and returns the task ID now extremely
simple and then we have this function
here run to completion which actually is
going to run the tasks this is kind of
simple and as a certain elegance too I
think while there are tasks to run in
the queue take the first task off the
front of the queue print the message
that we are about to run the task with
that ID try to run it by iterating it
with next capture the result that comes
back from next that should be known okay
if passing its own x-rays the stop
iteration with tasks has finished as
returned the result which we need to
capture from the accept so the exception
is captured as stopped we capture the
value payload from the exception and
stick that into the dictionary of
results for finished tasks if we get
another exception it isn't the stop
iteration something else has gone wrong
we basically do the same again except we
stick the exception object into the
dictionary failed tasks okay else blocks
on try blocks are something that's
particular to Python and a bit curious
you end up in an else you end up in a
try else if the try was successful this
is the least intuitive thing in the
whole language and I just have to tell
you that so we're going to come here if
we successfully iterated and it didn't
return we assert that yield it is non
because if we someone's yielding
something else they're not playing by
the rules we'd expect them to play by we
print the message and we take the task
and we stick it back on the back of a
queue so we'll go around again and so we
always pop the task off the front and if
it needs to be read it again we stick it
back on the other end it's quite has a
certain elegance to it I think that is
the entirety of the scheduler for
co-routines
everyone ok with that
let's go so let's try it let's import
our scheduler and instantiate one and we
will add our async search tasks so that
on the Lukas sequence of this predicate
the first number with six or more digits
and called run to completion and we only
have one task so it's just going to spin
round that while loop running task zero
next next next next next and eventually
it will raise the exception and will
capture the result here and then we can
go in and pop the results out of that
dictionary this is for task 0 which is
the only task we've created so we have a
schedule that can run one task
whew but of course we can learn more
than one task so using the same
scheduler instance we'll add another
task we'll just do two searches one four
seven digit Lucas numbers a month of
nine digit Lucas numbers now when we run
to completion
we alternately run task one two one two
one two
that's one completes first we get the
result half two takes a bit longer then
we get the second result here okay
nice so really simple concurrent
execution using co-routines implemented
using python generators so you could do
this in versions of Python going back to
like two point four three or something
yes long way back okay
so Lucas numbers is fine but we need to
really demonstrate this we need some
slightly heavier computation so and also
to demonstrate some problems you run
into with concurrent programming we need
some heavier computation so here's my
Nadi is primer is prime prime ality
testing function
don't use this for real it's horribly
inefficient but that actually helps in
this case and of course it works as you
would expect we can test prime allottee
of numbers the point about this is
because it's horribly inefficient
although I can test the prime allottee
of that in like half a second
testing the prime ality of that takes
half a week right it takes a long time
right I have to maybe a half a week but
I have to run through 1.5 billion
numbers too for this to return true it
takes a while
Python is not a fast language hmm okay
here's another co-routine this is very
similar to the search co-routine except
it prints all the matching results okay
so we're just looping round anything
that matches we print that we found an
item and we yield to the scheduler now
so we're just going to loop to the
sequence so here we go we import async
print matches is prime and lucus we set
up the scheduler we add a task so print
all the numbers from the luca sequence
which are prime ok off we go we get to 4
is not prime 3 etc and we get prime
numbers with of course increasing gaps
as we go up the sequence ok so that was
one that this is one task running just
task zero there we will now write
another message and another task to run
so I've cooked at this task here called
repetitive message all it does is it
prints a string at some time interval
now so infinite loop this task never
completes just runs forever
prints the message and then this is
basically a sleep right we just wait
until the duration expires then we break
out of the in a while loop and go round
again and print the message again very
straightforward
to make this cooperative we just need to
add the yield in the inner loop okay and
being good citizens will rename it as a
sync repetitive message
there's a subtle bug here if the
interval is a very small number like the
zero all right this will never yield
alright because you'll come in here
break go back here come back to here
break and you'll never get to this yield
you have to be really quite careful
about where you yield control you need
to yield control there you need to
ensure that all your co-routines and
even though I'm not writing async IO
code this rule applies to async REO you
need to ensure that all your co-routines
yield will either return result
immediately or yield now so you need to
be quite careful about the control flow
here and make sure you yielding at least
once
otherwise you'll hang the whole system
so we need to get this yield in the
right place which is there so let's see
this running
we'll import the stuff we need
instantiate a scheduler schedule a task
I spend a lot of time in airports if I'd
recently been to skip all this would say
mind your step
okay so we've set up a unattended
baggage will be destroyed every two and
a half seconds and we're going to
calculate primes from the Lukas sequence
concurrently rent completion let's go we
get our message after two and a half
seconds there's one at the beginning as
well but now we stop getting our
messages all right we should be getting
an attended baggage will be destroyed
every two and a half seconds but we
haven't we're stalled on task zero here
what's going on now eventually we get
one after a very large prime number
right that's because determining the
prime allottee of large numbers takes a
long time and even though our search
function is concurrent abnormality
testing function is not it's blocking
right so we've committed a great sin
here in concurrent programming of
invoking a blocking function right and
the whole system has to wait for that to
complete so there we are a sync print
matches is non-blocking but is prime
blocks on large numbers stalling the
whole system so we need to fix that so
we've just discovered a rule really
which is that everything you call
transitively from a KO routine should be
non blocking okay everything because if
you call anything blocking in the entire
call stack you're going to hang the
whole system so KO routines are
contagious to call ease is one way of
looking about looking at that right
okay so let's fix our is prime function
so here is the original is Prime on the
left and calling this returns a bool
true or false
as async is prime which is exactly the
same thing except now we have a yield in
this loop right and calling this of
course returns a generator object so we
need to iterate it with next and it will
work with our scheduler we also need to
fix the every call site of is prime
because calling it returns a generator
we cannot just call it we have to
iterate it and the way we call nested
generators from generators is with
yields from which if my memory serves me
correctly is Python 3 only now if you
did this in Python 2 you'd have to write
an explicit for loop here to iterate
that thing okay so this avoids having to
having to put an extra for loop in here
so now we're using yield from because
we're using yields from which eventually
arrived is going to a sync predicate in
this case is async is prime because we
have this yield form and this yield here
we no longer need this yield here so
this is a term I've invented bag yield
right you don't need it in this case as
long as you are yielding to another Co
routine that you're delegating to okay
so we can get rid of the bear yield I'll
come back to this term bear yield in a
minute this is the second rule which is
that everything that calls transitively
your Co routine must iterate the
generator so not only our co-routines
contagious two core leaves they are
contagious to call others and of course
you don't have to think about this very
hard to realize that very soon your
entire program needs to be asynchronous
except for the main function that is
logically where you end up yeah or at
least it's not asynchronous at least
non-blocking right
so it's very hard to take an existing
system which is not written in this
style and turn it into a system written
in this style this is a good thing to
adopt at the beginning right but you're
going to have a pretty hard time we
fracturing your way from whatever you
have something that works that like this
because of this contagious viral
property of asynchronous programming so
here's another case where this this
contagious thing crops up so we had
async repetitive message here which
basically printed a message and then how
to sleep this thing here is sleep
effectively right so let's do a nice
refactoring and extract an async sleep
function or extract generator
refactoring so here we have extracted
that all the code except the print into
async sleep which is here this now has
the bare yield and we can yield from
async sleep here okay so that's just a
straightforward extract function or
extract method refactoring done with
generators okay so rather than a call we
have to use yield form here
make sense
following good so this sleep thing turns
out to be significant so async sleep
always yields at least once because
that's one of the guidelines I gave you
right we come in here and you always get
to that yield at least once a sing sleep
zero yields exactly once right which is
nice a useful property
so any because of this any occurrences
of bare yield anywhere else in our
system can be replaced
we've yield from async sleep zero and
that means the only bare yield we need
in the entire system can live inside the
sleep function all right which further
means that if the async sleep function
is provided to you by the scheduler you
don't need any bear yields anywhere in
your system right so the only bear yield
then lives can live inside the scheduler
itself so by applying this replacement
of bear yield with async sleep we can
take all the other code we've written so
far we can find all the bear yields is
one here and one here and we can replace
them with yield from async sleep zero
now you just think well okay I'm just
added some code to no effect well
actually that's going to turn out to be
important in a refactoring that's coming
soon
they're just to prove to you that it was
actually a refactoring and we haven't
changed the behavior of the system one
iota run it again we'll set up a
repetitive message remember now we have
a sink is Prime rather than is prime
printer completion we get AB prime
numbers and we should keep getting a
repetitive message now because is prime
is no longer blocking yeah let's look e
going and I think the next one we have
to wait like half an hour so I'm not
going to do that you do have to wait a
while okay so we're about halfway
through I guess yes exactly halfway
through which is great so we've got this
far without mentioning async IO but I've
actually built something that does
exactly what async IO does I just
haven't used all the new fancy keywords
that we get in Python 3.5 and 3.6
in fact what I've shown you is very
close to how a seed in Clio is used in
Python 3.4 before they introduced the
weight and async keywords okay so
knowing what I've just shown you you
could any Python 3.4 code will look very
familiar and in fact what I have shown
you is so closely equivalent to async IO
I can actually programmatically refactor
why I've written into the real async IO
code or mechanically refactor so let's
do that so there's all the code we've
shown you so far on the right here I'll
show you the refactoring steps so the
first thing we do is we're going to take
any time I've written def async
underscore foo
and we're going to refactor that to a
sync desk through which is a new keyword
in Python okay
now we have a sink desk this is a way of
declaring a Co routine in pison 3.5 and
onwards next thing is of course I'm
going to import a sink IO up there there
it is now I'm going to replace my calls
to a sink sleep my own sleep function
with calls to a sink IO dot to the sleep
which is the light of provided function
so we do that refactoring now because
I'm calling the library sleep I don't
need my sleep anymore we can get rid of
that that's gone finally we can replace
any occurrence of yields from with a
weight which is a new keyword in - 3.5
and onwards and behind the scenes Python
is working exactly the same way with
iterators and generators and all these
things right so that's all the async and
a weight are doing the only thing they
do over and above that is they actually
give the function slightly different
types of Python knows they are awaited
bolt so that is now legitimate - 3.5 or
3.6 code so let's run it in python 3.5
slight differences here I have to have
to get the async IO event loop now
rather than creating my own scheduler I
don't need my own schedule anymore the
async IO event loop is pythons scheduler
instead of using scheduler but a dine
and do create tasks I'm going to come
back to tasks in a bit and explain what
an async IO task is so create two tasks
if I actually adds them to the scheduler
it doesn't just create the task object
actually schedules it with the scheduler
slightly confusing naming then we're
going to run our scheduler forever
and we get the same result just like we
had with our own code I'm not going to
make make you wait for the the next
large prime okay so let's get into the
some more details of what async io some
of the things we've established about
async IO
by building a fake async i/o so
co-routines implement tasks carotenes
await other co-routines
the event loop schedules concurrent
tasks tasks must not block if you task
blocks everything blocks by a waiting
another co-routine you facilitate a
context switch every time you do that
every time you use a weight that is an
opportunity for the scheduler to run
another task they might decide to run
your task again especially if it's the
only task but that is the opportunity
for a for the scheduler the event loop
to intervene and if you want to yield
control without needing a result from
the thing your yielding from you the way
to do that is async is sleep zero okay
that is that just says I want to yield
control that somebody else have a go
another task have a go
a word on terminology this is gets a bit
confusing this is a Co routine it's the
code
it's made of code and it's callable
right we can call this the thing we get
back when we invoke the KO routine is a
KO routine object right you might not
like this terminology but it's the
terminology that Python has settled on
in the documentation so this is the code
and it's callable this is the running
code and its execution state and it's a
way table this is a new word in Python
terminology so the co-routine object is
our way table
so very often we want to monitor a
running task the tasks we've seen so far
you can't monitor them in any way you
just have to run them and wait until
they are complete sometimes we want
dependencies between tasks we want one
task to report on the progress of
another task or we want a task to wait
for another task to complete so the most
basic way we have to do that is I think
what future so let's look at the code we
have here I have I was taking our search
function and I've modified it to accept
if future objects will come on to what
that is in a moment and all my monitored
search does is delegate to this regular
search function we had earlier this is a
regular search co-routine we had earlier
with a weight so we try this if the
search fails we set we call a method on
the future called set exception with the
exception value else the trial succeeded
we set the result the found item in the
future okay so the future is an object
which encapsulate s' the idea of a
potential future results and IOU if you
like
and then we have another task here
monitor future into which we can pass
the same future and we're just going to
loop around reporting on whether that
future is done yet every few seconds
okay so in the main code here we get the
event loop we create a future using this
factory function on the event loop I set
up my monitored search looking for the
first 13 digit prime that takes a while
and add the co-routine object that comes
back from that invocation to the event
loop as a task here I create another
task using monitor future and I pass the
same future instance here so I'm
creating the future here and pass the
same future into the monitored search
and into this monitor future task okay
and then I run until complete off we go
and we get waiting waiting waiting
waiting reported from the monitor future
task until the monitored search returns
its result okay we then get a warning
from async IO event leap saying hang on
you've terminated the event loop here
with loop closed but there are still
running tasks you probably don't intend
to do that okay so even though our
monitor future says while future not
done and should wind itself up when the
future is done
it never gets opportunity to wind itself
up right because because we exit as soon
as the future is complete okay so this
final iteration of this loop never gets
to happen so we'll come back to fixing
this error in a bit we'll just leave it
there for now future that there is a
class called future in async IO you
should avoid calling the future
constructor directly you should always
go via the create future factory
function on the event loop you're using
the reason for that is that different
event loops will specialize the future
implementation okay so at the very least
Windows and UNIX do use different event
loop implementations
so you should you should always go via
the factory function so now we're going
to have a task and a task in Python
I see I Sinclair is obviously a
fundamental thing we use them all over
the place I've been creating tasks
already I think the documentation for
what a task actually is is awful it's
dreadful
so I mean adduct meant a shin said
something like a task wraps acro routine
in the future and that's just very
ambiguous to me so this is a picture of
what a task actually is it's a subclass
of future so it literally is a future
and it has a reference to recur routine
this is a really terrible use of
inheritance in the design of the
language it never makes sense to call
set result or set exception on a task it
just that should not be available to
anyone who has a handle on the task
except the scheduler so I think it's
really not good design but that's how it
is and we have to live with it I think
it's a well-known fact that the people
who design person don't really
understand object-oriented programming
in some sense anyway so perhaps no
surprise yeah it well it's in the Python
standard library so it is I mean
well this could contain this could
contain the future rather than being a
future all right yeah so well the future
could be a implementation detail hidden
as far as we can hide things in Python
right so I mean Essex private right so
this is Python we're all grown-ups here
so so because the task is a future we
can monitor the search tasks directly
now rather than having to create our own
future object separately okay so when we
do that we get exactly the same results
as before but we still haven't solved
this problem of the task being destroyed
the monitor future tasks being destroyed
prematurely another very confusing
naming thing in the library is how you
should create tasks so the same reasons
as futures you shouldn't call the task
constructed directly you should prefer
to call the create task Factory on the
event loop that allows the event loop
implementation to specialize the task
even better though you should call
ensure future instead ensure future
actually calls this and I find this name
very confusing it's more general it
doesn't just accept Co routine objects
it accepts any a way table object which
is a new concept in Python and you pass
you must pass the event loop too it's
also idempotent so you can pass the same
thing to it several times and you just
it will only wrap it up once in a task
so this is the preferred way to create a
task in Python ensure future yeah in
spite of its confusing name it does
return a task so there we are you create
the tasks so we still haven't dealt with
the fact we're not cleaning up one of
our tasks gracefully
so to do that we can wait on more than
one task so we use an async IO function
called gather which takes a any number
of away tables so here and this returns
a future so here I'm gathering these two
tasks and getting a future back here so
now I'm saying run until complete this
future this future is is done when both
of the tasks I've gathered are done okay
so that will fix that that problem okay
let's come onto the event loop itself
which have been using was called the few
functions on it like create tasks and
create future but the API of the event
loop is huge I mean it it really is the
kind of Gorge class really just does
everything so these are the kinds of
things you can find in the event loop
interface starting and stopping the
event loop scheduling callbacks these
are not co-routines these are just
regular callback functions we can
arrange for them to be called at some
point in the future Factory is that I've
just told you you shouldn't be using
because there are better alternatives
configuration things you really need
there's more there's a bunch of stuff
for configuring our exception handling
works there are some methods for
configuring diagnostics debugging modes
methods for dealing with asynchronous
things like signals sent from the
operating system sometimes you know I'm
not going to go into these in detail
today I don't have time but sometimes
you do need to call blocking code from
your async code so what do you do you
can use a thing called executor so you
can take some blocking callable and you
can you run it in the event event loop
using running executor and there are
different executors you can pass here
there's a thread pool executor and a
process pool executor so you can arrange
for that function to be run elsewhere in
such a way that you're blocking function
becomes non blocking by being in another
thread and you can set the default
executor
there's a bunch of low-level socket
level stuff which we're not going to
bother ourselves with today and then
there's this much higher level protocol
level stuff create connection create
data Gramm endpoint create UNIX
connection we have some functions for
watching file descriptors adding readers
removing readers adding writers to file
descriptors and also we can deal with
pipes so what is all this io stuff doing
in the event loop right why do we need
it in there well okay it is called async
IO which is the hint but why does it
need to be in the event loop so that's
in the last 10 minutes here just put
some IO into async REO so I earlier
built a very simple round-robin
scheduler just ran each task in turn ok
the secret sauce in async IO is that it
can schedule tasks on the basis of the
availability or readiness of IO right so
the IO aware scheduler used in async io
is a bit smarter so as we clock around
the tasks run each one in turn if we get
to a task of the scheduler knows is
waiting for IO we don't need to run it
we can skip it you keep going around if
that IO then becomes ready you can go
back and run that task immediately as
the next task we run so we don't have to
wait very long and then we can carry on
round maybe we run that task it blocks
on IO the scheduler knows that it that
IO channel becomes available the data is
there we can go back and get it
immediately so that's why all this IO
socket level stuff pipe level stuff is
in the event loop in the scheduler it's
so that the scheduler can be aware and
wait on IO events using Ipoh lor select
or whatever operating system features
you have for wait for a single at that
lower level okay
click through these so it would take me
all day to go for all of this stuff in
details I'm not going to show you this
picture of the stack of abstractions
that async IO provides and so we have
seen just suspend herbal and resumable
functions basically concurrent functions
implemented called kuru genes that
implemented in terms of generators
python generators we've seen the event
loop which and tasks which layers the
notion of a task on top of that and
futures so that we can wait for
co-routines
and get some sense of when they've
finished and also the event loops adds
the ability to await sockets file
descriptors pipes etc on top of that
that we layer a thing called transports
which are provided by async i/o and
these basically encapsulate the notion
of a communication channel which
supports read and write ok I'm not going
to show you transports in detail I'll
show you them being used but I won't
show you how they work on top of
transports async i/o allows us to
implement the thing called protocols
which are don't use co-routines at all
they're entirely callback based approach
to programming I'll show you that in a
moment so that allows us to deal with
callbacks that telephone connections
have been made or lost or data has been
received and we can respond to those
events then on top of protocols async IO
layers a thing called streams which
allows us to begin working with cout
routines again so if we don't like
working in a callback style we can go
back to working at NATO routine style so
we have this huge stack which is founded
on co-routines and all these higher
level abstractions and eventually we get
back to working in co-routines
but at the top level working at a much
higher level of abstraction the kind of
behaviors we needed to handle in our
code so I'm going to show you we have 10
minutes 9 minutes left so I'm going to
show you a quick example here of
a chat server right simple thing so
here's a chatroom class we initialize it
with an event loop and we give it a port
and we give the chatroom a name and this
is important thing here we're going to
manage a mapping a dictionary for of
user names people in the chatroom two
transports and you're going to see these
transport objects there are things that
we can read and write to okay read from
and write you when we run the chatroom
it's going to call this method on the
event local create server and create
server will invoke this callable this
factory function for each new connection
to the chatroom
okay so our connection comes in it will
invoke fisting and it will create a
thing called the protocol which you'll
see in a minute with a reference back to
the chatroom okay what else does the
chatroom have well we have a way to deal
with users arriving register user so we
can only register the same user once in
the dictionary if it's a new user we
against their username we keep a
reference to their transport which is
how we can communicate with that user by
writing to that transport and we
broadcast to the chatroom user arrived
we can deregister a user when they leave
we can get a list of users mate rating
through the keys of that dictionary when
a message arrives from a user we can
broadcast that to all of the users
prefixed with their username so we know
who it's from
and here's the broadcast I want to
broadcast a message we just iterate over
all the transports and send the message
with the correct line ending for telnet
which is what we're going to use here so
I said that every time a new connection
is created async i/o will instantiate
one of our protocol objects so here's
the protocol object so the server will
instantiate this for each one for each
connection with a reference back to the
chatroom
and it's extremely simple there's
basically this state machine right it's
going to call connection made exactly
once it's going to call data received as
many times as necessary and it's going
to call connection lost exactly once
it's very simple so these three methods
are really all you need to implement to
implement a async i/o protocol it's very
straightforward
so when a connection is made I get a
reference to the transport from async
i/o I write hello welcome to the
chatroom name and I ask them to enter
the user name when I get data received I
have to decode that into a string and
then I push it into this accumulated
lines thing which cuz I just get bits of
data right they're not logical lines I
have to look for that line endings in
that data as it arrives so I wrote this
function which you'll see in a moment
just extract lines of text from whatever
data I'm getting and then when I get a
complete line I pass it to this handle
line when the user leaves the connection
is lost I just D register the user okay
you don't need to understand the detail
here this is just looking for lines of
text in the data that comes in and the
very first thing I expect the user to
enter is their username because that's
the first thing we ask them for so if
username is norm the first text they
give us is going to be their username
otherwise we send a message from that
user what's interesting here as of
course we might have a thousand users in
this room and this simple code would
handle thousands of users no problem we
have shared mutable states here right
but we don't have to worry about locking
or any of that because we know that only
one of our co-routines is executing at
any instance right so a completely
sidestepped the problem of the shared
mutable resource in a concurrent system
all the code I'm showing you here looks
completely sequential it is completely
sequential right it's the fact that we
can suspend an execution and you can
just assume that you have access to this
so the long
you're careful about when you're
yielding control to the scheduler with
your awaits you can assume that you have
exclusive access to any shared resource
between those weight statements what you
can't do is having a weight in the
middle of something where you have
essentially kind of transactional
behavior which either must complete
completely or not at all
so there we go register user D register
user there's a command to list users and
then we just write text to the
transports to get things to show up so
that is that and I'm just going to
finish by showing you so here is the the
chat server is the chat room code you
can see it's quite concise is the async
i/o protocol which inherits from the
base class in the library with the the
three message that just showed you
connection made data received connection
lost okay so we can run this that
service now running now I've got a
couple of terminals here I know you
shouldn't use telnet but it's convenient
then there's security people here on
there so I'm going to tell that into my
computer on port one two three four and
you see welcome to chat box enter your
username
I am Sheila okay
user Sheila's arrived in another session
over here well it's a chat box Jim and
you see we get Kim has arrived and then
we can say aye Sheila
there we go very simple right so nice
concurrent behavior multiple connections
no parallelism whatsoever an efficient
IO so I haven't tested it but I would
expect this to scale to thousands of
concurrent connections without any
difficulty whatsoever
and that ladies and gentlemen is it
thank you very much and I'm I think I'm
out of time but I will take questions if
you want yeah yeah use possible that
some a patient asks ask time do your
children if your your task is constantly
became not wait so it was waiting and
now it's not working
yeah let's duel were true to the stock
right it will return to the task
my understanding where it depends on the
event loop implementation right so you
can you can write your own event loop
based upon the stuff is in async REO and
async IO comes with different event
loops there's a reactor event loop and
the proactive event loop and you have to
choose the event loop implementation
used depending on the scheduling
qualities you want right so you actually
have in theory complete control over
what's going on because you could write
your own scheduler but in terms of the
details of how it does that scheduling I
haven't looked very deeply into the
rules I mean I honestly I've never
needed to use anything but the default
scheduler on the that PI that gives me
on the operating system I'm using but
underneath it's just using course to
select or a pole to wait on file the
Scriptures or sockets
okay I believe it's lunchtime or
something okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>