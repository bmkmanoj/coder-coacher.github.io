<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Embracing Docker simplicity - while harnessing platform power - ​Michele Bustamante | Coder Coacher - Coaching Coders</title><meta content="Embracing Docker simplicity - while harnessing platform power - ​Michele Bustamante - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Embracing Docker simplicity - while harnessing platform power - ​Michele Bustamante</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n0dWcB0Y2HU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon last session for today so
I'm Michele the Rue Bustamante and I'm
about to talk really fast for one hour
cuz we're gonna cover six platforms and
talk about orchestration and scheduling
and whether or what the features are
that you might care about so I called
this embracing docker simplicity and
harnessing platform flour mostly because
there are ways that you can leverage
docker in deployment scenarios that
don't involve our castration and then
there are things that are castration
gives you platforms give you that you
might find advantageous in certain
solutions that's usually in larger scale
solutions in particular and in those
cases you need to invest in more so we
want to get a taste of what are those
orchestration platforms do and at the
same time maybe see a few different
platforms along the way so I will
proceed to frequently asked questions
that I get all the time one of them is
how do I know if I need an orchestration
platform it's a very good question so
that is a question that I will not be
answering simply put because you cannot
answer it globally but what I can do is
show you the things we might care about
that help you decide so we're going to
do that the other question is how do I
choose the right approach or platform so
you know once I know that I need a
platform you know which one do I choose
and that's another question that I will
not be answering so I'm starting really
firmly at the beginning of the talk with
two things I will not do for you but the
reasoning behind that is that it's very
difficult it's a finessing process you
have to understand the customer and the
culture they have and if they have
affinity to a certain platform already
and so forth so what we want to do is
just learn what are some of the things
those platforms do uniquely partially
I'll be showing that but since we only
have an hour I'll be showing that very
quickly and speaking to it along the way
I need you to pay really full attention
if that's ok all right lots to say
considerations that we'll talk about
things like deployments lit scenarios so
a single container
you know instance versus load balancing
and then orchestration platforms at
scale so that's sort of the progression
that we might think about with docker
solutions and then we're gonna talk
about the features that would be related
to the orchestration platform that you
might care about which would help you to
decide do I need these types of things
and is there another workaround to those
things until I'm ready to go a
full-blown orchestration and invest in
in the effort that it takes to get there
okay so in approximately six minutes
we'll do a docker container lifecycle
will do docker compose networking we'll
talk about deploying containers and use
a simple example and then we will get
into scheduling and constraints an
important feature of orchestration
platforms and will be use Stucker swarm
to look at that and we will look at
orchestration management UI just sort of
from a high level in order to explain
hey these clusters actually usually need
UI to show you what's going on inside
them and then of course all of those
platforms give you visibility into that
in different ways they each have
different tools different UIs so we'll
talk about low bounding blow balancing
and discovery and see that UI with DCOs
we'll talk about recovery and
self-healing and Auto scale and we'll
see the UI therefore with Amazon and
Google container engine but also think
about that as kubernetes which doesn't
have to be deployed with Google
container engine it can also be deployed
on Amazon or Azure or on-premise okay
alright that's a mouthful let's continue
so the first demo we're gonna talk about
just docker 101 and you know some of the
basic things we might think about there
so I'm gonna come in here and just what
I've got installed on my machine
this is my Mac earlier I did a talk on
Windows so I was on my surface book so I
have a docker for Mac installed here
instead of a docker for Windows
obviously and so I have access to docker
from my terminal at this point right so
what I'm gonna do is I have no
containers running showing you that and
docker images we're gonna see I have
just a node base image that I can use
okay and so I'll give a list here and
we're going to go into my docker and
what I have here are an API and a
website I've shown the same demo in
different ways I'm
kind of repurposing that and just
showing that you're gonna have two
containers talking to each other and
what I'm gonna do first is go in here
and look at the API if we take a look
you can see that I have a docker file
and let's take a look at that and that
docker file is essentially saying I want
to use the base image for argon which is
a lightweight nodejs image on top of
Linux and then we're gonna you know copy
over my source and then we're gonna do
an NPM start at some point here there
you go and that's about it so that's my
docker file what the app does is really
it's an API right so it exposes some
endpoints so what I got to do first is
just go ahead and let's build that and
if we take a look you know docker build
is just going to go and build it for my
daus blonde docker hub registry so I'm
using the convention of targeting with
my tag the registry that I'm planning to
push to right so that's typically what
you would do obviously there's different
types of registries that are gonna be
touched on as we talk and when I go and
build this it will just go ahead and try
and you know rebuild the image it will
be an immutable image with my source
running on our gun as a base image and
once this is built I can go ahead and
run it so let's go ahead and take a look
at a run and what we'll do first is just
run it so I'm gonna expose port 3001 and
let's go back one more and we're gonna
call it API so my dns for this one will
be just API okay so now my docker PS or
doaker and we'll show that I have API up
and running and you can see that it's
running at four at three thousand one I
should be able to now browse so let's go
into one of these guys and we'll go
localhost 3001 I don't really have
anything exposed at the root so I'm
gonna type speakers and there we go
bunch of stuff so we know it worked
that's good enough for me so now we've
got an API running what about my web app
right so I'm gonna go back the track
here and I'm gonna go into web and let's
go ahead and do the same thing so
time I'm gonna do a build and we're
gonna call this my web build we're gonna
tag it with das blonde content - web
that would be where I push it in a
repository I'm gonna hit enter and that
will build the image for web so once I
built that image of course I'm gonna run
it and then that image is gonna try and
call the API image will it work is the
question anybody no it's not gonna work
because I haven't set up a network and I
haven't done a compose up and so it
won't know how to reach the API right so
what I've really just done is run two
separate containers but because when the
web runs in its context it won't be able
to reach out to localhost from basically
the you know the docker environment the
machine environment so I need to have
DNS I need to have them join a network I
need to have them to be able to
communicate and that would be something
that also matters in an orchestration
platform they just handle it in
different ways some of them provide you
with DNS through load balancers that are
part of the platform you'll see that
when I do DCOs
for example and some of them require you
to do some complex setup to achieve the
same goal like Amazon for example
because they don't have that need of
concept it's actually all infrastructure
so you have to set up more things so
this just building the web app and once
it's up and running we'll go ahead and
give it a run it's nice seeing red text
flyby because you know something's
happening plus it's really super
exciting
I'm very easily amused obviously and a
one two three it's taking a little
longer so this guy walks into a bar and
he says I'm not going to do that it
wouldn't be appropriate alright so
whenever this is done we won't build
that image again cuz you know that's
enough of that
but basically I'll go ahead and run it
I'm going
hit the API it's not gonna work and then
I'm gonna drop back remove the
containers that I started and then I'm
gonna do compose up on a network
I could also manually start each of them
I'm not going to go through that process
but I could just start the API create a
network start the API and join that
network and then I could you know join
the web app to the network as well when
I run the container and I provide
environment variables on the command
line as well but instead I'm gonna put
it all in a compose llamó file and then
they'll do a compose up and it'll do it
all for me so and we're about there good
looks like it's closed
that is some serious damage building
okay let me do this tell you what while
we're there I'm gonna go to a different
terminal window cuz this guy can stay
busy all he likes but I still have other
options so let's go in here and we'll go
to talker and what I'm gonna do is I'm
gonna cat this docker compose file and
let's take a look at what I've got in
here so what I'm gonna do when I do a
compose up is I have a version three
compose I don't know build the image but
it's already going to be built I'm going
to expose the same port so I would have
I'm gonna have a environment variable
override that tells it where the API is
traditional and then I'm going to
specify a network called fat medical
it's the fictitious site that this is
and so while this is going let's go see
where this other guy went or landed and
looks like we're done yeah okay
so if I run this this will run the API
which we already did now I'm going to
run the web app without a network and
this will expose the port 3000 so let's
give that a good go and come over here
and we'll go to port 3000 so I still
have three thousand one four speakers
for the speaker endpoint on the API this
is the speaker endpoint we're trying to
hit it won't be able to get there so I
have no content right so that's that
okay now let's go back to docker PS and
what we'll see is I've got these two
containers running I'm gonna do my
magical blow it all away force quit of
those so that they go okay and then I'm
going to back drop until I get to the
composed level and I'll do a compose up
hope excuse me
and we'll do that okay so that's gonna
basically run and do the networking bit
that I suggested I guess because it's
probably renaming the image that might
actually go do a rebuild so we're gonna
let that do its thinking in the meantime
one of the things that I guess I'm
taking you through here is the fact that
you can individually run your images and
that would be fine I could go to a VM in
the cloud and therefore you know have
docker running on the VM put one image
up there expose an endpoint port 80 and
that would be the only thing on the VM
it would be as if I had a web app you
know running in a web server only it
would be maybe if it's node it's gonna
have Express as the you know reverse
proxy and web server if it were Java it
might be you know Apache if it were
dotnet on Linux it would probably be
Kestrel for example in da net core so
that would be the idea in this case you
know when we do a compose we're getting
you know more than one composition
application and so when we run we're
getting multiple containers and then we
have to figure out well what are those
ports going to be so they can't use the
same port we would have to have port
3000 3001 unless we have a web server on
the VM that's knowing how to route in
which case we would set up more
interesting routing rules at the
webserver level so that we could have
wacky API and whack web for example
so all these manual things you'd have to
do are completely automated well if you
use you know some sort of scripting for
your cloud platform it's just that it's
not baked into anything like an
orchestration platform might give you a
little bit more out-of-the-box if that
makes sense and so we're thinking more
about hey what is my endpoint kind of be
I have to structure my task definitions
or my compose files to know this is
going to be my port for that thing
unless I get something called dynamic
discovery which we're gonna see what
DCOs and so forth okay all right so I'm
gonna let that do its thing and then
we'll come back to it but that was my
quick sort of super intro tour to get us
some foundation and once that's running
we'll show that it's working and we'll
all be happy
so docker compose will let me run multi
container app
occations but what we really think about
when we think about docker compose files
is task definitions every single
orchestration platform has the concept
of a task definition so you know with
Amazon with Google with DCOs
kubernetes there's always a service
definition a task definition an
application definition that essentially
defines the things that compose does so
a lot of times there's transformations
between docker compose which is native
to docker and swarm into these other
task definition formats what they're
trying to do is describe what is the
application how many services obviously
any constraints and ports and things so
you can see that you know part of that
could be the network's any volumes that
you're mapping if you need to have an
external volume from the VM it could be
environment variables which we've seen
and then there's a whole lot of other
configurations that might be platform
specific like when we get into DCOs
there could be configurations for the
load balancer on top of this so you can
extend compose to do more things compose
is native to swarm swarm is negative to
docker right so when we think about you
know composition we're thinking about
again tasks definitions now before I can
deploy a task definition to an
orchestration platform let's just drop
back again and talk about the images I'm
still at the end of the day building
these immutable images that need to
deploy somewhere and when running
locally I have an image registry locally
as part of my docker you know form a
core docker for Windows environments so
that I can run you know from my local
repository if you will if I want to
deploy these to the cloud I have to push
them to some accessible registry so
maybe docker hub or docker store maybe
it could be you know as your container
registry things like docker data center
now called docker Enterprise Edition
actually have their own registry built
into the platform
things like DCOs provide the ability to
run a registry as part of the platform
if you like so that it's all contained
as part of the same cluster of VMs so
the idea is based on the place you're
going to deploy based on the tool you're
using there might be a natural choice
for which
distri we're actually gonna work with
right I'll use docker hub for demos
because then it's just open and I have
an open account I use for that but in
production you know with with your you
know actual enterprise assets you're
likely to use Azure if you're an azure
the azure container registry amazon has
its container registry for when you're
in amazon google has it's and then if
you're going sort of native platform
like kubernetes TCOs you may still
decide to use the cloud platform choice
okay so these are just things to
consider so ultimately what I'm looking
for is a way to take these local machine
builds from code to docker image and
that immutable image now be something I
can push up to a registry and tag the
tags are really important because not
only do they indicate where they're
going to deploy probably compatible with
the registry but also they indicate
things like is it the latest version
what's the version number what was the
bill that came out of if it's CI what is
this tart is this our latest prod
release do we have a naming convention
for what means it's ready to go to prod
and then that would be now a promotion
of this image to we're ready to release
you so instead of version one being the
prod tag it will now be version 2 has
the protagonist that help me promote
okay and so pushing to the registry is
really as simple as doing a docker push
for example right and I might have to
set up the registry locally so I have
the credentials to push to asher or
Amazon or otherwise but for docker hub
I've already logged in so that will
already be accessible to me in addition
I might just automate those bills I
might just go ahead and do a check in or
a commit and that would automatically
generate the images and deploy to my
registry of choice which I would have
set up then as part of the pipeline for
4ci okay so that's the idea there so
let's go ahead and take a look back at
where we were which looks like it's done
so let's do a docker PS it looks like I
do have my web in my API running at this
point let's do a docker Network inspect
fab medical and it looks like I have a
fab medical with a docker API one
deployed in there and a joker web one so
there's two containers deployed as part
of this network okay
because they're deployed as part of the
network I should now be able to browse
and it should now show me some content
right so now we're hitting the API
because I had configured the environment
variable to look at DNS to find the API
under the name API directly right as
opposed to localhost 3001 okay okay so
that's all working now let's talk about
the CI part so what I have here is let's
see if we can list I'm going to go into
the API here and let's do a look at one
of the file so let me go into sublime
and oops let's do that again don't mean
to make you dizzy okay let's do this
there we go
okay so over here somewhere I have my
daugher and API so let's go into one of
the files server not found go away
please I'm just going to change a file
just to make it so that there's
something different okay so nothing that
we're going to be able to see because
I'm just going to go through this
quickly but let's go into here and now I
got this get status okay so we're going
to do an ad and we're going to do a git
commit changed a message and now we've
got something to push but before I push
let me just go ahead and go to our
content API CI this is my docker hub and
then here is my just belong directory so
in my desk long directory I have this
content API I'm gonna check into there
or commit this change when I commit this
change again this can be done different
ways with different registries right or
different CI tools but when I commit to
get I've actually connected this docker
hub content API CI and it's got some
settings that it will you know monitor
with a hook the the commits over on my
repository and it will go ahead and pull
and build the image so we're gonna see a
new image getting built and it's going
to label it with the latest so again
these are just standard things you're
gonna do not that you're gonna memorize
this process obviously that's pretty
straightforward and then over here we
can see the history of the builds right
so somewhere over here we'll get to you
know a day ago I have run this so now
you know I practiced this at some point
right
so the bid doesn't work you know it's
not because like I'm lazy so I'm gonna
go back over here and let's do a git
status and do a git push hopefully still
remembers my credentials
takes care of business okay
now let's come back to this and at some
point here we're gonna see this refresh
and start building the next image which
it will tag is latest so there you go
so now that I've got an image getting
built obviously that's not handling the
deployment side of it there's a lot to
talk about on the CD side because there
are tools that I find really really
useful for managing container
deployments once we get to the
orchestration discussion in short what I
would say is this what you want to use
is a tool that allows you to have sort
of a holistic picture of all the things
you're deploying to your container
platform so run deck is an example of a
tool that I I like introduced to me by
other folks that do that kind of thing
day-to-day so I'm not an expert audit or
anything but I've enjoyed that because
it gives me a way to go back in history
and see the container builds or you know
it can be multiple tasks definitions
that we push to Amazon or to the
container platform and you can see that
we went and pulled you know version 1.2
of all the images that kind of go in a
bundle and done that push and then if
something goes wrong we can roll back
that whole task and go back to a
previous run deck job and rerun the
previous job and it will go get those
immutable images and replay so rather
than automatically just getting these
you know commits creating new images if
that feels chaotic you can actually do
it in a more controlled manner also from
the outside in these are just things to
think about right so there's tools for
both sides at the end of the day this is
just showing that of course I can build
images automatically they are immutable
and I can start working with those okay
so let's talk about how we might deploy
that service so I did a commit right and
I did build an image and it's in it's
it's in progress and right now I'm gonna
have a new latest image of this API CI
and you know one of the things I might
consider doing is just going to a VM and
doing a pull right so if I don't have a
tool that automate
telling it to do so then I would have to
go onto the VM and do a get a you know a
a pull from the repository and then run
the container myself if I don't have an
orchestration platform that knows how to
run tasks then something has to do that
job the simplest possible thing I can
think of is a headless container
platform and although it's really early
days for this it's an opportunity maybe
to talk about Azure container instances
so that lets me go right into the azure
portal and tell it a container to run
and then it will provision the VM it's a
small VM just for that one container and
it will run that as it will pull that
from the repository that I tell it to so
let me head over to that screen here and
we'll go in - sure so what I have over
here is let's just let the screen
refresh a bit I have a Asscher container
instance namespace if you will or
resource group ACI demos and in here
I've already deployed a couple of
different images one was a Linux one and
one was a window so you can actually
have these loose containers that just
have a space to run almost like a lambda
in Amazon or a function in Azure
except for that it's a container instead
so it can be any code base you like so
it's pretty powerful concept that down
the road you can imagine somebody
putting management around stuff like
that and then you don't need a container
platform but we're not there yet but if
you just need to run some loose
containers and have them scale this is
an example so I'll use that as a pre
container platform idea I'm going to go
into the bash command prompt here and
we'll take a look so that'll create a
connection to the shell so that I can
deploy
and this is just a quick way instead of
using arm from outside and automating
from you know either
chair form or arm or something like that
so I'm just coming in here just to issue
a simple command the same thing that I
could do from a powershell script or
something like that and this is probably
and so while that's kind of thinking
let's talk a little bit about pre
container platform deployments so this
is an example of just go ahead and
create me a container so I'm saying AZ
container create use public port three
thousand one make it a public IP so I
can reach it put it in my resource group
for a CI demos and somewhere over here
we're calling it content API two I don't
have to do that I can call it whatever I
like and then it's coming from the image
das blonde content API probably need to
do that faster apparently so
this will just show how quick it can
spin up just like a function or a lambda
would so we'll get that done another
option that I might have in Azure
particularly is this sort of paas idea
around app services so app services are
managed
you know VMs so you don't actually think
about you know managing the VM you just
have these tools for scaling out and in
but it's actually provisioned for you
and you can define the size and so forth
so it's a pass offering that gives you
out-of-the-box platform as a service
basically right and now they support
Linux containers on those so that's a
preview item and eventually they'll have
obviously the same for Windows so the
point is it's not a class it's not a
container orchestration platform it's
just a VM that's ready for a container
to get deployed and that way instead of
you managing getting docker set up I'm
just going to hit go before this gets
out of hand
you would just be able to deploy and and
you know not have to manage the VM it's
not have to manage orchestration but you
won't have orchestration you'll still
have to think about ports I have a
container on a VM I can only use port
3000 once or port 80 once unless I have
a web server to do routing right you
still have to think about the topology
of what gets placed in the VM now this
has done its job so I'm going to school
scale out here
so we can see what happened and if i
refresh here I should see that it's
already there and if I click on content
API - that was it's super fast right so
it just provisioned me with this IP
address so let me go ahead and copy that
IP and let's go browse to that IP and I
think I gave it port 3001 just like
before and at some point this should
then just return for me the same content
that we saw before because it's the same
container
something like that
oh you know what sorry let me just put
the IP first did I tell it top public
public IP yes yeah possibly possibly
yeah cuz it's I told it to use that port
so okay well there we go alright so so
again for me to not care about
orchestration that means I'm either
doing this which would be the ideal
consideration so that I don't have to
worry about you know anything but what
port do I want and let's just let you
scale out but then I don't have a
management story so now I have thousands
of containers and I'm missing something
right that visibility that holistic view
of all the things so that's where it
starts to get complicated if I was using
app services I'm saving myself the
trouble of managing the VM keeping it
up-to-date security patches even
updating and patching docker if they
give me a provisioned docker ready app
service then a lot of that's done for me
that's great
and so it's saving me part of the
trouble but I still have to think about
ports and what goes on the VM itself ok
so just kind of get painting you the
picture you know if you have a simple
enough environment and you're like a
startup trying to save costs for example
you might just want one VM and not even
load-balanced but obviously there's
there's risks that come with all these
decisions right so let's talk a little
bit about that
so deployment looks like potentially you
know just a simple you know as your
container instance like like this new
feature that we have available in Azure
or it could be an app service which is
really just a VM that's managed for you
you still have the considerations if you
go to amazon asher or google all of them
provide ways for you to provision VMs
without an orchestration platform and
just put stuff on it and they'll give
you docker ready VMs or you can build
your own and have those be scale sets or
auto scale groups or depending on the
platform however they do they like to
call it i still and then i have to just
think about how I'm doing the topology
so if it's a single
I am topology it's probably gonna be you
know I'm trying to save money and I'm
thinking hey if the container goes down
I can restart it quick enough I'm not
gonna worry about it I'm not gonna have
anything monitoring its health and
telling it to restart I'd like that but
maybe I'm trying to save costs so I'm
just gonna manage it myself but there's
a single VM really okay and the answer
is no right even if container starts
really fast you're still gonna be in
trouble if you know you have customers
that care and that might not pay you for
your services your whatever it is you're
hosting or it has a bad image on the
company and so on so ultimately what you
really want is at least load balanced
VMs so then we get into a situation
where I might have two VMs and each of
them has one container at port 80 now I
can't use port 80 again so I would have
to host other containers on different
ports maybe have them called by the port
80 container so I could have multiple
containers on a machine that's no
problem they just can't use the same
port because it's a resource that can't
be shared right unless I put an index or
you know a proxy to do routing and then
I have a port 80 receiving everything
and that is what's load balanced at the
top level and then inside we do the
routing right so I still have to sculpt
a lot I have to design what am i
deploying one of the ports I'm using but
it gives me a model that's manageable
and even automatable at some point no
orchestration involved right so nothing
wrong with doing that it still gives you
all the benefits of docker for
development and the immutable image and
the idea of I know that this works here
and in every environment the same way
but we really need to talk about you
know even though it can work without
orchestration maybe we want to think
about what can orchestration give me on
top so that's where the fun starts
because we have all these things to
think about platforms of course we've
I've mentioned already right Amazon
Google as your container service
mesosphere as a platform that can be
deployed to Amazon or Asscher kubernetes
which is part of the Google container
engine but actually can also be deploy
to on Prem Amazon or Azure and docker
Enterprise Edition which is actually the
swarm native right docker native
platform that is now it's been around
for a while actually we did a deployment
a year ago for customer prototyping but
it's come a long way so that's gonna
start being a contender in the same
space right if it's not already so all
of these are options and they're not the
only ones I'm listing the ones that are
the most you know commonly that I run
into but there's actually other options
and then you got the infrastructure
managing the cluster the management
cluster the agent nodes proxy routing
discovery you know what kind of load
balancing setup do I need in the host
environment which docker registry will I
use not necessarily hard decision on the
registry but just a thing to think about
and then we've got the core features
right so this is where we get into the
meat of why do I need orchestration I
need you know something that will allow
me to deploy more services when I need
to scale them but not have to care about
ports right so load balancing discovery
and routing all of that kind of goes
hand in hand
I need auto scaling so that I can have
server density so we'll talk about that
I need self-healing so that if a
container goes down and stops working it
will try to heal itself and start again
so that I don't have to be watching and
and give me tooling that does that
automatically out of the box I need you
know to be able to perform upgrades in a
reliable way
right that doesn't actually bring the
service down so if I can't deploy the
new version then let's not tear down the
old one right let's keep the system
alive and healthy until we're sure that
we've lit up the new version and then
we've got you know in general versioning
which I won't get into much here in this
talk but it's just the concept that I
should be able to have side-by-side
deployments as well if I need to to have
versions that support previous and new
features scheduling gives you the idea
of okay so every organization platform
has master nodes or a master that is
basically in charge of quorum and
knowing the state and health of all the
workers so this master node or that
cluster is you know using whatever its
consensus algorithm is it's different in
the different platforms in order to
determine one leader and then obviously
ensure that if one of the you know nodes
goes down it can failover and elect a
new leader and all that's managed as
part of that you know topology part of
that platform implementation so it's
different for DCOs than kubernetes than
it is for swarm for example and then
things like Amazon they actually don't
let you see the master so they provide
it for you they implement that as a
feature of the cloud and so does Google
so you don't see it you don't have to
manage it you don't have to allocate it
it's just there and so you just worry
about your workers and the workers is
where you put the containers you always
schedule deployments to the master the
master decides where can I put those
things and it deploys so the easiest way
to look at scheduling and constraints is
just to look at how we would do that
with swarm so I'm gonna take a look at
that and again this is just a quick
visual just to say hey this is a swarm
picture it's one of the topologies that
could be deployed to you and you can see
it's got the managers which is you know
who's managing the health and state of
the rest of the cluster usually three to
five to seven nodes and then we've got
you know the actual agents doing work so
all of them look kind of similar and
what we're going to do is I'm gonna use
commands that make it look like I'm
coming in from outside to hit the Mount
Master endpoint the master in order to
issue commands to schedule services so
we'll do that and I'll use constraints
like memory constraints - em to tell the
cluster I need to reserve you know three
gig of ram for this container so that's
another concept that starts to matter
when we think about orchestration is we
don't let loose containers run around
using whatever resources they want we
learn to estimate what they need when
they're deployed now that's a hard job
because if you want to estimate that
means you have to do drills and you have
to learn the topology and how your
services behave and learn enough about
their behavior and their requirements so
that you can give good estimates but
what you're trying to do is constrain so
noisy neighbors don't bring the server
down you're trying to constrain
so that you can do some predictions on
usage of all the things on the cluster
and predictability of available space to
do say job execution of things that are
queued up and waiting for resources so
that's sort of how that works and you
know it works that way across all of the
platforms but let's go ahead and take a
look so I'm going to go into my swarm
and we'll go into swarm here and what
I'll do is connect to I have a Azure
container service has three templates
one for swarm one for kubernetes one for
DCOs
so I deployed the swarm template and I
have a cluster up there with one master
in three nodes right now okay so it's
not actually a high availability cluster
because that's a lot of nodes so okay so
what I'm gonna do is I'm gonna run a
container yeah so I'm gonna run the
content API and you're gonna see that
I'm looking for a five gig of memory
okay so what we need to do is take a
look and see if we can do that now how
I'm gonna look and see what I've got on
the cluster is I'm gonna run info
against the cluster and what we're gonna
do is scroll up a little bit here and
you'll see that I have five images
deploy to the whole cluster as it says
there's one container and there's one
running I've got three nodes so there's
three agents one that's got one
container running with five gig used and
only seven in total and then if I scroll
down you can see this guy's got nothing
used yet nothing used yet and then the
third likewise right so the idea is this
is what the manager node is is managing
is available resources so when I run
that same command again to deploy it's
got to find a new node the reason that
deployed so fast is cuz I've already
downloaded the image because if we had
to wait for that that would not be good
so and if I run it one more time it
should work because I have three nodes
but on the last one it's not
work so I'm going to start to get
failure because it can't find resources
available to deploy memory is just one
example of such a resource so obviously
the ports is another constraint that's a
hard constraint right on deployment and
other types of constraints we can kind
of quickly go through so scheduling
means that you take you know a
description of what the service requires
to run which could be a compose file or
a task definition or in this case
command line arcs same idea right the
run command is taking these args and
doing something with it and it's going
to schedule the services against the
master the nodes that have availability
will then pull the image and do their
job to run the container master monitors
the health and if something stops if the
container stops it will help it will
restart it if it requires at least a
minimum of 1 or 2 running it will try to
maintain that at all times so that's the
job of the orchestration platform each
of them has a way a mechanism for doing
so the constraints that you can use for
deploying those services can be implicit
which is the physical like port or it
can be explicit like the CPU and memory
and i/o requirements so you're trying to
contain the container so that it can you
know not again consume resources
excessively but again it this is
probably the hardest part of working
with orchestration platforms is that you
now have to understand a little bit
about how much should I give this
container and how do I know it's enough
now you'll find out pretty fast if you
didn't give it enough and you run in dev
and test and UAT
and stuff just not working you'll get
the proper error right like out of
resources etc but you have to do drills
this is what sort of means we have to do
some really good practices as part of
our plan in an orchestration adoption of
an orchestration platform it's not just
about that it's about the whole DevOps
story around it it's about understanding
how to recover from failure and really
testing it out it's about understanding
what can push that container to the
limit like
had examples in Amazon where you know we
we have let's say a search container
that uses a lot of memory because it
loads a bunch of information in memory
so that it can give you like a leucine
result like a shopping cart or something
and you know we had to hit that one hard
to see how much do we really think we
need to give it because it's one of the
heaviest containers in our solution for
example so doing those drills is is is
key standard operating procedures around
recovering from those and tuning the
system for that go live and then that
helps you understand how to continue
tuning as things evolve right so it's
it's it's not as bad as it sounds it's
just that it's not the same as hey why
don't I just deploy that and you know
let's see how it's going yeah so all of
these resource constraints are parts of
you know co-locating potentially
containers they should always go
together or these should never be
together and and other types of
heuristics so it's it's it's useful for
helping understand the distribution of
the containers across the whole solution
and the whole platform now another thing
we're looking for here is server density
we're looking for the idea that I can
have multiple instances of containers
and you know whatever space is available
fill it up but if we don't leave room
for upgrades and all of a sudden when
I'm ready to do an update swap out you
know the red one for a new version I
can't do it because I have no space now
so you have to have some space available
in CPU memory and so on disk space so
that you can actually do upgrades roll
out updates and roll back if something
goes wrong because otherwise you get
stuck so again having space is important
but then you can use that space to run
jobs so I can execute a job that just
you know spins up a whole bunch of
services using whatever space is free
finishes off and is
and then if my other containers get
deployed and start using more space we
start looking at scale out for instances
of the nodes or scale up by putting you
know more density on each node to keep
them busy
so orchestration UI there is a native UI
for swarm
it's called docker Enterprise Edition
and it's it comes with a fee and there's
actually a new thing that just came out
this week which you can set up a trial
for a one day demo and you can keep
renewing it so it just gives you a
cluster waiting and ready to go it
provides you with a UI on top of swarm
so that you can see what are the
services what are the nodes how much
resources are used things that you're
gonna see when I go into DCOs so I'm
going to spend more time on the other
platforms right now but just wanted to
point out that that also is another
example it's just it's more of the same
right so let's talk about load balancing
discovery with DCOs and as your
container service so I mentioned
Azure container service can have DCOs
can be swarm or can be kubernetes
and right now what that means is
provision me a bunch of nodes here's my
master I want three or five master nodes
here are my workers or agents and those
are maybe starting out at three or five
could be seven could be nine it's a lot
of notes right but you're not doing this
for like two containers you're doing
this because you need a solution okay
so load balancing discovery would be
something that all of the platform's
cover I just figured it would be worth
showing with DCOs and again it's gonna
be much of the same masters workers all
of that this is what the resource
deploys for you on at on em on the on
the azure container service template so
without a load balancer what I might end
up doing is I'd have a private agent
with an API on it exposing at port 3001
and my web app would have to know what
port that's at and it would have to you
know maybe expose itself through to the
azure load balancer at port 80 so
there's no internal load balancer setup
this would be like a simple example of
hey I'm not really taking advantage of
you know service
discovery and load balancing and
networking what I really want to get to
is this where the question marks are I
don't care what port you are I'm gonna
add three four five seven instances of
the API to handle load right now and as
that load is required it doesn't really
matter because the front-end just needs
to know what is the port that the load
balancer looks to for you know accessing
all those instances of web or API
containers so it should be fairly
automatic and that's kind of what I
figured I'd show quickly with DCOs which
ignore the slide so let's go to DCOs and
we'll go here too
yeah okay so I have a tunnel instruction
so I've already got cluster up there
I've already deployed a couple things
the web app and API that we already know
and love is up there ready to go and the
way that I did that I'll just quickly
show you
so the DCOs is what I'm going to do is
run this tunnel and that will allow me
to go through localhost port four five
five five so that I can hit it from
outside and see the management UI that's
presented so I've opened that up to my
machine so let's do this oops
so we'll do tunnel
and there you go so let's go to D cos
and D cos is gonna show me things like
how many nodes do I have how much memory
and CPU and disk am I using notice it's
a flat line that's because I'm telling
it how much each container needs it
doesn't vary it's saying I need this
it's a reservation right so we reserved
what we need kind of go down here you
can see how many tasks are running on
each how much CPU is less on each node
and so on and what I had done is I
deployed a service so I went into the
universe and I deployed a package for
marathon load balancer so I installed
the load balancer that's part and
available to DCOs so there's other
packages as well right so marathon
marathon load bouncer right so I already
installed that in fact I installed two
one for external and one for internal
use and then for services you can see
I've got all the services running but if
I wanted to deploy a new service right I
could go here to single container and
what this should look like to you is a
compose equivalent right it's
essentially a task definition the
service definition and it's going to ask
me things like what do you want to call
it API to
what's the container image das blonde
WAC you know content API for example how
much CPU T think you need again these
are the kind of best guess that you
start with and then you start thinking
okay how do I drill it how do I see if
I'm going over and then I'm gonna go in
here and set up things like networking
like am I gonna be part of the bridge
network probably what are my endpoints
container port 3000 one set Iraq cetera
or I can go to the JSON editor and I can
put paste that or I can post that you
know through obviously the the marathon
API so it's still just this it's a task
definition it's saying where to get the
image how many minimum instances I want
to have running all the time if I want
to plot a supply that
and what are my constraints you know I'm
going to CPU I need etc and that will
start to sort of populate the nodes and
leave less resources available so I've
already gone through that process but
you know let's go see an actual service
running so I'll go over here and we'll
cancel or just guard and let's hit up
the actual service so here's my API and
if I go into configuration and I come in
here and look at edit this would have
been what I deployed right so again we
don't have to inspect it in great depth
but just to give you sort of a taste of
all the features you know this is you
know the the docker image this is my
container port and this is my service
port so this is where the load balancer
comes in I can now add as many instances
I want of the API and then the web app
is only pointing at port 10,000 one at
the internal load balancer so that load
balancer will handle for me that
automatic sort of scale out and
networking and discovery capability that
load balancing across the container
instances which I'm looking for and so
to to take a look at that what I would
do is go to my service and right now
let's go to instances and this is
actually showing the tasks that's
running so I'm going to go back one ok
and what I can do is scale it so I can
add like say for instances and scale the
service and what that will do is
schedule to scale the the four and as
long as I have enough space enough
resources available to do so it will
schedule and deploy those if it didn't
have enough it would continue to try to
deploy it until resources are available
and I would get notifications that
things can't schedule the same way as
swarm did and rather it being an error
that surfaces and blows everything up
not that swarm does that it also has
recovery mechanisms but it will just
surface as keep retrying like pretty
much forever right it's gonna go into
waiting state until resources are
finally available
so this will try to deploy and if I look
at deployments it should show my
instances is it not doing that
interesting scale interesting
okay so it is I'm probably just having
refresh issues here which could be
happening or the pipe broke no
Internet could be faster okay so there
it is four instances so I'm just I'm not
getting the refresh fast enough to show
you but that now has four task instances
running now what's interesting about
that is the app if I actually go and run
the app endpoint so I think it's this
guy
this is DCOs and what I need to do is go
over to my DCOs
so this is my cluster in Azure when I
provisioned it it actually did all this
stuff for me it deployed you know public
IPs and an agent pool and a master pool
and so on and if I look for my agent
load balancer probably up here a little
bit
agent IP this will give me my DNS name
that I need to hit so this is the DNS
that I should hit to get to the Web App
and it should hit speakers and be able
to show me the speakers because it's
already set up to hit port 10,000 1 to
find any of the available for AP is
somewhere in there loading slowly
perhaps was working still loading and I
also have a stats page that might be a
better one to hit so my stats page will
show round-robin a through that's plural
there so in my stats you can see the
task ID that would tell me if that
changes which instance it's hitting so
if I were to refresh task ID should
change so this is now ending in 7a and
eventually it should round-robin between
the different instances
one of these moments
so or could be changing at the front and
I'm not noticing but the point is that I
did this just so that we could track the
round-robin between each of the
instances so I think this did change
because it was a date before so now it's
d8 it's just ending the same 7a so you
can see a bunch of different task
definitions and here is what it was
bouncing between so the point is when we
think about the orchestration platform
scheduling is one thing the other thing
is low balancing discovery and the
ability to employ more instances and not
care about stitching container
you know ports and things like that just
have it be accessible through some load
balancing capability you can do the same
in Amazon it requires setting up the alb
to do so so it's a bit more
infrastructure as opposed to internal to
you know the orchestration aspects
because that's hidden from you in Amazon
and in Google interestingly you don't
even think about it you just say deploy
service here's how many I want you don't
even see the in in kubernetes so you
don't even see the the networking aspect
you just say I want it to be a service
and then as you scale it it
automatically knows how to stitch up to
the DNS so it's kind of invisible it
doesn't make one better than the other
really I mean it's just each of them has
these little things they do in an
interesting way that might be pleasant
and then there's other things the others
do better right or have a better UI or a
better experience or better monitoring
visuals and things like that out of the
box so you can ultimately achieve the
same thing with all of them ok ok now on
that note we were talking about scale
and we're talking about you know load
balancing discovery heeling is another
one so in DCOs if I were to have a
health endpoint on the API that failed
then it would restart that task and if
it keeps failing then it would keep
trying to restart it to infinity and I'd
start seeing a lot of failed tasks and
that can be something I alerted on the
idea around the platform is if a
container fails and you're indicating I
need at least one or two instances then
the law of the land is you have to
restart it right so if I say I need to
instances and both go down it's going to
try and start two more and it will
continually try to do that but if I have
a fatal failure like I can't reach the
database the API needs to run healthy
and my health endpoints checking that
then it may not come back up until I fix
the problem down below and so that's the
idea right so we're gonna have this
concept of self healing and recovery
from things and then the autoscale part
is really more about I you know have
load maybe I'm hitting some sort of CPU
or memory threshold and as such what's
happening is I need to scale out either
the physical nodes or before that maybe
fill with more density the actual nodes
with the container instances so that
we're making the most use out of the VMs
that we have available right now
likewise things like I want to run a job
and I need to read some queue and
process if there's available survey
labrie sources it can run and do that
that work so let's take a look at Amazon
and Google container engine for that and
again this is just a picture coming out
of Amazon that shows you with ECS you
don't see the management cluster it's
just part of the you know the Amazon
hosting you know environment so that is
there there is always a management
cluster that's monitoring the health and
state of the ECS nodes but you're now
just dealing with the actual VMS
themselves for the agents the same thing
goes for Google container engine but
kubernetes as a platform just like DCOs
and swarm when you deploy those you get
the masters and the agents and notes so
it's important to understand that once
you start doing that you're gonna pay
for both sides
you're indirectly paying for both sides
in Amazon and Google just that they're
hiding the complexity of adoption having
to manage them here so you're not
managing a zookeeper or entity it's it's
under the covers instead of you having
the the cluster and having to care for
that health so obviously there's some
desirability in that no no question
write something nice about that when we
look at things like Amazon we're gonna
have thing possibly an alb that does the
load balancing so this is just a picture
to give you that I
of the load balancing and discovery how
would I set that up I can do that with a
lbs so I can start deploying additional
containers and they'll stitch up to the
alb and there's a really long-winded way
to do that if anybody is interested in
the tutorials across all four of these
platforms that I do in workshops um
you're welcome to email me for that
because I don't make it public that I'm
happy to share it with people here
because we're kind of covering all of
them it's actually kind of fun you go
through the same process with every
platform so it's kind of hopefully it
still works across all of them because
of course they change right so what I'm
gonna do is I'm gonna go into Amazon and
let's say a couple things so let's go to
over here okay so I'm gonna go to ec2
and I'm gonna take a quick look at
health and recovery so right now what I
have is this Web task here and you can
see our desired task - and running tasks
- if I go to the tasks and I take a look
at one of these web tasks and I select
it and I stop it then what that's gonna
do is take away one of my healthy tasks
and it's going to show me that I have
one running task and desire tasks - the
whole idea of self-healing is that my
task definition is saying I want desired
- and I will allow scale-up of double
that for example and so again these are
things that you might configure in any
of these platforms all of them support
some variation of this and so what'll
happen is as i refresh it will go ahead
and replenish that and eventually what
we'll see is that there's two again not
it won't take very long to get there in
fact the Refresh might take longer than
the actual act so so that's one thing to
consider so we'll see that is is going
to recover on its own the other thing I
wanted to show you is I can actually go
to this web app and right now I'm
running health web seven so I'm running
version seven of my task definition
which does not require a health check on
the API and so what I'm gonna do is I'm
going to break it I'm gonna go in and do
an update and right now health web seven
if I go to health web 8 it's now going
to expect the API to return healthy but
I'm gonna have the incorrect URL because
I'm going to forget to put the
environment variable that tells it where
the health check is just as a way to
illustrate the kinds of failure that
could happen so instead of failing on
the other side I'm failing on the
client-side
so I'm not gonna be able to validate
health and so therefore this is is going
to be failing so right now I have task 7
deployed with two instances probably
again we'll go check and then I'm going
to try to deploy version 8 so it's going
to try and if we go to view the service
it's going to try to deploy the next
version so you see it's over here health
web 7 is running and somewhere let's go
back up
there we go yeah so you see it had two
again and you can see pending so it's
pending on eight it's never going to get
successful so it's just gonna keep
retrying until I do something to say
let's stop this nonsense so it will pend
it will try to run it will fail
immediately and then therefore seven is
the one stitch to the load balancer
until eight comes online since eight
won't come online we won't get there so
what I'm gonna do is update again and
this time I'm going to update to the
version that fixes the problem so I'm
sort of simulating that immutable you
know task definition also and I'm
progressing through what could have
happened in a real scenario right so let
me go ahead and update that service and
then go view again and this time it will
show that it's trying to launch the
tasks it's got eight running that's
gonna fail right it'll show running for
a second just won't stitch to the load
balancer until it's verified as healthy
which it won't get to be healthy so this
is just showing that process and if you
look in the logs you can see the history
of that like you'll see that it stops
and tries again it stops and tries again
so it's all visible to you so this is
gonna now come back and try to do nine
and then nine will become healthy and it
will come online okay so what I'm
illustrating here is just again the
concept of I can define how many I need
to be healthy and my cluster I need to
minimum instances and it will try to
bring the two of the new version and do
a safe rollout of that without tearing
down what maybe the front-end is looking
for right so if this was an API
everything would stay up and running
waiting for the new one to come online
and so this is now pending nine this
will become successful and once it comes
online and is verified as healthy it
will go ahead and do retire seven take
it out of the load balancer and take it
away all the platforms support this kind
of concept as well so this is what
you're getting out of orchestration you
know there's a lot of power right behind
that beyond what you would have had to
do if you're just looking at VMs and
manually stitching these things together
now the other thing that's really
powerful is the auto scale stories right
so Auto scale in a cloud provider is is
going to be something that's usually
provided on two levels one is scale
the containers on the nose to fill
density like we talked about and the
other is to scale the actual physical
agents on demand I'm not a fan of you
know you have to limit your auto scale
because I think when you have a peak or
a burst that's unexpected you can end up
with 3,000 machines and pay a lot of
money and you weren't really expecting
that and and it might be for the wrong
reason right it could be sort of a red
herring something else that triggered it
in the system so in fact you like to
look at patterns and be monitoring that
and have a sense of the peaks that come
and go on a weekly basis on a daily
basis on a yearly basis and be prepared
for those things but waiting for
auto-scale to address it it's often too
late because it might take 15 minutes
for the scale out to even happen and by
then you've already got angry customers
so you know relying on that alone I'm
just saying I wouldn't necessarily do
but filling the density of the
containers yes and then potentially
doing you know some Auto scale with an
upper limit for this unexpected
situation and that's how we define it
for example in Amazon we would say only
scaled by 200 percent of what we have
now or 100 percent of what we have now
so if I were to refresh this it would
probably show me that nine is online and
seven is gone but I don't think we're
having a lot of luck with UI refresh so
ooh that was unexpected
here we go so okay so what I was going
to do is actually take you to Google and
let's go and take a look at the Google
container engine dashboard because what
I'm gonna do is gonna run a load test
against Amazon and that will take it
through a scale out and a scale up so
it's going to actually density of
containers and then it's also going to
do a scale out now we're not going to
wait for it I've got screenshots that
show that it happened but I just wanted
to get you an idea of how that starts
with monitoring and watch so if I were
to come into Google you can see that I
have a you know in my Google platform I
can see my Google container engine I can
come in here and look at the workloads
or the clusters that I have I have a
cluster for dab and test and you ATM
prod for example or other heuristics
I've got Web API and content API
deployed when I deployed these services
their discovery and load balancing was
taken care of for me so I didn't have to
think about it again it's a serviced
description but I didn't have to say
anything about load balancing and
stitching it just by default gave me a
public IP and stitched you know DNS in
for me so that the web app could call it
so each of these is basically just a
service with a service description as
you would expect that gets deployed I
might not come in here to do that
description and in fact normally I'm not
going to come to this UI at all this is
my holistic picture of Google Cloud just
like Amazon has it's for Amazon and
Azure Fraser but what I do want to do is
go to the kubernetes dashboard to see
things I don't think that's running
right now so what I'm going to do is
we'll go into cute control this will
show the pods that are running right now
so I've got actually you know the API
and web both running so I've got two
instances of each and if I do a cute
control proxy then what that will do is
serve up on one 27001 so that I can
refresh over here and now I have my UI
for kubernetes what I would get in
Amazon
in Asia and everywhere else and clearly
I'm gonna go a few more minutes over
time because there's six platforms in an
hour but maybe you can actually forgive
me that because you know that's almost
done so what's going to happen here is
I've I've got kubernetes up and running
and now I can come in here and I can
take a look at deployments that have
happened I've got my API and my web I
can scroll down a little bit and look at
the services from that perspective and
it will talk about load balancing
endpoints and things like that I can
come in here and look at pods which is
really how do you a couple multiple
containers and it gives me overall usage
statistics and and other details and
what I'm gonna do is I'm just going to
go ahead and run the load test here so
let's go to here and
that's my swarm that's my G cloud there
we go okay so um
apply oops there we go so it's my load
tester in that directory yes I think so
so what I'm gonna do is I can apply this
load tester what it's going to do is
deploy multiple containers that's gonna
hit Amazon with a million requests and
so that's gonna start this scale out
story over in my Amazon app from Google
so I just figured I'd use both this is
deploying what's called a job in Google
so it's gonna finish and be done as
opposed to a container that you deploy
that has an endpoint and stays alive and
must always be present and again all the
platforms have their sense of how do you
configure jobs as well another type of
orchestration workload right
so now that that's running I'm gonna
head over to Amazon and I'm gonna take a
look at my cloud watch so let's go to
services and cloud watch and what we'll
see is that I've got these alerts set up
for when we hit a certain amount of
memory so because the containers are
getting hit by Google and there's two
jobs running at frequency then these
alerts are going to start going off and
again I'm not going to wait to show you
those but I'll give you the screenshots
after that what it's going to do is lead
to my container services so if I go to
my clusters actually go here oh there we
go and if I come in here you'll see fab
medical two services three container
instances what will happen is there will
be the running tasks will increase to
eight ten fifteen and start to put load
on the nodes and then the nodes will
have to scale out so then I'll start
getting more container instances all
because of those alerts and how they're
configured to respond to the health of
the nodes okay so that's just an example
again of a thing that you could simulate
to do drills on your services to see can
I make them fail or will my system
actually healthily recover from all of
these situations
okay so again stuff that you'll do no
matter what platform you use
learn about the behavior of the system
and tweak your service configurations
coming back to Google then the last
thing I wanted to show you is we talked
about health checks I showed you a bad
health an endpoint that couldn't become
healthy that didn't deploy to Amazon
kept retrying and then when I put the
correct version that's the one that was
able to be deployed and retire the
previous version to back when we think
about health checks it's usually one
endpoint each of those container
platforms will give you a way to you
know configure like whatever a whack
health and in there you do what you
think is best things like you know is
the database that I need to function
there you know maybe some other socket
checks or things like that
now with that said one thing that Google
does that's kind of interesting is they
give you the concept of a live versus
ready and so the way that works is
instead so alive is you know have I got
a real problem you know
meaning I'm unhealthy and ready is more
like hey the Container started but we
need to load some cash we need to do
some things we're not ready yet and so
it's not unhealthy you don't need to
restart me you don't need to recover
that instance but you do need to wait
and so I think that that's a feature
that's kind of you know desirable and so
I think it was just thought of give you
a quick demo of that so what this is is
my kubernetes dashboard I have these
api's right so if I go into the services
I have you know this API endpoint and
then it gives me an external endpoint to
hit it so I'm able to hit that API there
and if I type something like you know
work it's going to stay busy for a
certain number of seconds and if I go
here and say cube control oops let's
just do get pods it's going to show me
zero out of one so one of these guys is
not ready right now
and it's going to end and then it's
going to come back to life
and you see it's already done it right
so that was an example of I'm not ready
right now because I triggered something
that that showed that it needed to do
some loading and that would be an
example of I'm busy I'm fully under load
but I'm gonna be back later
so now let's distribute to the other
nodes and if I had all of them busy I
have to scale out that would be another
scale out trigger but the other thing
you can do is you can trigger failure
hard work and so this is going to
actually just fail like 500s coming back
or in this case it's going to hang and
so what would happen here is I can come
in two pods and I can see that that's
readiness robe is failing
so I'm actually you know not going to
recover from this one and it's
eventually going to spin up another API
because I have a minimum two required so
again just showing you that all these
things that we've just talked about are
the things that we care about when we
think about orchestration and and and
platforms so you know let's kind of just
wrap this up and I'll try to I guess tie
it together to decision-making so again
these are just the alarms and the slides
to show that when the alarms go off for
memory usage I'm going to start adding
more running tasks and I'm going to fill
the density of the three container
instances eventually I'm gonna eleven
container instances because CPU
utilization went up and we said when
that happens we need more nodes so then
we added lots of running tasks but
across way more nodes okay simple
example this is a picture of the
kubernetes view point where we have a
master again master cluster you don't
see that in Google but if you deploy
kubernetes to your Aya's environment you
will see that of course because you have
to manage the masters and the agents and
then we already went through this so
just as a point of review then I've
talked about a lot of stuff and gone
over but you're gonna forgive me that
because there's a lot of stuff right
right docker without an orchestration
platform completely doable if you are if
it's manageable for you it kind of comes
down to that something will tip the
balance where you'll start thinking
about these features I showed you load
balancing networking you know scheduling
self-healing auto scaling and
filling with density running jobs and
being able to see the visibility of all
that in one sort of pane of glass so
those are the features that drive you to
think about orchestration and of course
there's got to be business value which I
talked about in my earlier talk today
you have to have a business driver to
bother with this or else it's going to
be a failed effort because it's too
costly and time-consuming to get all
this up and running do the drills
understand your environment know how to
recover disaster recovery etc and you
know get that visibility into logs too
you need your software to also be
instrumented the right way for micro
services in the process of going through
all of those things what I've tried to
do is give you a taste of you know
single deployments like as your
container instance and then docker swarm
for scheduling mesosphere DCOs for load
balancing AWS so you can get a sense of
what the dashboard looks like and things
you can do right managing tasks and then
kubernetes and Google container engine
which are one in the same so we covered
quite a bit I'm gonna just say I'm gonna
leave it there but hopefully you enjoyed
that and you know thanks for hanging out
a little longer so I could get through
it all okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>