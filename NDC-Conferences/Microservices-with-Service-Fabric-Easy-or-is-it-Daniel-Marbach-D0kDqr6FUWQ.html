<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microservices with Service Fabric. Easy... or is it? - Daniel Marbach | Coder Coacher - Coaching Coders</title><meta content="Microservices with Service Fabric. Easy... or is it? - Daniel Marbach - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microservices with Service Fabric. Easy... or is it? - Daniel Marbach</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/D0kDqr6FUWQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone you're doing it wrong if
you're not considering
platform-as-a-service
on premise or in the cloud and if you're
building applications today I think the
days of shipping data through to to tear
through countless instances to the
client and back and forth or over in
modern computation models such as actors
we bring data closer to the compute
layer and thus significantly reduce the
latency today I'm going to take you on a
journey and I will show you how you can
free yourself from this data shaping
paradigm and no longer ship data through
all the tiers towards a new stateful
middle tier architecture by leveraging
smart routing we can route requests that
are coming from platform as a service
offerings like as a service bus or as a
storage use into stateful service
components that are running inside
service fabrics that are that are
responsible of handling business
requests at the end of this talk you
know the benefits of stateful services
and service fabric and how you can
leverage them for data intensive
workloads
and how message patterns relay to data
partitioning and how you can ensure
proper routing into the cluster in this
talk I'll assume some basic knowledge
around service fabric I will cover some
of the concepts on the fly but basic
knowledge is required and also assume
some basic knowledge around standard
messaging patterns such as a command
requests reply and events of course if
you have any questions feel free to ask
these questions either during the talk
or preferably at the end of the talk so
sometimes when I'm sitting in my office
or I'm sitting at home and I'm working
on something especially when the weather
is bad again in Switzerland I get
slightly depressed and I think how could
I cheer me cheer myself up
and what I usually do then is I take a
piece of Swiss chocolate I love Swiss
chocolates you might wonder what has
chocolate to do with this talk with
micro-services and service fabric well
I'm going to give to you in this talk a
sweet but totally fictional story about
Swiss chocolate manufacturing everything
I'm going to show here is totally
invented by by myself so Switzerland is
well known for its extremely excellent
chocolate bars it's even better than the
Belgian chocolate believe me and I have
a few samples here so if you give me
feedback at the end of the talk
or if you ask a good question I'll give
you a give away a little bit of Swiss
chocolate so that you can try it out
yourself we have a problem our chocolate
is so good
that the international demand on these
chocolate bars is raising chocolate
manufacturers in Switzerland they
realize tanida they have a need for a
highly reliable chocolate order
management system the team responsible
for that new order management system
wanted to go to platform as a service
and the cloud because that's what
everybody does these days right that's
why we're here we learn about new things
and they wanted to do it as well but
they realized unfortunately they have a
few things running on premise that they
can't really migrate to the clouds we
also have legacy in Switzerland
especially when we have to integrate
with God government services so in the
architecture meeting of the team Carl
the team architect starts the discussion
with a horror story attention the next
slide is not for the faint of heart he
says many Easter bunnies have died last
Easter our chocolate is so good that
everyone people around the globe are
ordering our extremely delicious
chocolates last Easter the demand was so
high that our internal legacy
application started joking around it
became slower and slower and slower and
eventually it started to fail the ripple
effects were so severe that some of the
orders never came through
and he also said well losing orders is a
no-go
because one of our missions as a company
as a chocolate manufacturing company is
to make people happy by shipping our
products to the people around the globe
and and because we are not able to ship
even on the spikey demands our delicious
chocolate we are failing in that mission
if we continue to do so
we be wiped up wiped out by all our
competitors and potentially Belgian
chocolates that would be a disaster
so but he says but recently I was
stumbling over this awesome technology
called service fabric I was watching
some Channel 9 videos and I was I'm
really hooked on to on service fabric
let me explain you in this architecture
meeting what service fabric actually is
and he starts with empathy and
enthusiasm assess Asha service fabric is
a distributed system platform that makes
easy makes it really easy to package and
deploy applications that are reliable
and scalable both on-premises and in the
clouds and he says service fabric also
addresses all our needs of developing of
developing highly robust applications
and we can avoid managing complex
infrastructure on premises and we can
focus on implementing our
mission-critical chocolate ordering
system without to worry about all the
mythic retails of managing the
infrastructure and he continued to say
well it's hyper scale it supports
different programming models we can
apply replication and failover it has
upgrade domains fault domains and we can
do rolling upgrades and the most
important aspect and our managers love
that it has a built in load balancer yes
check and of course
car went on and on and on and on in this
meeting with his sales pitch after all
that's all the torque it takes through
the whole day right they talk talk talk
and get nothing done and if you want to
know more what karl explained to the
team about service fabric I suggest you
just read this blog
this post I posted here because after
all all the information that Carl gave
the team was anyway copy pasted from
Microsoft material so this is what call
showed to the team he started he started
wishes to do and of course every time
you switch to business to do everyone
has to wait in this architecture
meetings you have to wait as well and he
said look I have here service fabric
running on my local machine I have a 5
node cluster don't believe me look at
this here here this service fabric
running on my machine five nodes it
already has two applications deployed
this is the service Explorer it's
awesome I can view the different nodes I
can dive into the application here are
the few see notes if already
applications deployed and so on and so
on and he said look how easy it is to
create an application so he went to
service fabric application and then he
let me he said ok I have to remember
where I have to put this well let's put
it to projects micro micro service
chocolate micro services so he did that
and put it in here created the
application and said ok how about we do
a chalk order or the chocolate State
stateless service so he created that
stateless service and of course it took
some time until everything is created
the Microsoft machinery is going on and
and like like I he also mistyped the
service name so and he said well I
already have something prepared I want
to show you how robust it actually is I
have it already on my hard disk so I
just need to include it into the project
it's here
it's here I have an application that
will connect to the cluster and I have
it included here and what else well
let's let's add something else we need
some contracts because I want to show
you show readout and well because he
mistyped he mistyped the name of the
service he of course he had to change
something is in his application so
that's fixed right now and he said well
I would I want to show you in this
architecture meetings I want to show you
how service fabric can automatically
failover
and in order to do that I need an
application which connects to the
cluster to a service running in the
cluster and I need to expose this
service that is running in the in the
cluster and he created already state the
stateless service that is here your de
Chocolat so what I what he started doing
is well I already have an interface
called a chocolate service he needed to
include this and now he said well I'm
using now service fabric remoting to
implement it and service fabric emoting
automatically exposes a service inside
the cluster over remote procedure calls
so he did this he did he implemented
this method as a demo he said oh let's
do result the rest returned result and
in order to show that the service is
failing over we need to we need to
return I'm going to return the instance
ID of the service that is running it
that is available on this thing called
context and I'm because it's a
HelloWorld application I will also
return here hello world and that will
then be displayed by the by the console
application that is running outside of
the cluster but as always with stuff we
need some new code packages in order to
make this work in order to expose the
service we over remoting we actually
need to to add in this service package
reference and hopefully the demo gods
are with him so he goes into
my he says okay let's let's pull in this
service fabric remoting package that's
that one and it also pulls in this
internal transport which exposes over a
thing called communication listener
remoting service to the clients and of
course because because we are doing
Microsoft demos it's important that we
do right click deploy on the solution as
all the Microsoft demos are working so
we go here
I built it already and let's publish the
application to the local 5 node cluster
and see what's going to happen the
machinery is kicking off it creates a
new application it gets deployed as you
can see three applications running and
this is the chocolate my chocolate
micro service application and it
contains an ordered chocolates misspelt
as you can see here and version one and
now we need to connect to this thing and
let's let's start let's start the
connector in debug mode and let's see
what's going to happen hmmm
what ah good good catch
which one here yeah exactly
so here actually this is the correct
service name here cool
it's always good when when attendees of
the talk actually pay attention to what
what the architect is doing right sir
chocolate bar right chocolate bar milk
chocolate I cannot throw it up you just
get one
yeah I've talked couples as well of
course yeah haha and of course he also
forgets something else because it's not
enough to just implement an interface
and actually nobody noticed that I
thought you people already know how
service fabric works but they actually
what actually was forgotten is that here
when when you expose a remoting service
you also have to create a communication
listener and this is the remoting
communication Lister that needs to be
added and this needs to be wrapped in a
new service instance listener and then
this remoting thing needs to be added
because without that nothing will
actually work and of course because
because we change the service definition
we could now increase increase the
version arm redeploy it and then it
would automatically do rolling upgrades
but I just override it right now
so publish it again it will now tear
down the existing application redeploy
it we could also do that on the fly in
the rolling upgrade upgrade fashion in
production and now let's start the
connector again and now we see hello
world nice finally it's working but
that's not all now we can essentially go
back I'll move it a little bit to decide
and hopefully it will work with with
with this
screen size here but we can we can
essentially see check whether where it's
running all right we have here the
chocolate micro service and the ordered
chocolate we have to quickly see on
which note is running it's on note four
okay so let's let's deactivate and
basically just remove the whole data
package and of course it tells me you
sure you want to do this yes I'm really
sure I want to destroy my cluster and
now it's removing it and we should see
that the service chokes a little bit and
it automatically fails over redeployed
internally and it heals itself okay cool
so far so good
that was the demo that Carl the team
architect gave gave to the to the team
and of course the team was absolutely
hooked after this demo although he made
a few mistakes during the demo and they
decided they want to benefit from the
scalability and robustness of service
fabric and the team wanted to
immediately jump in front of their
computers and start hacking and coding
with service fabric and to build the new
ideas but Carl stopped them and said
hold on people before we proceed we
first need to understand the
fundamentals of scaling there is a thing
called the skill cube it comes from the
micro services design principles and the
scale of cube has a few axis and the y
axis talks about functional
decomposition or it's also known known
as y-axis scaling and in y-axis scaling
what we apply is we we do scale by
splitting into different things and he
said we already do that we do micro
service design principles so we
decompose into multiple bounded contexts
or micro services so we have order
management service shipping service and
manufacturing service and whatever
and yell said well we also have the
x-axis scaling and what we do there is
basically when we have these
micro-services we deploy them multiple
times into the cluster and so we have
multiple instances running of the same
service in the cluster it's also known
as horizontal deduplication
so the TLDR version of what he gave to
the team is he said well we make things
smaller and then we spread them out
right that's what we do and then I then
he said thereby I the Karl the Great
Architect of the universe came up with
the falling architecture blueprints so
first let's start with the low pole
answer every good architecture has to
have one right so requests are routed
randomly by this local answer to the
stateless weft here the stateless weft
here will contain our latest angular
react J's knockout kill myself' UI
framework from there every request will
be routed to the stateless middle tier
in the middle that is responsible to do
the necessary compute while service
fabric has this concept of reliable
services namely stateless services which
I saw one in the demo and the orders
will be managed by the storage tier and
whenever request comes in the request is
routed through through all this gear to
the storage layer and the data namely
the orders will be shaped back to the
client and so so forth and he said well
and if hell breaks loose if we have a
lot of customers ordering chocolate then
and our storage layer cannot keep up
with the demand that we have on our
cluster while we just throw in some
hipster caching technology like Redis
for example and everything will be good
right and it any offset and by the way
if you want to impress your your bodies
in the local deep bar or whatever you
can tell them this is called deep data
shaping paradigm because we're shaping
data through all the tiers
but then Mandy from the team she spoke
up and said but I'm sorry Karl well
nobody is nobody builds architecture
like this anymore
she said well we've tried that in-house
before you join the company and we
realized that building interactive
services that are scalable and reliable
is really hard because interactivity on
the front ends has strict constraints on
the availability and latency on our
services because when the latency is too
high it directly affects the end-user
experience because everything ripples
through all the layers and we realize
that to support a large number of
concurrent user sessions we have to have
a fry a high throughput that's essential
and she also said well with your
traditional three-tier architecture that
you just showed us we are going to ship
data through stateless font and
stateless middle tier to the storage
layer and back and forth and therefore
the storage layer essentially limits our
scalability because the latency of the
storage layer directly impacts our end
users because it has to be consulted for
every request and she said well we also
try to add a caching layer and what we
realized is well it did improve a little
bit the performance of the caching layer
but we lost basically the concurrency in
transactional semantics of our storage
layer and furthermore we realized that
at some point in time we have to
invalidate the caches and cache
invalidation is a really complex problem
so they started to think about
implementing a concurrency control
protocol something like something like a
raster consensus-based and then she said
well but at some point we gave it up and
we realized that with or without cache
caching the stateless middle tier
approach does not provide data locality
the data is never there where you need
it because for every request like I
already said we have to go through all
the
all these tears and then she said in the
world of IOT cloud service and acting
models the approach for distributed
computing is that we basically take the
state and move it closer to the compute
layer so then she said well this is my
successor architecture to cause approach
and that's my proposal
well service fabric has the concept of
stateful services stateful services
allow to consistently and reliably store
the states like the orders right where
the service lists inside the cluster by
leveraging the power of so-called
reliable collections and you explain
well reliable collections they have a
similar C sharp API like top net
collection but they are replicated
transactional and highly available
inside the cluster and said well with
that we can achieve that the application
hot state lives in the computer so all
the orders are always highly available
with low latency reads and writes namely
in the middle and the external store has
to be only consulted for exhaust or
offline analytics purposes and of course
we can keep the basically the orders
transactionally in memory and she all
said well like every approach it also
has some drawbacks because well with
storage here can have a larger capacity
potentially than then the capacity of
our cluster because we can only save as
much sate inside the cluster as the
cluster has capacity and furthermore she
continued potentially Carl has been
watching too many channel 9 videos late
nights and he actually forgot that it's
a scale cube right a cube has three axis
and there is also a thing called the set
axis scaling and then she walks back to
the architecture diagram where a call
draw this this cube when she says well
there is this set axis and in order to
achieve hyper scale it's not enough to
think about basically splitting into
multiple microservices and then cloning
them and having multiple instances we
also need to think about can we split
these micro-services as well into
multiple partitions so is it for example
possible to take order management and
partition order management into multiple
partitions and that's called the data
partitioning and then gee
hold on a second and then she says well
let me go back to the other architecture
diagram and she also said now let me
give you an example but I'm totally
pulling this out of thin air right now
so let's let's imagine for the sake of
this architectural discussion that our
order management system was only allowed
to have one chocolate type per order and
our our customers would be only ordering
one order let's say for example we have
customers ordering dark chocolates brown
chocolate and white chocolate and she
says I know it's a bit silly but please
bear with me well so this fabric makes
it super easy to develop to develop
scalable services by offering a
first-class data partitioning concepts
so conceptually you have to think about
data partitioning of stateful services
it's a partition is a scale unit that
makes a service highly reliable through
multiple replicas inside the cluster and
the state is balanced across all the
nodes in the cluster and a great thing
about service fabric is that whether
depending on the demand that the
partition has the internal resource
manager of the service fabric cluster
will ought to automatically make sure
that state the nodes get moved so the
services can move the route around
inside the clusters so that allows
basically the application in this
example partitioned by chocolate
to grow to the notes resource limit and
use all the memory and all the compute
that is available on a node and if there
the resource need grows which are
flowing more nodes and the service
fabric Lhasa which automatically
rebalance it and she also said well
service fabric has the concept of range
partition for example or otherwise also
known as uniform in sixty-four
partitions where you can basically say
you can divide the data into ranges for
example from zero to a thousand four
from zero to 300 or four from - in 64 -
plus in 64 and there is also the thing
called a named partition a named
partition works when you can basically
pocket your data into multiple buckets
and in this example we could for example
bucket our data in dark chocolate brown
chocolate and white chocolate right
other examples would be you could for
example have pockets like regions postal
codes
customer groups or other self-invented
business boundaries that you have in
your bounded context and of course
there's a third partitioning type in
service fabric called the singleton
partition we're not going to talk about
single singleton partitions the
partitioning type that is primarily used
for stateless services by default and
she says well I'm presenting this year a
little bit simplified because of course
you wouldn't you would not use chocolate
type as a data partitioning schema
because if we have customers like me
that all love dark chocolates
basically most of the customers have
basically ordered a lot of dark
chocolate and our customer our class
would be totally no longer balanced out
right we would potentially is another
data partitioning schema but that's just
for the sake of the example that men men
gave and she said as well and on the
other hand choosing the number of
partitions is quite complex because we
have to think about upfront how much
scaling needs we have in the cluster
because it's service fabric makes it
really hard to repartition once you
already have stored data right now there
is no built-in functionality in service
fabric you basically has to shuffle your
data out of the cluster and then
redeploy the service with the new
partitioning schema and then shuffle the
data in with the new partitioning schema
so you have to think about that up front
but then to the youngest member of the
team in this architecture the discussion
said well I can't really understand this
technical gibberish that mandy was
talking about and he asked for a
concrete example and she said well look
Joe imagine a customer comes in and gets
randomly assigned a stateless web node
on the stateless front-end let's say he
orders white-chocolate the stateless
front-end will use the chocolate type to
determine the partition key and the name
partition and then we will use the
service fabric building service
partition resolver resolve that the
partition that is responsible to host
that data and imagine service fabric as
internally something like a naming
service or a DNS that is exposed over
this service partition resolver that
allows to basically take a service name
take a partition key and transparently
locate the service inside the cluster
that is responsible for that specific
partition and as soon as it's as soon as
it retrieved the right service instance
for a given partition then by an RPC
call it will call into basically that
service and handover the data to that
service and we service fabric the only
way to do inter-service communication is
by exposing basically either an HTTP
call an RPC service like we did in the
demo or a top CF service or any other
kind of what they call communication
listener only then you can actually talk
to a service so we can say in this
example two
to that routing inside the cluster is
done by the chocolate type and she said
well of course you all know that I like
especially dark chocolate so in my
example I would enter basically on the
front and to my order and then it would
determine that I ordered dark chocolate
and would automatically route it to the
partition that on to the dark chocolate
partition but then Sofia threw in the
grenades and she said well Mandy thanks
for explaining this so carefully to Joe
but I'm getting shivers when I hear you
talk about all this RPC style
communication between services this
reminds me of a project that I've been
doing in my previous employer so we
build this huge intertangled
interconnected RPC legacy mess at the
time the term microservices wasn't kind
yet but I believe we did something
similar but potentially horribly wrong
so we had this dog WF order services
that had to connect to a slow
third-party component and had to
transactionally integrate with that
database the temporal and spatial
coupling that we introduced was horrible
because every time the third-party
service was no longer really responsive
or took a long time basically the
customer facing latency on the front end
went through the roof top and started
failing and she said well we couldn't
fulfill our Ashley's we actually lost
orders we couldn't froth all the orders
and rode the also size we cap
transaction is open for too long and the
database started timing out rolling back
the data inserts people got fired and
then she said well in case you wonder it
wasn't me who got fired
actually I had the great idea which
saved the project well what we started
to do we first introduced a
Curren programming into the game and
this drastically reduced the memory
footprint at our services hat and
allowed us to better satisfy the
resources on our compute nodes but of
course asynchronous programming did not
really solve the problem we had in this
project so we decided to decouple the
third party behind the message queue at
the time used the ross pms mq service
that was available on windows servers
because but of course nowadays you will
be using something more hipster like
rabid mq or ash storage use of Eris's
service bus and she said well this
allowed us to throttle the request to
the third party services and allowed us
to basically get a predictable load on
the service we apply this pattern
successfully in various areas of the
system and he said well the fire and
forgot nature of messaging actually made
our system scale much more she said and
here is my proposal and a slice only a
slight modification of the architecture
that mandy proposed we introduced
between the stateless front ends and the
stateful middles here we introduced some
kind of a broker middleware like azor
azor service bus and what will what will
happen is basically every time an order
is created on the stateless front end we
will shuffle that order as a message
into a queue on ash the response
rabbitmq if you're running on premises
and then the broker middleware will make
sure that the the listeners the queue
listeners that are running on the
stateful instances will basically fetch
this order and process these orders in
the right order with that we will no
longer lose orders it allows us the
possibility to basically automatically
scale up and down whatever we need we
can apply the competing consumer
patterns on these queues
we can throttling we can do retries and
it allows us the possibility to for
example only open a transaction when we
take out a message out of the queue so
we are building a much more reactive
architecture by introducing a queue into
the game but then Peter snatches the
whiteboard markers and furiously
furiously screams but that doesn't solve
anything after taking some moments of
breath and the team colleagues reminding
him that this is only an architectural
discussion and nothing has been set in
stone he says I'm really sorry about my
outbursts but I recently my son is not
sleeping really well and him in our bed
and putting his knees and elbows the
whole time into my face and back really
didn't help with my sleep aberration but
what I wanted to say is I completely or
we completely forgot the data
partitioning part that we talked about
so well where Spada partitioning we
essentially have multiple cue consumers
running in all these partitions in the
cluster and with just a single cue what
could happen is a dark-chocolate order
is in that queue but it gets picked up
by the partition that is responsible for
storing the white choc chocolate types
and you can imagine that this would lead
to inconsistencies in our business data
and therefore would again lose
potentially orders and this would not
make our business happy it will not make
our customers happy and I think we would
we would start to see hats rolling in
our engineering department so basically
we are back on square one right with
queuing and we need some kind of smart
routing and/or cue partitioning to
achieve hyper scale with data
partitioning so he said well what I
would like to recommend
is to have a dedicated queue per
chocolate type simple right so when an
order is created on a stateless
front-end the chocolate type is
basically used and used as an input into
a partitioning function and that
partitioning function returns again the
chocolate type and based on that by
using either a queue naming convention
or something there is a routing layer
automatically knows that a
dark-chocolate order needs to go to the
queue that is responsible for dark
chocolate so we essentially have a
uniquely addressable queue in this
example per chocolate type and only on
the specific partition we have one
consumer that is responsible for that
queue so we have a clear separation of
concerns from from a partitioning
perspective also on the queues and well
he said and we what we did here we
talked only about commands the command
pattern with messaging right we are
basically entering orders and we sent
them to an order receiver so we are also
because we're doing micro services and
we like to be fancy we also do UI
composition that's what we need to do
with microservices so the UI composites
in our order management micro service
that is living on the stateless
stateless front end will basically issue
a command or the chocolates to the order
back in part which is receiving
disorders it's okay that the sender of a
commands knows the destination of the
receiver because with command pattern we
essentially have logical coupling but
that's okay because we belong to the
same bounded context right we only
introduce here a message queue for the
scaling aspects and throttling aspects
and retry ability aspects so with that
we can basically temporarily decouple
the sender from the receiver
in integration scenarios where for
example ascender a message gender is not
living inside the cluster it's still
okay that the sender knows the
partitioning function of the receiver it
can just basically use the dark
chocolate eye brown chocolate type or
white choco type to to know to which Q
it needs to route to and then the right
partition message receiver will pick it
up and he said well for simplicity
reasons I would call this sender side
distribution because the sender
determines by the partitioning function
where the message has to be routed to
but then the PhD dudes from the team was
triggered by this all this talk about
command patterns and he's here I started
acting like a smartass but while the
team already was quite accustomed to him
acting like a smartass so they let him
go and he said but with messaging we do
not only have command patterns we have
much more for example we have events
right and with events it's not possible
to apply sender side distribution let me
explain why for example we have a
chocolate ordered event the publisher of
the chocolate ordered events cannot
enforce a partitioning schema to its
subscribers because well that would mean
in the end with publish/subscribe
patents all subscribers were basically
to have the same partitioning schema
like the publisher but with pops up
basically the subscriber that receives
the event defines its own business
processes that it triggered by this
event and therefore it also defines its
own partitioning requirements or needs
on the data so it's always the
subscriber of an event that defines how
data needs to be partitioned because
only the subscriber knows basically the
partitioning function of its own data
for pops-up semantics usually what we
see is that the subscriber is basically
abstracted behind a logical thing or a
logical cue or topic or exchange
depending on the cueing technology you
are using the fact that the subscriber
like in this example is scaled out is
not visible to the publisher so for
non-beta application scenarios usually
only a single subscriber of that logical
subscription group will get the events
so they basically act as competing
consumers from a subscription
perspective so what can happen is when
for example when we publish in a
chocolate ordered event and the shipping
service for example applies partitioning
by zip codes it will basically end up on
any of these shipping queues right so
what what the the shipping service needs
to do is when it takes out the message
it basically needs to compute the
partition it needs to apply the
partitioning function to the payloads
determine the partition key and then
basically check whether it's already on
the write partition if it's not on the
write partition the subscriber itself
needs to internally reroute the message
with one single hop to the right
destination queue because only the
subscriber knows is internal routing
strategies so Peter said the PhD dude
said well in order to to stay true with
the lingo that was introduced by Peter
let's call this receiver side
distribution because the receiver
distributes when necessary and he said
well I know it's not an official term
but anyway let's stick with this and
then he said well but it's not at all
well we also have requests reply for
example sometimes we want to
asynchronously confirm back to the user
that an order was processed
it could be that we have a callback
function on the stateless on the
stateless front-end or we could it could
be that we have some kind of data that
is created on the order sender on this
specific partition and what we want to
do is you want to be able to reply back
to the sender that is living on a
specific petition and in messaging there
is a pattern called the return address
pattern so what we need to do here is
essentially when the sender sends a
command to the receiver the sender needs
to create the Heather called for example
reply address that contains the it's
specific partition cue and then when
when the receiver processor takes out
the message out of the queue it can
directly reply without needing to
compute anything in that in that case
but then in complex business
applications like we are going to build
we also have to think of process when
access or sagas right so we might have
multiple aggregates and bounded contexts
that need to be integrated together and
communicate it together sometimes we do
that in the service itself or sometimes
we do that cross service by applying
orchestration process managers for
complex business processes and well what
what what I realized is basically a
process manager is nothing more than
just an application of all these message
patterns that we already saw
a process man actually has a state and
that state lives potentially inside a
micro service that has a certain
partitioning schema but it uses pops-up
pattern request reply pattern and
potentially the command pattern to
basically correlate multiple messages
together into one business process so
that's not really much more complex and
then the team decided to do a little POC
and if they decided to do it with an
service bus and public service fabric
they've chosen answer responds to
they didn't want to basically build
infrastructure bits and pieces to
connect to the queues and everything
like that so they thought it's good a
nice queuing obstruction and that's what
they came up with so basically they had
a stateless front-end called chocolate
order which applies sender side
distribution by using the chocolate type
and basically sending to dark brown or
white chocolate queue and then they have
a stateless pack stateful back-end
called chocolate order which has a
chocolate order process manager or Saada
that applies sender and receiver side
distribution and what it's doing it's
basically a chocolate order process
manager is sending for example a chip
order command to the shipping service
when it knows that for example the
buyer's remorse period is over and then
the shipping servicing itself in this
example to apply multiple partition
schemas is partitioned by using the zip
code so they use they're saying well if
you have a shipping order zip code
between zero and solution 3 0 0 0 then
we send it to the 33,000 queue and if
it's something between 35,000 and 66,000
then we send it basically to the 66,000
queue and this is what the team came up
with let me show you a brief demo
so this is the the solution they came up
with what we see here is we are now in
the home controller which is the
controller that will issue orders let me
briefly show you that that visually how
this looks like this is the awesome UI
they came up with in their POC so it
basically has three buttons and no an
order button button for charter
dark-chocolate an order button for brown
chocolate and an order bar button for
white chocolates whenever you click one
of these buttons it will automatically
issue an order of that specific type and
send it to the chocolate order back ends
and this is how it looks like so they
have here an object called message
session and they issue an order
chocolate commands with a send
instruction what they what they did in
this POC is they they realized since
they cannot inflate the partitioning
schemas on the shipping service when
they get back a reply or an event from
the shipping service the only thing that
they can share is basically the order ID
nothing else so they decided to
basically encode the chocolate type in
the order ID and they used a really
silly but simple approach so basically
the order ID becomes chocolate type
semicolon and then some kind of grids
okay and on the receiving side on the
receiving side we have here a so called
or the process manager or Saga so what
they need to declare is I am started by
message or Dakota which is the command
that will trigger in this case enter
response to create per order ID a new
saga it will automatically do all the
concurrency control and multi-version
concurrency the when multiple messages
are concurrently handled it will
automatically roll back the state if
necessary and retry the message and they
they were using the answer response
service fabric persistence package which
allows to either by using
mentions or by using explicit attributes
to basically add an attribute to this
process manager and say we want that the
state is reliably stored inside the
service fabric collection that is called
orders and reliably and transactionally
stored there that's it and then from the
perspective of actually handling that
message it's a pretty simple so they
just basically implement this handle
method where they get the commands and
then they save the chocolate type and
they apply a timeout to itself which is
the buyer's remorse period here in this
example of one second so after one
second if the custom doesn't cancel the
order it basically come back comes back
into this method and now you see here we
do not apply any partitioning logic to
this to this code it's all just business
logic the partitioning is transparently
done behind the scenes I will show you
you that later so here when the time is
comes back when the buyers remorse
period is over we just publish a
chocolate ordered and we do send locally
and make payment commands and make
payment we again add the chocolate type
here and now let me show you how you
handle this make payment commands so
this is basically the message handler
that is responsible to make a payment
and because the message handler itself
basically will be horizontal horizontal
e-skills we will have multiple instances
per partition type of this message
handler running but transparently behind
the scenes to send local call we just
saw will automatically make sure that
only the handler that is living in a
partition type for example dark
chocolate when it's a dark chocolate
order will receive that message and as
you can see here when we get a payment
response we then do a send to the
shipping service off the ship order and
what we do now here since this order ID
also has encoded in it
the chocolate type we can just send the
sheep order with the order ID in here a
zip code it's a random number between 0
and 99 thousand we send it over and then
on the shipping order handler which
lives in another service which has its
own data partitioning schema which just
handled that message and it will then
publish in order shipped and as you can
see because it's still related to the
ordered vocabulary it will just reuse
the order ID and now we have just seen
the whole process of chocolate ordering
and now let's dive into how we actually
apply apply the partitioning
partitioning is not contained in the
business logic it is basically part of
the routing infrastructure and let me
show you an example of the front ins
where we apply the the pattern of sender
side distribution so what we use here is
basically because we belong to the same
bounded context we use the service
partition resolver to essentially
resolve the address of the top of the
back end processing service and we tell
the service partition resolver please
give me all the partitions that that
this service has and then we tell it
well let's do some routing internally
and first of all we we tell this
endpoints that we'll be leaving in a
stateless instance we tell it whenever
for this destination endpoint chocolate
order which is a logical queue we will
have multiple partitions and what enter
response in this example will do it will
automatically create basically a routing
table internally and whenever we send
the commands it will automatically then
click the write partition not not fully
automatic we actually have to tell its
how we derive the partitioning function
that's what we do here we basically say
ok whenever this type whenever you sign
sent this type
please use on this message type the
chocolate ID property to determine the
partition key so when we set the
chocolates type property to
dark chocolate it will then take this
and then basically use this internal
routing table that we created here to
basically take dark chocolates to the
destination and automatically route it
to the right thing and we apply this
MobileMe for center side distribution
but also for receiver side distribution
for example internally in the chocolate
order back ends where we say okay
whenever we have an order shipped and
now we no order ship is not published by
us but we are a subscriber but we know
that we actually sent in the order ID
which contains the information that
allows us met back to our partition so
we basically tell it it's a bit brute
force because it's a POC so basically
they say okay we split by the semi
column we take out the order ID the grid
and we take the partition pipe that is
encoded in this ID and then we just use
this chocolate type to again basically
internally reroute whenever it is
necessary and and last but not least we
do the same for the shipping and this is
just an example to show how complex this
partitioning logic can be if necessary
again it's all infrastructure concern we
basically turn the ship order string
into an int and then we automatically
picked the right partition Heike based
on the zip code and return it to the
partitioning function and that's it and
when we run this let let it's already
deployed deployed in the cluster we then
actually see this life happening this is
the Diagnostics window I might need to
zoom in a little bit let's let's order
dark chocolates and at some point in
time we can essentially see zoom in a
little bit that's probably not can you
see that in behind okay or we see we got
the ordered chocolates and the partition
key was most dark chocolates and then
the order process then actually picked
it up
on the dark chocolate partition it sends
our buyer's remorse period command to
itself a timeout it received it or STS
receiver side distribution it received
it on the wrong partition and it applied
receiver side distribution and send it
to the right
dark chocolate partition it sends out
the make payment on the dark chocolate
to the dark chocolate order receives to
make payments on the right partition the
partitioned dark and then the payment
process goes on and then the ship order
handle basically receives it on the
wrong partition and automatically as you
can see here forwards it to the right
partition so that it's it's handled on
the partition which is responsible to
manage that zip code I know this was
potentially a bit fast but I will give
you the links to to the demo so that you
can try it out yourself so brief recap
well it's always a bit more complex than
Microsoft tells you like the team
architects did show that it's so easy to
just do right right click publish and
everything works fine even with the
state stateless services as soon as you
start doing stateful services you
basically need to make smart routing
decisions based on on the partitioning
you are going to use stateful
computation with low latency like I said
requires smart routing and smart routing
decisions but I think that with service
fabric stateless and also stateful
services combined with messaging you
basically get the best out of two worlds
you can decide wherever it fits to just
apply the RPC communication style that
is available inside service fabric and
use the service partition resolver or
where you need basically ordering of
commands offloading throttling for
example you can apply message patterns
and well if it was a bit too fast and
you're curious and if you want to know a
little bit more about how answer
response works you can go to this URL
there is a tutorial which explain how
which explains how an service bus works
you can download it and try it out it
has multiple lessons and if you want to
know more about how this how this
example I just briefly showed works you
can go either to to this URI which gives
a lot of diagrams and text and explains
how the azure service fabric routing
works with with engine response of
course the same patterns that I showed
here they they are universally
applicable also to to Azure service bus
itself if you're just doing SDK native
SDK if you're using mass transit or any
other service bus you can apply the same
patterns as well but we provide a sample
out-of-the-box and if you want to try it
out yourself you can download the slides
the links and all the codes here on my
repository key TOCOM slash on a marble
microservices service fabric feel free
to start a repo that always helps to
promote the stuff we are doing and if
you have questions feel free to shoot
them now any questions yeah yep
am I using some kind of consistency
patterns too okay
so service fabric reliable collections
and that was using the our offering that
allows to store the stating reliable
collections is a transactional system
that has concurrency control built in so
basically when we have multiple
concurrent operations that try to store
the same saga what service fabric will
tell us that someone else changed the
states so basically one loads the state
at point one another one loads it at
point three right and then the the one
at point one will basically try to save
it will try to save it and will give a
basically in concurrency exception that
bubbles up and what we do is we roll
the transaction and then it will be
retried and the next one will be
successful but it's it relies on a
retries to do the concurrency control
yes yes well there are different and
Jimmy Bogle wrote an excellent blog post
about different types of process
managers and of course if you have a
process manager that needs to integrate
potentially thousands of messages that
will be concurrently fired off and then
correlated back to the same process
manager if you apply this pattern and
you have a lot of concurrency you will a
little basically going to a lot of
exceptions and retries so in that case
you need to apply other patterns then
it's not the right thing to do but of
course the solution works for most
business cases but not for highly
concurrent updates any other question
yeah you get the chocolate by the way
yeah
okay so you said wouldn't it be more
efficient to have actors listening on
those cues okay okay well well for
example if using a service fabric actor
model they are also partitioned right
and an actor has a certain life time and
the nectar is activated based on an RPC
color message and then live for that
brief moment of time and then basically
gets decommissioned by the actor runtime
with queuing systems you need a stateful
or sorry a persistent connection to the
cue so that you can basically pick up
messages so you need some kind of
infrastructure service that knows about
the partitioning schema of the micro
service that is listening on one or
multiple queues and then basically
shuffles off messages so I'm not sure if
I understand your question okay I have
an idea let's let's talk together after
my talk okay and because we also have
other people have a question then we can
have a deeper conversation about this
topic cool any other questions
you're asking can you scale partitions
okay so what what you define basically
you your picking in service fabric
you're picking a partitioning schema
let's say you're using the range
partition or the name partition with
dark white notice let's say the the
range partition from 0 to 300 and then
you're saying I want a hundred
partitions for example so what it means
basically then zero to three is in one
partition 4 to 4 to 6 is another
partition and so on and so forth but
this defines how many instances of that
of that service of that partition
service you will be having in your
cluster that's your scale unit that's
why I said you have to think about the
number of partitions upfront right
because what happens is when you start
with a 5 node cluster you will have a
hundred instances in that cluster so
that means you only have a process per
node right so we will basically have 20
instances per node all right and then
when you scale out you're saying oh we
have more demand you basically add more
machines let's say you add five more
machines then you have ten and what then
service fabric does it automatically
balances these 100 instances so
basically it takes ten instances from
each node and spreads them out to the
other nodes that you just joined into
the cluster I hope that answers your
question okay but what you I mean you
cannot like I said you cannot change the
partitioning scheme on the fly yet right
any other questions
yep
upgrading the reliable collections okay
so the question was do I have any
experience with upgrades and reliable
collections yes I have experience well
what well we have to do this because we
provide this saga persistence that
stores state into reliable collection so
the reliable collections right now but
this reduce the data contracts your
lives right exactly and of course there
is I think there are even books written
to that topic from the top CF world the
data contract serializer has its own
benefits and drawbacks and there is a
whole set of articles on MSN which talks
about data contract versioning so you
have to apply the whole ordering of data
members you have to apply a namespace to
the data contract you have to implement
this I extensible interface thing all
that stuff you have to do it but you can
use a different serialize if you want to
okay and then three tries yeah yeah one
what what you can do is based on the
application the script description you
can for example deploy a pre hookup
process that does a partial migration on
that partition and then only then the
actual service kicks in for example and
that will be applied on while doing the
rolling upgrade as well
okay yeah take your chocolates yeah
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>