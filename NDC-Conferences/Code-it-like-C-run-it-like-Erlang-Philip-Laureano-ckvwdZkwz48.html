<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Code it like C#, run it like Erlang - Philip Laureano | Coder Coacher - Coaching Coders</title><meta content="Code it like C#, run it like Erlang - Philip Laureano - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Code it like C#, run it like Erlang - Philip Laureano</b></h2><h5 class="post__date">2016-10-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ckvwdZkwz48" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright let's kick this off hi everybody
my name is Philip laureano I work at
domaine and today I'm going to be
talking mostly about a cadet net but not
so much on the how to do certain things
within a cadet because within domain we
actually did this into production and
what we implemented was a real-time
clickstream event processing system and
this is more of a retrospective of what
happened how we approached it and
there's a few different things that we
did during the course of our journey
into reactive programming so as with
everything else I'll start with the
beginning we ran into a very interesting
problem which was how many people here
have batch process processes that take
more than a few hours to run every day
and how many of you have reporting at
some capacity within the business now
with a domain one of the problems that
we had is as you know if you're familiar
with domain we have a lot of properties
in both residential and commercial real
estate and one of the problems that we
have there is that we might have
hundreds of thousands of properties but
on the other hand we can afford to wait
till the end of the day to see whether
or not a particular property has the the
clickstream or the views and the
inquiries that it needs so in the first
version of what we called our stat
system what happened was it was pretty
much what you would expect from any
traditional system we had we collected
all our vents waited till the end of the
day and we stored it inside of a sequel
server instance and for a time that was
okay and at the end of the day what we
would do is we would run a store proc
collect all the stats for the day and
then by the time the agents would look
at it the next day everything was great
the problem was that was five years ago
the problem was that the scale of our
growth was such that it was no longer
practical to be doing this on a daily on
just a daily basis because as the more
and more properties we had there was a
greater need to make things closer to
real-time as possible so there's this
one question we have to ask in this case
what was a better way to do this is it
better to do one long batch process that
would take six hours or an entire day or
could we somehow split it into say
86,400 batch jobs in one day that take
one second so we did a bit of research
and we came up with a lot of things but
the first thing that we we found oh I
found was the actor model now if you've
been paying attention to Erin's talks or
Sergey stocks with Orleans there's a
couple of flavors of that but along the
way the actor model poses quite a few it
actually solves quite a few problems
that we weren't able to solve with a
traditional store Prok model first thing
that we tried was doing async we quickly
found out that it became unmaintainable
simply because well I'm a fallible human
I can't think past 10 threads so we were
looking for better ways to actually do
this and if you're not familiar with
async it's fairly straightforward but
the short explanation of this is it's
basically a syntactic compiled syntactic
sugar over what is essentially the task
parallel library now it's fine if you we
were dealing with just a few properties
but in domain since we had about a
hundred thousand properties and we had
all these different click stream events
coming in at once and when I say click
stream
I'm talking about when somebody does
something with a property listing on
domain whether it's click on email a
friend or they want to look at the
pictures we want to record every single
thing that they do and this kind of
clickstream data is really really
valuable because it gives us an insight
to what the market wants and then in
turn we could take this data and use it
to come up with better solutions for
some of the problems that we're
currently having at the time so with
async the problem that we had was mostly
around shared state you could spawn up
several thousand async threads but you
start worrying about whether or not you
have deadlocks whether there's something
some resource that you haven't released
and at the same time we didn't really
scale beyond a single machine it's very
hard to debug these sorts of things
simply because if you catch an aggregate
exception now you've got to drill down
into every single thread that fails even
though the c-sharp compiler does do a
good job over abstracting away a lot of
the state machine logic that is in place
now with the actor model we don't have
to worry about those things yeah with
the actor model it's as Aaron once said
in the previous session it's it's just
as old as the concept of relational
databases with and the idea here is that
there's no locks second we let actors
fail and fail hard because we could
easily restart them if there's any
issues and the other thing is that it
scales very very well to thousands of
threads per machine and in our case we
scaled it up to thousands of machines in
AWS so we had it had thousands of
threads not only go across one machine
to machine three machines but we got we
put together an infrastructure that
could tell whether there was a high CPU
spike inside of a cluster and we took it
to the next step in
made it closer to Erlang because in
Erlang what they do is if if there's a
you what you can do is remotely deploy
code so essentially within domain what
we did was combined
akka octopus deploy and if there's any
kind of situation where we sense that
the CPU is spiking we can provision two
or three more machines within ten
minutes and deploy another set of actor
systems and just keep it running there
was a quite a few challenges here and
I'll go over them but the challenge some
of the challenges that we ran into was
mostly around state and sinking state
and how do you sink that state across
all those nodes as they spin up and spin
down for the actor model itself it's
fairly straightforward and it and it
works on a few simple rules itself every
actor has one mailbox and the only way
actors can actually talk to each other
is if you send one message between the
actors the other important thing about
actors themselves is that they could
spawn child actors so how this works is
similar to what you would see in I OC
container where you push all your
riskiest dependencies out to the leaves
rather than leaving it at the core so in
this sense you what you would do is
you'd start spawning child actors and
push it all the way down until you have
specific implementations that are
localized and it will prevent the entire
system from going down that being said
actors have at least within a canet have
this ability to supervise other actors
you get these nice little trees that
what you when you send a message to the
top level actor it might delegate all
the messages down to all of its child
actors in practice what we've done is
we've had these coordinator actors and
what they would do is they'd we'd have
one that would read off of
sqs we have one that do it would do
calculations and it works very not very
very nicely because we didn't have to
worry about failures because we didn't
have this monolithic task parallel
library that we had to worry about it
and if there were and parsing aggregate
exceptions if it failed the other thing
as I mentioned before is the actors
don't necessarily take down the entire
system since we have child actors
there's a few things that you can do
with a cadet that allow you to either
restart failed actors kill them off or
continue with the air and just keep
going the other thing to note with a
cadet and as well as the act at JVM is
that it's fast
it handles roughly around 50 million
messages on a single machine and one of
the things that we learned the hard way
is that with this cut with this kind of
power we had to scale back our actors
because it was too powerful on our
machines there was one instance where we
actually get a nice email from Amazon s3
because we turned it into a logging
system and we were doing about a gigabit
a second of just log files and then when
we had we had to scale it back so this
is a very powerful system that we have
in place now with akka itself it's
pretty straightforward
so with actors being able to send actors
messages to other actors this is
probably the simplest example that I
could come up with now a message in this
case is pretty much any immutable
reference object that you pass from one
actor to the other it's also worth
mentioning that actors themselves are
location transparent so as long as you
have the actor reference and you'd call
a send to another actor you never have
to worry about where it is the other
thing is that with this
as simple as it is this is more of
syntactic sugar this is the strongly
typed version so if I go through the
constructor here you could see that I'm
sending a string in practice you
wouldn't want to send a string you'd
probably want to send something like an
immutable class with no public setters
just to make sure that you could
serialize it and have guarantees about
concurrency never have to worry about
locking or any kind of threading issues
and as you can see here this is pretty
much an example of what you would do for
an immutable message you've got all
public getters and private setters and
in a Quebec ax or a cadet net what it
does is it handles all the serialization
for you in some form of fashion it would
serialize this down to either Jason I
think the latest versions do binary
serialization you can always check with
Aaron and sending messages is really
really simple
so the TEL method is you've got one
message and then the second one the
second parameter there is the actor ref
or address in this case I just said
itself just because I want to reflect
the message back and process it it's
really really that easy the other thing
you should note is that with actors
there's this concept of an actor ref the
address itself there's a few things here
at the protocol it's fairly
straightforward so it's it's TCP because
you want to send it over an HTTP
connection you have the name of the
actor system so that's just in case you
might have more than one actor system
inside of a machine and at the same time
you also have the host name and port as
well as the path now if we look back at
some of the other diagrams where you had
supervision where you have a Youth slash
user slash actor name that's basically
the path in the tree for whatever actor
you want to talk to so for example if I
want to say user actor name one and
there's an actor under it called foo it
would just be slash foo
and I'd be sending messages that way so
it's fairly simple to be able to address
and send messages across the wire using
this kind of addressing scheme now the
other interesting thing that we found
out pretty quickly is if we want to do a
kind of broadcast is you could do a wild
card based broadcast just by using
something called the actor selection now
with actor selection what you can do is
if you're in an actor at a certain point
of a tree the context object what that
does is say from this point in the tree
onwards I could send these sets of
messages so in the same sense you could
send message to an individual system but
you could also use actor selections to
do wildcard broadcasts to child actors
if necessary
now in the simplest sense if you really
wanted to create an actor system it's
one line of code plus maybe the name of
the actor system if I inlined it in a
production environment of course you
there's a few more configuration
settings that you have to set you can
you can die you could do that through
either ho con or you could do it
programmatically which is what we prefer
inside of an AWS environment and the
reason why we went with programmatic is
that more often than not when we
deployed to these boxes in AWS it became
impractical to have to RDP into those
boxes and make sure that they have the
right configs we want to make sure
they're all homogeneous but at the same
time we want to make it as zero-touch as
possible and still be able to function
now the syntax for creating an actor
system is something that's been
inherited from the JVM and what this
essentially means is this is analogous
to what com did back in the 90s when you
said I want to create an instance of an
actor now inside of the props doc create
is basically a lambda
tells a canet how to create your actor
regardless of whether or not it is on
your local machine or on a remote
machine if you're going to be using
clustering so in the same way once we
create the actor you could just do a
tell on the actor without having to
worry about where it is at the same time
if I want to create a child actor all I
have to do is use context which
basically means that what wherever you
are in the hierarchy you want to create
a child actor of my child actor in this
case again it's the syntax is very very
regulus very straight forward simply
because there's not really a lot of code
that you have to do in order to get it
done
the one thing to remember in this case
is that while you're doing all these
things with actors you never have to
worry about locking it's very very safe
to have private collections that are not
concurrent collections for example you
don't need a sync lock object you can do
whatever you want with it provided that
all your state is immutable and not
publicly exposed the other thing that's
really interesting at least from an
architectural standpoint is does anybody
remember Windows 3.1 or ya or does
anybody not want to remember Windows 3.1
so what we realized is that this is
actually a degenerate concurrent
cooperative multi-threading distributed
system so basically it's like
distributed Windows 3.1 without the
disadvantages because when you spawn
actors you're basically saying you're
basically sending it out into the world
with the assumption that nothing can
break it because it doesn't expose any
state the other thing to remember is
that with these actors you can just keep
doing all these things in parallel and
you never have to worry
about how are they going to interfere
with each other and later on I'll show
you how we could spawn it up to several
thousand threads but still be able to
treat it as if it were a single thread
and not have to worry about it of course
not everything goes according to plan
with akka dotnet actors when things
start to hit the fan the problem is that
how do you handle errors when things
happen now there's something in from the
original paper from Carl Hewitt which
which is this idea of the air per kernel
and how this works is that the air is
bubble up from the child nodes all the
way to the top and in these child nodes
whenever things go wrong the parent
actor has has a couple of choices about
what they want to do in case things fail
in general there's really two strategies
you could come up with your own custom
strategy the first strategy is going to
be that you could kill off the actor
that caused the failure as a parent
actor or what you could do is you could
kill off all the child actors and
restart there's also the option to
resume but it's like vb6 on a resume
next you don't want to do that because
you want to make sure that when it fails
you restart it and keep it in the right
state and that's what we've learned the
other thing is so at this point I've
been talking about how to handle a
single machine multi-threaded scenario
but how do we handle cases where we want
to go distributed because it's great we
want to do it
use a canet we want to be able to spawn
any number of threads we want but how do
you make it so that this works on a very
large scale so for us we had this huge
problem the one megalithic database
server that just couldn't handle the
load we would get thousands millions
upon millions of events per day but we
couldn't process it at all and now we
have this challenge where we wanted to
do it in real time
so the first thing we can do when we
start talking about how to scale up with
the single machine is that there's these
there's a special kind of actors called
routers and router pools now with a
round-robin pool what that means is that
it is a specific type of actor that
spawns thousands and thousands of other
child actors of this type depending on
the load so what this means is if I send
a message the message will come in
through the router there might be up to
1,000 instances inside of that router
actor and it'll just go through every
single one of the actors in a round
robin fashion until every single one of
the messages are handled in parallel so
the nice thing about this is that I
don't have to worry about managing state
I don't have to worry about what to do
with every single actor because this is
kind of like a classic object-oriented
programming where every single class is
responsible for its own state every
single actor is responsible for making
sure that as child actors are behaving
properly and if it doesn't then the
router will kill off the the child actor
depending on the strategy of course now
as I mentioned before you don't have to
worry about locks and you don't even
have to worry about actor allocations
because the child the round-robin
routers have a parameter in there that
say what's the minimum number of actors
you want and what's the number what's
the maximum number of actors that you
want to spin up so in the main what we
did it was for a single machine
configuration we started getting all
these messages that it would flow in and
obviously if we're you're thinking of an
actor as a single threaded actor you
don't want to handle this on one thread
you want to spin this up about a
thousand actors so that we could
potentially process them all in parallel
so there's a few other pool router types
but this is more of a exercise for
Google there's random which doesn't make
any sense but because we want to evenly
distribute the load
there's consistent hash routing which is
more useful if you're going to go down
the path of clustering and then there's
a small mailboxes which is just trying
to route the messages to the actor with
the least amount of activity now I know
this the code is fairly small in this
case but this is pretty much all we
needed to do to scale to thousands of
threads per machine this is just a
simple PowerShell script that starts up
an actor system five times on the same
machine there's nothing magical about
this and that's the point we didn't have
to do anything special to do that the
interesting part is that we were able to
scale up our capacity in two ways we
could either deploy two more machines or
run more instances of the actor system
on a single machine so that we see the
CPU spike up to 100% now the interesting
part is how do we get these actor
systems to work together there's a few
it depending on what your requirements
are there's a few ways you could do this
now the first way which I don't
recommend is really going with a cadet
remote that's really just peer-to-peer
connecting to actor systems together
with nope no redundancy at all and
that's what akka cluster is based on an
aqua cluster is a good tool in the sense
that it's completely decentralized it's
a peer-to-peer node cluster that Alexis
leaders and there's quite a bit of
gossip between the nodes but there are
some drawbacks and of course we'll go
over that as well the one we chose that
was most favorable was the cloud-based
queues so we're on AWS we use SQS to do
batch jobs and what we found is that
it's in most cases AWS is reliable
unless you you know
it was a couple months ago where it did
went down go down for a couple days but
for most of the time it was perfectly
fine now a crow remote so as you can see
here this is just a simple diagram of
all the states every single node is in
inside of a cluster so what they do is
they start talking to each other and the
algorithm is basically determined which
nodes are connected within the system
and if they're not able to talk to each
other they actually talk through an
intermediary node so they could be all
of the sync
now clusters themselves make it so that
every single node inside of the cluster
behaves like a large monolithic system
in most cases it does work well there
are systems like the saboteur react and
whatnot that do this but what we found
inside a domain that this doesn't really
scale well at least for our requirements
and given that fact that we would have
lots of machines spin up and sometimes
disappear
depending on what the CPU load was so
again there's a few disadvantages here
the number one disadvantage with in a
cluster is the fact that if for some
reason let's say you had five nodes and
in those five nodes they were behaving
as part of a cluster and in our
environment we did a WS detected that
the CPU dropped below a certain
threshold and we could kill off two
boxes or let's say three if you kill off
another enough boxes within a cluster
somebody actually has to manually
intervene and tell the rest of the
remaining nodes that the cluster and all
the missing nodes have been removed
which is quite messy because it's not
very cloudy because we want to do this
in such a way that doesn't require any
kind of human intervention this should
just work so I took the time to just
draw a very crude diagram of how we what
the flow the data is with
in domain so we have a quick stream
where people do things like send
inquiries we say for example you might
have somebody click on a nice house and
say I'm really interested in buying this
property or renting it and I want to
contact the agent and figure out whether
it's still available we need to record
every single thing that everybody does
on the domain website and that's what's
happening inside the click stream now
the tricky part about this is that we
have to save this at this roughly around
the same time we have to do aggregates
for every single property on the domain
website so you can imagine if there's a
million properties that are active per
day we want to come up with the total
number of inquiries how many times
somebody clicked on something and have
those metrics available at any time and
they should be up to date within 10 or
20 seconds or at least something much
faster than the six hour or 12 hour
latency that we were experiencing in the
beginning so how this works is that we
found that we had to save it in a couple
places we had to save it in s the s3 so
that we can export it into redshift and
at the same time I didn't put it on the
diagram but we had to save it into
elasticsearch because what we found is
that even though in theory if we had two
copies of the same data one going to
elasticsearch one going to s3 they
should be identical at the end of every
day we always found that there was some
messages that are dropped so to prevent
any kind of loss what we would do is we
would do how we compile a hash list of
all the events that have occurred within
a day and compare it to what was
exported in s3 and compared to what we
had in elasticsearch we would do a diff
and then once we figure it out which
properties were affected as a result of
the loss of that diff we do an
end-of-day recompute to make sure that
we didn't we have the right numbers and
we still have the eventual consistency
that we wanted without sacrificing
accuracy or losing anything which was
very very important for us now the other
thing that we found is that Amazon sqs
is really really good at distributing
workloads at first we were going to go
with Kinesis but the API depending on
what SDKs you use it was a bit raw and
we needed something really simple we
wanted to reduce the consumption and and
posting messages down to a single HTTP
GET or post in this case so that's why
we went with Amazon sqs now if your is
everybody here familiar with AWS
SQS or how many people here use Azure
how many people here use AWS so that's
about half and half so when we were
looking into SQS it had a really
interesting property where there was
something called a timeout an
invisibility timeout and the idea was
that if you pulled a message out of sqs
you could set a time limit where we it
would become invisible and if you didn't
delete it it would pop back into the
queue as if it was never processed now
normally that doesn't seem like a lot
but for us it had an interesting side
effect because we could process jobs in
parallel and if the jobs failed we
didn't have to know whether they failed
or not because we would know that they
failed if they were still in the queue
so if it failed the first time and an
actor didn't delete the message off of
the queue it goes straight back into the
queue and we have a different actor
system process it all over again and it
would do it completely in parallel now
the best part about this is that when we
had five or six different actor systems
they didn't need to talk to each other
at all there was no sync time when we
had cases where we were getting we're
about a million messages
behind it would became a really easy
scenario to handle because now all we
had to do is create another set of actor
systems or spin up about five or six
more machines and then we were back in
business and they were able to process
the load there were a couple trade offs
of course because with this kind of
amount of power we could easily take
down any database server so as you can
see here the diagram pretty much shows
what I was talking about you have all
these data sources that post a
clickstream event which triggers an
aggregation request that we stick inside
of the queue on the blue side we would
have an aggregate of an aggregator that
would just spin up pull the message off
the queue pull all this data into a
snapshot in elasticsearch do the
computations and then throw it away and
save the result so it was a really
interesting scenario because we did have
it we didn't have strong consistency but
we did have eventual consistency that
fit within the latency period that most
people were expecting at the time
because at the time agents were
expecting things to happen within 24
hours if we get away with doing this
within a couple of seconds that and they
notice a few glitches here and there
it's not a big deal the fact that we
were also using event sourcing where we
might save events straight into
elasticsearch and there might be
inconsistencies between two sets of
actors trying to do the same aggregation
was not a big deal because we never
really deleted any data at all they
would get a different snapshot and maybe
one would overlap the other but we
compensated for that by doing end of a
snapshots where we would figure out
which properties were active and do one
last calculation so we have the final
sense of consistency that we are looking
for now from an academic standpoint you
might be asking so what's the difference
between clusters and grids and it's
actually quite simple I mean I don't
have to go through this list but what it
comes down
- is that clusters require consensus
when things go bad inside of a cluster
you all the notes basically say are you
okay are you okay what happened to this
node a what happened to note B is that
node okay
but within a grid we could linearly
linearly scale by adding five or six
more nodes and since every single actor
system is behaving as if all the other
actor systems don't exist it's very easy
for us to scale up and be able to push
new nodes out without even have to worry
about this so our sample grid
configuration looks like this so in a in
a classic grid configuration you would
have a master node that would start
handing out jobs but in this case we use
sqs because what happens is that when we
push something into the queue one actor
pulls one message off the queue at a
time just to be fair and we might have
twenty actor systems that are pulling
one message at a time and processing it
so every second they pull one message
and process it and I wouldn't know where
it's actually occurring on but it
doesn't matter I don't even need to know
whether successful or not because if it
fails it drops back into the queue and
it's processed all over again now for
easy two instances this is the
interesting part depending on the load
we we fixed it to a minimum of two
instances just so we had redundancy but
if there was a high amount of traffic we
would scale up to 20 nodes and that came
that would come out to be a hundred
actor systems in parallel now with this
kind of scalability it's not something
you could easily pull off in cluster
with cluster you wouldn't be able to
handle the fact that you're taking out
and putting in so many nodes at once
depending on the demand in which this is
what we needed the other thing that we
have is that we actually in domain we
have this custom tool that we built into
octopus which is what we call the robot
army where we could spin up entire
clusters in a line of code
and deploy certain versions of octopus
deploy packages straight into these
machines without even having to worry
about any issues I mean in most cases
what we were able to do is hit the
button go up get coffee come back and
everything was all good now in this
scenario when we are handling
clickstream events what was happening
was the AWS would detect a high CPU
demand the robot army would respond by
creating two or three more instances and
then push the actor systems out to these
boxes and then all of a sudden you see
this massive CPU drop that would be able
to handle the click streams as they came
in now compared to say your traditional
relational database this was way better
because we found that we could we had a
latency of about 10 seconds and that's
across any number of properties at once
and this was doing this hundreds and
thousands of times per second the other
interesting but notable thing that we
did was we log to slack so in cases of
failure one of the things that we ran
into is that since all these systems are
completely disconnected we need to be
able to log this information to a
centralized location so slack was the
best place so we just created a custom
channel with just a bunch of alerts in
it and then we were able to push out all
this information that now for grid actor
roles the idea behind grit grids was
something I pulled out of swarm
intelligence AI so if you ever look at
how bugs or ants behave you'll see that
within a colony they always follow a
very simple set of rules by themselves
they don't seem very smart but once you
start to multiply them to larger numbers
they start doing pretty amazing things
and in that sense that's what we did
with the grid so there's a few things we
did so when for pulling aggregation
requests off the queue we had one actor
that did it and then it would forward it
all the other actors that would process
it we also had another actor that would
just fetch the snapshot for a particular
property and all the clickstream events
that occurred during that day and then
do the calculation and then we just
dispose of the actor naturally and the
other thing to keep in mind is that when
we moved events from sqs to s3 buckets
we also had actors for that as well we
also had different actors that would do
this kind of reconciliation where we
would look at what we had in
elasticsearch and at the same time we
would also look at what we saved into s3
because ultimately what we were trying
to do was do an export into s3 and have
it imported into redshift so we can do
analytics on it so on one hand we had a
data warehousing solution that was the
ultimate goal but at the same time we
want also want to main consistency in
what we're exporting and calculating in
real time at the same time so there's a
few things that we learned when we were
going through this process now for top
one of the interesting parts that we we
found is that when we are doing these
kinds of aggregations we were actually
sacrificing availability we had weak
consistency and we did have strong
partition tolerance it was a really
interesting case because the only form
consistency that we actually had was
what we were storing in elastic certian
if if you do a bit of research in
elasticsearch you'll know that it's not
entirely strongly consistent but it is
eventually consistent and that's brigly
to the first point now given that you
could just with this kind of capability
with a code on net and working with
multiple nodes you could easily take
down any server that is elasticsearch of
that is your relational database you
name it with enough machines and enough
actor systems
you could pretty much do anything you
want with it so for us what we learn is
that it was more of an exercise in
restraint rather than trying to throw
all this capacity at it because even
though we had hundreds of thousands of
threads that could go at once we weren't
we were ultimately bottlenecked by i/o
at the end of the day now the other
interesting thing but it does sound a
bit contradictory is that grid actor
systems are great if they store
absolutely no intrinsic state that is
long term state when we are doing these
kinds of calculations what we would do
is we pull in the snapshot and it was
consistent for that moment calculate it
and then throw it away now what that
guarantees is that we could pretty much
kill off any actor and the next actor
that came along to process the property
itself wouldn't have to worry about
whether it was wrong or not because we
knew number one we never deleted any
events we're using events or see number
two we never have any kind of
inconsistency problems because towards
the end of the day we do one last
calculation when we have everything and
that would guarantee that numbers were
correct the other thing to keep in mind
is that we had to operate on the
assumption that any one of our machines
could just vanish at any minute and
that's because of auto scaling both up
and down and this is where the partition
tolerance part comes into play if we
don't have any state we keep the state
in an external node which is effectively
elasticsearch we don't have to worry
about any kind of consistency problems
so there were some pretty interesting
trade-offs because essentially every
time we would get a new event we start
querying events back into every single
actor which there is a bit of overhead
but it was a good balance for us because
we were finding that we still maintain
consistency we still main accuracy but
it it did take a while for everything to
line up and that was more towards the
end of the day but the side effect there
is that when agents started looking
at their reports they could just refresh
the page and do something and they could
see it happen in real time or close to
real time and that's what we were
looking for because we came from an
environment where we were waiting an
entire day just for somebody to click on
something in the number to go up now we
are in a state where if somebody just
clicked like four or five times and did
anything we'd be able to see it within a
minute or so which was like infinitely
better than where we were now it goes
without saying that if you have an
elastic cluster of actor systems you
need to have a matching data source that
could scale up with it in our case we
found that s3 because of how its built
could scale up well with the demand at
the same time elasticsearch depending on
your instance type can scale up along
with the number of actors that you have
so depending on how many actor systems
you have as well as the elasticsearch
instances and whether or not the auto
scale if we start hammering the server
what we have seen is that we would spawn
of another elasticsearch instance and it
would be able to take the load and once
the demand started to go down what would
happen is they pull the elasticsearch
instance out but it would still keep
running and that was really important
for us the fifth thing that we learned
during the storm from hell about a month
and a half ago was that it's better to
make sure that even though you think
that everything's going to be okay there
you always have to assume that you're
going to have some sort of message loss
that occurs so if we have all these
events coming in even if it's one in a
million if we have 20 million events
coming you're still going to lose 20
messages and the problem with that is
that within domain we can't afford to do
that simply because
one of those messages could have been a
message to an agent saying hey I'm
really interested in your property you
know that could have been a multi
million dollar sale depending on where
in Sydney or any other capital city
that's selling property so we wanted to
make sure that we don't lose anything so
the interesting story here is that when
the site went down because AWS went down
in the Sydney region we noticed that
there's a lot of data that we had within
elasticsearch that was not in s3 we lost
three days worth of data now three days
worth of clickstream data is nothing to
laugh at it is a horrible horrible
situation but since we what we did is we
came up with making sure that we created
two copies of the data every time it
came in we were able to use that diff to
fill in the blanks it took us forever to
do it took us about a day or two to do
it but we were able to fill in those
blanks so the important thing to
remember in this case is that as long as
you have a backup and you could you
could have the same kind of eventual
consistency where you can merge the two
sources and fill in the blanks then you
should be alright but the lesson to take
away from this is the fact that you it's
never keep all your eggs in one basket
because you will lose it at some point
in time the other thing that I probably
didn't mention in this case is that for
those of you who were interested in the
differences between how akka does things
and Orleans behaves it's really more of
a difference in philosophy
now with akka it's its base the
difference between the two is really
manual versus auto in Orleans there's
this tendency of let's take care of
everything for you and you basically
treat it as distributed c-sharp and
that's fine for some people but in this
case if we were to do the
kind of scaling it just wouldn't work
for us because if we take machines out
and all of a sudden I don't think the
Orleans cluster would do exactly what we
want to do and it wouldn't allow us to
tweak it in the same way that we would
be able to do with in AWS especially
considering they were in Azure so but
aside from that I'm pretty much open to
any questions because this I know this
is how are we doing with time yep cool
so what I'm going to do is this is more
of a QA thing because I know that I
didn't really go over the how-to side
though but there's a lot of interesting
stuff that we came up with insider
domain so the other thing I would also
plug is that in the same room in the
next session are going to be to our
DevOps engineers who pretty much pulled
it off and they're gonna be talking
about how they do auto scaling in
excruciating detail in AWS including all
the PowerShell scripts including how
they hook into octopus so please please
please if you can watch the session
because that's really worth it and I did
hear a rumor that they will open-source
it very very soon so all the tools that
I've mentioned here are going to be open
sourced so with that thanks for coming I
pretty much open to any questions
yeah
so there's for the actors themselves
there was a heavy amount of pub/sub so
how it works is when we were pulling
things from sqs we'd have one actor that
would keep on pulling SQS for a certain
amount of messages on a timer and every
time it would send something out the way
it works is that we had a base class
that was basically did all the pub/sub
where it would just do a tell and we
would keep a list of actor refs
and it would go through that list and
just basically do your standard pub/sub
and and forward it over the big one of
course that I did mention before is the
competing consumers bit there were a
couple instances where we try to farm
out a lot of the functionality just to
make sure that if we had failures we
didn't really take down the whole system
but I do have to mention that one of the
problems that we did run into is that if
you scale the actor too much the actors
too much you start running out of memory
one of the problems is that if you pull
it in too much information at once you
will basically run into an out of memory
exception and kill off all the actors
are in every single actor system so it's
a balancing act and it's really about
tweaking and making sure that you have
the right settings and that's just part
of a cadet and what else yeah
yep it's definitely not using our leg
yeah yep well in this case like I said
akka is really manual versus auto if you
the functionality for you to do it is
there so it's more of an infrastructure
tool rather than giving you something
that does everything because that
there's depent I would have to say that
akka is more of a true to what it's a
really true implementation of the actor
model but they don't really try to hide
anything and in that sense if you're
familiar with Kathy you're very
confident about how you're going to use
it then you just use what you need and
you don't just don't touch the ones you
don't need and so to answer your
question there is no car kind of
duplication in our case what we had to
do was we did have to create an actor
that would just push to two different
places one would save to elasticsearch
one would say to s3 but that was pretty
simple to do and we controlled all the
instancing we also determined how many
actors we'd spin up so overall it's it's
just that fine grain of control that we
needed and in this case you can't really
do a grid and say Orleans
it just doesn't make any sense because
that's not really what they want to do
well it's I guess it really depends on
which philosophy would fit your
requirements they're very different
so with Orleans the Udo controlling
instancing you don't control the
lifetime of an actor you just assume
that somewhere in that black box is an
actor instance that you could maintain
and you know send messages to and
they'll guarantee that it's there but if
it goes idle there's a good chance it
might get killed off and you have no
control of it it's kind of like garbage
collection you well they have this they
have this terminology where they say
they hydrate or dehydrate actors where
you put in plugins and what they would
do is deserialize itself and then once
it's activated then it would try to
rehydrate itself and handle the message
as if it was always always there they
call that virtual actors in akka if you
go with clustering the trade-off is that
they don't have any cluster metrics so
you wouldn't be able to directly
distribute the load across five like say
five nodes
whereas in Orleans they are very smart
about making sure that if you get you've
got five nodes it'll equally distribute
the work so it depends if you want to do
like transparent like distributed
c-sharp without having to worry about
the low-level details then that would
work in Orleans but if you wanted to
make sure that you could scale up
without any hiccups then you really have
to go manual and that's what we did
anything else well that's an easy crowd
thanks for coming
this in for Manning you can check this
one out because it is an intro to a
Canet and it's coming up pretty soon if
not out already so just have a look at
this one you could also check the
references here and as I mentioned
before the domain DevOps guys who do all
the magical stuff they're going to be
doing
quick in a very in-depth session on how
they actually do it and how they move
domain into like fully full auto devops
and octopus so please please check that
out thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>