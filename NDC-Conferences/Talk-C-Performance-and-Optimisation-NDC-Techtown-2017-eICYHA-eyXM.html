<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talk: C++ Performance and Optimisation - NDC Techtown 2017 | Coder Coacher - Coaching Coders</title><meta content="Talk: C++ Performance and Optimisation - NDC Techtown 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talk: C++ Performance and Optimisation - NDC Techtown 2017</b></h2><h5 class="post__date">2017-11-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eICYHA-eyXM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome to this talk on the c+
optimization for c plus 11 and onwards
so it's not just 11 but using features
of 11 so why whatever what am I going to
talk about and why am i talking about
this well one of the reasons that people
use HEPA but it's because they want
performance they want control of what
they're doing and as I've gone through
people have I've been looking at
performance stuff and people said Oh
certain things are much faster than
something else you should always do this
and I'm going on hold on hold on there's
a lot of myth and folklore about this
and so maybe there's a better way of
thinking about this one and people
saying oh you should always use plus
plus a rather than a plus plus because
it's faster and it's better and really
ok well we find out where there is
people say oh you should always be doing
a shift rather than a divide and
whatever and that's going to make a
difference so if you division by 4 for
instance you'll find almost all the
compilers go oh it's a division by 4
I'll do the shift so actually makes no
difference why don't you just write what
you intend rather than doing that one
and how would you know well you have to
measure people will say things like
avoiding array right because you can't
cash an array and you can't use
registers and it's lowers as well really
ok well maybe maybe it is maybe it's not
but this all strikes me as rather sort
of kind of low-level view of the world
and then people say oh we should be
doing multiple threads really do you
think that is that the solution maybe
that's actually not a solution maybe
that's just another problem disguising
yourself as a solution and what about
things like move semantics when are they
appropriate how do they fit into this
bigger picture of performance and
optimization and how the cache is
fitting here cache friendly programming
great sounds like a good idea what does
that actually mean and when is it
appropriate so what I've come to the
conclusion is that actually that there
is a whole process that performance and
optimization is not a thing it's a
process it's a method it's a way of
thinking and so this is the story that I
sort of kind of find out find myself
repeating my head again and again when
I'm lying awake at night is what do I do
about an application you start off with
the application it's too slow
some definition of whatever to slow
eased in I mean how fast is fast enough
is the first question I ask does it
really matter you've shaved 2
nanoseconds of the startup time of your
program possibly not so you start
profiling or timing and you start
looking that you go ok fine so we at
least got the point we're now measuring
some people don't even get to that point
they start just sort of oh it's
obviously there I'm just making some
guesses as to where things are probably
not a good idea to guess these days
machines and compilers are quite complex
things so first of all why don't we go
and find the unnecessary calls and
operations you end up calling a lot of
stuff that you don't need to call your
calling a database in the loop or you're
doing something that you don't need to
do and often these are quite surprising
you go really that takes the time I
didn't realize that was going to be the
thing that's slowing it down and when
you get rid of that maybe then you're
fast enough so what is fast enough
first question there's no point in
making things just faster all the time
and because if that doesn't actually
yield any value for your business then
don't stop when it's fast enough first
of all define what fast enough is notice
that this here is in fact mostly just
waste there isn't stuff this is just
getting rid of things it's not like oh
I've profiled this and loading this
configuration file is the problem this
is like though okay maybe that isn't if
it was loading another file why am i
loading that don't go and optimize the
loading of that file don't load it it's
waste these are not really the parts of
replication and so and to use venkat
stuff from the keynote this morning this
is you've failed to be simple you've
done more than you need it's not
actually the essential part of your
application usually just get rid of the
stuff you don't need and get rid of all
that locking well then can we actually
get down to the application itself so
that's the first thing get rid of waste
because that's just not helpful doesn't
doesn't make it faster eliminate it the
next thing is right okay now I start
running my profiler and I find out that
malloc or mem comp or mem copy is at the
top this is quite common I've seen
people doing stuff in
telephony and we worked out there was a
whole 130 calls to malik for every
request that came in it was just like
ridiculous amount of memory allocation
why why were they doing all that lot
well as you'll see as we go through
Malik is really a bad thing to do in
many ways it's also a lack of simplicity
because first of all Malik Malik is a
way of saying please go and find me some
memory that's not in use which is
another way of saying police might find
me some memory that's cold and not in
the cash all right so deliberately make
me go and do something that's slow right
okay so one by the time I found all that
lot I've gone off I've done this one now
I've got empty memory so the next thing
I now have to do is to go and initialize
it that means copying a load of stuff in
I have to do that again right okay and
the reason I'm doing this Malik is
because I've destroyed the previous one
so then gonna throw it all away so I'm
doing an allocation and a copy and an
initialization and throwing it away
rather than keeping the original one so
actually again this is just one of these
unnecessary calls you could doing keep
doing this why are you doing that one
don't go and make Malik faster just by
installing je Malik or the Google one or
whatever get rid of it it's fast don't
do this one work out why you do the one
here mem copy mem comp you may be
surprised by some by finding some of
these you may for instance find out that
this is really do two temporaries it's
not what you're doing you've done string
1 plus string 2 plus string 3 oh that
appears to be doing an awful lot of
memory allocation why we're doing that
one why are we using lots of temporaries
maybe we can avoid that one maybe you
can use in placement maybe remove
semantics we'll come and help us so this
is one of the reason we need to look and
see promise 11 for that there are
differences between GCC version 4 and
GCC version 5 on this one if you're
using visual studio you're already in
the GCC 5 world right now we've got rid
of a whole pile of waste and stuff we
don't have to do and we got rid of some
of that sort of non simple stuff we
actually end up with the application
itself the real stuff the real algorithm
and the thing we need to do
at this point we now start doing it now
it's worth starting to worry about my
algorithms now am I going to be dealing
with order and order n-squared order log
an order wine and all that kind of stuff
becomes more relevant because we've got
rid of the waste
we've got rid of the stuff that's that's
hiding the real thing
maybe caching is important to us caching
at the application level but this
requires us to know about things like
our operational profile by which I mean
knowing what operations you have the
access patterns is this V dominated are
you right dominated what's actually
happening you need domain knowledge at
this point to know how big your data is
how its accessed that's really where
you're going to get a lot of your
optimizations at this level and right
there on the bottom level if you really
need to go faster than out then you can
start worrying about things like memory
bandwidth and data layout and cache or
web programming but it doesn't make a
difference if you're doing these ones
here so back to this a lot of these
things here are really just picking
parts out of here at the wrong time and
trying to do stuff down here when you're
making mistakes up here is not going to
help you so for me performance is a
process by which you just chew your way
down get rid of stuff you don't need
then start working on the real stuff
then you can get into your algorithm mix
and then you can start saying and now I
can make a machine go fast right well
that's the talk really okay are you want
more right it's a multi-level problem
you started architecture and this your
design how do you actually design out
some of these problems not how do you
optimize them high just make sure they
don't actually happen or need I've not
talked about domain knowledge knowing
about access patterns structures etc how
do you use things like the STL the STL
is really powerful you just well it's
great use badly it's going to be slow
and I've got some lovely numbers to show
you just how slow it can be a lot of
applications do string handling badly it
can consume a lot of your time so a
little bit of that maybe memory
allocation some stuff about mu semantics
we also need to look at modern machines
CPUs compilers and what do they do and
she's get some tools get start profiling
see what you've got
that's the first thing you need to do is
to learn some of those tools right I'm
going to go through this relatively
quickly and recovered some of that this
morning but modern CPUs have got long
pipe lines they do lots of things in
parallel pipeline stalls are expensive
so you have to make sure that you can
keep the CPU busy it has a lot of branch
prediction run things in there trying to
do that one you may have for six or more
instructions in parallel if it's
available
that was the data fantasy stuff we took
and Ray was talking about out of order
execution so what you write isn't
necessarily what happens it's very
difficult to see what happens and things
may start in different orders basically
what happens is at runtime it builds a
hardware dependency graph of what those
things are and does whatever it can
branch prediction very important to
avoid the pipeline stalls it's a well
study thing I think the relate latest
AMD Rison processor has got a neural
network in it for doing branch
prediction that's how important it is
because a branch prediction if you've
got a stall 15 20 cycles or 20 cycles of
stall six instructions in parallel Wow
do the math and you end up with 120
cycles 120 instructions you just not to
done that's why it's important modern
processes may even decide actually I
can't quite work this one out but
actually why don't I just work out both
of them and then work out what the
result was at the end when the result of
the eve of the conditional has come in
I'll decide which of the two to keep
hyper-threading which is multiple cores
and the same thing at the same time and
also things like vectorization i've not
covered things like GPU here that's a
whole other search and stuff and I'm not
covering there here's the inside of an
Intel just to give you an idea of some
of the complexity 56 entry instruction
decode - so you can have it's got 56
instructions ahead it's looking at and
it's splitting those up in 292 entries
and it's reordering though
so if you think you know what's
happening the answer is you don't
it's don't even worry about it they've
they've done all that lot for you and
then you've got reservation stations so
if you're used to seeing register one
there is no register one register one is
actually a it's essentially like a
variable in a C or C++ program it's a
name for a piece of silicon currently a
slot on the stack and so register one
can mean lots of different places and
there can be multiple things called
register one at any one time and it has
eight things it can be doing in parallel
here with branches and divides and
whatever so the answer is it's really
quite complex insight trying to
second-guess that lot don't worry about
it I'm not talking at this level but
this is to understand the sort of kind
of machines your programming
interestingly enough instructions are
free but memory bandwidth is not so
here's a graph it could be any process
one here I haven't choose power and this
is the growth in terms of gigaflops per
second this is basic sort of raw CPU
power going up like this across
generations and look at the the memory
bandwidth going up here it barely goes
off and there was a bit of a jump here
because they did some special stuff but
but basically using C they you can throw
a lot of instructions of this lot CPUs
you're spending most of your time
waiting for memory one of the
consequences of this is that actually
only ten or twenty percent of the time
is actually the CPU doing real work if
you try to get some optimization of that
10 or 20 percent by turning on oh three
and using lots of compiler options the
compiler can only help you in bits of
this unfortunately it's your job to try
and deal with this part the compiler
can't help you with that
so there is plenty of room for you to
optimize and things the compiler can't
do so it's up to you modern machines
have cache hierarchies in them so here's
a fairly typical cache hierarchy
multiple cores like this l1 l2 and l3
caches and main Ram one two three cycles
here five to 20 at least this one here a
shared cache of however many megabytes
here and main memory hundred three
hundred
Michels what happened like that modern
machines are really good at doing this
they're designed to do that if you try
to do concurrency by the way you're then
trying to do this across here and that's
slow so this is why you don't want to be
doing concurrency and sharing of things
reading stuff up and down like that
showing across with that is not going to
help you here is a an interesting set of
numbers so this is a gate in a chain
cause thing but you'll find this in lots
of other places this is a logarithmic
scale so notice these are orders of
magnitude like this and we're starting
off at simple register operations less
than one cycle because we can have month
of ones they're set row memory writes
getting a branch prediction right one or
two cycles getting it wrong 10 to 20
cycles so it's an order of magnitude
they're noticing things like division is
still slow it's nobody's really found a
way of making that faster calls here are
about the same as that which is
interesting you can get some things also
stuff like all this floating-point stuff
like this do you realize that square
root is actually sort of kind of in here
it's actually really rather fast notice
this one down here by the way we've got
allocation and de-allocation small
objects hundreds of cycles this is your
malloc thing so and that's basically
without taking into account all the cash
effects because you're asking for cold
memory so if you do that one here so you
may actually be up here but notice
you'll now have many orders of magnitude
out of here the really slow ones thread
context switch context switches are
Attica of utter killer and performance
so guess what if you're single threaded
you don't suffer that most of the time
good throwing an exception that's not a
performance option please don't do that
in performance code that's for error
handling context switch here just the
direct cost but without all the other
ones that cache misses there kernel call
well you're not gonna be making system
calls in tight loop but everything else
here if you got rid of all those ones
you notice that memory allocation is the
Dominator the other thing about this one
is we have got orders of magnitude
difference in here
and if you try to do this with threads
threads are going to give you an order
of magnitude because you've got eight
calls it's an order of magnitude you
can't make up for this getting for this
fast just by throwing some threads at it
and guess what the threads are actually
can make things worse so first of all
get your single code the single threaded
core code working quickly and then work
out how to paralyze it without
contention so that's first thing stick
over here and try and get remembering
allocation that's the other part of the
talk
okay so we'll carry on with that
measurement basic kind of stuff about
orders of magnitude so you may have
order n which is linear order one which
is flat it seemed more like cash stuff
this would be linear search you may have
a binary algorithm order log n order N
squared like this which would be sort of
bubble sort kind of stuff one of the
things to know with that is there's
always a Mach multiplicative factor K
and for small n you may well find that K
dominates so for small values of n down
here your fancy hash algorithm might
actually be slower than a linear search
your quick sort here in terms of log n
or n log n like this
is not necessarily gonna be faster than
linear search for small ones Rob Pike a
long time ago said for a small n this
will be better and n is usually small so
again small numbers law from and Ray
this morning so don't always go for the
most complicated algorithm work out
whether you actually in have and big
enough to merit some of those larger
ones you're trading stuff off you can't
have everything fast you have decide
what's going to be fast and what you're
prepared to be slow that's knowing your
operational profile knowing your
patterns if you have a read dominated
system then you make read fast and you
make write slow in comparison that's the
trade-off that you make for instance if
you're going to say I'm going to keep
all my data in sorted order because I
only ever update it once a day that's
great it makes read really fast and once
a day you do this sort but you're not
going to go I'm going to pay I'm going
to do linear search all the time I'm
going to take it into binary simple one
here the game's programmers have a
terminology for this array of struct
instructor
raise you will see this quite a lot and
once you start seeing this pattern
you'll see it a lot if any of you deal
with Time series databases this pops up
all the time by the way are you gonna
have a row store or a column store if
you ever done anything with analytics
databases exactly the same problem but
at the architectural scale so if I have
a struct at house number fields in the
eye base and I'm just going through here
and checking something like this this
means that every time we go through here
on into check here and then jump to this
one and then up to this one and then
jump to this one and I'm reading the
whole of X and using only a little bit
of X in this one here I'm having all of
the integers and all the floats
essentially in columns rather than rows
and now my loop says oh I'm going to go
through just this comp and this means
that I process every byte I read and
there's a term called read amplification
which is if I want to read one byte what
I end up reading 64 bytes in order to do
this one then I've got a read
amplification a factor of 64 that's not
good I want to read 64 bytes and use all
of them it's like carrying a suitcase
and using only one thing yeah so you
have to be careful about this one this
structure of arrays is often what you'll
find the games programmers are using it
also happens to say look everything is
now dense and linear and memory and
vectorization can help me compiler can
do that automatically could be sse or
AVX or they're even versions now up to
512 why is that useful well instead of
reading one integer at a time like this
I can now read for two time so instead
of going backwards and forwards every
time I can pick up four at a time or
eight at a time it's AVX
GCC will do this sort of vectorization
of loops if it sees this memory is close
together but you're only going to know
whether you can do this if you know your
access patterns let's look at some
generated code so here is a simple
summation loop like this if I compile
this with just the ordinary with no
vectorization I'm gonna add and then I'm
going to add and move around four bytes
at a time if you look at the oh three
vectorized version and
this parallel ad here and I'm doing 16
so I'm doing 4 at a time on my machine
that's two and a half times faster on
that laptop it's almost for free it's
just because I've laid the memory out in
the right direction I haven't done
anything else I've just put it in a way
that allows the works well with the
machine factor to not
so this array of structs structure of
arrays you can end up structuring your
data if you know about your access
patterns so if I have back turn to this
thing here with many fields like this
maybe I'm only using one commonly it
might be something like the key or a
timestamp or something like that well
why don't they just say here are the
columns that I'm using most of the time
and I'll separate those columns away the
hot columns away from the cold columns
so that way I can do that one here so
that's my job is just to know that the
compiler can't know that one you may be
able to measure it at run time but
that's too late for the compiler you can
separate out the hot columns and the
other ones here then what will happen is
that the ones that you're accessing like
this your working set is done for you by
the caching Hardware and now you go look
ok these rows and these ones here are
held in in the fast memory everything
else is now cold so you're using your
machine as well as possible you're now
dealing with that one two three cycles
access rather than 100 or 300 cycles
it's two orders of magnitude by doing
this potentially that's a it's a good
start again just reordering your data
strength reduction one of Andres
favorites eliminating those operations
we talked about that one reducing the
number of them changing expensive to
less expensive ones sometimes we can
batch stuff up loop unrolling it's a
classic example I put it in more for
just a reminder rather than saying this
is something you should think about
doing but you should think about doing
it but it's guess what everyone knows
about that one so down to this story we
started off with you get rid of the
stupid things and then start looking at
the memory allocations and temporaries
this is where things like move semantics
come in useful
move semantics in C++ 11 are a way of
reducing some of the problems that C+
0-3 hads 0-3 said ah I don't know what
temporaries are there was nothing in the
language about temporaries so if you did
string 1 plus string 2 and you returned
this this temporary it treated that
temporary in exactly the same way as if
it was an ordinary variable you couldn't
do anything with it well wouldn't it be
nice if you say look it's a temporary
it's about to go away why do I have to
allocate memory copy it out and then
destroy the original that just seems
like wrong that's we got malloc oh
that's not gonna be fast oh copying well
that seems like a waste of time then I
have to throw it away and then carry on
doing it madness madness
so with C++ 11 move semantics came in
and said well actually why don't I just
take the stuff out of the temporary and
move it where I want Auto pointer was
introduced in 98 had moved semantics and
fought the language all the way unique
pointer is its replacement and has got
that built in our value references move
construct and move up move assignment
operators similarly so what does it look
like under the covers so the 98 version
would say well I've got a string so I've
got this X here that has a pointer to an
array of data like this so pointing to
room data here if I return one by value
it'll end up copying it so what happens
it says alright you've got this
temporary here I'll do the allocation
I'll do the copy I'll assign it and then
delete the original so allocate copying
free all those kind of threes that's
just wasted whereas in the cPanel 11
version it says okay well since I'm
going away when I just go chunk don't
steal it so you've just pulled that
pointer across and says that's mine and
this now has nothing in it so this does
nothing so we've eliminated this this
extra waste and that's available to you
super bus 11 all the standard classes do
this one consider looking at your own
classes to do this one if you have
temporaries around here's the
implementation detail what does it look
like a
of constructor like this just as pointer
manipulation it says right okay so I'm
pointing that the same thing as he is
and I turn his pointer off with null the
move assignment says I need to delete
what I've got first before doing that
one so that's a classic optimization
which leads to the super lost 11 rule of
zero you may have heard the rule of
three that says if you need one of
destructor copy constructor copy
assignment you probably need all three
that was what you did in 98 and then
when move came along they turned into
the rule of five by the way you probably
need all five including move operations
to go like no that's just too much guys
why don't we just say we're only ever
going to use things that coffee and move
correctly so that's from a performance
point of view a good idea so I had a
client that did this one and they
speeded up their stuff by getting rid of
their own resource management and
including all which didn't have move and
replacing it by things like unique
pointer and they got a significant
improvement in performance and code
maintainability because they just got
rid of their code and the hurdle this
point of management in it and just said
unique point and get on with it
so you stand you value classes like
string vector map if you do need to do
resource management use unique pointer
and possibly shared pointer
inside your own classes some examples of
memory allocation and how slow it can be
I'm taking 10 million mints and doing
push back or insert into a container
like this so I do 25 allocations on this
one with a vector it's totally allocated
134 Meg's but it's currently got 64
megabytes of that deck does a large
number of these it's actually 40
megabytes divided by 512 is that many
78,000 so that's about what this one
here's deck is actually array of pages
list an order set and set you can see
they're doing a load of allocation here
so what you're really doing whenever
you're doing insert into a set or
another set or list is testing memory
allocator which and we know that memory
allocations aren't far
in comparison to other things so that's
not let's not do that too often we can
make this faster since I have this thing
of trying to eliminate that memory
allocation what happens if I say look I
know how bigger this thing is going to
be so why don't I say back to with the
reserve instead of reallocating what
memory all the time do one reserve like
this one allocation hey it reserves the
forty megabytes required once so I have
a choice between 10 million memory
allocations or one there's a few orders
of magnitude there notice also the
amount of memory this takes up this is
got 24 or 40 bytes per element so you
can see that's 240 mega bytes there as
opposed to 40 mega bytes there so I've
also saved on memory which is going to
be useful in terms things like caching I
get dense linear access with vectors
vector is your friend and vector reserve
is also your friend that was writing for
what about reading so reading from
containers I got some stuff here about I
got a container I'm gonna do some random
numbers and do some random lookup inside
it let's see what happens there let's
try linear search let's look at some
specialist methods etc piece of code
here don't need to understand all the
details I'm just doing a whole pile of
push backs or inserts I'm just going
around here I've got my clock if before
and afterwards and every thousand or so
I'm just putting out the number so I can
do graphs I'm gonna show you all the
detail of that one and I'm doing a
linear find here this one by the way is
just to stop the optimizer getting rid
of everything because all of these guys
you're not using the result and will
throw all the answers it might be very
careful when using profiling with
optimized code well what does this tell
us so I'm doing 100 lookups small out of
10 million like this so vector looks
like this and list looks like this and
set is even slower okay right
interestingly enough there's about a
factor of seven or eight between the
ones which store stuff densely together
and these ones which don't these ones
use pointers notice that this uses more
memory and this is slower and this is
really just to do with memory bandwidth
it's just how fast can I read this stuff
and this is why list and these old ones
are slower than these ones because these
are denser in memory it's a memory
bandwidth issue pointers in data
structures are just going to slow you
down if you want to try and get speed
then see if you can work out how to
avoid having to go and chase a whole
pile of pointers obviously linear search
is not the most optimal thing and you
wouldn't choose to use a set or an
ordered set in order to do a linear find
that would just be a wrong thing to do
so let's actually start using the
Container correctly and it either has
logarithmic or hash based lookup like
this and there we go it's much faster so
that would then be this is order n this
is order log N and this is order 1 so
there's some differences when you've got
ten million it is going to make a
difference
so it's notably faster but we had got
some domain knowledge that we haven't
actually used at this point which will
come back to our mod set vs. set like
this if you start measuring this you'll
actually notice that you'll still end up
depending what the type is you may end
up saying actually look mem come and up
being the top of my list I'm doing a lot
of comparisons so you often find at this
point right now it's worth starting to
understand what the memory comparison is
remember you got get rid of the stupid
operations get rid of the memory
allocations now we can start looking at
things like mem Company how we doing
around Grissom's we're down into that
order log n order one type of stuff
there was an optimization I tried and I
discovered that guess what the GTC
library had already done it if I had got
a set with integers like this it had
already decided that it knew how to do
integers it didn't need to hash the
integer I said I'll do the integer so
when I tried to write my own hashing
function I ended up with the same speed
they said it was a float notice the
float same size is slower why because it
goes all I have to hash all those bytes
so I can write my own hashing thing and
basically says well why don't you just
take the float and convert it to an
integer and use that and then back to
that speed so if you're using hashes
watch out for the speed of the hash if
you see mem comfort that
you probably have got something at
saying I'm doing a hash and I'm doing
byte wise comparison whereas if you're
using things like in flike this you may
would be using integer comparison which
can be inlined rather than mem come call
so watch out for hashing if you are
using hashing make sure that you're not
putting a lot of time into your hash
function I had one where I ended up with
changing from a triple of database and
table and key names of three strings
ended up with lots of mem comp and I
realized that actually if I can catenate
and together and get one comparison
rather than three comparisons on and got
myself a factor of three just by doing
that domain knowledge given that I know
that I've inserted the the numbers in
linear order then I can say they're
already sorted so I can actually use
order log n a binary lookup on vector so
it's interesting enough that I've gone
now remember this was I was looking at
100 it was quite slow but now these ones
are vector and deck are fast and set
they're not so that's order log n this
is order log n but through pointers and
this is an order one so that one still
wins but these are these are not
unrespectable also the size of this in
memory is much smaller so watch out for
those wants kind of things domain
knowledge something I definitely want to
you to to think about is this is
probably where you're going to get the
most stuff out of this one which is
understanding your domain patterns what
are the operational profile what
performance levels what's good enough
what's your data how big is it
what's its distribution is you've got
lots of things the same is it all lots
of small things but with some occasional
big things what's the distribution and
requests how often is it updated so I've
got some questions for you two to think
about when you're thinking about
building data structures for performance
and these are the six questions I'm
going to go through those and now in a
bit of detail that's a summary primary
key access I'm going to look up this
thing by key and I'm going to get a
value out of it that's probably the most
common thing it's very useful
you can say well it could be order one
through a hash or I could have a binary
search or if n is small I can just even
do linear search if it's ten who cares
about all this stuff just get in that
you only need operator equals equals at
this point you don't need anything more
you can also split it up so you can
start doing it on multiple machines or
multiple threads all that stuff works
very nicely to avoid contention examples
product catalog if I'm trying to look up
a product there or customer records web
sessions which are sticky no SQL things
say it's key value store memcache for
instance there's no way of iterating
through the keys you can only look up a
key and that's it you can put mine in
look it up you can't actually go and
find out all the keys so that's the most
common form of thing actually the whole
world wide web is like this one the key
is just called the URL do you need
secondary non primary key access do you
need to look up by something other than
the key so if you need to do that you
need secondary indexes that will you may
hang on a secondary database index like
that though trying to find parts of a
record do you need to do things on
metadata like when it happened at cetera
rather than the key value itself author
whatever maybe you need to go to
full-text search this is significantly
more work you have to decide whether you
really need this one as an example
I said primary key access on the web is
URL non primary key axis is Google
they're quite different one has made a
business that that's that's a business
the other one isn't it's usually slower
this is an interesting one and this is
one that often people forget about is
what do you need in terms of bulk access
or range scans if you go well actually
look I'm trying to find I'm doing a data
logging application I want to see the
value of this or the engine temperature
over a particular range or whatever I
probably want to go from this time to
this time I'd not I want to go and get a
single point I want to go and get a
range of points so there's some implied
ordering about this one and that will be
because you're ordering on time so other
things with hashes you don't get that if
you try to use a hash you won't get this
ordering if you try to hash the key for
time that's spread out everywhere so
you have to know whether you need this
one because if you're paying for
ordering when you don't need it it's
slower you have to go or this is going
to be order log n rather than order one
curses traversal of state etc you may
find it slow to get somewhere to do the
seek and then you're pulling up a whole
part of things as you go Hadoop for
instance the Big Data stuff is he's a
brain scan based view you get this nice
dense linear access watch out for even
write amplification at this point
although bulk access very important if
you don't do this then you will end up
going slow as an example if you have a
REST API it doesn't do this because you
want to go all on the go and get this
one and then I'm gonna get the next
element and then we're gonna get the
next element and then the next element
like this you're going backwards and
forwards rather than one call go and get
me everything work out whether you need
this one same with it this is sometimes
called the n plus one problem reach a
right ratio very important to understand
how much you do with this one here if
you have a high read thing then you have
caches caches work really well but then
you have to make some decisions around
what do I do about cache right through
and write back and cache eviction how
big is the cache etc how do I keep
multiple ones coherent and I'm probably
gonna have some indexing structures
that's great but maybe you're operating
in a high right environment you're doing
data logging I want to go and get all
the values for I'm monitoring a chemical
plant or a ship or an airplane or
something about lots of data coming in
or maybe just log files out of my
application out of a bunch of service
caches don't help me when I'm doing that
except they probably help me to catch
the indexing structure I had one client
that said we have an 8 core machine and
it goes about one one-and-a-half course
speed they were doing process control
and data logging I said well that's not
really surprising because you are
locking everything all at once you've
got one big lock in front of everything
that says right lock all the data
structures and everything else update
its one value and unlock it surprise
surprises you've see realized your
application I said look you're indexing
structure is essentially read-only so
your metadata is often read heavy you're
doing 10,000 1,000 or 10,000 points
coming in per second and once every 10
minutes you might change the indexing
structure
it's like five or six orders of
magnitude difference there why are you
making the point zero zero one percent
case slowdown the 99.9% case why don't
use a reader/writer lock and so that all
the readers can carry on in parallel and
do all their writing so you have you
have you split up the writers for the
different parts because they're all
writing in different places but you have
multiple you only have one lock on the
metadata in fact the best way they could
have done that was use Atomics at the
bottom and and one locker on the
metadata but it's important to
understand that even once you've
understood how big that is
the next thing to understand is now what
is your working set size how much data
are you actually dealing with on a
regular basis how much of that is going
to fit in main memory does it fit on a
single machine does it fit into your l1
cache how do you fit those ones maybe
you need your indexing structures in
memory but not on disk you say look I'm
gonna have my indexing structure in
memory and then make one call to disk
maybe you do that as an example if I'm
doing so a news website like this yeah
everybody's looking at today's news like
this what happened last year where are
whatever a few access into this if
you're doing logging monitoring maybe
you're saying well actually I'm trying
to work out what's happening right now
and occasionally I need to find out what
happened to this machine last year it's
very much biased to this end and you may
find look that fits in the cache even
though I've got a lot of data the
accesses are all one place if however I
go to the airport then you probably find
that there's a few people here with some
frequent fliers and some crew who have
lots of access to the passport most
people fly once or twice a year so
actually this is a very long flat tail
and suddenly this is now loaded disk
accesses because I can't fit all of the
passports into memory here you have to
understand and that is exactly the same
thing
go and look this one up but know the
data base distribution and the last one
that people often forget is about
consistency of data how many copies of
that data do you have around how far out
of synchronization can they be if I've
got something on my screen here does it
have to be exactly the same as that does
it is ten minutes delay okay overnight I
don't know if for instance you're trying
to sell the last seat on the airline
well you need to have one copy of that
and everybody comes into that that's the
nasod sort of transaction you have to do
with but you've limited your scalability
if you're saying well actually look the
product catalog changes once a day I can
then distribute that that nice and
easily without that so understand
whether you need that your reference
data your metadata for instance may well
be less consistent than the actual
transaction data so back to you the
performance story we start off it's too
slow we're profiling we're trying to
understand where those where we're
spending our time first of all find the
unnecessary waste get rid of it don't
optimize it just get rid of it it's not
helping you at this point you may well
find that you've now gotten your memory
allocation so we've seen how that malloc
and copying of those other things can
can cause problems and temporaries so we
can use move semantics and other things
to get rid of this one we can pre
allocate our memory like an effector
reserve and those other things to try
and do that one we've worked out how
getting that closer in memory works
better for us we've seen how using we
make choices between order one order and
order login all those sort of kind of
things like that that's actually the
real part of where you have to know
about what's going on here this is your
operational profile that says I know
that these things are only this big I
can do it in this area I my data items
are only three bytes long or whatever
you can use those things to your
advantage it's mostly this one here it
has this sort of distribution that's
where you get this one at that point and
only then is it worth saying look if I
really need to still go fast and now I
can start working out how to make it fit
with my cash and hot/cold separation
those kind of things so here are just
some some of those ideas just put down
or so sort of kind of an overview notice
that these really are things which have
tens of thousands of cycles and then
thousands and hundreds so it's obviously
better to try and sort the ones that are
three orders of magnitude before these
ones here don't
start here I often see people going yeah
yeah we need to kind of work out these
kind of kind of stuff is a look not is
it doesn't matter what I have a pointer
or value there don't spend your time
here if you're wasting time here start
here and go left to right
measure and go down that's really what
this one's about if you anyone has any
questions about any of those ones that's
fine if you want a copy these slides I'm
happy to give those to you I think on
the whole it's a multi-level problem
it's not a set of tweaks it isn't Oh
shall I put the plus plus before or
afterwards
it's a engineering measurement
fact-based approach to optimization
because you know what you're trying to
achieve and you know when it's good
enough and you know what properties you
can rely on that's your domain knowledge
and their domain no optimizations are
going to beat any sort of kind of clever
algorithms that you can come up with use
your tools understand how what how they
work and what they telling you and
that's really how you're gonna get
performance and optimization it's not
just by trying to work out the latest
flashy toy they may see okay thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>