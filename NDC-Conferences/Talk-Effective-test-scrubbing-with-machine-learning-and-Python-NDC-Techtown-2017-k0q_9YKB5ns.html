<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talk:  Effective test scrubbing with machine learning and Python - NDC Techtown 2017 | Coder Coacher - Coaching Coders</title><meta content="Talk:  Effective test scrubbing with machine learning and Python - NDC Techtown 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talk:  Effective test scrubbing with machine learning and Python - NDC Techtown 2017</b></h2><h5 class="post__date">2017-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/k0q_9YKB5ns" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you and welcome to this white area
very wide but I think I will handle
anyway okay thank you for coming and
listening to us this is a report from a
project we had we have had for last ten
months which goes about scrubbing test
logs or test test test results which is
a big problem for us so we'll go
throughout and see how we have a handle
not this is a protectionists in three
parts first I will do some motivation
about how we how we come to do this and
the reasons for it and then Thomas here
he will he will do an online
demonstration of how this is actually
can be done live with no cheating I'm at
last we had a call here he is from
circus which is our which is which we
are doing a lot of cooperation with in
in Cisco for when it comes to testing
stuff and software validation so he's a
PhD student so pH D student to be nice
to him he will tell us about why this is
very dangerous to do or why I'm gonna
give you some recommendations so first
first we start with the reasons for
doing it doing this well what's the big
deal here everything starts out with the
continuous integration we are doing a
lot of testing Cisco we have a huge code
base lots of people working on it maybe
over over 100 check-ins every day and
all this has to be tested so it's not so
testing all the time only because we do
a lot of that thing we get the fast
feedback and big 12 feedback we get
faster development so it's a lot of good
things by testing a lot so typically the
developer commits key code to build it
be tested and a report is coming back
and this cycle here is for the first
gate we have is could be down to 10
minutes and that with real system
testing
so situation here is a little not that's
funny really the truth is we have 20,000
tests system tests running every day
integration tests running our real stuff
it's real we are doing a video
conference system so it's talking about
real calls with the real system in real
audio with real video and a regal
network and everything so it's a pretty
big lab we have to be able to do this
but unfortunately since we're running
already stuff it's actually there are
some percentage failing with a good day
it might be 3% on a bad day well it
might be five or six or seven it depends
on what's going on so well you can do
the math teach the 5% on 20,000 is still
a lot to work so it means we have
several hundred tests that we have to
take a look at so I think I just go on
further here so the problem here is we
want to find unknown stuff the new stuff
that's coming in
that's quite a kind of hard it's easier
to find then stuff for you when you
already know know know about and handle
the rest us new and unknown it's quite
obvious but it's just to make stated
clear that that's what you're looking
for here is things we already know about
so we keep doing things we don't we have
seen before
so analyzing test failures this a yeah
it's a big job and it's become bigger
and bigger as as long as we are adding
more tests all the time and checking in
work code so the sam'l pattern is of
course writing some regular expressions
and scan all the logs coming in and
detect that though here you have a
pattern that matches this bug this type
of problem and so on we'll be doing that
with quite success so we have a system
for that and also we have automatic
crash detection administer if a systems
our system crashes it will send a report
and we will analyze it them categorized
debug
so but the problem is still lots of
manual inspection needed here we have
lots of many lots of unscrewed bugs or
Oscar Bruns lying around and that's not
that's not a good thing because then
there is a danger of losing stuff or
things are sleep asleep sleeping through
for the two to long time and that thing
problems into the customers so so the
goal here is actually reduce manual
inspection not that Andres passion is
that is not a good thing to do you have
to do it but what is nothing funny to do
is to do take took it look at the same
problem all over again
that's what boring stuff and in fact you
don't get people to do that work or
skilled people to do that work over over
again yeah try that didn't work out so
we need to figure out another way to do
it so it would be great if we could
spend time with just known things or
unknown things that things like brings
us something new so that's what we set
out to do so just give an example here
this is a lot of tests test results
becoming in societies that we have
20,000 of these coming in every day some
of them are passing some are failing the
poster on fear well we don't care so
much about them if you have lots of
spare time you may go in and take it
take a look at them if they are really
passing but the truth is that they are
typically put into a box of post tests
but still there are lots of failed tests
here many hundreds and they are failing
in different ways so it's not enough
will be remitted out on a test that's
passed on failed failed is much more
interesting thing so we have different
failures so in this case there are five
different five different failures this
is for example for one test the typical
test
and they thought that it could happen to
have five different failures at the same
time so two of them is red here however
a yellow and six or black yeah we can
just categorize them and put them into
here some buckets so it'd be nice to do
it do it like this
take them away we know about them get to
get them out of the way but there are
more so I'm in the Dom analyzing our
test logs feel son end up with lots of
buckets so but the interesting one here
is the things we don't know about that's
the unknown problem three left the other
ones we have we know about the cost we
have and I use them I unanalyzed that
group before and we know it's that kind
of problem so the job is boiled down to
inspect one instance of the unknown
problem but buckets so if you analyze
that mom you're done that reduces the
work a lot so you might I ask her why
why why we are running so many failed
tests again go all over again would be
good to just you stop testing when it's
failing well we don't see it that way
you know if you stop testing you have to
know when to start it again and that's
that's of job as well and if you stop
and also another thing some tests are
wobbly or flaky tests that we call them
they fail now and then and also that's
most the most interesting works all
those kinds of works that are not steady
red or steady green you know so so we
had to be like to keep them going
unless they are too resource expensive
of course so but a solution here to do
to do this is to use the logs test logs
to group into similar runs so the idea
was to extract the features from from
the logs and use the shield learning
techniques to group on two together so
we can classify them two together
the sumption Arif here is that this was
hold of course that's we have grouping
things together yeah they are the same
so that depends on of course if you have
no signs of failure in your logs you
know if the past locust and a failed log
looks the same
well then you're stuck so but that's
usually not the case so the savings here
is you can it's enough to just check one
instance of each group make sure that
that's true and we have identified those
those problems that means either fix it
or report a bug or you know run it again
or yeah get all of people to fix it
so the only problems we just expected
inspect one one of them or probably you
will do that after you more to make sure
that the grouping is working so about
the test logs because that's a clue here
I'll look at transfer test logs in my
life I guess most of you have as well
but typically they are typically
readable it's normally have a start and
a stop and some middle where things are
failing or passing or well thanks but
there are some interesting things here
they have some runtime words coming in
they're like timestamps it could be
hashes it could be IP addresses it could
be whatever just my system names and so
on and also the sequence might vary a
little bit so especially with doing
system testing you know timings my bit
might be different so you cannot you
cannot do diff on logs doesn't work so
that's what we want to to figure out how
to do and now it's time to for Thomas
here to try to do a little demonstration
here it's not exactly how how we do it
but it seemed very similar
Thanks
good evening I'll now run through a
small or a brief demonstration of sort
of trying to illustrate trying to
illustrate trying to illustrate sort of
or to give you an idea of what we've
been doing and also to illustrate to you
sort of the the load threshold that
exists currently for getting started
with doing machine learning stuff so
what I will be showing you today is me
using a python library called
scikit-learn which is in machine
learning library and I'll be coding in a
tool called Jupiter notebook so having
any of you heard of Jupiter notebook
yeah so it's a as far as I my knowledge
it's really really great to to do
iterative machine learning stuff and as
you'll hopefully get a feel of how
hopefully at the end of the
demonstration you'll get the feel of how
how easy it is to use the scikit-learn
psychic library as well as how fun it is
to work in the jupiter notebook so just
open my Jupiter notepad session Jupiter
nope it runs in your browser or it runs
as a server and you open it in your
browser so you enter patent code
snippets into these cells now this is my
first cell I've had some code it's not
really that interesting it's essentially
just me importing stuff from the socket
live
that I need for this demonstration as
well as some utility functions that I'll
use later on so returning to the problem
posed by Mario's essentially we want to
take or essentially what we want to do
is extract more information from our
logs than the meta data provides so the
sceptre will go through is essentially
taking or royal text logs doing some
pre-processing on them converting them
into a format that you can apply machine
learning to as well as applying a
machine learning algorithm sort of
illustrating the output of that
algorithm so the first thing we need to
do is to get our looks
I've prepared a small data set to
illustrate the first portion of my talk
hopefully that is correct
yeah so it's small data set of five
looks now beginning with machine
learning you the first thing you want to
do you have have an idea what exactly
the data you're you're working with it's
all about what's contained in your data
so we'll have a look at the first log or
one of the logs so here you can see it's
a contrived test example containing like
typical things you find in yeah you know
test logs its time stamps the debug info
level as well as debug your name and the
output of the debugger so since our task
is to look at similar this between logs
the time stamp is an issue since it's
essentially something that at least in
our scenario it will differ every single
time you run the test meaning that you
get it acts as a noise element for the
machine learning algorithms that we're
going to apply later so one thing you
can do in order to improve your results
is to remove those elements from your
logs so we'll do that quite quickly
using simple regular expression
oops I forgot I want to apply to all the
lines not just the first so there you go
so now we were just removed noisy
elements now it's a good practice I
think to keep token that represents the
elements that you removed because just
removing the elements essentially means
or potentially means that you're
actually just removing information but
keeping the token you'll retain at least
some of the information you'll for
example in this case probably a better
name for this token is timestamp or
something so essentially it's it acts it
has two purposes one is to retain the
information that there is a timestamp
there but it will be the same across all
logs and also for debugging purposes
because mostly most likely if you're
working creating regular expressions and
rising your input yo at some point have
multiple regular expressions and it's
nice thing to know which regex
is causing your output to be strange or
malformed so well now you have a
hopefully you have an idea of like one
thing you could do to improve your take
your raw input and improve upon it a bit
and reduce noise so now the next logical
step is to convert your raw text data
into vector format which contains
numerical values that sort of represent
the signature of your text file so here
comes our scikit-learn library so I
could learn provides several methods you
can use for this but we'll use
a method called count vectorizer
essentially it can take our our list of
logs and what it will do is that it'll
it'll outfit the vector sequence after
its applied a vector vector sizing
scheme on our raw logs
so since I'm mute I'm just using the
default scheme which is take all the
words of all the logs and then for each
log create a vector containing columns
representing those words and the value
of those columns will be the number of
occurrences of those words in that log
so to list it further I'll show you the
output of one of the looks so here you
can see the signature generated by the
count vectorizer it's just each element
is a word and the number is the number
of words in that log another thing you
can another interesting thing to further
highlight what count vectorizer is doing
is that vectorizer keeps track of all
the words that it finds across all logs
they call which it refers to as features
so as you can see these are all the
words across all the five logs now since
I'm using the raw logs I'm getting these
timestamps as words and these are as I
hinted at earlier these are essentially
noise because algorithms will apply
later rules will see these as well
probably you'll have one word that is
very unique for each log and algorithms
will
usually say oh that looks very
interesting and assign it more
importance than the other words that are
the words we are really interested in so
if we do if we instead apply all the the
previous step with the tokenization step
to all our logs this is just a utility
function that does that before I send
the logs into the electro sir you'll see
that we end up not with those numbers
but with token instead so using just
words is probably the simplest type of
creating those signatures and using only
words actually all also takes away some
of the information embedded in the logs
for example the word ordering like which
words follow other words so perhaps the
logical next step would be to use or a
way to extract more information from our
logs would be to combine to use
combinations of words instead of just
single words so in machine learning this
is referred to as engrams
I'll just instruct the underneath in the
the wrong place so just instruct the
vectorizer to use pairs of words instead
yes it's not you can see it the features
extracts is now just word pairs
yeah and essentially you can create with
the vector I so you can create more
words or combination of word pairs and
word triplets or whatever you want you
have to experiment to find out what
suits your needs the best yeah so now
hopefully you have an idea of how you
convert your text talks into signatures
or at least some of the ways you can
create signatures from your logs now
I'll move on to using more realistic
data set which is taken from our test
environments it's also a bit more
there's a lot more data so it's also a
bit more interesting to look at the
output
the the let's just take run the new
looks at through the contactor i sir as
you see will get a lot more features
because it's a more realistic data set
so what I want to do next is to in order
for you to have a more intuitive
understanding of the data and how
exactly we are comparing them and sort
of saying that they are similar is to
plot the the vectorized data now as you
might see our question is that we have
891 features which essentially 819
columns which is 891 dimensions so you
can't really plot that directly so for
for the purpose of this talk we're just
applying a lossy algorithm that projects
our vectorize data down to two planes so
we'll just use my utility function here
and take our vector sequence and
products
yeah there you go so this anyway it's
clear to everyone yeah
so you see each dot on this plot is a
log and probably you have some intuition
already about which logs are similar
which looks maybe should be placed
together inside a single group or as
related to our problem like category or
specific problem but essentially like
what we're trying to what we're trying
to solve is that you shouldn't be
shouldn't be doing this you shouldn't be
looking at looks at all so now we'll
apply one of many possible clustering
algorithms
yeah
yeah this clustering algorithm outputs
based on the the the log vectors it
outputs a new array with of numbers
where each element is a log and the
number of the element is group assigned
to that log by the algorithm so and
negative one our logs that are not
assigned to any group so we return to
our plot again and now we use the new
information that we got from our
algorithm and use it to colorize our our
data points we want to use a full range
of colors yeah so here we go now we have
a plot combining or vectors as well as
or the output of our clustering
algorithm as you can see the negative
ones are colored in red so they're not
in any group see we have groups here
here and here and it looks rather sane
so at this point we have a functioning
machine learning pipeline which you can
experiment with it's a simplified
perhaps but it's this is all it takes
good to go from raw text logs to
something that you can at least play
around with and see exactly like how
much information you can extract from
your data so the probably after this
you'll want to experiment a bit with
input parameters of your different
different different classes I just used
so for example say that it's your
opinion that
these this cluster should not be two
different clusters and you'll just
you'll quickly find the input parameter
for this clustering algorithm and you
can adjust it increasing the size of the
the groups and you can play around with
it's almost infinitely if you want it
can be a lot of fun seeing how changing
stuff in the pipeline changes the output
so now Karl cotton is going to tell you
a little bit about why I shouldn't
probably spend not too much time doing
that hello does the microphone work
great excellent so now that marius has
introduced you to to the general problem
we're trying to solve and Thomas has
shown you easy it is to get started with
with modern Python Python to do this
kind of data science I want to take a
step back and reflect a bit more broadly
and give you some some recommendations
that we've based on the experience we've
had so far if you want to start out with
a similar project so let's look at the
clustering that Thomas's jupiter
notebook resulted in now as humans we
can look at an image like that and
immediately see patterns and then sort
of assess well how how well does the
coloring this is just a coloring map
onto our intuitive idea of a clustering
all right but but it's it's a bit
unclear if we can trust our intuitions
here I mean we have we've told you very
little about the actual data we're
clustering and it's and even though we
can kind of define and give mathematical
notions for what a good clustering is
for example saying that in a good
clustering the points within the cluster
are close together but the points who
are in separate clusters are far from
each other it's it's not clear how this
relates back to our data so
if this was a ideal AI or standard
machine learning task or the first type
of machine learning task that you're
likely to encounter if you take a
machine learning course the dream would
be that we already had a data where had
some data where for each data point we
already knew which error had correspond
to and then we would take that data set
we would split it into a training set
and a test set and we would use first
training set to build the classifier
that would internalize some statistical
internal statistical representation that
could be used to to classify new
incoming data and then we would we would
take the other part of our data set and
we would test it and make sure that that
the system actually has is accurate in
how it classifies the errors but in our
problem we we can't really do that so we
could take a lot of historical data and
sit down and tediously label all of it
but apart from being prohibitively cost
or cost intensive it would it would
quickly expire because the we are
testing systems that are evolving
continuously and the definition of an
error will our new errors will occur all
the time and and exactly the direction
all there might change so what we are
left with is what's called an
unsupervised learning problem so where
we are forced to create a machine
learning system by generalizing from the
internal structure of the data without
any knowledge of any ground truth so if
there's an O ground truth here can we
even do anything here well it's pretty
important then that you need to develop
some kind of safety nets right if you're
starting out with a project like this
and the kind of safety that I'm talking
about has both human dimension and a
technical image right so on the human
dimension it's clear all the systems
we're developing are solving some kind
of human problem right with human
stakeholders
and even if maybe it's your boss maybe
it's your customer it's at any rate very
important to clarify expectations about
what this system can and cannot do I
mean I'm sure you all see the newest
Gartner hype cycle report this is
basically a yearly report that tracks
technology trends to see what are
up-and-coming trends and what's
currently at the top of the hype so
guess what at the very very peak deep
learning and machine learning right so
every day you're bombarded with articles
about the about the wonders of machine
learning but the hard reality is that
create good to simply leverage these
techniques is not easy and it's very
easy to over promise to stakeholders
what the system can do so be very
careful about expectations especially
when it comes to expectations about risk
so I know that many of you here work on
embedded very very safety critical
systems and if you're trying to apply
machine learning to a problem where a
Mis classification is faithful then you
simply can't do it you have to be very
you have to ask yourself some hard
questions about ok what's what's the
consequence of a mislabelling right and
if that consequence is prohibitively
high you have to look other places but
if you have problems where if it works
most of the time it's ok then then
that's the kind of problem you should
you should use machine learning for so
make sure before you start out that
everyone's on the same page everyone
agrees on what this can and cannot do
and that everyone's properly informed
but so that's kind of the human part
right but then there's also technical
things you can do to kind of build
yourself a safety net even though you
don't have a ground truth and I want to
give you an example that we could apply
to the problem that we just showed you
so we found that after applying the
tokenization step that that Marius and
that Thomas demonstrated where we took
all the runtimes specific information in
the tests and remove them we found that
many of the tests actually get identical
chasms like if you like many many logs
that were
are identical like you take their
assassin and they have the same chasm so
one very reasonable assumption then
right is that if our clustering is good
at least all the things that are all the
documents that have there the same char
should be should be put in the same
cluster right now that's obviously not a
sufficient condition right but it's a
kind of necessary condition that your
clustering should satisfy right so if
you can identify necessary conditions
like these and incorporate them into
tests you can gradually grow a safety
net that will help you sanity check
different solutions because it's all
code after all and so all of you here
are experienced engineers you know about
testing you know about best practices
and like the previous speaker he also
said machine learning systems are also
software systems so just bring in all
that experience you already have and and
and and don't throw it out the window
when you when you approach machine
learning project
speaking of code let's go back to the
code that Thomas demonstrated it's
pretty cool I think that you can you can
express the entire pipeline in one slide
right so decades of machine learning
research is now available to everyone at
their fingertips at very convenient
high-level abstract abstract libraries
and this is great but with these very
convenient libraries it's very tempting
to treat it all as a black box right
there is just as long as it works I
don't have to dig it dig deeper into
what that into what's going on behind
the scenes the problem is if you're
doing unsupervised learning you have no
choice but to really or if you I think
you will find you will find yourself in
situations where you have to crack open
this black box so let's because this
code hides a lot of complexity I mean if
we just look at the account vectorizer
function that Thomas demonstrated and we
look at the documentation there's a lot
of parameters going on here I mean just
just count vectorizer takes seven
categorical parameters three numeric
parameters two parameters accepting any
iterable one parameter accepting a
numeric range and four parameters except
the inacol
and if we look at the at the clustering
algorithm you're applying and we look at
the documentation for that there's even
more so one can be one can be a bit
intimidating one one starts when one
starts to open this black box as if it's
impossible for a a practitioner or
layman to understand what's going on but
don't worry too much because all all
these things all these parameters have
sane defaults if you understand the
intended use case and so so usually
there will be an intended use case for
whether the data is assumed to be of a
certain shape and so on and if you
understand that use case then the
Paredes the parameters fall into place
and don't be afraid to and the tutorials
are actually really really nice they
have very relatable relatable examples
an ample of pointers to help you dig
deeper if you really need to understand
what's going on
so so open the black box black box but
start by identifying the intended use
case and then you'll get pretty far and
lastly remember to measure discuss and
iterate so consider Netflix for example
they're in a great situation when it
comes to machine learning because they
have very precise metrics about the
overall goal of their system right so
the more you and more time you and I
sink into TV shows the more money
netflix is making so any machine
learning system that optimizes that goal
is helping them succeed so maybe you
have similar high level metrics for the
problem you're trying to solve and if
you can identify those and quantify them
then you have an additional precise
safety net to help you test and verify
your system similarly I know most you
come from very from some sophisticated
technological organizations with lots of
experienced personnel so get everyone in
board and and and and ask and discuss
your problem with your colleagues and
try to get a competing perspective on
what you're trying to do with the
problem around and so forth and be
prepared to iterate a lot because when
you're doing unsupervised learning and
your your learning machine learning
while you're learning the problem you're
trying to solve and so on it's gonna be
a lot of trial and error so don't expect
to get it right on the first go
so to summarize some of the some where
our recommendations clarify expectations
with stakeholders and and everyone write
tests cut this code after all crack open
the black boxes and measure discussing
iterate because to try to end on a
positive note here you're all
experienced engineers you've all done
testing you've all built working
software systems and just just bring
that into into the machine learning
adventure and remember to do machining
machine learning like the great engineer
you are not like the great machine
learning expert you want so thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>