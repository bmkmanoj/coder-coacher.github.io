<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data + Docker = Disconbobulating? - Stephanie Locke | Coder Coacher - Coaching Coders</title><meta content="Data + Docker = Disconbobulating? - Stephanie Locke - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data + Docker = Disconbobulating? - Stephanie Locke</b></h2><h5 class="post__date">2017-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oSr2VRxromk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">then i get started i said earlier to
about half a room if you're already a
docker with this talk is not to you you
can go and see something else and get
more value probably out of that talk but
today we're going to talk about how you
can persist data when using docker and
kind of what the architectural options
are about that okay so first a little
bit about me I'm just launching my own
data science consultancy so scared and
I'm a regular presenter I've managed to
present in lots of places that all have
me so and they keep inviting me back
that's pretty nifty microsoft states
platform MVP I run user groups I run
conferences I've got battle as beards
coming up at the end of March and that's
a beauty tech conference you got to have
a beard to present it doesn't have to be
real though and we're doing that in
awareness of male suicide you can get
the slides and all the code and
everything on get up so you can go to my
github and is a retail I'll treat it as
well and my websites up there okay so
we're going to talk about today first of
all some scene setting going to cover
the Val hopefully you all know the value
of data otherwise you wouldn't be here
wanting to find out about how you can
make it work with darker but we'll cover
it I'll give a quick rundown of docker
who is who has used docker okay who has
it in production just the one of you
okay we'll look at how you persist data
we'll do some damage we'll see where the
pitfalls are and then we'll look at some
of the architectural decisions that you
need to think about okay never delete
never surrender I love data I want as a
data scientist I want as much of it to
be there when I need it as possible I
hate things like 90 day retention of
data because that keeps you storage
costs down I'd much rather have the data
for longer so I would rather not get rid
of it and if you put your data in Dhaka
and it just lives in the container then
it's probably going to go away and I
don't like that situation I also think
that when you're working with data
capturing it in an application
processing it or presenting it you need
to think about what the what the data is
now what its shortcomings are and how it
could change in future when you make
changes think about the whole process I
had a developer who was trying to get
his job done quite rightly and he had to
have the person who put through a
payment and the person who the payment
was put through on behalf of we had a
load of reporting and reconciliation
processes or relying on this data and
what he did to get his job done quickly
because he didn't like working with the
database he concatenated the two
usernames was a pipe in between them and
stuck them in the one database field so
he didn't have to make any database
changes yay no schema changes but now
everything else broke so I like to think
that because I I do some application
building as well and I I see the end to
end I really feel the value of thinking
about what your data should look like
and how you change it with you care I'm
currently battling an issue where I've
got more than 4 billion records in and a
gia table and it turned out when my
intern did an import for a grid sort of
look up he inserted some carriage
returns and so now a lot of my data has
carriage returns in it across for Millie
four billion records which and trying to
correct that is is hell it's been a
couple of weeks also been doing it so
the quality and no schemer very
important the types of data that are
kind of in scope here for whilst they're
talking about docker its configuration
data things like host names passwords
all the things that you might need to
provide your application and database to
get started so when you initialize
something got reference data things that
populate lookups customer details the
basic things like you might have a table
with days of the week these are things
that you need to think about when you
build a new docker container and that
you need to maintain over time we're not
going to add a new day of a week but if
previously you just have male female as
your genders in the lookup and you
wanted to add more because we're no
longer in a binary world then changing
that in lots of different containers
could be rather complicated telemetry
everybody got application logging so you
know what who's hitting 404 errors and
what pages and stuff no okay i'm going
to ask a question that hopefully
everybody will raise their hand to
everybody using source control okay good
okay I feel better when everybody raises
their hand okay so telemetry is
important especially with something like
dhaka where you might have lots of
different containers and this works for
like web servers and stuff generally
people are having different parts of
your infrastructure then you want to
know if any of those individual parts
are experiencing problems because you
can't see everything you're not just
sitting there with a single server under
your desk that you can log into and
check it's not that easy anymore so
building in some logging all that
logging have to go somewhere because
it's no point just generating it and
never storing it and then of course is
transactional data so if you've got your
building a Peter of the service delivery
site and it's multi-tenant so each
company can have their own pizza
delivery front end they're also going to
have their own back-end data and you
need to store that and you need it to be
resilient it can't just go away because
you know for computer turns off or the
doctor container turns off and it goes
away then you've disrupted their
business it's a big problem okay so some
of the challenges that you're going to
face when working with data is
refreshing it so that's the sort of the
reference data scenario or your config
what happens when you need to change
that information and it's in especially
if it's in numerous places scaling
access is another thing so Stack
Overflow they've got what to do I Norma
stata basis right ones their primary
ones for the event of a disaster they
get a phenomenal amount of throughput
through that but it requires some very
talented people to do it you've also got
the consideration that you might have
a bratz of access patterns from
different people with different roles
and responsibilities so giving people
the right information at scale is also
something that you need to think about
safe against disaster so BR is hard you
know that just having two servers in the
same data center not so good having them
across different data centers has
challenges like making sure that the
data get there and stays there so in a
financial services scenario or like in a
stock trading scenario making sure
things run quickly enough in terms of
getting that data onto your dr and saved
that's a really hard challenge and then
security so i did a lot of my thinking
around darker working for a company that
does multi-tenant software-as-a-service
stuff so we've got customers who don't
want other customers to be able to see
their data they don't want it to be
hacked obviously nobody wants their data
to be hacked but if we had a different
database for every customer then at
least if one customer was breached and
we weren't we using passwords cuz we
don't reuse passwords we're using
passwords is bad but if each person had
a separate database then if one was
breached all our other customers would
be safe if everything's in one database
then it's one pot just waiting to be
lost okay so that's my proselytizing on
data done you can consider yourself
thoroughly chastised and knowledgeable
about making schema changes with care
and never deleting never surrender so on
to darker okay who had who hasn't used
doctor at all okay so it's basically
stripped down mini the ends without the
offer
seeing system for you to particularly
worried about it's really handy for
people like me I break a lot of things
so docker allows me to contain my
experiments in a safe way but it also
allows people to deploy stuff deploy
their applications with all the
supporting kind of the infrastructure
gubbins in place yes so it's really good
to use docker if you don't necessarily
have a big team if you're the person who
have to worry about the sysadmin stuff
and the dead stuff because you can put
it all in one kind of big scripted file
you know you can have puppet deploying
your documentid to when your docker
containers being pushed onto it with all
the ports and all the networking all in
there and it's a very simple script it
takes a lot of the time of doing repeat
work away from you and the automation is
pretty nifty dr. is moving so fast you
know that they make phenomenal strides
in what they're doing every time that's
why you should use it but what you
shouldn't use it is for many of the same
reasons so controlling your ops your app
infrastructure stuff I'm not very good
at it I don't want to have to do it so
in some respect when I have a docker
container and I have to plumb in ports
and think about engines and engine X or
however it's pronounced and worried
about firewall bits and pieces that's
when I start getting into trouble so
controlling your art has developers and
you kind of go while I'm going to do it
all on my doctor containers that can be
a little bit of a problem if you don't
have the experience necessary to do it
well and it's rapidly developing so
this is how somebody described it to me
they use this gift exactly of we've got
bugs with this docker version so let's
move to the new version now I have new
bugs I better go back to the old one
they're moving really fast and and as a
result they're kind of breaking stuff
you have to have a bit of patience or
darker if you want to get everything
working with the latest versions it's
kind of a hypnotizing gift for me okay
so running it locally it's a command
line so here we're just doing docker run
so we're going to run a container with
passing some arguments to allow us to do
it interactively we're going to run a
bunch of container so I'm doing this on
Windows and I'm running and a bunt
aversion and I'm going to access bash
which means and especially because I've
not risked hurting on hypervisor and
running containers on my Windows 10
machine yet or running the native bash
that you can now do that I can cut my
teeth on my linux admin skills without
breaking my machine or without breaking
a vm because i have done that before the
difference the way you break a linux the
end is apparently sudo chmod 777 minus
our /i meant fearful stop but the false
lashes right next to us right so what we
can do is I made videos of this because
the last time I did this presentation
and my computer died I ended up with a
guy on stage who had to open Chrome and
everything with Mac because I can't use
Mac
so it's all videos basically what we're
doing is as we run the command that we
saw we end up with an interactive shell
inside our container that's moving a bit
fluently what we do when we have our
interactive document interactive window
here I'm just running LS so lifting
what's in the directory and I get a
bunch of files when I come out and I run
the same on my machine I get a very
different set of directives so it's
completely separated from your operating
system
I use docker machine a lot darker
machine is kind of the the platform
which you could run docker on you need
to use it for when you want to be able
to remote have docketing containers on
remote machines but you still need to
interact with those remote machines and
when you want to build clusters of
machines clusters of containers that's
one important aspect that you need to be
able to install and then the other thing
is plugins so docker doesn't easily
allow complicated networks it doesn't
allow you to connect a lots of different
file systems by default this is where
plugins come in so you can install one
of these on your docking machine and
extend things and again this is how I
avoid breaking things in fact i'm just
going to show this so I very often
create a document on a dia it's as
simple as creating a machine on my
computer and in fact it's one of the
easiest ways I've ever found of spinning
up a virtual machine on Asia and
removing it then afterwards each one is
a single command line so I'm going to
make the docking machine and then I'm
going to use an external file system
plug-in and I'm going to configure it to
work with the azure file storage again
I'm not an infrastructure person so I
don't want to have to look after my own
storage I don't want to ever have to
worry about backups or dr or microsoft
to handle that for me so throughout all
of this when anything right it's going
to write to the expert to as your file
storage and then we'll map the volume
okay okay so creating a document is yet
another command line so it's just dhaka
machine creates and you give it some
information here i'm using just one big
compiled script because i like to
automate things but it goes away it
creates the machine and that's what all
this is doing it takes a couple of
minutes once it's provisioned and
installed docker then we need to install
the plug-in again those just the scripts
for that and then the final bit which we
do is mapping of volumes and that uses
document and just a command line and
we'll see that happen thankfully you
only need you vm once or twice set up
okay so with external volumes this is
how you get away from having all your
data in Dhaka inside the container
inside the bed that's tuned to die using
the command line you can work with these
volumes so it's just docker volume
create and you provided some bits and
pieces obviously your file system has to
have to have a name you can tell it what
driver to work with so if I didn't
provide it with as your file it would
just assume that it was creating a link
on the local machine and I'm giving it
an output name so that when I work with
docker i can say use this logs directory
then when i'm setting up a container
when i create it so earlier we had
docker run- IT Ubuntu in bash here we're
putting in an extra bet we're putting in
this minus C and here we're saying when
you create this use this volume so map
our logs connection to our file system
and make it available in the docker
container so that is all you need for
kind of connecting to that third rail of
making that data persist is very simply
map and external network drive to the
docker container when you create it
that's pretty simple of course there's
access like anything when you write to a
file system there's considerations ok
let me just show you what these scripts
look like
so this is one of the scripts can
everybody see it okay yeah and we're
just using the command line we need to
provide it the information to set up the
machine this is on as you the same you
can have exactly the same experience
with AWS digital ocean and a number of
other cloud providers because they
basically plugins in much the same way
that the azure file plugin is so it's
really easy to do this with the same
pretty much the same code on every cloud
provider and I do it on the cloud so I'm
less likely to break my machine we need
a file but we don't need a file we could
run the command but I have a shell
script which installs d as your file
plugin and configures it and then I do
the mapping of the network drives so
again all this code is on github you can
steal it play with it and very most
likely improve it yeah but that's not
really where any of the value is that's
just creating machines so let's do
something simple fist
okay so here we're doing dhaka run and
using one of the docker containers that
I built you can see the code at the top
and it basically just says write the
hosting to a file and that gets mapped
and we can see the hostname and check
and validate that they get it creates
the test file if it's not made and we
can see that it's written the hostname
and it made it so we're able to pick up
information from inside the docker
container and write it to the outside
world very very simple persistence but
usually you don't have one docker
container right because the whole point
of docker is you have swarms of
otherwise you could just have a vm so
what we had multiple containers if
they're writing to the same file what's
the problem it'd be locked yeah it takes
time to write to the file they can do it
so we can see with a file I never
container reading whilst they're doing
it so we set up some of our simple right
to going there restarting in the
background and always writing to the
file and you can see that we get these
weights when we do a tail that's because
it takes a while for the locks to be
released and for them to write the host
things it's working they're all writing
to the same file but you get delays
writing to the same file is bad how do
you get around that well you don't write
to the same file you have the hostname
if you are writing your logs from each
year docker containers write them to
files with the host name in
and the date you know it's a pretty
simple way of getting around locks it'll
even get around me because I won't know
what your host name is so in files
covers our config scenarios coke of a
reference data and it could cover your
telemetry the alternative is using like
an API connection and writing out your
telemetry via a socket or over the web
if we want to keep transactional data
and we probably want to keep it in a
database by database I don't just mean
relational database you might want to
keep it in a MongoDB container of 0
anybody using Mongo any of you using it
in public yeah got pwned by the public
access security issue good okay so when
we're doing databases it's another
simple command with darker here we're
going to mount another directory so
we're making our external file share on
adyar maps to where my sequel lives
we're doing some port mapping because
just because on my local machine I might
want to be using 3306 and giving it a
password you can generate random
passwords and you don't and you can also
use secret files as well here I'm just
doing a very very bad password just
because it's easier for me to type I'm
giving my container and named docker
will make means for you and they're
pretty cool names and that's good
especially because if you're taking the
mentality that you're
containers are just cattle they're
disposable then you probably don't want
to know them by name they'll make you
feel guilty when you kill them but here
I'm going to break that a little bit so
I'm give it a name and i'm going to say
use my sequel instance
so when it runs it's going to SAT up and
it's created all the files for me on the
file storage everything needed was
mapped so now i can connect into the
machine into the container and validate
that everything works and it does so
there are no issues sticking the data on
the idea file storage and because it's
on the audio file storage they've now
taken care of all my backups for me in
that I don't need backups for dr
purposes you might want backups for
somebody dropping important tables and
things but at least i'm protected
against my office get going on fire so
creating a database is pretty simple we
can attach to an existing one so let's
say you needed some additional contract
with your container that your database
was in you could create a new version of
your container with this new can take
related to it and just kill your old
database container and put a new one
over the top of it this is great for
when you need to make application
changes but you don't necessarily need
to make data changes
so here I'm going to kill my existing
docker container so I'm removing it and
then i'm using basically the same
command that i previously ran to set it
back up and it just works so all the
files because they were separated they
persisted and i was able to use them
again that's pretty handy if you want a
simple database but you want to be able
to make changes to the container or your
data stays safe but you can still
dispose of containers so you can keep
the data and kill all the containers and
save on your computer and then spin it
back up and attach it next month when
you want to run the reports or whatever
okay nice and simple it's still a
monolithic database they're not
inherently bad but some people don't
like more monolithic databases now what
if we wanted to run multiple databases
of the same file we get lock issues you
can't do it even if we could do it so
here I'm going to run my prove that my
first databases running and then I'm
going to try and set up my second one
when we do it we're going to get a lock
issue if multiple databases were writing
to the same files and you expect
especially if you expect acid you know
you want everything to be consistent if
you've got two containers running at the
same time can you guarantee the
consistency so 11 database set of files
multiple databases not a good idea
so what that's laughter swerve is we
know we can persist data it's pretty
darn easy even i could do it we've got
the mechanism for writing files we can
stick our database on an external drive
and put a container over the top of it
really easily so if we have very simple
requirements or we just want to write
out two files that pretty much all we
need that works for a lot of kind of
simple cases but we still have some
challenges so coming back to the four
things I said about data refresh we've
got issues with refreshing reference
data scaling the access being safe and
security with reference data refreshable
things you've got to think about what
scenario is right for you so if you have
if you just have some very simple files
that maybe have con take values and you
want to change say the version number
from your application from 1 to 1.1 what
you can do is you can have that file and
you can as part of your dacha compose
script reference that file and then just
go through and kill all your docket
containers and rebuild them it's pretty
simple to do but that might be
inadequate to your needs what happens if
you need lots of look at data you have
some solutions you can always connect to
a central bv so a canonical source of
those values but you do that then you
have to go potentially cross database
that reference values that might not be
close to your data
you might want to be able to bring it in
when you create the darker container
that's another option and that's pretty
good if you have a set of robust
processes for deploying docker
containers and then the other option is
sticking it in some form of cash and
just having that cash accessible to
scaling access well we saw that you can
have just one DB and we know that you
can have one DB that performs amazingly
so that's where performance tuning comes
in if your database is suffering from
access problems then you can check
hardware at it but performance tuning
goes a long way looking at those queries
building dedicated stored procs instead
of using entity framework for
particularly problematic front-end type
things not letting our ORM always write
the code so that's one way of facing
you're scaling challenge the other is
using something that's more distributed
again you should kind of think about
whether your data fits the pattern the
MongoDB document DB you know these
distributed document stores especially
they might come in handy for serving
more writes and reads than you currently
can and then sharding so especially in a
multi-tenant environment with people's
data you can fix them in different
databases and then have a central access
point for them and then have resources
for each database so especially I
reference out here a lot just because
it's easy for me to use the if you've
got customer a who only uses your
application once a month
and you've got a customer be who uses
your application once a day then they're
probably going to need different
resource requirements so if each person
has a sharded database that relates to
their data then you can give customer a
very small amount of access and resource
and compute power versus customer be
your good have a bit more and that means
you can also build them differently as
well if you want to so you make more
money out of customer be by charging
them more and make more money as a
customer a buy provisioning them less
resource being disaster safe so in some
respects if you had a cluster of
machines with a swarm of docker
containers on then if one machine goes
down then your other machines can pick
up the slack so you can set up think
it's worth CouchDB you can have a
self-healing database cluster that's
quite a complicated solution but it does
keep everything in Dhaka and so if
you're comfortable with darker and doing
everything with that clustering solution
then that can work you ought to think
about your backups and restores so maybe
not in the event of a physical disaster
even in the event of physical disaster
if you don't have a timely way of
getting to a copy of the data and
getting it back up and running then
you're going to lose a lot of money
whilst that's happening and then the
other option that I tend to always
prefer is let somebody else take care of
it because it's important in the event
of a disaster we get up and running
quickly but it's not worth my time to be
doing that that's what that's not where
the value is in my salary my value is
adding you
new offerings solving business
challenges not worrying about meteorite
striking and then security file
permissions are like the hardest thing
in the world I love having a sysadmin
who can take care of these things and
database permissions you know my usual
thing is give everybody read write
execute sysadmin just because it's
easier but obviously that's no good with
customer data so if we're taking an
approach and again i'm gearing towards
software as a service if you've got lots
of customers and you want their data to
be heavily lock down then you might need
some access controls or if it was in a
database you might need some row level
controls man is one way of going about
securing the data it can also be quite
complicated from my view the other
option is to actually physically
separate the databases so it comes from
the sharding option again or if you say
provisioned almost like little NT er
architectures for every customer so
every customer had their own file
directory with their database sitting in
it and their own app front end then you
can secure the file system against
access and that would be in many
respects much easier than trying to do
access control on every document in your
document database so that's something
you need to think about when you're
designing your persisted docker data
solution ok so we talked about the pure
instance databases it's pretty good you
know you can scale resources make more
money it's a bit of a pain when you want
to migrate schemas or applications and
stuff in the sense that you have to
think about progressively rolling out
changes and one of the biggest issues my
current employers face where they did
sort of halfway house multi-tenant
solution every customers data is in a
schema but then their big data database
fell over and can handle all the data so
they went they put a hundred schemas /
database instance they can't do any
cross customer reporting they can't say
how many users they have for every
customer because to do so would have to
loop through every schema in every
database that's a pretty tough challenge
so there will be limitations to what you
can do if you do a scale-out solution
though sequel databases so I use
document DB for this kind of thing
because again Microsoft handle all the
infrastructure for me but it's pretty
handy because you don't necessarily have
to worry about the schema over changes
so if you want to roll out a new version
of the application which writes an extra
field then they can just start doing
that and you don't need a database
change you don't need to add a column to
a big table that can be quite handy you
can't always get the transactional
consistency you might need the food
situation and then you're also come down
probably to securing each document with
some access controls so again that can
be quite complicated databases as a
service and I include them sequel
databases as a service you let somebody
else worry about it I like that you can
do whatever architecture pattern you
feel necessary in it
you can if you do a sort of like a
sharded solution you can still do all
that scaling and it actually makes it
easier to work out the cost of each
database and if you do this if you it's
often better to cite your containers
very close to your database because
obviously we all want our front end to
go with you fast so if we have our
databases of service out in the cloud
and we're serving our website through
docker containers locally then you've
got to worry about the latency between
the tip so it kind of restrict your
location options for daca and what you
do with this is you wouldn't have any
database containers you would just
connect you would have connection
strings to your databases so you would
cut out a need for some docker
containers as I said earlier you can do
some self-healing solutions this has the
virtue of control for you and control
can be a very important thing but then
you have control and with great power
comes great responsibility so you have
to worry about the infrastructure and
you could also a course put this on the
cloud you don't necessarily have to
manage the infrastructure and not many
people have done this I think possibly
that's the biggest risk of this solution
is your breaking you grant and in many
respects you want to be conventional
with how you look after your data
because that's safer
schema changes if you're doing databases
I recommend something like Flyway for
making changes you can of course go
schema-less and you can also use feature
flags does anybody using feature flags
in their applications so wait yeah cool
it's a pretty nifty idea probably takes
some trials and tribulations and getting
everybody into the workflow of doing it
but can be pretty handy okay docker are
working just acquired a company called
infinite and they're producing the
distributed file system which could take
some of this pain so that'll be an
interesting development in the next
possibly six to twelve months we'll see
I'm very much for stick my data away
from my docker containers using external
volumes and things this is an
interesting piece which says no you
should keep everything in docker
containers it is worth reading for that
extra opinion and I think I've run over
nope attentive awesome so we have time
for questions and because it's ten to
yes mm-hmm
you
so sorry what if you think Max's
experience was first of all they wanted
to start using docker for tests and dev
purposes and they started building
docker containers with the data inside
the docker container the biggest
limitation they found was that was that
it it would soon exceed the tang
gigabyte docker image size and of course
the tang gig docker image takes a long
time to pull onto a machine so they were
encountering issues were keeping the
data in their data in a container that
way and then when they were looking at
the managed service aspect of providing
this solution for customers they were
then having to chef data from inside
containers to a dr environment and i bet
you had patching issues as well yeah you
should never patch docker containers is
what they tell us you should kill them
so if you can't kill it then you've not
built it in a in the right mindset is
what they tell us were darker and this
is where you came to that you don't feel
docker is production ready or data yeah
no and so they're the biggest issues
they're getting to multi terabyte sighs
databases and that's not easy to manage
in a docker container but also say
that's not easy for most people to
handle on a dedicated server it's a it's
a data challenge that most people
Oh
ya know initially so we're docker can't
exploits sands and data deduplication
and high-performance Oracle sequel
server type solutions part of us so
obviously like the azure sequel bebes
the maximum size is a terabyte
disparates using database as a service
kind of out of scope for this solution
the solution might be to just build
dedicated hardware it's difficult
because as you say it's a costly
exercise
yes I would definitely agree review that
if you have very big very data-intensive
solutions you can't really lift and
shift to the cloud I think it's definite
and all lift and shift into docker
containers and then into the crowd I do
think you need to either stay on
premises potentially or and all look at
breaking down your system into
bite-sized chunks the whole
microservices thing is it can be quite
handy I still haven't seen enough about
how they manage the data from all those
different micro services to feel
comfortable yes
you
Albert sort of the question then was
about compliance in a cloud environment
with data I was able to host financial
non pci data in the cloud in the UK
because it met the contractual
obligations and the Data Protection Act
requirements with PCI DFS as far as i
know at least with as you it has pci DSS
compliance for a number of its services
so i would make thee
interesting
absolutely and I think at this point
what I'd like to do is take that offline
with you and see if anybody has any
other questions because I'm conscious
that I'm capitalizing the time when they
could be getting coffee and going to
another session
okay so the question was what is the
virtue of having a docker container over
the docket a database in Dhaka where the
files are on a distributed file store
like as you this is having a vm yeah so
for me you'll still always need the
compute capability whether it's a docket
container or a vm so you're always going
to pay for that but if I stick it on
something like the azure file storage I
don't have to worry about disc sizes and
maximum data volumes it becomes
basically unlimited for me and so then I
can that I can pay for the compute and I
can pay for the storage but the storage
costs will actually be a lot cheaper
than having to provision in desks and it
also if you use sort of the container
app fabrics then potentially the costs
could be lower than having to pay for
provision VMs under the hood and by not
having disks I then don't have to worry
about what type of vm comes with X
number of disks I only have to worry
about how many cause it has how much ram
it has I never have to worry about the
hard drive speeds so from my perspective
it just gives me a little bit more
flexibility and the ability to separate
out the storage costs from the compute
cost any more
okay well you've seen the really long
link for a very long time now I've got
business cards in France if you want to
grab one thank you very much for
listening and powering through to the
end of this talk for me thank you all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>