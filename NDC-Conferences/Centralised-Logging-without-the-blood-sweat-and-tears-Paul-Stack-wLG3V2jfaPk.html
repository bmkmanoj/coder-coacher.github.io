<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Centralised Logging    without the blood, sweat and tears   Paul Stack | Coder Coacher - Coaching Coders</title><meta content="Centralised Logging    without the blood, sweat and tears   Paul Stack - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Centralised Logging    without the blood, sweat and tears   Paul Stack</b></h2><h5 class="post__date">2016-09-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wLG3V2jfaPk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning welcome to the first NDC
Sydney I'm very excited to be in Sydney
it's my first time in Australia so um
it's a long way so far it's a long way
from the UK I left in Saturday I got
here Monday yeah weird um so this is a
talk called centralized logging without
the blood sweat and tears okay this is a
talk based on the fact that I used to
run extremely large Mogan clusters like
I think at one point my last Xterra
cluster when we talked about it was like
10 terabyte 14 terabytes of data at some
point so this is a talk designed on how
to get started on this much smaller
scale much much smaller together so I'm
Paul stack
I got Twitter details on my email
address at the bottom if you hate want
to heckle me if you want to disagree
with me please get in contact and we'll
certainly have a chat about it
um I am an engineer at Hajji Corp yeah I
got a new job if for anybody doesn't
know how she Corp they're the people
that create terraform and packer and
vagrant the nomad and console and all
these awesome tools come in and sit down
friends I'm a reformed asp.net and C
sharp developer okay I don't mean that
with any disrespect okay that's all I
just do not work in the Windows
environment anymore
I work in go I write go away and I work
in Linux so I find myself that I can be
a little bit more hockey what if I go as
people who've used Oh No
so I I do that night I'm a DevOps
extremist so I was a developer moved in
the operations and now I'm actually
building operations tools for developers
again with so bringing some experience
reports into it which is really good and
I'm a conference junkie I think I've
done like 13 conferences so far this
year I think while I'm in said may I
have like four talks and then I go away
to talk somewhere else next week so I
love this
so why centralized logging okay this
isn't a talk and why we should centrally
log okay there are many many talks right
there about centralized logging
okay and why you should do it one thing
that we should agree on two things we
should agree on is central logs give us
that single place to analyze our log
files okay and secondly we remove the
dependency on SS agent and already
peeing into servers to find logs okay
you would not believe the amount of
people I still speak to you that have to
already pay or go to their operations
team and RDP into their server in order
to get their logs and as developers
that's extremely frustrating okay
because it creates an environment of a
we probably forget to care about logs at
that point and we probably don't know
what's happening in our infrastructure
we don't know how our application is
behaving in production the question they
ask yourself as we go through this is
what happens if we lose a server
overnight and that server was full of
log files how are you going to know why
your application crashed or why your
server crashed okay as we go through
this remind yourself on that if you
don't use centralized logging right now
I'm hopeful that this will give you a
little bit of code and a little bit of a
kickstart in order to actually say hey
look I can deploy this infrastructure
quite easily and more importantly very
cheaply very very cheaply
mr. Murphy he was a very wise man okay
his law says anything that can go wrong
will go wrong
okay does anybody in here claim that
they've never encountered a bug in their
production software amazing it's true
right every time we release our
applications to production they behave
slightly differently
to what we expect them to do know this
could be environmental changes this
could be configuration changes mostly
it's because of users okay because the
way users interact with our system is
completely different to how we believe
they interact
for system so because of that it's very
important to understand that you you
must log an errors happen so because
they happen we should catch those errors
when we should take advantage of that
rich information and we should actually
be able to make our software better
based on that is everyone in here
software developers any QA s-- ops
people
everyone's developers everyone windows
developers what languages if we got
outside of c-sharp excuse me
go excellent excellent you're my
favorites already any other any raise it
on go groovy is that still a thing I'm
joking I'm kidding I'm kidding I'm
kidding I'm only joking
it is still a thing and I've used it and
I actually really like groovy to be
honest if he and the interaction with
Jenkins and groovy is very useful so I
only say that in jest ok classic ASP Wow
there's always one that there is always
one you definitely need be centrally log
because that always goes wrong
so one thing I find is you do usually
say or you usually find someone in the
room who says well you know my software
works it works as expected and we maybe
get the odd bug every time ok or once in
a blue moon ok
those people are slightly delusional
because there's one of two things happen
in there at that point they're either
not aware that there's application of
their application is throwing bugs ok or
they're blinkered and they don't care
that it's throwing bugs or secondly
their kid in themselves because they
have no single pane of glass in order to
get access to their logs back to that
that hassle of having the file a request
to your ops person who in 60 days or
whatever their whatever their time frame
in order to get to your request will be
of our DP and in their server and
grabbing a file or our FTP in the file
bar in and then
it's really messy it's really painful
again what happens if you lose a server
and the Lord we're only on that server
there are many many many many many
logging tools 9 they really are ok
logs io log ly Splunk GRA log to elastic
you could go on and on okay the rise of
DevOps has basically meant that you
google DevOps tools and you'll get
hundreds and hundreds and hundreds of
tools back these days okay and the new
way of the new hello world is either
like new frameworks you log in tools new
JavaScript libraries this is what we do
now we like to pollute the environment
with extra tooling there's lots of
hosted solutions you can velasca front
by elastic log lee log Splunk there's
also free ones like elastic as well you
can host it yourself gray dog - there's
loads ok check them out I gave a a
logging talk III believe it was like two
years ago tonight on how to scale a
logging infrastructure to a billion
message isn't they ok we have to get
some terminology right when I say logs I
actually mean messages okay because a
message can have a different type okay
it could be a debug message it can be an
info message it can be a trace message
it can be an error message etc so we try
and use the term messages because we
like passing messages messages is a
great way of exchanging information
between our teams in our environments so
I I wrote this talk and hardly scale a
logging infrastructure to be to accept a
billion messages a day for those here
trying to do the math that's about
eleven and a half thousand log messages
a second okay and it was a painful
horrible horrible experience it really
was and we started with a really simple
infrastructure looking back it wasn't as
simple as I first thought okay so on the
top right hand side we have application
servers we had forwarding or emitting
agents on every application server
these were Windows servers back then
this was back when I was like when I had
hair
I'm and we used I believe we used an X
and X log and we use the specific
JavaScript framework libraries that we
used in order to forward from our new
jsr vacations and so on and those
messages were sent through a log
processor note low processor nodes had
two things on them okay we had a cluster
of three every node had a Redis instance
and every node also had a log stash
instance okay anyone know log stash
quite a few quite a few it became very
difficult because some applications
would connect to Redis to send their
messages other applications would send
their messages directly to log stash and
then we had to put some polarity in
place in order to say hey if you
forwarded it to log stash then send it
into Redis so we use that as a
persistent queue and then later we'll
have a we have the log index or nodes
which will go and read the data out from
Redis and though they'll broker the
messages through into elasticsearch okay
this was painful okay for many many many
many reasons okay firstly Redis Redis is
a persistent storage queue okay we
didn't realize I can see some people
shaking their heads we did not realize
okay that every time we were getting to
the edge of our our memory allocation
and Redis Redis was dumping the keys at
the bottom of the pile okay so it was
first then last night so we thought that
our login infrastructure was like
killing it and we were like processing
hundreds and hundreds of requests a
second yeah that wasn't the case it took
a long time to realize that though a
long time to realize that secondly we
had log stash here and we had log start
here we had a lot of moving parts to
this simple infrastructure it was
supposedly a really simple
infrastructure right and internally it
looked like this
so the log indexers were sending data to
reddit or excuse me the applications
were sending data either to Redis or
they were sending it to
logstash was forwarding the data into
Redis and the log indexers were reading
the data out of Redis and forwarding it
onto elasticsearch tire fire absolute
disaster it was and it got to the point
where one day that we realized that we
actually lost a byte 8 gigs worth of
data per server when a Redis cluster
crashed so the CTO was not very happy
and he suggested that maybe we go back
to the drawing board as you can imagine
what we actually did if we go and have a
look at the infrastructure right there
this was a cluster of servers and this
is a cluster of servers in order to get
the data from the application servers
into elasticsearch pair data center on
pair environment we introduced six new
servers six new VMs they were pretty
beefy VMs they were like 16 gig ROM they
were like 250 gig hard drive and they
had like eight cores yeah this was this
was in a physical data center so we were
like using up resources pretty fast we
had 16 environments and we had four data
centers so we were we were like churning
through money for a simple
infrastructure what did we do as
developers we went and complicated it
again that's what developers do ok no
bear with me there are a lot of moving
parts in this time the application
servers sent data to the low processors
we introduced Apache Kafka because it
was cool back then well it's still cool
it is still very cool what I mean is it
was new I think it was actually like a
patchy Kafka 0.6 okay I think it's dot
nine nine if I remember rightly
somewhere around there so Apache Kafka
was reading the data out of the log
processor notes of course it was having
this store it's offset and zookeeper
cluster and it was passed on it was we
were using Kafka as a persistent storage
okay so we thought right how do we get
the data from Kafka into elasticsearch
let's use another Kafka indexer
so we introduced another level of Kafka
indexers okay we had to reduce the
polarity right now this was this was
pretty successful this is how we
actually scaled to about eleven and a
half thousand messages a second okay we
were churning through like four
terabytes of data right now that they
across all of our environments and
across all of our data data centers and
we had one centralized place one so we
had this same environment sixteen times
and we were using cascade mirror maker
to move the data between all the
environments into the central logging
system so if you think of sixteen of
these all the rope this was expensive
really expensive just to give you an
idea of how expensive it was the
elasticsearch cluster in AWS was a
hundred and forty are three extra large
nodes a hundred and forty and each one
was using a 1.5 terabyte EBS volume this
was crazy but they loved it because week
it could handle any messages we threw at
it no the reason I'm giving this talk
it's because not companies can't afford
this scale right we couldn't afford this
scale looking back we were burning
through money
the elasticsearch cluster alone I
believe was costing us about twenty five
thousand dollars a month just for
elasticsearch okay now in order to get
the data from the application service to
the elasticsearch clusters we had three
kafka nodes we had three log collector
nodes or log processor nodes we had
three zookeeper nodes and we had two
Kafka indexers so we went from 6 servers
um we were not on 11 servers so 11
servers times 16 that's a hundred and
seventy-one
servers just pushing data around okay
not storing it and not doing anything
with it this was ridiculous but in order
to do it we had the again
reverse the polarity inside our lo
processors so the applications were
either logging to Redis or they were
Logan to log stash as expected but we
changed it that Redis was no longer the
cube okay
as soon as it went in the reddest log
stash was pulling it immediately out of
reticence sending it to Kafka Kafka is
designed for this ok Kafka is designing
for this major message overhaul because
it remembers the or it's it's strongly
ordered it remembers the order in which
you pass in it gets out in the same
order I mean it has lots of brokers that
it can read really fast and it can use
in different partitions that can spread
them spread the load so it worked really
well the trouble with it is there were
just so many of them
pager duty was never-ending and I do
mean never-ending there has to be
another way if you're a small company
there's no way in hell you have enough
people and enough money to manage an
infrastructure like this we had a team
of six people dedicated to our logging
metrics and monitoring infrastructure
does anybody in this in this room have
that over at that level of resource for
there for just for their metrics monitor
and infrastructure it's crazy right so
looking back in this like I don't work
about company anymore and I'm this is
not a case of slagging off this is a
case of as developers we very much over
complicated it so over the past couple
of months I've been really looking at
different ways in order to do this we're
going to use three simple tools in AWS
anyone use AWS in here awesome
more than I thought anyone use any other
cloud providers Oh even more which cloud
providers so I've been told that it's
you have to opt in here to use the
Australian as your data center is that
right now okay maybe it's on your
account by default excuse me there's two
here okay
that's because one regularly falls over
and this is not serverless okay I put a
joke tweet out there last night saying
look I've just built a logging
infrastructure service okay
it had 108 retweets and likes okay
because server list is cool this is
platforms of service this is very much
pass okay I understand that lambda is in
there I'm lambda is serverless
everything else is just using pass based
bad tools so the way it works is
application servers send data to your
Canisius stream anyone know what Kinesis
is so Kinesis is like if you think of it
as a very simplistic Kafka okay based in
okay so you have a number of shards
inside the Kinesis stream and the number
of charge relates to how fast you can
read and write data out of it okay we'll
talk about the metrics okay but it has a
persistent queue and it stores data it
retains data for a period of time that
you determine okay I promise you were
gonna get me some code it's not gonna be
me just telling rubbish tales for the
next 30 minutes
so from Kinesis we have to actually take
the data and we have to push it to
elasticsearch what better way of doing
that than Lamba okay lambda is
ridiculously cheap to do this type of
thing okay so we have a tiny like an
application invocation which goes and
reads data and batches pushes it into
elasticsearch this is all hosted and i
could do all of this in infrastructure
as code i like this I don't like
clicking around in you ice coat I'm um I
think somebody was taking a picture you
want that back the codes available I
promise so I'm gonna use terraform for
this and not because I work in the
company I I was am an OSS maintainer of
the code base for quite a while before I
joined the company so this is not a
sales thing at all you could use
CloudFormation for the
really easily you could also use another
tool called Sparkle formation and there
are like lots and lots of infrastructure
as code tools this is the one I use on a
day-to-day basis and have done for about
18 months now it actually just went 0.7
today which is like this is all new
features I'm sure when you by the way so
if that didn't get released this morning
when I woke up we have been using at
their brunch so let's look at it as
follows anyone named questions so far
anyone care I'm kidding everyone at the
back read that
excellent so I just delete something
that's never good okay perfect so we
declare a very we declare a provider ok
terraform has built in support from many
cloud providers ok
Azir google digitalocean Joyent AWS of
course lots and lots of other ones it's
got vSphere VMware vCloud director it's
got like you know there's there's tons
of support it's very easily extendable
there's another talk of mine out there
that you can go and have a look if you
really want to extend it to your own
purposes so we declare a provider of AWS
and we give that a region so we tell
terraform what region we want to deploy
our code in see of course I'm using some
technologies that are not generally
available across the board in all
regions so you'll have to bear with me
and the fact that some of these are not
in Asia Pacific data centers but I
believe they are going to rule later
this year and some of them I'm so we're
going to start with the provider we're
gonna create a module ok modules are
reusable pieces of code and terraform
think of it as like a third-party helper
library that you have an interface for
and you just push values into and it
will actually allow you to reuse the
code again and again so we're gonna
create a module called log stream and
the source of the module I'll show you
the source right now is a Kinesis module
very simple we're going to give it a
stream name we're going to give it a
retention period and we're going to give
it a descriptive
we're going to talk about shark hunts in
a second okay now the module looks as
follows we have an interface as we like
to write documentation we never like the
right documentation we're developers
what this allows us to do it just allows
the user of the module in order to
actually understand what the variables
are for really simple descriptions and
there are lots of tools out there that
will go and grab those descriptions so
that when you're writing terraform code
from the command line it will prompt you
and save the value that you need notice
for this it's really useful we set some
defaults retention period by default is
24 hours so one day reason being is that
it's slightly more expensive for the
longer retention parents and Kinesis so
if you don't if you want to like cheat
but then set it set a shorter amount of
time let's be honest if you're not
processing your logs within 24 hours
you've probably got different problems
so 24 is probably okay and then lastly
we have somewhat puts we need the AR in
okay so the amazon resource number we
actually need that valley to be possible
on the run at different places and how
it looks in the code is we declare a
resource named shard can't retention
period we want to capture all of these
metrics right here okay we want the
incoming records the Elko and records
the incoming bytes outgoing bytes we
want them iterate iterator age in
milliseconds so we understand how long
it's taken to churn our records this is
all documented I'm not breaking any
amazing discoveries right now this is
all in the AWS as the CLI and SDKs okay
we set some tags and that's it that is
the simplest module and terraform that
you can probably a might actually I have
a simpler one but it's really easy it's
really readable okay I understand that
we have some notation here of some
interpolation values for VAR dot that's
as complicated as are as
as our conflict gets so we declare it as
follows so if I wanted to create another
law another log stream I can go and do
that so let's go and create an
environment just so you can actually see
it in place we're gonna call this Sydney
I just want to show you how simple it is
in order to actually call this code I'm
gonna copy and paste like every good
developer does test stream
shark I'm warm
so terraform has some built-in commands
that come with it again this is not a
terraform demonstration this is just
showing you that using really simple
infrastructure as code you can build
this type of logging infrastructure the
code will all be available so if we run
the command terraform get so go off and
get me the module source modules don't
have to be on the same machine they can
be stored in github so that your sis ID
mints can be taken care of these modules
or your infrastructure team and you as a
developer can just be coding against
these modules so we're gonna go off and
get it and it's gone and gone it's okay
as it's local it just creates a symlink
doesn't do anything like really crazy
and if I run you can see it created that
terraform folder right here and there's
a copy of the module source in there so
you can always know what version of the
module you're running against and if I
run the command terraform plan it's
going to terraform is built on graph
theory okay terraform will work out the
dependencies between all the different
pieces of your infrastructure and
actually plot in what order they have to
be created okay and sometimes you'll see
values like this computed because those
are values that come back from AWS and
we don't quite know what they are okay
but mostly you can read it and you can
see what it's going to do and at the end
it actually gives you a nice little plan
and it says plant one to add zero to
change zero to destroy if we go to
Kinesis will seem long stream I had to
prepare my demo earlier because the
Wi-Fi and the hotel has been pretty
shoddy so we're looking at the actual
infrastructure a little bit this is just
showing you that we can deploy it really
quickly alright so there's nothing in
there for NDC Sydney or for Sydney or a
different little team there's only one
stream in there right now
so tariffs on apply what this is doing
is this is taking a snapshot or working
out what the infrastructure of my local
machine is like comparing that against
the environment that I'm going to deploy
it to in this case AWS and it's going to
tell me hey I need to make these changes
to bring the environment up to the same
speed okay so it's a deterministic way
of keeping your environment in exactly
the same configuration
this is a good thing this is a very good
thing and you'll see it's creating and
it gives you some output and says it's
still creating it's still creating just
because the AWS SDKs I'm actually going
to Oregon here so it's a little bit
slower than I would expect but it's
gonna go and create it and if I go back
to the AWS CLI I mean we can see the
test stream is actually active so
terraform is just going to tell us know
that it's created hopefully if it
doesn't crash and we can go another look
at it oh it's currently updating that's
why so it's not it's not a it's not at
the state that weeks that terraform
expects it to be a terraform expects it
to be at the state available so
available to use and it will hold the
connection open to that until it's at
that point because that's that's what we
tell it to do yeah you can see it's
still updating and eventually you'll get
a configuration so I've used the same
terraform module to deploy to two
different connected streams so you can
see it's a really simple wrapper or a
rod terraform that only just to pass
code in before modules were created and
terraform you would have had to copy and
paste all of that code okay I know we
are hopping with copy and paste not like
that we're not definitely not like that
and of course if I really wanted to
deploy it to a different region I could
just change the region and it would go
and would deploy my infrastructure to a
different region no I said to you that
terraform uses graph theory let's have a
look at what that means I can run the
command terraform graph go off and graph
all the pieces of my infrastructure and
tell me the order and tell me how they
relate to each other and it looks like
that that's garbage you can't read that
so this time we're actually gonna pipe
it through a tiny application I'm really
sorry I'm gonna we're gonna pipe it
through an application called Dom okay
it's a tiny package since its Mac and
we're gonna save it as a PDF so that we
can actually have a look at it and if I
say Open Graph dot PDF we can see that
Kinesis stream depends on the provider
AWS which depends because the module
this is the module base right here it
depends on the overall infrastructure in
AWS okay really it's not really growing
breaking because there's no nodes
interacting with each other
we'll have a look at that in a little
bit when we start to build all that all
the infrastructure together I have 30
minutes this could be interesting
so terraform destroy do you really want
to destroy yes we do and it will go off
and it will turn on the infrastructure
as we expect disposable environments
anyone have long running QA environments
Holocaust they are they how out of date
are they more importantly right that's a
it's a QA worst nightmare when they have
to like go and say hey can you like
update our environment we're like 18
security patches behind on Windows or
it's been it's been telling us for a
week that we have to reboot your
infrastructure can be brought up and
destroyed at any time I understand that
I'm showing you stuff built into AWS but
if you're in VMware or your V cloud or
your in Azure the same thing can happen
it's all there it's all available and
then we can see we get a nice though I
put at the bottom it says one destroyed
zero chain Cyril added the apply is
complete it's like a black eye refresh
and one of them will disappear excellent
that's terraform 101 we're gonna push on
let's kill that let's kill that so I
have a module for Kinesis I also want to
store some information in an s3 bucket
yeah anyone use lambda there are
different ways that you can use lambda
okay one you can open up the editor and
you can copy and paste the code directly
into the code editor okay - you can
upload a zip file so if you package it
locally or three and you can use an s3
bucket and it will work out the
difference based on the e-tag so based
on the version of the s3 bucket object
and it will update your lambda
configuration every time based on that
so we have a really simple bucket I did
say this this module was simpler that's
it there's a bucket it has an ACL
whether you want to force all the
objects to be destroyed inside the
bucket before you delete the bucket and
whether the versioning is enabled that's
I don't even think it gets simpler than
that
i-i'll struggle to find one that's
easier I promise you so we have a lambda
we have a module for the bucket we
declare our lambda function so this is a
new feature in terraform 0.7 where you
declare data sources okay before you'd
have had to use template files and stuff
like that now what it does is it will
hold that as a data source and it will
allow you to share that data source
between different pieces we still have
this is our entire lambda function right
here this is thanks to an open source
library from AWS labs that will allow
you to go and experiment with this so
the copyright is all still there the
license is still there but it basically
has three up it has three variables that
we pass in okay the region in which the
the lambda needs to be deployed the
elasticsearch endpoint that we actually
have to push the data to and lastly the
index that we want to push data into an
elastic search okay everything else just
just like simple no jail
actually it's not simple motives I can't
read JavaScript so it could be doing
anything for all I know if this was
paisa than I'd be as happy as anything
but all it's doing right now is reading
batches of information from Kinesis and
it's pushing them through this lambda
and it's actually pushing them into
elastic search it's it's it's ingress
and ingress and egress okay it's just
think of it as based on that so what
we're doing is we're actually going to
say hey go off to this template file
substitute in these variables and then
that's then available to be used within
anything that actually needs that Jason
in this case we're actually going to zip
the file up because we want to use the
zip file in an s3 in order to deploy our
lambda so there is a little resource
type called archive file which you pass
some source content into but look we
have a really nice data interpolation
here of data that template file that
lambda function but the main thing is on
the end that rendered go off and render
that template for me go build that
actual J's file and then save it as
lambda that's it and you'll see right
here Kinesis lambda is not J it's
actually built it for us and if we look
at it we will have substituted in the
variables I haven't done this this is
doing it for me I promise you so it
builds it for us and it's a it's a load
I'm specifically doing these out of
order because I'm going to show you how
terrifying builds the graph the whole
way through so right now we have a
Canisius tree we have an s3 bucket and
we have a javascript template that we
want to do something with in this case
we're actually going to upload that
lambda source that Jas template file up
into an s3 bucket and just so that it's
available in order to be ingested from
from lambda itself now the interesting
thing the lambda function
lambda functions it took me a little
while to get my head around them to be
totally honest with you okay because not
only do you have to declare the resource
but you also have to use I am on it
okay who like I am no one it's really
complex it's really horrible okay so for
those that don't know what I am is I am
as Identity and Access Management and
AWS so it allows different people and
different processes access to different
pieces of your infrastructure so for
example an s3 bucket we ideally do not
want that s3 bucket to be publicly
available if it's got sensitive
information in it or if it's got some of
our application code in it what we would
like to do at that point is we would
like only specific access from either
specific accounts or specific nodes or
specific services so like if we have an
s3 bucket that we're pushing code to the
only thing that should be read out of
that is maybe lambda okay so that's what
I that's what identity and access
management is it's adjacent that's the
best language for writing stuff okay so
what if we go and have a look up in the
lambda function we'll come back to that
so the lambda function is really simple
okay we declare an s3 bucket and s3 key
we give it a function name we give it
the rule that needs access or that has
to act on the lambda function itself and
we have a handler and a time ID now
let's have a look at what the actual
rule is firstly you need a rule an
overall rule that you can attach
policies to so this is saying that this
specific function is for lambda so you
have a principle and a service which
says that the actual lambda service is
lambda on Amazon AWS comm but we want to
attach extra policies to this so we want
to attach es star I would never use star
this is just a demo okay you can have
like yes describe yes get yes push all
these different things
I'm just saying that lambda has accessed
to do anything inside my elasticsearch
cluster it's probably not the best demo
to give if you're gonna use my code
don't do that
please don't but I also wanted to be
able to do all bits and pieces of
Kinesis so we want to read from Kinesis
we want to get from a read from Kinesis
we want to change the iterator and
kinases and we also want to like list
the shards and say Canisius to see
what's available and what's not so you
attach that policy to the rule and we
end up with and I am I am for lambda and
the permissions are as follows a s star
Kinesis star on all resources inside it
ok so that's going on it's deployed it
forth great so now we have a lambda
function okay that can read from Kinesis
so we've actually built most of our
pipeline right now
next we actually need to tell it what's
it reading from okay so you have to tell
lambda what the data sources data
sources in R in AWS are things like
DynamoDB Kinesis redshift s3 buckets if
you want to log directly the s3 buckets
that's fine as well but so you just give
it in this case the error ends okay go
off and give it the AR m for the the
Kinesis stream and give it also the
lambda function error and and that's
going to create the link between them
and we can go and have a look at well
how that actually does that inside
lambda in my function
they can the triggers are there so you
can see that the Islam the function is
connected to the Kinesis stream called
log stream on this AR m and the batch
size that we want to pull from is a
hundred and there are no records process
because there's nothing in that function
right now okay but you can add triggers
and you can do all sorts of stuff no we
get to the interesting bit elasticsearch
who uses elasticsearch not many not many
a little bit okay elasticsearch can be a
beast it can be as difficult or as easy
as you make it okay I love elasticsearch
so much it makes my life really simple
and what makes my life even simpler no
is the fact that Amazon have just
released support for elasticsearch 2.3
that happened last week I believe okay
so that makes it like usable it was 1.5
up until last week I don't know anyone
that's still using like a pre
elasticsearch to installation so this is
a good thing right now we'll come back
to the ion policy in a sec we're gonna
declare a module for elastics
elasticsearch we're gonna give it a name
okay this case in this case the name is
NDC Sydney 2016 just to show that I've
created one specifically for here we're
gonna give it a data note cont okay so
the new functionality and elasticsearch
in AWS allows us to declare master nodes
i'm data nodes in the same way you would
put in production okay think of master
nodes as like as the entry point into
your elasticsearch cluster okay those
master nodes are like that they're the
dispatcher
they're the ones that do all the work
and the data nodes are just empty data
stores that you just keep attaching and
as your data grows you just add more and
more and more data stores in and attach
them to the master nodes okay we're not
doing that this time just because i'm
trying to keep it as simple as possible
so we're just going to say we just want
one node 1 dana node we don't care about
zone awareness so what that means is not
a little balanced ok if you wanted to
load balanced you need to note
because that's the way load balancing
works if you take one thing away take
away that load balancer needs more than
one node and lastly we want some we want
an access policy the access policy is
who can talk to your last exert instance
I decided to make this public and wide
open so what I'm saying is is that all
actions on elasticsearch are available
for anyone and they'd open yes okay so
if you when the output comes out the
back if you really want to you can hit
this URL you'll probably cost me money
but that's okay
I'll take it back in beer tokens and
then lastly we take an output of our
Cabana okay so there's module we have
six modules okay but inside each module
we're doing lots of different things so
let's graph our infrastructure right now
okay because the order in which this has
been done is slightly different to how
we would actually create our
infrastructure for example I have
elasticsearch last that's probably the
first thing that's gonna be created in
my infrastructure right now because
other pieces of information have
dependencies on it
for example the lambda template has a
dependency on the elasticsearch note
because it needs to grab the URL so we
can graph this infrastructure
it's not a make
oh it's not in trash no I don't want a
good trash it's definitely not in trash
terraform dev there we go that's better
if we actually that's if we tariff on
graph it first we can see that it's
pretty complex and it has a lot of
different pieces okay that's garbage
excuse me yes yes just because everyone
as I say everyone likes I am and lastly
if we just open the grass we get a
pretty picture okay because now we're
actually building the dependencies
between our system so you start at the
top and you work your way down terraform
will work like what a king because of
that graph theory and it walks the tree
every time it will work out what it
needs to do first okay and what has
dependencies on each other so you can
see that it's got the archive file it's
got the s3 book it's got the lambda
function it's got the I am rule policy
it's got a Canisius stream that has no
dependencies on anything else but the
lambda event source mapping has a
dependency on so on and so forth
who hates drawing who hates drawing at
work why draw this can do it for you
this is what I tweeted and everyone was
like wow it's serverless it's not server
this it's not really support it's just
using pass but the main thing is is that
this is living documentation
and if we terraform plan it will compare
the state locally against what the state
is in AWS I'm really hopeful there's a
lot not like it's gonna destroy my
infrastructure right now but you can see
that it's going through it's going
through the entire list and it's
checking it one at a time and it should
tell me that there's maybe one change
that's yes yes cuz I - changes oh well
we'll skip that one look what that is
this a double yes normalizes um the
Jason for access policies and I amped so
sometimes it sends back like an array
will you pass the string and sometimes
it sends back a string where you passed
an array with just one element inside it
I haven't cracked that part yet I will I
promise but the main thing is I just
renamed their earlier index dot handler
techniques Islam that yes not handler
okay so it's going to tell us it's going
to give us a deterministic view of what
it's going to change please if you're
gonna use a technology I like this or an
infrastructure with code tool don't just
blindly apply to your environment always
always run a plan first so you can see
what it's going to change okay and what
you can actually do is you can terraform
plan right
Sydney demo txt and it will save that
plan as text so that you can see in
github or whatever source control you
use what it's going to do when you've
made those changes so if we go and have
a look in AWS we can actually see if
this worked firstly I have a lambda
function kinases es lambda that has the
configuration of index dot handler
because that's what terraform told us it
was going to change to something else it
has a role called I am for lambda which
I showed you here I am for lambda it has
a trigger which it reads from a Kinesis
stream called log stream let's have a
look and see if that's there
13 minutes and then no we're gonna a
couple of figures log stream log stream
has five shards they're all open we can
see all the different metrics that we've
actually applied to it so that we can
get them inside code watch you want
metrics metrics are a good metric of
your friend from there we have a
function yep and then lastly we have
elastic search itself okay we have a
domain of NDC Sydney 2016 elastic search
2.3 the best thing is it's a green
cluster it's only one node if it wasn't
green I'd be slightly worried I actually
have been really worried and it wouldn't
work but the main thing is if we go
inside the cluster we can actually start
to say hey look there's one node so
there's one day to note there's no
charge because there's nothing in there
right now we haven't pushed any data
into it there's no indexes we're gonna
we're gonna force an index in right now
if I open and cabaña okay
we have the monitoring so cluster green
cluster yellow cluster red searchable
documents node deleted documents metrics
and then lastly we can have a look at
the access policy the access policy is
what we declared we did it in a slightly
different way because we declared it as
statements rather than Jason but it's
going off and built the Jason for us
where we have the resource and we have
the action and we can say everybody's
allowed access to and you it has
templates if you really want it like
copy some templates I actually copied
the templates and lastly we have an
endpoint so our applications know where
to send our data to okay you need to if
you want if you actually want to send
some logs but more importantly we have
Cabana Cabana is a dashboard that allows
you to visualize and search documents
inside your elastic search cluster this
is bumbled and we actually can go and
create our index patterns okay so in
written six really simple terraform
modules that are connected together in
the graph we've deployed an entire
loading infrastructure really simple how
would you push data into it I'll shoot
me in a second
now if we go back and have a look at the
cluster it's going to be yellow because
we have an index and it's currently
trying to allocate that index owed
across its replicas and chart
hopefully it's yellow yes almost like
practice this earlier and we can go
inside the indexes where there were none
before and we can see that there's a dot
cabana for there's one and the size and
bytes is this and it's got specific
mappings okay serverless in action yes
no the codes all gonna make github I
promise I'll post it and hash tag up NDC
certainly okay it you can go and have a
look through it there's nothing in there
that's proprietary I'm probably gonna
remove my AWS account ID hi I have left
so many account IDs and access keys on
stuff I gave a talk in London a couple
of months ago
and I opened up an email the talk was
being recorded and everyone could see my
username and password for a specific
tool that I use error really bad so how
do you send information to Kinesis okay
on your application server okay you can
use a lot you can use log stash you can
use beets or something if you want to
use log stash install the log stash
package and add those seven lines right
there okay so you declare an output the
output is telling elastic error telling
that log stash I want to send this data
where I'm reading it from to somewhere
else it'll be the stream name it'll be
the region and you'll send it as Jason
it doesn't get any simpler than that as
long as a log stash output okay so
that's that's really key so the costs
this is know where it gets interesting
so I told you before that my
elasticsearch cluster alone was $25
approximately twenty five thousand
dollars a month my entire logging
infrastructure was thirty eight thousand
dollars a month okay
it was pretty hefty okay I probably
would have added some more stuff to it
just because I like like saying oh I
manage infrastructure at scale it wasn't
really at scale
it was it was causing more problems and
it was worth so Kinesis streams are 0.15
US dollar for every minion put requests
okay we have five shards five shards
will cost I'll come to that in a second
five shards are approximately like fifty
something dollars a month its fact it's
fifty seven dollars sixty or five shards
can handle 1.3 billion put requests a
month okay that's an input of 10
megabyte a second and a read of 20
megabyte a second okay if you're gonna
use that that's pretty cheap for 1.3
billion messages a month ok Kinesis
streams if we want to hold the data for
longer than the seven days it works out
at 0.02 US dollars per day or per hour
excuse me so you would have to do it
works at the extended data for seven
days works out to be $59 and 52 cents I
can't really cheap you're probably not
going to need to spend that $50 because
if you're still take if you're still
having to read your log messages in
seven days time I'll reiterate you've
got some problems or alternatively you
probably shouldn't be logging it because
if it's not useful to you and you're not
reading it why baller lambda lambda is
22 cents for every million invocation
queries so if we're going to push 1
million messages through lambda a day
its 22 cents US cents not Ozzie cents
elasticsearch is nine cents per hour per
note cheap I'm using an m3 medium there
so it's a general-purpose machine that's
got eight gig of ram and it's got a
couple of processors it's enough in
order to like churn through elastic
search queries and lastly if you want to
add some EBS volumes the elastic search
to store you know if you I think by
default it comes with
a four gig SSD you got anyone more data
you're gonna need more size it works out
to be thirteen and a half cents per
gigabyte per month with the entire
logging infrastructure I have put
together in this github repo so I am
assuming 1 million messages a day I'm
assuming that I'm going to store the
Kinesis data for seven days just in case
my data Lake needs to be filled from it
because that's the new cool thing to do
I'm going to push a million invitations
through lamda every day I'm gonna stick
with one node all right yes one node
just because it's a Deb setup or a local
machine setup or you're just testing
this out and I'm gonna add a hundred gig
SSD a general-purpose SSD to that
machine the total cost is a hundred and
forty-four dollars eighty-seven a month
I feel like a Salesman I feel like this
is like amazing discoveries or something
right
that is really cheap okay for being able
to process a million load messages a day
as you're starting off at your company
with centralized logging if you you
can't tell me that you cannot spend one
hundred and forty-four dollars a day in
fact you can cut out by 50 bucks because
you don't need to they extended lifetime
I'm really trying to sell people this
right this is ridiculous
but it's so cheap it's like I would
spend that on coffee a month if I was
out in London
okay London's a really expensive city
and it's like six quid a day probably
ten quid a day for coffee I'll have a
couple I like caffeine I can I can do
that personally for my own personal
projects we should all start using this
type of technology all of these hosted
services are available I have used three
hosted services in order to process the
same level of data that I had six
middleman services to get the data from
my elastic from my application server to
my elastic search cluster this has it
all you could probably productionize
this for probably I reckon in the region
of the books
$400 a month I posted this last night
and somebody was like yeah a million
messages a day it's not a lot I was like
it is if you're just getting started you
don't really go and you're not going to
want to go and spend twenty thousand
dollars a month right now because you're
not going to use that start small
let your application infrastructure grow
in the same way as your excuse me let
your infrastructure architecture grow in
the same ways your application
infrastructure that's in a jar we have
this the fair decisions until later the
same thing happens here when you can use
these hosted services you can grow them
and you can add more nodes and you can
you know the nice thing about lambda is
you don't have to go and tell lambda hey
I'm going to protest a million requests
it's going to keep charging you for as
much data as you push through it the
same for Kinesis it's really cheap folks
I have two minutes forty seconds because
anyone got any questions the man at the
back
can you just repeat it and really sorry
yep
interesting question so rather than
using this you could use like Claude
watch logs or something like that and I
would use this because I can push many
pieces of information through it I can
push Claude watch logs into this so I
can make my Claude watch logs searchable
as well as my application logs I can
push metrics into this and then use
something so have lambda omits that's
the event okay in order to push that to
your metric server so this is like a
they give us like a Swiss Army belt
okay you push Jason through it it's
gonna stream it and it's gonna index it
in elasticsearch no as you're starting
in a centralized logging infrastructure
you're probably not going to know what
you want to log okay but more
importantly you're going to want to know
when you need to like expand your
infrastructure a war story on this is in
our Redis infrastructure the first one
that I showed you an applicant going to
say something rude they're a developer
I'm actually ddosed our own
infrastructure okay because he didn't
realize that he had turned on trace
debugging in his server so it was
sending but the pieces of information he
was sending were like 20 megabyte log
chunks so the infrastructure that I
built by hand and like loved like a
china play and glue and back together
every so often wasn't able to cope with
that this is much more designed to be a
little bit more elastic and it'll it'll
churn through the data because it's you
know it's all platform as-a-service
amazon are better at doing this stuff
than I am I trust them to get this right
if there's if there's anybody that can
get it right it's probably AWS right so
I would start with something that's
really simple and then if I need to
change or any
to push more data through it just keep
growing and growing and growing so a
clogged wash logs are clogged watch log
groups are a great way for your
infrastructure in order to push
information in have you ever tried to
use the Cline watch II why I didn't say
that simple good answer
any other questions what do I use to
monitor my logging um I've not run this
specific thing in production
I used to use sensor big fan of sensor
just because it's a pull based
architecture and I used before that I
use nodule which was horrid so don't use
Nigel sorry if anyone you worked for
Nigel but it's just really old and it's
it's not designed to scale in the cloud
it's designed for like a constrained
data center that has like a number of
nodes that are known okay
sensor is more of a when a node comes up
it checks in the sensor and says hey I'm
a node of this type give me all the
checks that I should understand I should
understand and it will perform them and
send the data back there's a lot do you
sense it's probably my favorite one
there are lots of other tools out there
that you can use obviously I just find
it really simple really really simple
any last questions you're all free to go
thank you all very much for your time
enjoy the rest of the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>