<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Elasticsearch Do's, Don'ts and Pro-Tips  - Itamar Syn Hershko | Coder Coacher - Coaching Coders</title><meta content="Elasticsearch Do's, Don'ts and Pro-Tips  - Itamar Syn Hershko - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Elasticsearch Do's, Don'ts and Pro-Tips  - Itamar Syn Hershko</b></h2><h5 class="post__date">2017-02-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c9O5_a50aOQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">for just before we start how many of you
are using a sixth
today good this is not an intro-level
talk all right this is kind of a talk
where I'm expecting the crowd to
actually know about elasticsearch what
it does what it doesn't do so if we're
fixing anything else if we live to more
interesting interesting talks and I will
just I will push some of the content
that I planned originally to talk a bit
about a current situation of going on
and with the last Excel security so just
expect that as well and they'll explain
a bit so my name is Lisa Mar I've been
doing the elasticsearch search engines
these kind of stuff for quite a few
years now I'm an elastic stretch
consulting partner I'm operating through
a brand called Big Data boutique we're
doing all of sorts of things in the
cloud using big data as well as elastic
search installations worldwide including
training and Microsoft MVP and a
contributor to both the sales Internet
and an elastic search and our agenda
today again not an intro talk and I'm
going quite fast now because I just have
more content to add because of the
current situation is going on
meaning elastic stretch clusters been
hacked for ransom worldwide so talk
about this for a few minutes and show
you what needs to be done in order to
protect that I think it's it's actually
even more important than giving you some
hair tips on how to use elastic search
in general we all will talk a bit about
designing a cluster and your document
structure I'm just giving you some
pointers to what things you can do
better or just based on my experience
things that maybe you are not aware of
and I was considering will talk based on
that we'll talk more about queries and
some some things that could happen in
the analysis chain that you are using
whether you are aware of that or not and
I'll show you some stuff that could be
happening in a cluster and may be worth
your attention we'll also talk about
index mappings some tricks some do's and
don'ts over there
we'll talk about cluster management how
to manage a cluster or to be more exact
some tips about managing your cluster
how it should look like things you
should be doing and shouldn't be doing
and we'll I'll try to actually get to
talk a bit also about data ingestion how
to correctly ingest lots of data into
your a success production clusters so
that's our agenda I wasn't planning on
talking about that but about a week ago
and clusters elasticsearch clusters all
over the world have been starting to be
hacked the data was being starting to be
deleted and some message that looks like
something like that was being posted
into the clusters basically if you had
the clusters of cluster with lots of
data you suddenly found that your
cluster is entirely empty and found this
ransom message on your clusters asking
you to pay for it to get your data back
anyone here has seen that before
in the internet except from Nile who
actually tipped me on that alright so be
aware that this is currently happening
this is like about it started about a
week ago and clusters all over the world
are we are talking about like 5000
clusters right now right so about 5,000
clusters worldwide are we know that have
been act we know of some clusters that
are not yet been hacked or but are still
open to the web and could be hacked so
just if you have elasticsearch clusters
just go back to your company and make
sure that your clusters are secure and
that's what we're going to do now I'm
going to show you some stuffs and do's
and don'ts about that just going back to
the preview screen notice the URL so if
you're using a nastic search you know
that the elasticsearch is accessible via
the HTTP REST API over post nine two
hundred and usik as you can see this is
like this is really the couple of days
ago as you can see did some public IP
address accessible from my home from my
home network on port nine 200 and
warning is basically an in
name with that note with that message so
that cluster was exposed to the web and
that's what allowed the attacker to
actually take the dash out deleted and
then asking for ransom so how do you
protect your cluster basically that's
lots of pointers I'll go through them a
bit quickly so we have time for the
other content I'll explain and I'll be
happily stay after the talk or during
the conference if you have further
questions just come and ask me I'll be
happy to answer protecting your data is
important so in order to not fall for
those ransom attacks the first thing you
should do is protect your cluster from
the public at ajaan expose it on any
public network now that sounds obvious
sometimes people don't do that the
elasticsearch part is a bit better it's
actually doing that by default just
letting you know when things are not
configured correctly and most of the
clusters that we see that are exposed
inversions before version 5 okay so two
points something and even one point
something people are still still using
that and those are a bit easier to get
wrong and that's where things start
happening so whatever you do don't
expose your cluster on the public IP the
way you do that you just go to the last
external file and tell elasticsearch
what address to bind to DNS or IP just
make sure it's a private IP and not a
public IP so whatever you cluster is the
whatever way your Kolkata is configured
to don't bind into a publicly accessible
DNS or IP that's like the first thing
you should do the configuration is
called network das in that just for you
into issue so you have it that's the
first thing you do the second thing you
do you make sure because now you close
down public access to your last excites
crafter's you may have some client
applications talking directly to
elasticsearch that's it's totally an
anti-pattern but some people do that and
that's okay I'm not judging but now that
you've closed down your public access to
your cluster you'll need to somehow you
know make allow some access so you
basically create a software facade
through your application you'll filter
or you'll approxi those requests through
your code I recommend not using the
elasticsearch DSL directly for my
application to the elasticsearch cluster
even though is even if it goes through
some sort of a stab just create your own
API against your Web API and just
translate that into the elastic search
queries but again the most important
part is not to communicate directly with
elasticsearch and to filter those
requests and so for example not allow
write only a lot of regions or in the
filter the rate as well that's very
important next one disable HTTP access
from nodes that don't really need it so
I'll I'll talk about cluster topology
later on the talk but we have master
nodes
data nodes client node only the client
nodes really need Claire HTTP access
master node definitely don't need HTTP
enabled on that on them dozen owed on
some installations you will have HTTP
enables on Sam you Don again find out
where you can turn off the HTTP access
and turn it off and again even if you
can use torches that not a default port
that alone will make the attacks less
less common obscure a security by
obscurity is always a good thing
sometimes you will have public nodes pub
of elastic switch nodes publicly
available and good example of good use
case use case example is a Cabana or
crops or all of those utilities that you
use to monitor and query your elastic
search class basically tools that are
already available for you and just you
just using secure them as well all right
so basically I would recommend only
exposing them from a private IP and then
connecting to some sort of a VPN and
accessing
that only through that VPN most VPNs do
they have two-factor authentication
stuff like that so it's very highly
protected and that's the way to do it
but sometimes setting is out that app is
a bit hard to do and onto those cases
it's okay to expose those endpoints to
the public network to the Internet
basically again binding to a public IP
but do you put some sort of
authentication layer around that so I
have a blog post detailing all of that
if you want more details and in that
blog post I have a link to a compare
simple and genetics configuration that
does all of that it just adds some
authentication there on top of
elasticsearch cabana and crops you can
go ahead and look how it looks like
that's very easy to set up and it's
really really important to do for those
of you who are running elasticsearch
before version 5 especially versions 1.0
1.7 you are very susceptible for attacks
using scripts malicious script so
elasticsearch until version 5 had groovy
as the default scripting language before
version 2.0 it had different scripting
languages one of them was groovy as well
in some versions and those dynamic
scripting languages are not sandboxed so
an attacker could potentially inject a
malicious script that should be used for
querying but it will it will be used to
somehow gain access to the machine
itself so disable then I max crypting if
your before version 5 version 5 already
took care of that for you the default
scripting language is is is safe to use
so again what is planning on actually
giving that intro but it's super
important just remember that and go
check your clusters and feel free to
reach out if you need any any further
advice so that's about that we have 15
minutes left to talk about the actual
content and forgive me if I skim through
some of it so let's start with design
how do this diner class to how to design
your search and your
plus a logging analysis so we see two
main patterns using elastic search the
first one is what I call the monolith in
there you have your data store somewhere
and you replicate that data to elastic
search and use the elastic search is
some sort of over search facade and a
way to basically dig through that data
usually the recommendation is not to use
elastic search as a single source of
truth we'll talk about that in a bit but
that index is the Manolis index is where
all your data is and that that is being
continuously updated the documents will
be updated and you can't really you know
char get into different manageable
entities it's being used a lot for for
search a text search your special search
those kind of things even image search
record link as I have many text
documents I want to find similar
documents anomaly detection stuff like
that so that's one usage pattern the
second is but with usage pattern we see
quite a lot and that's about 80 90
percent of elastic search users use
elastic search for that is for
centralized logging right so I have data
that is time serious data and I'm
putting it into elastic search
continuously in the most used cases when
people using elastic search for logs
about 95% of usages that data will not
will not ever change it just logs
things that happen so events think about
logs in your system centralized logging
think about IOT so lots of sensors
sending lots of data and that that it
doesn't change it's only a thing that
happened in the past
lots of times you'll see usages of
elastic search in that respect with
audit logging and stuff data so because
that data doesn't change you can safely
just put that data in index and let's
say call that index using the name using
the name of the date that is currently
representing all the events in that
index and then when tomorrow comes just
lock that in there
then move to index into a new index so
we call that pattern a rolling indexes
and time series data time-based events
and again about 80 90 percent of
elasticsearch users use elastic search
for those use cases using the rolling
indexes pattern you starting with
elastic search five gets a lot of
attention and gets a lot of additional
features and I'll cover some of them now
but those two patterns are the thing to
consider when you're approaching elastic
search what is my usage pattern and once
you realize these whether it's the Mona
Lisa or is it a rolling INXS pattern you
can start approaching elastic search and
try to understand what feature set you
can use so if you're having a mano list
if you're having just one index that
gets data from your data store and just
being updated over time and you search
on that and fast it into aggregations et
cetera then the most important thing I
can tell you is that you should treat it
as a volatile index
don't ever trust that index is there and
just make sure that you have did a way
to just recreate it from scratch using
your data but I'm basically saying don't
treat elasticsearch as a single source
of truth that's what that is going to
give you lots of power for example like
we'll discuss in a bit you're going to
learn a lot of stuff about the way your
mapping works the way you should create
your documents based on your data
assuming your data is in SQL Server or
an SQL database sometimes you have
different design decisions around the
structure of the document you're going
to index into elastic search and that's
going to change over time if you learn
about the usage patterns of your users
that's why once your index is it treated
as a volatile some as a volatile index
so you can just delete it and create it
from scratch
obviously there are these ways to do
that without downtime then you get a lot
of power in on moving forward and
improving on the experience so that's my
first advice to you the second reason
why I'm saying
because elasticsearch themselves are not
recommending elasticsearch to be used as
single source of truth I mean they have
a document an official document it's
called a seal a success resiliency
document feel free to search for it
you'll see that they're still handling a
lot of corrupt data corruption issues
and network issues stuff like that this
is in a much better situation that it
has been like you know three years ago
things have improved drastically but
still the official combination is not
used a toxic search as a single source
of truth another thing that you should
note mainly for the case places where
either indexing takes a lot of time or
you're afraid of being ransomed or just
in places where for example in logs in
the rolling indexes pattern you have
your data only in elasticsearch and that
actually happens it's okay you should be
using the backup and restore API now
backup &amp;amp; restore or mainly backup the
backup operation in a success is very
very cheap so what you should be doing
is just creating a backup every 5-10
minutes and it's perfectly okay because
all that being happened is just files
are being copied to the repository where
your index is being backup so it's
perfectly okay and that's something
issue you choose and you can consider
there is also the ability to three index
very efficiently with elasticsearch and
that allows you to take data from I mean
one index and reindex that into another
index using different mappings
elasticsearch has a Redux API built in
there are six which five also supports
your indexing from different clusters so
it gives you a lot of power on that
respect
other than that elasticsearch five also
adds some more api's I'll just mention
them and move forward but the shrink API
for example allows you to support large
scale indexing it allows you to scale
out your cluster to many many shards and
then recharge it basically too small a
number of shards and to enable for
faster and more efficient reads
also optimize the disk space and
compression it's a new feature in five
and it allows to very strong digestion
cabal capabilities in elasticsearch also
the rollover API allows you to do and
better rollover in rolling indexes
pattern so you can now make sure that
your indexes maintain a certain size in
terms of documents in the index and your
age and this is how you can optimize on
queries and and in index sizes on disk
let's talk again a bit about document
oriented design that's a very important
topic in in elasticsearch elasticsearch
is a document store it allows you to
work against documents it is not xql and
it does not have the concept of
relations or I'm lying to you a bit it
does have something and that's what I'm
going to show you but you shouldn't be
relying on that and that's my main point
here so let's take let's take a look at
the part something that you probably
have seen one way or another I have any
commerce store and that ecommerce
website has categories and it has
product obviously with elasticsearch is
represented as documents and so I have
in this case one document that is a
category and then to document that our
product now those documents are related
to one another in the following way a
product can be within a category and
then the cut each category has a
required member level that's something
that I put on a category level and then
I'm expecting my users that's being
logged in to only see products under a
category they're allowed to see right so
I have some sort of relation between
product and category that relation is
for doing some stuff but as you can see
the documents are completely separate
from one another if I wanted to query on
to just show documents product documents
that relate to a category in a certain
way for example using that required
member level
touken I would have some issues doing
that because those documents are
completely unrelated elasticsearch
allows give me some tools to handle that
and that's just I'm just showing you the
tip of the iceberg
going into a full-fledged design with
discussion here I completely out of
scope I just want to give you a taste of
how it will look like so the naive
approach would be to put the products in
the category in the category document
alright so now I have all the products
and the provider map member level the
token is accessible from this same
document what is the problem with this
approach so I have one big document say
I have one category a category with
thousands of product that document is
going to be way too big and every update
to every product is going to trigger an
update to the entire document of the
category which is a completely different
concern also note because this requires
each product to have only one category
unless of course I'm okay with just
duplicating product in other category
documents so things are not that pretty
as it turns out there is another concern
here which is a bit less intuitive the
fact is that this design which is
actually entirely similar to this design
in elasticsearch hierarchy is being
flattened out into just filled names
with all the possible values so product
dot name I have two instances of that
field so essentially it will be indexed
like that I'm losing here the ability to
connect between the names and their
prices I cannot now ask or get a correct
answer for the question a USB camera
that costs $35 or use the camera that
costs $100 both will return this
document which is actually on the report
the category documenting of the product
documents
so things get start to get really messy
here elasticsearch has a solution for
that for to be more exact to be specific
it's called nested documents I can
essentially tell elasticsearch those are
sub documents and elasticsearch will
take care of indexing them on their own
and then allow me to do that cross type
of of query but things are still a bit
nasty right they still need to update
the categorical document when and update
a product or I get back the category
documents whenever I prefer product no
it's not that nice another option is to
use the parent-child approach
parent-child allows me to have two
different types or two different sets of
documents so a set of documents which is
the categories and then a set of
documents which is the product and I can
Anakin tell elasticsearch on insertion
that this product document is actually a
child of a proper category document and
now I can search in products I can
search your categories I get the two
completely separate complete completely
different sets of documents but I'm now
in one query single query I can
correlate the two I can ask for product
that has a parent and then specify a
query on the parent which is the
category or do it the other way as for
categories and I'm an integrated query
on his child that also queries on
product on the child documents so it's
very nice in this actually works quite
nicely the issue with this is that I'm
delegating the work to the query time in
search engines in general we prefer to
do as much work as possible during
indexing time and then optimize for read
time I want to get to a point where my
queries are 50 to 100 milliseconds it's
most on the 99th percentile that's where
I want to be this this thing here is
essentially is some sort of a joint that
happens on real time and that's not
something I want to have if I can avoid
it
in some scenarios you can avoid it
we can do something that's called
denormalization which is kind of in that
SQL databases we would do normalization
here we can just do G normalization and
duplicate data and in this case I'm
embedding the category data into the my
product data now I have in one document
all the product doctor and I can search
for it I can aggregate on that akin to
whatever I want and I also have the
additional metadata on the category that
I can filter on and do more interesting
stuff on on both on search and on
aggregation and it's very handy again
I'm still assuming I have one category
for each product and if I'll go deeper
and I'll recall most sophisticated
system and charts requiring to have more
than one category for a product that
could get interesting oh maybe I'll be
thrown back to the discussion on nested
documents for example so things are
starting to maybe become a bit more
interesting but just again to give you
the tip of the iceberg of a design
discussion that you should be having
when talking on about monolith index
that is a replication of the data store
just to give a few bullet points always
prefer flat documents with elastic
search try aiming to a design that only
involves flat documents try as hard as
you can not to have hard relationships
no joins no nested documents which is
it's pretty much of wedding between two
different types of data you can get to a
point where you have relationships but
there are a bit more loosely typed so
maybe they are done on the application
there maybe it's parent-child which is
still some sort of a relationship but
it's more loose and more things like
that you shouldn't be afraid of the
normalization the normalization is maybe
some over duplication of data and maybe
it's some time data together but if done
correctly can save you a lot of pain and
actually with elasticsearch updating
data on the go is not that hard to do
you can always write a small Python
script or whatever to do some updates
for you there is an update by query API
in elasticsearch and there is many ways
to actually fix mistakes or you know
update data as you need to as long as it
doesn't happen too frequently even if
it's once a day or something like that
don't be afraid of that
the main point when when having a design
discussion is try to aim for the query
part so try to optimize for reach try to
optimize for fast queries and that is
the best advice I can give you don't
think about how the data looks like
think about how you want the data to
look like on query on the query side
when you display it for the user or how
are you going to are you planning to
query that data and here again the point
about the elastic search index is being
volatile all right so all the time your
questions might change or the way you
want to represent the data might change
and that's actually going to work to
your benefit because now you can just
scrap the data and move to using a new
index and you can do that in zero
downtime using index aliases so moving
on to the query side first if you had
never heard of the Cabana console or
previously called stands go now and
check it out
it basically allows you to quickly
isolate on elastic search and
investigate your indexes including also
complete and and all that base all that
you need in order to work flawlessly
with elastic search starting with the
elastic search cabana force before I
think it was embedded in Cabana already
now to just change its name and it's
called console very easy to do there's
also a chrome plugin which I use I find
it very useful in chrome it it could be
it's called sense and it's very very
handy and when you do queries and as I
said you want to optimize for query
latency you want to optimize your
document design for queries so
in order to do that you need to
understand let's make a query run faster
or run slower so when in classic search
when you query there's going to be two
elements they're going to influence your
query latency so the way lastik search
works it when it queries it applies two
models one is the boolean model whether
a document answers the query or not
satisfies the query or not so that's the
boolean model that just basically go to
the index find the documents that match
the query and then get them back after
after elasticsearch does that it tries
to evaluate the score or the relevance
of the document and in most queries it
will do quite a lot of work to get that
done with using some mathematical models
and that we can put under the title of
the vector space model now depending on
what you trying to do that can be quite
expensive to do so think ask yourselves
very very strongly what where do you
really need scoring to be applied and
elastic search gives you some ways to
actually bypass that that scoring part
is if you need to be about that balloon
part it is important to understand that
all queries in elastic search and
basically in the screen which is what's
powering it under the hood are
essentially two types of queries one is
the term query term query is just this
is the term this is the field I want to
go to that field in the index see if
there is this term in that index under
that field and give give me back all
those documents because most of the time
I'll be asking about more than one field
or more than one term in one field there
is a way to combine term queries using
the boolean query which I'm sure all of
you have seen before but my point is
that all queries in elasticsearch
whatever query you are running
essentially going to be boiled down into
those two queries so whenever you are
running a query ask yourself what is
this going to look like after a rewrite
done by
elasticsearch behind-the-scene so for
example a wild card query F star bar
will be rewritten to whatever I have in
the index that satisfy this pattern and
then I'll have a big bullying query with
multiple term queries that would be
declared as going to be executed
so that's query here that you see here
is a very high-level
query that is not essentially going to
nobody says it's going to be fast or
slow which will really depend on how
many terms you have matching in your
index so that's just something to keep
in mind before elasticsearch 5 that was
the case also for numeric query how many
of you have moved to elasticsearch five
all right how many of you are
elasticsearch two how many of you are
elasticsearch one how many are not using
elasticsearch okay so elastic search
before a stick search five and numeric
queries and geospatial queries all of
those queries that are not operating on
string were in fact operating on strings
right so those numerix would have been
translated into some form of the string
that led that it makes it searchable and
those range queries on numerix would
have been translated to like older
queries with tons
sometimes even millions of term queries
and that would result in very slow
queries okay so that's very important to
know it has been changing on
elasticsearch file but before
elasticsearch five that's a pitfall that
you can definitely miss and this is
where you pretty much want to have those
queries a cached and be optimized as
much as possible for example not not
participate in any scoring in order to
do that you just want to take that query
and put it in a filter context that will
tell elasticsearch you're not interested
in scoring and that you give
elasticsearch each chance of actually
cashing it so try to find queries that
are quite intensive and
put them in in the filter in food filter
context which is part of the bull
inquiry before elasticsearch - that
would have been called a filter we don't
have that since since elasticsearch -
but it's a filter context ever since if
you're using scripts take into account
that it will slow down things a bit more
not only because of the language being
used because you can do that correctly
using the expression language or
painless language which are basically
compiled to bytecode so the execution
time doesn't matter but there is an
evaluation that happens on every result
and that's a bit slower or slower than
you would expect in without scripts so
scripts will also slow you down a bit
another thing to note is about paging
elasticsearch Lucine are not meant to be
used for deep paging if you are querying
elasticsearch and you're telling a sixth
search give me the first page which is
ten results by default and then you go
to the next page you are essentially
making double the search so the next
search on this for the second page is
basically telling elasticsearch do the
first search do the second search as
well and then only give me back the last
ten results this is how elastic search
engine work behind the skills so doing
ton of pages strong in size is not
recommended for normal operation if you
go deep so two three four pages it's a
no-brainer it doesn't really matter but
if you go deeper and deeper that's going
to be slower gets slower and slower as
you go deeper so this is why another
reason is that this paging is not stable
because every time I go to the next page
I am essentially running a new query so
that nobody guarantees that in page five
I will not see a result that already saw
in page four that's because things can
they may in the exchanges obviously so
some results can jump between pages
between my paging operations there is
two ways to
to avoid that or to make that work
better so elasticsearch five introduced
something that's called search after and
search after lets you basically quickly
scroll to the last known position in
using some low-level mechanisms and then
start from there
okay that will basically allow you to do
deep paging and perform them much faster
there is another way that's called
scroll and that's meant usually for when
you really have to have stable paging
meaning every page has its own and you
know that that page is definitely the
right page and also for highly
performing queries when you need to read
lots and lots and lots of results back
so think one hundreds of thousands and
millions of results back and then you
can do scroll starting in monastic
search five you also have sliced scroll
you can also do this in parallel for
multiple readers so it's really nice my
point is just try to avoid deep paging
as much as possible and if you always
what you do is just join search results
your user doesn't really need more than
three four pages max another question
you're doing search I guess you're going
to drink some full-text search you do
some analysis using some analyzers that
take your text break it down to tokens
and then normalize those tokens a bit so
do you know what we're on I'll just do
I'm asking because there is an
interesting case of the Swedish Kinect
are there any Swedish people in the room
awesome so you'll know what I'm talking
about
there is a Swedish Kinect thing that's
called that's basically a way to connect
or to do to shorten words right come
please correct me if I'm wrong I'm not
I'm not Swedish speaking so what
happened is I was at the client it you
know what let me start from the
beginning there is this unique
organization the Unicode organization is
basically is their job is to make sure
that we have standards for things that
related text they came up with a Unicode
standard that's called uax
29 that's the standard that basically
defines what text segmentation is
meaning um I have a text and now I want
to know what are the tokens in the text
in order to know that I need to know
what to talk in eyes on is whitespace
something I need to talk in eyes on as
punctuation marks is dot within a word
something I need to talk in eyes the
question is depends right because in
English dot within a word can mean an
acronym in other languages it is not in
Hebrew for example you have quote marks
within a word and that's actually legal
in other languages you don't Chinese
Japanese Korean what is a world boundary
and what is not so they set down and
wrote this comprehensive guide or
standard you can read it in the internet
it's quite comprehensive with lots of
examples and I went and said that some
client site and we had some issues with
the search and to demonstrate what we
saw is something like that we had two
words and the editor of the content
forgot to put a space between the column
and the next word so what happened is
using the elasticsearch default analyzer
which implements that Unicode standard
it basically produced this one token for
those two words after some investigation
and turned out to be just a private case
of the Swedish connect because the
Unicode standard defines again it people
smart people sit down and thought about
what would be a comprehensive and the
most correct very possible to tokenizing
segment words and tokens they also
consider the Swedish connect which as
far as I understand is in our cane case
to be something that they need to take
care of so they defined this this
segmentation case which is a letter a
column another letter from the English
alphabet to be in a whole token and not
tokenize on that column in the middle
so there are some more examples of that
of that Swedish connect but what what is
call this is places where your edit or
your text edit or forgets to put a space
between two words and you had a column
between those words it will just render
as one signal talking okay that you you
would never guess that that's what
happened and that's actually what
happens based on the Unicode standard in
pretty much every listing and elastic
search implementation allottee so ask
yourself really well do you know what
your under-eyes er is doing and that's
just one interesting example obviously a
sense I sent them an email asking them
to review that I never got a response
back I think it was like two years ago
or something
so that's that's that if you want to
know what you are now is doing you
actually have an end point to let you do
that you can just pass a text and
specify an analyzer name to to use for
to analyze the text and you'll get back
a list of tokens that will get into the
index another recommendation I have for
you that's not a default yet I'm pretty
sure it is going to get to the default
so default analyzers will do lower
casing for you which is quite obvious
because will you know usually most of
the systems are pretty much multilingual
today and we European languages have
accented characters and not every
keyboard or not every keyboard user know
how to use them so I'm recommending to
use the ASCII folding filter as well as
key folding will take letters that have
accented letter basically and translate
them or reduce them into their ASCII
equivalent based again on some own some
standard mapping and I'm recommending
the use of the ASCII folding for filter
on every analyzer and that's an example
of how to define a lile analyzer using
gas KeyFolio filter how many of you are
know and use and grams okay okay
so basically I've seen way too many
clients lightly using Engram
and I started to realize that people see
somewhere in some documentation
apparently an official recommendation of
elasticsearch to use engrams for some
scenarios and 99% of the cases where I
see engrams are being used that's
incorrect and there's better solutions
for that so whenever you hear people
tell you to use anagrams
stop don't do that and try to reconsider
so two reasons people are usually using
anagram to try and come up with a
suggestion algorithm or suggestion
feature to the to their search
so engrams because of the way they work
they let you tolerate spelling errors
okay so people do two things one they
start using Engram and second they use
instead of just using four or five grams
which will blow the index but not do too
much damage they will use two to ten and
grams which means they're going to bloat
or index I'm sorry from speaking too
fast and not explain what engrams is if
you don't know why it is better I'm
serious so instead of using Engram go
and use gestures which is just a way to
do that correctly using elasticsearch
API you can also do fuzzy search which
will pretty much do that for you in a
much smarter way all right so that if
you need suggestion so this so just
change or suggesters
just go use two gestures if that doesn't
help then maybe try to look at Engram
the second reason people using engrams
is because they have like model names
and stuff like that so they want
something that mimics the contains
search in SQL or whatever they came from
and instead of doing that the right way
they're using engrams to support
searches like that so I want to find
this ws 500 is like I don't know a
camera model so if I want my user that
if you typed 500 I'll be able to find ws
500
so using engrams for that and death
that's usually when I see people using
two to ten
instead of doing that check out the word
delimiter talking filter which will do
this in a much smarter way to just
tokenize on those changes so WS would be
talking 500 would be talkin Wi-Fi so
Wi-Fi and Wi-Fi so that's just done
correctly just a healthy when you debug
some some debugging tools for you so if
when you debug elasticsearch or
enquiries and you want to understand
what's going on behind the scenes we
have two things that are available to
you so quarry explanation can explain
why a result came back and why it got
the score that it got it's not very
readable I agree but it does let you
give you enough information mainly this
part here text current Fox which gives
you the field name and the actual talk
and that goes much that's very useful
when you want to read to understand why
a result came up and even better why
result didn't come up so you have two
ways to approach that in every search
request you can add a explain true and
that would add include the explanation
doesn't do that in production because
it's very slowing down the queries very
much and if you have a specific document
that you want to understand why I didn't
come back come back in a search in the
search requests you have the underscore
explained API that lets you send the
query and the document ad and get back
the explanation and that is very useful
to understand why the document didn't
come back so mostly useful for text
searches and very strong debug tool
another quiet question ooh a debug tool
in elasticsearch is the profile API it's
an API that lets you again ask you
lastic search about a specific worry but
this time not yet why a result came back
or not but what are the performance
metrics of that query within the elastic
search cluster so pretty much every
every component of the search is being
analyzed and measured for for latencies
another happy
this thing that's called named queries
in some cases you want to be able to
send multiple queries and get back
results and then know which of the
queries in the original query match the
result so if you have a boolean query
and you have multiple shoot clauses in
the result you can using name queries
you can get back the names of the
queries that matched each and every
result and it's very useful it can also
sometimes replace use of aggregations of
term allegations and just again another
hat tip try it out and so I have skin
because of some more content cover so
let's talk a bit about mapping so how
many of you have heard about in of index
templates alright mappings you know what
I'm talking about all right
so a mapping is the index schema that
that is what's going to define how the
data has been indexed what analyzers are
being used and stuff like that
what happens is that usually it's much
easier to define the mapping over that
it doesn't change after a certain point
in time just define define it in elastic
search and let elasticsearch do the
heavy lifting of creating the index and
applying that's nothing that's being
used using the index template so instead
of creating the index explicitly and
applying the mapping explicitly you can
use index templates which are you can
just treat them as documents and you
just put them the mapping until
elasticsearch template Colin and some
pattern and then every time an index is
being created using the that this name
matches this pattern the index template
will be applied to it you can have
multiple index template and will be
applied to it by some order that you can
specify so order 0 meaning this is going
to be the first one that's going to be
applied to the index that matches it
another thing you may not be aware but
there is a field that you never define
and maybe never use
called in underscore underscore is being
used usually by key balanced or when you
search on that bar just right type ward
it will go to elasticsearch and query
the old field if you don't use Cabana
you don't need it if you use Cabana what
happens is that every piece of data that
you put in an acid search is being
directed to the old field and that's a
blacklist approach basically so
everything is going there unless you
tell elasticsearch not to on a specific
specific field not to index it into the
old field in my opinion that's a bad
practice and I think you should do the
other way around I think we should have
some sort of a whitelist approach so
what I recommend usually is to disable
the oil field you can do that in an
index configuration and then use elastic
search mechanism that's called copy two
so you create the old field by yourself
and use the cop copy tool to do a
rightist approach on every field that
you do want to copy from your documents
to the old field for indexing and let's
talk about type so except from one use
case which I mentioned earlier the
parent-child use case there is hardly
any use case that I know of or that that
would justify using different types in
one index let me explain if you have
many documents and those documents are
in a certain have a certain schema all
right so if you put them in one index
you and that index is going to be your
manageable unit that is how you this is
how you scale right number of chars
number of replicas this is your
retention policy so if you're using
rolling indexes so you can only work on
a single index when you backup when you
delete and and do things like that so
it's it's your escalate in terms of
correlation see is your retention policy
this is basically the manageable unit
that you have you cannot do anything
with type types are just a logic unit
just separate mappings in of documents
within a single index so it doesn't give
you anything
there is no reason for you know to just
index it into another index and then
basically benefit from another
manageable unit and worst case you just
managing two different indexes but worst
case scenario which is usually what
happens now you are you have better
fine-grained control over your data
right so except from the parent-child
scenario which you have to have two
different types in one single index you
just you should prefer using different
indexes instead of different types in
one index let's talk a bit about
classroom management so you have your of
your data in somewhere in an own elastic
search and duct elastic search cluster
is hosted somewhere whether it's
on-premise whether it's on cloud
obviously you protected it and it's not
exposed to the web this is what we
already covered and let's talk a bit
about patterns and best practices to
maintain it so first of all if you're
using elastic search is a key value
store or is some sort of a caching
mechanism just don't I just move to do
this using a technology just that main
event for that the reason for that is a
elastic search doesn't have recharging
capability so once you need to charge
your data folder you cannot do that with
elastic search elastic search we just
require to do some sort of our indexing
and stuff it's not meant for that also
key value stores assume the data is
sometimes updated and a sixth search is
- it's not good it's not meant for
updates right so when you update data in
elastic search you basically do a delete
and another right the way elastic search
works under the hood it works quite hard
as as soon as it has many deletes and
does it's not meant to be used as a key
value store or to maintain something
that's frequently updated if you're
using elastic search for searchers and
stuff like that so when you need to do
deletes that's perfect
fine but if you're using an ethics
adjust as a key value store just get by
ID you you're better off doing it with
the Redis Cassandra and other tools that
are meant for that
speaking about clusters there is this
feature in elastic search that whenever
there is a change in the settings of
your cluster for example a new node is
being launched or one index started
growing too large so whenever the
cluster is a bit unbalanced using many
many metrics that elastic search
collects elastic search will opt to do
some sort of rebalancing operation
that's quite costly that means basically
a lot of i/o operations because now I'm
going on elastic search with recharge
and then just try to move them to
different nodes and swap charts between
nodes because because it it tries to get
to a balanced state it will not just
move one node it will always move one
node and then another no and not sorry
not no charge and then move another
shard to compensate for that
so rebalancing will actually gain curve
latency issues with your queries if you
have if you have a requirement for low
latency queries you should definitely
lock down rebounding and it's very quite
easy to do there is an API for that I'll
show you in a second even easier way to
do that although it's been throttled
automatically which I still see in lots
of scenarios where rebalancing actually
damages performance quite quite severely
in many clusters so just my advice just
disable it as much as possible I'm
speaking about charts there's one
frequently asked question is what is the
optimal shard size in my opinion that a
I just measured in two metrics one is
the number of documents per shard and
just as far as I know I can as far as I
can tell based on many scenarios I've
seen there is a sweet spot of around 1
million documents so once you have
around my 1 million documents
char did not index very sharp then
searches will operate very fast while
still taking as much optimization as
possible from you know the way the disk
did it stored on disk and stuff like
that and the second metric is the size
of disk so once you go past five to ten
gig a shard because shard is the unit
that elasticsearch manages in terms of
scaling out that would mean that's
killing it out or that moving in between
nodes or even initializing it into
memory when a node comes up it takes
quite a lot of time and over five to ten
to eight gigs that's taking too much
time so optimal size and disk is around
five to ten gigs and optimal size in
terms of document count is around ten a
one million documents as far as the
bugging tools I showed you some for
securing there are more voice noting
like a cluster allocation explanation
why discharge on this node etc some
explain api's some profiling api's I'm
going to skip that cops is a very good
tool for looking at your cluster and
seeing what's going on it's not
compatible with version 5 because
version 5 deprecated sites plugin which
is what cops we're using too was using
the guy who created cops created
something that's called cerebral I don't
recommend using that I actually
recommend using cops but using that
through nginx or some static HTML server
is because that's just static HTML it
allows you to see a good picture of your
classroom know what's going on and again
don't forget to lock it down under some
authentication mechanism as far as
classic topology goes make sure you
don't mix and match hacks or rolls like
so elastic search cluster should have
dedicated servers for each role so we
have three walls actually four but
that's it
we have three roles one is master or
master eligible you want to always have
three of them okay exactly three so that
would allow you to deal with network
partition if you have one you cannot
deal with network partition if you have
an even number you could get to a split
brain situation and everything above
three is just a waste of money
in most cases so have three master node
master readable node one of them would
be the master at every time a lastic
search will decide which one it is
that'll just have as many as you want
depending and sizing will depend on the
actual data that you're going to serve
and that is a bit hard to scale out so
master nodes can be quite laying
machines they don't have to be very busy
just have two cores one for gigs memory
that's enough data nodes are going to be
quite busy the sizing really depends on
how your data looks like and what is
your application factor that you want
amount of memory that you allocated to
the JVM really depends on your usage
nowadays you don't have to on version 5
don't really have to dedicate a lot of
lots of memory because you can take
advantage of dock values for those of
you both familiar with it so data node
is going to be pretty much a constant
number because it's hard to scale them
out mostly because of rebalancing then
you have the client nodes which you can
scale up and down I'll turn down because
it's very easy to do that
they are just start again standard knows
not anything too complicated and they
will be the one to handle traffic and
enquiries this scatter and gather
geographic distribution don't do that
just stay in a single region in a single
data center I've seen clients who
actually go and create it elasticsearch
cluster on geographically distributed
not a good idea take a look at red nodes
if you need to do that or there is more
ways to do that and I'll be happy to to
have a offline discussion about that
unfortunately we don't have much time
left so I'll pretty much skim through
this one this is an original this is
these images from the elasticsearch
documentation and it talks about the new
way to ingest data I think the change is
something that's called
ingest notes I don't really recommend
using them because I prefer all the
ingest work to be done on dedicated note
and this is what a locust ash 5 does
locust ash in general does and also 5
does it in a much better way way more
optimized the rewrote some stuff my my
preference is usually to have ingestion
data ingestion and parsing stuff like
that's done externally to the clusters
so if you heard about ingest mode that
would be my recommendation
so more recommendations here I'll post
those slides there so so you can see
them I hope you enjoyed and thank you
very much and don't forget to protect
your clusters and again if you need any
help with that feel free to reach out
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>