<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>State of the .NET Performance - Adam Sitnik | Coder Coacher - Coaching Coders</title><meta content="State of the .NET Performance - Adam Sitnik - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>State of the .NET Performance - Adam Sitnik</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CSPSvBeqJ9c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello everybody thanks for coming
to my talk my name is Adam Sydney and
today I will speak about state of the
dotted performance Who am I I'm one of
the core contributors to benchmark
dotnet is the most popular library for
benchmarking dotnet code I also maintain
the awesome dotnet performance list and
I contribute to core CLR and Cora fix
lab I work for Paulo it's Norwegian
company so it's very special for me to
speak here today and also because we
make things in other work my main goal
is to share the knowledge review and to
speak about all of the new things I'm
not going to speak about these
improvements I would like to show you
the new shiny things that you can use
and make your applications faster but
one very important update on Monday I
have realised that David solar who has
implemented pipelines our God was going
to speak about pipelines have you been
to his doki's today and the audience was
full so instead of pipelines I have
decided to take this time and speak
about the future of 7.2 which is super
cool in terms of performance so let's
get it started it might be surprised
that I start with valuable because it's
just a tuple it's very flexible and we
no longer need to create a dedicated
pipe when we want to return more than
one thing from a method but the key is
that it's a value type and there is a
huge difference between value and
preference pipes and it's a key to
understand it to have a good performance
and today I will speak about few value
types so this is why now I'm going to
briefly describe the differences and to
make it simpler I will talk about value
tuple and tuple of two integer fields so
first of all we have an extra overhead
each instance of a reference type
contains two extra field
pointer to the metal table and object
header so for example when you take a
lock on your object CRR is writing
information about this to the object
setter when you
invoke visual method it jumps to metal
table and then it jumps to the
implementation of your methods also
reflection is using this but the point
here is that we have extra overhead in
terms of memory so for two simple
integer field we have tripled the amount
of necessary memory for 64-bit
architecture which is recently the most
common on when it comes to value types
we don't have any extra overhead
what-you-see-is-what-you-get but they
are more limited you cannot take lock on
them they cannot say finalized errs and
you cannot vary from them so it comes
for a price but to understand how it
affects the performance we need to
understand how CPU works so when you
want to read something from memory CPU
is first of all looking for the value in
the cache in the first level of cache if
data is not out there it goes to the
second level if not there it goes to
main memory but this has different
prices so accessing first level half of
a nanosecond second level fourteen times
slower going to main memory is 200 times
slower and CPU knows about two
principles of data locality first one
temporal which say which says that if
given value is being used right now it's
likely to be used soon and second
spatial which says that the things that
are located next to each other in memory
are likely to be used together so think
of for loops when we are iterating we
use first one second third fourth fifth
so CPU does some optimization when you
access the first element of the array
and the data is not present in the cache
we have a cache miss and this copies
whole cache line from main memory to the
cache because it predicts that you will
need the other values very soon so
usually the size of a single cache line
is 64 bytes and when we try to fit value
types and reference types of our example
type in single cache line we can see
that we can fit eight instances of value
type and less than three instances over
ference type and it affects the
performance I decided to write a
benchmark for you with the help of
benchmark net we have we are using
harder counters feature and we are going
to look for cache misses the benchmark
is really simple we just iterate through
an array or value types and reference
types and we just add both fields
together so we are just touching the
fields so it's more like a benchmark of
CPU cache but how do you think for a big
data set how huge the difference can be
in that case for 100 millions of
elements doing this with reference types
was more than three times slower and as
you can see the cache misses were were
more than three times common and this
affects performance this is really
important to understand modern CPU
architecture because data locality has a
great effect on the performance and
other great benefits of value types is
that they are not affecting garbage
collector so when you allocate a local
variable which is a value type is
invisible for garbage collector because
it gets allocated on the stack if you
allocate an array of integers then
garbage collector allocates an RI on the
menu skip and the integers I are inside
they are nested but it doesn't care for
the content so they are transparent for
garbage collector so anytime you create
value types and you don't box it you
don't put any extra pressure on the
garbage collector and you don't allocate
any managed memory which is very
important if you want to go really high
performance and of course for the stacks
allocated value types we have data music
delegation when the method ends the
stack is we just unwind it and
everything is being wiped out but of
course there are some disadvantages as
when you take a look at the basic class
library most of the types are crosses so
can you name few of them
so by default they are being sent to
methods and returned by copy and it's
expensive to copy if you have a
structure that has five fields and you
just send it to another method all five
of them are going to be copied when
you're doing this with reference types
only value of the pointer is being
copied single sink so the coping is also
not object when it happens let's
consider this example we have read-only
field which points to value type and we
call just a method on this field and
what c-sharp compiler does it omits
following code for us because it can see
that the field is read-only so that it
should be immutable but inside of a
value type you can access this keyword
so the method could use this keyword to
change its own state this is why c-sharp
compiler is emitting curve like this for
us to have the immutability so there is
a way to avoid copying we should do
things by references and c-sharp 7
offers us the way to read references
from methods still access the references
and create local references as you can
see the code is quite complicated and
for those of you who believe in clean
codes and want to maintain the code
clean it's really important that you
should use it when you need it so when
you've been providing your application
or designing your it for performance and
you are sure that it is required to use
references then you should be using them
you should not use them by default
anytime you can and let's run some other
benchmarks let's iterate to an array of
big structures and just initialize them
big structure in this example is five
fields so when we do it by copy we take
a copy from our I work with local copy
and then we put it back to the array so
we copy it twice but when we do it with
references
we dare efference the our item and work
with local reference we don't copy any
memory and the difference is quite huge
in my example it was more than four and
a half times faster to do it was with
references and I have diffused raske
words twice and as you can see on the
chart we can see all the deeds and the
new deed reuse it and the legacy deeds
and can you tell me how is it possible
that this feature work with old deeds
have Microsoft released apart for
Windows no this feature is implemented
this c-sharp language feature is
implemented on top of CLR feature called
managed pointers so it's just about
exposing an old existing feature that
wasn't used before it was also of course
about implementing all of the safety
rules for the pointers but some of you
can say that we could previously also
used pointers in C sharp because we
could go unsafe we still can go and face
the code isn't pretty at all we have to
switch some safe context we have to pin
object in memory and we have to play
with am presence and stars so let's
compare the unsafe way and the safe way
what I got was that doing it in the safe
way was faster because I didn't need to
pin object in memory and pinging has
another overhead for garbage collector
because it has an extra work to do
because in the first phase of garbage
collection its compacting the memory so
it might be used moving objects in
memory and moreover
we didn't need to go unsafe when we
doing this in the safe way and I didn't
know that unsafe have limitations and
years ago I've been running a demon ever
and it has failed because the default
setting didn't allow me to have a full
trust so if you want to
your coat everywhere you cannot use
unsafe because somebody might use it in
environment with a limited trust and the
coat wheels are failed during the
runtime execution so it can be a no-go
as of today it's very hard to find some
knowledge about the manage references
and valid people's but one of the core
members of Rosslyn team has recently
started blogging and as of today in my
opinion this is the best way to learn
more about this so if you'd like to
learn more about references you should
go to this others which is quite easy to
remember must override calm and just
learn from the guide that has actually
implemented the sitter's so to sum up I
see shop 7 performance value types have
better data locality and they don't put
any pressure on garbage collector and as
you know no DC is better than any DC you
should use value tuples to have clean
code and great performance and you can
use references to get rid of unsafe and
to avoid copying of volatiles
speaking of unsafe have any of you ever
used stack alog okay
one person so psycho log is a keyword in
C shove that in unsafe context it allows
to allocate memory on the stack it's
very fast for small tanks of memory it's
almost 2 times faster than new so it
would be nice to use it but when we
compare all different types of memory
and we have small managed objects large
objects manage objects like allocated
memory and unmanaged memory allocated on
the unmanaged part of the heap we can
see that all of them have quite fast
allocation but when it comes to the
allocation there is a huge difference
because the native memory can be D
allocated on demand in the terminus D
way so you can actually predict how your
code is going to behave
when it comes to manage memory we have
garbage collection collector which
implements a heuristic so it decides on
its own when it's going to clean up the
memory and when it does so it's
suspending all of the active threads of
your application so it's doing nothing
else but just cleaning up the memory but
on the other hand the managed memory is
very easy to use
we have innumerable extensions which are
known as link you it's common it's safe
and unsafe memory have you ever used in
public API dot exposing unsafe methods
it's it's really not very common but now
people at Microsoft they want to - not
me they want to make dot as p.net
correct competitor to other cloud web
frameworks so they need to go they need
to utilize full power of whatever they
can take to be in the top of the Tekken
power and the other problem that we have
when we want to support all types of
memory is our our API so if you want to
just accept the NA right it's like this
what if you want to take a part of the
array you accept an RI start index and
the lands ok and now let's handle the
unmanaged memory we have four methods
and what if we want to specify input and
output so quite many of them maybe this
is why very few public API s have
something like this and do this is not
of course full so the solution is span
span is a simple type that allows us to
work with any type of memory stack
allocated memory memory from the
unmanaged heap and memory from the
managed heap it's fully type and memory
safe so we don't need to worry about
safety and it has almost no overhead
because it just about very simple
pointer arithmetic and how do you think
is it a value type or a reference type
yes it's verified it's stuck only value
type and I will talk more about stuck
only soon so when I said that it
supports any kind of memory we can
allocate memory on the stack create
panels of it allocate memory with the
marshal IP I just use the pointer and
creates panels of it we can create it
out of an our eyes and we have even
implicit Castille pareto so you can just
assign a light to span in boom you have
spam and as you know strings are
immutable in the pet so in order to
create spam out of string we need to use
read-only fun which is immutable the API
is very simple I would recommend to
think of it like of an RI which can
point to any type of memory because it
has the length indexer which allows us
to get and set elements it also allows
us to do slicing and this is super
important because it takes part of
memory without copying them because it
creates a new span with does a different
pointer it does not copy any memory this
is one of the most important features of
Spawn you can also clear it and fill it
or copied somewhere and we have one very
nice method which also named dangerous
get enable reference if you need to pin
the span you should give this method but
I believe that the method might change
the name might change
so previously when we wanted to support
all kinds of memory with input and
output with all the variations we could
have something like this which span we
there have method that accepts one span
or two spans which is input and output
so it simplifies all of the public API
and span supports existing runtimes is a
new shiny thing for dotnet core but to
keep us all happy microsoft has decided
to support dotnet standard 1.0 so you
can actually use span with your old
existing desktop doesn't applications
that targets
something like dope net 4.5 or something
new but we had two different
implementations one for the existing
runtimes and one for the neuron things
the one for the existing runtimes has
three fields so let's say that we want
to create a span out of the second half
of the RI then the pointer will point to
the beginning of the RI the offset will
be the amount of elements to the middle
and length will be does the length of
the second half
why don't we merge pointer and offset
because the all the garbage collectors
existing ones wouldn't know how to
update the pointer when the memory is
being moved during the compact phase so
the new runtimes days have merged it as
of today only dotnet score 2.0 has
native implementation for span so
they've been just nursed
it's super simple to field and some
people call these two versions slow and
fast spun I have decided to write some
more down the benchmarks for you and in
my opinion the most important part part
of span is index or getter and setter
because what you are going to do is span
all of the time it will be just
accessing the indexer the volleys so
we've benchmarked on that I was able to
compare the version for doesn't 4.6
for.net core 101 and for.net core 2.0
and as you can see that difference is
not really huge it's from 12 to 13%
slower so it means that you don't need
to exclude span for the existing
runtimes in terms of performance because
it's just very fast and one very
important note there is someplace for
further improvement although
our assumption and the goal for span was
that it's going to be equals in terms of
performance with the RA and I have
measured it the red bar is a line index
together the orange bar is span index or
Gator so as you can see they're on par
so this is important but valid only for
dotnet core 2.0 which has the native
support for span for the old runtimes
we would see these numbers again and as
I said previously the biggest advantage
of span is the possibility to create
slices without any allocations so
previously in dotnet when you wanted to
take only part of given text a string
you had to use the substring method
which was allocating new memory and
copying the part of the string to your
memory and we've spun with just do
simple pointer arithmetic when you want
to take only the first part of this of
this text dot earth core what we do we
create a new span by calling slice and
what slice does it sets the pointer to
the beginning of the string and sets the
lens to nine so it's too super simple
operations and when we compare the
performance in this very simple scenario
we can see that substring is like three
times more slower and it was allocating
memory and one important thing as you
can see here we are always going to play
with pointers and the length so it's
size independent because we are not
copying anything so actually the cost of
slicing will be like constant in time
the reason like why really Microsoft
needed spun was that they have profiles
a lot of web standards web server
scenarios and they have realized that
they allocate a lot of memory when it
comes to formatting and parsing
compression and decompression because
what we do with web requests we parse
the web request headers then we format
something we play with Dyson's and stuff
everything is about strings encoding
decoding compression the compression as
of today this API vests are not
production ready they are incur if Excel
up I would say that they're quite good
but not polished enough but what I would
like encourage you to is to go to
graphics lab and if you would like to
shape the next version of.net you can
actually do this so you can actually
help others to create the new I to make
it happen I believe that such chance in
the world of dotnet wasn't given to us
for the last in the last ten years at
least span has a serious problem
it's called struct Turing so every
architecture has offers us atomic
updates but usually it can update only
one thing at a time and the thing has a
size limitation when you take a look at
the interlocked class you can see that
it accepts bytes integers shorts Long's
and also pointers but nothing bigger
than that and in span as you know we
have at least two fields so it's
impossible to update both of them in
atomic way and let's consider this
example we have public class which has a
private spun field and we have a public
method that allows to update the state
and another public that allows to read
the state and we have two threads and
state of the memory when the first
thread decides to
at the pointer to a and length to 100
first thing it does its points to a so
the value of memories a and 0 because 0
is a default value for integers and then
in another set another atomic update it
says the length to 100 okay
but now we want to set the pointers to
set to be and the lens to be 1 what we
do is another atomic update and we have
new pointer and all length and the real
problem begins when other threads what
once you won't want to read at the same
time because what it can see is B and
100 and we just wanted to set the max
length to 1 so when it tried to read the
10th element of the buffer it should get
our eye index out of bounds exception
but what it got it read a random value
from memory and how can this problem be
solved maybe we should synchronize
vectors no synchronization is slow if
you ever designed for performance and
you think that you need to synchronize
something I encourage you to do some
brainstorming think about this for a
while and try to find different way of
doing things the solution is to make
span a stack only type so what does it
mean it means that it can live only on
stacks and stacks are not shared between
multiple threads so only single thread
can access stack at the same time no
need for synchronization
moreover GC has a little pointer few
pointers to track because the stacks are
short list the methods begin and end
stacks is being unwind it so at the same
time we will just have few spawns on
stacks moreover we have safe concurrency
and no need for locking and we have very
safe life time because when we allocated
memory on the stack we've stuck a log
and we have stuck allocated span and the
method and the
stack is being unwind it and everything
is being cleared out the problem is when
you have pulled the memory this is your
responsibility as the developer to
return it to the pool before the methods
have ended but stack only means that we
cannot put it on the heap and this is
where real struggle begin let's consider
this example we have method that accept
something that implements ionomer
ability and we have a value type that
implements this interface and then we
just create an instance of this value
type and we send it to given method as I
told you at the beginning value types
don't have the method table pointer so
when you want to invoke a visual method
owed on them in the non constraint way
what compiler does is emits a boxing
instruction which boxes the value and
put it on the hip and gives it the
method table pointer and now we can use
it as the interface and the problem here
is that when we bug things they are
being put on the hip and hip is not
stuck so stuck is preventing us from
implementing interfaces in that case
this is White's plan is not implementing
any existing interfaces like ienumerable
for example because somebody could by
accident box it and put it on the hip
and it's not the end when we use
async/await it's great for our code
productivity but what fish of compiler
does it emits async method builder which
creates basing state machine and at some
point of time it might put all of the
parameters that you have sent given a
synchronous methods on the hip so this
is why we cannot use buns as a
synchronous method parameters moreover
they also cannot be filled in non stack
only types because they would be
allocated on the heap one day
aidan they also cannot be generic
arguments a long list of limitations but
only when it comes to passing them as
arguments and when it comes to generic
arguments and Microsoft has few ideas
how we can make still span usable for
everybody so the idea is not
production-ready yet if you take a look
at documentation then dimension
something like memory if you take a look
at the code
there is no sign of memory type it's
called a buffer now and it's work in
progress they work very hard the span is
being used in pipelines right now and
really pipelines are going to shape the
API of this pump and the idea to
overcome this problem is something to
have a type that has no limitations it
can be sent and put anywhere and it will
be our own demons Button Factory so when
you want to use spun you just create a
span which will create the span for you
in for example synchronous context or
wherever you need it and sample
implementation could have a pointer to
unmanaged heap and just when you create
it you create a span from the pointer
and when you are done using it you call
this pose which is freeing the memory
the same comes from for the pooled
memory we can just return it to the tool
but as I said this API is not production
ready yet and the types are called
buffers and own buffer and reference
counted buffer if you'd like to find out
more you can go to karate club in dotnet
repository and check this out so let's
sum up the span it allows us to work
with any type of memory it's very fast
it's small it's just an abstraction for
pointer arithmetic it allows us to
create slices without copying memory
which is very important and it allows us
to work with unsaved memory in very easy
way
it also supports dotnet standard one so
you can use it in your existing
applications
it's on par with RI when it comes to
performance and but it's limited due to
the stack only requirements which can be
worked around now I would like to play a
video for you
it's video from a game game is called
age of ascent is being developed by
literate games the backend is being run
on dotnet girl on Microsoft Azure and my
question to you is following what has
just happened
yeah you were right DC let's take a look
at the hip of the menaced process so we
have two types of objects small and
large anything that is bigger than
85,000 bites is considered to be large
object anything smaller is considered it
to be small objects and small objects
are allocated on the Jen vo when they
survive the first collection they are
being promoted to join one if they
survive another they are being promoted
to dental and this model works great for
short living objects if you think of web
application web request starts and ends
and most probably any memory that you
have allocated during handling of this
web request should be cleaned out this
is why ginger is very small to have very
fast collections of small space of
memory but the long living objects are
supposed to live engine 2 which is very
big and it's really time consuming to
clean it up and we have large object
heap which is a separate part but the
real problem is that every large object
is marked out of the box as Gentoo
object so when we run out of memory in
large object heap VC performs full
garbage collection and this is what we
have the scene full garbage collection
in production what can we do about this
we can just pull the memory this very
simple pattern to pattern and finally we
have good implementation that we can use
and rely on RI po is full of raisable
managed arise it has a default Mac size
it's important I will talk more about
this soon and it's available in the
system that buffers pocket it's been in
production for more than a year you can
use it don't worry and use it it's very
simple first of all we need to obtain an
instance of an pool we can do this with
shared property which returns us
acceptable with default max price if you
want something more arable has a create
method which allows you to create your
custom pool
once you have it you rent the buffer and
you specify the minimum length what you
get will be most probably bigger than
what you have asked for you just use it
like you used it before when you are
allocating our ice with new and after
you are done using it you just return it
to the pool and yeah let's take a look
at some benchmarks let us don't trust
the theory when we compare 100 bytes we
can see that allocating was faster when
we get to 1,000 bytes it almost equals
but when we go up and it's 10,000 bytes
we can see that allocating was much more
much slower by nine times slower we
don't want to get here but when we read
the limit of large object so it means
that 85,000 bytes here we have 1100
thousand bytes we get then two
collections the red color this is what
we want to avoid so we should be
definitely pulling large objects and as
of now you should observed that the cost
of renting and returning is almost
content it's always from my box is
something around 32 nanoseconds so it's
size independent so you can also predict
how long your code is going to take to
aggregate but when we reach the max size
of pool it has behavior that you maybe
wouldn't expect but when you think of it
it's kind of obvious and you ask for
something that is bigger that the max
size of given pool it's just a locating
URI for you every time and when you want
to put it back it's ignoring the
reference to avoid some synchronization
in general so you want it to save time
but you have actually spent more time
and you have allocated the memory the
solution to avoid this problem is that
you can create custom pool that is aware
of the size although I said previously
we have the create method which allows
you to create a bigger
pull of bigger all right
did somebody rise event no Microsoft
wants to help us to avoid this problem
and we have an extra event for that even
even so we have new etw even provider
which is called system buffers are I
pull event source if you want you can
use purview to profile and to what for
this event what you will see is output
like this and what you should be looking
for our buffer allocated even this is
the thing that we want to avoid we want
to pull memory you want to see buffer
rented even tougher returned in equal
count so every large object is
considered to be gentle object-- which
means full garbage collection when we
run out of space in what objective are I
pool was designed for the best possible
performance it was tuned by people like
young Coty's and Steven taupe and then
Adams I don't think that we can go any
further and support for pulling managed
memory and you should use shirt by
default it's important our goal was to
keep the last object hit small when you
create too many pools too many custom
pools you will end up once again with a
lot of large objects so this is
something that we want to avoid and this
is why most of the libraries that are
commonly used like kestrel is p.net core
image sharp they using the shirt pool so
they don't create any custom pools in
your code
and the other problematic scenario that
was found when Microsoft was profiling
their web application is following we
have a method which is a synchronous
it's executed very very often but most
of the time it could be executed in
synchronous way but all of the time when
it could be executed in synchronous way
we allocate tasks and tasks our
reference types so we create an extra
overhead for garbage collector if you
want to some to get some real example
you can think of a network stream it
downloads whole chunks of memory let's
say 4 megabytes and you might have an a
synchronous JSON the steriliser working
with the stream so most of the time
stream already has the bytes for the
days on the fertilizer so it could
provide the data without any allocations
but previously it was impossible and it
was always allocating new tasks when the
results when the data was already
present was already waiting to be
consumed the solution for this case is
to use a volatile volatile is a
discriminated Union which means that we
can create value tasks out of result
something that is a value is present or
from a synchronous task so it can wrap
non executed tasks and it's supposed to
help in this single scenario
unfortunately valid us doesn't have any
other usages this is why I believe that
mostly people who work with networking
and creating software for other
developers will play with validus but
it's important to understand how can we
take full advantage of the latest if we
are in this scenario so first of all we
need to take if synchronous execution is
possible if it's possible we call the
synchronous method and we just return a
value task with given result we didn't
allocate anything on the heat when it's
the opposite we need to call the
synchronous method we
will allocate a synchronous task and we
just wrap it into a valley task and by
default this method is not going to be
inlined because it's going to be too big
and it's returning a big value type so
you might need to apply aggressive
inlining attribute if you also would
like to gain benefit from inlining and
how not to consume that the value task
we cannot simply await it because
c-sharp compiler will once again emit a
lot of code behind for us and it will
make the code complex and we wanted it
to be very simple for our hot path the
solution is quite trivial we just call
the method which returns a task value
task and it's possible to be in line so
we have another benefit and we check for
the footpath is it completed okay I just
take the result otherwise I await the
inner tasks and here comes to play the I
think run all state machine context
switching and all of this so we were
interested in the hot path and
unfortunately we have to do it this way
like manually if we do it in the wrong
way
as I told showed you on the first
example it's actually going to take more
time that when we do it properly so it's
a trap and as you can see the benefit is
that we don't allocate any manage to
memory the difference in a time isn't
that big and this benchmark was just
about the super positive example for
measuring the overhead so it means that
in all of the cases I knew that I could
execute in synchronous and I was
returning simple value so it's just
about measuring the overhead and as you
can see on the right we have done zero
collections so for tasks there were some
Ranger collections but they were also
some gen1 collections and why did we
have done one collections so tasks
reference types and they have finaliters
to handle unhandled exceptions so
anytime a DC decides of something is a
garbage but it has a finalizer it's
promoting it to the next garbage
collection generation and it puts it to
the finalizar queue after cleaning up
the memory the finalize errs are being
executed and then the gen1 can be
collected in next clean up so they
survive at least one generation
collection so it's important to avoid
creating a lot of them if we can in high
performance scenarios it's important to
remember that it's not about replacing
task is to about to help in this single
scenario where we can avoid hit
limitations and maybe we can get some
benefits of the method inlining
and to do so we need to play with the is
completed property and the result
property yeah so the method is
implemented in a way that it takes
if task is no if it's null it checks it
returns true which means that it was
created for a value and one very
important thing value test is a part of
core effects and it's open source so we
can analyze the code on our and there is
one extra feature for the performance
kicks so as you know c-sharp is being
compiled to IL code which offers us a
lot of feature but c-sharp offers us
only a subset of the things that IL can
do some of them are sometimes required
when you want to go really high
performance
for example you would like to do managed
pointer arithmetic previously we didn't
have managed pointers in C sharp so this
methods are not exposed in c4 and this
class is a simple single il file
compiled for you and which you can
consume in your c-sharp code so if you
ever want to get something from IL that
is not available in c-sharp there is
this new class which exposes most of the
possible methods for example if you want
to copy memory it omits the copy bulk
instruction which is supposed to be
implemented by runtimes and which is
supposed to be the fastest way to copy
memory another example if you want to
get size of any type then in c-sharp we
have a sizeof operator but it works only
for primitives like integers if you want
to get sizeof of your custom structure
it doesn't work so but as I said it's in
a situation when you really want to take
full advantage of dial who of you have
been today to Adam talk about Dublin
standard ok and who of you is developing
up libraries for other developers ok so
we have quite a lot of app developers
and also some libraries so previously we
had when you were trying to publish a
nugget package you had to target all of
the frameworks that you wanted to
support so if you wanted to support
dotnet 4 ok this was your first target
what about mono another target
what about xamarin another target and
here Microsoft wanted to add few more
frameworks dotnet core 1.1 2.0 Universal
Windows platform so what the people
should be now targeting 6 of them know
and Microsoft has decided to like
inverse the dependency and you can think
as of double standard
of a set of interfaces and now every
framework is supposed to implement a
standard and as a library elder you just
target the standard and your code can be
executed on any framework that is
component with the standard but if you
are just a library consumer and what you
need to know is that you need the
lightest Visual Studio and the lightest
nugget client and then your tool chain
will understand what doesn't standard is
and you will be able to use all of the
new things in your existing desktop
dotnet applications and the good thing
is that all of them target almost the
lowest standard only system buffers
which contains our actual target 1.1 and
as I told you previously I was supposed
to speak about pipelines but David did
so I decided that I will take this time
and speak about something else and
recently I've been watching Microsoft
built and the PM of c-sharp said
something like this during his talks we
sort of won't push unsafe code out of
c-sharp in a way or more to the corner
having more low-level code that is safe
and efficient and at this moment I have
smiled and then he said something like
this you can go to the github and see
what is running there so I decided that
I will analyze all of the issues
available documentation language design
meetings notes and prepare a list for
you of all of the new possible features
that can help us in terms of performance
but one very important feature I think I
will now speculate so I'm not Microsoft
employee I'm not promising you anything
I'm just speculating and I'm just saying
about things that I have found they are
in the official repository but there are
plans they can be changed
first Peter maybe will be able to define
stack only user-defined types so will be
do will be able to do so by applying the
rescue word in front of type definition
what compiler is going to do is going to
meet is res like attribute but how can
they prevent the old compilers from
using this new stack only types which
need which needs a lot of safety rules
to make sure that everything works
correctly the current idea is to also
omit obsolete attribute which will trick
the old compilers because when they see
the obsolete and it's ref like they
think okay this is obsolete I cannot use
it the new compilers will be able to
ignore it this is the plan I've seen
that a lot of people don't like this
idea and I also have an idea what would
happen if I as a user would define stack
only type and then decide to make it
obsolete
another thing so maybe some background
information when I say that CLR offers
of type and memory safety we need to
understand how is it done so when you
execute your il code for the first time
CLR is doing il verification so it
verifies the il and it makes sure that
you didn't write il by hand and didn't
produce any code that can do some magic
with pointers and to create any problems
with memory in that way in a truce
memory safety when you go unsafe you
have no longer that Gandhi and possible
plan is to make stack only spans to make
it possible to allocate memory with
stack a log and make it in safe context
without using the unsafe the problem is
that the compiler will need to implement
a lot of safety features and also take
responsibility for making it work for
all of us in all of the possible
scenarios so this is why I have put
three question marks here another
feature I believe is that this is one of
the rather obvious features as of today
we can take a reference to a field but
it's impossible for read-only fields and
this is why a lot of people break in
with ability
for example if you take a look at the
another time some of the value types
fields are not read only but they should
be just to avoid clapping of memory so
we will have a new set of keywords read
only ref which for the incoming
parameters might be shortened it to in
so we'll be able to take read only
reference of a read-only field written
and only reference architect read only
reference but we will not be able to
create a read-only local reference
another thing which was already merged
but to a brand is read-only structures
so we will have the possibility to
define a read-only type and from now on
as I told you previously in any method
if you have a value type you can use
that this keyword and you can use it to
mutate your own state so this is why for
now see sharpest emitting code like this
to avoid copying to make sure that if
you will be really in new table so
nothing bad happens here
so we've on read-only keywords the type
will be read-only and this keyword will
point to read-only reference so you will
not be able to change anything so I also
believe that it's not only for
performance but also for making
immutability a first-class citizen for
the c-sharp language and it might solve
a lot of issues that we have so far
this feature is quite trivial let's take
the extension methods for value types
ref like so we don't need to copy when
we want to invoke an extension method
and by the way this is possible as of
today in Visual Basic
so Visual Basic in at least one case is
better than C sharp as of today and now
bleah table types so I told you that we
have unmanaged memory and managed memory
and we cannot put reference types to
unmanaged hip because when garbage
collector is updating pointers it would
not know that some of the pointers needs
to be updated
and we can put some very primitive types
on the unmannered tip and they are
called bleah table types so we might
have the possibility to have the table
user-defined types and we are going to
have a compile time verification of all
of the safety routines as of today the
idea and the proposal wasn't merged
there is still ongoing discussion
but what I as a performance I would like
to do with this is to extend one of the
span methods and to be able to cast tons
of laundry table types to another types
we table types and think of this you
have a network stream which has an array
of bytes you create span out of this its
span of bytes but you know that
internally it's representing value types
bit 3 table value types so with simple
operation like cost we would like to be
able to get our types without any memory
allocations does with few pointer
arithmetic and few math operations
without any allocations so let's sum up
my talk I would like you to start using
value types more often please use
references to avoid copying and try to
use them to get rid of unsafe to be
fully safe you should use fun to avoid
copying when it comes to slicing when it
you can use it in parsing formatting you
can help core effects lap you can start
to contribute and make it happen faster
moreover you can also use span to take
full advantage of the unsafe memory
because it's an abstraction you don't
need to care what kind of memory it is
you should also pull the large objects
it's very easy with our I poo you can
use a value test but only in this single
scenario unfortunately and if you want
to go out of C sharp and useful I yell
power you can use the new unsafe class
and please please support feature point
seven point two ideas it's very
important I'm one of the contributors of
few repositories and
it's really cool when people click
thumbs up because then the maintainer
ZnO that somebody likes this idea and
would like it to be implemented it's
very important is the first step for our
dopant ecosystem at least say that you
would like something at least click once
it doesn't cause anything I'm not asking
you for any donations so here is the
list of all of the sources which you'll
be able to find under this address I
have put all of the places where I took
knowledge from and if you would like to
learn more about the benchmarks here is
the link for the benchmarks so thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>