<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building for Resiliency and Scale in the Cloud - Scott Allen | Coder Coacher - Coaching Coders</title><meta content="Building for Resiliency and Scale in the Cloud - Scott Allen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building for Resiliency and Scale in the Cloud - Scott Allen</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SFLu6jZWXGs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody such enthusiasm before
lunch
my name is Scott Allen I wanted to talk
to you a little bit about what I've
learned over the last few years trying
to build applications that deploy into
Microsoft Azure that tried to be highly
available they tried to be resilient
they tried to keep operating even though
the seas are stormy and things are
failing around them and I just want to
preface all this by saying this is very
much thought I had a picture of money
there we go a business decision it's not
a technical decision if you want to be
highly available or not because if
you're trying to be highly available
you're trying to exceed some SLA numbers
from your cloud provider and as soon as
you do that you can just start hearing
the dollars or pounds goodbye because
you're going to be using more resources
and not only just operational costs is
more expensive you'll also be designing
more complex code let me fix these
slides real quick
they think they're in some sort of a pub
comp configuration so they try to
automatically forward and hopefully that
takes care of the situation rebooting
see if that stays up and I can give you
a good example just less than a month
ago or about exactly one month ago I was
working with a company and we're trying
to determine how much uptime they need
and I said you know given the resources
you're using you'll probably be about
99.9% uptime and I said oh that's
horrible we have to do better than that
and I said ok oops we're still advancing
at 99.9% uptime your operational cost to
run what you need in Azure is about
$20,000 and you know here's an alternate
configuration that will give you better
uptime but it's going to be $60,000 a
year to operate and they said oh well
we'll get back to you about this so it's
very much a business decision and as
soon as I get this operational hopefully
it won't advanced on me
so we'll talk about how to be highly
available which is somewhat closely
related to disaster recovery you'll see
the two have a lot in common because if
you're trying to be highly available
you'll typically have redundant
resources available if something fails
over and since you have redundant
resources available like maybe a
redundant sequel server instance that
means if you also need to do some
disaster recovery plans you can factor
in these things that are copied all
around the globe your pieces of data
that are copied into replicated data
structures so one of the most important
things about building a highly available
system is to understand the cloud
provider that you're working with and
the specific features of that cloud
provider are the different resources
that they provide that will give you
high availability so if we were to break
out the different features inside of
Microsoft Azure I'm not going to list
all these but I usually categorize
things as infrastructure services
platform services or security and
management related services and if you
think about wow I'm just trying to build
a web application there's a number of
different ways I could deploy a web
application and that these different
services so under infrastructure
services under compute one way to deploy
a web application would be to provision
virtual machines inside of azure or AWS
but we're talking about a shorted a
provisioned virtual machines deploy my
applications on IAF's or under Linux and
I could also deploy a web application
under Microsoft service fabric I could
also deploy a web application under an
azure app service as a web app so three
different choices and three different
implications for how I'm going to
architect and design and look at my
Asscher capabilities depending on which
of these I select so it's important to
understand the different resources in
kasher all of them come with different
service level agreements for starters so
one of the first things you do when
you're trying to build something that's
highly available is trying to understand
what is Microsoft's guarantee for
available availability with the specific
resource so if I choose to deploy to a
virtual machine
I would went to Google or search for a
sure sort of SLA virtual machines and
that should bring me to this page or
directly to the virtual machines page
and on each of these links and there's
one for virtual machines and there's one
for app services and there's one for
sequel server Microsoft will describe
and very plain English two things
they'll first of all describe what the
SLA number is that is what is the uptime
amount that you should expect and it's
measured with a number of nines
so 99.9% or 99.99% that's one thing
they'll give you another piece of
another piece of information they will
give you is under what scenarios that
SLA is valid so for example if I just go
out and provision the virtual machine
one virtual machine in Azure I should
not expect to get an SLA for a single
machine if you read the SLA for virtual
machines what I have to do is provision
at least two machines and put them into
an availability set to get the
guaranteed number so obviously that
hasn't impact on my cost and it has an
impact on how I provision and deploy
these things but the SLA requirements
and the SLA numbers for virtual machines
would be different than if I deployed my
application under app services so
typically it's described in a number of
nines as your app services if I remember
correctly as three-and-a-half nine so
that's ninety nine point nine five
percent that's five minutes of downtime
a week of four hours a year as your
sequel server instances or ninety-nine
point nine nine four nine so they'll
give you about a minute of downtime a
week and fifty fifty two minutes of
downtime a year and now I have to look
at my application if my application is
running under app services and it's
using Azure sequel server it would be
reasonable for me to expect that I could
have closed well over five hours of
downtime a year and Microsoft would
still be meeting their SLA s because
knowing what the SLA is for app services
and sequel server these downtime amounts
I have to add them together because the
app service might be down for four hours
and sequel server might be down for 52
minutes and this could occur during
different time windows
so the total the system could be down
would be five hours and Microsoft would
not make owed me any credit they would
still be meeting what they had
guaranteed in their SLA so once you
examine the SLA s and see what your
expected uptime is or at least the one
that Microsoft tells you you will have
that's when you make the decision do I
have to beat this number or not and when
it's when you have to beat this number
which is again a business decision but
that's when the real engineering comes
in because now you have to figure out
how do I do better than this number and
that's kind of what we're gonna talk
about today how do I bet do better than
the expected downtime per year or per
month or per week and to do that one of
the things you have to do is again
understand the capability of each
feature that you're using inside of
azure because each B's are feature
whether it's an app service or sequel
server has different capabilities when
it comes to how do I recover from
disaster how do I stay highly available
so for example blob storage very simple
case I'm writing blobs into blob storage
these are files that my customers upload
I have to have that feature available
for my system to be available and really
Asher and storage of storage is not
available then everything is kind of
hosed but Tom what is blob storage offer
for availability and also to some extent
disaster recovery or they offer
redundant storage by default you get
what's known as read access geo
redundant storage which means every time
I write a bit into a blob in Azure
storage that that will not only be
replicated locally within the same data
center on to three different physical
areas of this data center so it's there
even if a single hard drive fails but it
will also be replicated into a different
data center in case of the ultimate
disaster which would be the failure of
an entire data center or the loss of an
entire data center my data is still
somewhere else and so this replication
feature is very easy to enable and yes
you could pay less by disabling that
replication but if you're trying to be
available if you are paranoid about a
disaster
you will enable even leave that read
access to your redundant store
replication feature on if you're using
and those are the different options
locally but under storage so it's
written three times in the same data
center the default the bottom one read
access G over done and storage will talk
about the read access part here in just
a bit but that's basically take my bits
and asynchronously copy them out of the
data center into some other data center
so they are always somewhere on this
planet app services on the other hand or
virtual machines on the other hand if I
want to stay available I know that I
have to use availability set
availability sets are a way of
describing to Azure look I have this
pool of five virtual machines and i want
them to be available as often as
possible so that tells a sure a few
things first of all it should distribute
those virtual machines in a way such
that none of them all depend on the same
hardware like a network switch so if the
network switch inside of a rakish server
blows up it shouldn't take all my
virtual machines offline the azure
provisioning system should be smart
enough to distribute my own virtual
machines across several different
containers inside that data center so
that one physical hardware failure
doesn't ruin multiple machines it also
tells Azure look if um intel has some
sort of crazy CPU bug that no one would
ever imagine and we need to update the
hypervisor
that runs the host system and update the
guest systems with new spa software
patches don't take everything down in my
availability set at once take just one
machine down at a time update it bring
it back up
and then go to the next ones that's what
availability sets do with virtual
machines you can go into the portal or
use scripts to create availability sets
and when you do you create these things
here on the bottom of the slide or
bottom of that dialog fault domains and
update domains fault domains are about
hardware failures update domains are
about turning things off to reboot for
software patches so given the number of
machines that you have you might change
the number of fault domains and update
that means you're using four as your
sequel server there's a couple different
features yes there's replication it's
really easy for me to go into an Asscher
sequel instance and say replicate this
database that's running in South UK
datacenter replic
to East us or West us or somewhere else
on the planet that's really easy to
configure and sequel server also must
have the pricing tiers come with a point
and time restore feature so it's
constantly backing up your database once
a day or once every 15 minutes it just
depends alone on how much you pay and of
course replication very different from
point in time restore replication is
very good if something fails hard drive
fails data center goes down I have that
data replicated somewhere else it
doesn't prevent the bug in a program
that accidentally truncates a table
that's where the point in time restores
useful so notice blob storage currently
has redundancy but currently there's
nothing offered by Azure that has a sort
of restore feature and that's why anyone
who's truly paranoid about you know one
of their applications just deleting
stuff out of blob storage there's
there's third-party tools available that
you can point to a storage or to a to a
storage account or storage container and
say take everything that's in there and
copy it over here someplace where the
program can't access it right because if
a rogue program goes in and truncates a
table or wipes out all the files in blob
storage a change gets replicated
everywhere so you you lose it even if
you're replicating that's not something
to think about so that's an example of
the dialog that comes up when you're
trying to choose a replication data
center for sequel server you know I can
just click one and move a data center
and poof it ends up over there and for
app services we have the ability to geo
distribute map services so I have a web
application it's deployed let's say in
the south UK Dennis data Sandman but I
want to make sure that even if that data
center is not available let's say for my
customers in the United States that I
still have something running for them so
I might deploy it in South UK and East
us now the trap now the question is when
my customer comes to my website Scott
Allen calm or whatever it is how do I
figure out if I should send them to the
app service instance that's running in a
South UK or the East you ask we'll talk
about that scenario here before the end
of this talk about how you can do that
so hopefully this is the information you
expected and I'm just going to talk
about some of the basic features right
now of an app service we're just going
to talk about maybe the architecture of
a basic application and what we could do
to that application to make it a little
more highly available so let's imagine
we have an app service that hosts our
web application or our Web API of
something that talks HTTP or HTTPS and
our customers are accessing that with
mobile devices desktops whatever the
first thing to know about an app service
is that everything app service has
what's known as an app service plan in
Azure and the way to think about the app
service plan is think about the plan is
describing the virtual machine that you
will use to host your app service so the
app service plan really virtual machine
you can put multiple services on a
single app service plan because again
it's a machine when you're running a
Windows app service plan you can you're
running basically iis you can also
choose a Linux app service plan but just
like we can on a any Windows Server
running IAS I can deploy multiple web
applications to that IES server I can do
the same thing with an app service plan
by default you get when you first go in
and provision an app service unless you
say otherwise you get one instance of
that app service plan so there's just
one server running your web application
but it's very easy to tell a sure hey I
need two instances or ten instances and
you will want at least two instances if
you're trying to be available and
there's auto-scale settings inside of
azure I'm not going to go into some of
those but it's relatively easy to sell
as or hey if CPU percentage goes above
70% then add another instance and then
wait 5 minutes and re-evaluate things
and perhaps add another instance of CPU
is still above 70% or memory percentage
or HTTP queue length there's a number of
ways to do that so you can scale up and
scale down which is basically changing
the number of instances of your app
service plan and I do want to point out
before I forget back at those SLA
numbers when Microsoft tells you that
you should expect 99.95% uptime for your
app service plan
that means if you do everything
correctly you can't blame Microsoft if
you have a deployment process that
requires you to restart your app service
asher does provide deployment slots to
make it easy to deploy a new version of
my application into a deployment slot
and then tell all asher to basically
swap slots so my staging slot becomes my
production slot and I have zero downtime
because they will just manage changing
around DNS records for me to start
sending traffic to my my new deployment
when you scale out to multiple instances
if your app service plan after
automatically puts a load balancer in
front so if one of these instances of
that virtual machine goes offline if
it's getting updated if the hard drive
explodes those sorts of things there's
at least in this case still another
instance running and ready to respond to
requests so we have multiple instances
running you're using deployment slots so
we have zero downtime and let's say this
application is using blob storage and
azure sequel so a very simple
application setup we have summed up
redundancy built in already with our app
service plan and deployment slots but
basically what else can we do and this
all might be inside of a resource group
by the way well the first so a couple
you know again looking at the scenario
this is one of those scenarios where I
could have basically five hours of
downtime a year I'm just using the
built-in stuff multiple instances but I
could still have four hours of downtime
on my app service and almost an hour on
has your sequel what do I do
so there's a couple simple things that
you can do in software to sometimes
improve your uptime numbers and one is
as simple as connection resiliency so
there's always in the cloud transient
faults happening in fact I don't know if
if anyone has tried to use Azure sequel
or tried to use Azure sequel back in
some of the early releases of Azure
sequel but it was always having these
transient faults from a do net
connections where the a do dot
connection would just fail and you know
you try again a little bit later and it
would work but the transient transient
faults were very disturbing to people
the
because in the old days we didn't write
our programs with connection resiliency
we just tried to attempt a sequel server
and if it failed
well that threat is over that you know
we stop we throw an exception it's all
over connection resiliency is simply
okay it failed let me wait a little bit
try again let me wait a little bit try
again hopefully that I'll go through and
this can happen not just because sequel
servers down by the way this can happen
because maybe you under a provision in
sequel server so you need to watch out
for that too when you assign an azure
sequel instance you need to select the
number of data throughput units that
uses this was just basically how
powerful do I want my sequel instance to
be the more powerful the more arcane but
if I don't if I don't have it configured
to be powerful enough they can simply
start to ignore some of the work that
you try to ask it to do and just it'll
fail those operations and so connection
resiliency actually is quite easy to do
for a lot of scenarios because if you
are using the official Asscher SDKs a
lot of them already have this retry
logic this resiliency built-in so for
Azure storage example if I want to talk
to blob storage or queue inside of my
storage account or table storage there's
retry logic in the official c-sharp
nougat package in the official
JavaScript NPM package there's nothing I
have to do although I could configure it
a little bit so this would be an example
and c-sharp code of saying I want to
connect to my storage account here's the
connection string now I'm gonna create
what's called a blog client to be able
to talk to blob storage specifically and
any requests that I make with this
client here's some default options first
of all location mode I want primary than
secondary in other words if I try to
connect to my primary instance of blob
storage to read a blob or write a blob
and it's down well this would actually
only apply to read scenarios if I try to
read a blob and it's not available go to
my secondary storage location where
things have been replicated replicated
and read it from there but you can also
retry so keep retrying that primary
using a linear retry of 500 Melissa
so wait 500 milliseconds we try again
wait 500 milliseconds we try again the
default is an exponential back-off where
it keeps increasing the time between the
retry attempts and the maximum number of
attempts is 3 so that would be a way if
I want low-level control of my
resiliency I could do that for a sure
sequel there is nothing in the official
SDK because you know sequel is a little
bit different you can talk to it through
various protocols but if you're using
with the entity framework there's an
option both an entity framework 6 or 7
or whatever that last version was an
entity framework or disable to save
sequel options enable retry on failure
so that'll work through those transient
connection attempts and all these retry
logics I should point out are smart
they're not going to retry operations
that are obviously going to fail so for
example with enable retry on failure
without there as your sequel if I try to
let's say insert an object through the
entity framework that's going to
generate a foreign key violation or
primary key violation entity frameworks
not going to retry that it makes no
sense to retry that operation it's
always going to fail if I you know
missing some valid database constraint
but it's smart enough to listen for
connections that are exceptions and
error codes that might happen because
the networks not available or the host
is not available or sequel servers too
busy at the moment it understands that
yes I could retry that operation and
maybe it'll work in 500 milliseconds or
3 seconds from now the other thing
before I leave here before I leave retry
logic to point out is that sometimes
retries make the situation worse it's
really interesting how systems behave
under load and that's why I always
suggest people try to load tests and try
to generate some forced-air conditions
just understand how your application
behaves but if a server let's say your
front-end web application is calling a
back-end API service over HTTP and some
you know something's wrong you have this
retry logic well maybe that back-end
service is just
it's the CPU is pegged the memory is
exhausted on that virtual machine and
now you have this retry logic that just
keeps sending more requests it doesn't
help
sometimes you just need to stop and say
well we're gonna give that thing five
minutes and see if it recovers maybe it
has to reboot I don't know what's going
on with that and that's when simple
retry logic doesn't work that's when you
might try something and it's the circuit
breaker pattern so I'll show you the a
library that can help with this in just
a minute but let me describe it the
circuit breaker is a stateful component
that wraps your interactions with some
other service and the circuit breaker
like an electrical circuit breaker by
default is gonna be closed so
electricity in other words messages can
flow between these two systems but it's
stateful so it's keeping track of things
like oh I see you've made ten successful
requests over the last second good I'll
keep the connection closed or the
circuit closed but maybe this you've
programmed the circuit breaker to say
huh I've seen 10 failures over the last
second something must be wrong when we
pop open the circuit and now instead of
your retry operations hammering the
service the circuit breaker makes them
fail fast and just immediately says now
this connections currently shut down
we're gonna maybe retry it later but for
right now we're not gonna send any more
requests that serve because it's failed
so much over the last second or 30
seconds what have you and then at some
point you can program the circuit
breaker to go half of them which doesn't
really work well for like electricity
but in the half-open space it'll start
to allow some things to trickle through
so maybe it'll select one out of ten of
the next connections and allowed to go
through that's kind of a test is the
back end service responding now and if
things look like they're working it also
those again things are back to normal
and maybe that gave that back-end server
a chance to reboot or patch or you know
clear out some HTTP queue that was
building up and now the system's back to
normal so how would you implement
something like that obviously sounds
non-trivial anyone use poly net or oh
good yeah so awesome library to
encapsulate policies about what you want
to happen when something's not working
so if I get an exception do I want to
retry it if I get an exception
do I want to use a circuit breaker to
shut things down and feel fast and the
documentation is quite good just to give
you a simple example of the API yeah I
mean it's like a fluent API so I want to
build a policy that will handle / 0
exceptions I'm gonna retry it 3 times
which chances are you'll still / 0 but
hopefully you can see that you could
look for particular types of exceptions
that represent transient faults you know
the network not available fall toast not
available fault server too busy type of
faults and those are the type of
operations so you might want to retry
and this handles apologies for making
anyone nauseous if I'm scrolling you can
build retry policies circuit breaker
policies timeout proper policies excuse
me I need a drink
bulkhead isolation policies which can be
quite useful this is like on a ship when
you have a bulkhead one of the things it
does if someone punctures a hole on the
side of the ship it prevents the water
from spreading throughout the ship it
just floods one area
likewise in software development
sometimes we have these components like
a good example in.net is the whole HTTP
client issue I don't know if you know
the HTTP client issue drama I could
probably spend an hour talking about
HTTP client but the suggested use of
HTTP client this as a singleton but
anytime you get into using a singleton
so I'm gonna use one object to connect
to all my services HTTP client that
becomes an that can become an issue if
something is inherently wrong with that
HTTP client maybe this one service over
here puts it in a bad state bulkhead
isolation would be about okay we're
gonna use this client and it's isolated
this is HTTP client to talk to this
service or this collection of services
and that in a bulkhead separate from
this other one over here so caching
policies fall back policies and then
policy wrap is a way to say hey take my
circuit breaker and I retry and this
other thing and wrap them all up into a
bigger policy so pretty cool library
called poly if you just search on
whatever search engine for github poly
net I'm pretty sure you'll find this
repository and you can read through the
documentation all right so moving on
graceful degradation another another
feature to consider so my app service
talks to sequel server what happens if
my sequel server is not available it's
experiencing one of those downtime
moments what can I do well for reads
maybe maybe just maybe I could come go
to some other data source that might be
available and read some data that was
cached maybe five minutes ago or half an
hour ago and of course this depends on
your application if you're building an
application that provides customers with
stock quotes or Bitcoin quotes then you
can give them five minute old
information right that could ruin
somebody but if you're someone who is an
e-commerce vendor and you just can't get
to the inventory service to determine if
you have enough widgets for this order
or maybe it's okay if you read what was
there five minutes ago and tell the
customer hey we still have 10 because if
you don't have enough you could always
send them an email afterwards and just
tell them their shipment will be late it
works for some industries right it
doesn't work everywhere
what if sequel server is not available
and I need to write something into it or
maybe if I have a backup that's a
message queue and the message queue is
still available I could write that
command that describes what information
I need to place in the database or how
I'm manipulating the state of my
application I can place that into a
message queue instead of directly in the
database and one of the things you'll
realize here is that reads are separate
from rights and if you've heard of a
CQRS strategy CQRS design pattern that
can be a very effective pattern to use
for both scalability and reliability of
an application so there's been many
talks over the years at NDC about CQRS
I'm sure so you can look this up but
it's basically about separating reads
from writes so just having a different
path for your code for this two
different operations but this is easier
said than done right this is where not
only do I pay more operational costs for
running sequel and Redis or a second
instance of sequel or something for that
second data source and some Q's not only
my paying more operationally but now my
code is much more complex
that's sometimes a more important factor
now when someone gives me a report that
Hayes such-and-such a customer
experience this issue on the website
then I have to say okay well I wonder if
that was reading from the database or if
the database was down at that time and I
was reading from somewhere else it just
it complicates things complicates not
just the code but also debugging issues
and figuring out what's happening inside
of your environment so you again really
really really want to beat the Microsoft
provided SLA numbers to introduce some
of these more advanced architectures
what about this what if we wanted to
build an application with an app service
that is using this queue that is using
blob storage that is using a Redis cache
that is using Azure sequel slightly more
sophisticated application but here's
what I want to do I want to take this
application now that I've made it more
redundant and I want to deploy it in
multiple places so I want to employ
deploy it into two different data
centers one in South UK one in East us
now even if East us is down I can send
all my customer traffic to the one in
South UK how do I do that that's where
you can use an azure load balancer named
traffic manager so there's three
different types of load balancers in
Azure let me talk about Traffic Manager
first traffic manager is a DNS dns-based
load balancer meaning you will go in and
you will configure traffic manager to
respond to DNS queries from your
customers so when they go into the
browser and type in Scott Allen calm and
the operating system goes off and says
hey what I P address has got a long calm
that request will go to traffic manager
and then you program traffic manager to
know about your different deployments so
you say hey I have this app service here
and this app service here it knows those
two different IP addresses and you tell
traffic manager there's there's several
several different ways to program it one
way to program traffic manager is to say
actually let me just think we have time
to just create one real flow
I'm sure if I have any screenshot for it
I hate talking about things in the
abstract but shouldn't take too long I'm
not gonna enter passwords and all that
I'll just talk about it
I can tell traffic manager respond use
using performance-based rules so traffic
manager will determine which of those
two IP addresses to hand out based on
the customers location and what the
expected latency is between that
customer and that particular data center
so a customer in East us will probably
get routed to the app service instance
that's also in the east us unless they
unless that app service is down because
by the way traffic manager is also doing
health probes once I tell it about these
two different app services it's
constantly sending HTTP requests to an
endpoint that you provide so you could
build something in your app service like
slash health check until traffic manager
go to slash health check and if it goes
to slash health check and receives a 200
response it says ok you must be good but
if it returns anything other than a 200
even like a 301 which isn't really an
error code but if you return anything
but a 200 traffic manager sent us home I
think something's wrong over there I'm
gonna stop sending traffic but it'll
keep checking to see if it recovers
anyway so you can tell traffic manager
about users based on performance which
is interesting you can also tell traffic
manager to route based on geography
we're just interesting for geofencing so
if you need your new customers to state
to be routed to a data center that was
also in the EU and US customers stay in
the US and so forth you can do that you
can also tell traffic manager to do
priority based routing so you can say
send 90% of my traffic to the first app
service and send 10% of my traffic to
the second app service and sometimes
that feature is useful for things like a
bee testing so I have one app service
where I've deployed my new features I
can tell traffic manager you send 3
percent of my traffic there because I'm
just doing a test to make sure nothing
explodes in spectacular fashion
and after that runs for maybe 8 hours
then I'll also deploy that into the
other app service and hey changed
traffic manager rules to just balance
between the two and you can have nested
traffic manager profiles by the way so I
could have traffic manager that routes a
user to another traffic manager because
they live in the US and thus other
traffic manager routes based on
performance or priority what-have-you so
in the portal it's real really easy to
say create new traffic manager you can
also do it with a script of course you
tell it about your different deployment
to different app services and just tell
it how you want to be route between
those yeah so traffic manager is one of
three load balancers and I just want to
point out because um sometimes people
get confused about this but a short
traffic manager is sort of the highest
level Network load balancer it's a DNS
based load balancer you configure the
DNS timeout there
the default I think is five minutes 300
seconds which means every five minutes
your client should re query whatever DNS
server to say hey what was that IP
address for Scott Island calm again
which means if your app service fails
your customer could be looking at blank
screens or errors for you know four
minutes and 59 seconds until something
triggers another DNS query so you could
could you could go into Azure traffic
manager and set the time to live the TTL
to like thirty seconds but then you have
the overhead of now your client is
making a lot of DNS queries all the time
trying to find out where to go so
traffic manager DNS base load balancer
the lowest level over here on the left
the azure load balancer that's a network
layer for load balancer meaning and
operates at the tcp/ip level meaning you
typically don't put this in front of web
applications you typically put this in
front of virtual machines that are doing
socket based protocols like RDP or stuff
like that so for most web developers the
azure load balancer isn't interesting
and so what's confusing is what do you
use as a web developer you use the azure
application gateway that's a layer 7
load balancer so it operates at the
application level of the networking
stack which means it understands
protocols like HTTP and HTTPS natively
you can use this as your application
gateway in front of your web application
which I'm gonna come back I have to
remember to give you the caveat there
you can use this load balancer in front
of your web application to do things
like inspect HTTP headers there's a
firewall with up so you can control
traffic a little more tightly you can
have a set affinity cookies if you need
them so if you need a customer when they
come to you with their browser or their
service if you need their request to
always come back to the same server that
application gateway can set an affinity
cookie to make sure they arrive at the
right spot and the thing to be aware of
with Azure application gateway is yes
you can place it in front of a web
application that's deployed on a virtual
on virtual machines so if you're using
pure virtual machines may be putting
containers on this machines that's where
you would configure as your application
gateway I don't need to put this in
front of an app service my app service
already has basically an application
gateway it's just one that I don't
control and so a lot of people but a lot
of people say hey there's certain
features of Azure application gateway
that I like how can I use that with an
app service and the answer today would
be the only way to do that is to create
what's known in Azure as an isolated app
service and it's actually in the list of
pricing plans for app services now but
the thing to be aware of with the
isolated environment is tremendously
more expensive because the word isolated
basically means what Microsoft is doing
for you is not just putting you in the
regular app service environment they're
creating a whole special app service
environment for you and giving you a set
of virtual machines to run that
environment but because they're giving
you a set of virtual machines it gives
you a little more control over your
environment your app service environment
a little bit of a tangent here yes but
you know people come to me and say hey
how my app Service has by default a
public IP address how do I get rid of
that and the answer is well you don't
with an app service unless you get an
isolated out service because then you
can put in front of then you can put it
behind the Gateway you can put it behind
virtual networks you can do all these
other interesting things
the big one that gets some people is
when you have a regular app service just
know that a secure connection to that
app service its terminated before it
reaches your app service so there's
another network appliance or piece of
machinery inside of Microsoft data
center it takes out SSL connection and
decrypt it and then forwards HTTP
traffic to your app service which
disturbs a lot of people because it
means there's some unencrypted stuff
going back and forth in the data center
right now the only way to get true
end-to-end SSL encryption would be to
use either the isolated app service
because then you can configure this
application gateway to terminate SSL and
still forward a connect encrypted
connection to your app service or to use
virtual machines anyway moving on
hopefully not all needs sense I'm just
you know I get in a rambling mood when
I'm jet-lagged so three different load
balancers that's how you'd use them this
was just describing traffic manager in a
little more detail so I'm gonna skip
over with that but it was just you know
the this traffic manager was configured
to use priorities the what the one I
described to you earlier was not
priority based routing the one I
described to you is what they call
weighted routing weighted routing is
where you say 97% traffic here 3%
traffic their priority routing is
basically here's my primary endpoint my
secondary employee my third endpoint and
traffic manager will just start falling
back to the other app services or
resources that you registered if one
goes down and I should point out the
traffic manager is not just app services
inside a traffic manager I can point it
to an app service I can point it to a
web application on a virtual machine I
could point it to a web endpoint that's
running injury to AWS or an endpoint the
toasted in my own data center that
something that has a public IP so it's
not just restricted to to Azure stuff
it's basically a way to get a DNS load
balancer that works everywhere this was
an example of showing our two instances
of the same app service just run one
running in East us run one running and
West
you I did have some screenshots so after
I create traffic manager this is where I
could go in and set the routing method
performance weighted priority Geographic
and yes this would be sort of the
default name like PS music store dot
traffic manager net but of course you
can buy purchases custom domains to
assign to that because you'll have an IP
associated with that traffic manager
profile and yes of course you can
purchase SSL Certificates so that you
should be yes traffic arrives there to
your custom domain name and this is
where you could say yes for this traffic
manager I want the performance routing
method with a 300 second time to live on
the DNS responses and then this is where
I add my endpoints and say here's my
first deployment here's my second
appointment all right load leveling
something else to think about it's very
easy for front-end services to receive
messages from customers that have to
kickoff back-end services that need to
do a lot of heavy lifting and that
becomes a problem when you have a lot of
customers arrive let's say at 9 o'clock
9 a.m. on a Monday morning or when you
know some activity happens in the news
and all these people were creating
operations that are overwhelming your
back-end because there's just so many
requests the web server can handle it
the backend cannot that's one of the
biggest struggles sometimes in
distributed systems is just trying to
get things to work together all at the
same capacity so that is that is where I
would make sure to introduce again
message queues have some ability to
place messages into a queue from the web
front-end so that when the traffic
spikes like this the backend the front
end will just place a message on the
queue that says hey we need to do this
work we need to batch update this thing
we need to place this ticket reservation
in the back-end service however you
provision it can just pull messages off
of that queue as as it can to process
those things and it might need to you
know take a half hour half an hour to
work through all the messages during a
rush but that's ok it was better than
that service being overwhelmed and
shutting down
when all the customers arrived at once
we've used a sure service boss over the
last couple years and it's been very
effective at a number of things that as
your service boss whoops
has all sorts of nice features in it
about message deduplication and the
ability to schedule messages in the
future so I could say you know drop this
message in the queue but not until 11
a.m. tomorrow morning
a type of thing is kind of nice it has
hues so simple cues that you can use to
decouple to services and typically you
use the hue when service a wants to tell
some other service to do something so it
puts a command in that queue to tell
some other service to do it it doesn't
know what service will do it but one
other service will pick up that message
and do something which is different than
topics where I can place a message in a
queue and there can be multiple
listeners on that topic so I may place a
message on the queue that hey customer
Scott Allen just placed some order for
an umbrella or a wind jacket or I guess
I need a heavy jacket this week here
really was hoping it would be a little
warmer weather anyway that message that
gets dropped in a topic could be handled
by a few different services out there
they can also describe and listen for
things that will you know one might be
the email alerts one might be the
inventory management different services
can work on that particular command and
then relays and service bus in general
have been very effective for us because
we deploy a lot of custom software over
the years behind company firewalls just
for historical purposes we're slowly
moving stuff into the cloud but even in
the cloud we still need something
deployed at the customer site to be able
to load data and as your service boss
has the nice feature of you can open up
a service bus listener behind a firewall
and still be able to effectively
communicate with it because I can drop a
message in a queue and it shows up
inside the firewall and it's processed
by a piece of software running at the
customer site so it's kind of quite
useful and as your service boss pretty
easy to use just trying to make things a
little more concrete so you can see what
the code is like if you haven't worked
with service boss typically your
connection string would look something
like I've created a service bus called
PS messaging service boss windows done
that would be my namespace so I have a
connection string here that would
include some sort of access token in
that connection string so I'm going to
create a cute client I'm gonna put
together a message body that is a simple
string you can put together messages
that are populated with XML or JSON
whatever floats your boat create a new
instance of a brokered message where I
will give that message body to and then
just tell the client to send raw for
poof someone else can read it and the
the read operation is pretty
straightforward too I would create a
crate create a client and listen for
messages on a particular queue load
throttling is something you might also
want to consider depending on the type
of application you have but let's say
you are building an API and you expect
customers to be able to sign up and use
your API maybe they're paying money for
your API and maybe they're not but you
can always get that one customer you
know sooner as soon as you give someone
a programmatic API someone kind of uses
if someone even accidentally might write
a for loop that just hammers your
application service making API calls you
want to prevent that you could write
that in your code inside of your
application or you could put some sort
of API Gateway or API manager in front
of your app server so there's a number
of these API gateways some of them are
open-source so you could certainly
consider them as your asher's version of
an api gateway as a tool called api
manager and effectively it works as a
proxy that sits in front of your api's
which could be spread across multiple
things so there's a number of
interesting use cases for API manager
one is this let's say I have an app
service that I want to monetize and I
just want to make sure it doesn't get
abused one easy way to do that I don't
want to write the code myself is to
create an API manager in Azure tell it
about my app service the address that
it's at and now my customers will
connect to API manager instead of
directly to my app service and when they
connect my API manager
it can do these things like implement
throttling rules that you give it so
it's very easy to go into API manager
and say hey for any particular customer
we're not going to allow more than five
calls a second or five hundred five
thousand calls a week and you can also
build a single API endpoint or namespace
that your customers go to and have those
messages forward across different micro
services that are spread all across
different things on your back-end and
built by different teams so for example
someone comes to Scott Allen comm slash
API slash widgets API manager might
route that to this app service which is
on a completely different name and if
someone comes to Scott Allen Scott Allen
comm slash API slash invoices maybe that
comes over here and it's implemented
with Azure functions or something so
it's a nice way to coalesce these things
together to API manager can consume
metadata about your API so if it's
available which allows the the the
traffic API manager here in the middle
it allows it to understand your API s
and allows it to export documentation to
the developers who are signing up to
user api's so if you can produce a
wobble document or a wisdom document or
swagger document and give that to API
manager it knows about all the
operations that are inside of your
service so this is setting up an API
manager you'll notice there is a cost
associated with it it's not trivial but
hopefully you'll be able to monetize
api's and be able to make this money
back it has a once you set it up it
gives you a developer portal where you
can go in and manage your API is it also
gives you a public portal where you can
send your customers and their developers
you can send them here and they can sign
up and register for a key and browse
your api's and browse your documentation
so it's quite a useful tool just going
through some of the screenshots of it
you can divide your API up into
different products apply policies this
is where I would apply throttling
policies don't
this list of policy statements on the
right those are just some built-in
policies that I can drag and drop over
so if someone someone in my organization
wrote an API but they didn't have pours
headers sent in there if I put it behind
an API manager I can add this course
headers for them without them changing
code so I'll be the course headers down
here uh yes that was a topic I didn't
quite want to talk about I just wanted
to end the talk with a couple topics
topics here if you go down this road of
trying to be highly available trying to
do disaster recovery you have to have to
have to have to have to automate
everything that you can you cannot go
and ask someone in your IT department or
your developers to say hey stand up a
new instance of our application in the
east you aced East us datacenter because
we're gonna deploy there now has to be a
script that's checked in somewhere that
someone just executes it's been
parameters so you just pick the
datacenter and the names that you want
for that particular installation
sometimes it's difficult to maintain
those scripts and create those scripts
but the net benefit pays off in many
ways over time and you have to test you
have to test you have to make sure not
just the traffic manager let's say is
configured correctly and working for you
but the more you set up these complex
systems that involve traffic manager and
to app services instances and geo
replication and this thing will read
from Redis when it can and I will read
from a sequel database if it can't you
have to test out the scenarios just to
see how your applications gonna behave
and thinks you're wrong because you
don't want to be debugging things in a
panic mode when something actually is
wrong it's always very nice when someone
presents you with an air from the
application or a log from some part of
the application where you say oh yeah we
saw that last week when we were trying
things out we know that's when Redis
goes down that sort of thing so test and
introduce failures and you know take
that Netflix chaos monkey approach of
having something that goes out and
and forces services to be in a faulted
State you know turn things off disable
things see how the application behaves
when something's turned off there's
valuable understanding how things will
behave and of course monitor everything
the building built in as your monitoring
tools are okay but you have to spend
some time with them to make sure you
properly configure the alert and you're
getting the right metrics a lot of
people use some third-party tools which
is cool too
and with that I'll just say yeah I have
Pluralsight courses but if you have any
questions feel free to email me
otherwise thank you for listening to me
on this topic and I hope you enjoy lunch
and the rest of the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>