<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cancer Genomics - a biologist and a developer - Samantha Langit &amp; Lynn Langit | Coder Coacher - Coaching Coders</title><meta content="Cancer Genomics - a biologist and a developer - Samantha Langit &amp; Lynn Langit - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cancer Genomics - a biologist and a developer - Samantha Langit &amp; Lynn Langit</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jbOflGXBQao" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">there we go hello everyone
this is a cancer genomics data pipelines
I'm Lin Lang it and I'm Samantha
language and we're going to be
presenting information from Cicero
bioinformatics Australia Samantha's
first going to start by talking a little
bit about the problem that we've been
working on first let's talk about DNA
there are around three billion DNA
letters or nucleotides in one person's
genome that's three billion data points
per person and with up to 25 percent of
the population possibly getting
sequenced by 2025 that's a lot of data
that we need to be able to process
Samantha why is that that sequencing is
becoming so much more accessible well
with increased computational capability
this drives down both the time it takes
and the cost of getting sequence making
it much more accessible to greater
portions of the population and it's
really in many ways the ultimate Big
Data problem so to introduce ourselves
I'm going to introduce first Samantha
langit Samantha is my daughter she is 18
she has dis graduated from high school
in the United States and she has
intended to go to college to be a
bioinformatician Samantha's career in
bioinformatics however started last
summer when during the summer she had a
great opportunity and she was able to
attend Stanford University through a
special program for students who are
interested in these fields during that
time one of her courses that she took
was cancer bioinformatics and because of
one of the comments that she made after
studying at Stanford and coming back
home I decided in my profession to be
interested in this her comment was gosh
they're not using the cloud and cloud
data pipeline patterns as much as I
would have expected so samantha is going
to represent the bioinformatics position
in this talk and then Samantha I guess
can talk about me this is my mom Lin
Lang it she's a cloud architect she's
worked with Google Amazon and Microsoft
cloud
and she's going to be talking about the
speed the cost and the simplicity of
each possible Big Data Solutions to some
of the genomic problems that researchers
face in this area so because I know that
this is a technology conference and not
a biology conference what I wanted to do
is I wanted to take patterns around big
data pipelines that I've been observing
and working on with the bioinformatics
community and generalize them so that
they could be of use to a larger
audience as I've done work in big data
domains from advertising education IOT
and now bioinformatics at a very huge
scale on the public cloud I found a
series of patterns that are transferable
so I'm hoping that you'll get that out
of this talk to start I've created this
pipeline process that I use in building
these high-performance data pipelines
has five parts to it the first part is
to clearly define the business problem
that's kind of standard stuff but I do
find when there's huge data volumes that
there's this tendency of using the
latest and greatest technology really
without regard sometimes to matching it
to the business problem so you got to
start there so we're going to talk about
two bioinformatics problems in this talk
and show you pipeline solutions then we
need to look at not only the quality and
the quantity of the data but the type of
data genomics is really interesting and
that the data is has some unique
characteristics that Samantha is going
to talk about the third phase of the
pipeline is probably of the most
interest to this audience it's the
process by which as an architect in
working with community I've helped the
bioinformaticians to densify the
candidate technologies for the common
parts of the pipeline the ingest the
extract transform and load or extract
load and transform the business
analytics the machine learning and the
visualization now I'm a big proponent of
lean startup or lean type of processes
and I apply this to data pipelining in
that we build MVPs or portions of the
campi of the sections via the candidate
technologies we iterate learn and then
we assemble them and then test at scale
so in both of the problems that we look
at we're going to discuss
from an architectural point how some of
the decisions were made and some of the
general patterns I'm seeing in these
huge data pipelines so to get us in the
right mindset
my background is traditional data
warehousing actually even DBA for some
of you follow my work for a while used
to work at Microsoft seven years ago was
a very big user of sequel server wrote
three books on analysis services so I
have this background in traditional data
warehousing that one of the reasons I
come and do these talks is I've made
this sort of transition to broadening my
toolset including these types of
traditional solutions for those types of
problems but also building new types of
solutions for problems of the scale of
genomics and I would tell you that one
of the first things if you've worked
with this sort of a solution that I
would recommend that you try to do is
literally put it out of your mind
because none of this architecture do I
find fits in these big data pipeline
solutions you just basically have to
start with something totally fresh in
looking at these problems I worked with
some bioinformaticians and I found two
types of problems that we have pipelines
solutions that we're going to talk about
today and Samantha's going to talk from
vinyl chromatics point about the first
problem that we worked with the first
problem is genomic sequencing results
now recently in molecular engineering
there's been a huge breakthrough in the
development of a technology called
CRISPR caste 9 this allows researchers
and medical workers to edit the genome
at the level of almost to the
specificity of specific DNA letters
now the NIH in the United States
approved this for human health research
and it could revolutionize the way that
we treat cancer however in order to find
the exact possible great targets for
CRISPR to edit we need to find a look
through a huge amount of data those
three billion data points that we
mentioned earlier and find very specific
results so when I heard about this
problem I thought this sounds like the
kind of problem for which the serverless
architecture that we've heard so much
about this conference would be a perfect
fit now I happen to do the majority of
my work on the Amazon Cloud because
that's where my customer demand is
coming from the west coast of the United
States but this pattern is of course
implementable in other cloud vendors
such as Google or Azure as well just
this is a base architecture that I was
expecting to work with bioinformaticians
into design and build to address the
problem that Samantha talked about so if
you're not familiar with it you start at
the bottom level with the data tier with
s3 and usually DynamoDB and then at the
compute tier you have lambda functions
and you interface with the rest of the
world with API gateway now this is a
simplification you'll often have machine
learning and other services associated
but just in case you're new to this
architecture this is what I was
expecting to work with bioinformaticians
to build as a solution to this
particular problem
now speaking of bioinformaticians I made
a trip this year out to Australia for
another reason and while I was there I
had the good fortune to be invited to
come and speak at Cicero you may not be
familiar I wasn't this is similar in
Australia to in the u.s. the National
Science Foundation it's a National
Science Research organization not only
in Australia but in that section of the
world so it's a Commonwealth Scientific
and Industrial Research Organization
they have a very large bioinformatics
group that's working on solutions to
bioinformatics bioinformatics problems
for humans animals plants so on and so
forth so they're based in Sydney and
this is a small section of the group
that I worked with the woman there is
dr. Dennis Bower she's the lead for the
group and there are two of her people
that she works with now interestingly
they were working on the CRISPR cast 9
problem and
they actually came up with a solution
and that solution is called GT scan 2
and Samantha is going to take me through
a demo of this for you now so what are
we looking at here Samantha this is what
you see when you first see the site for
GT scan 2 it's a search query database
to be able to find the best possible
candidate targets of DNA sequences that
CRISPR can edit so let's look at what
happens when we click scan so right here
we're going to look at the genome that
we have available for use the chromosome
we can specify that and we can also
specify if we don't know the exact
letters but we know the placement of
them we can specify the place in the
genome that we're looking for let's load
up an example so Samantha when this
comes up is this all that the researcher
is going to need so that they can find
that place as a potential target for
editing for experimentation is this this
all they need not necessarily even in
the sequence itself there could be areas
that CRISPR might trip up on while
editing so we need to find exact
sections that would be most useful for
what we're trying to prove to do that
let's look at example and in the example
here what what are we looking at can you
explain this screen to us a little bit
more this would work kind of like a
Google search might work up at the top
in the query box we were having the
section of DNA that we're looking for
that we specified when we said scan now
on the bottom here we have a list of
results organized from top to bottom in
relation to their relevance for example
this top result right here or that
specific result right there that my
mom's just clicked on that's highlighted
down below it has high activity which is
good something that CRISPR looks for and
relatively few off targets which is also
something that CRISPR needs
and so if you click on a black what does
that show you that shows you a segment
with low activity that probably means
that that DNA is either bunched up or
not in use so it wouldn't be a good idea
to hook the mechanism on and try to edit
from there so Samantha we're just
looking at a small portion of the three
billion data points here I see from 20
to 9 5 whatever it is 2 to 9 5 so we
might have to sort of dynamically search
the genome results sound like you said
like searching Google to find potential
targets for editing that's how this tool
is designed to be used is that correct
that's correct it compares this result
to a number of reference genomes that it
has in a database to find the best
possible target great cool ok so like I
said I was expecting because when I
worked with bioinformaticians previous
to the group at Cicero I found that many
of them had big gaps in their knowledge
and so they were trying to do processing
on local clusters even desktops may be
using ec2 instances I certainly didn't
find anybody building a service pattern
that was scalable for this kind of a
bursty workload that was until I met
these people at Cicero the way I met
them was the underlying architecture
that they built for GT scan 2 was
actually server 'less so one of the
reasons that I open with this is because
I'll admit it a lot of us work on what I
call silly problems I live in Los
Angeles so I've worked on snapchat
ramana Thais errs Instagram you know rhe
servers YouTube ramana Thais errs and
one of the reasons I've decided to give
talks about this topic is because dr.
Bower and her group have three open
racks they cannot hire people who are
qualified to help them with this
architecture it's very very interesting
so I'm talking about this worldwide in
fact if you want to move to Australia
they will relocate people worldwide or
anybody watching on the video who is
familiar with a serverless architecture
because for this type of a problem they
have gotten an extremely good result and
in fact they have won awards around the
world for this architecture it is their
intention to place this tool into the
public domain
for bioinformatics research worldwide
now my interface was with them was
because I'm an Amazon community hero and
when I reached out to them they said can
you please come and talk about some of
the newer Amazon services because one of
the things they're interested in is some
of the orchestration which you might
have heard about at this conference
called AWS lambda orchestration and also
some other machine learning
functionality that they're considering
adding to this pipeline but that being
said this pipeline is an example of an
implementation of really the core server
this pattern and they took a problem
that was basically not solved by any
other bioinformatician applied just a
standard serverless pattern it's a
basically a static it's a spa website
that has an api gateway that splits the
target information scores it stands it
out uses an SNS topic and uses dynamodb
for persistence so also in fact amazon
has published this on Amazon's blog as a
reference architecture and this is not
only useful for bioinformatics like I
said in the infection to this this is a
service serverless architecture in
action working solving the problem so
started this out as a reference use case
and as I continue to work with them and
augment it with new or Amazon services
if you're interested in this use case
you can follow Cicero's blog my blog or
Amazon that's the first problem that
however isn't the only problem a more
difficult problem that Cicero is working
on that we're working with them on is
something Samantha's going to talk about
a second problem genomic research is
faced is that sometimes they don't know
which specific gene is controlling the
outcome that they're looking for in this
case they need to look through the
entire genome with genome wide
sequencing data Association studies
these analyze large cohort data from
reference genomes or imputed SNP array
data and use statistical clustering to
try to figure out which section of the
genome which gene if you will is most
likely producing the outcome that they
look for so that they know what to edit
so in this particular cases it says
they're viewing datasets with millions
of features features are like columns
for those of us that come out of the
classic database world so super wide
data so again just making sure that
we're understanding the samantha this
problem is entire groups whereas the
other problem is more individually
targeted is that right correct both of
them are probabilistic but this would be
much more computationally intensive yeah
so again looking at the pipeline pattern
here and kind of drilling in because
there's a whole bunch of things that are
happening now with machine learning on
the cloud and this problem was
particularly interesting to me because
this problem of not only implementing
but properly scaling machine learning
algorithms on the cloud is really
something that in my architecture
practice I'm being asked to do very
frequently now because of the volumes of
data coming in whether it's from
bioinformatics or IOT or you know some
other type of method just so much data
is being produced that these very
complex probabilistic cloud solutions is
something that my customers are looking
for me at least to try out with them so
this pattern that I'm seeing is when a
customer is trying to figure out which
machine learning algorithms are going to
provide most value to them is it's an
iteration within an iteration or a set
of iteration so that the iteration path
tends to be you bring in the data you
process it the processing for machine
learning might be different than just
regular adding up the data because you
might do things like bucket eyes that
look for outliers all the statistical
analysis that you might want to do and
then often people have a data science
event will want to visualize the data so
including a visualization component or
an ability to visualize during this
iterative development process of a
pipeline is something that is relatively
new pattern for me in building out these
big pipelines another thing that I'm
seeing and this is represented by the ad
hoc query box is this desire for
multiple languages some data scientists
use are some use Python some use you
know some machine learning library so
the ability to kind of plug in languages
into
the test environment is something that
I've had more and more demand for so
Cicero had a solution in place or has a
solution in place that they are using
they have an on premise solution for
doing these G wave studies and that is
using Apache Hadoop so they have a local
Hadoop cluster and the idea there is
Hadoop is much more scalable for the
size of data that they're working with
and it's many many many terabytes of
data and they have also selected Apache
spark to you make use of in-memory
computation we'll talk a little bit more
math in a minute and then they have
written a domain-specific open-source
library when I met them in February they
were starting to experiment with placing
this on the cloud they really again like
they did with GT scan you know they are
a research organization they want to
make their work more globally available
and it's their goal that
bioinformaticians can collaborate as
these datasets are gathered worldwide so
they were looking to leverage cloud
pipeline patterns so this is just a
picture of what they have their library
their open-source libraries called
variants Park it has two primary aspects
to it it is a domain-specific library
for bioinformatician so the processing
is uses words that bioinformaticians are
going to understand like variant
analysis which is matching the genome
sets to reference genomes like the
thousand genome database and the
secondary aspect of has machine learning
this is in their corporate data center
and as you might expect they are running
into some challenges if you've ever run
a Hadoop cluster locally it's a resource
intensive even though you know it's open
source software the expertise around
Hadoop is hard to find and it becomes
very very expensive very quickly so kind
of an aside I have done quite a bit of
work on Hadoop actually just over the
years and done some training on it so if
you're just getting into it can just
look me up on the web and have number of
resources available there but a trend
that I found in working with the dhoop
over the past 12 to 18 months is with
the development of the
a spark library which is designed to sit
on top of Hadoop although it can sit on
top of other things like s3 or Azure
blobs Sun and so forth you have this
unified library that does distributed
compute in memory of commodity machines
so what we had previously is we had a
bunch of disparate libraries that were
really difficult to deal with and Hadoop
and as a trend in really huge data and I
think it was actually important for the
region market because as I've researched
you guys have a lot of oil and gas
sensors industrial IOT and I've done
some work on that domain as well so
those volumes of data warrant Hadoop and
what's happening with the Hadoop
ecosystem with SPARC is maturity of the
technology SPARC interestingly well you
know we're with SPARC developed UC
Berkeley in the United States and who
developed it a team at UC Berkeley yes
it was a group of bioinformaticians
basically to solve this problem so Matea
I forget his last name he is actually
the guy who came up a spark what's
interesting about it is if you look
across the top here you can see that you
got sparks equal spark streaming spark
graphics ml Lib and all these languages
and this is how a bioinformatician or
more broadly a data scientist thinks
this is something that again I want to
bring trends to you because I know
you're developers and you may not be
working in bioinformatics when you get
into true Big Data and you work with
data scientists they really are not
usually married to one programming
language they will use multiple
programming languages and I find that to
be very different than application
development I've done over the years so
kind of a meta takeaway thing is if
you're working in big data it's time to
learn spark it's really an important
technology and it's bringing Hadoop of
age and it's used in this solution so
not only did they use spark but they
used variant spark so what is very good
spark against Memphis variance park is
an open source library that allows for
the processing of data from stud
like genome acquired data Association
studies in an incredibly fast
sophisticated manner yes when I was on
Cicero after I talked to them about some
of the new Amazon technologies for GT
scan - I said it what else you guys
working on what else could you use the
cloud architect for they said well we
have this variant SPARC and you know we
think it's pretty great but we really
haven't had much pick up in the
bioinformatics community and one of the
things I said I said you have a
reference use case and they said well
yeah we published a study and I said I
said how do you get it and they said we
go to github now this is something
Samantha Nye actually have quite a lot
of experience in but from a different
domain separately from our professional
work we are well we run a nonprofit
called teaching kids programming and
that is an open source library and our
target for that is school teachers so
one of the lessons we learned in working
with that project is that people who are
not developers they don't go to github
do they not at all they really don't
so for teaching kids programming we
actually got a grant from Microsoft and
we have a website and everything is
indexed and it really increased the
uptake so kind of a high level lesson
about using open source libraries in any
pipeline and getting sort of an uptake
on it is to create a reference example
and so that's one of the things that
we've been working with Cicero on we're
going to kind of move towards that but
we talked before we do that we want to
talk about why variant spark is so
interesting so to that Samantha's going
to talk about it by means of comparison
variant spark tested the speed of their
application versus other common sources
of statistical reference and this paper
was published in 2015 they found that
variant spark tests on the same data was
80% faster than atom a similar service
from Berkeley and 90% faster than both R
and Python yes it's interesting
so benchmarking is really really
important and frankly benchmarking is
some of the work that we're looking to
do with Cicero so if any of you guys
want to join the party here I love to
benchmark big data pipelines I think
it's just a really fun thing to do so
you can just pond
talk to me after because I know it's a
big learning for me whenever I get that
opportunity because in fact the
underlying architecture of variants Park
has basically improved since this
benchmark was done so various Park is
this open-source library it's built on
top of apache spark specifically it's an
extension of spark ml the idea is to
massively paralyze the generation using
the random forests machine learning
algorithm which is sets of decision
trees to identify genes efficiently and
Samantha you want to talk about the
results that they got on their benchmark
this is a landmark system it can analyze
up to 3000 samples with 80 million
features in less than 30 minutes
now remember samples include the entire
unit human genome this could enable
real-time diagnosis of patients with
diseases like cancer by finding similar
patients in the reference genomes in
fact it's already in use in Australian
medical research it contributes to motor
neuron disease research and well you
guys might know this as ALS popularized
by the ice bucket challenge yes so it's
a really exciting library and again one
of the reasons that I chose it as the
subject of my talk is because there are
some other competing libraries out there
that are they're also good but variants
Park has got some really impressive
results and I really would like to see
more bioinformaticians so if you work in
a medical facility here in Norway tell
your bio and fermentation friends to try
it out because the results are really
pretty darn impressive using the modern
Big Data technologies I could do a whole
talk on machine learning as you guys
we're all learning machine learning
because if you know you haven't used new
sticks since college or whenever you
learned it it's a lot to relearn and
just as an aside I'll tell you I am
personally relearning statistics and I'm
finding it to be very very useful the
process for creating successful machine
learning sections of data pipelines is
really complicated and hard and the
things that I'm finding that make you
have success around it might be
surprising to you
it's all about understanding it goes
back to thinking about problem maybe
you've seen in a different way or you
call the 10x programmer you don't want
to have the 10x statistician in other
words if you have the one person who can
implement machine learning and nobody
else can understand it that is
definitely not the way you want to do it
so the problem is around education and I
see this again and again with customers
I see machine learning algorithms that
the one person implemented when that
person leaves or when there's some sort
of problem and the whole pipeline falls
down I worked with this is inside a
YouTube Bri monetize er that was had
millions of dollars in start-up money
and they completely folded because of
the 10x data scientist so how do you get
around this you involve a team you work
together you go through these iterative
cycles with data prep statistics
probabilistic algorithms and most
importantly data visualization you want
to create a vehicle or a workspace or in
our case it will be a jupiter notebook
where the team can understand what's
happening together so to drill in a
little bit Samantha do you wanna talk
about this actually I had something to
add about machine learning share that's
all right
this is imperative to the bioinformatics
community in terms of research we want
to be able to use these tools the first
time and we want to be able to repeat
their use again in different communities
who have never heard of it before so
this process of making it accessible to
multiple people regardless of you know
their interest in code or statistics is
absolutely imperative yeah it's true and
just one more thing I read a study that
70% 7-0 70% of the published studies
that they examined for this particular
study the research was not reproducible
and I was stunned because of lack of
technical I guess sophistication and I
was just stunned so I did some other
work with docker just to help some
people on that but but anyway so coming
back to this machine learning so
variance Parc uses wide random forests
and sample of decision trees and other
other libraries out there that use
things like logistic regression
regression which to you right now
probably means nothing but we're
to show you through an example in our
notebook of how this can be visualized
so researchers can understand the
differences so working first with the
library before we get to the notebook
these are common best practices around
the library when you look at a librarian
I'm actually working with a team right
now to clean and work with the library
you want to make sure is it usable is it
performant
so we need to revalidate the
benchmarking is it extendable I found a
whole bunch of commented out code
typical kind of stuff you see a lot of
this especially coming out of research
of universities the folks that are doing
research they have never really had any
like discipline around coding practices
and so maybe they're not even doing unit
tests they have all kinds of comments
all kinds of code paths that should be
deleted so you want to have the rigor
when you're bringing this research code
into production quality are they using
the best language again this is all
about understandability reproducibility
the libraries written in Scala I've
never done anything in Scala before so
I'm using myself as a guinea pig and I'm
not finding it to be the easiest
language I do most of my prototyping
work in Python and I've actually been
talking to the people who make SPARC
about what the performance difference
would be if we just switch it to Python
because it would certainly be a lot more
understandable by a greater community or
should we have it in both are they using
best version of SPARC I found that the
sly breeze on SPARC 1x and they have
machine learning running on top of it
SPARC has a gone to 2x and there's a
whole bunch of improvements in SPARC ml
so before we benchmark test it we have
to follow clean coding processes and
again the pattern here is if you are
writing one or more modules particularly
if they have machine learning you have
to have the software disciplines that
you know and love from work already
apply to whatever modules you're working
with and then is this machine learning
algorithm understandable this is a
problem we're really wrestling with all
right another challenge for this
particular solution was how best to
deploy cloud Hadoop so we have really
three options we could do infrastructure
platform or software as a service
and I recommended to the team that they
go straight to software-as-a-service
because they're a very small team
they're having a very difficult time
finding technical talent who also know
bioinformatics they need their
bioinformaticians to focus on the
bioinformatics problems and in the case
of Hadoop with SPARC there's actually a
vendor that I've worked with quite
frequently called data bricks data
bricks is the commercial home of the
people who make SPARC they basically
made a company and they add services
around SPARC and in the US where I'm
working they're basically on fire they
actually have conferences and I work for
them I just think it's a really
fantastic
offering for these domain-specific use
cases they abstract away they write now
they run on Amazon so you're running on
ec2 optimized instances and they provide
Enterprise Security and other services
that particularly in bioinformatics are
going to be needed and let the
bioinformaticians kind of get to their
job so one of the great things that they
provide is a custom implementation of a
new type of workspace called a Jupiter
notebook now there was actually a talk
at this conference that I recommend you
watch on Jupiter notebooks Jupiter is an
open source technology the
implementation and data bricks is their
custom version of it and it's something
that you can try out so I'm going to
actually show you how to try it out here
for up to one hour for free using the
Community Edition so to start you just
click create cluster and you have a six
gigabyte memory cluster of hadoop with
SPARC on it and they have a whole bunch
of reference Jupiter notebooks so the
Jupiter notebook is a website that has
multiple sections that you can plug in
to it one section is going to allow you
to do documentation it's basically
marked down and so you can think of it
like a wiki here if I click in you can
see it's markdown what's more important
is for the runtimes that are associated
to where this thing is running and this
notebook is running on an Amazon ec2
machine happens to be in the US
a cluster I can run you can see up here
by default it's Python so if I click
right here I'm basically checking that I
have spark on this cluster and you can
see I have spark so this notebook is a
demonstration of an aspect of the API
and for you who work in BS code or
visual studio or eclipse you can think
of it almost like a new IDE a different
kind of IDE a data science IDE so you
can see that here we're going to load
some sample data and I just going to go
ahead and click run and what this is is
an interface to the spark cluster
running in this case on a single machine
and then this just goes on and I can go
ahead and I can do processing and then I
can also use other languages that are
installed on the cluster you might
remember from the spark slide that there
is spark sequel and this is similar to
an C sequel in addition just running a
sequel query and hopefully you can see
this okay
you can also graph and this is so
important to the data science workflow
so this is a general notebook this is
something that you get with data bricks
oh also inside of here you have revision
history so you can check it into github
you can put comments you can permission
it it's a fantastic not only demo of
variant spark rather than a github repo
it is also a reference way for
bioinformaticians to work together I
think that when Samantha starts college
up in the fall she's going to be using
these day in and day out in her research
as a bioinformatician and you know we as
a developer group we have a serve a new
interface to consider when we're working
with these big data solutions and that's
like one of the reasons I wanted to show
this to you so that's kind of how these
work in general but we're going to have
Samantha show how this works with data
for excellence which
so go ahead now we all know that this
technology can be used to solve very
important questions and as we said
before revolutionize the way we treat
diseases like cancer however much of
that patient data cannot be released for
privacy reasons so the team at Cicero
got separate data equally real but for a
little less dire of a problem
genetically who might become a hipster
if you have yet to see a hipster here
are some reference images there's one
there's another we weren't really sure I
went over to a Mark V and I think it is
and I spotted some people like this in
Greater Luca is that where you go to
find hipsters around here we checked it
we checked it out we checked it out so
yeah so we're going to we're going to
actually show you and you can switch to
the notebook alta we're actually going
to show you a working notebook that
doesn't look for cancer it looks for the
genes for a hipster
this one actually and academic can right
there and variants for a coupe stir so
go ahead said now we can't summarize
every single aspect of a hipster after
all they are people and they are
different but we found four distinct
traits or the team at Cicero found four
distinct traits that they wanted to
aggregate and look through the genome
for four specific indicators that these
traits might develop these are the
growth of a monobrow mono bro set
translating mono broken the development
of certain horizontal cells in the
retina that indicate a preference for
checked shirts the growth of a beard and
a high preference for coffee and sanity
you can see they're there they've cited
the genes and they've cited the genomic
reference papers so these are actual
genes that correlate this publish
research not kidding okay so that's the
problem so if you want to scroll down a
little bit actually oh if you want to
click over to the cluster over there the
cluster yeah so I've on an academic
shard here and you can see if you click
right there that I've got six nodes this
is just a demo cluster and then if you
go to libraries you can see that I have
variants Park which I just upload click
on variants Park it's a jar file so you
just upload that and then go back over
there you we are going to go and you're
going to go to the attach right there
yeah and then you attach this notebook
to the cluster so it's your UI for the
cluster and again it's a fun example
great for this use case but also from
from a you know Tran standpoint if you
go to a data science conference like
this is what the data scientists use
they don't use vs code they don't use
visuals that they use to burn notebooks
also these are renderable in github like
they will show so I mean just as a
general trend you can install them
locally to just to play around with
notebooks there's also a commercial
vendor called beaker notebooks that has
all the runtimes associated and I used
it to learn like Julia and some of the
newer languages so very important
notebooks okay let's get to the task of
him so what's the first step third step
zero what we have to do you would
download the variant spark library to
your computer we did that yep we would
import the library into data bricks we
did that we'd create a cluster yep and
then attach the notebook to that cluster
great okay so now we're ready to run our
analysis and find out about the hipster
dumb here okay so what's this for
section Samantha this is the part where
you load the data that you'd collected
so this is the part where you'd load up
your samples and try to figure out from
there which genes you were looking for
and this would be can you go ahead and
run that and they're using Python there
notice the % Python because it was just
more convenient for them and because
this is a Scala notebook because
variants parks written in Scala so you
can see on line one on that little
section there and because they wanted to
use the URL libraries and they're
loading this from s3 and this is VCF
data and where does that come from
set of reference genome yeah the
reference genomes and the sequencer
that's the data off the sequencers so
okay so this is our samples our data so
now what's our next step you would load
the variants that you already had based
on your references ok cool so let's run
that because we're doing matching to a
reference genome so you can see like in
lines 1 2 &amp;amp; 3 we're calling the variants
Park API basically we're instantiating
it and then we're calling on line 5 on
the VIX context which is the variants
Park context whether in the SPARC
context like the database connection the
feature source and we're bringing in the
samples ok and now what's happening on
three there Samantha we would be loading
our labels or basically the headers in
the excel columns that we were looking
for yeah and why do we have to have
labels on our data in this for this
situation here well the human genome of
course has so many different features
that we could possibly look for that
here we want to specify and make very
sure that the algorithm only needs to
look through what it has - yeah and it's
a supervised machine learning algorithm
so we provide it with labeled or known
data versus unsupervised where you it's
like clustering where you just group the
data together so it's it's known
labelled data and then and out a set of
algorithms right ok then what's next run
it through variance break so we got the
analysis yeah that's our that's our
import analysis and then then we're
going to run our analysis ok and this is
our big computationally intensive thing
so this is our set of decision trees so
we're breaking it down and we're
matching and we're iterating over groups
of trees to find the best matches
because it finds correlations right
between the gene types so whether the
monobrow and the beard or the monobrow
and the check shirt or which things are
most closely correlate what do you think
is going to be it's going to be hairy
caffeine drinking we'll see so this
takes about four minutes and this is an
area where Cicero came to me and said we
want to make sure that what we've
written that runs on our local cluster
because this is the intense machine
learning part is optimized properly
cloud so what we've done is if you
switch over to the repo over here
I actually forked the 3po so again I
love I'm a lifelong learner and so I
love to collaborate with people so I'm
collaborating actually some university
students who just finished data science
and we are refactoring Cicero's decision
trees and k-means algorithms which is
what they first started with so if
anybody wants to party on this project
we'll be back in states they'll be over
Skype what we want to do is refactor
this out and then we want to start
looking at some of the high-performance
deep learning libraries that Amazon and
Google have supported their open source
so MX net and tensorflow we've been
doing some work with tensorflow
just trying to see if that's going to be
a better fit because interestingly spark
the spark people announced it spark
summit that their core object is a data
frame that's how they parse the data
that they have now made something called
a tensor frame which is optimized to
work with tensor flow so to me this is
the real interesting problem space here
so long as we make sure the algorithm is
the correct algorithm which it seems to
be how we then optimize it so this is my
fork and you can just send me an email
or something and or work it yourself if
you want to play with it so how are we
doing on this is it is it running 500 ok
still running so the result of this is
we're going to see which of the what
they call phenotypes that are most
associated which of the genes
specifically has the most impact on the
development of all the hipster
characteristics and I think this is the
results table already loaded yeah we
already loaded it so now let's visualize
it so so this is running sequel and this
is what like I'm saying this is a reason
for using this notebook interface
because we can use sequel so they're
dying to know what's the top result well
it looks like here that the gene with
the most statistical importance in the
development of these hipster
characteristics is the one for the
development of a monobrow so look in the
mirror I've got some hipsters in the
room I don't know so again the greater
point is in this interactive notebook
the data scientists can work in sequel
or you if you scroll down you can see
there they're visualizing in Python with
what's with that it's ggplot compendious
pandas python with pandas which you did
that at stanford didn't you yes first it
just was it fun it was not anyway it's
grown so maybe maybe our was more fun
yeah you could say that okay so so
scroll down we have the same thing in
our scroll down same thing in our scroll
down and other the fun people at cicero
actually made a infographic because they
were they were pleased with their
results and you can see it truly is the
mono brow so another aspect of working
in this collaborative environment that
is great for bioinformatics is the
ability to do alternate analysis and
they actually did this so if you scroll
down a little bit can talk a little bit
about that sure so a competing service -
this one right here comes from a hale
which is kind of the same thing but they
calculate the importance of certain
genes using a statistical method called
logistic regression and you'll notice in
their analysis right here that always
gone to the plot it's easier to see yeah
yeah they did not include the monobrow
gene as most statistically important
they got a different result using this
statistical method which may seem
humorous but it's kind of scary in a way
and it goes to my greater point of the
importance of understandability of
machine learning because these
algorithms are complex and there are
relatively few people who can explain
them so a very big area of opportunity
in this world of big data is the proper
implementation visualization
documentation and explanation of machine
learning and it's an area that I intend
to be working in over the next 12 to 18
months with the bioinformatics community
so if you're interested you know follow
my work I'll probably do some IOT stuff
too because just have work in that area
but it's just fascinating to me that we
can use two different bioinformatics
libraries and get two different results
so switch back
all right so a couple more things about
making the decision for using
software-as-a-service
for this particular use case you know
this conference has been a lot about
serverless it's been a lot about you
know most serverless basically and i
don't want to go away from some of the
other patterns that work in this
particular case not only does data
bricks abstract away all of the amazon
implementation details they're also
really smart about working with amazon
so in this case it's just a matter of
you know using the slider to use ec2
spot instances and this was actually a
real problem that cicero was
encountering when i talked to them they
said well we know spot is out there but
we don't really have somebody to sit
there and bid on spot and see when spot
kicks up and that kind of stuff if
you're not familiar this is just unused
compute and it's a very typical use case
when you have these big and variable
computationally intensive workloads that
you just did some really slow price on a
spot instance and you can make the
workload go a lot faster
I mean just to kind of bring it to the
real world one of the runs that they
were doing on prim I think it's a one
run to get one result was 55 hours and I
walked away and I said you know I'm
doing near-real-time ads for you know
some social media service in LA and I
want to bring those technologies to
problem spaces that really matter
I don't want bioinformatician staff to
wait 55 hours every time they run
something so I'm going to take
techniques that I've learned in other
verticals and I'm going to help the
bioinformatics community to use these
techniques whether it's through vendors
or you know other processes so this is
the solution as it stands right now the
recommended implementation so it's
really simple they're using s3 buckets
and they basically have software
as-a-service
on this you know basically big cluster
and they have the spot instances and the
idea is that this will be able to scale
out and be easily replicated for
bioinformatics communities worldwide so
in terms of the pipeline pattern what we
did for this analysis is we selected s3
and Hadoop for
ingest it was s3 and then into data
bricks file system for the ETL it was
spark for the analysis that was variant
spark ml for the visualization it was
notebooks with sequel R in Python and it
was a software-as-a-service
recommendation now just to kind of tie
it back together because they're two
different problems two different
patterns you'll remember the other GT
scan the Google for genomics if you will
sort of that was a pure service
implementation working well scaling just
working perfectly and the way that was
working out was you're going to use at
the top level there for the VCF files s3
and dynamo for the ingest s3 pretty much
everything else in lambda and api
gateway so for some of these big data
problems serverless has been the right
answer for others something that hasn't
been much talked about in this
conference software as a service and
there's many many other
software-as-a-service implementations
another one I'll just start out there
and doing some work around is algorithm
iya which is a machine learning
algorithms as a service there's lots of
stuff out there to look at so that's
pretty much what we have so I'm Lin
laying it and hopefully you found this
useful and I'm Samantha laying it thank
you very much we do have a couple
minutes so if you have questions I
cannot see anything so you're going to
have to really yell out we'd be happy to
take questions all right guys just weary
at the end of the conference I
appreciate you coming out okay well
there's no questions thank you again
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>