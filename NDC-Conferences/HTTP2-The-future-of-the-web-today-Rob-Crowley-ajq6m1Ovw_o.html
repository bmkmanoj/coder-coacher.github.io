<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>HTTP/2 - The future of the web today - Rob Crowley | Coder Coacher - Coaching Coders</title><meta content="HTTP/2 - The future of the web today - Rob Crowley - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>HTTP/2 - The future of the web today - Rob Crowley</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ajq6m1Ovw_o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome to HP too and my name is Rob
Crowley so for a day job I work up a
quest where I'm the technical lead for a
POS and their authentication platform so
on a day to day basis I spend the vast
majority of my time around the back-end
server side of things and parent of that
is performance is really key to my role
and that's really gonna be one of the
the viewpoints or lenses through which
we're gonna look at HDTV to today so
we've got quite a number of things I
want to cover so first of all I want to
give you context I want you to
understand why HTT db2 is a thing how
did it come to be what was the rationale
behind it we're then gonna look at the
major features what changed with HTTP 2
what you need to be aware of we're then
gonna dig into one particular feature
server push which i think is probably
the most misunderstood feature or Paris
of HTTP 2 I'm going to look at the
current browser support for that so
there's a few rough edges right now I've
hoped for today I'll give you the
information that you will need to be
successful using and/or applying it to
your own servers but most of all I
really want everyone in the room today
to get a solid understanding of the
category of problems that htv-2 will
help you with and for you to go back and
continue to learn it yourself
realistically we only have an hour today
so I will not be able to make you
experts it is a 1 hour but if I can
instill in you that passion to learn
that little bit more
that would be successfully so htv-2 is
really you know the next incarnation
what is the the workhorse of the
Internet there are millions upon
millions of devices that use HTTP every
day to transfer data across the web but
HTTP is not an island unto itself it
actually forms part of a stack and
there's actually a number of different
protocols in place to actually get your
requests safely to the server and back
again so we're going to spend a little
bit of time digging into the role that
each one of these protocols has to play
because this will give
better understanding of some of the pain
points that we have with existing
version of HTTP what Daleks and what we
tweaked it improved in HTTP too so when
we're talking about HTTP we're really
talking about the left-hand side of this
so we've got like the application level
protocols up the top then we've got TCP
at the transport layer IP so ipv4 ipv6
at the internet layer and finally we
have network access down the access and
I but what do each one of these layers
do for us so what we're doing anything
about explaining that records it's
always good to look at the OSI model
there's a standard for defining you know
what responsibilities should sit with
you know each layer so starting with the
network access layer here we're really
talking about physical bits going across
and network so it so it layer what we're
talking about physical medium how do we
said a physical base across that wire
layer to would go up a level of
abstraction without talking about
reliable delivery between two nodes on a
physical either they are the physically
connected Network where the IP protocol
or the Internet Protocol comes into
players at layer 3 here we're talking
about addressing so again the into this
is huge it's not all on one physical
network you need to have addressing and
routing and that's where the internet
protocol comes into play then you have
transport so in this case TCP this is
all about reliable delivery of data so
retransmissions if it fails in order
delivery so this is really where your
quality of delivery comes in this is
everything that says ok how will that
data actually get from the client to the
server in the same order without
something like missing or going awry
along the way session that is about
continuous communication between the
client and the server so at the web
today isn't about a single request at a
single response but we load web
applications that have numerous
interactions between the clients of the
servers and this is where we maintain
that connection and then finally at
layer 6 or 7 this is the sort of level
of abstraction that we mostly deal with
so when we're Weissach writing web apps
we deal with data that's already
decoded that we're dealing with
resources not data's or frames you know
it's actually at that resource or
application level and there's one other
bit of introductory information I think
we should all be aware of because this
has a huge influence on web performance
today and those two things are bandwidth
and latency so if the eight fallacies of
distributed computing taught us anything
is that bandwidth is not infinite and
the network is definitely not
heterogeneous you can see from here that
from the client to the server you may
actually pass through any number of
networks and some of those networks
might have very high bandwidth insofar
as do you might be able to face a lot of
data across that pipe at one point in
time or in this case we can see cable
there's less data the volume of data you
would be able to send end-to-end is
gonna be constrained by that bottleneck
that's a key thing that is your
effective bandwidth there's another
facet as well which is about latency and
then is the speed of light and that's
about 300,000 kilometers per second
which might sound incredibly quick but
in the era thee'st is actually
appreciable and in fact it's actually
the fair-haired or problem to solve
we're bandwidth we can just put in more
cables we could just put it more you
know optic optic fiber here or if you
know cabling over there I just increase
that bad word we can get out of that
trouble with money we haven't found a
way to solve the speed of light as yet
so what we've got to do for the next few
minutes is take a whistle-stop tour
around the history of HTTP where it
started and where it took us to today so
it all started with Tim berners-lee back
in 1990 and at HTTP not that neither
really this was the ultimate minimal
Viable Product like he really got how to
do simple I don't think you could
actually have it any more simple than
this you could open a connection you
could issue a single get request to a
particular resource you would get some
hypertext document back and the
connection would be closed
that's it but for the use case that he
had admired which is basically passing
you know scientific
between sizes hazard it was perfect but
of course other people wanted to do
other things with it as well so between
the periods of 1990 and 1996 there was a
huge amount of change I really adhoc
change so we weren't happy with text
alone we wanted to introduce images and
rich hypermedia
we wanted to say okay well actually you
know sending these images back and forth
across the wire every time that's not
two Samaritan we want to introduce
caching and really with HT wonder oh it
was really a reaction to what was
happening so Wow so much stuff was
happening in the space let's just write
it down so it wasn't a formal
specification that you could actually
code against it was really just an
attempt to catalog what had happened in
that space fast-forward to 1999 HD 1.1
this was the first formal standard that
you know actually came to be so this is
the first time the view actually took
that implementation and coded against it
there was a fair chance that a client
would actually be able to operate
against it and it was an absolutely
massive success so in fact HTTP 1.1 is
the web that we all know is what we
would have been building against their
entire careers and it's really been the
cornerstone of the web for almost two
decades and it's absolutely amazing face
and it gave us a couple of really
interestingly powerful features so you
would have remembered back through HTTP
not deployed and near wondered oh it was
a single request response we would have
opened a connection made a request
pulled out the responses then closed the
connection it 1.1 we got keep alive so
that was the first time we could
actually make multiple requests on the
same connection really good for
performance they also look to introduce
a feature to improve parallelism across
the protocol was pipelining probably
some of you might have heard of that and
that's because it never really took on
there was a lot of implementation
complexity around this so effectively
what we were looking at with HTTP would
that what is effectively still having a
serialized
one request one response so what we've
got to do now is travel back in time I
take a quick look at well what websites
looks like in 1999 this is Yahoo so this
was one of the most popular sites on the
web at that point in time it's actually
some of the content still you know
pretty pretty relevant but if you think
the page looks impressive like check out
the back up like that one case we
actually have tables that are like four
levels deep you know there was
absolutely zero semantic markup here and
yeah I was around this time it was
awesome you know tables zero you could
absolutely get a job as a web developer
you know you knew colspan
you were ready for promotion so Google
you know possibly the company that you
would say that is the biggest web join
today they're still in beta you know
they haven't even gone you know live yet
with you know a full production product
and if you're wondering what the layout
is here yeah it's a table so you've got
an idea now of what HTTP the post all
looked like what sort of sites we were
building you know at that point in time
so what got us thinking about a new
version of HTTP being you know required
so obviously everything's looking good
you know then this happened okay even
with the massive amount of tables and
clippers at my site I didn't quite
manage to break the web what started to
really sort of like tip the balance
though we sort of watched be coid web
200 and this is really just a phrase
that Tim O'Reilly popularized but it
still it to mark this sort of change
between consumers just pulling down data
from the web to actually contributing it
back so user created content and this
really changed the sort of like
landscape of the web so we went from
just having plain documents to web
applications so we were looking for
richer experiences and in fact between
2004 2009 the average web page size
dribbled you know that is parent leave
due to content apparently because people
weren't happy with the limitations of
page table in terms of my crafting
experiences so we introduced JavaScript
and a whole bunch of
all the things again to you know make
the bait the experience more interactive
and to put that in context there's a
site called HTTP archive that basically
records interesting stats about sites
you know over time and unfortunates data
OD stats back in 2010 but what we could
see here is back in 2010 the average
website had about 80 connections to pull
down the entire you know the entire
while 18 resources to pull down the
entire site and about 700 kilobytes of
data you can see that this is a massive
spike so this is accurate as of
yesterday we now have the average site
requiring 108 connections to the server
to pull down the data but even more so
over three Meg's worth of data that's
absolutely bonkers
I don't think Tim Berners that he could
even have imagined that what he
originally created it but there's a more
subtle problem here is that TCP is
actually optimized for log data flows
like it is brilliant for downloading
large files like that's what it's really
made for the transfer of long streams of
data but as you see yourself we've got
three Meg's of days that are roughly you
know 100 days to 110 you know
connections
that's not long-lived that's incredibly
short lived in jetty and in fact to
compound that we're actually seeing an
HTTP 1.1
74% of our connections just carry a
single transaction that's exactly what
we were getting with hasty to be not
online requests pull it down close it so
there's a there's a quote that I really
like it originated in the motor industry
with a Jackie Stewart and he's basically
saying that you don't have to be an
engineer to be a racing driver but you
do have to have mechanical sympathy so
what he's saying here is you don't
actually have to understand every single
way to put a carrot together but you
have to understand how the care drives
to get the most out of it so from a
computer science perspective we mostly
though this term to do with like Marisa
Thompson and the work that he does with
disruptor in terms of building you know
high-performance you know trading
platforms in Java he's also using like a
ring buffer so you don't actually do any
garbage collection it's absolutely
incredible stuff but he looked
the JVM and he built an application that
they worked in harmony with the JVM we
are absolutely not
doing that work websites we are not
working with TCP at all we are trying to
work against it there's no surprise then
that it's kind of highlighted a number
of deficiencies in HTTP wonder Oh with a
current way that we're using it
so there's head-of-line blocking lack of
parallelism and protocol overhead so
we're going to step through each of
these and again this is really what you
see will have arisen or the problems
that have come about because of the way
that we're building sites so HTTP is a
serialized protocol so you can only have
one request active at any point in time
so if you had a queue of requests they
will be processed sequentially so as you
can see from there you know any flow
requests then will hold up any requests
that are following him and what about
moistens seem like a massive issue to
begin with when you actually look at the
effect on bandwidth utilization that
actually becomes quite serious so in
this particular case we're making a
request for index.html server thinks
about a little bit processes and service
back that content there we make another
request for the stylesheet and we get
that back at this point though in the
request graph nothing is happening like
the line is just date so if you looked
at like you Google developer tools do
you ever looked at how a web page loads
you'll get loads of just like white
space in between little bursts of
activity that's latency that's exactly
this network idle time that I'll be
showing you so the other major issue
that we have is protocol overhead and
these really come about from headers so
HTTP is a stateless protocol so any
state that we need to pass we actually
send it to the server to comes back to
us again and a couple of major problems
with this is that on average no we send
800 bytes worth of data back to the
server and back again this is
uncompressed so we spend a lot of time
gzipping the bodies of our resources
this headers and the cookies they're not
compressed at all particularly if you
consider slow networks but
might go to your mobile as well that
could have a real serious impact on
performance so where there's a will
there's a way and we have overcome or
done our best to overcome these by
really cobbling certain best practices
together over h3 what daro and the one
that we might be most familiar with is
domain sharing and what we're looking to
do here is overcome the lack of
parallelism in the protocol so I said
before you could have one request and
one response and obviously we want our
data faster we want to quicker so
instead of just limited to the six
concurrent connections that the browser
will create to any host so as a matter
of interest when HTTP spec was a read
was originally created it was a maximum
of two but again as our experiences have
you know demand that we build richer and
richer things this is now grown to six
but six is not enough so what we do then
is we actually just create separate
domains that might go to the exact same
host and then we could have twelve
connections or eighteen connection
there's brilliant we could just as have
as many as we want to be to pull it
older and in this case an Origin is a
triple of the protocol the host at the
port so these could absolutely be on the
same machine and you might think that
this is an absolutely wonderful outcome
but again it comes with a cost actually
quite surprisingly high cost when you
actually dig into it so what does it
actually take to establish a TCP
connection so every TCP connection
starts with the three-way handshake we
probably all would have heard of it you
send the synchronize packet you get a
magnet a sing acknowledged packet I did
you get the acknowledgement that's that
that round-trip happens before any
actual data can be said so the
round-trip time if you looked at like
the speed of light between perth where i
work and where we earn though is 20
milliseconds but we all know as we've
seen before that we can't actually get
our network to operate as if they were
in a vacuum so in real life is actually
more like 50 milliseconds so before
we're doing any
this is 100 milliseconds before I've
started even said the ADI data but we'll
use HTTP no don't worry so again this is
an unoptimized handshake
so I'm actually showing two round-trips
here again this could be 300
milliseconds before adi data it gets
served
there's probably some people in the room
now and go oh you could use TLS false
start there but I could you know remove
whatever chip you could but this is just
to illustrate again that the cost of
even created connection before any data
is said is actually quite high and we're
not done yet so 300 milliseconds has
passed and we're ready to send our first
actual piece of data but how fast the
Internet we don't know and TCP has
certain algorithms that basically
prevent us from just firing as much data
down the pipe if it's not actually gonna
reach the server
these are congestion controls and it's
got tcp slow-start so to run you through
an example let's just say I wanted to
grab what image my my avatar which is
just 50 50 kilobytes in size so tidy
should be happened almost instantly so
that but you could only send 10 TCP
packets at most on your first connection
that is the biggest initial with the
size you can have so that means the
server can send at max
14600 bytes where did that number come
from so the maximum transition unit for
TCP is 1500 bytes and then there's you
know I've taken all you know sub header
you know some sort of like boats off the
top just for ipv4 headers so we're there
with 14,600 that comes back to the
server and I acknowledge each of the
packets so I can add each one of those
acknowledgments to the initial with the
size of Ted I'd hit presto I could serve
20 the next time and finally the last
round trip I had said the remaining six
TCP packets still doesn't seem too bad
you get data roughly again 300
milliseconds but the key point here if
that wasn't already created connection
if that was a warm connection that would
be one round
because the image would have easily
fitted in the maximum TCP window size
even if you don't introduce window
scaling so what you could just see for
creating short-term short-lived
connections you have an upfront cost of
actually establishing the connection but
then even before you get a maximum
benefit of that connection it takes a
number of round trips as well and you've
already seen 74 percent of connections
only have one resource so this is
happening a hell of a lot even worse
than this each one of these 18 TCP
connections that we've now created all
vie for limited resources so TCP has
very clever congestion control
algorithms but they're not aware of each
other so on your machine each of these
are fighting for that little bit of bed
with you may have when I can't get
enough packets will be dropped and again
you'll be back into the exact same cycle
as I showed you before so I hope we're
seeing now is while we might think
there's certain optimizations that we
can apply the evilly just domain and
debate shared all the things there's
actually quite a lot actually going on
the back end so so far we've been
talking about application level
optimizations just things you can do to
improve your own page speed I worked at
Bank West and through many people
working the enterprise as well and we
have a very clever networking team and
they actually apply a quality of service
policy over our network so if you
actually have an application that's
behaving badly you may actually be
running against not only TCP but your
organization's quality of service policy
as well this app has opened up a teen
connection what's it doing absolutely
liberties as well so I can say well he's
working in an enterprise have a chat
with your networking team I see what
policies they've applied or may not have
applied so a couple of techniques then
that we use to you know improve
performance so we create image sprites
so and the rationale here is instead of
pulling down loads of small resources
we make one big image and we basically
pull down the image and then we select
certain you know portions of that but
this is actually quite effective so we
actually reduced like a lot of relative
tubs to the server but as always there's
trade-offs so again in this case the
little icon that looks kind of like
something you'd see in a toilet or if
that changed all of these other images
if you've had a caged on the browser you
have to invalidate and pull it down as
well so there's always trade-offs so you
get poor case semantics with these and
what we really do dealing with is trying
to trade cache efficiency for reduced
requests on the network do you really
have to see about the volatility of your
resource to see whether these techniques
make sense so on a similar vein we stick
resources together so head-of-line
blocking is costly and again we're
looking for other ways to reduce the
number of requests that go across the
network so again instead of downloading
many small ones let's stick them
together adèle know the few large ones
with the same content in some images
this some though we're talking about
JavaScript add CSS and we concatenate
them and we minify them and it's great
same sort of thing
what happens if just one of those
JavaScript files change the entire
bundle is invalidated so this probably
starts you thinking about well what
should go in a bundle and you should
always try to group the volatility of
the resources together so if you've got
your own Jerris group resources that
changed rapidly don't stick it with
jQuery or something that doesn't change
a lot you'll just force your users to
download that time and time again so
it's actually quite a lot of tuning
involved to actually get your the most
out of techniques like this we inline
resources we basically stick other
resources within other resources and
again there's perfectly valid use cases
for this so you know web performance
optimization technique you know years
and years is to inline your CSS for
above-the-fold content on your page so
it doesn't have to roadtrip because
browsers will block read there are
JavaScript and CSS you know that's all
good if it's just in the page you can
render it straight away but we also
stick in
and other things data you are eyes to
actually embed them in pages embed them
and CSS again to remove these road trip
times there's a lot of pernicious sort
of side effects of this as well if that
image appears on multiple pages all of
those pages will have to be invalidated
to actually refresh that image you could
be sending more data across the wire as
well all in all these practices create a
tension between what we want to do in
dev and what we want to do in production
so while we're in development we just
want to have the gyroscope we want to be
able to debug it we don't want it
minified that uglify did everything else
we just want to be able to use it so how
do we actually handle that that's extra
complexity that we have in our build
pipelines and no tools can help so we've
got you know grunt and gulp amongst
others so this is probably the simplest
god file you will ever see we're just
grabbing a bunch of JavaScript we're
sticking them all together in a bundle
file
we're minifying them we're creating a
name based on the hash of the content so
we're sticking it in this distribution
folder anybody who uses this stuff
themselves will probably know it's about
you know a quarter or a tenth the size
of the words you will actually have and
the cost of such tooling is additional
complexity but we just accept this stuff
as being normal or you have to have your
gulp file what do you go to do Rob
but what I've started web development I
had two different ways of adding an
event handler to see whether I was
targeting you know ie or you know
Netscape right now we'd absolutely think
that is bonkers we will start seeing
that you know all of this sort of a
concatenation that will just become
unacceptable in the future there's
certain things with minification will
continue to do that that makes sense
like reduce the amount that you said to
cross the server but all of these are
the hijinks that we have to you know try
to like you know coerce resources
together yeah it's it's something that
needs to change so fast forward that to
2000 I'd and a number of researchers
asked speedy then we're saying okay well
how can we actually less or allow web
developers to remove these sort of bad
dates that we've been doing and I
started looking at well what would we
really want to do if we were to build a
new ver
htdp what features would we look to
introduce and I'm not going to spend too
much time on speedy itself because this
is the precursor for HDTV to so many of
its features have met more or less wood
to wood into htv-2 so as we explained
HDTV to your treads is fully understand
what would have the unit speedy and the
first step of this exercise was
basically to look at what the
performance constraints on web pages
were so for that thing ran a controlled
experiment so they took a number of
websites and first of all they looked at
bandwidth so they basically says how
does page load time change with regards
to bandwidth so they started with a
wooden megabit per second link and they
saw they've got a page load time of over
three seconds then they doubled that
speed to two megabits they've got an
amazing performance improvement but then
they improved it to increase it to true
megabits a little bit of a performance
increase all the way up to five megabits
and after that you're only getting
single-digit improvements so what you
can see from there is in many cases
increasing your bed with past five
megabits per second isn't actually going
to improve your browsing experience so
the NBN will improve downloading or
streaming video from Netflix it probably
won't actually increase your browsing
speed or your browsing experience but at
all then they looked at latency and they
said a ratcheting down latency from 240
milliseconds on the Left down to zero on
the right what you could actually see is
as latency decrease you've got a linear
improvement over time so what this tells
us is in most sort of like modern
context in which we use we've got good
network it won't actually be bandwidth
that is contrary to you it will be
latency and this is really the
foundation of all the principle which
HTTP 2 was designed so HTV 2 is about
low latency transport of content on the
web that is really the
angle at which it came and adoptions
rising fast so this came out in 2015 and
already we have over 16% of the sites
worldwide using it and that's accurate
as of yesterday and that might and sound
like a large number but it actually
represents over 50 percent of the
traffic on the web so you could really
see the large sites like Facebook like
the others as well they have adopted it
as well
and htv-2 respects all the existing HTTP
semantics so headers you know cookies
all of that that hasn't changed
you know let's let's face it it's
probably a good design choice breaking
the way wouldn't have been a good idea
so one of the really major changes with
HDTV 2 is how data gets transferred
across the wire so instead of all these
separate connections everything goes
over one single TCP connection per
origin but you're probably thinking well
didn't you just say that we got really
bad bad with utilization or the TCP
connection what did that make before was
absolutely horrible and this is where
the binary framing there comes in so
unlike its predecessors HTTP 2 is a
binary protocol
no more tell netting people you can't do
it anymore but what we actually do is
take the request as it would look like
an HTTP wonder oh and we actually frame
it into sections of data call frames and
then what we can do with there is again
in this particular case we have headers
and they will map into a headers frame
we got data and data will map into one
or more data frames there's actually
nine different types of frames as
defined in the spec and some of them are
pretty much the same functionality as we
as we would have already expected but
there's some other new features that
allow us to do some pretty cool work
which will explore it a little bit so
once we have a request and we've matched
that request to you know a number of
frames they form part of a stream
I know across the wire we can actually
multiplex these streams so it's no
longer one request one response but we
can actually just send multiple streams
across the wire and it's actually
bi-directional so in this case we've got
a little bit of stream Ward bitter
stream - bit more stream water - so
that's actually how they've got
efficiency of the wire so no more head
of line blocking at the application
level because we could actually
multiplex all of these streams back and
forth so again to a diagram perspective
this is actually great because instead
of this sort of cat-and-mouse game that
browsers need to play today with HTTP
1.0 where they're saying I'm passing a
web page I know I have six connections
that I can send to the server as I pass
the Dom and I find you know another
resource it should I ask for that know
what happens if I get a CSS so the
stylesheet that's coming up that really
should have higher priority should I
hold on to a request that they play all
of these guessing games we don't need to
do that though basically as the browser
or the client comes the posture comes
along with a request it just sends it to
the server because it knows that it
conveyed have that request or that
stream prioritized so we could apply a
quality of service over so remember with
HTTP 1.1 the row we had 74% of
connections carried a single transaction
with HTTP 2 and the binary framing there
that plummets right the way down to 25%
so we can see we're actually getting a
lot more value out of the underlying TCP
connection and there's plus sides to
this as well so know that we can
actually say we don't need to glue all
of our resources together simply to
reduce the number of round trips
we're actually finally in a position
where we can get value out of HTTP
caching or as a lot of ideas we really
should have been with this so again I
hinted at it a couple of slides back you
say oh well with HTTP wonder oh we had
to like carefully
massage the request that we said to the
server but no we could just send them
straight to the server and this is
really comes down to you know now having
to invert this relationship whereby the
browser was responsible for
prioritization now it's the server and
how does it actually do that so would
stream prioritization each of the
streams themselves can have a number of
factors that define this priority so it
can have a weight and a waist basically
defines a number between one and two
five six which which hints to the server
how important is this resource so for
instance you might say an image might
have a priority of five but then you say
this CSS is really important I want to
give this a priority of ten and I guess
the key thing from there is that these
are really just hints that you send to
the server the server is not obligated
to respect them I generally will but it
has certain heuristics itself as well
what you can also do is you can set up
dependencies between streams so in this
particular case we're saying that one
stream a has been delivered equally
divided the available bandwidth between
streams B and C and you could see that
there's going to be equally distributed
because the weights are the same and
it's a simple algorithm like that simply
dependency and waste but it allows us to
do some really clever prioritization of
workloads so we can also dynamically
change the priority of a stream and
listen to this for a scenario so again
the user loads of page and there's an
image in the top left-hand corner and
you say okay well absolutely this might
be quite a big image but it's really
important to be user at that point in
time but then we scroll down that image
goes off the screen before we would have
no way of telling the server to stop
sending that we would have still got all
the data back down to the client now we
can just change the priority of that
stream I'd go I don't really need that
image anymore you could hold on to it
and then we will reprioritize the
available bandwidth to other resources
as well so I guess the key thing from
here this isn't a static graph that gets
created
the static page loads this can change
based on context throughout the page
load type read the time and rod tub HTTP
to effectively means HTTP okay the spec
itself doesn't mandate that you use you
know SSL for your connections but in
practice you had to and there's a lot of
plus sites to you know using you know
secure connections be obviously as users
on the web we get our data
you know encrypted and our privacy
respected but this actually wasn't the
major driver for going down HTTP and
it's a bit you know surprising but the
Internet is absolutely riddled with
middle box debris there are NAT devices
proxies IDS's when optimizers that all
look at traffic over port 80 and they
try to do their best to improve things
so they will look at it and say okay
well I'm expecting htdp wood that'll
shaped traffic that comes across let me
see if I can compress it as we just
learned though HTTP to traffic is binary
they don't understand this so in the
face of this they either do one or two
things they are they just fall over or
they think it's the various or dodgy and
they will close the connection and
initially when they went to roll out
speedy they did some tests in this area
and they saw that between twenty and
thirty percent of traffic fell to this
exact problem and again if you're
building a new protocol thirty percent
to try for any other web would
absolutely have not been acceptable
swapping it over to HTTPS everything
worked smoothly so while the HTV you
spec doesn't mandate that you have to
use HTTP it does of subcarriers to say
if you do you have to do it to you know
a certain level of you know standards so
you have to use at least TLS 1.2 most of
us will be as well if you had like the
SSL happily this year from last year as
well you probably be aware of that and
it also has an opinion about cipher
suites so
it recommends that sofa sweets were
perfect forward secrecy are used so by
that it's effectively in some of the old
or assay based cipher suites they reuse
the key for negotiating their connection
between the server and the clients to
actually encrypt the traffic as well so
if you've got your hands on that key you
could actually use that to decrypt all
the past traffic that had gone between
the client and server as well really
scary proposition so we're perfect
forward secrecy you can't do that so if
even if you compromise the key for a
particular session it will not allow the
attacker to decrypt old traffic as well
so the spec defines quite a long
blacklist of cipher suites there you
should dot the go she ate over-hasty to
me - and if you're interested you could
check it out of the spectrum and this
actually had some really positive impact
on the web as well so HTTP adoption has
more than doubled in 2016 and now it
comes from other 50% of traffic on the
web so I think even a respective of HTTP
- this is a really good outcome but for
many people deploying HTTP to HTTPS will
actually be the biggest issue
particularly if you host third party
content and you've got mixed
authorization or authentication on
that's all a bit that's all a bit nasty
there's actually a really good blog post
by Nick raver what are the ideas a stack
overflow it's really like a book and he
basically details all of the challenges
the Stack Overflow faced with shifting
to HTTPS over like a two to three-year
period and that's absolutely amazing
read so if you have some time absolutely
encourage you to take a look at that
so HTTPS also gives us some benefits in
terms of how to actually negotiate HTTP
- connection so they created this open
so application level protocol
negotiation that as part of the TLS
handshake allows you to negotiate what
protocol to use without doing another
road trip and it's quite simple as
parent of the client hello the client
basically lists what available protocols
are understand so if it understands HTTP
- it will say okay that's cool I will
put HC in the list the server will then
take a look at this and say what
protocols can I talk and then respond
back to the server
so this this performs the exact same
role as NPA did in speedy except with
speedy it was actually the clients that
ultimately chose what protocol to use so
when it's actually got ratified into a
standards they switch that
responsibility to the server to align
with other protocol negotiations that
does that exist on the web so we looked
at before the protocol overhead was a
really big problem with HTTP 100 all of
that uncompressed header data so as part
of the spec was actually a separate RFC
they created a CH pack and what this
does is it comes in to parents so first
of all we have a static dictionary that
is defined in the specification and what
this does is it Maps common values so
for instance here the accept header two
numeric values within this dictionary so
what this lets us do instead of sending
accept or a user agent header let's just
say across the wire we will just send
the number 19 that's it so it's a really
simple translation but incredibly
powerful and in fact if that header
hasn't actually changed between this
communication it won't send it at all so
we can radically reduce the amount of
data said back and forth between the
server's so there's a static dictionary
for the predefined standards like user
agent authorization often comment values
for status headers as well like okays
and no contents that Modifieds
but there's also a stack there's also a
dynamic dictionary as well and this is
for maybe headers or values that you've
created yourself as pairs of the
application and they will they will also
go through the same process of being
Huffman encoded to a numeric value and
then that key just being passed back and
forth so domain sharding we said before
that we can span connections across
multiple hosts and you know some flosses
lots of minuses and actually HDTV too
has a trick up its sleeve for this so it
will actually coalesce two connections
if a number of criteria are met
so if you have two separate origins that
actually map to the same IP address and
are covered by a TLS certificate that
covers both domains so for this
particular case we can see that the CND
subdomain of example and example comm
will have the same we'll have an SSL
certificate of covers both so we will
actually coalesce that connection to
just have one between the client and the
server so this is a good technique again
if you have a current site that you've
got to have some users that will
negotiate HTTP 1 dot X and some will
negotiate htv-2 you can actually stare
architecting on the back end to actually
optimize for both scenarios so I've been
talking a lot about latency and maybe
there's a few people in the room here
that says I wish they'd just sorted our
cookies so latency was really the
biggest issue that the working group saw
it doesn't mean the HTTP 2 will stop
here and they will continue to iron out
some of the other issues it's just that
latency was the biggest one they wanted
to get fixed up front I'm probably key
thing here is this is from the head of
the via the working group and it's not
magic you're not just gonna stick it on
to your size and it go 50 percent faster
probably a lot of the hacks that I
mentioned before actually know
counterproductive so we actually have to
start reversing a lot of this to
actually get the major benefits of htv-2
and I guess the key thing from there is
take a scientific approach don't assume
make a change and measure it and there's
some tuning that can really help you
with this which we'll discuss later so
one of the key features of htv-2
is server push and this really changes
the way that clients and servers
interact so what we can see here is an
example where the client asks for index
dot HTML and including it gets back to
HTML but the server says oh if you ask
for the index dot HTML document chances
are you will want the CSS and the
JavaScript as well here you go
and that's absolutely fantastic because
we actually save ourselves another
request or another round-trip from the
server and of course the client can
actually
then to you know reject that push by
just calling a reset stream so this is a
similar technique to what we might have
done if we were looking to deep-fry Ora
toes or just cancel a request from the
server we can say I don't want this tree
buddy board and this actually opens up a
huge amount of potential benefits for
Spyro servers so you could build a web
server that then and jetty has an
implementation in the Java space already
there we'll actually look at how users
are actually interacting with the
website and actually based on the
referer header build up user trending of
traffic and then actually optimize
itself about what resources to push so
there's a whole level of optimizations
that towards possible before but this is
really as an industry what we do so if
anyone's familiar with the JVM it's got
hotspot which will effectively look at a
piece of code that's you know operating
slowly and then either look to inline
the methods are improved as well this is
the same sort of a road type performance
improvement that we would have at that
space just apply to a different context
and if you have certain devices let's
just say that you're on an emerging
market or in your mobile phone and you
really say actually I'm happy to just
leave with the latency of maybe that
extra sort of road trip I don't just
want the server to speculatively push
data to me in the initial connection
negotiation you can just define in the
settings frame I don't want to be
involved with server push and it's as
easy as that a key thing to understand
about server push is this actually a
really low level networking feature and
in fact anything is not exclusive to
browsers can actually leverage that so
from a kind of like a page flow
perspective I've actually just for the
for the interest of brevity I've
actually include I've actually excluded
two cases here the preload case in the
image case but when a page actually
looks to resolve a resource it will
actually go through a number of hops so
first of all it would look at the
serviceworker though I have a caged copy
of that resource at the service worker
level if it doesn't it will go to the
HTTP case if it doesn't exist there it
will then fall back to your push cache
and the push cache itself lives in
the http/2 connection it is tied in as
part of that connection and there's a
couple of things we could have from that
so you can actually push things that I'm
act as being no keishon no store it
doesn't respect HTTP cache adders which
is cool it's absolutely what you'd
expect but you can only read something
from the push cache once that might
sound like a bit of a limitation but
effectively in many cases it will just
move from there to your HTTP cache
anyway but there's a couple of things to
be aware of if the connection is closed
you push caches god so from there don't
push things that you think the client
may need at some future point in time
push things that they will need almost
immediately and more gotchas so if you
have any authentication on your site or
authenticated requests an
unauthenticated requests will actually
go down different connections to the
server and you really need to handle
these appropriately or any
unauthenticated requests won't hit your
push occasion and this mainly means for
if you're doing a fetch including their
credentials there may be certain cases
where this won't work and if that's the
scenario you'll have to go into the
serviceworker and actually change how
that network hole is made and browser
support for server push it's a little
inconsistent right now like like the
binary framing there is absolutely
rock-solid but there are certain age
cases with server push that you know you
could really see you know that you won't
be getting the most value from it
so chrome very good Firefox very good
Safari shopping age kind of okay but
really from today's from today's
scenario I would really only want to
push to users using Chrome and Firefox
so from their use user agent sniffing to
just determine what user agent the
client is using and based on that you
know push or don't push this is just
because it's an emerging technology but
hopefully if someone's watching this
video you know it a year
time they could probably ignore all the
advice that I'm giving for this 30
second period so a key thing is if you
get server push wrong it will actually
hurt performance so if you're pushing
resources that the client doesn't need
or the client can't understand or use
then it'll be worse than not support to
get at all so really measure and test
this so if you are pushing content
actually make sure that that is actually
being used on the client side so an
interesting observation is again the
best performance you can get out of a
network based system is actually by not
using the network it might sound a bit
counterintuitive but don't send data you
don't need to send and right now we have
no way of the client actually being able
to say to the server I already have this
like don't send it to me again I don't
want it and there's a new standard
that's gonna be coming out called cage
digest and that will allow the client to
basically say here are all the resources
I already have here you go server if you
wanted to push these to me I'm good so
that's not available as yet but I really
see this as a key feature that will
actually allow us to have you know more
valuable adoption of server push so
right now we have a number of ways that
the client can already proactively pull
down or ask for content we serves from
the server so this is preload and
prefetch so a preload of would
effectively force the browser to make a
request to the server to pull down a
particular resource so you might do that
if you have a bulky page form you might
pull down a page would subconsciously
did for you know page 2 so it's
available where the user wants to
actually navigate the next prefetch is
the same but it's really just a hint it
does a force the browser to make that
request so an efficient server push will
always be faster than either of these
two techniques but again given where we
are with server push support I'd
actually say he should be your staple
bread and butter for today and then look
at server push mainly when case digest
comes along so what are some sort of
like perennial good ideas to be doing
with web performance
effective of the version of HTTP so
minimize DNS lookups so that's where you
get the domain name and it has to be
resolved to an IP address that's can be
quite a costly process at certain times
so minimize them so requests are
actually blocked when you're resolving
the name that's a key takeaway we use
existing TCP connections bit of a
no-brainer with htv-2 you just got one
per origin but pay attention to
credentials and non credential requests
you know creating a connection is
expensive so minimize the number of HTTP
redirects so again they will rather be
three or ones or three or two possibly
three or sevens that you throw back to
the clients that will then have to
follow another link that's just more
road trips try them either handed out of
the server but yeah bitter boys that
deployed so again minimize the amount of
data that you send on the wire so if
there's any resources that you just
actually don't need to send just clean
them up
other than that effectively set your
cache headers so be aggressive with them
so again you could actually use you know
never inspiring cage headers if you said
you know like a hash the file there as
well there's certain techniques that you
can do from there as well so use a CDN
so CD ends are at their best when
they're actually geographically caching
data close to your users but there's
other uses for them as well you can
actually do on cased origin fetches so
even for content that's personalized the
can't actually be cached you know in an
intermediary proxy what you can actually
do is the CDN you can actually talk to
the CDN and you will have your handshake
terminated with a very quick handshake
the CDN can actually keep alive a ward
set of connections down to the backend
servers as well so you get all the
benefit of an already warm TCP
connection on the backend so again some
sort of like on obvious for quite
powerful techniques so hopefully at this
stage you've said HTTP twos can really
solve some of the problems that I have
so here's a couple of tools that will
actually help make your life a little
bit better so chrome internals
absolutely fantastic it will let you
browse all of the HTTP through traffic
and actually see frame by frame what
send me across the wire so this should
be your bread and butter on a day to day
basis from a
performance perspective which
webpagetest is absolutely fantastic that
will actually let you do a number of
tests from geographic locations around
the world to your site to actually test
across different there were condition
conditions as well really powerful
Fiddler's result because see sheriff
doesn't spoil the Alpen yet but you can
use Wireshark there is a bit of a gotcha
with Wireshark as well insofar as you
actually have to export the SSL key log
file to actually a low wire spec to
decrypt that traffic and that's actually
due to again the perfect forward secrecy
so there's an environment variable that
you can cyst that then chrome will
actually export that key log and you can
import that into the Wireshark settings
then and the it will be able to decrypt
that session traffic it's ok knows the
blog posts are where online if you want
to take a look at that so so far we'll
be talking about HTTP two quite a bit so
what's going to be coming next from a
protocol perspective so serving HTV to
over TCP we've sold head of line
blocking at the application level that's
absolutely fantastic but we still have
head of line blocking at the TCP
connection level so a single drop TCP
packet can introduce head of line
blocking for every single stream that's
obviously not so great so researchers
are currently looking at quick and from
here it's actually that's gonna be based
on UDP which is generally use for Jazzy
or you know sort of like latency
sensitive media but what this will do is
it will actually improve the
multiplexing of streams such that if a
particular packet of data grabbers drops
for what are those streams only that
stream itself will be affected not all
the streams and again another key goal
of quic is to reduce the cost of
initiating a handshake between the
client and the server so this will
actually allow for a zero round-trip
resume if you actually resuming a TLS
session or one round-trip for a new
session and in fact some of these
features are actually already making
their way into TLS 1.3 and in fact
another really good benefit of basing on
the top of UDP is they can actually
iterate incredibly quickly so TCP is
actually built into all the
kernels of all the operating systems
that we have so it's advancement is
glacial with UDP that can quickly
iterate on this
so actually what you'll see over time is
the features that we've actually proved
off the right-hand side and UDP well
actually did start making their way into
the TCP stack so congratulations you
know though HDTV - so seriously continue
to learn with it I think it's going to
be a really powerful technology and the
fact is it's big it's really the biggest
change that's happened in eighteen years
at the web so I strongly encourage one
everyone to take a look at it and do
something awesome with it thank you
we have four minutes for questions so so
the question is if you optimized for
HDTV - would you actually be dis
improving the experience for HTTP one
clients if you start unbundling the the
resources so the short answer for that
is yes boss what you need to do for your
site is profilers see how many users are
using HTTP 1 how many are trying to
negotiate HTTP 2 and then you will
decide on a tipping point you know and
then one will actually make sense to
start migrating across do it at that
point in time I guess other features as
well that if you're gonna say ok well
where does the industry go as a tread
maybe you just start building in new
stuff from a htv-2 perspective but maybe
you're absolutely fine with having a
little bit of you know impact to you
HTTP what that would uses I guess the
other thing is well history - will most
affect users are a high latency network
so again also look at you know the
connection teams are those so if you use
like real user monitoring and another
sort of like traffic profiling technique
like that you can then actually say okay
well if we have users the go she 18
hates to be what that would or a high
latency connection just swap them over
right though they will get variable
benefit out of movie - HDTV - for those
scenarios
then the micro optimizations you get
with modeling so generally you will see
between about a 20 and a 30 percent
speed boost from going to htv-2 without
doing your thing and that's from the
studies that they've done are both
speedy and htv-2 since the cable
obviously if you have a size that is
heavily shaded you might and see that
level of benefit of you know initially
but you could start either like react
detecting your back-end to either have
multiple shares actually point to the
same host and actually their be able to
you know support the you know best of
both worlds for HT water h3 to cards but
yeah it's really a measurement learn it
take small steps from there so I guess
like the key thing from there is
instrument that you have to have
instrumentation for everything like
don't make decisions for migrating
across the HCV to be based I've got feel
like measure it and what you feel is
actually got a benefit your users you
know migrate across they will thank you
if you make things slow with the short
term you know I think we have time for
one more question really good question
so do we see that WebSockets will be
replaced by certain features like your
server push in fact the WebSocket
community had many of the similar
findings to what we had with HT to be
two as well so they initially try to put
WebSocket traffic over port 80 and found
that the middle boxes absolutely you
know caught them with the same issues
from there I actually see them as
complementary technologies I think for
bi-directional flows WebSockets still
make a huge amount of sense
I think server push again ease it's a
bit too early to say whether you know it
will reach like broad appeal like beyond
the sort of like client-server web page
sort of architecture but no I definitely
don't see them as you know dot a
WebSockets going away anytime soon
I think that's it for time but if anyone
would like to have a chat later you know
feel free to come up Cheers thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>