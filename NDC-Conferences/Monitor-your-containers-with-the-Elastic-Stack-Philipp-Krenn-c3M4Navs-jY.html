<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Monitor your containers with the Elastic Stack - Philipp Krenn | Coder Coacher - Coaching Coders</title><meta content="Monitor your containers with the Elastic Stack - Philipp Krenn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Monitor your containers with the Elastic Stack - Philipp Krenn</b></h2><h5 class="post__date">2017-04-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c3M4Navs-jY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and today we'll talk about monitoring
your containers using the elastic sex
since I work for elastic I'm actually
part of our infrastructure team so we do
everything around automation AWS and
stuff like that docker containers
testing and that Dennis I always call it
a UNIX pipe I kind of pipe the
infrastructure work into developer
advocacy so I'm out at lots of
conferences doing talks meet ups
sometimes visiting clients yeah so we
want kind of to to monitor all the
things both in general but for this talk
in particular about containers so just
to get an idea who is running more than
ten servers or containers already in a
few very careful okay and the next
question is then how do you log if your
answer is kind of well we SSH into the
boxes and then tell effort everything
and you have more than ten boxes that is
probably not the right answer
so just to get the idea who is already
using something centralized for logging
okay what are you using elastic LX Tech
anybody Splunk that that's very nice
here the rich people you're always
welcome at elastic you know I knows I
don't know spike that well I know I know
it has disadvantages disadvantages and
its price tag but we can discuss that
afterwards but let's not go into that
discussion right now so what are logs in
general for us logs are always like you
have a timestamp you might have
optionally some meta information and you
have some message and that message might
contain more structured or unstructured
data so you can just throw it somewhere
and store it and then do a full-text
search on that or you might want to
extract more meaningful information from
it for example if you have an web server
lock like Apache or nginx or something
you might have something like a return
code how long the request took what the
user was requesting it all of that stuff
you might want to extract that and often
you have just message
is where you want to keep the original
message so now that we want to store it
centrally water kind of our options and
like the two meaning to do that is well
elastic the company which started off at
as elastic search has elastic search
originally for full text search that was
kind of the history started in 2010 that
was the original logo our Photoshop
skills were not that great back then and
the tagline was you know for search and
let's still working great if you're
searching on any of these three sites
which I probably don't need to explain
behind the search box there is always
elastic search doing the actual
searching for you and then people wanted
to do more and those started office
community projects their authors
probably don't want to hear it that they
were bought in so we always say they
joined the family but we have these two
other products which then join the
family so Cabana is the visualization
you have lots of data and want to
visualize it somehow and see what is
going on and logstash as the name
already implies you can get logs but
it's absolutely not limited to logs we
have more than 200 connectors I think so
you can read data from lots of sources
parse it and enrich it enrichment would
be something like you have an IP address
and you want to get the geolocation for
that and then store it somewhere else
that somewhere else is often
elasticsearch but his hat doesn't have
to be so you could throw your stuff on
Amazon s3 if you generate flat files so
there's kind of the glue keeping new
stuff together and getting data from A
to B and that worked very well people
called it the Alex a classic social la
Cubana
most people are still referring to it as
the Ark stack because yeah you have the
animal and it's it's very easy to
remember everybody calls it the earth
stack no we don't it's actually very
fitting but no not right now that's the
first and take the focus back and yes
Vic is widely used for example CERN with
the thousands of computers and
they have centralized logging Salesforce
I think for the cloud offering they log
everything into our goldman sachs is a
different story
I think they mainly do auditing and
compliance because every now and then
they do somebody does something bad and
then they have to pay a big fine and at
some point they figured out it's
probably cheaper to to do proper
auditing and then paying 100 million
fine every few years we see how it turns
out so that was all nice and work well
the only complaint was since Lok search
started off as Ruby and is now JRuby you
would always need to install the JVM
just for logging and if you're a Java
shop that is fine if you're more on the
dotnet side or any other programming
language just adding the JVM or at least
JRE just flogging is kind of a pain and
people were very reluctant to do that
that's why we added this little fellow
it's called beats written in go so
you'll get native binaries so they can
collect information and just forward it
they don't do any parsing or enrichment
they're just agents forwarders shippers
whatever you want to call them so they
will just gather information and forward
it problem was that doesn't work well
with the arc because there is no B in
ELQ and we didn't really kill it that is
maybe too drastic but let's just say and
the elk is retired and living happily
ever after so we try it then with this
one this is the Belk or the elk B where
you can see we have all the letters in
there but at some point we kind of
decided well this doesn't really scale
and elastic is all about scaling because
what if we add another product later on
we need to make up another animal and we
need to replace all the marketing again
and it's kind of very cumbersome and
yeah doesn't work that well so um that
was only temporary one of our engineers
liked it so much he created stickers or
have been made before it was even
official so some people even have that
as a sticker though we never did that
officially and then marketing kind of
settled on the on the boring version
like classic stacked to strengthen the
brand and whenever somebody doesn't meet
up or any other public event and they
still call it elk we
Colonel erased the elk alert and
somebody will reach out to them and say
please don't call the elk anymore is the
new branding and yeah we would like this
much better but it's very hard to kill
the elk like you just cannot get rid of
it it's after some point and this is
kind of the stack so you have like with
agents beats forwarding data either
directly into elasticsearch or
forwarding it to log stash and locks -
dusty passing an enrichment
elasticsearch which does storing them
and you can cure you later out and
cabana for the visualization part and
those four are all open source and free
all Apache 2 license so you can just
take them and do whatever you want with
them so that is the open source stack
and we recently like last fall I think
end of October we had a major version
makeover since all the projects joined
the family at different points in time
they only all had their own version
number and it was not really aligned and
even for us it was kind of complicated
to keep track of well you have
elasticsearch 2.4 you will need Cubana
4.6 largest log stash is 2.4 and I think
beats was 1.3 then and if you ask back
what is the right combination for
elasticsearch 2.1 I would have no idea
and for most people it was like that and
when we cannot remember it our customers
cannot either so we changed the version
number everything is version 5 now and
that's why elasticsearch for example
jumped from 2 to 5 and obviously that is
much better so this is not just another
release this is just 3 times better now
since we skipped so many versions no
it's just the new version number you
didn't miss that much it's just version
5 everything and we're at five to one
but two right now and five to two will
come out soon we're not making as much
money as plunk but we do have a business
model as well this is kind of the
description of how we make money so
those with the dashed line around those
are all open source you can just use
them for free
we provide everything host it so if you
have a rather small cluster and you're
on AWS we provide that on AWS
which is actually a competing service to
what AWS is providing and we have
commercial extensions will go always
hand-in-hand with support so if you need
anything around security monitoring of
your cluster alerting if some specific
rule matches that would be commercial
but that is only a plug in so you can
just edit or you can remove it at any
point now what we'll talk about today
that is only the open source stack so
that's just for the general ecosystem
and yes that is how we make money and
pay my salary for example but we'll just
focus on the open source stuff today so
let's jump into beads since for the
monitoring part the beats are the most
relevant you just want to install this
lightweight agent and forward
information let's see how that works and
in particular how that works with docker
then or containers in general so the
first thing we have is file beat always
think of file beat like echo is call it
tail F over the network on steroids so
that is kind of the general idea it's
just like you're reading a file
everything new that comes into that file
you just take it and send it over the
network sounds very simple and pretty
much is you then can do the parsing so
if you have for example a log file and
you have the return code if you have an
HTTP request the response you have the
HTTP code in that message and you want
to pass the dot you can either still do
that with log stash or we have a new
node type called you just note that can
parse that as well
but let's leave the parsing aside for
today
nice attributes off the file beats are
they are at least once so you have your
file and it will just send it over and
it wake sure it is acknowledged before
it's actually assuming okay this has
been sent so you will not lose any log
messages
it supports back pressure back pressure
means the receiving system cannot keep
up at some point anymore and will tell
the sender will back off a bit not so
fast I cannot keep up slow down and once
it can keep up again it will tell it and
then it can send it fools
again and it's board's graceful downtime
which is pretty simple since you're
logging mostly to a file if you just
read contents from a file you have a
pointer to a specific location in the
file and you know everything up until
here I have already sent and everything
afterwards I still need to send so even
if the other side is not reachable you
just keep that marker and you don't need
to keep anything in memory or anything
you just have this marker and know this
file I have read until this point and as
soon as the other side is available
again I can then send it so what this
will look like you have receivers here
we're just sending to logstash as you
can see our photoshop skills have
improved slightly logos are a bit nicer
and we have on the left hand side we
have lots of beats these would be our
containers or instances and they just
Center information and they can send to
any of the receiving instances they will
just pick one if that one is not
available they will go to another one so
this can be set up pretty highly
available and how they make sure that
you're not losing messages you have this
connection between beats and laxus at
the bottom and you send a batch of
messages so we have previously we have
sent the acknowledged part and now we
have read something and we're sending
that this batch and only when that has
been acknowledged we will actually mark
it as act and gray in the stream of that
one file and only then will it be
assumed to have been sent successfully
and if we it is not acknowledged from
the other side we will try to resend it
afterwards and that registry file that
keeps track off until where have I sent
something in that file
that's nice thing about files since you
don't have anything in memory you don't
need to discard something if your buffer
or buffers are full and you can just
keep that marker and know from where to
keep going
other nice attributes is it supports
multi-line messages multi-line messages
are often a pain because it's hard to
parse them but if you have stack traces
or something like that that's normally
multi-line you can do filtering on the
client already for example you can
define a pattern and say only what
starts with I don't know error
or Warren I want to send over the
network I want to log more on the
instance but I don't want to store it
all in my central instance because it's
just too much data and I don't want to
burden my network so you can't just
already filter out on the client-side
and if you already have Jason you don't
need to do any parsing anymore and you
can just send the JSON and insert it
into elasticsearch directly since
elasticsearch stores Jason you're good
there so what does it mean for docker or
containers we have 101 options and we'll
go through all of them quickly so don't
laugh you'll see so if you have the
locking overview those are your options
and some of them are applicable with our
stack and some you cannot use so some
providers have their own login
connectors we don't but we can hook into
other processes so first one is Jason
file which is also the default basically
you're writing out the JSON files and
file feed runs and just collects the
JSON files which is very nice because
it's very simple you're sticking to the
default you still support docker logs
you keep all the metadata from your
doctor containers in that major dance
that is by default you can change that
your JSON file will be unbounded so it
will just keep growing and growing you
will need to configure the right
rotation and it might be slow to log in
Jason it might slow down your containers
depending on if you have that issue or
not the alternative would be Iran just
okay server so you can run a local
syslog server and then have file bit
collect from syslog which is nice
because it's super configurable but well
you need to run your own cisco server
and yeah multi lines you need to parse
them again so it's not that nice
possibly it it is powerful you have all
the features you you want but it's not
that nice to actually run your own
system server maybe
generally yeah there's pretty much
nowhere around the general D it's or at
least in the Linux world file bit can
not yet collected but hopefully soon so
you will just write it into the journal
D process you don't have a log file
anymore it is nice because it's widely
available you can use docker locks it
has the right metadata major dancer days
file bit does not yet support it it will
be pretty certain that we will add that
feature in the future but we don't have
it yet
so if you want to use journal D at the
moment there is a community bit called
journal bit that can collect the
information right from journal D nothing
is you don't have to write everything
out to a specific file and need to
reread it from there again then there is
Gelf which is supported by default and
works and it's pretty fast dance it to
being pretty fast as its GDP base so
there is nothing like back pressure or
acknowledgments if your messages are
lost or your receiving site is
overloaded your messages will just get
lost which might be fine if you just
have lots of logs and you're more
interested in like yeah I get most of
the stuff but if my system is overloaded
I don't care about all the details
anyway it's fine if you want to have all
the logs it's probably not what you want
and final option is in your application
you can just lock the specific file or
volume you have marked out which is
simple to install if especially if your
app supports rotation otherwise you will
need to rotate manually somehow it is
very scalable but again metadata might
be an issue so you just mount some some
folder in and have file with run outside
and collected those log files and
forward them so as you've seen we had
101 options luckily it was only binary
so what we would suggest today is either
use Jason the default use the Cystic
server if you want to run your syslog
server or mount the volume in
future once we support general D that is
probably the preferred option or you
want to run the community beat at the
moment next up our metrics metrics are
just whatever metrics on the system or
application side you have like I don't
know how many requests you have what
your average response time is all these
things that is what metric beat is so
metric beat has a system module that can
collect lots of system information like
what is the CPU doing yeah memory all
these things and we support some
services like this is the current list
it will of course grow over time so if
you have one of the web servers MongoDB
Postgres my sequel Cassandra Reddy's we
can just ping these services and extract
statistics from that and just see how
that application is doing how does it
work with containers yes you can just
read the data from C group in Prague
which is part of the system modules so
getting the CPU and memory information
from the container that is or
historically that was always reading
proc which has a few very nice
attributes so if you have access to the
doctor API you have access to the doctor
API so you can do whatever you want and
just for logging you probably do not
want to have that so once you have
access to that API you can just stop
your containers or start new containers
and maybe your logging system should not
be able to do that maybe depends on your
situation
we are more of the opinion maybe you
should not do that so for security
reasons it's maybe a good idea just to
rely on proc and not require actually
access to the doctor API to access laws
with proc all the different container
technologies not just dr. I support it
so if you want to use one of the more
exotic ones for whatever reason you can
totally do that with proc it
automatically enriches all processes
with the information from the C group
the downside is since it's not doctors
pretty specific there are no
double-talker specific informations like
the meta information docker has
it's just not there in Prague and then
what people mostly say but I only have
doctor and I only care about doctor and
everybody just won't see you stalker and
somebody in the community and then said
well this is actually something we can
build and they implemented something
based on the doctor API and they called
it dr. Pete and I think we it was it
reached it reached version 1.0 and they
were nice enough to write a blog post on
our blog and we released it and I think
one or two days later sent us or
their lawyer sent us a very angry angry
letter and said anything that starts
with docker in the name is trademarked
to them and even though it's just a
community project and open source and
we're not making any money on it that is
their trademark and we cannot use that
so then we didn't have dr. Pete anymore
was a bit of brainstorming where do we
go from here
so then we renamed it so now it's called
doc Pete
at first we even thought about just
dropping de and just calling it doctor
repeat but now we just went with knock
Pete so doc Pete is now available we
just renamed it and it's yeah that's the
same thing sorry for the confusing name
but trademark disputes and we have also
included that into met repeat 5 to 1
which is current version that has the
docker module included as well I can
show you a demo with that then later on
as so you don't need to use the
community module anymore that is
actually contained in a metric bit what
you already it will of course require
specific permissions so to run that you
will need to set some read-only
permissions on pro surfers cgroups FSC
groups host FS so you will need these
permissions to run the system module
yeah you you can take pictures but AIDS
syndicate documentation and PL publish
my slides afterwards as well but feel
free to take pictures for monitoring a
service
you will actually need to link the
service so your docker container that
wants to monitor it can access that
resource so for example for my sequel or
a Pesci or whatever it will just do a
request to the service and then store
the response of that so you will need to
link those services to get actually able
to access that the way you should run
that is normally as a sidecar so you
just have your own metric beat process
and that runs next to your regular
containers and then that can just
collect all the information you want to
have yeah you can stuff it into your
other containers but that's probably not
the idea of containers and then one of
the questions that often comes up is
like is it even fast like elasticsearch
in the beginning you said this was made
for full-text search and now we're
storing metrics and this is kind of a
very different problem and yeah some of
our competitors always want to do
benchmarks but our opinion is pretty
much disowned benchmarks even under
similar conditions the results are
probably not that comparable so for
example here we are benchmarking the
housecat and the squid and the similar
conditions the squid is much more
intelligent because the house cat is
dead and that is pretty much what all
the benchmarks do anybody publishes
that's why we don't publish any
benchmarks so in the recent version we
have kind of worked a lot to make that
scenario more powerful or more
performant so previously we only had
floats for example now we support half
floats as well so you only get half the
precision but you only need half the
disk space as well and then we have
scaled floats as well as scale floaties
you just store an integer number but you
know it needs to be divided by I don't
know 4000 or 100 and to get the actual
float values so you're limited in the
precision somewhat but again you need
you will need less disk space and it
will be more efficient so that is why
even though it's full-text search we
have improved both the columnar reading
of values and the storage side so it can
handle metrics quite well as well
and then the next bit is packet beef
packet beat is like network packets who
is using wireshark phew okay Wireshark
is really nice as long as you have one
or two servers but if you have more it
gets quite a pain because then you need
to capture on all the different systems
your network packets and then you will
need to collect them and probably
combine them to actually get an overview
of what is going on in your system the
idea of packet beat is it's using the
same base library in lippy cap so it
will capture your network packets and
then extract the meaningful header
information meaningful header
information would be it knows you have a
an HTTP request and an HTTP response it
knows the whole thing took 100
milliseconds the response code was a 200
and the URL you actually hit was fubar
that's actually all available in the
header and that's what mostly will
interest you so you can see what is my
average response time is my response
time getting worse and to suddenly have
more 504 for hundreds than before do I
have more requests in general so this is
all the information that packet beat
will extract for you we support some
protocols HTTP
other databases DNS ICMP so for them the
wire protocol is actually supported and
it will be able to extract from just a
request in the response to my secret for
example it will know the response or how
long the food thing took on the wire was
15 milliseconds for example and the
query you sent was select star from
whatever so it can extract all of that
information without actually needing to
store the payload and that way you don't
need to instrument your database anymore
you can just do it all on the wire and
if the support for the protocol is not
yet implemented we have something called
flows so it actually knows that it's
just a TCP connection but it still knows
a is communicating with B and how many
packets and how much traffic or if you
have encrypted traffic obviously we
cannot break the encryption and look
into the encrypted traffic without some
more complicated setups
but it will still know how who is
communicating with who and how much
traffic is that and again this should be
run on the side I'm not in the main
container so Tucker what containers do
we actually have so we have some
containers luckily we started off naming
them in the right way so we always had
our product name - docker and so we
didn't have to rename afterwards so we
have elasticsearch cabana and locks - we
have the beets will come soon they are
not yet available but kind of to run it
as a sidecar that will be available no
firm date yet but we're working on that
or we're about to start working on that
now and kind of the minimal example to
run elasticsearch plus cabana to
visualize it would be that but it's the
latest version you just run cabana and
elasticsearch and linked it - together
you mount your volume with the data and
that's all you need to actually run the
minimal elasticsearch Cubana example one
thing we had in the very beginning but
we have removed is latest because this
will just cause trouble later on it is
nice for testing and TV and development
that you just do latest and get the
current version and you don't need to
care about anything but what people will
potentially do is they will start some
cluster for production with three nodes
and after one year they want to add two
more nodes and if they have just
configured that with latest and they
will screw up their cluster entirely
that's just to be safe that is we we had
it for like I don't know the first two
or three releases in the five branch but
since then latest has been removed so
you cannot even access that if you have
still cached it you might get weird
results and all the versions
I assume very few people do but yeah oh
and one more thing which you might have
seen or might not have seen our
containers are not on the docker hub we
have our own docker edges
so if you go to docker hub and get
cabana and elasticsearch those are
docker hub official packets they call it
themselves docker hub specific and we
often get complaints about those but
those are actually not ours
so our containers are in our own
registry I think our main motivation was
to actually have better download
statistics and like better overview of
who is using what so you will get the
wrong one if you just do on docker hub
get me latest elasticsearch version yeah
and we can actually see how that is
going so we have that is keep on I said
large enough no it's that large enough
yeah it's probably better so let's
refresh that so this is generally Cubana
the version 5 so if you have seen an
older version it was more white or if
you have a senior very old version is
what more black this is the colorful
version like this is 5 is more colorful
so the main things you will be
interested in we have discover discover
is just what day to do I have and what
can I do with that
visualize is then you can click together
some visualizations on some data you
have and dashboard you can put those
visualizations together to explore your
data so for example here I've just
ingested quite some log files and let's
see what the type is so I have I'm
monitoring here Cubana and my one docker
container so the general setup of this
is all a virtual machine that is running
the setup is I have everything running
in that virtual machine plus a Redis
container and I'm also monitoring that
Redis container so everything that is
Tucker here that will be my Redis
container with its information so I can
actually filter it down and say I only
want that one so if I click that plus
sign
it will filter down okay these were over
the last 15 minutes kind of course
change time frame over the last 15
minutes these where all the locks that
my doctor container created so I have 30
year went events over the last 15
minutes and if you look at one you can
see okay this was my type a stalker and
okay that is some readies lock parent
agreed to stop sending dibs finalizing a
Oh F so this is one of the log files we
have extracted or you could just see
just the raw JSON format this is what
the raw JSON message would look like so
this is since we I'm using for the login
here I'm using the default Jason locks
and I've just collected those Jason
logged with file beat so how to actually
do that
oops no not you yes this is the right
one just to show you how how that is
configured which is pretty easy it is in
E to see if I'll be five it is Ubuntu
but it doesn't really matter so what is
configured here is I've just said okay I
have three log files I'm interested in
here so var log syslog for the host
system Barlow Cabana Cabana log for I'm
just monitoring the Cabana instance and
then I have one docker container
actually so I have one Redis container
this is this one container that is
running and and I'm running that as well
and what I'm doing then is I'm
collecting all these log files and I'm
forwarding them to log stash and lock
search can do some processing in my
example log stash will only pass sous
log into nice and messages it doesn't
need to parse the docker or the Cabana
files because those are already JSON
files so those are already in a
structured format so we can just keep
those originally just to give you a
quick overview of
how Laxus just look done so four locks -
which does the parsing it always
consists of three parts first you have
the input which is kind of where do I
get my data from then you have so-called
filter which will pass your data and
then you have an output where you want
to forward your information so the input
is very simple we're just expecting five
beat we're just expecting file bit to
forward something running on a default
port which is fifty forty four then for
the parsing part we just say oops
yeah we're collecting from Redis as well
that is not what I wanted to show you
here this year for example is for syslog
and syslog has a specific format and you
want to parse that suze log message
apart so if the type what i've received
is syslog iran rock rock is basically a
regular expression and I just have this
select timestamp in uppercase letters
this is a predefined pattern you can
define your own patterns or just a
regular expression and that will extract
that one field and store it in a message
sushi lock timestamp and that's the same
with such low cost data and extract all
of those informations and once it has
parsed that it will actually forward
that to elasticsearch which is again
very easy here I've enabled security as
well so username and password those are
the defaults so it's just storing that
information into elasticsearch what that
would look like so I can say no I don't
want to have this docker filter anymore
go away so I have my hundred fourteen
messages again in the type oh I didn't
have any sister log messages in the last
fifteen minutes so let's say we want
over the last four hours I hope we had
some smoke messages over the last
yes we did and now I could just say okay
I'm only interested in syslog filtering
down to that and if we have done
everything correctly it should extract
some meaningful informations so you can
see yes the syslog those we have parsed
out with our log stash rules so we know
for example that was the process ID or
the program name and the syslog severity
so these we have extracted from that
message and we could for example now
also search over all these messages and
just see did I ever reboot my machine
I'm not even sure let's see no obviously
in the last four hours
I didn't reboot my machine but I think I
fixed my time yes so yes I've restarted
the NTP paint daemon since my virtual
machine time always goes out of sync if
I put it to sleep so yes so you can just
do regular full-text search over all the
data here and then since we're lazy
we'll just look at the few predefined
dashboards how can I visualize what is
actually going on on that machine so
since I only monitor my one local
machine this is just one but you could
of course have more for example let's
see what does our docker dashboard give
us so little surprise there is just one
docker container running here but if you
had more this would show you all the
doctor containers you have in your
system and you could actually filter
down on just the one or the ones you're
interested in but if what you would get
all of them so you one of them is
running 0 up or 0 or stopped you can see
ok how are the doctor images repped how
many containers are running and you can
see CPU usage memory usage and what is
the network doing and that is actually
extracted from the doctor API now and
just sees what is one or more of your
containers doing you can also see the
same on the system level for the entire
machine
but this is just one virtual machine I'm
monitoring yeah you can see how many
processes 101 process is running in
total let's switch it back to 15 minutes
since I didn't have it running over the
last four hours and here you can see
wild guess which process is using most
CPU and memory a little surprised it's
Java especially for memory like you can
see there is it resized it again so you
can actually see it properly so yeah
Java is what used most memory then
ismetric beat then is know to pay
attention what process is note Cabana
yes Cabana by now is a note and angular
one mainly application and then that
other beats and docker are using some
memory as well and we can see ready
server which is actually inside my
container that is also listed here in
the processes needing memory but it's
yeah pretty tiny and depending on what
you're interested in you could just
focus on I don't know the processes the
network the CPU load to CPU load which
is yeah lots of numbers and some curves
and you can see okay my one instance
what is the CPU usage over time and you
have all the different attributes you
can jump into here and then we're
actually monitoring the network as well
so let's go to the dashboard at first
this is showing you all the network
requests you have on that machine you're
monitoring we don't have any client
locations since this is running on
localhost and localhost does not
translate well to geo locations so that
first one is empty but you can see here
those are all the web transactions we
have guesses what is a web transaction
on my virtual machine exactly that is
what Cubana so i'm
all the clicks I'm doing in cabana here
will actually be reflected in the web
requests so it's very self driving that
virtual machine then we have database
transactions transactions as database
I have run in MongoDB here and those
transactions are just it's just the
metric beat MongoDB module that is
pinging the database and then we have
cache transient transactions that is
Redis and Redis is pinged as well and I
have set it up that ticket Pete is
collecting the network packets then
throws them into Redis as a queue
logstash collects the data out of the
queue and stores it into elasticsearch
if you have a distributed system and you
have lots of components in it and you
might have spikes it is a very common to
have a queue to actually throw your
messages into a queue first and then
have it consumed out of the queue
smaller setups normally use Redis the
big ones often go for Kafka but for the
yeah to keep it simple I've just used
Redis so Redis is receiving all the
network requests and that is what we are
monitoring here as well and then you can
see response times
most of my response times were very low
like in the zero armed or up to ten
millisecond range and sometimes you have
higher response times we can also do
percentiles errors luckily they've
hardly any errors yeah and you have
latency still grams as well and then you
can actually jump into specific
protocols for example for web
transactions it can tell you okay these
were the numbers of requests over time
in general how many did you have
what is the the ratio of four hundred
200 500 whatever per URL and you can see
these were my top URLs for example slash
server status that is the URL that will
be pinked by the nginx module and this
doesn't exist and this is why we get all
these 404 errors all for example we
could see
what MongoDB is doing though I don't
have any sensible queries running here
you can see okay it's running some
queries the throughput is very
consistent since it's just a ping er
pinging in I don't know every half
minute or something like that
pinging that the moon will be with that
command but if you have more interesting
queries they would show up here and you
could actually see which are my queries
with what is my throughput and you have
the same for my sequel Postgres there's
also a Redis visualization that is not
linked here and you can see okay we have
very consistently three client
connections we have one server running
version 3 to 6 and yeah it's one
standalone server and the most common
request it's getting is the poll so
whatever service you have you can pretty
easily keep the overview of what is
doing here ok any questions so far
yes
so the question was the dashboards are
they did I put them together or not
no admittedly I was lazy but this is
kind of being lazy is a feature unity so
these dashboards can actually package
with the beats so if you download the
beats it will put in two so if you
install the dbn packages
let me cheat I don't know by heart but
it's all automated yes so if you install
the Debian packages this is where the
dashboards will be put and you can
simply run that and it will import your
dashboards into Cabana and you can just
use them so I didn't do any
customizations on that part here I just
used what we had pre-built but of course
you can customize these visualizations
so if you're interested in you only want
to build your own you could for example
say this one there is a little pen here
which might be a bit small to see but
you can just click on that pen and then
you guys will jump to the visualized
part and here you can actually see what
did you put together to generate that
graphic and you can see or it probably
it's too too small and you cannot see
but I took the maximum of this specific
field on the y-axis and then the maximum
of the blocked so it would show both the
connected and the block we so the white
space there's always a block because
it's zero and so we have both of them
and the x-axis is actually a date
histogram so it's just over whatever
time frame I selected will show that and
you can totally customize that or you
can create your own visualizations like
that so that is how that plays together
okay
oh yeah that is this is something I'm
now throwing in him to all my
presentations we used or like a week ago
I think we started to have some issues
with elasticsearch service which were
not that well protected and yeah like
for people started encrypting your hard
drive and then trying to get some
Bitcoin from you and now first to switch
to MongoDB unprotected Mangala building
instances and now they switch to
elasticsearch and what they do is they
take your data if they are nice and just
seal it and delete it and leave you a
message like you need to pay up to get
your data back if they are not nice they
will just delete your data leave the
message and will only tell you after you
have paid that they didn't take it back
up or they will just not respond and I
hope it's still working but I thought it
would be fun to actually see one of
these instances so the easiest way to do
that is that the website is called Sudan
IO which in frequently scans the
internet and you can just see what is
running where so if we search for
elasticsearch here it is just giving you
a fair number of hits so you can see we
have for example I'll make it a little
bigger yeah
most of these so what it does is it
scans the internet on mainly the default
ports but also other ports and will just
collect where specific keywords or port
or something are reported back and it
will just scan the internet for example
here it just all the responses that
contain the term elasticsearch it put
into that list and you can see more than
a thousand it found in the US etc so if
it's a 401 the people already kind of
did the right thing so they have a proxy
to protected instances but for example
yeah luckily here we have one instance
on that IP address that didn't protect
themselves and we can just take a look
what if anything happened to that
so what you want to do is you can just
do curve and we can just we can do that
oh sorry my bad
it's elasticsearch runs on port 9200
all version 2.4 dot 0 is I don't know
two years old or something probably
somebody started some test instance and
then forgot about it after a time but
that is potentially a good target so
what we can then do is we can say give
me all the indices you have which is
basically the tables in a relational
database so cat in this s and we have
something yeah that that doesn't look
good if that if you have this please
read in your data store that is not a
nice design so I want it so what we can
do is we can then say please read just I
want to see what's what's in there the
query is I want to search for everything
and I wanted to be easily readable oops
and I somehow miss type oh no actually
why did you pretty equals true
it thinks I want to pause it I need to I
need to quote yes that is nicer to read
and you can see somebody yeah by now in
the beginning I think they asked for 0.1
or 0.2 bitcoins they've now increased
the prices now we're at 0.5 i think
bitcoin is it's um something like 700 to
800 pounds today but it's very volatile
so it will change pretty much every day
so yeah that is what they're asking for
today i assume it's just a test database
nobody cares for it anyway so they will
not get any money
but i've seen that people trace the
specific addresses they found the
bitcoin addresses and saw that at least
a few dozen people actually paid up to
receive their data back so now that
we've seen the bad part how do you
actually avoid getting the bad part yeah
we've seen that we have done that we
even got by accident the same IP back so
what you want to do is or by default
elasticsearch will only bind to
localhost so you will need to explicitly
change that so somebody has changed that
for whatever reason good or bad so the
first thing you should do is if it's
possible just bind to the private
network address if you have a private
network or just put it into your virtual
network in on Amazon it would be the VP
see put it into a private subnet so
nobody can ever access it the second
thing is if it must be reachable from
the outside put at least a firewall in
front of it though I had people who said
like oh they had a public IP and they
had a firewall rule to block it but then
they screwed up the firewall rules for a
few hours and within that few hours
somebody already stole their data
so yeah firewalls can work but make sure
they are actually doing their job
otherwise they're not helping you much
and something I'm I'm not sure how well
that works just use a random port not
9200 which is a bit more security by
obscurity I'm not sure how much it will
help it might gain you some time because
the attackers will just go for the
default ports first though it won't be a
protection long term
don't make your data stores accessible
by the outside and then the next thing
is if something needs to access your
database proxy it since
elasticsearch is just based on based on
HTTP proxy it is very easy like you can
use nginx and it can do SSL Plus HD
basic auth so you can protect your
database with that very easily or we
have something our commercial extensions
we have the security suit those are
included there as well
of course our state's guys will always
tell you the expects or better they have
the main advantage they're integrated
into elasticsearch the main problem with
the proxy sees if you have something
that work or goes around the proxy so if
your proxies all correctly set up it
doesn't protect you if you have some
other way to access elasticsearch if you
don't have the proxy but otherwise if
your proxy is set up correctly it will
protect you so concluding to container
solve all our problems especially for
stateful services I'm and my favorite
answer is no they won't
but yeah so I always compare them the
elastic sector actually monitor that to
Legos so you have all these building
blocks it normally will require some
assembly so you will need to install the
services you will need to configure them
in locks - if you need to parse
something apart you will need to provide
those passing rules there are a lot of
public rules out there but you will need
to do parsing yourself so it's not
something a box you buy and it does
everything for you but it is a building
block and you can do pretty much
anything you want both for logs
full-text search business data we
internally use it for pretty much
anything so our newsletters our
buildings everything basically goes into
elastic search and we have dashboards
for whatever we want to know internally
yeah the dashed line that is all the
open source stuff we provide hosting as
well and the commercial extensions
especially if you're a serious company
and need security or stuff like that of
compliance reasons that is what you will
want
we always called this kind of the story
how people involved in our system so
normally what people do is they start
off with the open-source version and
then they introduce it at their company
and since its reads just they started
and at some point it becomes more and
more important so they need to continue
their journeys so what we then offer is
we do trainings for developers and
operations we have support for during
development we have consulting services
if you want to go live with something
and want to have somebody check up your
architecture and we have production
support of course as well and that's
pretty much it
I've got lots of stickers over there if
you want to have stickers
other than that questions who is using
the elastic stack for logging already
just sort of interest okay not yet
everybody so I still have some work to
do but at least somebody it's a good
start and I have a question how much of
what you said applies to Windows
containers there's any of it work on
Windows sorry come again if you're
running a Windows container with
something like a server core or nano
server as a base can you do the same
kind of thing is there a beats version
for that or is it just so you mean to
run it inside that or yes you're getting
the same kind of instrumentation of a
Windows container this place to run
running on Linux yes since it's go yes
we have the binaries and everything that
can can run go can run file beat or
metric beat or whatever you want to have
also packet beat a lippy cap is
supported on Windows as well so witness
is not an issue what is an issue is AIX
because they don't have go support
probably I don't know if that applies to
anybody but as soon as you can compile
go we provide binaries to the most
common platforms so I don't know from
small systems to large systems
everything is supported but it needs to
be able to compile
otherwise you're stuck there there are
some alternative implementations in c2
forward locks for example that is what
you would often use on AIX but normally
it's just yeah go and you're good to go
any other questions how did the solar
folks like it odda
it's not sort of this blank folks like
it looks a lot like Splunk it looks a
lot less blank
yeah I'm not a fan it wasn't my decision
to buy Splunk yeah that is unfortunate
yeah I've never used blank I must admit
so I don't have a strong opinion yeah
thank you you will get your extra
stickers afterwards yes and you focus on
the logging story but obviously
elasticsearch is a full-on document
database so are you finding customers
use it for what used to be called out
and use it for logging or you're talking
just part of what customers actually use
it for so I'm always careful with
database we were very careful to say
like elasticsearch is not a database
it's we'd call it data store whatever
your distinction then is yes so that the
log in use case is not even a majority I
think 30 40 % of customers are actually
using it for logging full text search is
very much evolving and going strong and
we have overtaken solar in use and
downloads and pretty much everything I
think even though that that is also very
complicated comparison and we don't like
to compare it always apples to oranges
but no we're not focused on the log use
case so we do full-text search both
historically but this is fully supported
and improving all the time
we are doing locks we're now doing the
metrics what is coming more and more is
like going a bit deeper we've recently
bought
like half a year ago and machine
learning company which was basing on him
so you will have some metrics and the
machine learning will then learn what is
normal and will then alert you if
something is not normal so they have
like an upper and lower bound and we'll
just shout like hey you're not within
that range where you should be
because right now you you can either
have people stare at graphs the entire
day to learn stuff which is very
expensive and not the most fun job and
the alternative is you can just set some
hard rules like if you have more than X
requests or fewer than something or if
your ratio is different but that is
often not flexible enough so machine
learning is one of the big things that
those are coming like we are broadening
the scope but we are not kind of
forgetting our history so full-text
search some people use it as their main
data store if there is a good or bad
idea depends on the use case and we're
always saying be careful yeah but no
we're not forgetting like the history
and logging is not taking over this was
just one example on how to monitor your
containers because it is yeah
one fancy thing many people struggle
with or containers are great because
logging and metrics are getting more
important like the more distributed
stuff you have the more important a
centralized money logging and metrics
get so we're of course trying to push
into that direction as well yeah I think
we're pretty much out of time if you
have any questions find me I'll be here
for the rest of the day and don't forget
to grab your swag thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>