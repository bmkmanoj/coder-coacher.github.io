<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Data Intensive Microservices with Apache Kafka - ​Yaniv Rodenski | Coder Coacher - Coaching Coders</title><meta content="Building Data Intensive Microservices with Apache Kafka - ​Yaniv Rodenski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Building Data Intensive Microservices with Apache Kafka - ​Yaniv Rodenski</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KIiq8lIgUVk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Jennifer Lansky I work as a
solution architect for Couchbase here in
Australia I also have a an open source
project that just made the Apache
Incubator and most of my most of my life
for the last seven years were around
dealing with data mostly big data and
and working on analytics projects but
that's not what this talk is all about
so we're gonna talk about something
slightly different we're gonna talk
about micro services and when their data
is getting to be a little bit too big
and how we can solve that so famous last
words let's build a micro service if if
you've been a part of this industry for
the last few years and you have a pulse
at a certain point in time you had to
say that to your boss let's build a
micro service that's the way to go
we'll build a micro service it will be
you know well-defined it will be
autonomous and it will be scalable so we
have our micro service and our micro
service even persist data to a datastore
and it's scalable you know has no
dependencies autonomous well-defined
and even our datastore is hopefully
scalable if your datastore is not
scalable
talk to me after this I'll hook you up
but we can scale out right we can have a
load balancer Microsoft service is very
scalable and we're happy and we
celebrate right that's how it begins
we're all very helpful about our micro
service
it's a greenfield project or at least a
Greenfield part of an existing project
and everything is good until that moment
of course where we have new requirements
like another micro service that needs
the data that we just processed in our
micro service or we need to put our data
in another datastore right so we end up
with
like this and then we're not really
happy and we don't celebrate anymore we
need to go back and start working again
now a lot of systems get to this state
quite rapidly right we have multiple
services that we need to hand off the
data to we have additional data stores
like a search engine or our analytic
store it HDFS or s3 or as your data like
thingy and we get to the point where
there's a lot of load on our micro
services and that that is not an inbound
load that's an outbound load where we
have to serve the data that we just
processed in this micro service to
multiple consumers and there are various
ways that we can you know deal with this
right so we have our micro service right
now it is no longer autonomous it has
dependencies and we're not scalable
anymore so different ways that we can
deal with this one of those is using our
database as the integration point that
seems to be a pattern a lot of micro
services take these days you have our
secondary service service be acquiring
the database for the same database that
our micro service just processed and put
there and we can have another micro
service that just kind of ETLs or even
just scoops the data into our search
engine and additional stores and there
are a lot of situation like that where
you see those kind of loads hitting the
database and that is not necessarily the
right use for your database don't get me
wrong I earn my living by selling
database licenses but still for some
things it's just too much for the
database are they
bases usually have an additional
overhead for querying for for indexing
for those kind of things and it's
massive misuse of our database to use it
just as an integration point right and
another kind of aspect of that is that
databases allow us to do things like
change your data right we keep update
them and in that process you can miss
some of the updates our additional micro
services might not get the correct
version or the initial version of the
data they might me some of the change
changes along the way that's not a
reliable way to store your data so
another way to think about this whole
problem is to think about it like this
what if we could have a data stream and
have different consumers running on top
of that so this micro service can read
the data as it streams through the
system other micro-services can get the
same data from the same data stream and
we can even get the data either directly
to some of those stores because some of
them need the raw data if you're doing
analytics if you're doing you know
clickstream analysis on on Hadoop you
don't want the data to be processed by a
macro service before you're doing your
analytics work so you can do that but
you can also have your microservices
processing the data storing it in the
database for what a database should be
used right our data as it should be
stored and reflects the point in time
that we're looking at and if you need to
integrate one data store with another
you can still go through the same stream
so obviously this is not something that
just our little micro Service had to
deal with it happened for to a lot of
companies and and in different scales
and at a certain point in time it
interlinking so LinkedIn around 2010 hit
a very similar problem and I'm gonna
show you a couple of diagrams coming
from LinkedIn and the problem that they
dealt with and the solution that they
had was Apache kofta an Apache kafka is
a transaction log system I'm going to
continue and repeat the words
transaction log and that's because
transaction log usually is something
that we use in another part of the
system which one is that
someone said the relational database
databases I work for a none relational
database we also have a transaction log
transaction log is the way that
databases usually store their data okay
so we're talking about a pub/sub system
that is based on the same concept the
same low level concept as databases do
and it's very important in Kafka because
capcom has some of those attributes the
databases have but it is designed to be
a pop subsystem a messaging platform a
streaming platform if you may its
distributed it works in very large scale
and as I said it was developed by
LinkedIn it is now an Apache open source
project and a very successful one it's a
top-level project a lot of contributors
a lot of organizations are using it at
scale and you also have confluent which
is a commercial company supporting it so
this is LinkedIn around 2010 and
LinkedIn found himself in a very similar
situation to our micro services in a
larger scale right they had all these
systems some of them are what we now
call micro services for mainly marketing
reasons but some of them are even data
stores and what they found is that they
have all those integration points
between the different parts of LinkedIn
it's very hard to go
it's very hard to understand where data
comes from and where it goes and there
are some additional attributes that are
missing okay so they replace this mess
with this streaming data from different
systems into Kafka and having other
systems like their their social graph
Hadoop and so on picking the data from
Kafka this is the scale that Kafka was
designed to work and this is what
LinkedIn did and they found along the
way some additional additional benefits
as I said Kafka is a transaction log
based system and it's a persistent store
so your data is stored there and
persisted for a certain period of time
that you can configure one of the I will
treat it as an urban legend because I
heard it a couple of times but I
couldn't find anywhere in writing from
LinkedIn that this actually happened but
there is an urban legend that LinkedIn
had a crush which brought LinkedIn
completely down and what they did they
brought their server our servers up and
what they could do using Kafka was to
replay LinkedIn up to the point where it
crashed and actually debug that on
addict on a dev environment so there are
a lot of benefits that along the way
LinkedIn found in Kafka and once we go
over how Kafka works and some of the
attributes it will be very clear how
that can help different systems so this
is basically Kafka metal at a very high
level we have producers and producers
talk to a Kafka cluster made out of
brokers and on the other side we have
consumers and our producers write the
data into topics topics are basically
our streams of data inside our Kafka
clusters and they are made out of
partitioned
transaction logs again I'm gonna repeat
that it's a transaction log just like
our database and those partitions can be
distributed across our cluster so we can
scale out and they can also and should
be replicated
so our Kafka cluster will be fault
tolerant and our producers are quite
simple this is an example of the Java
API I'm gonna use the Java API in this
talk mainly because some of the more
interesting bits are not available in
other languages at least not yet so
producers write directly into those
partitions in the topic right we have
our producer Kafka producer when we
initialize it we give it some properties
like a list of servers to connect to
timeouts and so on and we can create a
producer record which is basically a key
value record so Kafka holds key values
and we just use the producer send now as
I said the producer writes directly to
the partition and this is being done
based on the key by default what the
producer does it uses the default
partitioner that just hashes the key and
get a partition which is within the
range of the number of partitions we
have but because there is some
implications to how we partition our
data for example data is being is
ordered only within a partition ok this
key should represent some sort of
business logic but sometimes we want to
control the way it is partitioned we
have an interface called partitioner
that we can implement and we can decide
on our own strategy how to partition our
data other than that the producer is
very lightweight and very
straightforward
so let's go back to our overall
architecture and let's talk about our
consumers our consumers are slightly
more complex and a little bit more more
powerful when we talk about consumers in
Kafka we can actually have single
consumers but we can also have what we
call consumer groups and our consumers
do a lot of the heavy lifting this is
one of the things that makes Kafka very
scalable Kafka for example doesn't care
about where the consumers are currently
at with the reading when we write into
Kafka into our topics okay
our messages are being stores s bytes on
disks with what we call an offset it's
just like an index in an array and while
reading our consumers know they know
them or sorry manage their current
offset so Kafka doesn't need to keep
trace of who's reading what Kafka just
stores the data it has an offset seen
think of it as a very large-scale array
that you can access by a position by an
index right so our consumers can go to a
specific offset and read a batch of
messages transfer that or sorry get that
it's being transferred to them over the
network and process it that allows Kafka
to be very scalable when we initialize
our consumer in our property we can give
it the group ID and the group ID
basically associates that with a
consumer group so we can have our
microservice all of our the instances of
our microservice initialize with the
same group ID and they can read
different parts of the stream right they
can balance the load amongst themselves
so this is our consumer Kafka consumer
and we initialize it with the properties
we call subscribes
to a list of topics right we can read
more than one topics and this is very
common pattern we just loop and and pull
for new messages okay so we can either
manage to offset ourselves or let the
API do that for us for consumer groups
what kafka dot what the API is do is is
store the offset in a Kafka topic so
this is Kafka 101 now we have producers
we have consumers we have consumers
group we can scale our micro-services we
can actually solve some of the problems
we already talked about right we can
stream all our data into Kafka feed from
from another macro service on or our
client systems we can read we can read
in scalable manner so we solved we
actually solve a big part of the
problems that we already had but wait
there's more I love that guy I still
haven't bought any of the renovator
stuff but one day I will okay so we
actually have a much better or much more
advanced consumer in Kafka debt I want
to talk to you about and I think that
for applications like micro services
this is actually very beneficial
Kafka has for the last couple version a
new library called Kafka streams and
Kafka streams basically brings a lot of
the capabilities of stream processing
frameworks that we see a lot in the
Hadoop ecosystem and in the big data
landscape into our micro services so
Kafka stream is a part of Apache Kafka
that means it's again an open source
project you got the community you got
the Apache Software Foundation backing
it and you can write stream processing
applications using it one of the biggest
differences or one of few big
differences between
kafka streams and other stream
processing technologies and we're going
to cover another major one is the fact
that it's self hosted anyone here used
spark streaming storm any of the stream
processing frameworks no - excellent so
one thing about all these two stream
processing frameworks like spark
streaming and so on and patchy flink is
that they run on some sort of server
software right if you want to run spark
streaming you either need a spark
cluster a mezzos cluster or a yarn
cluster to run it same wood flink
all of them have dependencies in this
big data ecosystem which is good it's
very powerful it comes with a lot of
capabilities for analytics workloads but
if we have our own web server our own
micro services sorry Web Services is too
long
the old way to go yes so if we're
running our own micro service stack and
we run it on kubernetes with with docker
or however we want to run it we don't
want to shift into another piece of
infrastructure spin up and a dupe
cluster just to do some stream
processing just to get the semantics of
processing a stream of data so that's
the old way of running those framework
that basically gives us this continuous
processing of data that runs through our
system right we want something that we
can run within our micro services and
this is one of the things that the spark
streaming brings to the table so we have
a couple of interesting API zhing in
spark streaming yeah Kafka streams sorry
too many frameworks so the first one and
and very important one is the Kay stream
API Kay stream is basically a stream of
data that runs through our system it is
built on top of our consumers so it has
the same restrictions or the same
attributes as our consumers
right our consumers right read from
different partitions messages are
ordered only within those partitions but
other than that we just get this API
that allows to do things like this is
just a stream that I get off Twitter and
our case stream just gets all these
tweets and what we do now we take all
the tweets and we make an array of words
out of them obviously this is just for
slide for the slide obviously we would
like to do some better parsing maybe
some regex
there are different white spaces that
you can hit with tweets and flatmap is
also very common in stream processing
and in fact in in data processing
applications it basically take those
arrays and make one stream out of them
so now we have our key that we stored
which is the same key that we
partitioned and a word for each key we
can do things like map map takes one
value in our stream and maps it to
another value so instead of a key of and
the value because we're not interested
in our key anymore
we can just omit the value in value
she's obviously very good coding but I
almost forgot to say what I'm doing here
the example that I'm showing here is is
the hello world of every data processing
application nowadays it's word count we
basically want to count how many times
each word appears in our stream and we
can do count by key count by key is very
interesting because it's an aggregation
and we have this function that
aggregates now and because we have this
function that aggregates we're going to
do two stream because we want to
generate a case stream again so there's
a bit of a subtlety here when we did the
aggregation we actually stopped
returning streams and we returned
something a little bit
I'm gonna talk about but basically what
we have now is a stream that gives us a
word and it's count how many times it
appeared so what is the thing that we've
created with count by key this is very
interesting and I think the most
powerful concept in Kafka streams and it
comes from what confluent called the
stream table duality so to understand we
have a stream here the stream is
basically people and their location so
basically myself and my wife we both
started in our stream in Melbourne we
came to Sydney and my wife is already
back in Melbourne right this is stream
when we look at this stream we just have
a chain of events we have your name in
Melbourne marina in melbourne univ in
sydney marine in sydney and more in
melbourne that's all we have but if we
want to look at this point in time in
what we call a table view this is what
happens right now right now your navies
in Sydney or NEADS in Melbourne and for
some things this is what we're
interested in right we're looking at
this point in time for other things we
actually want the streams we actually
want to see the event as they go through
the system so this is the duality
between stream and table in the CAF
castrum terminology and this brings us
to another api called k table k tables
are basically a table representation of
our data and they allow us to see the
data as it is right now in our stream in
our microservices and that's a very
unique capability so for example we saw
count count gives us a count point count
of the instances right now but we can do
other thing so we can do reduced by key
reduced by key basically for every key
allow us to do an aggregation on the
values okay so if our value
is something like a number that we want
a sum we can just add them to each other
right now for this specific table that
we've just seen all I want to do is
return the later location right I'm
interested in the latest value in my
stream
so this basically creates a new K table
and Mike a table can be or is being
updated as the string goes through the
system so why is that important it's
important because if we have you know
different pieces of data let's say we
have a list of items that we are
purchasing okay and we're also
interested in were currently the person
doing the purchases is located in normal
applications our consumers will get the
stream as it goes through the system and
we'll have to get the data from some
data store like our database and this
means they need to go over the network
they need to do a database query to get
you know my location my current location
if they want to see at this point in
time right this is a lot of what our web
service our micro services are doing
they're getting the stream of data
whether it's Kafka or just messages
going through the systems they process
it they update the database with the
current state what Kafka streams allows
us to do is to have both of those things
in the same microservice so we can have
a K table being updated in memory and we
can have the stream and what we can do
is join them and we can have a new
stream that contains this in memory data
that is a current representation of
what's going on in our application right
now your navies in Sydney Mori Lee is in
Melbourne and this is why I can spend my
money on a new MacBook obviously do not
need so this is some of the power of
Kafka streams think about it as
a in-memory cache that being updated by
your stream at all time so K tables K
tables basically bring us this
distributed data store that states
within our micro-services and if we
design it correctly because our
consumers read by read from the same
partitions by the key we can have data
locality we know that if we do it
correctly all the events were the key Z
and Eve are going to be in this instance
and also the current location of genève
is going to be in this instance we do
not shuffle data around the network
anymore
we have locality in this data source
stored is being updated by the stream so
we can still query it it still is a sort
of a database again I sell databases for
living and not this database but it
gives us a lot of the semantics of
databases in memory based on our stream
and that this is why it's very powerful
and and this is how we join ok this is
basically a k.k stream which is the
result of the join between user products
which is another case stream and user
locations and what we do is basically
create a new key value and I just
created these products locations per
object it's just an object so I have a
key and a new value that contains both
of those data items it's very simple
very straightforward in other languages
once this API will be available I'm sure
you can do things like using tuples and
so on yes it's being done so I'll repeat
the question the joining is being was is
the joining being done by Kafka it's
been done by Kafka streams right
it's the API that does it within your
microservice though those are two
different basically what we're doing
here
reading from two different topics one
one is the locations topics one is the
product topics right Kafka is unaware of
that but what happens is because both of
them has the same key and consumers read
by key because they read a partition we
know that the same key will be locally
on the same on the same instance of our
micro-service right so if we scale it
and we have more consumers running we're
reading more partitions then we know
that they still keep that locality one
caveat in that is that sometimes you
cannot do that and there's a que global
table which allows you to do the which
allows Kafka streams to do the shuffling
if needed so you can actually have the
same table represented in multiple
instances it's a bit more complex API
you sacrifice performance but sometimes
you cannot partition to cater to those
needs okay this yes you had another
question
you mentioned the Java is like where all
the interesting stuff is yes mainly
mainly ice-creams anything happening
especially was dreamed for the other
language so I know that the consumer and
producer IP eyes are supported in
multiple languages I don't work for
confluent I'm not involved in this
project I'm just a heavy user of it but
it's definitely something you can you
can find out on the Google Groups and
you can in them as far as I know they
haven't published any plans right now
but it wouldn't surprise me they will
support some other popular languages I
would assume that at a certain point it
will get to Python because python is
very very popular in this space yeah
okay so we've again solved a lot of the
problems that we had in in this slide we
have another problem if we go beyond our
micro services and that's a problem that
we have more than one data store that
needs our data and that's that's also a
very common thing especially today in
the polyglot storage era right we have
multiple data stores that that needs the
same data sometimes they need the data
after it was some transforms transformed
but sometimes they just need the same
data so Kafka can also help with that
and sorry one point it I forgot to make
clear is that when you need the data
transformed you also want one mastered a
lot of time one master data store that
is the source of truth for your
application so you might want to write
your database and then replicate
to the different stores okay
so what we really want is a way to take
data from our database our master
database and somehow stream it into
additional data stores like our search
engine like our elastic core or solar or
or whatever search engine we're using or
whatever additional data stores we're
using so one way to do that would be to
have just micro services shoveling data
from one store to another if and we can
use Kafka for that Kafka gives us
additional durability it's a good way to
transfer data from point to point
especially if you wanna move it between
places but Kafka actually has another
sub project called Kafka Connect which
does all that work for us so Kafka
Connect allows us to set up connectors
that can read from data stores they
they're called sources and write to
other data sources these are called
sinks okay and Kafka Connect is
basically a tool to stream data from one
data store or one data source to Kafka
and from Kafka to other destinations and
that could be many to many one to many
whatever you like okay it's very
scalable I'm gonna talk a little bit
about that basically it's an additional
cluster software that runs can run on
the same machines or different machines
you can play with that and gives you
some some management capabilities okay
it's very easy to replicate data from
into Kafka using Kafka Connect so as I
said in order to do that we have the
connectors connectors are basically
implementation of a few interfaces that
deal with all of the technicalities that
you need to do there is a really growing
list of connectors for almost any data
store that you can think of that include
things like sequel server Twitter
the Twitter firehose you just have a
connector that you can stream data
directly into Kafka without writing code
almost any database that you can think
of I've I've looked into doing things
like db2 for a large financial client
and really the the worst kind of data
stores that you can imagine they take
there might be some sort of support some
of the support is commercial and you
have to pay for it
especially when you're dealing with the
likes of IBM and some is open source and
and and a lot of good open source
software out there but you can implement
yourself and our caveat here is that you
should really know the data stores that
you're working with so for example I've
seen some implementation that just query
a database and just that just putting
additional load on the database there is
one for example for couch base and couch
base and meet something we call DCP data
change protocol it's basically the data
as it streams into couch base we also
have an API audit publish it to whoever
wants to read and that's the way that we
wrote the Kafka source for example so
but the technicalities of implementing a
connector are very simple it really is
just a few api's that you need to
implement as long as you know that the
data stores that you're working with so
for example the Couchbase connector this
is literally all you need to do is to
take the jar you put it in a
pre-configured folder that's your
connector you have that now Kafka
Connect can use that jar to to stream
data and you configure it so if you want
to configure
and Couchbase is a source and you can
see that our connector class is
Couchbase source connector you configure
things like the number of tasks that you
have and again tasks are just instances
that that run as consumers and those can
be scaled across the cluster connection
to the cluster the bucket so bucket is
basically a logical key space in cash
base it's it's the unit we store data
that could be a table in other databases
that could be collection in in other
databases whatever the database call and
the topic that we stream the data to
that's all you need in order to stream a
cart race bucket into Kafka literally
that's that we got production sites
working with a little bit more
configuration there's some SSL
configuration that you can add but
literally not more than that and you
have your data streaming from your
database into Kafka very similarly yeah
you can connect on on the other side and
just stream your data into your search
engine draft database Hadoop cluster and
one of the initial workloads that that
Kafka was built to to cater for was
actually offloading data into a dupe so
Kafka has a lot of you know built-in
api's the old project is called kami
after alberta me because you know when
you start with those type of names it's
very hard to switch theme but there's a
new implementation of that so it's very
easy so now I think we can do this we
can get from the point where our micro
services are coupled to each other and
we need to do a lot of shuffling of data
around into this world where we have a
stream of data we can have our micro
services to actually a lot more than
what they've done before right we can
use the K tables we can have these kind
of live caches within our applications
we
can you know do very expressive
processing on top of our data run it on
kubernetes docker swarm wherever we want
to run it without you know changing
anything in our code we can process the
data here put it in a data store and use
Kafka to stream it to other systems or
just stream the data directly from Kafka
so I think we're we're in pretty good
shape but wait there's more
yes so so here for example and and we
can interpret that it's that's the
beauty of art right so here I meant that
you know our microservices stored the
data into our very scalable database who
we will not name and the database can
just replicate itself to a search engine
for example using cough go connect but
it might as well be that the search
engine need the raw data and just picks
the same data streams as the micro
services using Kafka connect right so we
can play around so those are the
different yes exactly so you can do I
didn't repeat the question so the
question was if the data goes directly
through Kafka connect to the database or
not so so kind of the last thing I want
to go through with you is the conference
platform because there is literally so
much more to Kafka than we can you know
cover in one hour the confluent platform
is a commercial product by confluent it
has two variation there's an open source
one and a commercial one I'm not gonna
tell you more about the licensing
because I don't know a lot and I don't
represent confluent but confluent is a
company that was created but a lot of
the original team members of Kafka
within LinkedIn they spawn confluent and
they created this platform and this
gives us some more capabilities that we
usually need for micro services and for
other applications that might use Kafka
so here we have the Kafka core right
Patrick Kafka contains Kafka itself
Kafka streams and Kafka connects so
those are core Pachy projects don't need
to pay anything you don't need to
download it from confluence you have
everything available on the Apache
repositories and so on there are
additional open source parts to it so
there's the connector some of them are
being developed by confluence some are
being developed by different data stores
like cartridge did for the Couchbase
connector and some are by the community
we have the client a lot of investment
by confluent in those clients including
for example the c-sharp client another
very interesting bit is the scheme of
registry and one thing we didn't talk
about is when we use these pop
subsystems a lot of the time we
sacrifice some of the capabilities of
calling an API directly we sacrifice the
way the basically the capability of
defining our API some way right when we
stream data to Kafka at the end of the
day for Kafka it's key and few bytes
that it needs to store that all Kafka
knows about our data the schema registry
allows us to use Kafka but also use a
format called opera anyone here ever
used or no opera so a pro is a
serialization format and if we see
realized to Avro and we configure
through our producers the schema
registry what will happen is the first
time oh we can either push a schema for
topic or the first time we see realize
an object to a topic it will store the
schema for us and sorry there's an
additional API call that you need to do
with your object for
to send a schema but that allows us to
basically have a schema for our topic
and Avro is a very powerful but also
very complex way to manage schemas Avril
has different capabilities like backward
compatibility forward compatibility and
and or you can decide not to be
compatible at all bust but still have
your schema stored so this is one thing
for example that Kafka does very well it
brings us back to the whistle days right
where we have our schemas becoming
monstrosities but still it's a fairly
common need in micro services and in
services application we have a what they
call breast proxy which is a quasi REST
API to store data into Kafka so if you
want to send directly from a client it
only knows HTTP like Java Script you can
still do that I wouldn't necessarily
recommend that but it still is a
possibility and going into the confluent
platform we have things like the control
center that allows us to monitor and
govern our clusters another bit of cough
cough the open source project is a
project called mirror maker mirror maker
is basically Kafka - Kafka replication
that allows us to replicate data between
two Kafka clusters
located for example in two data centers
the problem with that is that mirror
maker doesn't have a lot of the security
bits that you need to do that at scale
and enterprise level so the conference
platform does do that and they have a
lot of you know really interesting bits
coming all the time they have enterprise
level support and that really allows us
to use Kafka in in heavier loads right
so we actually are bit quicker than I
anticipated and that's all I wanted to
show you today but but really I think
the main message here
is that Kafka can really power and
enable micro-services applications right
cover comes from or is best known for
what it does in the big data space and
this is how I became familiar with Kafka
the first time I used Kafka was to
offload data into a Hadoop cluster using
chameleon MapReduce and and and that was
a long time ago and as I worked more and
more with Kafka I realized that other
applications that I'm doing like
services can really benefit free from
Kafka so that was my journey it's it's a
very powerful system it's a transaction
log based app subsystem remember that it
has it's not a full-blown database but
it has some of the main concept of
database stores and not an interesting
bit that just came in the last couple of
version is exactly one delivery
guarantee okay so we can have exactly
one delivery with Kafka obviously with
those things and and for those of you
who remember the days of distributed
transactions there's a lot of overhead
for that and there's no black magic here
Kafka also I believe actually didn't use
that in large scale just played around
with it but Kafka will probably suffer
from the same pains of scaling
transactions but it does have that it
has that power of true data store run in
scale we have very powerful clients if
you're not using Kafka streams
c-sharp the Python the Go api's are also
supported by confluent okay so you're
not just sacrificing by not yeah using
Java it's just the streams ideas yes
no no the streaming is the only one only
one that is not available so if you're
using other languages and you're not
gonna use Kafka streams then you can
there are fully supported by confluent
in the community I've been involved with
people using Kafka from C sharp and even
Avril seem like a problem like so many
versions of digitization civilization
and - did you see this I think you
haven't tried it with C sharp what did
you see similar issues with versioning
and afro
yes I've seen I've seen issues with with
opera versioning and and actually the
avro jars outside of Kafka ended up Walt
so it is a common issue my only
suggestion and the question was
regarding Averell breaks between
versions sometimes it's actually common
for a lot of those formats I've seen
that with pork as well so the problem is
sometimes that they're changing the
underlying the actual underlying format
and that sometimes break my only
suggestion for you is and in this world
you the open source world you have to be
a little bit braver and go on the google
groups and have those discussions and
get the support from the commuters so
there are quite a few resources and all
of them available from through the
confluent website so that that's the
only advice if some something is
breaking they also support as I said and
this is one of the implication of having
fully support Apache project you have a
mailing list you have a lot of the time
Google Groups and those are the places
to get an answer the mailing list people
will answer you okay so that's that's
the way to go that
okay another very compelling thing for
Kafka is the unification of micro
services and stream processing once you
start working in the stream processing
world and you're used to doing those you
know fluent API is one-liners that
really transform your data and do a lot
of things in very few change functions I
will not use any any big words for that
it's very hard to go back to the to the
times where you have to loop through
data structures and and you know
transform your data
the Kafka streams API does a great job
of that so you can use that within your
micro services Kafka Connect for me it's
magic it takes so much work that I used
to do as a data professional for setting
up those polyglot systems and just
waters it down to a few configuration
files and that's it I'm good to go
we can enforce schemas we can use the
full-blown confluent platform if we'd
like to but we can keep it open source
and really it's a great community so my
only advice is if you haven't already
just install Kafka and take it for a
spin you you won't regret it
yes so where's the authentication and
authorization concerns across the string
managed in here
what again again I'm sorry I so the
question is around how does the
consuming service at the other side okay
so it's not text okay sorry so it's not
the security implications it's you need
to also have some context with your data
okay so there are two ways that you can
do that right and that's a very good
question the question is how can I have
the security context of some context of
my call on the other side on the
consuming side and there are two ways
that you can do that you can have it as
part of an object serialized into Kafka
and then you just read that and that's
one use case if that data data repeats
itself right it's something like a
session for a user and you want that
constantly available you can have that
in an additional stream and you can join
that using K tables and K streams and
that way you have very little data
running through the second topic the
topic that gives you the contact you
have that in memory within the instance
of your micro service and as data comes
you can join those streams so that's
that's a very good question
shows the power of K tables yes let's do
the mic because I forget to repeat the
questions yeah a few questions yes so
for the cork of API I mean the hook of a
client is the dotnet client as cool as
other language clients or there's a
deficiency there
I see an O here and I haven't used it
extensively I I have to admit I'm most
of the work I do is on the JVM but it is
supported by confluent and if you raise
your voice and you say that you need it
and you want it there is that's the way
to influence those type of projects
right if people shout enough you get
better support I'm not sure that it's
not as good as some of the other api's
it's just that those type of projects
and the dependencies for example with
Avro it's it dependencies on on
additional libraries it might break so
and as I said I had and and this is an
enterprise-grade product right I was
working with cloud era and and the
version of pork a changed and broke
cluster processing I think 50 terabytes
of data we just we had a an Impala
database it just one day one okay so so
those kind of shifts sometimes happen in
those type of projects you the best way
to deal with it is go on the mailing
list and and seek advice there and and
obviously not just upgrade all the time
to the latest version yeah so what's the
actual communication between the clients
and Kafka is it like API or what's the
so yes you you have the the consumer and
producer IP is for that Easter is I'm
asking many questions but I am very new
to it is there a static Africa app
available as a service from anybody but
without installing your own there
there's been some announcement by
confluent that's one option in Microsoft
Azure you have an HD inside HD inside
kafka cluster okay so Kafka can be
managed either manually you can just
install different instances of class or
nothing wrong with that you can run it
as a framework on meses or you can run
it on yarn in Hadoop okay so with
Microsoft there is HD inside kafka
service with with confluent I've seen I
can commit exactly what it is but I've
seen some announcement and some
discussions regarding Kafka as a service
solution as well and just two more
questions sorry there's a thing called
Apache knife I how is that related to
Kafka good question really good
questions computing products or know so
so Apache now if I basically gives you
if anything it's competing with Kafka
Connect but it allows you to basically
connect with data sources and
destinations and do some transformations
in a UI environment
and one of those day destinations can be
Kafka right knife I itself that is not
backed by any stores like like Kafka
that gives you that you know transaction
log capability gives you that you know
all those powerful guarantees about
delivery but but it's a very interesting
project itself I will if you're using it
disc L my go-to approach and I haven't
used it in this scale at all would
probably looking into streaming the data
through nine fight two Kafka and what
did you use to create those slides they
were really good it's an app called
paper and I used my daughter iPad for
that she's here if you want to borrow a
ride any more questions those K streams
and K tables are they maintained in the
memory of the client code that's right
that right like that has that code yes
yes so the K table is maintained in the
instances of your Kafka streams
application that can be any application
right it doesn't it the one thing is
here is that it can do some other code
outside that API if you need to but you
can also run that API within your java
application right now hopefully some
other languages and yes so the K table
is in that consumer being constantly
updated in memory and it's up to you to
partition correctly so you don't blow up
that instance and again with the K
stream yes
it's basically consumer that gets the
data continuously okay how do you handle
snapshotting so if your if your consumer
restarts and they lose it cuz it's just
in memory how's that okay probably I
want to consume for the beginning of the
stream yes so that that is that is being
managed by the offset
okay and and again I don't want to get
into too deeply into the delivery
delivery guarantees but yes that is
something that that with the offset that
is being persisted in a Kafka topic as
well
cough and potentially some additional
more complex mechanisms depending on
your guarantees this is how it's being
managed but yes this is definitely
something that Kafka streams manages yes
one second let's get you the mic yes
they're gonna be available online okay
if that's all thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>