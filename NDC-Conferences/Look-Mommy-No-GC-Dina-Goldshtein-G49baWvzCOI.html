<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Look Mommy, No GC! - Dina Goldshtein | Coder Coacher - Coaching Coders</title><meta content="Look Mommy, No GC! - Dina Goldshtein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Look Mommy, No GC! - Dina Goldshtein</b></h2><h5 class="post__date">2017-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G49baWvzCOI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so hi everybody again starting
again
first of
thank you for coming I know it's late
and well on the upside I can keep you
here as long as I want just kidding so I
work at riverbed we do application
performance monitoring and I'm going to
talk to you today about how you can
optimize the memory traffic of your
dotnet applications so first I'm going
to give you some motivation I'm going to
show you some real life questions and
problems that people encounters that
have to do with memory traffic I'm going
to show you several ways with which you
can identify that you have a memory
problem in your application because
obviously you can't start optimizing
before you know where the bottleneck is
then we're going to see a few examples
actual live demos of code patterns
parents specific coding patterns that
can lead to problematic and excessive
memory allocations and finally we're
going to finish with some general
recommendations about how you can lower
your memory allocation and thus time
spent in GC even further so memory
allocation is easy right it's cheap you
just have to move your heat pointer you
know one simple one simple command it's
not free though and it adds up and it
has side effects on top of adding up or
rather one major side effects and that
GC because the more memory you have the
more GC you need to do and it takes time
GC has to actually stop your application
from running in order to do that memory
clamping that it does and so the more
memory you use the more GC you need to
do and the GC itself takes more time and
you know things kind of get out of hand
big performance problem and the thing is
that at least from my experience usually
we spend most of the time when we plan
our work think about thinking about
optimizing algorithms
and mainly our CPU usage and it's not
until we encounter a really severe
problem that comes from excessive memory
allocation that we suddenly remember oh
that's right I should have made that
better
I should have optimized that thing and
that's what we're here to talk about so
I'm just going to give you a few
examples that I found online there are
really a lot of questions Stack Overflow
that has to do with that have to do with
memory allocation and GC so here's the
first example here there's a guy who's
using unity to write a game he had some
very severe performance problems in his
update function update is what you do
when you need to update your frame and
it has to be really efficient if you
want the game to run smoothly and you
actually ran a profiler and he found
that 33% of the time in the update
function is GC collects now 33% that a
third of the time that's a lot it's not
normal it's a huge degradation in
performance something you shouldn't Lee
don't want to have inside your update
function another example someone is
asking why or his code is causing so
much GC he doesn't know why he wrote the
code sample it doesn't matter in this
moment my point is that he wants to know
why it happens it wants to understand
his code or another example someone
wants to detect what or who or why
someone is calling GC collect he was
using a third-party library and he had a
performance issue and he had an idea
that maybe that third-party library was
calling GC collection side and he wanted
to know how he could check that so the
first thing that we're going to do today
is talk about identification of the
problem
I do hope that by the end of the talk
you're going to know enough about how
dotnet works with regards to memory
allocation
garbage collection and you will be able
to understand how the code that you
write effect affects memory traffic and
so you will be able to answer these
questions and more so again let's start
with identification because without
identifying the problem we can't start
optimizing the easiest way I think to
detect that you have a garbage
collection problem is using performance
counters performance counters are
available system-wide in Windows they're
built in there kind of like these data
emitters which are embedded in different
locations in your system and they can
provide performance information
numerical information about the
different areas where they were it sits
so you can get information about the CPU
usage and about memory allocations and
disk usage and networking and of course
dot map and the nice thing about it is
that it's available on demand you don't
have to relaunch recompile get sure that
your application you just need to use
the right tool to kind of hook inside
and see these performance counters in
action so for example two available
tools for you which come built-in inside
Windows R type curve which is a
command-line tool which allows you to
monitor any performance count where you
want in real time or perfmon which has a
nice graphical UI which allows you to do
the same thing as a side note I will
mention that you can also create your
own performance counters and omit
performance information about your own
applications so that's maybe the DevOps
people or you know someone else can
monitor easily your application and
production but that's not the topic of
our talk so first let's see how we can
use person one to realize that we have a
problem I have a virtual machine here
with Windows and I have and I have
an application ready for us it's called
the Jack compiler by the name you can
understand that it compiles something
it's actually an exercise I did for
university quite a few years ago it's a
made-up language we used to learn how in
compilers work so we implemented the
compiler and we are going to run this
compiler on some big problem excuse me
on some big program ok I'm starting it
run now I want to monitor what my
application does so I'm going to run
perfmon nice graphical UI I go to the
performance line tour area and it comes
ready with displaying the CPU time in
our machine I'm not interested in that
right now so I will just remove that do
it and I will add the performance
counters that I am interested in which
in this case have to do with memory
consumption so I'm clicking the plus
here and I can see a very long list of
groups of performance counters they
actually grouped into groups by area by
by the system that they put a monitor so
you have energy meter sack service file
system something HTTP obviously we won't
go over all of them because there's a
lot of them so I'll just skip right
ahead to the dotnet CLR memory where is
it here this to the dogmas CLR memory
group I'll expand it and I can see the
actual performance for our counters so
for X for example I'll obviously be
interested in the bytes and all heaps
that's the entire managed memory that
our application uses and I can either
monitor it for the entire system or it
can select a specific process in our
case where is it
it doesn't run I thought started
starting ok now it's running I apologize
try again ok I think I need to close
that and so plus dotnet dotnet memory by
it's an all heap and okay here it is
thank you I will add it to my list you
can see that we can get the information
about the number of GC handles the
number of GM 0 collections the number of
induced GC which might be interesting
the number of pins objects and so on
I will obviously select the percentage
of time in GC that's a very important
metric for me and I also want to see
let's say be allocated by its per second
and I will start that and immediately I
can see a real time graph monitoring
these values now in order to see all of
them on the same plot I will have to
scale them ok and here they are now the
most important one in this case is this
blue line which symbolizes the amount of
time the percentage of time spent in GC
and straight away we can see it's a
problem it's actually the scale here for
this graph is 1 and so we can see that
on average our program spends about I
know let's say 60% of time in GC that's
totally not normal so I now know for
sure that I have a problem with GC do I
have a memory leak probably not the red
graph here
that's the bytes and all heaps that's
all of my managed memory it might be
high it's scaled it doesn't look higher
at scale so it's high but it's constant
it doesn't change however still we do
enough locations in such a way the GC is
very stressed
ok so now we know we have a problem we
don't know where it is though we don't
know what caused this excessive GC so
we no need to figure out what's going on
in our program luckily for us there's
another tracing mechanism built in
inside windows and that's etw event
tracing for windows in a sense it's very
similar to performance counters in the
sense that it proved it's all over the
system it can provide information from
both user mode and kernel mode from
drivers from services from dotnet from
the file system networking whatever you
want it's even inside third-party
components like Chrome it can also be
turned on on demand just by starting to
write to and analyzing that you don't
have to relaunch or recompile your
application but the big difference
compared to performance counters is that
this is not limited to numerical data
egw is actually a logging framework a
structured logging framework which can
provide any data at once
basically I support hundreds of
thousands of structures log messages per
second and it has very low overhead of
course depending on the amount of data
that you want to emit now again as a
side note I mentioned that you can write
your own etw provider and you can omit
your own into W messages and again
that's not the topic of our talk though
it's worth mentioning that so let's see
what other data we can get using etw
this time I'm going to use a different
tool called perfu it's not built-in
inside windows but it is freely
available from the Microsoft web site
and in fact this recently became open
source so you can you know totally use
it enhance it whatever you want and and
this is although not very visually
appealing a very good functional UI for
both collecting and later analyzing
these etw logs so if I start my
application again
and I want to record what's going on the
system I can go to the collect menu here
this requires administrative rights so I
will have to restart it as again and
again not very visually appealing but
functional there are a few options here
which we can configure I'm not going to
go into much detail because that's not
the focus of our talk but you can see
that you can select different areas of
the system that you want to monitor CPU
sampling dotnet dotnet allocations
inlining file i/o all sorts of stuff
like that now I'm not going to collect
now because I don't want to waste time
on running the application and
collecting the data but generally you
just do starts collecting wait a while I
don't know 10 seconds whatever and stop
collecting and after you stop collecting
what you're going to get is a file with
the ETL extension not sure what it
stands for
eg log I don't know doesn't matter and
the same application perfu is actually
able to analyze this file for us and
provides very convenient statistical
predefined views for viewing these these
log messages so the first thing we're
going to look at is the steel called CPU
effects now I'm not sure if you noticed
but I actually didn't select anywhere in
the collect menu that I want to sample
specifically my Jack compiler and that's
because the collection is done
system-wide so as I have information
here about all the processes to run on
my system so I'm going to select in this
view my Jack compiler and the first
thing that I'm going to see is this nice
table with lots of function names and
the percentage of time that the
application spent in these functions and
straightaway I can see that 58% of the
time my application spent inside string
concatenation okay interesting maybe
it's okay maybe not but that's important
information now
if I want to know who called that string
concatenation I can double click it and
it will lead me to this knife well I
don't know it's quite nice but this tree
call stack view which will allow me to
expand it and see the code path which
led me to the call to synchro catenation
so I can see that 12 13 percentage
thirteen percentage of the time I was
led to string concatenation from the
assignment function and another 12% was
from the Cole function and so on I can
now go back to my code and see what's
going on there let's look at more views
that we have here we have a memory group
interesting right and we have a GC stats
view let's open it up again we have
information for all of the process on
the system but we're going to select the
Jack compiler and what we can see here
is some statistical is a statistical
summary of the GC for my application so
for example I can see the total amount
of allocations and I can see the total
GC CPU time and I can see the percentage
of time the CPU took and and so on in
fact if I go down I even get more
information about the different
generations in the GC and if I go even
further down I can see a huge list of
all of the collections that happens in
my application obviously we're not going
to look at it now but the data is there
okay let's see what else we can get from
this excuse me here it is okay we have
another interesting and you see view
here and that's view summarizing all of
the allocations that our application
made and in fact that's probably what's
going to solve the mystery of what's
causing our problem takes a little while
to analyze okay again we have to select
our pro
such a compiler and okay how easy was
that a hundred percent of our
allocations is system strings now
combining to the fact that we know that
sixty percent of the time we were
spending in string concatenation you
know that string concatenation might not
be the most efficient thing in the world
thus mystery solved if I want to know
more specifics about where the strings
are allocated like magic I double-click
my type and I get this nice view tree
like view again where I can see where in
my program these allocations were made
so cool if UW gave us a lot of
information another way with the solve
or look for memory allocation problem in
problems in our application is using
static analysis static analysis
basically allows us detecting the
problem before we even start running the
application we don't have to wait and
run it and like wait for some specific
weird scenarios to occur the idea is
that an automatic tool goes over our
code and searches for specific coding
patterns which are known to be
problematic or excessive in allocations
for example string concatenation right I
mean it's really easy to detect string
concatenation automatically you need two
strings and you need a plus operator
between them sounds like a good Python
exercise another area where it could
help us is try to detect a memory weak
even by detecting where we forget to
call dispose on our I disposable objects
that's less trivial than finding string
concatenation correct but still it's a
coding pattern and once you have a
pattern an automatic tool can do it for
you of where static analysis is not
limited to just finding memory
problems it can help you in other coding
problems as well like passing the wrong
number of dynamic parameters to
console.writeline or if you try to use
an uninitialized local variable for
example now they're available there are
several available tools for you to do
static analysis starting from a built in
static analyzer and Visual Studio
ranging through resharper which I guess
you are all familiar with and then you
have Coverity which specializes
specifically in static analysis and
costs many thousands of dollars per year
so so yeah now let's try I don't have
Coverity but let's try to see what
Visual Studio could tell us about our
Jack compiler which and we can see that
we could avoid all of this problem to
begin with so going back to my machines
to close some stuff close good I have
Visual Studio here do you see the code
the back there good
so here is my Jack where is my Jack
compiler here is here's my Jack compiler
we're not going to like to really read
the code now right
I'll just open a few random files you
will understand Daniel T why I do that
and what happens is that whenever a file
is open visual studio runs its static
analyzer on on the open files and emits
warnings here at the error list window
now just I forgot to say that it's not
exactly built-in inside Visual Studio
it's actually a it's a free extension
available online for free so it's not a
problem and it's integrated inside
Visual Studio
it's called CLR heap allocation analyzer
its sole purpose is to
memory allocations your.net applications
and so let's look at a few examples of
the of the warnings that this extension
amidst well so let's see I'll double
click here and I'll read it to you it's
a little small non value type
enumerators may result in a heap
allocation so if you do forage and on
non objects or non reference type list
you might cause heap allocation or
another example here is a good one
consider using string builder this is
one of our famous or infamous string
concatenation in this case it's just two
strings so you know maybe it's not at
that but actually have more more
examples here when we concatenate a
whole bunch of strings and so it tells
me you're better off using a string
builder let's see another example and
then we'll move on okay let's do that no
I like it laughs okay you can see a lot
a lot of string concatenation we're
already know that that's the problem
right okay that's nice what is that
non overridden virtual method call on a
value type adds a boxing or constrained
instruction so basically we have a we
have a value type here and we call a
virtual we call a virtual method and it
causes allocation and in fact we're
going to talk in more detail about it in
a few minutes so it seems like we could
avoid a lot of problems by just running
the static analyzer on our Jack compiler
before we started running it good the
last method of detecting problems and
your application is dynamic analysis the
problem with static analysis is that
some problems are still very hard to
find and are not characterized by very
specific and constrained coding patterns
moreover there are some scenarios such
as memory leaks which are very hard to
diagnose without some sort of temporal
analysis of what's going on in your
program and that's what dynamic memory
profiling is for again you can do that
both on native and managed memory and
they're a bunch of tools available out
there they're mostly commercial and
what's interesting here that you might
want to maybe we're asking yourself we
got a lot of information from etw right
and indeed some of the tools used etw
as the source of their information
however the main difference between
using etw yourself and using a
commercial tool is that the commercial
tool will usually provide a nicer view
of the results it would make some of the
analysis for you it would point you into
the correct direction if you need to
diagnose a leak
using etw you would have to go over a
lot of objects and might count them or
whatever a memory profiler will usually
do all that comparison and analysis for
you and just give you the results okay
that's the type that is leaking that's
what's holding it in memory now running
a memory profile takes a while so we
won't do that now but I do have a
screenshot here from a tool that is
named dotnet memory profiler that's very
not very imaginative and what you can
see here is that I ran it it's actually
also has an extension for visual studio
and I ran it on a certain c-sharp
application not the Jack compiler and I
took several snapshots and then I used
all that memory compiler to compare
between these snapshots and it actually
tells me that the number of response
objects grew from my first snapshot to
my last snapshot this might be a leak
not necessarily it's quite possible that
it's got all going to be freed up in a
minute
but it might be a leak now if I want to
know what's keeping these objects from
being collected I can just go to another
view inside the dotnet memory profiler
and this time it will show me that in
fact there is a static object called
response cache well not everything is
clear right so this response cache which
is static that's the route that's
holding all of my response objects in
the memory at this point I might decide
that this is indeed a bug I can go back
to my code and fix it ok so I think we
have a good understanding of how we can
detect that we have a problem now let's
try to see a few examples about how we
could reach such a bad state in our
application which coding patterns that
we all use by the way might lead us to
such behavior so first of all I'm going
to start with an example I have lots of
stack overflow screenshots today there's
a guy here he's doing some graphics I
think a lot of my examples have to do
with graphics it's interesting so he has
a dictionary of vectors and what he
noticed is that whenever he accesses
this dictionary there is memory
allocation happening and he was
surprised because dictionaries are
supposed to go first of all he's just
accessing like he's not allocating
anything himself he just wanted to look
inside the dictionary but also he was
quite surprised because dictionaries are
supposed to be really efficient right so
do any of you have an idea about what
might be causing this memory allocation
boxing ok good that's the answer I was
hoping for it's the correct answer not a
trick question indeed it's boxing but
what I want to do next is understand
like really down to the metal what's
going on and what's causing this boxing
and to do that I will do my own
benchmark on something Oh which is a
little different but simplifies what
happens in this case
so in
one sentence I'll remind you that the
dictionary is actually a hash hash or
table right but a hash table has Senate
size so some bins in the hash table
contain lists of elements right the
elements that have the same hash
function so eventually when you access a
dictionary you're going to calculate the
hash of your object and then you're
going to have to go through that list of
elements and find the one that you're
looking for so what I'm going to do now
is that I'm going to benchmark the hell
out of searching elements inside list
okay so let's imagine that we have a
data structure and it contains a single
integer the guy had a vector it was two
integers or two doubles whatever we're
going to have a single single member
single member and then we're going to
have a list or array doesn't matter of
such data structures and we're going to
search through those and the key thing
to understand here is that there are
actually three ways to implement that
scenario the search is always the same
right we just you know use contains
we're going to see that in a moment but
the data structure is so we could write
it in three different ways
we could either you know it just let's
look at the code instead we'll be much
easier to understand
boxing okey okey so
don't mean that okay so the struct
itself as I said it's going to hold a
single data field and there are three
ways to create such a data structure so
we're going to use a struct you could
just you know right at the simplest way
we could struct with a property value
easy three lines you know looks perfect
then another thing we could do is that
we could override the equals the base
equals method that we have in every
dotnet object the code is simple that
didn't do anything special again
not trick code we take we check the type
and we compare the value very simple and
then I could even go as far as
implementing the I equatable interface
the I equatable interface basically says
that the type has an equals method which
doesn't accept an object but accepts the
type itself and in this case when I
implemented it's much simpler because I
don't need to check the type I already
know that it's the correct size because
that's the input of the function and I
just compare values so those are the
three different ways in which I could
implement such a stroke now let's look
at the test itself by the way all the
code is available on github you'll see a
link at the end of the slides and you
have some extra test cases here which
we're not going to discuss but I
recommend that you look at them later
it's interesting to try to understand
what's what's going on so for my
benchmark I'm going to initialize three
lists of the same size of course of
these three different structures now the
initialization which happens here is not
going to be measured as part of the
benchmark because I'm only interested in
the search time and then
I'm going to do the search itself so we
have three benchmark functions which are
marked with the benchmark attribute I'll
explain that in a moment and they all do
the same thing basically each of them
goes over the list and it asks whether
it contains the last element of that
list why laughs because I want my
benchmark to go over the entire list if
it stops after the first one it's less
interesting for me I kind of want to
check the worst case possible now just
one or two sentences about this
benchmark attribute here bench correct
benchmarking is not easy you need to
think about warming up the cash you need
to think about the overhead of the
measurements itself there's a lot of
stuff that you need to think about and
there's a free library online and some
nougat it's called benchmark dotnet
again I'm not going to get into much
detail about it but it just makes your
benchmarking life much easier basically
you put that attribute on your benchmark
functions and then in your main you will
simply call benchmark runner
doctrine and your class with all the
benchmarking functions whatever you
route you're right and the class
constructor is not counted as the
benchmark so that's why I have the
initialization of the lists in this in
the beginning and the constructor and
then I have the methods which implement
the benchmark themselves later now
benchmarking takes a lot of time so
we're not going to run it live but again
you can totally recreate the results at
home
I just prepare the results in advance
voila these are the results now let's
ignore the first line for now I promise
I will get back to it later so let's
concentrate first on understanding the
difference in the results between the
case where we override equals and the
case where we implement I equatable know
two things here the time for the
override is longer and there are memory
allocations while the time for the
interface is shorter and there are no
memory allocations at all let's
understand how that happens so how does
searching work well it's easier right we
have our collection we basically go over
it and compare you know all the elements
to the elements that we're searching for
so if we're dealing with primitives for
example it's pretty obvious how to
compare them right but if we have
user-defined structures it's not so
obvious anymore so we need to create a
compare which is going to compare the
elements for me and it turns out that
this is where the difference lies we can
see it a snippet from the create compare
function here and we can see that there
is a difference there is a specific
special comparer created for the case
that the time implements i equatable and
another compare for the rest of the
cases and if you think about it it
actually makes sense right because after
all if you implement it i equatable it
must mean that you want to use your own
you know equality function that make
sense
now let's look at the compare ourselves
so this is the object equality compare
that's the regular compare oh the one
that's used for for everything
everything but i equatable the code is
pretty straightforward right that's the
key command here right x equals y now
what is this equals function this is
just a regular object with no
constraints so this equals function is
bound to the base class object equals
method which accepts objects as an input
now so let's try to understand what we
have we have a struct which is a value
type and we have a compare
which calls equals which accepts an
object we need to pass the value tied to
an object so boxing number one why is
obviously boxed what about X well again
X is a value type right it'll struct
however equals is a virtual method and
we need to transform X into a reference
with all of the Tedder's and virtual
method tables and so on in order to be
able to call that virtual equals method
on our X that's boxing number two okay
so now we understand where all those
memory allocation is coming from that's
the code for the generic equality
compare the names are really bad by the
way I have to say I can't remember which
is which like ever so the code for the
Equality compare which is specific for I
equatable it looks the same
you can play really compare it it looks
exactly the same only that and you can
see that in the comment here that in
this case the equals method is bound not
to the object equals the object method
but rather to the eye equatable equals
method you can see that by looking at
the code that that's the way it is now
with that information what's going on we
have the struct if I equatable we pass
it to the equals of that i equatable
awesome no boxing so minus 1 bucks what
about X though now this is a little
tricky and the reason that X doesn't get
boxed either is that the CLR was planned
in such a way that calling interface
methods on value types is jetted into a
direct function call so this is not a
virtual method call and instructions you
don't have inheritance doesn't make some
new virtual methods so the gist
transforms this into a direct call
again - two boxes no memory allocation
whatsoever
and that explains the results indeed if
we run etw if we sample a little bit our
test case with the overridden equals we
can see that what is that like forty
percent of the time in the test was
spent on creating new objects so that
really explains what happens to that guy
with a dictionary because his problem
was that he is vector the key to his
dictionary did not implement I equatable
and once he implemented I equatable on
his vector all of his problems were
solved cool now I didn't forget that I
ignored one using I didn't forget that I
ignored one test case and that's the
case where we searched abstract that
didn't have any sort of specialized
implementation for equals now that has
less to do with boxing the memory and
more to do with the fact that the
default comparison for structs is not
reference comparison but rather a value
member by member comparison and if you
don't implement it yourself dotnet is
going to implement it for you but it's
going to use reflection and all sorts of
very non efficient stuff and that's what
that's the result in this horrible
performance so if you ever have a struct
and you know that you're going to
compare it to like anything ever just
you know the best thing would be to
implement I available but like as we
override the equals method okay so to
our next scenario another c-sharp GUI
which we're all like using lambdas
everybody uses lambdas they're so
awesome they make the code shorter so a
quicker explanation about how lambdas
are used how are they compiled so if we
have a lambda which
captures estates from its scope what the
compiler is going to do and that's
totally a compiler goodie here what the
compiler is going to do is that it's
going to generate behind the scenes
class we don't know its name we don't
see it in the code and this class is
going to have a field which is the state
that needs to be captured by the lambda
and it's going to have a method which is
the actual code for the lambda and if
you want to call that lambda then you're
going to have what the compiler does it
for you instantiate this
behind-the-scenes class it will save the
state inside the field in that class and
then it will create a new action
invoking the method implementing your
actual code okay now what I want to do
is to try to measure the effect of all
of this on our code what I'm going to do
as I thought of this a lot of times we
use lambdas in tasks right we create
tasks we pass a lambda that runs the
code so what I wanted to do is see how
this affects our task creation so I'll
just open it up tasks already now since
I didn't want to measure the running of
the tasks themselves but just creating
the tasks I wrote a task stub which
doesn't do anything except that it has
the same API as the real task class for
creating tasks so mainly it has two
functions it has start new which accepts
an action and it has a start new which
accepts an action and a state which is
to be passed to that creation to that
function okay and let's look at the
scenarios that we're going to check so
most mainly what we're going to do is
that we're going to check the case where
we have states that
we want to pass to the task and the case
where we don't have to pass state to the
task so it's a little artificial example
I know but let's imagine that we have
some data structure and we want to sum
up some numbers to a global sum and for
some reason we want to do it from
multiple tasks all right so first
example let's say that we want to
capture the state and that's the data
that we want to sum up to our global
member so we could have a loop go over
the elements we create our data and then
we start a task with the lambda which
captures this data from the scope then
we could use the fancy tasks API and
instead of capturing the state we will
use the API which accepts the state as
an object as an input to the task
creation function so in this case the
lambda is actually receiving input it
received receives the data as an input
and this input is then provided to the
task by the task class provided to the
lambda excuse me by the task itself and
then we have two cases where we don't
have any state that we want to pass so
we just have some constant value that
we're adding to our tasks so we just
know do a loop starting a new task
adding some constant value no data to be
passed and another case well sometimes
we don't have a lambda we actually have
a function with a name and I could do
this sort of silly lambda right but we
Sharper keeps suggesting that it turned
that into a method group which makes the
code much simpler right so I just do
that it's nice and easy
let's look at the results okay so let
that sink in for a while these are the
results we're going to see them later
again so let's just analyze case by case
what's going on
the first thing though what I want to
show you is that there's a
correspondence in general between the
total allocations that we made in our
benchmark and the time that the
benchmark took and I think it should
surprise you by now judging from all the
examples that we saw before so now let's
try to understand where all these
allocations are coming from in each case
so let's start with the case where we
capture states when we capture state we
have that behind the scenes class that
we need to instantiate okay so
instantiate the class and of course we
need to instantiate the bailer itself
obviously we save it inside our field
and then we need to instantiate an
action which is going to run the method
in that behind-the-scenes class that
implements our addition to the global
variable so we have three things that we
allocate in each iteration the second
case we pass the state as a parameter
and when the state is passed to the
parameter there's actually no capturing
and if there's no capturing then we
don't have to instantiate the
behind-the-scenes class so in this case
we're only going to need to create the
data will we pass the data so we need to
create it obviously but then we just use
a method which the compiler created for
us it's a method inside a static field
it accepts input and it implements the
addition and the compiler is actually
smart enough to do the action creation
just once it caches it inside a static
field so we have data which is
infatuated for each iteration but again
I mean that's what we wanted and we have
a single instantiation of the action
those are the cases where we have States
graph what about the case where we don't
have state well that's easy
we don't have States at all so we don't
allocate the data we do however need an
action which you know sums up this
constant value that you're using and
again the compiler is smart enough to
cash it in a static field and sanction
the action just once now here's the most
interesting thing though you would
expect this case to be exactly the same
as the last case where I used a named
function right but it turns out there's
I think it's a bug in the fish our
compiler and for some reason when you
use a method group it fails to do this
optimization of creating the action just
once so so in fact you get a worse
performance than you would have thought
and this pretty much explains the
results so in the first case you have to
allocate data you have to allocate
action and you have to allocate in the
behind-the-scenes class each iteration
so that's the worst case then the second
case you have to instantiate the data of
course and well basically that's if you
allocate the action just once so it's
better in the third case you have the
action which was instantiated wise once
you don't have data you don't have faith
so that's the best case and then sadly
although you would expect it to behave
the same in this case the action is
allocated over and over again and I
guess that the action object takes more
space than our data objects because it
seems that the total amount of memory in
the last case is higher than in the case
where we have state awesome so now we
understand that and if we join these two
things together
boxing's plus path lambdas in two using
our favorite link right then looking
fully understand that link is the
culmination of all evil now family I
don't have much time because I don't
want to keep you here long much longer
than you need to but I will just tell
you that I wrote I wrote a benchmark
doing some mathematical computation
which involved
a double loop 1 loop inside of another
and I implemented it in three different
ways
once you know just like using loops
another time using a single loop and a
little helper thing inside from dotnet
and once totally using link you can see
the code and github later trust me that
I implemented it correctly I checked
that the value returned from these
functions was the same and then I ran
the benchmark and I got these crazy
result okay so the wrong version is
actually almost 10 times worse than the
version which just used loops and look
at all of the memory allocations we see
that the memory allocation is about 10
times higher and as we already saw
there's a correspondence between time
and memory and so you know it's really
bad I can't really say otherwise the
last demo that I want to show you today
is the case of premature allocation it's
a really cute one let's see here let's
imagine we're a web server and we need
to return responses and these responses
are based on you know lists of
collections of results or whatever which
we're going to have to put inside an
HTML list so we could do it in three
different ways
the most naive representation would be
to use a string builder and that's
already a of course an optimization over
just concatenating strings over and over
again but basically we go over the data
we concatenate the items and encode the
results and return us when the data is
empty this loop is not going to get
executed you know everything works a
better version we could implement is to
cache the empty response which I saved
here right this empress part just holds
an empty HTML list so if I detest that
my data is empty I just return
an encoding of the empty list or of
course I could go as far as optimizing
this whole thing I mean after all why
should I encode this empty result over
and over again
I'll just cache the encoding the final
encoding result for the empty bytes and
if I detect that there is no data to
return I just return the pre calculated
results let's look at the results of the
benchmark Wow
astounding again right I think it was
rather obvious that the the second and
the third are going to be better so the
second is like three times better than
the first one but the last one it's
basically infinitely better than the
first two I mean there's nothing on a
computer that runs so little time so it
was basically all optimized out like it
actually does nothing and that's
obviously the best you could hope for
and imagine this is your web server
which you know has to answer millions of
requests per second whatever and this
optimization is so easy you could
totally go ahead and do it like right
now good so in the time that we have
left I want to give you some general
recommendations like morning on
hand-waving the style of more ways you
could reduce the stress on your G C in
your diet application again we're going
to have some real-life examples from
Stack Overflow so the first
recommendation to you is don't call GC
collect yourself okay it's okay if you
do it for debugging or testing or
whatever but you really need to make
sure that you remove it before deploying
to production because generally what
happens here is that you have a memory
problem so we're selling through in self
oh okay I'll just go GC kolaks
and everything will be okay but you need
to trust the CLR the system is just the
system that it will call GC collect when
it says fit and what you're doing is
like putting a patch or like putting
band-aids on a bleeding hole in your
hand
because if you have already a memory
problem then calling GC collects will
just take up more time if you have a
memory problem and will cause more
process and more performance degradation
for your application instead you should
use all the techniques that we discussed
today - I know like avoid getting your
the hole in your hand or whatever no
caching and again we already saw this
screenshot from Stack Overflow and the
beginning of the of the talk this guy
was using a third-party library and the
third-party library people thought it
was a good idea to call GC collect and
just ended up giving more performance
problems to this poor guy who is using
this library I'm not sure which one as
well doesn't really matter
here's a funny story in this case what
the guy had he had a sort of a
producer-consumer kind of thing and this
application kept crashing with out of
memory exception in a desperate attempt
to solve the problem he put GC collect
inside his producer cred and that solved
the problem okay cool
now if you go online and read what he
wrote next you would find out that the
actual problem he will eventually he
solved the mystery they actually dropped
the actual problem was that his
consumers weren't fast enough and simply
weren't fast enough they just couldn't
keep up with you know the producers
producing the the jobs to do and you
know memory would just get filled up so
when he pulled GC collects inside his
producer said he artificially inserted a
delay in producing jobs so I mean you
know I put the animal in brackets I
don't know if it's fortunate or
unfortunate but this is a really good
example of how you catch up the problem
and you don't see the real root cause
and that's what you need to solve so
that's a cute example
my second recommendation to you is
whenever applicable which is the whole
we not always prefer value types value
types are might be much more efficient
to use in case where you have a raise or
list of many data structures because of
the way they are stored in memory let's
first look at the reference reference
sites what happens in reference types is
that each reference each objects also
has an 8 or 16 byte header depending on
your business with all the metadata and
information that has to store with the
objects about the type virtual methods
whatever so you have the data itself and
then you have the D headers and then of
course the array itself is made up of
references right through these objects
and of course not to mention that now
all of this data is not continuous in
memory structs on the other hand don't
incur any extra cost at all it just
backs up continuously in a single chunk
in memory so that's good for caching
it's good because you don't allocate a
lot of stuff you can do many allocations
and you don't cause many structures that
stress out the GC now so I'm not saying
you should always use structs because
they do have extra overhead in passing
them as function parameters or returning
them from functions but if you have a
scenario where you lots of calculations
on a raise of such objects it's
definitely worth considering using the
struct like for example if you do image
processing that's why we have the point
to the example here and this is exactly
what happened here in this question
someone is implementing a ray tracer in
c-sharp I'm not sure why someone would
implement a ray tracer in c-sharp over
C++ but let's put that aside vectors are
used quite a lot in graphics
he had very severe
memory problems the performance problems
in general and they were all fixed once
he transformed his vector from being a
class to being a struct so again based
on a real story next recommendation is
complex reference graphs are stressful
on your GC they cause more cache misses
again like in the case with the array so
in general you could replace them with
just simple arrays of course it depends
on the size and it's true that a nice
object area oriented the design would
use classes and subclasses and reference
and so on but it has a call but it
definitely has a cost
another very important recommendation is
you should make note of finding figuring
out what your most expensive or your
remote or your largest objects in your
application and pull them buffer them
reuse them however you can just not to
allocate them over and over again there
are many objects pool excuse me
implementations out there in fact
recently I saw that Microsoft uploaded
their own to nougat I didn't use it
myself but it has like a gazillion
downloads so I I guess if you don't have
your own object pool just try that one
it's going to save you a lot of time
another idea you could do especially if
you have lots of byte arrays instead of
allocating small byte arrays each time
you move one you can allocate you can
manage the memory on your own you
allocate a large buffer to begin with
and you kind of let consumers chunks of
that of that buffer according to what
they need until you just take special
note that dealing with sub arrays might
cause allocation like copying or
whatever and so again there's a library
available for you in nougat it's called
the system slices and it provides an
allocation of three
API for dealing with sub-arrays school
alright again based on a real story and
in fact this one happened to me as well
personally so I'm feeling very
emotionally connected to this one again
from from the graphics area someone who
is writing a WPF application and he had
to display an image which was updated
over time he was using a bitmap source
and each time he got new data he created
a new bitmap source to display in his
application but the truth is that when
you have such a scenario what you need
to use is something called writable
bitmap which provides an API for images
that you can update without allocating
the pixel buffer again and again you can
simply overwrite the pixels whenever you
have new data instead of creating the
overhead of a new image
excuse me and as we all know images
these days tend to be quite large
the last recommendation I want to give
you the final recommendation I want to
give you is to avoid the finalization
it's true that you can rely on
finalization to free up your resources
but you don't know when it's going to
happen and in fact not only do you not
know when it's going to happen there
actually might be some weird scenarios
that happen with finalization for
example since finalization happens on a
different thread if you access other
objects in your finalizer you might
create concurrency problems or there are
some other weird scenarios where you can
somehow resurrect objects in your
finalizer or you could accidentally
finalize an object which is being used
that's crazy so as much as you can I
know it's not always possible but if you
can implement a disposable and call
dispose manually so that was my final
recommendation to you and that's that
I hope you learned new ways to optimize
your your applications today I know I
certainly enjoyed timing all that stuff
the results were surprising at times for
sure
now just one final note I know that the
differences in the performance seemed
huge but that's because all of my
examples were specifically tailored for
memory allocation and for the points I
was trying to prove obviously not all of
your applications are a hundred percent
about memory allocation and all of all
the some of the stuff that I showed my
major code less clear less maintainable
so my final recommendation really is
detect the critical code path in your
code you know how to measure performance
you know how to how to profile your
applications detect those critical code
paths and optimize them if you have a
plan node graph keep it in its reference
ignorant bliss and you will define thank
you so much for coming I hope you
enjoyed the rest of the MVC tomorrow and
almost on time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>