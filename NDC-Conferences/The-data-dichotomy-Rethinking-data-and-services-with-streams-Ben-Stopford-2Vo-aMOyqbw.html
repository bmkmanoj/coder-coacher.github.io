<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The data dichotomy: Rethinking data and services with streams - Ben Stopford | Coder Coacher - Coaching Coders</title><meta content="The data dichotomy: Rethinking data and services with streams - Ben Stopford - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The data dichotomy: Rethinking data and services with streams - Ben Stopford</b></h2><h5 class="post__date">2018-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2Vo-aMOyqbw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay well we'll kick off thank you all
for coming and it's really nice to be ya
and had the opportunity to speak at this
conference before but it seems pretty
awesome good hi my name's Ben Stopford I
work at this company called complement
so that's the company of that basically
sits behind a patch of Kafka which is an
open-source streaming platform whatever
be talking a little bit about that today
on and off and today really gonna talk
about the intersection between two
different worlds and the world of kind
of business systems micro services that
sort of thing and the world of stream
processing which really come kind of
comes from a very different area or a
different stance with a different set of
requirements and but the two when
combined are pretty interesting
hopefully you'll see that today so when
we build software on a kind of daily
basis we tend to have a couple of
different masters there's kind of this
very obvious master which is that we
want to build features you know so
you're you're you're rare users come to
you and they want they say you want to
build a report or maybe put something on
a screen these kind of things are very
visceral they're very obvious this is
why you go and sit at your desk in the
morning and kind of write code but at
the same time you also tend to think
maybe more subtly about some other goals
things like things that make your
software work not just today but also in
the future when we think about software
lasting overtime we're not really
thinking about those files sitting in
github on it on a file system well
obviously they could sit there
indefinitely we're thinking about their
ability to evolve over time and we
actually do this intuitively all the
time every time you write software
you're thinking about you know carefully
modularizing your code so that it can be
reused making it understandable so that
when you come back to it later you can
actually work out what what it was
supposed to do likewise you'll do things
like write tests
I'm said whilst you might write tests to
assert the behavior today really the
value of testing comes in the future its
protect its future proofing your
software same is true for continuous
delivery continuous integration all of
these practices are about kind of
thinking about and preparing for the
future but when we look we think about
software architecture putting different
pieces together to create a system um we
actually don't tend to think about inks
in the same way when we don't always
think about it in the same way so I
think for me a lot of this this talk is
about trying to work out how to build
systems that last their systems that we
can actually evolve in the same way that
we evolve our code in our code base it's
very much about this idea of evolution
software and the systems we are right
evolving over time so to take a slightly
different tack I wanted to talk a bit
about stream processing anyone do stream
processing today got a few okay so very
high level this is kind of an example of
a stream processing system so you
probably all have a mobile phone in your
pocket
maybe it's an Apple one or a Google one
or whatever and this that's kind of a
sort of canonical stream processing use
case and the reason is is it's kind of
like a billion phones out there and
they're all transmitting messages back
to your various different providers or
applications that you're using so if
you're like Apple or Google you'll be
recording like every time somebody hope
it's an application or every time the
application crashes and these are this
creates a very sort of high throughput
workload of actually very small messages
right lots and lots of small messages
coming from lots of different handsets
around the world so how do you kind of
deal with that kind of problem well you
probably use something like a streaming
platform so you'll put the data into
Kafka so Kafka is providing a buffer
which is going to buffer all that
information up and then the stream
processing layer is something which
allows you to perform a computation as
these events arise so a few if it's
let's say we want to do something very
simple like work out some statistics for
how much how many how many people are
using a certain application we should
sort of put those into HDFS or maybe
some other file system and then every so
often we could kind of run a batch job
over that and do the count the
computation that we want to do but
typically we don't want to do those kind
of batch computations because they're so
slow you'll get a result maybe you want
a day for these kind of workloads so
streaming is really about the idea that
we want to go to compute those kind of
results as continuously as they happen
and when you um the tool set that you
use to address this kind of problem is
slightly different so has a few elements
first it has a high throughput messaging
system you need something that's gonna
scale out and something which is going
to provide your high of high
availability the streaming layer itself
is optimized to deal with this kind of
problem and that means an area needs to
better to compute incrementally but also
it needs to map manage state one of the
things that it actually does is it gives
you the opportunity to create stateful
stateful stream processing nodes so that
you can localize the state inside the
stream processing node so that it can
operate very efficiently and then
finally you're typically in this kind of
architecture you have some kind of
serving there which you're attached a
GUI to and query directly you can
actually query a stream processor
directly and if you want to also kept
your streams for example for provides
the interface but here I've used
Cassandra which is another pattern it's
pretty common so let's take a little
example imagine we have a number of
users around the world who were just
using their credit cards and every time
you use your credit card you get an
authorization attempt and we're
interested in finding potentially
fraudulent activity so we have a in
input into this particular computation
is authorization attempts people using
their card and what comes out is this
potential a potential for a fraud and we
can do that very simply by at a very
simple level by counting the number of
temps in some particular amount of time
and this is expressed as something
called K sequel
looks a bit like sequel in a database as
you can tell except it runs on a
streaming stream processing engine so we
can start with our input a stream of
authorization attempts from there we're
going to create an output stream of
potential fraud possible fraud fortunate
events then we're gonna select the card
number and then the count and
importantly this is kind of where it
differs from your standard database
sequel we're gonna do that over in this
case a 5-minute window so that means
that we're gonna get a rolling average
over that a rolling computation over
that 5-minute window and then we're
gonna add a group by the card number and
then we're gonna filter that based on a
count of more than three it's a very
very simple computation which is going
to run continuously and that's kind of
like a very canonical stream processing
use case the stream processing really is
a tool for dealing with data in flight
so specifically manipulating it joining
different streams together and then
adding computation on top of that this
is what's this got to do with business
business applications you might wonder
well today we tend to build ecosystems
increasingly we build these kind of a
key ecosystems so subber has been around
for a long time well over a decade now
micro services which are in many ways a
reinvention of sowe're in a in the
context of a really if evolution of
hardware and software same kind of
principles and then things like
event-driven architectures but in all of
these what we're really doing is we're
compartmentalizing our problem into
specific concerns which match our domain
so here I'm gonna keep using this retail
context throughout this presentation so
we have an order service and we have a
customer service and it's a customer
service were obviously hold our record
of customers and the stock service will
hold our inventory of what's inside bar
our warehouse and each of these services
has a specific responsibility should be
pretty obvious
so the the problem with these kind of
architectures tends to be around data
none of those questions are actually
particularly in pinyin ated around data
like so it was always just like well if
you need some information you just go to
a service and get it and that kind of
makes sense for a certain number of use
cases but anything that's more data
centric actually becomes quite difficult
so you can imagine even in a retail
context I have a set of these different
services these are kind of is going to
form into these islands which in many
ways makes sense but whole variety of
these different services are going to
need access to these core data sets so
they're going to need to have some
information and order information and
the product catalog in order to do that
work so where this becomes a little bit
more tricky is when we start to have
slightly more data in terms of
operations right so if we're doing
something like really simple like
displaying something on a dashboard just
like a lookup table it's pretty simple
but as soon as we do something is a
little bit more data intensive let's say
we just have like a grid where you can
scroll up and down and in the background
you're hitting a variety of different
services you have to scroll that fast
before hitting all of these different
services particularly if you're doing a
kind of join which involves accessing
maybe orders and then payments and then
customers becomes actually quite
difficult to manage you have to sort of
thinking about batching and then caching
things locally and then polling for
their bit but for new updates etc etc so
these kind of slightly more data
intensive operations turn out to be
quite difficult to perform so one very
simple solution is just to have one big
database actually a lot of companies do
this quite a lot of companies have a
have some kind of central operational
store um it's a tricky person to get
right
obviously intuitively it's son of quite
nice it's like I've got all my services
I can just put all my data in the
database everyone can access it it
sounds like it's gonna make a lot of
sense in a static sense like today that
will probably work but what the pain
that comes with a shared database is
actually it's is this ever
thing as each of these services changes
over time because they are independently
deployable they typically typically
gonna be run by different teams the
database provides this-this-this
relatively high coupling point which
makes it difficult to evolve so
databases provide this kind of very rich
form of coupling so what this kind of
leads to is two different forces which
compete in the design of the software
that we build so on one side we're
taught to encapsulate all done this
right all done all kind of oo modeling
etc etc and this makes a lot of sense so
let's say let's say we have a single
sign-on service it has a an interface
nice and neat interface could authorize
and I have some business service which
calls that I use encapsulation to hide
the state and behavior this sits inside
my service makes a lot of sense nice
clean interface not too much for people
to couple to I can basically change the
internals of that without it being too
much of a problem all kind of makes a
lot of sense loose coupling is good
problem with data systems is they don't
really have anything to do with
encapsulation in fact they do exactly
the opposite a database does everything
it can to make it the data that it holds
as easy for you to get at as possible
actually provides this wonderful wealth
of tooling which allows you to slice and
dice data in a variety different ways in
fact you can awfully you know you can
reform it in two different views you can
make it look completely different to the
core data set it has it's literally the
antithesis of encapsulation so if you
can imagine like a service on the
left-hand side it has babel on the
inside and it's doing its best to
encapsulate that to not expose it to the
outside world and the reason it's doing
that is because it wants to be able to
change it wants to control over that and
it was to limit the interface that
exposes but a database is exactly the
opposite and has some data reside it
actually provides amplifying contracts
which is richer than me it than the data
that it holds internally so you kind of
get this competition
between these two different forces one
side encapsulation is encouraging you to
like limit your interface so you reduce
coupling but then you have these data
systems which are actually amplifying
providing an amplifying interface and
that's not to say data systems are bad
we need data systems we need these
amplifying interfaces because they do
useful work they allow us to introspect
data to build different types of data
centric problems for our users so we
actually need both of these things but
we have to understand how they sit in
our in in the systems that we built so
this leads to this kind of dichotomy one
side data systems are very much about
exposing data and that allows you to get
on with your job today
whereas in both services are really
about hiding it which is about
protecting yourself against potential
coupling in the future which allows your
system to evolve so these two forces
basically fight against one another and
the systems that we built so it said
quite sensibly really actually mostly
from experience and the microservices
shouldn't share a database who's heard
this show hands oh very knowledgeable
crowd I like it so Sam Lehman talks
about it in his book it's talked about
in the incest all over the place I'm not
going to go into it in too much detail
other than what I've said already but
suffice to say that it's a sensible path
the pattern and at the heart of it is
this is this tension so we want all the
good stuff that comes with the database
and bility to slice and dice data but we
don't particularly want to share our
database with anyone else kind of know
that right we'll kind of somebody says
how can I have access to your database
you're like are you gonna couple
yourself to it I'm not sure it'll really
want you to do that but you also need to
share datasets in a sensible way so we
need some mechanism of doing it okay so
that's all pretty abstract let's try and
take a little example I'm gonna keep
using as I said this retail example so
how do we share the
between services today well that's
really strange sorry my picture looks
different to that picture so we have a
web server and we have an order to
service a shipping service and a
customer service and this is a very
simple use case so you can imagine this
could be Amazon and where you're buying
whatever you walk bored of life Amazon
or maybe Apple you're buying an iPad and
you have a web server and you order
service gonna manage the orders the
customer service is gonna hold your
customer of information and the shipping
service is the thing that's gonna
basically make sure that that iPad makes
it from the warehouse all the way over
to your door and you can right then go
off and enjoy your new Apple product so
let's say we do this with rest if we do
it with rest then the web server will
submit an order and that will go to the
order service and the shipping service
would expose an interface called ship
order and so the order service will call
the shipping service and then it would
need to look up need to know where to
send your iPad to so it would make a
call to the customer service get
customer get your address and then the
shipping process would continue that's
all pretty simple intuitive why would I
want anything else well we can do this
with an event-driven model and this has
some slightly different trade-offs
involved what you're worth looking at so
again we submit the order but in a pure
event-driven model we don't services
don't actually talk to one another and
this is gonna make sense in a minute so
in a pure event-driven model we just
talk events so the order service
actually raises an event that says this
order was created or requested um yeah
she doesn't know where it's going it's
going to the shipping service but it
doesn't know that the shipping service
decides to describe subscribes to these
these are order events because it knows
it's in its interested in them and it
continues to kind of off to do its
processing because it's triggered by the
notification that sits inside this event
on the other side we have this customer
service
and customers information is a little
bit different because it's not about
things that are happening now it's about
things that happened in the past like
the last time you updated your customer
information was probably the last time
you moved house which was maybe a couple
of years ago so the customer event is
not something that we can analyze in
real time but this kind of what this
this these two sides of this particular
problem expose are that really events
have two hats so an event is a
notification it's a call to action all
right so this event happened this
business event happened do you want to
do something about it but at the same
time inside that event is the
information itself so it's also state
transfer there's a way of moving data
from one service to another so in this
case you can actually use the customer
service to omit customer service can
omit events and those events can be
collected over time inside a database
inside the shipping service which it
could then query locally so um the core
point here is that we can use either of
these patterns and we can mix them up as
we see fit so we can just use events the
notification in which case the order
service comes in it raises that event
and the shipping service which is doubly
decoupled from the order service because
the order service doesn't know anything
about the shipping service now I can
just react to it but it can still maybe
make a rest call to look at your
customer information directly to the
customer service alternatively we can
use this events for replication process
we're still actually using the
notification for the order service but
we can use the fact that events actually
allow state to move to recreate the
customer data set inside a database
inside the shipping service so actually
using data sets for events for locality
in this case this is useful for a few
different reasons one is it's actually
going to make it much faster for the
shipping service service to potentially
access that data but
also this is particularly useful if
you're moving over let's say a different
geography or maybe you're running on a
mobile phone etc etc so events have two
hats on one side their notification but
on the other side they can be used for
data replication so events are a key to
scalable service ecosystems because
they're actually decoupling the various
different components inside them and
streaming is a toolset which allows you
to deal with these events as they move
so let's take a quick look at what a
streaming platform is it has a few
different components it has very simple
interfaces producers and consumers which
you can access in a pretty much any
language you can think of we have a
bunch of of clients which we support in
net and go etcetera etcetera and there's
a whole bunch of community ones for
Kafka there are connectors which
basically just allow you to get in and
stay tuned out of a database and then
there's this stream processing API and
which is literally just another API but
actually provides this very sort of much
more functional interface for dealing
with events and a synchronicity let's
look at these quickly
we'll start with the log so Kafka is a
distributed log that's actually the
project I work on on a day to day basis
and you can just think of it a bit like
a messaging system think of it being
really being designed for that kind of
use case that we talked about at the
start the one we had the mobile phones
missing so we want to store and process
large numbers of small events and the
reason that use case works so well is
that it has no there is literally no
bottleneck within the system I can have
a very very big topic I can shard data
on the way in it can be held on a whole
bunch of different machines where it's
actually how it ranged is different
queues and we can arrange how data is is
sharded to arrange for strong ordering
guarantees and then many different
consuming processes can share that one
topic and process them at the same time
but some other quite nice properties so
the log can actually be used as a
storage system can hold it mean you can
put hundreds of terabytes into Kafka no
problem
even with compacted topics for sugar and
to talk about in a minute when the nice
properties of this is that you can
actually then rewind the log so you can
store these events which holds
everything matter it's like every order
that ever came in or every customer that
ever came in and you can rewind those
events and replay so that's really
useful for this pattern called event
sourcing sometimes and we're going to be
talking more about this this idea of
moving data around and when you move
data around a whole datasets around you
actually want to make them smaller and
the way you make it smaller is actually
to make a log which is every single of
change that ever happened like like a
version history you know and get you
want to collapse that just to the latest
version which basically looks like a
database table so you collapse it by key
come that's what a compacted topic does
so it allows you can basically it will
compact that data sit down so you just
have the latest value for each key and
it effectively would marry exactly with
the database table so um that's the log
we have these connectors if you've used
yes bees before this is actually pretty
similar pattern it's a mechanism that
allows you to get data out of a whole
variety of different databases and
getting very quickly into Kafka or
alternately pulled there out of Kafka
and put it into pretty much any database
that you you choose please don't use
catechol like in the ESB you can some
people do but it's typically not a great
pattern but these are these tools are
useful nonetheless and then finally does
this this idea of the streaming engine
which is in some ways a little bit
harder to understand but it's actually
pretty useful so there's a whole bunch
of streaming engines out there I'm gonna
talk about the one that literally ships
with caf-co which is called K streams
and this is basically they're all
roughly equivalent in many ways although
you'll see each have their own sweet
spot but effectively they're a database
engine for data that
moving allows you to basically manage
data as events and actually importantly
marry the concept of events and tables
so a very high level it's continuously
running query the query that we started
with where we had a bunch of credit card
people using credit card authorization
attempts and we're constantly computed
in this query and outputting a result to
another stream that's the kind of
canonical use case so much like a
database you can do joined you can
create views which is very useful for
mapping whatever the data model is on
the wire to your internal domain model
before you start adding sort of further
predicate so aggregates etc you can
filter you can aggregate and all over
you can add a window which is how you
deal with a infinite string so how you
reason about an infinite string but one
of the things that I think most powerful
about the streaming model certainly the
straight stateful stream processing
engines is their ability to deal with
both streams and tables the reason that
this is it is important is while several
a lot I like a lot of use cases which
fit perfectly into pure streaming most
use cases are some kind of combination
of the two so you have like an event
stream that you want to react to but you
also want to kind of look something up
if you want to look up customer
information whilst you're processing a
stream of orders so stream processing
engines let you basically manage these
two things I can have a topic which is
represents an infinite stream and I can
make a reference to a table as I go
along so the other nice thing about
stream processing engine is it handles a
synchronicity it does all this for you
so let's take a little example we have
imagine we have two services order
service and payment service on the left
hand side the orders service and they're
both are certain submitting events and
you can Majan like when you create an
order like you're buying an iPad your
message will be sent to the order
service and you also make a message to
the payment service and there's probably
some ordering around that whichever way
you did it it probably
create the payment and then you create
the order but actually you can't reason
about that in an asynchronous system
they could turn up in any order
because the whole process is
asynchronous one might be delayed for
whatever reason you could get a race
condition so if we want to write a very
simple program that just sends the user
an email to say congratulations you
bought a new iPad it cost you this much
etc etc we need to join these two things
together and the way that works is the
email that can basically subscribed
using the caffeine streams API or
whatever you're using can subscribe to
these two different streams and what
this stream processing API will actually
do is it will buffer for some amount of
time that you configure and/or buffer so
that you can do that join and not have
to worry about which event came first
handles that asynchronicity for you so
when the payment and the order event are
both present then your email code will
be triggered and you can send email so
that's kind of how you buffer and join
streams it's very simple so you can do
this in case equal if you're off the JVM
if you're on the JVM you can use the
clavicle streams API it's basically a
few lines of code create stream the the
names in quotes are the topic names and
then the join is just obviously a couple
of lines and we're just calling that
email service that email function to
send an email but this table thing as I
said it's pretty interesting and what is
a table really well firstly I think of
the use case so we're adding let's add
the customer service in so let's say we
need the email needs to know where to
send or email your email is on your
customer information which is located
inside the customer service and we
wanted to kind of do this in a streaming
way what we'd actually do is create a
table of customers inside her email and
what a table is is actually the same
thing it's the event stream which is
held inside Casca its materialized
locally but instead of just retaining
maybe five minutes of data you retain
the whole dataset and you actually use
one of those compacted topics we talked
about earlier
do this so you don't have to move too
much data and often also filter it so a
table is really just a stream with
infinite pretension again very simple to
do is one line of code instead of
builder dot stream it's build a table
and the API deals with all the rest of
it and then when we do our join its
effectively exactly the same thing again
we use a slightly we use a to pool this
time instead of a key value but
effective we're just gonna get if the
emailer will be called every time the
order the payment and the customer
information is all available
so what really doing is materializing a
kind of view streaming view which our
code is reacting to so we can also do
this I said with case equal with case
equal you were basically have a
separation so the the case equal engine
you just run as a separate node or a
sidecar and that does the computation
create an output the stream which would
then drive the the emailer and I might
write an email note of C sharp or
whatever I like so because of this has
got a background and stream processing
we can also scale these things out we
can have high availability of our
services one goes down the other one's
just gonna naturally take over all of
that good stuff just falls out of the
out of the box so really if we step back
a little bit stream processing is about
two things it's about processing data
incrementally relaxing two events so
that's their notification hat of an
event but it's also about moving data to
wherever it's needed that tool also and
that's really that data replication hat
that you get with events so we can
localize the datasets that we can
process it quickly or actually also as
we'll see because that gives us autonomy
which allows us to evolve independently
of the rest of the ecosystem
so let's walk through some strep steps
to building a more realistic slightly
more realistic streaming ecosystem with
these tools there's ten ten steps here
to building this little ecosystem first
one is basically to take responsibility
for the past it's actually pretty
unusual that you see someone start a
completely greenfield project normally
if you're completely greenfield you're a
startup in which case you should
probably build building a monolith
anyway because you need to get something
out the door quickly and microservices
and other the distributed patents do
come at a cost
so you'll typically have something if
you're working a big company there'll be
some legacy somewhere they start with a
very simple monolithic application three
tiers and let's say we evolve forwards
so we're gonna split out an order
service this is kind of our path to try
and extend our architecture so it's it
has different components which are
independently deployable so one of the
things we kind of want to aim for in
this model is having an asynchronous
call and we want to have synchronous
interfaces on edge services so
internally we're gonna raise events
rather than talking directly to services
because it allows the different services
to remain decoupled so instead of
talking directly to the order service we
can making a request through a broker so
the web server can send an order
received event and I guess picked up by
the order service and then the order
service would then validate the order
and then create an order validated event
which we picked up back by the web
server
next we can use the connects interface
typically using CDC though although we
don't have to to evolve away from some
legacy system so really this this tends
to be pretty useful because you
typically have a database which is
already filled with like useful
information and a lot of that you just
need to get into this stream processing
pattern and so we can attach oops
we've so we can attach a connect
interface on to the back of the database
I mention that term CDC there are a
number of connectors which implement
this things will change data capture and
there are a number which just basically
go through a front door or for some of
the no sequel databases do something
that's in between but CDC is actually a
really efficient way of getting data out
of a database so a database at its core
on has a transaction log which records
basically it's much very similar to a
log in Kafka it should record every
single change that it ever does a CDC
base it just like tails that log and
turns it into events so when you're
picking a seem to be when you have a
legacy application you need to pick a
sensible scene where you can kind of
like carve the workflow away I actually
see people do it in a data layer quite a
lot but actually a really good way to do
this which a lot of people use this just
to pick a seam that's inside the
database itself so you literally just
turn a database table into an event
stream so in this case we've pulled the
data set of products which maybe we
don't want to build a service for right
now we're quite happy maintaining
products inside of the legacy
application we've pulled that into Kafka
and then we can start to use that
Product data set in our evolving
ecosystem so next we want to make use of
schemas schemas are very important where
you have independently deployable
services so if you just like have one
deploy when one deployable unit is just
one app you can often get away without
using these but really schemas are the
API for an event-driven system the thing
that described the contract
so if I have an ecosystem where I have
independently deployable surfaces then I
have to I need some way to kind of
reason about this contract or this event
so you want to use some kind of schema
to describe the messages to flow and
really to allow you to kind of evolve
that over time so if I if I attach a new
service this ecosystem I want to know
that that data set is always going to be
accurately compatible so cathica ships
with a thing called schema registry in
the conflict distribution or you can use
your own so you schemas next is rice is
applying the single writer principle
it's the reason you need to do this is
that in this kind of model you have no
global consistency point we didn't use a
big share database which provides a
global global consistency point so we
need some other way to reason about
consistency and ordering and a good way
to do this is with it so basically
assign responsibility for effectively
what is it what actually is a mutation
logically it's mutation to a single
service so the order service in this
case is responsible for progressing the
workflow of an order forwards in time so
no one else is able to do that writing
and this gives us a word is effectively
a kind of causal consistency model so as
assert the service on the left creates
an events it kind of flows through CAF
curve the validation occurs inside the
order service the order service creates
a new record said it's validated and
maybe that flows out to this email
service the important point here is that
actually each of these services is
separated in time actually disconnected
in time but the event flow because it is
record by record in an ordered queue is
each each of these services basically is
a consistent view of changes as they
happen
so this gives an element of slack in the
system but at the same time allows you
to manage consistency and we'll give an
example of that in a little bit so a
single rices does a couple of things
firstly it creates a local consistency
point in the absence of global
consistency also it it actually makes it
much easier to do things like change
schemas later
if you have seven different services
changing a particular event stream it's
actually quite hard to to upgrade that
schema because each of them ice to kind
of go as a single the upgrade tends to
have to happen as a single release the
six we can store data sets inside the
log so as I said Kathy I can basically
store data indefinitely anybody can use
these compacted topics to to create a
more like tabular like view so what that
really means is that we can externalise
these data sets so these are shared data
sets which different services need to
use inside the log and that gives us a
nice plug ability to the architecture
let's say we want to build a new service
we're doing repricing so we might make
use of an existing event stream and we
might also make need to make reference
to the product data set and we can join
these two things together inside a new
pricing service and obviously because
it's an adventure of a model we can just
plug this right in so in this case the
event stream would be something we would
react to but the products
would be an entire data set which we
were just materialized locally either in
a database or inside the stream
processing engine as we see fit so it's
very easy to plug new services into this
kind of model it's kind of like a real
advantage to this type of approach and
then we can evolve this forward so we
start to create this kind of single
source of truth so again we don't really
have a shared database we have any
shared functionality for analyzing data
we're really just sharing these data
sets as streams there's things where we
react to or as datasets which of which
we can take in their entirety how do you
actually query a log obviously do need
to do these to run queries against the
data at their stores there it's stored
in there that's what we actually start
moving data to code and this should be a
little bit unintuitive for a long time
we have not done this for a long time we
have always moved code to data and that
made a lot of sense right they just
heavy code slides which should obviously
move a codes to data that's kind of the
database mantra one of the problems
we're taking that mantra is that if you
continue doing it forever you would
basically build all of your systems
inside a database that would be the
logical conclusion right we have a big
ecosystem made by Oracle and we would
all write our code inside that and
typically we don't want to do that other
than the fact that you probably don't
want to write business code in sequel
you even if you did you probably won't
want to maintain it inside the database
because it'll be really hard to to
manage that right you need to be on a
managed code yourself so you have the
freedom to pick the tools that you want
to use and have independent deployment
cycles so the one of the nice things
about the streaming approach is it gives
you this flexibility you can of course
move code to data you can also move data
to code so let's see how that fits into
our little evolving ecosystem it's in
this case we might have the stock
information so what what they - what
items we have in the warehouse and the
order service is going to need to do
some validation one of the things is
going to
do is validate to work out whether or
not there are enough iPads in stock for
you to fulfill your order so we can
materialize that data set inside the
order service so that it can query it
locally and importantly this gives the
order service autonomy over its data set
so we can manage its own release cycle
the second trick is because the data
sets actually stored inside kafka we
don't need to absorb the whole thing
into the order service we actually only
take the data that we need this is
actually pretty useful so to validate
have whether you have enough iPads in
stock you need two pieces of information
you need to know the product code of the
iPad and you need to know how many items
there are in the warehouse
so you basically got a key and what is
effectively an integer or if your Amazon
is probably a long so this idea of data
movement analysis optimize the local
locality but it also gives our services
autonomy because they're internalizing
that data set where they have control
but the sites on the cert on the second
side on the other side we need to also
be realistic about this actually moving
data over the network isn't typically
the bottom net you've got a distributed
log behind you you can move this that's
really fast you can dis rate your
services over many different machines
they actually the thing that tends to be
the limiting point is indexing the cost
of indexing is actually relatively high
so if you use a database which has got a
using a large number of indexes like
something like elastic search is pretty
slow to update that tends to be the
limiting factor so we can get around
this by either making indexes in leaving
indexes in memory or scaling out several
multiple machines or by this trick of
keeping data sets relatively focused
because we can always go back and get
more if we need it so using the log as a
database
this basically means event sourcing but
we can actually use whilst we were so we
pulled this stock data set into the
order service but if you order service
just wants to say lightweight
and doesn't actually want to have an
external database we can actually use
Kafka as a kind of database so in this
case let's say the order service it
knows what's in your warehouse it also
needs to reserve an iPad all right so
the bunch of iPads in the in the
warehouse bill that number would go down
as iPads were shipped out but as soon as
somebody creates an order and that order
is validated we need to record that that
iPad is effectively reserved for the
duration of the order so we need to kind
of write some intermediary state so we
can do that literally just by writing we
actually write to the table that's
actually the way that you do it create a
table and you write to that table and it
will be backed up in Kafka oh you could
obviously just write directly to the
stream if you preferred now if you do
this then what actually happens is the
data set stored in Kafka and when the
order service starts it loads they load
that's a data set that we've created of
each stock that we've reserved into the
order service and in actually still
Kafka streams implements a thing called
a state store which it's basically just
it's a type of database of sorts that
really is much simpler it's ER it's a
disk resident hash table which allows
you to basically store these data sets
and access them very quickly so because
we're moving data around to services is
a relatively difficult thing to do and
one of the things we need to do is make
sure that we can do this in a reliable
way
and stream processing engines ship with
a bunch of features which basically make
this this problem practical because
they're really optimized to solve this
problem so there's a few things that
stand by replicas which is effectively
means that you have a replica of that of
any data set in one instance of a
service in a secondary node and they
effectively run it as a highly available
pair or as many as you need and if one
goes down where the work event will
immediately fall over to the other one
and the state will be there ready for
you
likewise there's Dix dis checkpoints
which is exactly what you think you
might think it's during this data to
disk if for some reason if service goes
down and comes back up again it can
access the disk checkpoint it'll be a
little bit behind so it would just catch
up from the log to get itself back to a
consistent state and we talked earlier
about compacted topics which are really
about reducing the amount of data that
we need to move around so nearly there
number nine is using transactions to tie
it all together so transaction is
actually pretty interesting in Kafka I
think they're in some ways a bit like a
transaction in a database but in some
ways not so yeah it's kind of the same
it's about having a tortoise 'ti so
let's say we're gonna keep looking at
that orders service we have that order
requested event which comes in somebody
orders an iPad when that happens we
actually need to do three things I mean
do those three things atomically first
thing we need to commit are offset
that's a internal Kafka thing but that's
basically ensures that you don't get the
event again it's just saying I've got
that event and I'm done done with it
thank you very much
let me need to do two other things we
need to crate we need to once the order
is validated we would create the order
validated event but we also want to
record that the iPad has been reserved
in each of these are going to be
basically rights to different topics so
this is actually right the right of an
event which we're using as an event
stream this is actually much more like a
right to a database because it's
internal to our service and all of these
three things we can wrap up in a
transaction this actually this just
happens automatically if you use Kafka
streams you just enable transactions
it's all atomic and obviously because
it's a stream processing system it's
designed to run this scale there's no
it's like this is not like an EXO
transaction which sits for several
milliseconds this will work at big data
volume no problem
so transactions allow us to tie this
these kind of business operations inside
this service which with where we've
centralized these consistency control
concern
using the single rider principle allows
us to update various different pieces of
state and events atomically it's quite a
powerful idea and finally we bridge this
synchronous asynchronous divide in a
streaming media system so I'm not gonna
this this is actually a more complicated
version of that one service we've been
talking about so far I'm just going to
talk about it briefly so what we have
here is on the left-hand side we have a
very simple vessel interface where we
can post an order and we can request an
order to get it back by its key and what
we're actually implementing the pattern
inside here is CQRS so what soon as that
order gets created it it it's your order
event this critter's is written to Kafka
and split out into a set of different
workflows so we have three validation
engine just a fraud service an order
service and in the venturi service this
was the one that we were talking about a
lot today and each of those can run in
parallel and can perform its appropriate
validation creates another event which
goes back to the orders service this is
all a synchronous this effectively dams
those streams these streams all flowing
constantly the Ord service here the C
cross the query side of CQRS
is effectively damming the stream and
creating something that we can query and
joins the whole thing together
so that's kind of a very simple example
of how we can build a streaming
ecosystem which bridges a request
response synchronous interface with
something which is effectively a
synchronous asynchronous core and a
synchronous interface I should also say
that that's that's um so the code for
this is is online I have a link for it
at the end and there's a post that
describes how it all works but this is
this is a sort of a test case as it were
now you can build an ecosystem this type
of ecosystem there's a number of systems
I've seen that use this type of pattern
definitely a pattern which is very
scalable right so you probably wouldn't
need to use this kind of pattern for an
order service if you were just building
a small website but it's really working
of something with something that
requires well scale this is the kind of
person that you would use but what's
really more important about this is it
kind of shows the seed for how you build
more complicated ecosystems and the
answer is to have as when it come to
design these things the question you're
asking is what needs to actually scale
on what needs to be independently
deployable so in this instance it was
actually very easy for these different
services to be independently deployable
your question would be for yourself is
do I need that so what this model really
is is a it's a sink or model so at the
core of the system is asynchronous and
we having we have synchronous interfaces
on the boundaries and as we scale that
out and keep these historical event
streams the event stream has become a
kind of system of record so this is this
idea of a database inside out anyone
seen Martine Clements talk on our
databases that out it's a really good
talk I think there's a link for it in
the Associated blog post but the
interesting thing about this is it's the
stream actually behaves a lot like a
database in that it stores data sets for
you but each of our services is able to
embed the processing that it needs to
slice and dice those those those streams
so each of these services share a set of
data set I have a shared data set but
they own how that data is sliced is
sliced and diced which really brings us
back to that beta of dichotomy allows us
to share data but retain control over
how it sliced and diced so spit like a
database but like inside out
and I'll see we can scale this out as we
scale this out we cannot add more
services we can span different
geographies we might have a set of
services running in New York and London
and Tokyo etc but these more globally
disconnected ecosystems so what do we
have
I'm at a high level for me architecture
actually has little to do with writing
something on a whiteboard it's quite
easy to design a system on a whiteboard
it's much harder to work out how that
systems going to evolve over time
actually because it's not just a
technical problem it's got a lot to do
with people so for me the essence of a
good design is its ability to evolve
over time and actually that's just true
for your software as it is for a system
architecture so the question of and
interfaces are typically isn't enough
not for a big ecosystem we want to
embrace a synchronicity often because we
actually have business processes which
themselves are asynchronous like
shipping an item to your door but also
because synchronous interface is a
particular when they get deep actually
provide a lot of coupling there's almost
like a law of demeter if you've come
across that for the way that the
system's interact so we want to leverage
the duality of events both the note the
notification as well as the ability to
move data from process to process and we
can use a streaming toolset help us to
solve those kinds of problems to deal
with a synchronicity
actually slice and de at dice data as it
moves so remember the data dichotomy
data systems really about exposing data
these services are about hiding it that
means we want all the good stuff that we
get with a database but we don't want to
share that database with anyone else we
do want to share the daily sets that
they hold so want to be I shared the
data set but we want to manage the
slicing and dicing ourselves
and have autonomy over web weather how
that evolves over time when that changes
so for event-driven services we want to
broadcast events retain them in a log
compose a system as a set of streaming
functions and then we cast that event
stream into a view when we need to query
it and that's the essence of services
built on this kind of streaming platform
and that's all I had to say thanks very
much for listening again my name's Ben
stuff but if you want to find out more
the slides are available on my website
there's a blog series on the confluent
blog and that's closed for those those
code examples in that URL and yeah you
can find all this on my website if
anyone anyone has any questions not sure
if we have a microphone here but I'm
happy to say now it would also be happy
to answer anything that people have any
questions people have at the end and
will catch me around and as you see fit</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>