<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microtesting - Matt Davies and Rob Moore | Coder Coacher - Coaching Coders</title><meta content="Microtesting - Matt Davies and Rob Moore - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microtesting - Matt Davies and Rob Moore</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pls1Vk_bw_Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright let's get started good afternoon
everyone my name's Rob Moore my name is
Matt Davis and we're principal
consultants at ratified so they're gonna
be talking to you about something that
we call micro testing now the easiest
way for us or probably the simplest way
for us to describe what it is is the
tagline here so we're trying to be a bit
smarter about how we test by embracing
simplicity and continuous improvement at
least we thought that was the easiest
way but earlier I was talking to Andrew
best there and he described it to me as
pragmatic testing so let's run with that
yeah so yep a fair number of you are
probably wondering what's micro testing
why did you call it that because we made
it up it has a completely made-up term
we came up with it last year when we put
this talk forward for a yo conference at
that time typically even now if you look
at the agenda for NDC there's a lot of
talks about micro services we kind of
just wanted to jump on the bandwagon but
a little bit more seriously we've been
finding with the sort of practices and
techniques I guess that we've been using
the approaches we've been using for
testing over the last couple of years
that we've been writing less tests and
spending less time testing but actually
getting more out of it and getting the
same or better confidence so it's kind
of a strangely appropriate title anyway
yeah and as consultants we get to see a
whole heap of different testing
strategies we get to see just about
everything under the Sun we get a lot of
opportunity to do research into this
stuff ourselves and we we get to play
around and try to write some of this
stuff ourselves and what we find just in
looking a lot of what's out there is
just like a complete garbage
there's awful awful testing and we've
written a lot of it we've written most
of it we can't make any excuses so what
we want to try and do as a part of this
talk is to try and share some of the
lessons learned try and chest and
practical advice and tips for how to do
some of this stuff in a better way yeah
and I guess leveraging some of the
experiences we've had over the last
couple of years and some of our
successes and failures and stuff yeah so
we've decided to break this talk up into
a number of chapters first chapter is
about confidence and you'll understand
why as we go through this chapter is
really easy and so we're basically going
to
you all about why the testing pyramid is
the answer to all of our questions it's
gonna say all the testing pyramid so
this is the testing pyramid you might
remember this from under your days in
university or your early days learning
about this stuff as a developer you've
got a relatively high ratio of unit
tests down the bottom a smaller ratio of
integration or service level tests above
that and an even smaller level of UI
level or user facing tests and to be
honest we completely fundamentally
disagree with this for a whole heap of
reasons so it makes us really sad so
we've got a sad face there the testing
pyramid mandates that you do everything
according to a very rigid structure it
says that you're going to have this many
of this type of tests this many of this
other type of tests and it forces you to
I guess try and figure out what these
like names for things even mean I mean
hands up if you've ever had an argument
with someone over whether or not a test
is a unit test or an integration test
right yeah wait that time that you spent
is a waste you could have been spending
that time delivering value rather than
arguing if something's a unit or an
integration test true yeah that's it
delivers value yeah and the other thing
to consider is that this testing pyramid
doesn't take into account the concept of
what we call speed versus confidence
absolutely so when we talk about speed
versus confidence what we mean is speed
is you've got some sort of test suite
how long does it take from starting the
test suite to when you find out
according to that suite of tests whether
or not there are any bugs in your
software that's the speed and then
confidence is once you've run that suite
of tests how confident are you that
there's a bug in your code or put
another way how confident are you to
click a button to push your code to
production how confident are you to let
your product don't push a button to go
to production absolutely so if we look
at trying to graph I guess speed versus
confidence and we look at say manual
testing or automated UI testing these
execute the full production stack of the
system on behind like a simulator like
what an end-user would do so you get a
lot of confidence from running these
types of tests but relatively speaking
they're very very slow if you compare
that to your traditional unit tests they
typically Iran in process in memory they
take a few milliseconds to
but the amount of confidence that those
tests give you is relatively very small
because you're only testing a tiny part
of your code and quite often you're
mocking out production implementation
details now if you take one of these
unit tests you add in some real data
base calls you've now got out of process
communication it's an order of magnitude
slower but there's a whole class of
errors that you will now pick up in your
tests around things like does the sequel
work are your data based migrations
correct
does your RM correctly hydrate your
entities etc etc now if we try to graph
these what we kind of finding out here
is this seems to be an inversely
proportional relationship between speed
and confidence now if we come back to
the testing pyramid there's nothing in
here about speed or confidence it's just
a very rigid structure and on that point
of a rigid structure the reason why
that's such a problem is pretty evident
when we I mean we always have a lot of
experience going between different
clients we see a lots of different
circumstances one of the things that we
notice is between different teams or
between different companies there's a
lot of scenarios where your approach to
testing should be very different so say
you've got the Juno rocket by NASA now
if someone's kind of like messed up some
code in that and then the rocket
explodes how many years have you wasted
right and that's why I'm gonna care if I
have to pump a few more million dollars
in to get really good test coverage on
that yeah and it absolutely makes sense
to do that in that scenario on the other
hand if you've got say like an admin
system that's used by like two people
who sit next to you it might be enough
to just have one you are test that just
sits over the whole thing and you just
kind of say that's enough if anything
goes wrong these people are just kind of
bare and available and it's actually not
much effort to sort that out horen no UI
test at all yeah and that's actually a
really good example the admin system of
an example where even within one system
different features in that system will
have different against confidence levels
that make sense and different I guess
amounts of testing that make sense so
admin system over here we don't need
much tests over here you've got a bank
with a mortgage application form if you
mess that up and users can't actually
fill that out you're potentially losing
millions of dollars you're going to put
a bit more rigor into the mortgage
application form now
Jimmy Bogart has a really good talk that
he did an NDC oslo 2013 called holistic
testing highly encourage you to watch it
he introduces this as his testing
pyramid in that talk now
I really like this so what he says is
you know have a high proportion of fast
tests a slightly slower proportion of
slow tests and even slower as slower
smaller proportion of slow as hell tests
this is good it takes us a step in the
right direction because at least now
we're talking about our speed of
feedback which is really important but
it still doesn't go far enough in our
opinion because it's still very rigid
and it doesn't talk about confidence hmm
so you might be wondering what's the
testing pyramid that we use what do I do
yeah oh there we go oh is the testing
pyramid it's supposed to be on this
slide is it under here Damian where's
our testing pyramid do you have it no so
is it hang on so make it burn basically
yeah so as we kind of said we completely
disagree with the idea of having a
testing pyramid it locks you into a
pretty rigid structure so let's just get
rid of it and let's come up with
something individualized based on what
we need to do so then you might be
wondering okay we've thrown out the
testing period how are we gonna decide
what kind of tests to write it was gonna
be lost they're gonna be walking around
in a daze of confusion yes yep
absolutely but I guess what it comes
back down to for us is what's the most
important thing to think about if you're
trying to decide cool this is a feature
I'm writing what kind of test should I
write
how should I test this should I even
test it at all etc cool so to start to
make a decision around some of these
things we've got a really good quote
here from Jimmy who from the same talk
that Rob mentioned and what he's
basically saying here is that the goal
of testing isn't actually to test its to
come up with a level of confidence that
our app is going to work it's just a
means to an end you can have all sorts
of different levels of code coverage but
you could have a hundred percent and it
doesn't necessarily mean that your apps
going to be successful it's it's
completely unrelated its confidence
that's important rather than a level of
code coverage so let's really drive that
home the goal of testing
it's not to test so if you're kind of
finding yourself writing a feature and
then just you just without thinking you
just start running a test because that's
what we do we just write tests does that
test give you any value you should stop
and think about that in our opinion the
goal of testing should be to get enough
confidence enough being a really
important word in that sentence as
quickly as possible because type
feedback loops are really good they
speed up our teams we know that deliver
developer velocity is typically
something most companies care a lot
about so that leads us on to talking
about speed of release I would make the
assertion that you're probably not going
to release your software until you have
some sort of minimum confidence level in
that software if you think that the
software is buggy in some way you're
probably not going to deploy it to
production it doesn't make any snow on
all your users to leave you right
exactly
so we've kind of come up with this
completely arbitrary graph looking at
the amount of automated test coverage
that you have versus speed of release
now I guess this is just based on what
we've seen in a number of companies that
we've mean - on one hand if you don't
have any automated test coverage at all
you probably got a team that has a fear
of refactoring and obviously that means
you're probably going to get technical
debt build-up and that's going to slow
you down
you're probably relying on a lot of
manual regression testing very slow
doesn't actually pick up all the bugs
and they're probably going to be a lot
of regression so you probably going to
be spending a lot of time fixing bugs
all of these things slow us down this
low down our speed of release but on the
other end of the scale what we often see
is people going the opposite and testing
everything to the nth degree and putting
these huge UI testing you know test
Suites frameworks that are really
brittle and give us low confidence and
have things like intermittently failing
tests or they will spend a lot of time
running tests we've been into companies
that have like 24 hour test runs that's
a crazy crazy feedback loop together
good to get feedback to get it's pretty
disillusioned with the whether or not
they're actually giving you any value at
all and exactly and because they're
spending a lot of time testing to the
nth degree for every single feature
they're spending a lot
of time writing tests hmm sometimes like
orders of magnitude more time writing
tests than actually writing the feature
now as we said in some circumstances
that's appropriate think about the Juno
rocket but to be honest these days a lot
of companies really care about speed of
release and there's a whole heap of you
know added benefits that you get if you
release software quickly so this is
something that's probably pretty
important to think about so the main
things that we want to put as takeaways
for this section is that we really want
to consider speed and confidence when
we're writing our tests we really think
that the testing pyramid is too rigid
for our purposes and we want to come up
with something custom based on each
application that we're doing and
confidence is really the most important
thing to think about when we're trying
to come up with our testing strategy
absolutely so we kind of talked a bit
about what we don't like now how do we
figure out what we do do so we've got a
we've got a bit of a quote here which we
just try and apply to everything we do
which is challenge yourself to start
simple and inspect and adapt and the
really important word here is challenge
because as I'm sure you've come across
before it's really easy to come up with
something super simple it's amazing it's
super clean and then you go I know six
months down the track and it's become
this complex monster so the other thing
of course is quite often you tell
someone to start simple and then they
start by shaving a yak and have this
crazy thing so by really challenging
yourself you're more likely to actually
start with something said yeah so if we
go on what we've got is I guess three
rules that we try and apply what I'm
trying to come up with a testing
strategy the first of those rules is to
try and challenge ourselves to set a
minimum confidence level that's
absolutely as low as possible and
there's a few key things there setting
that confidence level minimum as low as
possible is really important and if you
think about going to you're like a
typical product owner and saying to them
hey we want to accept a level of
confidence that's not a hundred percent
we want to accept the fact there might
be some bugs
you're probably like your first reaction
is going to be like that they're going
to go crazy they're not gonna like you
much at all despite the fact of course
that you know and there's always gonna
be bugs but that aside is yeah but they
want to think that you're doing
something that's a hundred percent
confidence right something else that's
really important to them and this
might require a bit of recoating a bit
of training is that the speed of release
on a lower minimum level of confidence
can be a lot higher so it's just
something to balance up whether that
move quickly that inversely proportional
relationship another thing to keep in
mind with this is if your team's using
techniques like continuous delivery if
there is a bug that team can fix it
pretty quickly so that kind of offsets
that risk in most circumstances yeah the
second thing that we're trying to think
about is how do we come up with a
testing strategy that meets that minimum
level of confidence as quickly as we can
so this is where we start to apply a lot
of software development practices to
test and that's something that we can
quite often forget we can quite often
end up with tests that aren't refactored
and don't have a lot of thought put into
them but it's something that's really
really important and the last thing is
making sure that we're actually adapting
that over time it can we've been to a
lot of places where it's really easy to
fall into the trap of ending up testing
your app the same way a year down the
track and so many lessons you can learn
based on what happens so it's just
really important to think about how we
adapt this over time so these we kind of
started this chapter with the business
right so where does the business come
into this that setting of a minimum
confidence level as low as possible
that's actually a business decision and
a lot of people who look at testing
think that automated testing is purely a
software development thing it's not
figuring out how what's that level of
confidence that users are going to
tolerate what's the risk that we're
going to tolerate it's very much
something that your product owner should
typically try and decide what's your
communication mechanism with your
end-users you know is it really tight
and you can kind of deal with the fact
that on the end of a bug but it's okay
we've got it covered without fixed in an
hour you know it this changes the
dynamic of what that minimum confidence
level is yeah that creating a testing
strategy that makes that minimum level
of confidence as quickly as possible
that's very much a technical thing
that's that's where we can shine as well
we can come up with something that
really hits that confidence as quickly
as possible by really applying
everything we know how to do and the
last point there about adjusting the
testing strategy over time that should
very much be something that everyone is
involved in that should be it's
something that we're really taking into
account when we
do out retrospectives when we're trying
to continuously improve is how do we
apply like different metrics how do we
figure out if we're actually improving
this strategy some things you can look
at is look at how many bugs are getting
into production
like how confident are you when you're
deploying how long does it take from
someone having an idea to that idea
being in production yes there's all
sorts of things you can really look at
and track and apply overtime to see if
you're getting better at this stuff so
that's really important we think
definitely so let's say you you apply
this technique you start off with a
really simple testing strategy every
time we build a feature let's just do
this type of test or something like that
that's the fastest test we can write for
a minimum confidence level so you might
wonder is that an oversimplification
that doesn't sound like it's going to
work in the real world well I'd say yes
or no sometimes we may well start with a
testing strategy that simple but the
whole point of what we're talking about
is you need to inspect and adapt over
time the testing strategy is not going
to stay that simple over time you're
going to figure out our we need to cover
this type of thing because otherwise
this sort of error seems to happen and
then we're gonna add this sort of thing
here the good thing about that is we're
not applying a rigid structure and
saying you have to test the same way
everywhere
we're being smarter about how we test
yeah and this is how you prevent it
getting too complex as well sure it
might start simple you also don't want
it to go too far the other way so
absolutely sorry now that we know what
our testing strategy is going to be what
we can do is we can hire all of the
consultants and get them to write like a
hundred page report we're going to put
them in a room with our enterprise
architects they're going to start
whiteboarding a synergistic testing
strategy and then what we're going to do
no I don't do that okay
well what we actually want to do is we
want to empower people right we want to
empower people to be able to make these
changes we want to try and make these
make this testing strategy not
necessarily even per project but
sometimes per feature sometimes
different depending on the different
area of the system and we want to make
sure that we're adapting it over time
that's the key stuff out of it please
change your testing strategy over time
based on how it goes don't keep it
static so main points out of this is
that try and make that minimum
confidence level of business decision
try and educate people on how to make
that decision try and empower them and
give them
right tools to make it effectively and
really focus on how you can help I guess
it meet that as quickly as possible in a
technical manner and and the last thing
which I know is said a few times but
really make sure that you're continually
improving it awesome okay so we're gonna
change it take a bit of a change of tact
we're gonna look at a few different
types of testing the first one is the
funny type of testing and I say funny
because it's a weird word subcutaneous
it takes a while to figure out how to
pronounce that but I can do it fairly
fluently now which is good it's a weird
word it's a medical word basically means
under the skin and that'll make sense in
a sec when Matt explains what a subject
is so I'm basically a subcutaneous test
is testing not at the UI level but just
underneath it so if you've worked with
MBC before for example that can be
testing at the NBC controller level it's
whatever level is the first entry point
into your code base you'll use real
repositories real data access real
services behind that but make sure to
mark out anything that's external to
your system which will typically just
kind of add fragility and not give you a
lot of confidence by testing and there's
better ways of testing up if your system
owns the database and performs
migrations on it it owns the database as
part of your system don't mock the
database out so what we get out of this
is a few different things we get a huge
amount of confidence out of this type of
test because it's using the same code
pads that you're actually using in
production and you're taking away some
of that problem that we have when we're
constantly marking out different
features that exist you can refactor
really easily I mean hands up if you've
ever broken like half of your test suite
by refactoring something right like we
have all the time and the problem with
that is when you do that and you then
have to change all of your unit tests
because they now no longer work with
your refactoring you're not getting any
confidence from those tests they haven't
assured you that you haven't broken the
app because you've just changed them
yeah yeah you need to get a voice back
dude it's also not refracting at that
point we're factoring is not changing
tests or factoring is just changing
implementation details yeah if if we
wanted our tests to break it should be
if we've changed some
a business rule right that's that's
where they add value that's where
they're helping us so um what we're
trying to aim to do here is just reduce
some of the mocking it doesn't work in
all situations which we'll cover in a
little bit but it certainly helps a lot
in certain ones another thing that you
can do here which is kind of cool is
combine this with the testing behavior
and acceptance criteria about the
features you're trying to introduce and
that lets you do really powerful things
like add a subcutaneous test for a
certain type of acceptance criteria
which you can then show to a product
owner to give them a lot of confidence
that you're actually testing the sort of
business rules that they want to see in
the system and that helps them make the
decision that we kind of alluded to
earlier and you can do a TDD and use
that test to drive the implementation of
the feature so you can still do red
green refactor with this type of testing
hmm sorry they sound pretty good right
subcutaneous says they're pretty cool
and certainly you know we've got to
admit we've had a lot of success in
using yeah so they're the Silver Bullet
we've all been looking for all right no
no they're not okay there's no such
thing in software as a Silver Bullet
yeah context is everything okay so in
particular for instance there are some
disadvantages and some may be
difficulties of using subcutaneous tests
that you'll come across one of them is
if you've got really complex code maybe
some sort of complex algorithm or
something like that it's really hard to
drive that from that high level and get
a reasonable amount of coverage of that
something that makes sense in this
instance you get a combinatorial
explosion but this is okay for two
reasons firstly sometimes you don't need
that level of coverage of that code
maybe a couple of happy passes all you
need in which case subcutaneous tests
are great or if you do need that level
of coverage then what you do is great we
do maybe one subcutaneous test if we
even need two and then we decide for
that code we're gonna dive down to a
lower level test which is much easier
and doesn't have the common tutorial
explosion problem that's the whole point
of what we're talking about we shouldn't
use the same type of testing everywhere
blindly we should make smart decisions
it can also be hard to know why
something's broken so the whole thing
about subcutaneous tests is you're
testing a fairly large proportion of
your code with each test right so if
something goes wrong can be sometimes
confusing to try to figure
where it actually went wrong now there
are some techniques that we can use to
make that easier and we're going to go
through a few of them towards the end of
the talk quite often what you will have
as well is you know given you you're
testing the full stack of code that
includes a fair number of cross-cutting
concerns so if you go and break
something in a cross-cutting concern
what you can find is that then that
breaks say like 40 subcutaneous tests or
something like that
that can be quite confusing and
disorienting to some people personally I
don't mind but I've come up with a
fairly thank you where I basically look
at one of the broken tests ignore all of
the others figure out what's wrong with
that test why did it break fix the
problem rerun the whole test suite a
bunch of them will now pass rinse and
repeat until there's no more read tests
it's a fairly easy approach yeah and
then the last one is saying that because
we're testing a reasonable amount of
code the tests themselves are more
complex and they're more complex in
three ways if you think about a range
act assert the arrangement of your tests
and figuring out all the test data to
get it in the right format probably a
bit more complex what you're actually
calling definitely more complex and then
come coincidentally what you're
asserting probably also more complex now
again we have some techniques we can use
to deal with this but essentially all
that boils down to is using the same
practices that we use for our production
code for our tests we should refactor
tests and we should use patterns and
practices in our tests that we know lead
to better tests again we're going to
cover a few strategies towards the end
of the talk that will help with some of
this so main things from this section is
that cool that's a that's an interesting
new type of test it's definitely worth
checking it out it's really good for
balancing that speed and confidence
because it hits a fairly good balance in
a lot of situations between both we find
that mocking implementation details
leads to a lot of pain so try and avoid
it where you can there are situations
where you need to do it absolutely so we
wanted to talk a little bit about unit
tests so unit test is a bit of an
overloaded terms we kind of alluded to
earlier what we've decided to call them
just for the sake of this present
is an implementation test so by this
we're referring to an in-memory test in
process they're usually extremely quick
you can run like 10,000 of them fairly
quickly but they give you that lower
amount of confidence because you're
you're generally only testing an
incredibly small part of a system and
you don't necessarily know how well that
works in with everything else they
inhibit your ability to painlessly
refactor that break all your test
situation but they're really really
useful in the right situations so if
you've got a particularly particularly
complex piece of code that you need to
that you really need to figure out if
you're doing TDD for example a lot of
feedback we heard and especially in the
area early era of T today was it gives
us a lot of ability to more easily
design our code in our heads before we
write it so that's kind of cool what you
need to consider if you're following
that kind of approach is actually once
you've come up with your test is it
still delivering you the sort of value
you want over time if it breaks very
very often if it breaks every few days
you really need to look at it and go
should we actually keep this test or
should we delete it and a lot of people
are terrified of deleting tests right
it's it's just something that's like not
that familiar to us absolutely I mean
and then the other decision point is
maybe it is worth keeping it and dealing
with that pain because the fact that you
needed that test to drive that complex
code in the first place might mean it's
helpful later on as you're supporting
you your code and changing it it's
there's no there's no there's no right
answer so I think it has to be
contextual cool so next I want to talk a
little bit about UI tests UI automation
this is something that we find is a
struggle in a lot of places this is
something that causes people a lot of
pain so automated your our tests just to
like it'd be clear I guess great in the
right situation they give you a huge
amount of confidence when they work so
if you've got a particular test that's
clicking through a system as a user and
it successfully clicks through that
system and gives you back the right
output you've got a fair amount of
confidence that someone coming along as
a real user and clicking through that
system is going to get the same result
so that's great the problem with them is
that they're typically extremely slow
and they're typically extremely fragile
so what we want to do here is think
about where we can apply the
that gives us a lot of value and
oftentimes that's in particularly
complex you eyes if we're talking about
a really complex client-side application
maybe where there's a lot going on these
become really really useful and there's
a few other ways to get that same level
of value so it's just important to think
about the fact that there's a trade-off
here or think about that that mortgage
application form scenarios those
high-value scenarios where you need to
know that that thing works according to
the end-users right in that case you
make the trade-off you trade off some
slow and fragile tests in return for the
high confidence you need in those parts
of the system yeah but again that
doesn't mean you apply that for every
single feature across the whole system
only that one phone unless that makes
sense so just just be smart about how
you do these things what we can
typically find ourselves falling into a
trap of a lot is over UI testing and
ending up spending a lot of time trying
to sort out issues rather than
delivering value now we're not covering
it in this talk but one thing to note
about automated UI testing is that you
know just like what we talked about with
subcutaneous testing a few slides back
using our patterns like refactoring and
and you know a lot of different software
patents and all these objects page
objects etc are really important for
automated UI chess so you can like a
limit the fragility of them and then
you're just dealing with weird things
like when you upgrade Firefox selenium
webdriver doesn't work anymore and stuff
like that yeah so the next thing we're
going to talk about is just walking
through a potential scenario where we
might walk into a client and and and how
we might decide on a testing strategy
just to give you a bit of context for
how this might go so context is really
important here yeah so we've already
talked about different scenarios where
different testing strategies make sense
so to try to limit this this is a
typical scenario we might see some sort
of small to medium line of business
application across people different
industries doesn't really matter
generally with a relatively small
proportion of sort of complex
algorithmic sort of code not real-time
or life whatever anything like that and
a team using continuous delivery so they
can fix things pretty quickly right in
that sort of context what we might end
up with initially is something like this
we might say cool when we do a feature
we'll do a happy path or maybe some
important errand paths for subcutaneous
tests if we've got any API calls we'll
been up an in-memory full-stack HTTP
server it's kind of similar in a lot of
ways to subcutaneous tests because
obviously there's no UI RF and for any
of those calls and then we'll use a
small but judicious use depending on
what makes sense in different parts of
the apps app sorry of things like
convention test route tests integration
tests you know implementation tests and
end-to-end tests if that makes sense if
we're talking across multiple systems
and you know maybe a few UI tests here
in there right convention tests in
particular if you haven't looked into
before really really awesome it's the
type of test where you write them once
and you'll the test will automatically
adapt to code that you add after that
and happy to chat about that more if you
want it really is after you use
reflection typically to look at your
code and make sure you're following
Stoke patterns basically very powerful
type of test test outdoor convention all
right yeah okay so what we're gonna do
now is look through some code so the
code we're gonna show is kind of see
sharpie and we've got it on there's a
link at the end so you can go and see
the code and get the slide deck and this
sort of stuff is possible in just about
every language though so again like
something we can check out on T about if
you've got kind of questions don't use
something else we've used these
techniques for instance in JavaScript
before pretty successfully so this part
of the talk is basically going to be
looking at okay
we've we've got subcutaneous testing
you've said they're really good you've
said there some disadvantages and some
difficulties what techniques do you use
to try to overcome that so this is
basically our learnings yeah cool yep
alright nice so the first thing that
we're going to demonstrate is an
implementation test that you might
typically see versus what a subcutaneous
test version of it might look like so
we're just using Visual Studio code here
so here's an example so we've got a
student controller the student
controller takes a repository and in
this case we're mocking it out this is
in substitute syntax we've got a test
here that says that the index action
should show students so what we're doing
is we're setting up some expected
students here and then we're saying that
the get all method on the student
repository should return those expected
students and then we make our assertion
so cool when we call the index action on
the controller it should render the
default view and it should have a model
where the view model count should be the
same as the expected students and then
the names should be what we expected
them to be pretty simple standard sort
of stuff right you might then combine
that with a test that's testing just the
student repository in this example we're
saying that there's a real data base
being spun up in the background as well
and then what we say here is that the
get all method returns students in the
database order by name so we set up some
expected students we loop through them
and basically save them into the
database we call that get all method and
then we check that what was returned
matches what we expected ordered by name
so this is something that you might
typically do we've certainly done this
in the past now if we were to convert
this directly to a subcutaneous test
without applying any of the other
patterns that we normally use so there's
gonna be a lot of things in here that
make me really uncomfortable that will
do that just to show you then it might
look something like this
so in our test setup when we new up the
controller we're using a real student
repository with a real database
connection okay so we're not mocking any
of that stuff out it's just fall in to
end what we're going to do is add the
expected students into the database like
we did in that database test we're going
to call that index map our method check
it default renders the default view and
then pull out a reference to the view
model that was passed to that view and
then check that that view model had all
of its names in the right order so this
is essentially a combination of those
two tests we wrote before so now we only
have to write one test which is nice it
tests a whole bunch of things and you
know that that get all method is nowhere
in here at all that implementation
detail that there's a get all method has
disappeared we're going to replace that
with an API we could replace that with
whatever we want it's still just work
yep so it's pretty cool now there are a
few issues still with that but bear with
us cool so we want to try and look at
some I guess patterns that we can apply
to this stuff in order to to start
cleaning this up even more one of the
things that I'm sure you've come across
the forest behavior driven tests
behavior driven tests in combination
with subcutaneous tests can be really
powerful because they they give you a
huge amount of confidence that the
behave
that you're describing in your test
matches what it is that you're actually
testing and they give you a huge amount
of focus I guess as well in that you
know fairly easily what it is that you
want to test and and you've got a fairly
easy path to follow in that if you add a
certain type of rule to the system you
go ok cool we make a decision at this
point as to whether or not that makes
sense as a test it gives you focus
because you're basically got a test
there's talking about behavior and
you're focusing on trying to implement
the behavior as opposed to where you're
writing implementation tests where
you're focusing on trying to write the
implementation it's a very different
state of mind cool and I guess the last
thing is that we find writing them in
this fashion it makes them a lot easier
to read and a lot easier to write as
well so worth having a look at so let's
have a look at a demo so if we convert
this subcutaneous test that we just
showed you to use in this case something
called test stack b25 there's a whole
heap of other libraries this is just the
one we typically use then starting to
look a little bit different now we're
also using a pattern here called class /
scenario so every test is a class as
opposed to having multiple tests in one
class we actually find this is quite
nice because what you typically find is
we'll use something else code organized
by feature for the embassy app so you'll
have like a features folder in the
testing and then for each feature in
this case maybe it's students or
something like that then in that folder
they'll be like you know in this case
it's the view existing students scenario
and then you might have successful
registration of a student scenario and
like you just have a list of them there
it actually is quite nice from that
perspective but anyway again we're
newing up the student controller with
the real dependency in a real database
then we've got these different methods
now so given existing students and then
in here we're just saving those students
for the database like we did before when
the user views the students so I was
calling that index method then the user
user should see students order by name
and we're putting that other logic we
had before now the nice thing about this
and if you remember one of the
disadvantages I said was it's hard to
know where something's wrong when the
tests break in this instance especially
with a library like being a defy it kind
of says hey this test broke with this
exception in given existing students at
that point you've really
strange where you need to look it's
something to do with the data that
you've put in the database right so
that's quite useful another thing that
we can look at doing so you would have
seen in some of those examples where
newing up a lot of stuff right that's an
implementation detail and a way that we
can start to deal with some of those is
by driving our tests using the same
dependency injection container that
we're using in production if we're using
a dependency injection container that
lets us avoid those newing up
implementation details that we don't
really need in a test it makes sure that
we're using the same code paths that
we're actually using in production we
kind of covered before that we're trying
to avoid mocks where we can but there
are situations where you need to use
them and this is a really easy way to
substitute those in and if you have to
yeah and you don't have to worry about
which component it is uses that mock you
just say DUI container here's the mock
go and inject it wherever you need to
it's quite cool now you'll see that this
in particular really starts to clean up
our code so if we take this same step
sorry tests and we look at what it might
look like if you introduce a DI
container there's a whole bunch of stuff
we've now changed one of them is we've
now actually introduced a base class
called subcutaneous MVC scenario and
we're passing in the type of controller
the reason why we do that is because now
we don't need to new up the controller
you can see here that that's setup
method if we go back to the BD defi
example here all that's gone so this is
actually in reality and implementation
detail the fact that the controller
needs a student repository we shouldn't
care about that if you then refactor
your code base to add the query pattern
or whatever it is that you want to add
our tests shouldn't break so the
container allows us to do that so by
passing through the type the code inside
of the base class will just resolve that
controller from your production di
container and any dependencies it has
and any mocks that you've registered so
it's pretty cool the rest of its pretty
similar we set up the database with
students we've now got a nice helper
method here to execute the controller
action based on Orlando simply so that
we don't need to have a private field
for the action resume we can just use
the action result here just a nice
little optimization but the rest of it's
essentially exactly the same but this is
starting to look really nice now there's
there's basically no implementation
details which is pretty cool
okay so the next thing that I guess we
can look at is how we can deal with
marks so as we kind of said before in
this type of test it's really important
to mark external dependencies we really
don't necessarily want to be calling
those where we can avoid it but m'p
Roache we can kind of take in order to
help make this a little bit less painful
where we do have to do it so something
we've been trying to do is hand rolling
marks instead of using a marking
librarian which is pretty crazy but it
does two things a it makes mocking a
little bit more painful so and if you've
got someone you the project they're a
little bit less likely to do it but the
second thing is that you can codify
certain behavior within your marks that
make your tests a lot more readable so
if you've got code that you have to use
in your tests when you're doing a
particular mark setting it up in some
certain way you can provide a method
that's just a part of that mark that
does that for you and then your tests
don't need to duplicate that logic each
time they do it so we'll show you what
that looks like okay so we're going to
move on from this student controller
scenario for the rest of these so here's
an example of an interface you may have
in your production system a date/time
provider it's pretty standard sort of
thing returns date/time offset from an
hour method and this might be what your
production implementation looks like
we're just returning date/time offset by
UTC now pretty easy this is what a
simple hand roll MOC might look like
when we construct the MOC we grab a
static value of now and then we just
always return that that's actually quite
useful because it means that when you're
using this thing in your production code
that you're testing then afterwards when
you're doing assertions you can just
resolve this out of the DI container
call now and get the same value that was
used so it's easy to do sessions but we
can be way better than this
we can start doing the codification of
behavior that Matt was talking about so
here's another example of what this
could be now in this example here we've
got a couple of new constructors kind of
interesting we've got one that takes the
date/time offset so that you can
actually explicitly set the time that
can be quite
useful in certain scenarios we've also
got a version with a string so that you
can do data-driven tests with attributes
because they have to have static compile
time constants so they have to be
strings but we can get even better than
this we can do things like set now so
now you basically created a time machine
yeah so you can resolve your static
date/time provider from your test called
dot set now and change the time so you
could do something like register a user
and then set now to a different time and
then perform some other action and check
the last modified date is different from
the created date that's really easy to
do in that in that sort of instance even
better move time forward by a timespan
the code to do this if you if you didn't
have this nice codified method would
look something like this provide so to
provide something to your di container a
new static date/time provider and then
resolve the current date time provider
get the now value and add a time span to
it that's just like you start reading
that and your brain melts you don't know
what's going on it's not clear code it's
not self documenting however if instead
you just had call resolved a static
date/time provider cut called dot move
time forward by time and hours one
awesome that's it's pretty obvious we
just move time forward one hour much
more readable right this is where our
factoring comes in then you can get even
more crazy if you really want to and you
can do stuff like this you can say cool
let's create a using block and then
inside that using block let's
temporarily change time by four hours
and then after the using block it'll be
back what it was again you can do all
kinds of cool stuff like that and this
is just one example using time right but
you can do this with pretty much
anything that you're mocking out which
is typically in a subcutaneous test test
at least going to be external
dependencies like time or API calls cool
so another couple of patterns that you
might have come across before and so the
object mother pattern and the test are
to build a pattern we've seen this a
fair bit and something we've found is
that these two actually work really
really well together they give you a
huge amount of consistency and clarity
in how you set up data within your tests
if you think about a typical test
there's usually a fair bit at the top of
that test that's trying to figure out
okay how do we set up this test in the
right state so that everything just
works especially in a subcutaneous test
whereas we
before going on your arranged act an
assert are all going to be more complex
so you're quite often going to be
setting up reasonably complex chains of
objects yeah so what we want to try and
do here is figure out a way that we can
start to remove a lot of that
duplication and complexity and make it
really clear to someone who's reading
the test exactly what we're doing with
our data set up and this is just one way
to do that okay so for this one we're
going to take a slightly different tact
we're not actually going to show you a
subcutaneous test for this example we're
just going to show like your
implementation tests that's fine it
still demonstrates the value of these
methods so in this case we've got given
demographic with state and age range
when checking if hello when when
checking if the demographic applies to a
member then return true only if the
member conforms to all parameters it's a
bit of a weird one but anyway that aside
essentially we've got some sort of
domain where you've got a demographic
it's got some properties in it like an
age range and stuff like that and then
you've got members who you know they
have like an address and the name and an
age and all that sort of stuff and the
demographic will check whether or not
but can you can sorry you can check
whether or not a member applies to a
demographic this is actually based on
code that we wrote for a client sorry is
a real thing now this is where the
patterns show up so for the member we're
gonna say object mother dot members dot
Fred right Fred's just a member we don't
we don't care about this particular
member we just need Fred we just need
Joey we need whatever right we just it's
just a member but what we can then do is
customize it and say call we want a
member in the state of state the one
that was passed through in the
data-driven test and we want a member
that has the age of the age that was
passed through this is the combination
so this bit here is the object mother we
just get a prebuilt object or in this
case actually a prebuilt test data
builder and then we customize it in this
test trying to do that with just object
mother is gross
now demographic we can say call we want
object mother demographics top members
in W a the cool thing about that is you
could probably use that in a bunch of
places right it's probably something
that's quite standard to all of your
tests you've just refactored some of
your if you were just using test are to
build a code so that you're not using
the same dot in state W a everywhere and
then in this case we
they say cool minimum age of 18 maximum
age of 19 and the demographic contains
that member when the state is w a in the
ages either 18 or 19 so this stuff then
takes on a mind of its own when you
start looking at building lists of
entities so in this case we say cool we
want a W a member and we want a list of
three products the first one has a
campaign for all members so that one
will apply to the member the second one
has a campaign for a CT that one won't
apply because there in W a and the third
one has a campaign in the state of the
state to members and in this case W a so
the first one and the last one should
apply and in this case we're testing a
query so we're saying given products
with a variety of state rules when
querying products for the member then
only return the products that apply to
the member so when we execute that query
we should only get back the first
product and the last product now if you
think about what this might look like if
you were using constructors right in
particular this product list you'll be
like newing up three products and for
each product you're going to pass in a
bunch of stuff like a name and other
things and then you're also going to
have to pass in a campaign which is
probably on a domain method because you
probably wouldn't pass that into the
constructor so like that codes just
going to get huge and a lot of that
stuff doesn't matter it doesn't help you
in understanding this test and what kind
of data is actually essential for it to
run if there's a lot of noise like for
instance the product name who cares
this test is concerned with States the
nice thing about this pattern is you
only put the data there that matters for
this test you've removed a bunch of
noise which makes the test a lot easier
to read it's awesome
cool so the last pattern that we want to
talk about that can help you out in some
of this stuff is approval test so
approval test it come up all the time in
work that we do it's basically the idea
that you use the human brain as an
approval mechanism instead of writing a
whole people of different sessions and
the way that that works is you run some
code you'll get the output of that
sometimes it could be like a complex
piece of Jason you can do this with
images you can do it with all sorts of
things the first time that you come up
with the test you'll save that result to
a file somewhere that just gets
committed with the codebase and then in
subsequent runs of the test if that
output changes the developers changed it
gets a diff window with whatever happens
so you can pick up little differences in
in that output that have changed
unexpectedly or you can mark stuff as
okay if it changed and you did expect it
and which in certain scenarios if you're
trying to test like really complex
objects you can spend a long time
writing assertions and keeping those
up-to-date over time whereas this just
lets you do it automatically using
whoever's making the change it's pretty
cool like we said in the subcutaneous
test scenario a range actor cert are all
probably a little bit more complex so
this is a technique you can use to try
to make that assertion bit a lot simpler
let's show you an example of that
because it it really speaks for itself
when you say it yeah if you guys haven't
heard of Approval test before by the way
so maybe a third of you yeah so this
stuff blows people's minds when we show
it okay essentially when you show images
but we don't want to do that for this
demo yeah okay so we've got just let's
say you're an ISP yeah and you've got a
shopping cart for users right and you
want to test the scenario where the user
comes to your shopping cart they say
cool I want mbm and I want fight and I
want the international call option then
what would happen in this case we're
saying cool is an MB M boy the NBN wide
and international calls checkout
scenario and it's a subcutaneous MVC
test for the checkout controller and so
given the user has started a session so
we're just going to save object mother
dot sessions default and given the user
has added nvm voice and international
call bonus option so add to cart product
store NBN quite international called
bonus option and then save it so we save
that to the database they've done
that previously given the user has
provided their checkout details so we're
just gonna build a checkout view model
gonna populate some random stuff in
there like your name and credit card or
whatever it is that it has when the user
checks out the cart so we call the index
action on that controller we pass
through the view model then register the
order so we'll pull the water out of the
database and then normally you know you
probably have a bunch of assertions you
check that the view model correctly got
mapped to the database entity that got
persisted but then the really
interesting part here is and send
requests to the provisioning system so
in this example let's imagine you've got
some sort of provisioning system that
you send a JSON object to a message bar
so HTTP API doesn't really matter in
this case we're going to resolve the
mock provisioning system we're gonna
call dot requests and and that's
something that would be this like
codified behavior so basically that mock
provisioning system would probably just
stash away requests that come through to
it and in this case we're gonna say
there's probably only one we assume
there's only one so it's gonna get
single just gonna get whatever's in
there and then we say requests dots
should match approved this is using
should be there's also something called
approval tests net and in a bunch of
other languages what you would then see
if you actually let's look at the
approved one which ones that one so that
one yeah probes cool so this is an
example of what we've previously said is
the correct thing to be in that request
right so we've got a JSON object there's
some products there's one with an NBN
with an against an address and we've got
a username and the plan and you know
we're gonna get 50 megabit that's pretty
good it's better than the hotel
absolutely 24 month contract there's a
VoIP one and actually mm-hmm
there's actually meant to be another one
here but let's say that you just saw the
international call plan you'll see why
in a second and then we've got the
customer details yeah now so you just
did some refactoring and you broke your
test and you get a different note pop-up
and what you see is this oh we broke it
so the great thing about this right like
like so let's take a step back for a
second if you were going to try to
assert this with like typical assertions
right look at that you would have just
like hundreds of like this should equal
this this should equal this it's crazy
right
you're doing an assertion protest when
this comes up the first time you get a
diff window you just look at it and make
sure that it's right and you go accept
this file done that's literally how fast
it is and then in a scenario like this
we say what have we done we've broken it
the international calls option isn't in
there for some reason don't know why
that is but I know exactly where to look
and this is where some of the power of
this stuff comes in right because if you
had a whole heap of sessions it's like
are this product wasn't there something
like that
like it's not as clear and immediate as
this this tells you where you need a
look and look down here oh the work
phone it's got the same value as the
home phone I bet that we've messed up
the mapping for that view model it's a
copy pasted accidentally did dive home
phone rather you know yeah but how many
times you guys done that I do it all the
time
this stuff is really powerful and really
cool so the last demo that we're gonna
show you is to try to put all of what we
just talked about together so we've got
a bit of an example of a scenario that
we actually wrote yep we actually wrote
it for one of our customers I mean it'd
be modified but you get the idea that
pretty much demonstrates all of the
patterns we just talked about okay here
we go
successful user registration scenario
this is a subcutaneous MVC test for the
user registration controller given the
valid user registration data has been
entered so we're going to say cool we
want valid user user registration data
from the object mother we're going to
execute the index action with that view
model we're going to pull out the user
that hopefully is in our database then
we're going to then the user should have
been redirected to the success page so
we're checking that there the user
should be persisted so the saved user
shouldn't be null and the user the users
personal details should be correct now
this is using a library called should
Lee which has some really nice syntax
that allows you to perform multiple
assertions where if one of them is
broken or multiple ones are broken is
still going to run all of them and if
multiple ones are broken it's going to
show you all the ones that were broken
so this is like a nice way of doing
multiple assertions rather than just
this should be this this should be this
and then the first one that breaks means
you don't run the others this one does
it runs all of them it's really cool so
we've basically checking here that the
view model has been correctly matched to
the database it's quite powerful we're
doing the same thing here for the postal
address
and then here we're checking that the
user was actually the bcrypt
encrypted version now if any of you were
in our talk this morning you know you
shouldn't do this and don't store
passwords in your system okay bad
obviously yes and then here we're
checking the created and modified date
should be set to now so we're resolving
our I date/time provider and calling now
and checking that it's what was in the
database and then we're checking is a
registration email got sent so over here
we're getting our mock email sending
service we're pulling out the email that
was sent we're checking that the two
address is exactly what we thought it
should be we're checking the subjects
correct and then we can check with an
approval test that the body of the email
contain the content we expected we've
tested a lot in this test hopefully you
can agree with me this is incredibly
readable and if something goes wrong in
any of these bits it's actually pretty
quick to figure out what's going on and
then to go and resolve it this I think
shows some of the real power of this
testing strategy or technique I should
say so my strategy isn't so and I guess
some questions to ask to kind of leave
you at the end of this talk is are you
happy with the amount of speed of your
test you happy with the amount of
confidence you're getting out of them
you happy with the speed of feedback how
can you go away and try to make the
testing that you're doing simpler you
might want to do less of it you might
want to do it apply a simpler approach
you might want to apply some of the
refactoring or some of the patterns
we've we've covered there's a whole heap
of different ways that you can start to
simplify what you're doing and are you
trying to think about how you're
continuously improving your testing
strategy are there any takeaways you can
you can make to try and go back and
start brainstorming this stuff with the
people you work with nice so that's
everything we want you to say if you
want to get the slides or the code said
that URL and that's about it thank you
thank you
sorry any questions yeah absolutely
really happy that you asked thing Arish
yeah we should repeat it so for everyone
that couldn't hear and for the video
basically the question was if it's a
business decision what the minimum
confidence level is and you've got a
product owner that's not a technical
person they're a little bit kind of
confused and they don't think it's their
decision or they don't want to make the
decision or they can't grasp the concept
that you know hey we might have a risk
of having bugs you know come on so and
we had a we applied something to try and
sort this problem out in one of our past
projects we basically had a product
owner who was was saying that type of
thing they're saying okay well I don't
really want anything to do with the
automated test that you're right I'd
just just kind of leave it it doesn't
kind of help me make any decisions and
what we did is we applied two of those
techniques so beta deify syntax and
subcutaneous tests and each time we made
a change to the system we just generated
a basically report of all the acceptance
criteria that were added or removed or
changed in the system and they had that
in front of them when they were looking
at the button to deploy to production so
they could actually use that to make a
really valid business decision as to
whether or not people had added
kind of too many tests hair don't care
about that one whether or not they
hadn't added enough whether or not
they'd added the wrong rules and that
really informed that and like helped
them make the right decision if they
didn't have that kind of thing in front
of them we so often ask like product
owners hey you should be the ones who
deploy to production but without
something like that it's really hard for
them to make that decision in a safe way
and they kind of end up like a little
bit disillusioned with having to do that
so that's just one approach that you can
try and take couple of points on that so
in that case that video file library
actually
spits out HTML report by default and
what we did is we just hooked that up as
a build artifact so for every like
deployment you could kind of go and see
like the set of tests that we're running
to shows all the given win then
statements for each feature based on per
feature if that makes sense
the other thing I'd probably say is that
it's one of those things where it comes
down to education right so on day one
yeah with the product donor like that
they're not going to be able to make
these decisions yeah I guess it's up to
the team to have a really good
bi-directional relationship with them
which any good agile team should have
and and sort of slowly give them the
confidence and explain this stuff to
them and and you know explain it in
terms that they understand right if we
do this that means you might have this
type of bug we can fix it within five
minutes or ten minutes into production
or whatever it is are you comfortable
with that if you put it in terms like
that they typically understand it and
especially if you then say if we test it
in this other way where that bug will
never be possible it'll mean that it
takes us 50% because two days to do that
yeah it takes this beauty' percent
longer to develop this feature which one
you know how much is it worth it you
know you put it in those terms that they
understand then hopefully if you had the
good relationship then you can kind of
you know it's not easy right and you
can't do it from day one necessarily
yeah there's a couple of different
question was if you have to integrate to
your database you may need to spin it up
and down in every test if you have a
thousand of those tests it can be quite
slow first thing I'd say is if you go to
that top link and you look through that
series in particular the one where I'm
reviewing Jimmy bow guards video there's
a bunch of different techniques for
isolating databases that I cover in
there and some of them don't require you
to spin up or down a whole database
every time so that's one thing and then
a cold well that was one of one of those
things that you can do
for example is you can use database
transactions to give yourself very
similar effect so spin up a database in
in basically an empty state bring it up
to whatever schema level it needs to be
at start a transaction perform your test
and then just tear down that transaction
the end of the test which is
significantly faster there's a pretty
small class of errors that you don't
catch with that kind of approach but
again that's kind of a trade off if
you've got a lot of that type of test
then that might be a good thing to look
at if you happen to be using an entity
framework by any chance that the bottom
on this or cutaneous test presentation
there's some code in that one that has
that data based test fixture class I was
using my code examples for this talk and
that one has that but in a way that you
can have difference between verify doing
the work and sorry seeding doing the
work and verifying but they're all
attached to the one transaction so
something like clever little code in
there that kind of works that's the
entity framework but you can do the same
thing with like other things as well
yeah that's the other option that's
certainly one of the ones you can do you
just need to be careful then about the
IDS you use in your tests and making
sure you're not Masumi like in the code
samples I had we were doing things like
call pulled the single thing out of this
thing you can't do that then you have to
actually look by ID to grab the thing
out but yeah that's another option
absolutely to be honest
okay so the first question was of you
you've got like Donna 2.0 you can't use
some of these cool libraries what do you
do I'll be honest I haven't you ever
used like a project in like just years
and years that's less than four so like
a good luck but I don't know I guess you
know do the best you can right try to
find libraries that do support it or try
to like yeah that's not in support so
let's look I mean little bit to be fair
there's some of these techniques that
you don't necessarily need libraries for
so I'm sure there's still some things
that you can get that you can apply so
the second was around async things yep
absolutely we just like to say that BT
to file library that supports async for
instance so we just make the test like
given whatever async and stuff and it
would do in a weight or whatever so the
question was how can you involve testers
because in particular we're trying to
set up data then testers are really good
at and setting up great data to break
things right I could do literally a
whole talk on the role of a tester and
an agile team so I'm gonna have to try
to done that one day oh yeah okay number
one developers and testers should work
closely together so if you're writing a
like a subcutaneous test pair with your
tester hey dude like help me or not do
necessarily it could be cold that's good
no you know it could be help me figure
out what tests what test data I should
set this up with and hey which which
features do you think we should test in
this way or what should the BDD syntax
be which which scenario should we run
the tester has got a really good mindset
for that sort of thing sit down with
them pair with them you're not separate
people who are in the same team right
and then another scenario we had was we
had some testers that
learn how to write this type of test and
they were writing some of them as well
yeah which was obviously a lucky
scenario but there's there's definitely
a huge amount of value that they can add
just by like sitting down next to you
and working together on this stuff and
it's it may be a bit of a shift in
mindset and again maybe a bit of
training and recoating to what you've
done in the past to get them like
they're helping out in that kind of way
any other questions yep so in the the
question was do we use our domain to
save objects in the test database from
seeding our tests so yeah absolutely in
the tests that we just saw sorry in the
code samples that we just shown we were
absolutely doing that in some
circumstances we would absolutely do
that and it works really well in other
circumstances we might not want to tie
that because you could consider that in
some ways to be an implementation detail
and in which case you you kind of the
only problem with that is you kind of
suffer the sort of like you'd have to
call sake controllers for everything so
you'd have like a lot of duplication
between the tests that has advantages
and disadvantages right so it's about
being pragmatic quite often we find that
works really well especially if you use
object mother and test starter builder
because object mother and test starter
builder actually give you a bit of a
kind of abstraction layer between you
and your domain if that makes sense so
you're not calling the constructor you
can refactor constructors all that sort
of stuff
you've got constructor calls in one
place in your test starter builder for
instance so we find that often
alleviates most of the disadvantages are
tying it to date domain objects but you
know be pragmatic right in some cases it
doesn't make sense in other cases it
works like cool anymore maybe one last
question
cool nope great a practice laughter
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>