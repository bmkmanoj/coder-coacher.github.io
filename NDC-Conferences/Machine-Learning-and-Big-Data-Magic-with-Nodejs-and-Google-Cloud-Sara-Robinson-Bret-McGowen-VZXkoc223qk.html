<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning and Big Data Magic with Node.js and Google Cloud - Sara Robinson &amp; Bret McGowen | Coder Coacher - Coaching Coders</title><meta content="Machine Learning and Big Data Magic with Node.js and Google Cloud - Sara Robinson &amp; Bret McGowen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning and Big Data Magic with Node.js and Google Cloud - Sara Robinson &amp; Bret McGowen</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VZXkoc223qk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to Big Data and machine learning
magic with nodejs and Google Cloud
about us my name is Sarah Robinson I'm a
developer advocate on the Google cloud
platform team you can find me on twitter
@ s robb tweets i live in new york it's
my first time in australia super excited
to be here in addition to being a
developer advocate I'm also a Harry
Potter advocate big fan of Harry Potter
if you want to chat with me about after
would love to I'm also a big fan of
Hamilton the musical I'm Bret McGowen
I'm also a developer advocate on Google
cloud platform
I'm alas a Harry Potter fan more of a
Lord of the Rings fan I'm actually going
to New Zealand next week so hopefully I
will check out the little Hobbit and
tour and all those stuff should be fine
I love Hamilton but not as much as saira
we sit next to each other in New York
office so mostly I just listen to Sarah
talk about Hamilton all the time
you can follow me on here on twitter at
at brett mcg so as developer advocate
sir and i we work for google cloud in
google cloud if you're not familiar it's
public cloud from google we've got all
the normal compute stuff like virtual
machines platform as-a-service but we
really have a lot of interesting things
around machine learning and big data so
as part of our role we'd love to hear
what you guys are doing in whatever
cloud you're running on Google or not
see what kind of projects you're working
on and maybe how we can help we take
that feedback back to our teams at
Google to make our stuff better and help
you solve your problems so oh I should
say even though I live in New York City
I'm from Texas
all right much better how can you tell
if someone's from Texas they tell you
they will always tell you exactly so
howdy
welcome and let me turn it back over to
Sarah so I wanna start out by talking
about at a high level what is machine
learning
so essentially machine learning is
teaching computers to recognize patterns
in the same way that our brains do so
it's really easy for a child to identify
the difference between a cat or a dog
but it's much much more difficult to
teach a computer to be able to do the
same thing so idea is we write code that
will find these patterns for us and over
time these models will improve as we
give them more and more data through
more experience so here we have a
diagram of a typical deep neural network
this particular neural network is
classifying an image that's either an
image of a cat or a dog we can think of
the input to this network as the
all pixels in the image and then each
layer in the network is looking for a
specific set of features in the image
maybe it's the shape of the ears the
eyes the nose and the output the final
label is gonna be a prediction whether
it's a cat or a dog in this case the
output is a dog but let's take a step
back for a moment and pretend we didn't
have any neural networks and we were
gonna try to do some human powered image
detection on our own so if we were to
write an algorithm to identify pictures
that's either an apple or an orange how
might we do this without machine
learning so what are some of the things
we might look for in our algorithm you
can shout it out color color is a good
one and so we could look at the color
and in this example we could say are the
majority the pixels in the image red if
so it's an apple otherwise it's an
orange so that would work really well
for these two images but what would
happen if our images were grayscale then
we might have to look for a different
type of feature what might we look for
with these two images Sam if there's a
SEM or not we could look at the texture
both good examples so if we if we looks
for either of those then we'd be able
able to handle the problem of
potentially black and white images but
what if we got crazy and added a third
fruit to our equation if we added a
mango we'd have to start all over again
so you get the idea but all of these
images are pretty similar they're all
circular they're all fruits so image
classification should be much easier if
we have two images that look nothing
alike so if we have for example a
picture of a dog and a picture of a mop
should be really easy right the mop is
not living or breathing has no eyes nose
or ears should be pretty easy to
distinguish between these two things but
it's actually not it's actually pretty
difficult
so here we have four pictures of
sheepdogs and four pictures of mops and
it's kind of hard even for the human eye
to be able to tell the difference
between these two things so the idea
here is that we don't want to write
specific rules for each type of image we
might encounter in our application
because chances are we have to deal with
more than just animals and fruit images
we might have photos of everything and
in addition to photos we might have
other types of unstructured data that
we're dealing with we might have video
audio and texts so we want to talk about
today is how we can help you make sense
of this unstructured data through some
machine learning products provided by
Google cloud so on the left-hand side
here we have a few products to help you
build and train your own custom machine
learning models using your own data
the tensorflow is an open source
framework that lets you do this and if
you want a place to run your tensor flow
models run the training and inference we
have a product called cloud machine
learning engine which lets you run your
tensor flow models on managed Google
infrastructure well we're gonna focus on
today is the right-hand side which I
like to call friendly machine learning
and essentially these give you access to
pre trained machine learning models as
an easy-to-use REST API so Google's been
solving machine learning problems for
many many years and what we're doing
with these API is is we're exposing some
of these models to developers and today
we're gonna tell you specifically how
you can interact with these five api's
using nodejs so I'm gonna hand it over
to Brett to tell us about the vision API
all right thanks Sara so the vision API
is a machine learning trained API from
Google we have a nice nodejs wrapper
called Google Cloud / vision so we've
got NPM modules diffusing nodejs but we
also have these nice little client
libraries for dotnet
Python Ruby Java go a whole suite of
languages so at the end of the day it's
just an HTTP REST API but if you want
something that's a little more idiomatic
and works well with your workflow we
have a bunch of client libraries for
different languages to do things like
work with the vision API so let's talk a
little bit about what it can do there's
different kinds of detections that can
do
so when you send it a photo it can
figure out different things about it so
the first thing it can do is label
detection so label this image tell me
what is this a photo of so in this
example it would say maybe cheetah and
it gives you some percentage of
confidence like I'm 90 percent sure that
this is a is a cheetah you know maybe
I'm 80% sure it's a mammal of some kind
it kind of gives you a range of things
that can identify as well as a level of
confidence about that the second thing
you can do is face detection now be
aware this is not facial recognition
all right this can't tell you who this
person is and you can't even tell you
that this person in this photo is the
same or different than another person in
another photo literally all it is is
seeing that there are faces and will
look a little bit more about that in a
second
OCR so you give it a photo and it can
find the words and the text in there and
actually extract them out and give you
information about that explicit content
detection is pretty much what it sounds
like finding things that are violent or
maybe adult-themed
in your images this is actually
surprisingly or at least surprisingly to
me a really really common thing that
people do with the vision API is the
explicit content detection so if at any
point in your application you maybe have
users submitting images from the
internet so random internet users
submitting photos what could possibly go
wrong
so there's a couple of ways you could
handle that traditionally right is you
upload an image and you just immediately
put it into some queue or some
quarantine so that a human being looks
at it and says like alright this photo
is good it's nothing too weird
let's posted but then you don't get like
real-time social interaction so a happy
medium can be to run your image against
the explicit content detection and just
come it comes back with a score and if
it's below a certain score just go ahead
and post it live if it's above a port a
certain score maybe block it or put it
into some queue for a human to review so
you can get that kind of real-time
social responsiveness in your
application landmark detection you give
it a photo and it can tell you oh this
is a famous landmark around the world I
know what this is and I know where it is
I'm in logo detection also kind of what
it sounds like if can identify famous
logos in in photos so what are some
real-world examples of people who are
using the cloud vision API so Disney
came out with this movie called Pete's
dragon and they wanted to promote it so
they created this game
so the name of the dragon in the movie
is Elliott and the way the game works is
a scavenger hunt so it gives you some
clues so it would say Oh Elliott the
dragon he's hiding behind the couch or
he's next to the lamp so go find the
picture of the lamp or go find a picture
of a couch and you can reveal Elliott
the dragon so you would take your phone
you'd go to the couch you go to the lamp
you take a photo it would run it through
the vision API and say yes this is a
picture of a lamp and they would overlay
the dragon in your photo so you sort of
you find Elliott the dragon as part of
the experience so very fun application
using the cloud vision API to see if you
actually were able to define one of
these clues and show you the dragon
realtor.com so these this is a company
that does houses for buying and selling
of houses listings so say you're walking
by or driving by a house that you're
interested in so there's a few things
you can do you know you can like write
down the URL that's on the sign
you can like write down the name of the
agent you can write down the address or
using the realtor app you can actually
take a photo of the for sale sign it
uses the OCR the text extraction it
actually reads the for sale sign that
you took a photo of searches the realtor
database and then returns you the
listing information so much much easier
than like trying to scan a QR code or
actually write things down so these are
a couple of real-world applications of
the cloud vision API so let's talk a
little bit more about face detection not
facial recognition face detection so it
has identified three photos here there's
actually a photo of Sarah I myself and
another colleague we were in in Petra in
Jordan last year just kind of a fun
little side break during a conference
and we were taking this like
three-person selfie to be ridiculous and
it has correctly identified that there
are three faces I don't think it was
recognizing the camel faces yet but
maybe someday and it tells you some
information about it so first of all it
tells you where there are faces so a
bounding box give you some coordinates
in the image but then also let's pick up
let's pick a face I'll pick a totally
random face from someone in this image
maybe I don't know Sarah and see some
information about her
so it says head where likelihood like is
she wearing a hat or head wear of some
kind very unlikely that's cuz she's not
so I
wearing one and our colleague Robert is
wearing one so that will return likely
that we're wearing head where joy
likelihood very likely it's vacation
there are camels you can pet why not
very very likely to have joy there's
four emotions it wouldn't attacked
sorrow surprise anger and joy so you can
kind of actually do sentiment analysis
right like if you have a photo of people
interacting with your product or
interacting with your service you can
programmatically see people are really
mad when they use your site or they're
really happy hopefully but also you can
identify different features of the face
so where are the eyes where's the nose
chin different things like that so
landmark detection you give it a photo
of a famous landmark and it tells you
what it is right so here's a photo of
the Eiffel Tower you run into the cloud
vision API and it says I felt ow
no it says Paris Hotel and Casino in Las
Vegas it's actually not the Eiffel Tower
and the cloud vision API was not fooled
it was actually able to detect this is
not the Eiffel Tower so here's some of
the JSON is returned again it gets a
score eighty so it's like eighty percent
confident that it's the Paris Hotel and
Casino in Las Vegas it also has like
latitude and longitude like where in the
world literally is this thing and also
the first entry in the JSON response is
called the mid-19th graph API so as
Google has been cataloging the world's
information for search over the past
many years you know it's it's valuable
to know that the the text string Eiffel
Tower is not literally just the letters
e iff e l right like Eiffel Tower is the
thing it's an entity it's a landmark
it's a tourist attraction it's a public
building it exists in Paris like what is
the Eiffel Tower right so this is a key
into the knowledge graph API to actually
get data back from Google like what do
we know about this entity so if I were
to feed at this M ID it would actually
tell me all that information and often a
link to like a Wikipedia page to learn
more if I want to know more about like
what is this entity so if it does this
it does this for a lot of things in the
vision API a few more things besides
just types of detection that it can do
so it is crop hints so it can give you a
suggested bounding box of like where is
the most interesting feature of this
it's sort of within this photo like
where would you crop it to sort of zoom
in web annotations which I'll talk about
in more details in a second it basically
because we've catalogued a lot of images
on the Internet
we can sort of cross-reference the image
that you have analyzed with our internet
with our internet catalog of images to
get more information and then text
annotations so this is basically if you
are so in the example I showed is taking
a picture of a street sign it's just a
few words like no parking here but what
if you were to take a picture of like a
menu or a page from a book something is
like a lot of text so this basically
lets you do bulk text reading from just
a photograph so web annotations well
like I said cross-reference your image
with images from the internet so this is
a a photo of a car and what does web
annotations come back the first thing it
comes back with is this the make in the
model so Ford Anglia and again here that
mi di talked about so if you wanted to
know what the hell is a Ford Anglia you
can actually get more information from
the knowledge graph API but it's a
little bit more than just a Ford Anglia
you'll notice that it seems like a
little bit more going on there's like a
portrait in the background there's some
like strings of holding it up so it
turns out it actually knows that this
car is on display the art Science Museum
in Singapore so that's already a little
bit more information than just we could
get like literally from the pixels and
this image and not only is it on display
in the art Science Museum in Singapore
it specifically is in the Harry Potter
exhibit in the art Science Museum in
Singapore and so actually references to
Harry Potter the literary series so if
you've seen the movie you know that they
basically steal this car to go fly to
Hogwarts because they miss the train and
you could actually well it won't tell
you that much but but you could you can
see that it's from the literary series
so literally more than just knowing that
it is a car but like what car and where
because it can cross-reference this
image with other images of things that
it knows about on the on the Internet
and if you want to know where did you
where did we find similar images on the
Internet to get this cross reference
information returns that data as well so
if it found your exact image somewhere
on the internet it will return this full
matching image and if it found a rough
approximation of your image you'll get
those URLs as well both the URL of the
actual like jpg or whatever or the page
on which that image was found so kind of
a sneaky usage of this API is you know
we've all been to these like forums or
whatever you post an image and you check
this box is I certify that I have the
rights to this image this is mine and
I'm gonna upload it right and everyone
just checks sure even if it's not your
image what you actually could cause
reference that image using in cloud
vision API and say not so fast like
clearly you just search the Google Image
Search to find a picture of this because
we found it on all these other pages
right if you wanted to see is the
content that people are uploading does
it exist elsewhere is this really their
image on this might be a way to do that
all right so that's conceptually what is
a cloud vision API let's see a little
bit of a code in the live demo so first
here's an architecture diagram of the
cloud vision API in in action so I'm
gonna use something called cloud
functions from Google cloud platform so
cloud functions are these like
serverless little functions basically
they'll no js' JavaScript functions that
run automatically serverless ly you
don't to manage anything you just give
it a code and these these functions run
in response to events that happen in the
cloud so in this case it will respond to
a file change event in cloud storage
which is basically how you save binaries
data or blog files blob storage in
Google Cloud so will save a picture of
this adorable koala will automatically
trigger a call to a cloud function which
will call it to the cloud vision API and
that we just talked about
get back all that analysis data save it
as a JSON as a JSON blob and then put it
in say like a gist into github now
normally what you might do in Annacone
on sort of list world is if you want to
do some additional processing like maybe
create a thumbnail of this that would
just be the next step in the code but in
sort of a functions of the server list
serverless
functions as a service comma service
comma micro-services comma buzz word
world we actually could have another
service that does that separately right
so we just have another cloud function
totally independent from the first one
because it's not a chain it doesn't need
to know anything about the vision API
and it separately of the same trigger
will run
do a resize and maybe make a tiny
thumbnail or one thing that you see a
lot of is you basically pre render
images for different media formats so I
have a big image and I want to scale a
tiny one down for a thumbnail maybe a
smaller one for a phone version a medium
one for a tablet or desktop and so forth
whoops so what does that code look like
so here's my function called analyze
image and I want like dig super deep
into it but just at a high level so a
cloud function has an event parameter
and the event this basically generally
looks the same for any kind of cloud
event but within it is something called
event data and that has property
specific to this type of event so in
this case is a file change event in
Google Cloud storage so we're going to
create a reference to the image using
this data media link and then we're
gonna call the cloud vision API that we
just talked about so you'll notice that
this is actually all the code there's no
service accounts there's no API keys
there's no OAuth that is because by
default most of the code that you run
and you go in Google cloud platform runs
Auto authenticated to make other API
calls within Google Cloud so this is
really nice because if you're moving
some code from say development
environment to staging environment to
production environment you don't have to
actually manage and ship a lot of these
API keys and tokens you will
automatically be authenticated to the
project in the environment that you're
running in so it makes that a lot easier
I mean you can supply via special
credentials you can but by default it's
Auto authenticated to call Google Cloud
api's so run some analysis get a
response back write it to the github
github gist a little bit too much code
to put on the slide so you can use your
imagination actually we have it posted
on github there's a link at the end so
let's actually see this in action all
right so this is our Google Cloud
Storage file listing it there are no
objects in this bucket because there's
nothing so let's upload a picture of an
adorable koala but it's actually gonna
be a specific photo I was stalking
Sarah's Instagram earlier and she went
to Lone Pine Koala Sanctuary in Brisbane
and got this so I may have may have
stolen this photo off her Instagram so
sorry Sarah when when you say hey go
ahead and set up your dimmer on my
laptop be careful because who knows what
other things I will do so we're gonna
upload this koala and it's going to
automatically trigger those two cloud
functions want to do the resize and then
one to do the cloud vision API analysis
and so if we tab over here we can see
what is it
4:40 all right here we go so probably
this line function execution started
downloaded file so you can see extracted
labels mammal fauna fauna marsupial
koala object object that is my favorite
kind of animal and then also where is
thumb yog created koala thumb so you can
see our two cloud functions running so
that's beeps let's go back and see did
we create a thumbnail maybe uh-huh yeah
so there's a super tiny version that it
created and then we should have a gist
github gist link let me grab this oh is
that the whole thing I don't that's yes
got it you can tell this is live because
it's not working yeah maybe hold on I
just have to like it copied the whole
log line that's fine it's always very
exciting to watch someone edit text boom
alright so here is the gist they created
saved it to github with that same mammal
fauna fauna marsupial koala Wildlife
data so super cool so that is the cloud
vision API so I'll hand it back to Sarah
to talk about whoops just kidding before
you put this in your application you
actually can play with the cloud vision
API just in your browser right just on
your desktop upload an image drag and
drop see what the vision API has to say
so if you have you're thinking about
maybe using the cloud vision API but you
don't actually like write a prototype
app you can actually literally just in
from your browser drag and drop some
images it'll give you the JSON response
and it'll visually give you response to
the API what kind of analysis it does so
you can see like if this is a good fit
or not for whatever use case maybe
you're thinking of
oh and by the way if you were wandering
going back to that dog and mop example
how good is the cloud vision API at
telling the difference between a dog and
a mop or a broom so we fed all eight of
these to the cloud vision API for the
dog 99% confident that it's a dog and in
fact even the Komondor is 77% act
confident about the specific breed I
don't know what a dog like mammal is but
it's 76% confident but I remember
learning about that one in school yeah
maybe a dingo was a doglike mammal as
well we'll say we could if we have time
afterwards we'll try it so we've got a
dog right got this right we said if it
came back with broom or mop like we'll
give it credit for so we ran it on all
eight and here's how it did from the
dogs three out of four I got dog or got
to breed specifically this one it got
fur which even looking at it from my own
eyes like what in the world is that
thing
oh great in a card we'll give her credit
for that one as far as the the mops and
the brooms at the bottom one of them
done when it got textile rather than
identifying it so I don't know if we can
be that generous with our grading but
still seven out of eight for a test that
is intentionally designed to be
misleading is I think pretty decent so
now I'll hand it back over to Sarah to
talk about the natural language API
thanks Brett
so now we're gonna switch gears and talk
about text analysis with the natural
language API we also have a handy google
cloud node module and if you want to use
the not the National image API in your
node application just NPM install at
Google cloud slash language and what the
national language API allows you to do
is to get a better understanding of your
text so you can do you can extract
entities sentiment and syntax from text
that you pass to the natural language
API I'm gonna dive into each of these
methods and then we'll see a live demo
so first I'll talk about the entity
extraction endpoint I'm big Harry Potter
fan as I mentioned before so I just took
this sentence from Wikipedia about JK
Rowling and wanted to see what the
entity analysis endpoint would return so
I passed it to the API and returned
these five entities and we can look more
and see what types of JSON responses we
got back for each of these so what's
interesting here is that the first three
entities are actually all pointing to
the same thing they're just different
ways of referring to JK Rowling Robert
Galbraith has a pen name shoes for a
different book series but I won't get
into details on that so it was able to
API was able to normalize these identify
that they're all referring to the same
reference so in the JSON we get back the
name of the entity the type which is
person and then we get some additional
metadata so we get the mi dewitt brett
spoke about before which was an ID that
maps to JK Rowling in Google's knowledge
graph and then we get the Wikipedia URL
for the Wikipedia page about her so
these all points the same one for
British we get that it's a location we
get the Wikipedia URL for the United
Kingdom page so if this had instead said
United Kingdom born novelists it would
have pointed to the same entity and then
we get a similar response for Harry
Potter and if you have entities in your
texts that don't map to Wikipedia page
you still get an entity response back
and it just has an empty metadata object
so if it had my name in here Sarah
Robinson it would find it as an entity
it just wouldn't have a Wikipedia page
so I don't have one yet maybe one day so
that's our entity analysis the next
thing you can do with the natural
language API is analyze sentiment from
your text so if we had this which is
what you might see as a restaurant
review the food was terrible I would
definitely not recommend this restaurant
so if you worked at this restaurant
chances are you probably want to flag
this review and maybe follow up with a
customer but you probably don't want to
read all the reviews maybe you only want
to find the most positive and the most
negative ones and then take some sort of
action on those follow up about the
experience so what you can do with the
sentiment analysis endpoint is you get
two values back you get score which will
tell you how positive or negative is
this text ranging from negative 1 to 1
so for this we get negative 0.9 it's
almost fully negative and then we also
get magnitude which tells us regardless
of being positive or negative how strong
is the sentiment in this text and this
is going to be arranged from 0 to
infinity normalized based on the link
of the text so in this example this text
is relatively short it's just a sentence
so we get 0.9 relatively small number so
that's sentiment analysis and the next
thing you can do with the natural image
API is get a bit more into the
linguistic details of the text so if we
take this sentence as an example the
natural language API helps us understand
text we get a couple of things back and
this visualization just shows us the
JSON we get back what it looks like so
the first thing we get back is a
dependency parse tree which basically
tells us how the words in a sentence
relate to each other which words depend
on other words and JSON will give you
details on how each word relates to
other words in a sentence then we get
the parts label which is the role of
each word in a sentence so helps is the
root verb api's nominal subjects the
period at the end is punctuation details
on what each word is doing in the
sentence then we get part of speech
which is is it a noun a verb pronoun or
adjective we also get dilemma which is
the canonical form of the word so here
we just have one lemma for helps we get
help and this is useful if you're
counting how many times a particular
word is used to describe something in
your application you probably don't want
to count helps and help as two separate
things you probably just want to count
them as as one thing and you can use the
lemma of the canonical form to do that
so like if this head and said set
understands we would also get a lemma
back for that as understand and then
finally we get additional morphology
details for each token in the sentence
and this is gonna differ based on which
language your text is in that you send
to the API a few weeks ago we announced
some new features from the natural
language API so when it first launched
it supported English Spanish and
Japanese and we now support a couple
more languages and another thing we
added was an entity based sentiment so
instead of returning just one sentiment
value for your entire document it can
now tell you the sentiment around
specific entities I'll show you that in
a moment and then we're continually
improving the models behind the
sentiment and entity analysis and the
API and so I'm gonna go to brief demo
if I look at see the natural language
api product page you can actually try it
out in the browser and generate a
visualization like we saw before so what
I just want to show here is the entity
based sentiment so if I am writing a
restaurant review and I say I likes the
sushi but the service was terrible
we're gonna see what the natural image
API has to say about that so we can see
here that we get sentiment for each
entity so here the entire document
sentiment probably wouldn't be as useful
to us but here we can see that sushi
returned a score of 0.9 on a range of
negative 1 to 1 and service returned to
score of negative 0.9 so super useful
granular information there we also get
the full document sentiment again in
this case the entity based sentiment is
much more useful and we get that
visualization which I showed you in the
previous slide so we'll share the link
to this page at the end you can generate
your own visualizations on texts that
you pass the API I wanted to talk about
a company that's using the natural
language API in production this company
is called whoo check and they are a
customer feedback platform so what they
allow their customers to do is gather
feedback as their customers users are
moving through different pages in their
application and Moo tricks job is to
help their customers make sense of this
feedback so if you've ever seen a form
like the one on the top right here where
you're on a website and it says on a
scale of 0 to 10 how was your experience
on a specific page so you give it a
numbered score and then it might ask you
for open-ended feedback so it's pretty
easy for bootrec to make sense of this
numbered score obviously from 0 to 10
but what's much more difficult is to
make sense of this open-ended feedback
that you're getting and they're actually
using each of the natural language API
methods to make sense of this feedback
and so they use sentiment analysis to
just gauge did the numbered score they
gave line up with their feedback so
maybe they gave it 10 but then they were
actually complaining about something so
that they can normalize that and then
they're using entity and syntax analysis
to extract on the topics of each piece
of feedback and see how people are
talking about that specific thing so
maybe they
have a high priority customer who is
angry about usability instead of having
to read the entire piece of feedback
they can then route that in
near-real-time
to the right person to address it so
just an example of company using this in
the real world next I wanted to show a
demo highlighting the syntax analysis
feature of the API and the way that done
that works as I wrote a node script that
calls the Twitter streaming API and what
this does is it lets you stream tweets
based on a particular search term so in
this case I'm looking at all the tweets
about with Australia in them now the
streaming API doesn't give you all the
tweets in the firehose it just gives you
a subset of them so we're gonna grab a
subset of tweets about Australia and
then I took the text of each tweet I ran
this for a couple of days sent it to the
natural language API and the natural
language API gives us a JSON response
back and then what I do with that JSON
response is I store it in a bigquery
table bigquery is a big data as a
service tool that we offer on Google
cloud platform it's fully managed lets
you query lots and lots of data really
fast so I add that into our bigquery
table and then we can do some analysis
on this Twitter data so if I go over
here to the browser here we can see a
subset of what the data in my table
looks like so I've got the ID of the
tweet when it was created the hashtags
in the tweet which is returned from the
Twitter API and then I get this giant
JSON string which is the response from
the natural language API this is the
bigquery web UI so you can use it to
inspect data in your table you can also
write queries directly in the browser
you can use the bigquery REST API to
write queries as well but in this
example we'll show you in the browser so
you might be wondering if I'm writing a
sequel query how am I gonna parse this
giant string and the answer is bigquery
has a feature called user-defined
functions which lets you write custom
JavaScript functions that you can run on
columns in your table so if we happen
over here I have written a function that
is going to look at the most common
adjectives used on Twitter to describe
Australia in the past couple days I just
ran the script for a couple days so
what's really crazy here is it's running
this custom JavaScript function on all
the tweets in my table when it's done
running I'll look at exactly how many
tweets I'm running this on so
essentially what we're doing here is
we're taking each token so the natural
language API returns a token for each
word in our text and it tells us the
part of speech so we're just looking at
the part of speech tag isn't an
adjective if so we'll count it seems to
be taking a bit of time while it's
running let's look at some details for
this table so it's running this over one
hundred and forty eight thousand tweets
and this is the schema of my table so
I'm getting an ID of the tweet the text
when it was created the hash tags which
you saw the tokens which is that string
return from the natural language API and
then the score and magnitude from
sentiment analysis usually doesn't take
this long but live demos you never know
so I'm gonna hop on over here to my next
query and run that so what this is gonna
do is we'll see which query finishes
first it will be race this one is going
to look at the emojis in my tweets so I
love emojis I want to see which emojis
people are using on Twitter when they
talk about Australia so this is just
looking at a particular character code
that makes up an emoji to see if the
token is an emoji and then we're just
going to do a running count of those
this man is still running usually it
doesn't take that long so I'm gonna look
in my query history here and see if I
already have version of this let's see
about one second
hmmm alright well that doesn't appear to
be working this one is still running -
all right live demos you never know all
right I'm gonna keep these running and
we can pop back to them at the end see
how they do so let me just check this
one yes still running maybe it's a Wi-Fi
issue I'm not sure um okay I couldn't
find that other one all right I'm gonna
hop back to the slides and that works
you'd be able to see the most common
adjectives people are using on Twitter
along with the most comment emojis just
to show you an example of what you could
do so this is this is what the output
would look like if we counted the emojis
I ran this one on a different data set
and then I created an emoji tag cloud
just to see a way to visualize which
emojis people were using most often to
talk about a specific topic just another
example of how you could visualize the
data so now we talked about natural
language I'm gonna hand it back over to
Brett to talk about the speech API all
right thanks so this is speech API this
is basically a spoken speech into text
and again as Google we've been training
these machine learning models over years
and years on things like ok Google oh no
one usually I get at least someone's
phone to go off it ok Google but anyway
what's it but I have a very like
universally common no at least so you
optionally yes so that's actually that's
actually one thing that's alright so now
you can actually personalize it to your
voice but if you don't then it's just a
generic voice model so I usually get
someone with that so anyway if you want
that functionality where you can say ok
Google and as your user is speaking it
can be like transcribing their spoken
word into text that is what the cloud
speech API can let you do so it supports
over 80 languages actually as of two
days ago no longer is at 80 languages it
is now over a hundred and ten languages
so you all are probably the first people
yes
y'all probably the first people actually
to hear in person about this release so
we're very very excited we've added over
over 30 more languages to to our speech
API so if anyone speaks a weird
obscure languages language I may ask for
volunteers in a second well we're not
there yet Australians too weird that's
it's indecipherable even by machines so
another thing that we added the other
day is time offsets so this basically
when you give it an audio file it will
turn it into text so the famous Abraham
Lincoln speech four score and twenty
years ago it can actually it'll
transcribe the whole phrase of the whole
text but it also will give you
timestamps for individual words so for
example the word for it will say it is
between 1.3 and 1.4 seconds that's like
the snippet of audio or that specific
word exists in your audio stream so
really really useful for doing like
extractions of where we're in this giant
audio file on this particular word come
out and that also was brand new we just
released that so a customer that's using
this in real life so azar basically is a
chat app where I can talk into my app
but I can talk to other people in other
countries who maybe don't speak the same
language that I do
so it basically dovetails this speech
API with the Google translation API so
that I can speak in my language it turns
it into text translates it into the
other person's native language so they
read what I'm saying in their language
and when they speak back it comes to me
in my language so really really cool
sort of opening up these opportunities
for these awkward selfie conversations
around the world so very very cool so
I'm gonna show a demo where I'm going to
basically create an audio file we're
gonna use a tool called socks that's
basically a command-line utility that
records audio input from my laptop
create a JSON request send it via HTTP
POST to the API and then we'll take a
look at the response so I'm gonna do it
literally just using curl and
command-line but again there's a nice
nodejs wrapper for this as well as
dotnet Python
Ruby and all that good stuff so that's
exciting I don't think I I think are we
sure I think this might be a Wi-Fi issue
because this should not take that long
yeah I'm just gonna refresh this who's
staying at Hilton can we have your
access code yeah so the bigquery counter
is client-side so he was disconnected to
the internet it was just gonna run
forever and ever but don't worry it's
not gonna bill you for that all right
well we'll come back to that let's see
did this work let's go to my favorite
I'm sorry all right looks like it's
working it's always something weird in
political in there so hopefully that
didn't nothing weird came up all right
so I created this command line script so
I can say something and it will turn it
into speech so hello Sydney hope
everyone is still awake today all right
so here is the JSON request that we're
about to spin send to the API so here's
some data about the actual audio file
itself a sample rate its encoding type
the language so English US speech
contacts so this is basically phrase
hints so the cloud speech API is train
it's a general-purpose machine learned
machine learning trained model which
means it's designed for just general use
well in your specific applications or in
your business you probably have certain
terms that just aren't in common your
like technical terms or like you know
weird product names or whatever that we
just don't know about so what you can do
is you can actually supply us phrases
words as part of the transcription
request so we know there's like these
terms that we don't really know about
but we can now use it to take a best
guess that maybe I'm saying something
that the cloud speech API might not
normally know about Mac's alternatives
so this is it will give you like I'm a
its best guest but then you can request
hey but also give me one or two or three
or four maybe like alternate
transcriptions that that maybe I can
inspect visually manually to see if it's
a better transcription and then here
we're actually in
baiting the audiophile in the request
itself so we're doing a base64 encode
putting it in the JSON but if you had
say like a bunch of audio files that you
wanted to transcribe you could actually
upload them to say like Google Cloud
storage and then actually just send the
like a link there and say transcode this
file so you don't actually embed it in
the request itself so if you want to do
like a big batch transcription of audio
so let's send it and assuming we're not
still on the Hilton guest Wi-Fi login oh
here we go
so hello Sydney hope everyone is still
awake today are we alright yeah sweet
well I'm 97.6% confident that we were
still awake which is a coincidence
because that's how confident it was that
it correctly transcribed my audio so
that's English as you imagine this
product is developed in Silicon Valley
in the States so my American accent is
it does pretty good so he here speaks
like a different language than maybe we
can try and see how good it is all right
look you want let's let's see if we have
the language code where is this thing
where am i all right
we enough medical order here oh how do
we spell it pas
nope so I also don't think this language
code thing has been updated since we
added new languages strike one
so I'm gonna get another one alright
here we go
all right you want a volunteer to come
up and speak that so I'm gonna use FA as
the language code anything you want
please don't swear but so you stand here
and then it's just it's the mic of the
laptop so whenever you're ready think of
something particularly hilarious notice
getting so just hit enter when you're
ready and it will run for it will record
for five seconds and so male host epsp
all right so let's see it's going to
turn on that and we will probably need
you to verify if it was even close
just even close all right so let's see
things of the power of the translation
API I will translate to English this
phrase and see what did it come back
with sweaty head that's what you said
right yeah as I say we released this two
days ago so maybe there are some bugs
but uh but thank you for volunteering
for unrelated reasons sweaty head was
actually my nickname in high school but
probably I shouldn't explain why what
did you actually say that was the Google
truth API injecting and solving and
correcting your statement so thanks
alright and now I will turn it back over
to Sarah for the video intelligence API
thanks Brett so the video intelligence
API is our newest machine learning API
and as with the other API is we have a
node module you can use to interact with
it I'm sweeping this run npm install at
google cloud /video - intelligence and
what the video api does is it lets you
understand your videos in the same way
that brett described so the vision API
lets you understand photos this lets you
do that sort of analysis at the video
level so you can understand your videos
entities that shop frame or high level
video level so the best way to
demonstrate this is through a demo and
i'm gonna hop on over here and make this
a little bit smaller so here's our demo
this is a video of a Superbowl
commercial for Google home and I'm gonna
play just the first couple seconds of
the video so we've got a lot of scenes
in this video it starts with a mountain
pass then we have a street scene a dog a
garage I'm not gonna play the whole
thing but the idea here is that there's
a lot of different scenes changing a lot
is happening in the video and if we were
to manually analyze this we would have
to watch the whole thing write down what
was happening every scene and then maybe
store that in a database somewhere and
the video API actually does all of this
for us so all we need to do is send it
our video it returns us back JSON
response with details
exactly what is happening in every scene
and then in addition to that granular
analysis it'll tell us at a high level
what is this video about so maybe we
have a video with costumes and candy
it'll tell us it's a video about
Halloween so it does some analysis
aggregate analysis on all the detailed
scene level data that we get back so
what I have here below the video is
basically just a visualization of the
JSON that we get back so it tells us
that it's there's a dog in the video and
it knows exactly when that dog appears
and tells us there's a cake at the end
if we scroll down we can see some more
data it tells us even what breed of dog
that was and if we look here it also
identifies that mountain pass from the
beginning scene which is pretty cool so
this is what it can do with one video
but if you're using the video API
chances are you have lots and lots of
videos that you want to analyze so if we
go here to the home page of my app I've
got lots of videos here let's say for
example that I work at a media company
and I've got petabytes of videos sitting
in storage buckets let's say
specifically it's a sports media company
and I've got tons of hours of sports
footage and what I want to do is create
a highlight reel this is something you
might want to do if you have a lot of
media is fine all the clips specifically
of one thing maybe baseball so I want to
find all my baseball videos and compile
them into maybe like a minute long
highlight reel so with the JSON data
that we get back on each video makes it
really easy to implement search across
our entire video library so if I search
baseball here you refresh that it tells
us not only which videos have baseball
but it tells us exactly where we can
find the baseball scenes in those videos
so this one this video is almost
entirely about baseball this is my
favorite example here because this video
every year Google publishes a year in
search video highlighting top searches
from that year so lots of different
things happening in this video there's
only one moment where there's a baseball
clip in the video and if your job was to
find that clip there's a chance you
might even miss it if you're watching
this video it's only like a two second
clip you might skip over it accidentally
but the video API is able to pin
point that clip directly so if we go to
it here we can see indeed there is a
baseball clip in this video this is from
last year when the Cubs won the World
Series I'm from Chicago so I was pretty
excited about that making-meter in
search results so that's what the video
API can do and we'll do one more search
so it's winter in Australia right now
although to me this is very mild winter
living in New York but I think we can
agree it'd be nice to be on a beach
right now so machine learning cannot
take us to a beach but it can show us
all our Beach videos which is the next
best thing right so I'm gonna search for
all my Beach videos and even if the
video is not of you know entirely about
a beach we can find just those small
clips where we find a beach in our video
so here we can skip to those find all of
our beach scenes across our videos we'll
show it one more um so this demo just
kind of shows you how the video API is
kind of transforming how you analyze
videos so something that even just a
couple of months or years ago would have
taken hours with the video API it just
takes seconds or minutes to transcribe
before I jump back to the slides I want
to revisit these demos okay it finally
finished it was a Wi-Fi issue thanks
Brett so here we can see the adjectives
from that Twitter analysis that I ran
and oh we didn't refresh this one so
still running but I'm gonna refresh that
and let's see I'm gonna search my
queries
I think it was project query there we go
all right this is the correct emoji
query hopefully it won't take 900
seconds I won't make you wait that long
but we'll give it just a couple seconds
to run again this was using the natural
language API to find our most seasoned
OGIS there we go
so maybe there was some sort of big
musical event happening people were
using a musical note emoji these are the
most common emojis used so back to the
video API that was a demo the video API
and I wanted to talk momentarily about
the architecture of the demo app and so
how it works is all of the video files
are being stored in a Google Cloud
storage bucket which you saw earlier
with Brett's cloud functions demo and
I've got a cloud function listening on
that bucket so that's triggered whenever
new files added to the bucket it will
check is it a video file if it is it
sends it to the video intelligence API
for processing and one thing you can
pass in your video API request is the
URL of another cloud storage file where
you want to write the annotation JSON
when it's done processing you don't have
to do that you could take some sort of
action once it's done but in this
example I'm just having it when it's
done right to a cloud storage bucket so
I've got the videos in one storage
bucket and then the video metadata in a
separate storage bucket and in the front
end is a node.js app that's running on
Google App Engine App Engine is our
platform as a service products on Google
cloud so the front end of the app
doesn't actually directly call the video
API all it does is it grabs the
associate the videos and then it grabs
their associated metadata and it
implements the search functionality
across that JSON so I built it with
another developer Alex wolf if you liked
it give us a shout out on Twitter and
then just a look at what the JSON looks
like for label detection here's a video
that has a bird's eye view scene in it
and indeed label detection is able to
identify that there's a bird's eye view
and what we get here is a segment object
so this tells us in microseconds what
was the start and end time that the
particulars in this case a bird's-eye
view appeared in our video if it
appeared more than once we've got
multiple segment objects it also gives
us a confidence score how confident is
it that it correctly identify this label
so in addition to label detection I
don't have a JSON snippet for it but it
can also do shot change detection which
means show me all the times at the scene
like the camera moved to a different
scene in the video so it'll just give
you the timestamps of all the times the
scenes changed so that was the video API
so we just gave you an overview five
different machine learning ap is on
Google cloud but we've got one more
thing in our talk title we promised big
data so we wanted to end with some big
data analysis specifically related to
JavaScript and node and to do that we're
going to use this bigquery table and we
have a public bigquery table that
contains all the public code on github
it's a giant table it's over 2 terabytes
and it's updated frequently this is just
a visualization to show the breakdown of
public code on github we look at it by
bytes we can see that see accounts for
63% of code and by repositories
javascript accounts for the most with
14% of total repositories being
javascript repos so there's all sorts of
interesting analysis we can do on this
on this github table the first thing I'm
going to do is find all the JavaScript
files on github so I'm gonna hop back to
the browser and we've got another
bigquery tab here and just to show you
what the table looks like there's a
couple different tables that have this
data so we have the contents table which
contains the contents of every single
file in the head branch on github and
this table is the biggest one it's over
2 terabytes and preview of what it looks
like here's what a row in the table
looks like and these files are empty but
if I skip ahead we can find some files
that contain text so that's what this
table looks like and you can easily join
that with the files table to do more
analysis we're going to query the files
table in this query example
so this table is a little bit smaller
has the file path of every file in the
head branch on github along with some
other metadata so here we're gonna count
how many JavaScript files there aren't
github and while it runs does anybody
want to guess how many they think there
are all of github oh nobody guess all
right well the answer is over 300
million which is pretty crazy that is a
lot of JavaScript files on github that's
a relatively simple query so the next
thing I wanted to look at was can we do
any sort of analysis on what the most
popular node modules are based on this
data so if you're a node.js developer
you know that the way you define your
dependencies is with a package.json file
and using bigquery's user-defined
functions which i talked about
previously it's pretty easy for us to
write a JavaScript function to parse the
package JSON we just need to look at the
dependencies object and then count the
different dependencies within that block
so if I go over here we have a query
already written to look at all the
package out JSON files on github now I'm
not gonna run the full two terabyte
query here because i wouldnt want to
make you wait for however many seconds
it would take to run so i've cheated a
little bit and i've extracted all the
package JSON files into a table so it
doesn't need to do the full scan it's
already got the package JSON files so
this query is only gonna process almost
7 gigabytes instead of 2 terabytes so
I'm gonna run it here and what's really
cool about these user-defined functions
is that it's doing this complex string
parsing and it's running this on every
single package of JSON file on github
which is kind of crazy so if I look here
at the table that I'm this is the table
I extracted with all the package JSON
files there are 2.7 million of them and
it's running this custom function on all
those files which is kind of crazy and
something that previously would have
taken a lot of time and resources to do
while that's running oh I was gonna ask
if anyone had guesses any other guesses
on most most common node module
for eight which one I don't know that
one all right let's see if that's in the
top Express this is number one low -
debug I don't see Google Cloud vision in
here but you never know so these are
some of the most most common ones I'm
guessing a lot of you are familiar with
these so if we go back to slides so this
is what the full query would look like
if we just have this nested Select to
grab all the package of JSON files on
github and just to show you that we
actually did run that query but we don't
want to make you wait for it to finish
processing it took 238 seconds which is
actually really really fast considering
it was gonna process 2.2 terabytes of
data we ran that earlier today we have a
strange not to prove it so that's all we
have I encourage you to try out all the
machine learning api's and products we
talked about and Brad talks about the in
browser demo for the vision API we have
a browser demo for all five of these
api's so before you write any code you
can hop on over to the browser try out
these api's see what the response
actually looks like and we have the code
for all the demos we showed in the talk
in various github repos so definitely
encourage you to check it out if you
have any questions I'm all respond to
hashtag sweaty head
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>