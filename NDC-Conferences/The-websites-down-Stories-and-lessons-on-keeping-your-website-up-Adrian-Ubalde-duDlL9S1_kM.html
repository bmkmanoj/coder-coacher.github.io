<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;The website's down!&quot; Stories and lessons on keeping your website up. - Adrian Ubalde | Coder Coacher - Coaching Coders</title><meta content="&quot;The website's down!&quot; Stories and lessons on keeping your website up. - Adrian Ubalde - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>&quot;The website's down!&quot; Stories and lessons on keeping your website up. - Adrian Ubalde</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/duDlL9S1_kM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone welcome to
websites down stories and lessons on how
to keep your website up my name is
Adrian and Balda and I'm a site
reliability engineer for the property
website domain so I'm gonna start off by
talking about particularly stressful
exciting time at domain earlier this
February where the website went down a
handful of times in in that one week
occurred and I now refer to its hell
week and then I'm gonna talk about how
we faced those challenges and had to
figure out how to improve our current
website in terms of liability and
resilience to failure and how we were
able to apply those lessons to
applications that we would build from
now on so hell week it was a let's just
say it was a really bad time in terms of
availability the website went down on
three separate days in the space of that
one week so we had a brilliant start to
the week out with our first outage
around 14 minutes that Monday evening
thankfully we were able to bring the
site back online by simply resetting our
web servers and afterwards after looking
through our logs and looking at all the
system health data for our various
back-end api's one of which is a Redis
cluster that we use for storing user
session data we we were you know being a
pretty high traffic website you were
expecting to see quite a lot of traffic
to the Redis cluster we get you know
upwards of a thousand Jesus sessions
during peak time but after looking at
the data we noticed an abnormally high
spike in CPU and read requests so our
hypothesis was that there was a problem
with how our website was using our Redis
cluster
for session state so unfortunately just
two days later we had our next outage
but unfortunately no amount of resetting
our web service was bringing back our
website online at least not in a stable
way for more than a few minutes in fact
we had a total of 21 outages in that one
afternoon for a total of about an hour
of downtime so we had an emergency
meeting that afternoon where we came
together to diagnose those outages and
try to figure out a way to stabilize our
website and after going through the
various options we had we decided that
we would replace our Redis session store
with a DynamoDB managed service session
store and in the meantime while we were
developing that code change we were
going to switch over to an imprecise
memory session store and thankfully that
brought back our website and stabilized
it and after about an hour after we'd
finished the code change we switched it
to have our servers over to DynamoDB and
crossed our fingers and thankfully our
website remained stable and upper did
about this time of that day was about
9:30 so we decided all right we're just
gonna leave it as is and monitor it
overnight and then the next morning we
switched all our servers over to
dynamodb so all was good until two days
later we had our final outage of that
week like the previous incidents we saw
a dramatic spike in the number of read
requests to our session store so that's
when we decided we'll just switch over
to a pure in-memory session store mode
and then would go through our website
functionality and just try to remove or
at least refactor anywhere we relied on
session state so now that we we came out
of that week of outages we now had a had
the chance to step back and and look for
ways that we could improve our website
in terms of reliability and resilience
to failure but not just our current
website but any of the applications
would be building from now on but as
part of that we wanted to also evaluate
how well we did in terms of reacting and
handling theirs outages during that week
and one of the biggest one of the
biggest challenges was how hot and how
hot it was
- like quickly and and effectively go
through our log files and one of the
challenges was that we just had all
these all these log files spread across
all our all our servers and now the
challenge was that not all our
developers had access to our logs and
once yeah once a once he had your hands
on those logs like searching and sorting
and filtering through those logs was
really cumbersome with the tool set that
we had so that's when we decided would
start looking into Amazon Elastic
searches managed service as a way of
addressing these problems so we started
writing our application logs and our
incoming traffic logs to an
elasticsearch DB and that replaced the
need for for us to have to search across
multiple files
spread across multiple servers and
instead be able to search sort and
filter from a central location using a
user interface that both our developers
and our DevOps teams could both access
and by using cabaña which comes
out-of-the-box with Emma
elasticsearch has managed service we
could like filter easily by time by
selecting the right bar or a period of
time in that section if you wanted to
you could sort sort by a particular
piece of log data so for example if you
wanted to find out what the most the
slowest end points in your application
were then you could sort by time taken
in descending order and finally you
could also easily filter your logs by
using elastic searches querying
capabilities the problem was when we
went with this logging solution we we
ran into out of disk space issues
earlier on because we weren't entirely
sure how much how much space we needed
to store all that data it was a ideally
we wanted to store at least a couple of
weeks worth of data so we could do a
week-to-week comparisons so to get
around that we started by fixing the
most commonly occurring errors and that
saved a significant amount of disk space
and the second thing we did was reduce
the number of data nodes in our
elasticsearch cluster from two to one
effectively removing replication so
although we traded off having you know
redundant copy of our logs and some
search through port and performance we
effectively have the amount of disk
space that we required for storing our
logs so now we had a better way of a
more quick way of analyzing our logs but
the thing was we still didn't know
exactly what had caused those outages
during that week even after spending
quite a bit of time poring over our logs
and looking at the system health data
for clues so what do you do when you're
not entirely sure what the problem is so
what we did we we took a step back and
and looked at the problem by starting
with what we
had in place so we had a asp.net
application deployed across a load
balance set of servers it's called that
to a handful of api's and our
application was proxy by a CDN and by
looking at this we asked ourselves was
there a way to protect our end users
from an outage in our asp.net
application that's when we we started
wondering what role our CDN could play
in giving us this insurance policy so
first of all what's a CDN CDN is
basically a proxy that sits in between
your end users and your back-end
application a CDN has the ability to
cache resources like HTML Javascript
images CSS which comes in really handy
in the event that your back-end
application goes down your CDN is able
to offload traffic to your from your
back-end application and then as an
example we had a breaking change to one
of the micro services that powers our
property details page on domain where
there was an update to the data returned
from the micro service back to our
website in a way that it broke it start
a contract between the two so how did
that look and user makes a request to a
property details page which gets handled
by the CDN CDN makes a call out to our
website our website then calls our micro
service for property details data which
returns a JSON response which no longer
adheres to its data contract with the
website website generates an error upon
seeing that era the CDN will check if
it's if it's got a copy cached copy of
that requested page from a previous
request and if it if it does then it
will return our successful response back
to the user so great
and this number roughly 400,000 is the
number of successful property details
pages that would have otherwise been
error pages if it hadn't been for the
offloading and the caching that we'd set
up in our CDN so great now we have a
more effective way of analyzing our logs
so we could more quickly react to
outages we had a way of protecting our
end-users from a back-end application
outage but how could we improve the
resiliency and the reliability of any
new applications that we that would
build from then on so no long after the
there's outages of that week we began
the rebuild of our home page and with
resilience as a top priority we began by
asking ourselves what well a couple of
basic questions first of all what could
go wrong and then second of all what
happens if X Y or Z goes wrong so we
began by looking at what we had already
in place you've already seen this we
have an asp.net application deploy to
multiple low balance servers calling out
to a handful of api's and you know we're
proxy by our CDN so it's quite clear to
see from this picture there are quite a
few things that could go wrong each of
those calls to those api's is a
potential point of failure so in the new
design we decided that we'll pre
generate the HTML for our home page
would host it in an AWS s3 bucket which
has which is basically a static file
hosting service it's got a virtually a
100% uptime SLA stores redundant copies
of your files across multiple data
facilities and within each of those
facilities across multiple devices and
are also order scales to handle incoming
traffic
as another layer of redundancy we also
cache our homepage in our CDN for an
hour sir
if there's ever an an outage without
back-end application or or something
else goes wrong with that connection the
end user could still see a cache copy of
our homepage as another layer of
redundancy we also cache our home page
in the end users browser for an hour so
that in the event that we've miss
configured our city and for example or
the end users having internet
connectivity issues as long as they
visited the website of all the home page
before they should still be able to load
up the home page so in our new design
and user makes a request for the home
page which gets handled by our CDN CDN
makes a call to s3 which returns the pre
generated HTML which goes back to the
CDN where its cache for an hour which is
then returned back to the end user where
it's also cached for an hour and so
comparing the two we've we've reduced
the number of things that could go wrong
by reducing the number of AP is that we
depend on and by reducing the number of
API as we depend on we're also as a
bonus improving the response time for
loading up the home page and not long
after we started to rebuild all our home
page we also started the rebuild of our
property details page so like before we
asked ourselves the same questions what
could go wrong and what happens if X Y
or Z goes wrong and like before we
looked at what we already had in place
load balance asp.net application calls
out to some API which is proxy by a CDN
like before a lot of things could go
wrong each of those API calls is a
potential point of failure so in the new
design
we've replaced that with a single call
to a dynamodb table where we've where we
store all the listing data for all the
listings across our side which we've pre
aggregated in a separate offline task
and because it's in we're using DynamoDB
which is a managed service it means that
we don't have to spend as much time
maintaining that infrastructure
it also lets us provision pre provision
its capacity according to our throughput
needs and like before in the home page
now that we're just calling one API
we've also improved the response time
for loading the page back to the
end-user and we also decided that we
would build a new front-end application
for the property details page and by
doing that we effectively built a
bulkhead in our website
much like the bulkheads there's metal
partitions you find that you could seal
shut and use to divide your ship in two
separate watertight containers so in
order to basically contain the damage to
one compartment and keep it from
spreading to the rest of the ship in our
case to the rest of the website
we don't only do that for for the
property details page we've also we do
it across a lot of our website the home
page as an example it's a pre generated
static home page HTML page which is
hosted in an s3 bucket we also do it for
our property profiles section of the
website we also do it for our search
results page and our new homes page so
by having all these separate microsites
what that allows us to do is for for the
property details page we could provision
it and scale it independently
of the others parts of the website we
could also release it independently and
this is one of the main ways that at the
Maine were able to release as frequently
as we do into production without with
minimal risk of a site-wide
catastrophic catastrophic failure
another way we enforce the damage
containment is by the by using circuit
breakers whenever we make an API call in
our system so the way circuit breakers
work that they sit between your upstream
services and its clients they start off
in a clear state client makes a call to
service and if the service returns an
error the circuit breaker keeps track of
that error and once the number of
consecutive errors reaches a particular
threshold that's when the circuit
breaker goes from a closed state to an
open state for a predetermined amount of
time say a minute during which any calls
from the client to the service are
automatically stopped and what this does
is it means that you're struggling
service is given time to either order
correct order heal itself or it gives
the development team that's responsible
for that service time to manually
intervene and diagnose and address the
issue and the final way that we enforce
the damage containment in our sites is
by making sure we explicitly set
timeouts whenever an API call is made
and what this means is any slow
responding services don't result in
downstream clients from having their
outgoing requests cue start to build up
or other resources like memory to start
maxing out okay we have a way of
containing damage to
a particular part of our website and we
also have a better way of doing log
analysis we have a way of protecting our
end-users from an outage from our
back-end application one area we I often
see overlooked is is making sure that is
the right alerting putting put into
place for a new application so at the
main we have a set of uptime targets and
and and and user satisfaction targets
based on response times so we need a way
of measuring how well we're doing and be
notified of when we're starting to
deviate from their targets and we uses a
number of services for that we use
Pingdom to maintain a set of uptime
checks against our various websites and
against various endpoints within each of
those websites we also used New Relic
to basically alert us about any errors
or regressions in response times you
know stuff that our Pingdom alerts
wouldn't tell us about and we also use
ops trini to make sure that our on-call
enginee gets notified of production
incidents directed at phone well one
thing we wanted to make sure was that
when we were setting there's alert
thresholds that we don't set them too
low so infact drowning out our team with
although it's every few minutes which is
a good way to have them completely
completely ignored eventually when
there's no real need for for
intervention but you also don't want to
set them too high because you might
actually miss out on a genuine
production incident so we started off by
setting them lower than usual and really
having theirs alerts sent out to a
handful of people warning them you know
you're probably going to get more or
less than needed and we're gonna work on
tweaking those thresholds says new
traffic came in cool so just a quick
recap I start off by telling the story
of all those outages during hell week
how it forced us to figure out how to
improve the way we do a loggers analysis
so we could get to the root cause of a
future outage how we used our CDN to
give us an insurance policy against the
another back-end application outage then
I talked about the the rebuilding of our
homepage and our property details page
to be more resilient basically by
reducing the number of points of failure
and finally talked about the need for
putting in the right amount of loading
and the tools that we use that domain
and if there's if you're interested in
jumping into some of these topics a
little bit more like reliability and
resilience there's a couple of resources
I highly recommend the first one is a
book called and release it design and
deploy production ready software it's
actually it was published a quite a long
time ago but its content is still really
applicable to today and the second one
is a website called high very high
scalability calm which has a lot of
interesting case studies for for real
high traffic websites that need to scale
well and need to handle failures in
production really well so that brings us
to the end of my talk thanks for
listening if there's any questions happy
to answer them</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>