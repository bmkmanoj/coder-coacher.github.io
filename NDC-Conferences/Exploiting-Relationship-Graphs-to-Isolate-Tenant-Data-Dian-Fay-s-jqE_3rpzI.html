<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Exploiting Relationship Graphs to Isolate Tenant Data - Dian Fay | Coder Coacher - Coaching Coders</title><meta content="Exploiting Relationship Graphs to Isolate Tenant Data - Dian Fay - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Exploiting Relationship Graphs to Isolate Tenant Data - Dian Fay</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s-jqE_3rpzI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so it started a little disclaimer
first I know the abstract said node and
poster has the technique that I'm just
going to be talking about is generally
applicable to any our DBMS and there's
only a few points that are actually node
specific here so with that out of the
way the situation that we developed this
technique in is that were small business
to business enterprise web accessibility
shop so we do a lot of testing and
remediation for clients who are much
much larger than we are so in order to
sort of keep up with the needs of our
clients we want to try to minimize our
infrastructure and streamline our
processes as much as possible
to do for example a multi-tenant trial
server that we can later break people
out of and move them to full instances
possibly to some analytics to see how
people are actually using our
applications but our problem is that
since our tenants are so much bigger
than we are and have their own needs in
their own wishes for actually managing
the pace of upgrades we can't do two
software-as-a-service
so that is out because one tenant may
want to work with an older version for
as long as they want to and we're just
trying to keep up and release new stuff
so the problem tries is sort of how do
we have our cake and eat it too with
regard to having a multi-tenant capable
setups that we can later pull data sets
out of so fundamentally our problem is
we've got too many databases floating
around out there at one database for
tenant we have dozens of clients those
databases it's just it's a lot to manage
and so any way that we can reduce this
and bring these together and consolidate
as much as possible that's just less to
have floating around it's less to deal
with a trust this matters because of
time money and effort we are a pretty
small company especially relative to our
clients so the less time we spend
juggling databases configuring things
Rhian dexing doing maintenance the more
time we have to actually develop
software
money I'm perfectly happy to spin up and
in four large and just leave it out
there for coupla months but other people
seem to object to it so if we can just
pull these together have fewer databases
we have less maintenance costs less
server costs and it's just it's a lot to
deal with
- just juggle all these remember where
everybody is and be able to pull stuff
in and out of their databases and manage
separate versions so we can have a
multi-tenant application with one of
three possible database architectures we
can do separate databases one per tenant
which from a data perspective this is
not an improvement it's the status quo
we want to be able to do better than
that if at all possible it's sort of our
last resort next option is a little bit
less overhead is to have a single
database and separate schemas per Kennet
this is better for us because we have
fewer things floating around but sorry
another thing is that it does allow
version independence you can have one
schema app version 1 1 once game version
1 2 and so on and so forth but it's got
still a lot of application security
overhead you have to have different
users and manage access privileges to
different schemas just like you would
have for the databases you have to
juggle connection pools which is bit of
pain and analytics are still pretty
difficult if we want to do those because
you have to stitch together result set
of separate queries for separate schemas
the third option is partitioned tables
which is it's actually pretty great it's
got the lowest server overhead security
and access becomes strictly an
application concern there's only a
single connection analytics everything's
just right there so if at all possible
we want to do this so if you get started
designing a schema for a partition table
structure this is sort of a reduced
version of one of our accessibility
testing products so we have a tenant
table where we have a single broker
ten-inch that we're looking at each
tenant have users and tests which are
related by a junction table
users have devices which they used to
perform the tests and raise issues so
the problem for us with this schema is
that how do we work at the level of
individual tenant datasets so the tenant
row and their related roads in these
other tables so we have tenants in a
shared database if one of them buys a
standalone license like we had and
insurance industry client moved to
standalone server recently so we need to
be able to pull their data out and just
give it to them in a tiny little package
without everybody else's stuff coming
along for the ride because they probably
don't want anybody else to see just what
sort of shape their sites are in so most
of you probably know sequel server this
is a Postgres dump script this is a
fairly standard way to throw data around
Postgres has PD dump my sequel has my
sequel dump sequel server you got to go
through a couple of context menus and
wizards and there's the generate scripts
option Oracle is the exception here it's
got a binary dump format but what it's
doing here is saying to copy into this
table these these rows which are tab
separated and it goes field by field so
if we have something like this for our
entire database can we generate that
something it only has the data for the
tenant that we're interested in so ways
to do this the first thing we can do is
just attack it with a blunt instrument
and just go for brute force just pull
our data out as we can especially that
we've given the schema we just take it
from the top you go to the tenant table
because that's going to be our entry
point we pull out the row that we're
interested in and we just pop it out
into our output moving on from there we
go to the users table pull out the users
related to that tenant dump them out to
the output
from their devices and we start to see a
pattern here where the further you get
from the entry point the more complex
your queries are so your exporter
complexity is going to be essentially
tracking schema complexity the more
complex your schema the more complex
your exporter which becomes also
especially problematic if you have
something very complex where you have
converging schemas for instance if
devices can be accessed through another
table say tests you have to get not just
all devices which are linked to tests
but all devices which are owned by users
and you have to do a lot of singing to
figure out okay how do I get all the
devices for this tenant through any or
all paths that we could have them you
can also track intermediary IDs and use
in lists to pull out these data that
it's not really much better another
major problem is that it's very fragile
if you make any major changes to your
schema you have to come back in here and
do some maintenance and update your
exporter you can ameliorate this to some
extent by automating the columns part of
it by pulling out those from the result
data set but if you add a table drop a
table rename something juggle
relationships a bit you're going to be
back in here doing work and it's
thankless and crappy unfortunately we
can approach this a little bit better in
a better way if we go back a little bit
and have a sort of discrete math
refresher so a graph is fundamentally a
set of vertices connected by edges
definition of a vertex or an edge is
abstract to greater or lesser degree but
it should be consistent and that you
should be talking about the same sort of
thing when you talk about a vertex or an
edge there's lots of things that are
represented as graphs in the wild
everything from hiking trails could be a
graph where the path is an edge and
stations belong at our vertices network
topology diagrams our graphs pair
straight up where your hardware your
routers switches were stations those are
vertices communications going back and
forth between them or edges
also if you shopped on Amazon or Zappos
you can frequently see sort of people
who purchase this product also bought
this that the other behind the scenes
that's a graph products are vertices and
the act of purchasing or having multiple
purchases in somebody's account is an
edge there's a specific sort of graph
that were interested in though which is
a digraph or directed graph the only
difference here is that edges have
directionality so for here there's an
edge going from A to B there is no
corresponding edge going from B to a in
the other direction
so some examples here would be stuff
like social media follows we follow
somebody on Twitter they don't
necessarily follow you back so that is
constitutes a directed graph between you
and that other user package dependencies
are represented as digraphs in
everything from NPM to maven you get and
stuff like rpm or the arch repository
those generally come in as trees which
is sort of a special case of a digraph
where you don't have convergence so B to
D and C to D if you pull those out you
have a tree with the route a it's that's
a better so but crucially for us the
another example of a diagraph in the
wild is for T's a data bases
relationships make up a directed graph
so if we can somehow represent our
schema as a digraph we can start to
manipulate it and do things with it to
start pulling out related data so if we
go we have to first define what our
vertices and what our edges are going to
be our vertices is pretty obvious it's a
table or more specifically a set of data
within a table or set of rows so for us
we're just going to say that every table
has to have a primary key constraint so
that we can query it consistently
it's includes junction tables which are
going to be vertices in their own right
even though in E RDS they're frequently
elided you just have one edge going
between the two tables that are related
by the menu to
relationship so our vertex is going to
be a table name and a collection of
primary Q values our edges are going to
be the relationship you might note that
this is actually a little bit backwards
the source is the reference column in
the relationship and the target is the
referencing one you have to think about
it in terms of dependency rather than
who owns which columns we also have to
store the actual columns that make up
the key for example if it's possible to
have multiple relationships between the
same tables if our tests can have user
who created the test and user who's
currently assigned to it we have to
track both of those relationships in
order to guarantee that we have an
entire picture of the data set so this
is a representation of our schema as a
digraph so if we can start traversing
these relationships and just bounce
around between them and just cover the
entire graph this is not going to be any
kind of fancy pathfinding thing it's
just
let's visit everywhere and pull up
everything that we possibly can if you
can get a connected subgraph so some
rows from this table some herbs from
this table summers from that table all
related to that gives us a single
tenants entire data set in isolation
that we can work with can actually get
this we need to start creating the
metadata of our database the ansi
standard defines an information schema
which allows you to query your structure
in your metadata just as you query your
other tables and your actual tables and
views everybody implements it to varying
degrees of conformance these are often
views on implementation specific tables
postgrads has the p.g catalog sequel
server has a system catalog where you
got the civils or whatever sorry but
press there's three specific items were
interested in table constraints gives us
all the primary keys in the entire
database referential constraints to the
same for foreign keys and finally key
column usage tells us for a certain key
which columns actually
get up in a given table so the next step
for us is going to be actually building
our sub graph but you're going to have
to do through a fairly involved
recursive process so we start obviously
at the entry point to the side out here
we could start anywhere we just need a
table name and a primary key value and
we can pull out any data that's somehow
somewhere related to that row in the
table that is easiest because we know
the tenants that's going to be sort of
our obvious entry point just go as a
tenant ID and our process is going to be
we add first the primary key information
to the vertex then we start scanning for
foreign keys that involve
that vertex trust there's two of them
there's users and tests both have a
tenant ID the order in which we take
these is completely arbitrary it's
easiest for us to go just alphabetical
by source for consistency sake so we
start creating an edge but we don't know
what the primary key values are on the
other side of it so we have to actually
recurse along the edge as we're building
it to get the vertex information to the
other side and complete the edge so we
do that we go to tests add the test
priority information to that vertex we
finish the edge and we start do the same
thing all over again scan for foreign
keys involving tests the first one for
us is going to be issues with our scheme
so we do the same thing start building
the edge we recurse we've finished the
edge by adding the primary keys to the
issues vertex and then there's only the
foreign key back up to tests and this is
the first time that we hit our recursive
base case which is we're not adding any
new information at all because the edge
direction guarantees that any issues
they're going to belong to tests that we
already covered so instead of recursing
where you roll back up to tests and then
the next foreign key is actually going
to be device ID from tests so this is
the first edge that we Traverse from the
target that works exactly the same press
we process the vertex
the same way the only real difference is
that because technically because test
has device ID in it we already know the
primary keys but it's really easiest
just to treat it exactly the same way
so after devices we recurse to users and
same deal we add the vertex at the
primary keys but scanning the primary
cues does reveal that we could have
incomplete information because we came
to devices from tests
there could be devices that users have
that they haven't started testing this
at this point so we don't necessarily
have all the devices for that tenant
even though we have all of them for that
tenants test so we actually window for
cursing back to devices which brings us
to another sort of edge case here pun
not intended or you can have multiple
vertices existing for the same table if
we go back to devices because I'm going
to have devices that overlap of users
have them seven test some Naughton tests
so we'll just consolidate those in order
to make sure that we don't have the same
row in devices appearing multiple times
in the directed graph or multiple
vertices which would then cause them to
appear multiple times in our output
because if we do that we try to insert
the same thing multiple times crashes
and burns and it's useless to us so in
complex schemas however you could have
rows that to get to four multiple paths
that do not overlap in that case you can
have multiple vertices this is looks a
little bit weird if you have the same
block of inserts or copy statements for
the same table multiple times but it's
harmless because the same data does not
appear in both so all users devices does
overlap so we merge the new entries into
the existing vertex and then we get the
base case again which is that there
aren't going to be any new tests because
the only devices that we've added aren't
being used in tests so we roll back up
to users and then just like with devices
where initially only got those that are
used in tests since we're only looking
at the current vertex and its neighbors
the entry point is really it's just
another table so we could theoretically
have another tenant connected to some of
the users that we just picked up
we shouldn't be really really terrible
if we did because that would mean we
have some sort of pollution going
between individual tenants data so it's
connected we don't want that but we have
to just go back up and check but also
you could have users that don't have
devices so we need to go back double
back up here and pull out all the users
related to that tenant and just make
sure that we visited every row in that
vertex so if we do that and blast table
that we haven't visited is the junction
table so we go in there even though
there's the compound key it's the same
deal for us we are already just building
the edge getting the full primary key of
the junction table out adding that to
the vertex do many-to-many relationship
does imply that it's possible to have a
test that is not related to a tenant so
we have to go back there just to make
sure and at that point we mapped out the
entire connected subgraph we have all
the rows present for that tenant in our
vertices if they write note that we
haven't base case all the way back up to
tenants and recurse to users there's no
need for that because we've actually
already visited users from tenants so we
have all the data that we need there is
one caveat here these will mess you up
if you have for instance the JIRA style
scheme where impact can be such trivial
moderate minor serious critical blocker
you have a lookup table is common
pattern for approaching this where you
have just a table this Keys mapped to
the values for the impact or any other
situation where you have sort of an
enumeration if you do that you can start
coming in from one tenant curse into the
issue impacts table and then you start
looking for any issues that have
packs with those primary keys and
suddenly you've got your entire data set
out there and you don't want that in
Postgres we've set for decides that the
question entirely we use enumeration
types which are a custom type that you
can create that converts a string
visible value into a number in the back
end so it's more efficient for indexing
but for other DBMS is that don't offer
that functionality you would have to
look at defining some sort of stopped
able to say if you reach this table
don't recurse from it just assume you've
hit the base case and pull back out so
at this point we have the data set how
do we actually convert that into a
script that has the single tenant data
that we can pull out into somebody
else's database so type a sort is sort
of a graph very function where you order
vertices by dependency this is how
package managers decide what order to
install things in for example the way it
works is that you start by looking at
the vertices which are not targeted by
any edges so we're going to start with
tenants we're going to query the full
rows from tenants prevented scripts
those out to our output which stress is
going to be standard out if you use
Visual Studio the console because from a
command-line application what we can do
is we can take standard out pipe it out
into a file or put it directly into the
input of another command so poll tenants
ouch
we remove the vertices and edges that we
just looked at and then we just repeat
that same process until we have nothing
left in the graph at all there's one
possible scenario could hit which is
that if you have a cycle anywhere in
your graph you're basically screwed the
good news is you have to really really
try to get this specifically you have to
at some point stop enforcing your
foreign key constraints because the only
way to insert or
the owner to generate a cycle is to
insert data before data it depends on so
you can insert one row here and another
row and then turn on foreign key
constraints and suddenly you have a
referential integrity problem because
you have a cycle going between the two
and you don't know which is the source
and which is the target
let's know to get started with the turbo
sort we take our tenants if you just
generate that copy statement and push it
to our output that precept users which
we treat the same way pull it out type
it at the output the devices work
similarly the one that frees up for us
the tests which you might notice that
this is not a JavaScript date
it's a post-grad date Azra polling this
information out the driver that we're
using and this is the one specific thing
for node the PG driver includes column
type information so we can use that to
pass our results out to a formatter and
pipes and through there on the way to
the output if you don't have the type
metadata and your results you can use
information schema columns table or dia
and that includes type information you
can just correlate with your output and
make sure that you have your results
coming out in the format that your
import will expect so the last two
tables for us with all our edges and
other vertices gone our issues and user
tests the ordering doesn't matter here
at the same level so we just type those
out in whichever order so and great
talking about it but we actually try it
out I've created the example schema that
I've been using I have a dataset
generated for that so we can actually
pull that out
it's kind of retyping on the back screen
so all the databases that we have
Arachne is the name of the application
it's after okay it's after the Greek
Weaver in mythology who had the bad idea
to challenge Athena to contest I got
turned into a spider for troubles um
so please and psql is the command-line
client for postgrads that will let us
actually query the data and look at what
we actually have in there so here's all
the tables how you look familiar
got the same set i've got data for three
tenants in there so Acme's list the
tasks like to place and they've all got
users you've got users for every single
one actually came out more or less the
same order there's a bit of a mix-up
there which good so exit out of there
and actually run or acne and it tells us
how to use it pass in connection options
we give it the name of an entry point
table and a primary key value
like so turns through a little bit and
then generates all of our copy
statements in the order we expect so it
starts with tenants move on to users and
devices pulls everything out in the
dependency order that we were looking at
with TOFA sergeant so if we pass it a
different primary key we got another
tenants data we get Cespedes here so I
make sure I dropped that first so now
given this output if I create a new
database and apply the schema because
this is a data only export I can I have
the schema generation statements already
in a file that I can just run against
this database as we'll create to set up
that we're looking at and then
I can run Arachne again and because it's
outputting to standard out I can use the
pipe to just hook the output of Arachne
into the input of psql and it will run
the statements in PDP database I can see
they're just copied I'm standing away it
copied everything out that we're looking
for and if we go back into P SQL into
the new database again so if you only
have the single tenant now and single
tenants users should show up so there
you have that and for the mouse come
back here so this is on github I'll get
to that a little bit but tress what this
is allowed us to do is to treat you
such-and-such data set as a logical unit
that we can work with so we can backup
individual tenants and this is the thing
that's made it possible for us to run
this the partition table setups as
opposed to a multiple schema set up
which makes things just makes my life a
lot easier which I'm always in favor of
so we could have a situation where we
have all three tenants in a single
database and turn it into that I just
pull out extract a single data set put
it anywhere we want so we can restore
them we can back them up on independent
schedules we can keep independent
backups so this has ramifications for
removing tenants for data retention
policies we're really just scratching
the surface of what we can do with this
so far other thing other big selling
point here conforming to constraints is
a big deal PD dumps orders by dependency
by default my sequel dump doesn't if you
have a my sequel dump script it will
actually turn off your foreign key
constraints run all the inserts and turn
the constraints back on this is actually
kind of dangerous because if something
goes wrong
along then you're left in an
inconsistent state with your foreign key
constraints off but ordering by
dependency lets us take this scenario
that we had just from pulling PGP out
and Regan for everybody else into that
database without downtime this is
honestly more of a cool hypothetical
than something we actually had to do yet
but being able to apply the output of
Arachne into another live database means
we can offer things like teardown grades
for shared servers there is another sort
of bonus usage which is as a schema
validator if you dump out your database
and restore a single tenant and you have
data missing then you know your foreign
key constraints are not complete and you
know more or less where to look it's
where you don't see any data so being
able to handle arbitrary schemas saves
us from having to do export or
maintenance we can take any data set
from anywhere we could use alternate
entry points you know if for whatever
reason we wanted to start with a
specific user and just pull out their
entire entire data set we would wind up
with exactly the same output in the same
order because we would visit the graph
go back all the way up to tenants and
then that would be the first one that
gets Topa sorted out so something else
does let us do it is another developer
and I @bq have been working on a fairly
major change recently it's been I want
to say probably a couple months actually
we got rid of our users table everything
about users was now supplied by key
cloak a single sign-on provider so this
has been an absolutely huge code change
for the application so we've been
tearing stuff out revising tests or
revising the entire infrastructure to
handle users coming from somewhere else
but this didn't have to change at all it
worked exactly the same way we had to do
no maintenance at all so it's been great
because I would probably forgotten about
this until the last minute
and she would've been horrible so one of
the cool things about working at DQ is
they let us get away with open sourcing
or tooling so I put this up on github
it's sort of a reference implementation
for the technique I want to say this is
sort of a final note this doesn't make
the partition table strategy the
automatic solution you could have
requirements for medical or financial
records where you are required by law to
segregate your tenant data or if you
have architectures where tenants need to
be able to interact with each other then
you're kind of stuck this is not going
to help you with those but for what we
do where we have tenants that are
completely apart and just living in the
same space without interacting or
sharing any data means we can ignore the
major pain points of the partition table
strategy almost entirely so it's me that
a lot easier for us and that's it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>