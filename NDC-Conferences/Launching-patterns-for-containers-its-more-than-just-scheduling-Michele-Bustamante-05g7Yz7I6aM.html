<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Launching patterns for containers - it's more than just scheduling - Michele Bustamante | Coder Coacher - Coaching Coders</title><meta content="Launching patterns for containers - it's more than just scheduling - Michele Bustamante - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Launching patterns for containers - it's more than just scheduling - Michele Bustamante</b></h2><h5 class="post__date">2017-05-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/05g7Yz7I6aM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everybody how you doing it's the last
session of the bay isn't it Wow you're
like it's better be good I'm tired okay
so we're going to talk about launching
patterns for containers which is a
super-fun topic almost better than
security okay my name is Michelle LaRue
Bustamante I'm here I have a company
called slience we do a lot of different
types of consulting but my practice is
our security and micro services so we've
done a lot of different platform
deployments with customers over the past
few years as this sort of space evolves
and so I've been you know this is
actually a new talk that I'm bringing
just to kind of go over some of the
patterns that I've seen with launching
containers and also some of the
scheduling issues and things that go on
so with that I'm going to get started so
the first thing I guess
well it's not really advancing first
thing I guess is worth talking about is
the container platforms that we might
discuss so we've got a lot of choices
there are hosted platforms like Amazon
and Azure and then actually many other
third party platforms right who try to
provide you with a containerized
deployment model that is managed in some
way and then we've got so Google as your
in Amazon being the big three right and
then we've got the platforms that sit on
top of that so Google container engine
based on kubernetes Amazon actually has
its own proprietary partially paths and
then mostly I as implementation and then
we've got in in Microsoft's azor we have
the azure container service which is
really just a quick way to deploy any of
the three orchestrations which would be
swarm docker swarm DCOs
and also kubernetes now and so there's
that option plus just doing straight VM
anywhere you want to go with those three
platforms I just mentioned right service
fabric is more of an internal
proprietary implementation of not
necessarily a container engine only
although it now supports containers but
a scheduler of its own right for service
fabric native application
the reason that I bring up all of those
is really I don't know why this is here
by the way can we go away I have no idea
what that is
sorry some sort of Dropbox thing so a
lot of them have things in common right
the idea being that I have a management
cluster and the management cluster is
responsible for understanding the health
of all these nodes which is where you
deploy your actual services it's also
responsible for the scheduling aspect
which is deployment right so deployment
is scheduling an orchestration it
doesn't mean that there's any order like
orchestration it means in other
situations it just means that it's
looking for a place to schedule deploy
bring alive
keep healthy the services that you
described and those could be single
service micro services or it could be a
composition of multiple services in a
micro service described by some sort of
task so these are common commonalities
between all of the platforms and so I
think that's just the first foundation
that I would provide here now the
infrastructure behind that is going to
be that you have a management cluster of
some sort you have an agent node cluster
of some sort it's not cheap to run these
things so you're usually looking for
server density in order to make this
worth your while you're looking for a
new platform that's going to be able to
host many different applications that
you might be building and deploying it's
not just one like we would think about
with VMs in the past right where I have
a bunch of websites on this set of nodes
and a bunch of api's on these and you
might sort of isolate them pragmatically
so that they don't step on each other
right so for for multi for basically ten
into a noisy neighbor isolation right
and so we're looking for a platform that
can give us the noisy neighbor isolation
without having to deal with having
separate sets of closed clusters for
example so we're going to have probably
three nodes in our management cluster
we're going to have probably closer to 5
in our agent cluster realistically once
you start deploying real things not
because your services each individually
take a lot of overheads but the
if you're using services like Kronos for
scheduling or Kafka for messaging or
elasticsearch for that type of thing
you're basically already then installing
three big things that normally would
have their own three node cluster so if
you can fit those on to your five nodes
then that's not a bad thing as long as
the machines are big enough we also need
things like a discovery infrastructure
that means sometimes part of the
discovery process how do I get to that
service if it's got five instances
spread across the nodes I'm going to
have some form of load balancer that's
stitched up to the actual physical
public agent let's say and then that's
got to have a way of routing within the
cluster so that's a typical model but
some of its infrastructure based so I'm
just bringing it up here we also have
the docker registry that will be where
you actually deploy your images and so
we need that accessible if we're on Prem
we sometimes can still baseline on a
cloud based registry I would highly
recommend that so it can be a hosted
thing that you don't have to manage and
then that way you know what does it mean
if you're not connected to the Internet
from your office that day you're not
deploying that service so what are the
odds of that probably pretty low
is it worth taking the risk so you don't
have to manage this all year round
absolutely right so that's that's my
philosophy there are some core features
inside these platforms that you care
about service registration and discovery
meaning when I deploy a service or
schedule it somehow it has to get
stitched up to the load balancing and
discovery mechanism so there has to be a
way for clients calling the service to
get to the one or more instances of this
API for example and when I add another
instance it should automatically join
that group right so there's processes
for that that are maybe different within
each of the orchestration engines but
it's a big important piece and that
involves load balancing and routing as
well sometimes it's tied to the
infrastructure a little bit so like in
Amazon you would actually use a Amazon
or alb which is really a lvb - so it
gives you routing capability at the
physical you know infrastructure layer
and then it would route into your ECS
cluster for example so those are some
features that matter auto scaling is
part of it so this is all really part of
scheduling right because scheduling
means deploying deploying means being
available and spread across the load
balancer and somehow be reached
it also means auto scaling so the
ability to add more containers when I
have maybe a lot of burst requests for a
particular API and it's using a lot of
its CPU and memory then it should spin
out to a couple of more until it doesn't
need them anymore so there's things like
minimum health is two instances maximum
could be up to five or ten you could put
a limit on that and then we have
self-healing when something goes wrong
it should try to recover so we see
sometimes situations where inode will
flap for example and the reason that
that happens is something is failing and
it's not able to stay running that
wouldn't happen with an API or a website
where you just have an error it would
happen maybe with something more severe
right where it can't actually reach with
a health check one of its data assets or
something like that so you've decided to
incorporate a data asset check in the
health check and by doing that you're
making sure that the service reports
lack of health for the right reasons so
we'll see example of that but the idea
is we want it to be self-healing we also
want it to be upgradable so I should be
able to deploy new services and have
them scheduled and then have the old
services retire and in an appropriate
way right so that means that the old
service the original v1 for example will
stay alive and healthy and will keep
running until we have successfully spun
up the new v2 and has that stitched up
to that load balancer and then it will
drain the the v1 right so there's a
process for that that usually you
wouldn't have to get involved in and
then we've got you know versioning which
is really just another piece that's a
little bit more finesse by you which is
how do we want to handle backward
compatibility with api's and things the
main thing we want to point out is that
scheduling and orchestration should not
imply order
so you should be able to take a micro
service and deploy it and update it and
not affect the rest of your ecosystem of
services if you've done it right that
doesn't mean that you can't bring up a
v2 and nobody's using it yet and that
doesn't mean that you can't version the
internals as long as you're backwards
compatible but it means that you're
thinking about those things and you're
not depending on order of orchestrated
deployment if that makes sense so these
are core features that we expect to be
provided with any of the orchestration
engines or platforms and so just to
point out again
scheduling starts with scheduling
meaning we have take a service
description and that will be maybe a
composed file or an equivalent across
the various platforms that will describe
what service are we deploying is there
more than one is there any sort of
networking dns to be described are there
environment variables that should be
made available because ideally I'm
building images I'm pushing them to a
registry and those images are immutable
so when I develop and check in and have
CI available or setup then I'm going to
have images generated every time I check
in and those images will not be touched
again and they will be marked as latest
and V whatever and then they will be
tagged with maybe some other tags that
help it get deployed to Amazon or Azure
or wherever the container system is and
that image can stay as is and get
promoted through the ecosystem of tests
and you know acceptance and UAT and and
prod and so that way you've got this
immutability and the only thing that
would change is the task definition the
service description all these are
interchangeable terms the compose file
the environment variables that help it
get launched that means choosing ports
or letting it be dynamic ports things
like that so the service description is
applied and usually sent to the leader
and then the leader decides hey based on
all these healthy nodes I have who's got
space where should I scheduled you and
if it can't successfully bring it up on
one node it'll go to another and another
and another so that's the idea okay
okay so that's a green screen in case
you're wondering one of the things I
want to point out this is something that
I did in a swarm demo but I don't want
to get into swarm because we're going to
do other better things but the idea is
if I have a couple of agents and I have
a certain amount of CPU and memory
available and I have a container running
likely that container has reserved some
space some CPU some memory a port and
those are resources and those resources
are finite so the idea is your container
or your task description should be
indicating what are the resources I need
to run so that the scheduler
orchestration engine can decide how best
to purpose and move those containers
around you can also do interesting
things like have affinity like I should
always be located with this one because
I want to actually do DNS on the same
machine for some reason instead of
distributing it through the load
balancer that would be one example that
could happen although that's probably
rare it's just an example
another would just be because you know
maybe you want them co-located so that
they use the same resources and you've
got let's say you had seven nodes in
your cluster and you want these two to
be just for Kafka related activities or
three probably because it would have to
have at least three or five so you would
say these are going to be just my class
my Kafka knows I'm going to give them a
tag that's the only kind of thing that's
going to deploy there and then you know
the rest of the nodes are for the rest
of the services so it's a nice way to
correlate how things should live
together in your cluster make sense so
one of the things that will happen is
you know you'll deploy containers and
they will use whatever they have
requests they'll reserve it so it's not
available for anybody else resource
constraints then have things like
physical hardware limitations like disk
space IO so on port reuse would be
another one right I can't deploy
containers with a fixed port and except
to get more than one on a machine but if
I let it be dynamic port which I should
then it should just
use whatever ports available and stitch
up to the load balancer and that's
usually what we would expect to happen
explicit constraints on CPU and memory
and i/o are a bit more challenging
because you have to do some drills
before you go live to production to
figure out what we really need you're
going to have some services that are
memory memory intensive because they may
be cash a lot of things like a shopping
cart or something like that there's
going to be other services that are i/o
intensive because they're maybe having a
lot of communication to other api's or
things like that so the idea is that you
have to learn a little about your
services you're going to start up with a
bit of a swag I think I don't need much
for the service to run and then you're
going to get surprised because it won't
run and then you'll say well maybe we
need to rethink that and you'll start to
learn the characteristics the
personality of each of these services so
before you go live you'll take the time
to drill because not just because I said
so but that would be one good reason
another good reason would be because if
you don't you could take risks on your
go live in production right like that's
not a good thing to do we've been
through this exercise and gotten pretty
close and still gone live and had a
couple of things surprise us over time
so that's why you monitor and have logs
and all of those other fun things that
are required to live safely in a
microservices world so you know the node
distribution is going to be determined
by these constraints that you provide
and those are things that you can do at
the command line for example as part of
you know running the docker container
and that's the kind of thing that's
happening for you when you schedule so
we'll take a look at a task description
and so on and the other main thing the
reason that this is so valuable is the
noisy neighbor protection right so now
our containers are actually asking for
what they need and they're not going to
be able to get more and so they start
you know pegging at their own memory
that they've allocated like 2 gig of ram
then they're going to need to spin up
another instance somewhere and find
another machine that's available for
that so that's the sort of auto scale
story that goes with that make sense ok
so another green screen
because I just wanted to point out that
you know when you do try to schedule
something and ask for more than is
available so let's say I already am
using three gigs of the 3.5 available on
this machine then when I do that
scheduling instruction and if none of
the machines are available to launch it
will say you know no resources available
to schedule of course this is me doing
this in the command line just for the
sake of having a bit more close touch
and feel on what's happening but this is
the stuff that internally the
orchestration engines are doing for you
right so you're just describing what you
need and it's handling the rest for you
okay so we're looking for server density
that's one of the main things the colors
are really not related to anything
except to indicate they might be
different microservices so pick your own
colors the idea being you know I might
start out where I'm not using a lot of
the three notes but as soon as I start
deploying multiple instances of services
and I start to add other applications
because you should be able to co-locate
multiple applications in a distribution
right and so eventually I hope to fill
those notes really tightly so to where
you know I'm making it taking advantage
of every bit of space available across
all of my applications so I'm now
getting what I pay for right now the
good thing about this is you can do it
the bad thing about this is you need
some spare because if you don't have
spare then this little red guy gets
deployed but you need to for health and
he's saying what about me I don't have a
place to go and you simply won't be able
to schedule that service that instance
cannot launch the other thing that will
happen is even restarting a service when
you do the restart command it needs to
spin up a new instance and then get it
stitched up to the load balancing
environment and then drain the other one
and that process requires space so you
have to have some free space so you want
to be alerting and monitoring on your
nodes to know that you're getting close
to you know let's say 60% CPU usage or
more usage that kind of thing because
you want to have some spare I have
almost no spare on my cluster right now
because I've been running it for three
days doing workshops and things and I
literally had to go in and brute force
delete a whole bunch of containers to
make this demo work today so that was
fun but you know those are the things
that happen when you're doing testing
because you're not really having a
smooth deployment process and you make a
lot of things fail for fun
or not so scheduling variations you
might have things like long running
tasks long running tasks would be things
like your website not everybody puts
websites in a container infrastructure
but there's really no reason not to
that's more of a strategy that is around
your micro services you know I guess
plan as a solution for example some
companies already have these monolithic
websites and they're happily hosted in
Azure paths
let's say advertise is nice and it's
easy and you know they've got a couple
of five different nodes that are load
balanced and multi-regional over traffic
manager and no need to change all that
but maybe the reason they need the five
nodes is because they also have all
their api's up there and because
everything is co-located like a monolith
so they could start breaking it up and
move the api's into a service here allow
that to scale out more widely and then
eventually slim down the front end right
so that's one example some companies
build the middle tier and they want to
make that their scale-out story for many
hundreds of applications in their
industry so like the hotel industry is
an example that I've come across where
they just have a middle tier of services
that everybody's going to use so it has
to be able to scale but they're not
really so interested in let's put the
UI's in here those can stay on regular
VMs and or paas environments whatever
they're using right so that's
long-running task those have to be
highly available so they need a minimum
to to be healthy right and if one fails
it will spin up a replacement
they'll be on just different nodes so
that will give you your hive
bility they're not going to put two on
the same note so that if the node goes
you're done you just get to decide what
the minimum starting point is two would
be the very minimum because that means
you have a che or high-availability
right so web app web api would be a good
example of that
and then there's non h a tasks that are
maybe background processing so i'm
pulling from a queue and i'm or i'm
consuming topics off of Kafka and that
process of consumption really just you
know iterates and maybe I might just
Gail out implement a scale out strategy
for those consumers so that they do have
multiple at once to process faster on
the topic that's possible it all depends
on you know what kind of messages are
coming through and what kind of order
you need to enforce and things like that
right but the point is the the
processing tasks if they fail is anybody
hurting right not really
messages are going to be staying in
Casca they're going to be you know
preserve they're going to be archived
they're just waiting for somebody to
pick them up so the whole point of that
would be maybe you don't have to scale
that one out so big unless you having a
problem getting throughput of the
messages through the system to their
destination so you can let it also have
one instance have that fail because
something's wrong like maybe the API is
trying to call has a bad database
connection and so the consumer starts to
flap you know it can't run it tries to
process a message it throws up on it it
stops we want it to stop because that's
an early warning of problems and failure
in my system and I want that warning so
I'm okay with letting it fail right so
these are non h.a and may be okay to
fail and restart on demand yeah and then
there's jobs you know so I want to
schedule jobs that run on a particular
schedule per hour let's say or maybe
they're just I'm going to have a job
ready and I'm going to launch it on
demand and that container will run and
do something maybe it's a janitor maybe
it's some other thing you know maybe
it's going to generate reports and you
don't know how often so you just do it
on demand
some of them could be repeat so like I'm
going to run this job X number of times
I've had some customers that need a job
that can run twenty five hundred
instances of it every day to pull some
files from these remote locations and so
what do you do if you want to write
2,500 different containers no you're
going to have one container and probably
some automation process that launches
the jobs and that gives them each a
different configuration to run with and
so you can literally schedule 2,500 jobs
and what they will do is just wait for
room to process and if there's no room
they wait right so interesting things
like that that you can do so with that I
think we'll go ahead and just take a
look around right so let me go over to
my DCOs cluster so a couple of I guess
observations right first let me see if I
can get you into a clearer view okay so
I'm I'm in a folder here that has a
couple of things available to me so for
example if I cat my tunnel just to show
you what I'm doing is I'm tunneling into
a DCOs cluster so I've got a plus stir
up and I'm going to set it up so that I
can browse to the administrative UI
that's all so let me go ahead and run
that and there we go and so once that's
done I can browse so I'm going to go
over here to my DC OS and refresh and
that ought to give me a decent picture
okay so if this wasn't working then I
would obviously see an error so I'm
going to start by going into the nodes
area so in here
I get a picture of health overall for my
nodes I can see that I've got a hundred
percent usage on this cluster that's
because that's my Kafka node so oh no
sorry that's Kronos sorry so this guy's
actually using up the full CPU it's
requested 100% of it
ooh CPU now these are small machines and
again like I told you
Kronos Kafka elastic would be three
examples of things that you know need a
decent amount of you know CPU and memory
to run effectively right so once I start
putting those in the same cluster as my
apps which is totally fine
I knew need a little more horsepower
right so I have to do all kinds of
dancing in here to get things deployed
at you know so that my apps don't ask
for a lot but the reality is your apps
probably don't need a lot to process an
API receives the request the payload is
reasonable it calls a database it's
really pretty lightweight right so we'll
take a look at that so this is one node
and if I take a look over here we've got
another one at 80% with 50% ish of
memory left and another one over here
with 50% and quite a lot of memory left
so this guy you know I've been running
this cluster for a little while right it
was a lots of different demo things and
stuff so I've certainly left some
garbage behind because that happens when
you're playing so this is the node that
appears to be you know interested in all
of the applications right now and so you
know things will distribute between one
or two of the nodes right now because I
only have three available for deploying
the applications I have one public node
which has a load balancer and then I've
got an internal load balancer and then
I've got you know multiple applications
which we're going to go through so when
I look at my overall picture here I'm
looking at what memory am I using for
the whole cluster but of course
individual machines might be full and
seems to go for disk so I'm not using a
lot of disk right now ok so that's a
picture of health and then I can come up
here and take a look at my services
actually before I do that this will show
a different view of the same allocation
so I get a picture of you know how much
CPU and memory and disk allocation I've
got over my whole cluster somebody asked
me wants why's it flatline I don't trust
a flatline
that's the reservation right so it's not
saying what you're using right now in
the
Tanner it's saying what you've asked for
up to that limit to reserve make sense
and then of course this is all my tasks
and whether or not any are staging or
failing or whatnot
so that's it all right so let's go to
services so first thing I'm going to do
is I'm going to take a look at and I
know this is kind of a small windowsill
and maybe I can squeeze that guy over a
little bit maybe maybe no okay nevermind
okay so what I'm going to do first is go
in here and you'll notice that I have
these groups of applications I have
identity events which is one application
and that one's depending on Kafka to run
and then I've got another one over here
which is just an API and a website so
what I've done is I suspend one in favor
of the other right so I'm going to
actually do a suspend on this whole
group and that will essentially shut
them down and then I'm going to go to
this group and I'm going to spin up the
API and the website so if I go to the
API and you know just for the sake of it
right like if I hadn't deployed this yet
there's a lot of ways for me to get this
deployed there's a rest endpoint I can
push the description too or I can come
in here and say you know create
application or deploy a service and it
will bring me to here and I can create
all the settings for the container where
as where to pull it from and so on and
then we've got of course an existing
application and I can go edit that same
configuration so this is actually an API
it's asking for point two CPU it's
coming from my docker hub account just
blonde and the content API repo and if I
kind of just take a tour into the JSON
this is essentially your service
configuration which again is the
equivalent of the composed file and so
on so I would have to build a script
that generates the right sort of
configuration for cert you'll find
there's common service characteristics
that form in your solution like
consumers of messages look this way API
is tend to look this way web apps tend
to the
this way and so we know how to generate
like the stitching to the ports for
example like for this one it's an API
and I'll show you it's pulling from this
image and it's using the internal load
balance report of 10,000 right so that's
how the web app is going to address this
API I'm using DNS it's going to go to
the internal load balancer and it's
going to hit that port so I can scale
behind that port is the point the
internal container port is three
thousand one but it's going to map to
port a port a dynamic port so I'm not
specifying one here
that way it can stitch automatically
into the internal load balancer which I
specify here so this is a marathon load
balancer set of tags and that's how it
knows okay you've asked to be part of
the internal load balancer group and
then of course by providing the service
port it knows what port it should be
using for this API so forevermore this
guy will be port 10,000 make sense okay
so when I deploy this of course it will
usually create one instance but because
I suspended it before it's actually at
zero instances so I'm going to start by
deploying one and we'll do that and
scale it and then while that's deploying
you can see that was pretty fast and so
the deployments are done we're green
that usually is the picture of health so
we know we have an active task if I
click on the task I can go in here and I
can see logs that's usually interesting
to do when you have problems for example
coming here and go to the error see if
you have any errors go in here see your
standard out any of your logs will go
here too so that's just a couple of
examples of things you can do let's go
back up to services and back into API so
I've got one running my configuration is
what I just showed you and if I ever
can't see it running at all I can go in
here to debug and sort of see that the
last time it failed it fails because of
this it couldn't get the image I gave it
a bad image name actually so on purpose
right
okay so API is running that's not going
to help much until I run web so I'm
going to go ahead and run the website so
I'm going to
that config and right now the website is
loops set for zero instances so why
don't I go ahead and say one and we'll
go ahead and deploy so that will be not
a che yet right so the point is that web
configuration is also stitching up to
the external load balancer at ten
thousand one meaning the public facing
port 80 to this cluster so I have to
marathon load balancers deployed and
this is saying what my IP address will
be so this is attaching to the azure
load balancer physically so that means
if I come up here and I go to that page
is going to show me the website and if I
click on speakers it's calling the API
and if I've stitched up to the API
correctly then I should see the sessions
and the speakers right if I go to the
stats page what I'm going to see here
stats HTML the task identifier indicates
which of the API instances we're hitting
but of course I only have one instance
right now for example and also which
front-end tasks I'm hitting and so these
will not vary at all as long as I only
have one instance of each so we have to
look at how we would ought to scale that
or how we would scale that so right now
what I'll do is I'll go ahead and scale
the website and we'll add another
instance and then I'm going to go to the
API and I'm going to scale that and
let's go ahead and try and add three
instances now these are each asking for
0.2 CPU age and I've started to deploy
quite a lot of stuff so might not hurt
to come down here and see kind of spare
do I have you can see it's getting
pretty tight but they also all may be
running so if I go to that node you can
see a bunch of things are running and
they're all on the
point zero point seven right so my IP
address if I go to services and come
back over to fab medical it looks like
I'm running both web instances that's
good it looks like I am not quite
finished with the API so there may be
problem deploying all three because I'm
asking for resources that we don't have
to deploy right so the question is is
the app healthy in the mean time so if
we come over here
the deployment hasn't finished and as I
refresh you can still see that I'm
getting a variation in the tasks so it's
brought up the two and it's just it's
brought up one of the three that I asked
where I already had one it added two
more but one of them isn't come up yet
so I won't be able to hit that until
such time as it does but you can see
that the task IDs are flipping right so
so that's the idea so that gives me my H
a but I'm not complete here of course
because of the waiting and I'm waiting
because my I'm asking for too much if I
were to reduce this to 0.1 how much CPU
do I need how should I know
does anybody know the answer to that is
there a way to tell how much CPU I need
how much memory I need I already told
you earlier right we're going to start
like that and then we're going to try
and Whittle it down until you know we
get it as small as we can and then we do
drills we're going to see if we can
break it it's what you have to do so I'm
going to deploy this and it's going to
say I've already in the middle of a
deployment that's not finished do you
want to override it yes I do
but the apps still going to be running
just fine right so I'm going to still be
able to keep doing this and nothing's
going to break because it's always going
to make sure that the instance is the
minimum instances are running that I've
asked for and it will unn it'll drain
the others as it can and it looks like
now we're healthy so a couple of new
ones a couple seconds ago and and then
I've got the the web app so everything
oh maybe not sorry I lied it should be
getting there though I hope apparently I
need a dictionary
so it's still thinking it looks like
it's coming up though because it's got
four now that means it's brought three
and it's going to drain so I think we're
still hopes have hope to get there let's
see for TAS overcapacity by one until we
drain I'm going to go take a quick look
at my notes and see what I've got
Oh yikes maybe not it's going to get
tight let's go back here okay so this
hang on this is my API or my web yes
it's my API still thinking they're
saying it's all healthy so let it keep
thinking this point to guy is the one
that needs to go away there we go okay
patience is a virtue sometimes it takes
longer than expected so when I look at
the completed test you can see that
sometimes you've killed a task that
might be because you intentionally
scaled it down that's what happened here
sometimes we see failure is because
there's an error and I'm going to click
in and go look at the logs and see what
happened to that service for example
okay so what I've shown you just there
is how would I deploy a service again
the platform's vary a little bit but the
point was there is resources I'm going
to ask for what I asked for is going to
determine what kind of resources I are
it needs therefore the scheduler has to
decide where can i deploy those services
and that's going to you know be
orchestrated for me right and we've got
our high availability which is another
good thing and we've got our ability to
scale which I just showed you right so
there's the starting point now this set
of services medical that I just showed
you is this website that's running here
and what I'm going to do is I'm going to
flip over to a different example with
cough cough so I'm going to suspend this
whole group and come back and hope to
god that the other one comes back up and
that I have enough space right so there
you go
did I say that out loud okay sorry
so what I'm going to do is you know you
can't really start the whole group at
least from the UI here together so I've
already got these api's and things
configured but I'm actually going to
whittle it down just a little bit and
that's not going to deploy anything
because you notice it says zero
instances right so all I did was update
my configuration the task definition so
what I'm going to do is I'm going to
take this API this api's job is one job
in life and its job is to process the
messages that are sent from the UI
through a consumer so it will be called
by the consumer in order to write to
adopt EB okay so I'm going to scale this
service to one for now and I'm going to
have an API that's running next thing
I'm going to need is the consumer right
so the consumer is actually consuming
Casca messages so I actually maybe
should have showed you that we do have a
Kafka instance running I only have a
single broker for space right I would
normally have three as a minimum so
maybe just a point out something and
that is that Kafka is an executor and so
when you deploy that DCOs schedules the
Kafka executor and then the executor
says how many brokers do you want and so
on and it manages its own brokers in
your DCOs cluster it's not a container
which we're glad about it's actually
running on the metal and it's a process
it's just being scheduled which you can
do right so the scheduler can process
can run processes it can run Linux
containers docker containers and so
forth right so in this case it's a
process it's an executor and it's got a
whole bunch of config settings that say
things like how many brokers do you want
so for example if I were to search for
broker broker and I'm not getting there
there we go somewhere broke up with a
worker and it will say how many I'm
asking for
somewhere it's really hard to find these
things in here so maybe I would have to
go to the config so hold everything oh I
was in the I was in the task sorry
that's why the task definition is is is
up here so this is where I would go so
this is saying a broker
let's see broker is it highlighted how
many brokers do I want broker count one
so if I I would normally have three as a
minimum so that's the idea so it's got
its own number of a Virant variables
configuration settings but it manages
for you so DCOs is responsibilities just
to keep the executor alive the
executives responsibility is to keep the
broker alive and manage all the bouzouki
per interactions and so on right okay so
what Kafka is is of course a message
broker and it's going to be receiving
messages from the client and so where I
was going with that is my consumer is
going to be the one processing the topic
and so the topic there's actually a
topic for history and there's a topic
for login data so this is an interactive
application that will let me simulate a
login the lockout login logout reset
password and putting those events
through the system in a event sourcing
style format with past tense events
carrying the fact that happened right a
user logged in a user was logged out
exceeded retries that kind of thing and
so I'm going to start the consumer and
again this guy I only really need one
instance right for it to run because if
this consumer dies the message is to
stay waiting and I want to be alerted so
I like it when that goes red here
because that's just a way to fail fast
right again there's lots of ways to do
stuff that's just what we happen to
subscribe to in some recent examples so
let's go to the website so the website
I'm going to actually again sort of push
down how much it's asking for and go to
deploy one instance of the website so
now I have a website sending message to
Kafka Kafka being consumed by the
consumer so listening on the topic and
then calling the API and then the API
shipping those updates to the user
database
so once the websites running I'll be
able to do a run-through of that and
just to show you I do have the database
over here so right now if I were to run
a query on management and let's do that
and let's see how many users I have in
here so CDOT username let's see what I
have so these are the names that I've
put in there so far just so that you
know like I have nothing up my sleeve if
I add a new name it didn't already exist
no smoke and mirrors okay so the web is
still thinking this guy seems to take
time to run though so I think I need to
be patient I'm at the same time I'm
going to go and check out these nodes so
I do have space it's looking ok let's
come back over here and I think we're
who are we flapping could be so now I'm
not quite sure if I just saw that flip
and I've got this completed count of 57
if that goes up again seconds ago right
then that means it's failing and it's
trying to keep starting it and it's
going to never give up
so we need to figure out why that might
be flapping as soon as I see if the
count goes up here so give another
second I'm suspicious that that might
happen or maybe I'm actually just
successful let's see darn it I wanted it
to fail no actually it is running will
make a failure later don't worry I I was
looking forward to that okay well maybe
we found our failure after all it may
not actually be flapping after all
that's not so good let's see ah vici it
disappeared came back staging again
okay we're flapping so it's running for
a bit and then it's blowing up why I'm
going to go to my instance and I'm going
to check the logs see what I've got
could be a health check issue sometimes
so data consistent okay so this is
nothing important
but there looks to be some other logs up
here standard oh let's go to standard
air no these are actually not these are
just warning so they're not an issue so
let's go over to the actual service so
I'm going to go up a level and let's go
to debug task failed with 137 okay so
usually when that happens it's it's I
mean something is running and failing
basically in fit and it's not able to
launch so I'm going to go edit my config
here and see what I could have done so
let's go in here and it could've me I
need more resources I don't think so but
I'm going to just toss that back where
it was and we're going to take a look at
the configuration in here am i hitting
the right broker that could be another
reason to fail but that looks good it
looks like I'm pulling from the right
repository it looks like I have all the
right ports that I had already in place
so that's all good so there's really no
obvious reason so let's go ahead and
deploy that and see if by making a
change in deploying I can actually
recover and so go over here to active
tasks it's running but is it going to
fail is the question give it a try
okay I'm going to just see what the
latest error is so let's go in here so
this is the failed request and we'll go
back to those logs and see we can find
there's got to be an explanation um so
this is all just health check and this
data protection key is just a warning so
that's something for asp net producers
created that's good so that's expected
what it could be doing we will see I
knew I shouldn't have subtended that
tired it's all working
running seconds ago so 62 we're
bordering on the unexpected at this
point I'm going to see what I can do to
look at resources okay
two minutes that was the last failure
two minutes ago seconds ago not good not
good
hmm I have to think now a minute ago
well no that's good that means it's
still there
so we may have actually succeeded maybe
maybe not unavailable no that's not what
I want
just regular endpoint ooh okay so it's
not coming up something's wrong
now I'm going to have to figure that out
let's see how many try should I give it
this one got a new image somebody tell a
joke or something really okay
accepted resource role any that's fine I
could call this slave public and see if
it goes
okay if it's a resource issue I might
need to get it onto another node and the
public node has some space and since
it's the public facing website I can
tell it that it should have affinity
with the public so let's see about that
although very seriously this is running
right before I shut it down so this is
fun okay what else can I do make the
smaller not to use so many resources no
other real realistic things that it
could be if I had versioning on my
images that would be helpful but I'm
pretty much calling everything latest so
yeah that lets see you events demo and
that's demo web okay I'm going to just
show you the end of the demo then if
that doesn't work out and you're going
to pretend you saw it and then I'm going
to dinner lies you with this and it's
going to be awesome
yeah just be ready for that okay so tags
what have I got in here everything's
latest and it hasn't been updated by
somebody else checking in stuff behind
my back so it should be fine it's my
blog right okay so let's see if it's
running and if it's not running then I'm
going to have to use brute force
somewhere here let's see what we got
are we actually running this time I'm
just going to drink this is vodka by the
way I'm just going to have a little bit
just just a lot maybe okay so who vodka
that's a good name for a user one two
three email vodka at demo fail comm but
it actually succeeded so well that's
okay so the is getting filmed that's
awesome I just said all those things
so so this UI is going to send a message
when I hit create user so I'll ship that
off and then once I have a user that's
been created so we're not thinking about
events in the system so why you eyes are
all going to send events of a thing that
happened and then it's going to be
processed by the backend so the consumer
will pull the events are the messages
out of Kaka and then call the API and
the API is going to project a current
state store of that message that says
user created and it's going to update
the user record so when a user's
registered that's one message when the
user verifies their email it will update
the fields that verify the email when I
hit login it will really do nothing to
the user record because that's just a
login record but it would be useful for
history right so there's that I can log
out I can have a failed login attempt up
to two so this is just simulating the
end of that message so every time I log
in there's going to be a message
generated by that runtime api somewhere
else and it's going to say if it hits
the the limit in the business rules it's
going to say you know failed attempts X
and it's going to lock me out and set my
user record to locked out with a number
of failed attempts and then I can so
simulate that with a lock account
simulate an unlock and simulate a
password reset so these are all things
that I've interact with UI it would
trigger either a message that's doing a
write so if I'm separating my rights and
optimizing read then messages are for
all the rights and then sometimes when I
am reading or logging in other messages
are generated that indicate a state
change as well right and then some of
those messages may just be history so if
I come into my dog DB I should be able
to run this and see that new user so
vodka is here and then if I want to
check the history table I should be able
to see history because there's a second
consumer running that can produce
history so for example if I said show me
history
show me C dot underscore
message type and let's run that so now
I'm going to see real history and I
could of course argument this by user
right now I'm just showing you quickly
so user created user logged in fail
locked out unlocked and then there's
more data in each of these messages I'm
just showing you the list right and so
this becomes now a different projection
a history projection and where it's
coming from is this guy over here my
history so that might be old history
just saying because I had the suspended
all that later and I obviously had some
other data in there but you know this
going to work too and the point is that
it will generate for then that user that
I just ran right the vodka user so
you'll see email confirms and all of the
the messages that came so I've got a
projection of the current state a
projection of history that I could have
brought on later on that replays those
messages and I have a message archive
now when we talk about the scheduling
aspect the thing that matters about the
scheduling aspect is the following if I
go into this API and I edit it and I go
into say the container settings and I
call this API two and I redeploy so
something's going to happen where the
API is not available now right so let's
go see it's going to is it going to be
unavailable though good question
so this API is now waiting so there's a
deployment waiting waiting is bad so
waiting automatically means something's
up and so if I go back into API over
here and look at debug it's going to say
it couldn't find the container so is the
system down probably not
why
if I create a new user beer because why
not fear at thirsty calm and so let's go
ahead and create that user and let's go
over here into our doc DB and go back to
my management table and to that listing
again of user name and from the query
and I have beer so the API is up and
running even though I have a failed
deployment in them in the works and
that's because it won't retire the
existing until I have a successful
deployment and so where is this so I'm
in a particular task because go into the
API deployments still trying to run I'm
going to go edit this I don't see my API
- that's kind of weird
hmm did it not save that that's weird
okay what's that oh yeah sorry yeah I
knew that of course I knew that so this
is going to replace that deployment and
then it'll come back to life now it can
only do that if it has space so if I'm
really tight on space on the node it
won't even be able to bring up the new
one and retire and drain the old one
right okay so let's try a different
failure which is kind of important if I
go over my services and again hit that
API what I'm going to do instead is I'm
going to change an environment variable
I actually have a doc DB service
endpoint and we're going to call this n
DC mini - and deploy that change then I
have a new environment configuration bad
database reference and it will so it's
not going to bring down the other
service right so I need to make it
actually go to zero so that I can deploy
the bad guy so I'm going to do that
don't do this in production this is just
a demo people so now I'm going to go to
one and get a bad deployment out there
isn't it nice to know that's hard to do
okay mission accomplished so we're
flapping it's trying to access the
database right away now the question is
what's going to happen with the consumer
if I start hitting the webpage so let's
go ahead and go back home
and we'll call this one martini because
we're on a roll
okay great user so that's written to
Kafka the consumer is going to pick up a
message and it's going to try to process
that message so it's going to also try
to hit the API and when this guy tries
to do that at some point it will fail to
replay that message and it will just
stop and it will start flapping as well
we want that flapping because that's a
good flap in the sense that I'm not
losing anything my still h.a api is up
and running so I have high availability
no problem but what I'm not what I don't
have is what I do want is a quick
indicator right of the failure of the in
gesture or the consumer so that I can
investigate right what's really wrong
with the whole system and it's just a
you know a great way to sort of quickly
get alerted instead of letting it have
messages and I have to go to the logs
and so I mean this is like surfacing an
actual component not running which is
sort of an earlier warning if you will
so we like to do that sometimes okay so
moral of that story is you've got the
scheduled h.a things you've got the in
gesture that would be okay to try to
pull and flap and fail eventually if it
can't access the resource it needs to do
its job and lastly we can talk about
jobs so let's do that so quick example
of some jobs that can run so here's what
we're going to do I'm actually going to
show you very quickly if I wanted to add
a job schedule
you know I can put just a command right
literally I could say something like I
don't know - see less than let's see we
want to do like ping comm or something
so like I can ping a destination I can
call this one ping - and I can go to the
Advanced Options and do some things I
can set this to zero that just means
don't schedule it and it will just run
once and so I'm going to go ahead and
add the job so this guy will say it's
overdue already because it doesn't have
scheduled and it will run when I tell it
to and this will try to schedule that
job on my cluster now all that's going
to do is run a console that just
iterates a ping just for the point of
saying that you can run commands you can
do janitor activities you can do things
like that but probably more interesting
are things like having an actual job so
this is an example of a job let me go
edit this that is it's running a
container these are the environment
variables for the container what if I
had that concept that I gave you with
the customer where for example they had
to pull data from many different
locations in their organization and they
did it on schedule so if the job could
be scheduled the same container but run
2,500 different ways and something
automates pushing that like your CI
process your CI CD process for that job
and then that job is just here and it
knows to run every day so it will
schedule all these guys and so that job
when I hit say go go job 3 and job for
it's going to queue those jobs to run
but they need resources right so what's
going to really happen is they're going
to wait they're not going to quite get
out here to the DCOs until there's
actually some room for them to run so in
terms of tasks active tasks we're not
going to see any activity on those jobs
until you know we have some space to run
so if I go to the nodes for example
and let's say I go to this guy I may or
may not see one of the jobs running just
yet so you see this is showing me the
activity here it's still trying to run
the API it's not working out so I'm
going to go to the services and I'm
going to get this guy suspended let's
get rid of that old demo let's make some
space let's go back to the nodes and
let's watch what happens on this guy
so this is now waiting possibly for
something to get scheduled I'm going to
go over to Chronos and I should be able
to see things are queued oh and it might
have already run there you go how would
we try that again job jobs job there you
go
and at the end by only one more minute
no just kidding and so this is showing
me the job all right
I found the job that's running job three
it will schedule as it has time all 2500
could sit there they'll schedule there
wait for resources it doesn't interfere
with the rest of your cluster and that
would be all we have to say about that
and so this task is running the job and
it's going to end up with some output as
it iterates through the pings and at the
end of it it will retire and the Chronos
will show that it ran at whatever point
in time and so I guess with that I've
shown you at least four or five
different scheduling sort of
capabilities and also sort of an
overview and that's all I have to say
thank you
and a demo finally works that was
awesome
I had to clean off images from a VM to
get that to work so hey it's just going
to say that I won't tell you that the
beginning because I would have really
set us up for failure so I was just
hoping for the best
yeah alright Cheers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>