<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Resilient Applications In Microsoft Azure - Scott Allen | Coder Coacher - Coaching Coders</title><meta content="Building Resilient Applications In Microsoft Azure - Scott Allen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Resilient Applications In Microsoft Azure - Scott Allen</b></h2><h5 class="post__date">2017-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CeAm6f_YzAU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good it's good to be back at NDC Oslo
again it's my favorite conference in the
world to be honest my name's Scott Allen
and I came to talk to you today about
resilient applications in Azure so if
you want to build resilient services
resilient applications what I want to do
is show you some of the azure specific
features that you can take advantage of
as well as some general principles and
practices that you could apply with any
cloud provider and what we're really
talking about is I went to build an
application or a service that is going
to be highly available there's no
perfect availability in the cloud
something has to go down at some point
so we just need to figure out how highly
available do we want our application to
be yes I would like my personal blog to
be up all the time but I'm not willing
to spend the money to make sure that
it's up 99.999% of the time it gets very
expensive and we'll talk about that and
that's why the amount of resiliency that
you build into an application or into a
service yes there's going to be some
technology choices that you have to make
but there's also some business choices
that you have to make how much are you
willing to spend to give yourself a
highly available application not spend
just in Azure resources because you're
allocating redundant storage or
redundant virtual machines but also
there's going to be a price to pay in
your application and your code base the
complexity and the code that you write
because as we'll see instead of you know
writing a simple controller for a web
api that just depends on a database if
you truly want to be resilient your code
will be prepared for those times from
that database is not available so maybe
you have to fall back to a read of cache
or some other information source and
that just conflict complicates the code
complicates debugging and cost more and
it turns out that if you're highly a lot
of the patterns and things that we'll
look at will make your application not
only how you look highly available but
also allow it to recover from a disaster
maybe a natural disaster that takes all
takes out an entire data center that
would be really bad or just a disaster
that takes out a hard drive on one board
machine so what do you want to recover
from if you're worried about the natural
disaster that takes often it and entire
data center that's a lot more expensive
to make sure that you can handle that
situation than just recovering from a
failed harddrive so those are the kind
of topics that we'll talk about and the
first thing that you need to understand
when you're doing any of this work is
it's important to understand the cloud
platform that you're using whether it's
AWS or asher
or Google and understanding the
different resources that are inside of
that platform not only how you can use
them but answer has the huge surface
area now virtual machine storage
networking app services developer
services things like HD insight machine
learning you have to look at each of
these individually if you're using these
in an application or service you have to
look at them and first of all ask how
can this fail what do I do if it does
fail because many of these resources do
have some sort of contingency plan if
they go offline and it's also important
to understand the availability numbers
for each of these resources and under
which conditions those availability
guarantees apply so for everything that
was listed on that previous slide you
can do a search for either service level
agreements and you will come to a web
page that looks like the foot lift act
and everything that you can get to an
azure every resource that you can create
and so if you're using let's say a
virtual machine and an azure sequel
database it says if you're really
planning for high availability it's
important to go into this page and look
at the service level agreement and let's
say click on virtual machines and in the
opening paragraph for that service level
agreement Microsoft will give you two
pieces of information under first of all
what is the guaranteed uptime like 99.9%
or 99.95% and also the conditions under
which they will guarantee that uptime so
if you go out and provision a single
virtual machine and azure there is no
SLA or guaranteed uptime if you go out
and provision a free or basic app
service to host a website again there's
no SLA there so you have to understand
under what condition
there's an SLA for an app service I
would have to go to at least the paid
basic tier to get a 99.995% SLA and then
once you have once you understand all
the resources bigger application is
using and what the guaranteed uptime
numbers are you sometimes just have to
work through things like okay if I'm
using a one resource that is 99.9%
available and by the way if Microsoft
doesn't meet that number within a month
they'll also state in the SLA what you
can get for that and it's typically just
a credit on your subscription but if I'm
using one resource that has 99.9 percent
guaranteed uptime that's 43 minutes of
downtime a month but let's say that's a
resource that depends on another
resource some database service is
something like that that's 99.95%
available that's another possible 20
minutes of downtime that of course those
two don't overlap so my application my
service could have 60 minutes of
downtime in a month
and Microsoft wouldn't have to provide
me any credit today if they have met
their guaranteed uptime numbers and
those are the kind of situations we have
to study and say is that enough for my
application do I have a service level
agreement with my customers or my
business partners or the people that are
coming to my website and if I'm down for
60 minutes a month will that impact my
business or hurt my business and if not
that's when you have to start taking
some approaches and some strategies that
we'll cover in this talk about improving
those numbers so be again going back to
the topic of every resource and azure
has some redundancy or some contingency
plan built in for when something does
fail so for example it's important to
understand that if you're using blob
storage it's very easy to configure
redundant storage in the portal so for
example if I go to any storage account
in the azure portal that's visible on
the back ok thank you
hard to see up there and go to the
configuration for that storage account
every every storage account all
for me some different types of
replication that I can use to make sure
that when disaster strikes I'll still
have my data so there's locally
redundant storage that's I don't believe
that's what the fault it was the default
for the way I had something set up here
but locally even under storage is every
time you write a file into blob storage
as is going to make three copies of that
blob inside the same data center so even
if they an entire rack of servers
catches on fire and they take it out of
the building there'll be two other
copies of my data somewhere in that data
center and that might be fine for my
business but if I'm a little bit worried
about an entire data center being
offline then I might go to Geo redundant
storage which would not only keep those
three copies in the one data center it
would also make copies in another
partner data center that's hundreds of
kilometers away at least and then the
one that a lot of people go with is read
access to your redundant storage and
I'll talk about how you can take
advantage of this in just a bit but
that's basically G over done in storage
where my application writes a blob into
blob storage is a synchronous
asynchronously replicated within the
local data center to other nodes so that
I'm sure I have three copies of that
data but it's also asynchronously
replicated geographically to another
data center where I can still access a
blob and read it so that's good but of
course read access to your redundant
storage is going to cost just a little
bit more than locally we're done in
storage for virtual machines if you need
to set up a virtual machine in your
application or for your service it's
important to put that inside of an
availability set so that Asscher can
give you an SLA and say we will
guarantee that you should have 99.9%
uptime basically when you set up an
availability set for a virtual machine
and Azure as our lets you configure two
important features of that availability
set or when it comes to high
availability one is the fault domain so
you would put these different virtual
machines in different fault domains the
idea being that if I have two virtual
machines they're not missing fault
domain
then if something like a power supply on
a rack of servers goes bad it shouldn't
impact both machines at the same time or
if a network switch on the top of that
rack goes bad
it should still be another one somewhere
in that data center that is available in
running so there's that's Fault the
mains and those also update domains
which is if a sure has to reboot your
virtual machine because it's updating
the hypervisor let's say for all the
machines on that physical server a
durable guarantee that it's not going to
take both your machines down if they're
in two different update domains that's
easy to configure through the portal
also for as your sequel there's a couple
things you can set up replication for
sequel server and Azure with Azure
sequel so you can have a a primary and
up to I think it's three or four
secondaries right now so when you commit
a transaction into an azure sequel
database that transaction will be
asynchronously replicated to other
sequel servers that are geographically
distributed and if the primary sequel
server fails as your sequel can failover
to one of those secondaries and there's
also point in time restore with sequel
server and that's sort of an interesting
situation because replication and point
in time restore are really two different
things
replication is a must-have but my big
fear is always that we'll release an
application that has some logic bug that
will update a table and set a million
customers to inactive and as your sequel
will happily happily replicate that
erroneous data all around the world and
the only way to recover from that is not
through the replicated data the only way
to recover from a catastrophe like that
is to get it to a backup and to a point
in time restore so with a lot of the
azure sequel plans you can have I
believe it's 5 or 15 minute increments
automatically or you can just go back
and say hey I want to rollback to 2:30
p.m. this afternoon before I deployed my
application and then there's app
services app services we'll talk about
in more detail here but with app
services it's easy to take an
application or service and deploy it
into Azure app services and deploy it in
a couple different data centers it's
very easy to deploy these things and
main
this thing from that's one of the
scenarios we'll drill into more detail
here in just a bit let's start with a
just a simple scenario and we'll keep
building it up let's say you're not
willing to spend a lot of money on high
availability all you want to do is
implement a web application deployed
into an azure
app service and that app service is
going to use blob storage and Azure
sequel we're not going to worry about
replicating sequel server we might set
up blob storage to be a to be geo
replicated just so that we make sure
that we save any files that get placed
in there there's still a few things we
can do in this situation that aren't too
expensive that will give us better
availability numbers one is every app
service that you create in Azure is
associated with an app service plan and
it's the app service plan that describes
the type of hardware your web
application is going to run on so do you
want a two core machine or a four core
machine be one three gigs of memory or
seven gigs of memory and the app service
plan you can also describe how many
instances that you want of that
particular server so it's really easy to
go into an app service plan and say I
want two instances and Azure will
automatically have a load balancer in
front of those two app service servers
so that one goes down there's still
another one that's always running so
that's nice for scalability but also for
high availability or higher availability
and another thing you could do that's
very easy is set up connection
resiliency so the idea if your app
service tries to connect to a sequel
database and it initially gets rejected
and that could happen for any a number
of reasons there could be something
going on at the network at that time the
network could be saturated you also
could be connecting to a database where
you haven't provisioned enough power and
as your sequel enough dtu's and that
server is basically just saying no we're
not going to accept this request right
now we're overloaded if you could keep
retrying that connection to get to the
database and issue a query that would
give you a little better with this
resiliency in your application and
fortunately a lot of the SDKs that
Microsoft provides for various adder
and resources have retry logic built-in
all you have to do is just use the SDK
so for example as your storage yes
behind the scenes everything in Azure is
HTTP so if I want to upload a blob and
the azure storage all I need to really
do is an HTTP POST operation and I can
put something in a store storage but if
I'm just doing low-level
HTTP put and posts and things like that
there's I would have to build my own
retry logic if I just use the azure
storage SDK wrapper and there's one for
c-sharp code there's one for JavaScript
and Ruby and all the popular languages
that will already have some retry logic
built in in fact this little bit of code
here would show you that when I create a
blob client with some sort of connection
string that would grant access to a
storage account I can set up some blob
request options to say first of all
location there my location mode is
primary then secondary so if I have that
read access geo redundant storage set up
for my storage account what the SDK will
do for me automatically is try to go to
my primary storage account location
let's say it's in eastern United States
and if for some reason they can't cannot
reach the blob storage there it'll
automatically fall back to a secondary
which I might have configured for
central UF's or West Europe or somewhere
else in the world and that'll just
happen for me and then there's different
sorts of retry policies the default P is
an exponential retry where I think it
will retry three times and it'll just
keep increasing the timeout after each
retry B there's other retry strategies
that are in the SDK and you can write
your own retry strategy
you just have to implement a specific
interface and plug it into the SDK for
the entity so as your sequel is one
exception there's no there's no SDK for
address equals just using a do net but
in the entity framework both the entity
framework before dotnet core and an
entity framework or there are some
resiliency features built in essentially
with entity framework or this would be a
snippet of code from an asp.net core
application or dotnet core application
there's a sequel there's an extension
method for sequel options where I can
say enable retry on failure and the
entity framework is smart enough to look
at the error codes that are coming back
from Azure sequel to say oh that's the
type of air that I could retry it's not
the type of air that you know hard
permanent failure or a logic error
something that I could retry so those
things are built-in same thing for
document DB same things are as a rebus
cache so a lot of these SDKs have that
logic built-in you just have to
sometimes just turn it on and take
advantage of it and that's good
otherwise what do we do so let's say
we're trying to get to sequel server and
sequel server really is down so I am NOT
even with all my retries I'm not making
it to sequel server to document TD
that's when things start to get a little
more complicated and a little more
costly not only because you have to
purchase more resources but also the way
you implement your code so the database
isn't available what do I do
well if there's a reading to do a read
operation maybe I could set up an azure
Redis cache and Azure and I'm
aggressively caching information that I
read out of the database so now when the
database is down I'll have the proper
abstractions in place in my code to go
to the maybe I'm going to the cache
first in any case and if the cache is
down I go to the database but you could
work it in either direction and for
write operations ok the database isn't
available so maybe I'll drop something
in a storage queue or an azure service
bus queue and one thing that you might
notice here is that I have read
operations that go in one direction and
write operation to go in another
direction so if you're familiar at all
with the cqr apps for command query
responsibility segregation that isn't a
fantastic pattern to use in application
programs applications and programs that
you want to be resilient and scalable
CQRS is something to look at let's drill
into this a little more in a little more
detail ok so now I don't have a simple
just app service anymore
I want I'm going to deploy my web
application or API to an app service and
as
which is really easy to do from visual
studio and that app service is going to
always talk to a cube when it needs to
you know instead of talking directly to
the database I'm just going to drop
messages in the cube that will
ultimately update the database but those
messages will go through as your web
jobs that can just a synchronously fit
and run and listen for messages in a
queue or as your functions if you prefer
to do the same thing in a service layer
and that would all be my storage account
I could also set up a CDN and a share
yes yes yes we'll talk to document DB
will talk to Azure sequel basic
architectural diagram here but the piece
I really want to call out here sorry is
the queueing so to me building a
resilient application is going to
require queueing if you're really
serious about it some sort of message
keys because message queues can do a
couple things not only hopefully will a
message queue be available even if a
database is not in a good team but a
message queue can also help you do
things like load leveling so part of the
trick to high availability is not just
avoiding disasters it's also avoiding
the crushing weight of too many users
coming to the site at once and
overwhelming a portion of my application
either the web server or some of the
backend processing and by introducing a
queue what I can do is take all these
requests from users to purchase orders
or purchase plane tickets or whatever
they're trying to do and drop them into
keyed messages and and slowly process
through them or process through them as
fast as I can but not overload some of
the back-end systems that are now trying
to make too many connections because
there's too many users on the site and
for those of you that have a used server
service bus I just wanted to show you a
small code example and show you in the
portal just how easy that is to set up I
do have a service queue already
configured service bus Kido
which is here we go when you go in to
create a new resource and Azure you
would ask for service bus queue and you
fill in some basic configuration
information like the name of the queue
that you want so this one is called or
the namespace for this is PS dash
messaging and it has one Q inside right
now called orders so there's basically
there's queues and there's topics in
service boss queues are really about I
have a message I'm going to put it in a
queue and there's one person listening
to that queue to pull it out in the
processes we're topics or more like a
pub/sub mechanism where i can say i have
a message to put into this queue this
topic and it's really the same api but
there can be multiple listeners on that
topic to pull out messages and perform
different actions so maybe one component
is listening to a topic to send off
email notifications and another
component is listening to that same
topic to update the database and the
code to use this orders queue is
extremely simple how do I send a message
to a service bus queue it's literally
just three lines of code so this is a
very crude program but basically create
a queue client from a connection string
a connection string would look something
like this
so my endpoint is PS - messaging that
service bus windows.net
and then there will be an access key or
a secret token in that connection string
give me a cue client you create a new
brokered message what I can do is to
Jason's put a string in there put JSON
object put XML in there whatever I want
into a brokered message and then just
send that to the cube so it's really
about four lines of code there and to
read something from that queue
one way to do it you just set up an
answer function or an azure web job or a
service on a virtual machine it sits on
a loop and not even a loop just listens
for a message client a phone message so
for every message that arrives in that
scene queue will pull that out get the
body from there we could deserialize it
using Newton soft or something this
program is just turning out the body but
it assumes to be a string printing out a
message ID simply just giving you some
ideas about resiliency but let's get one
step further so let's say I have that
set up I have queueing in place I'm
using an azure Redis cache I'm using
Azure sequel and this geo replicated but
now I'm I have an application that I am
willing to spare no expense to make sure
it is available all the time it's highly
available as I can make it and I'm even
worried about an entire data center
going offline or an entire data center
going away that's where I could take
that environment app service queue
database and replicate it through a
couple different data centers inside of
azure and then use a load balancer
called traffic manager to send my
customers to one of those two regions or
103 or 104 let me talk about traffic
manager for a second load balancing is
actually important not just for scaling
up and balancing a load across multiple
resources load balancing is also
important for high availability because
one of the things the load balancer can
do in Azure is detected oh that
particular machines down now so let me
send all the traffic over here to the
machine that's currently up and there's
three different load balancers and adder
although they there's only one that has
the name load balancer so if you go into
the azure portal and create a new
resource and say I want to create a new
add your load balancer what you're
getting is a layer 4 load balancer so
that's a load balancer that works at the
tcp/ip level it will accept incoming
tcp/ip connections and then you will
configure endpoints behind that load
balancer until it forward to one of
these places one of these virtual
machines
and it'll forward things along so
that'll work for anything that works
over TCP or UDP so you could be doing
HTTP traffic or you could be doing
WebSockets
there's also what's known as the azure
application gateway that's a layer 7
load balancers so this load balancer
under what's at the application protocol
level and it understands natively
protocols like HTTP and HTTPS and
WebSockets
so one of the neat things about the
azure application gateway is because it
understands HTTP you can have it
implement things like sticky sticky
sessions it's basically a reverse proxy
so you could sit it in front of a web
application and tell it that when the
client makes a request it should set a
cookie so that all the requests
subsequent requests from that user go to
the same machine because for some reason
you know we want to keep the user on the
same machine probably because we're
using in-memory session which is turns
out is actually not a good idea for high
availability but it's possible with that
particular load balancer and then
there's traffic manager the one that
I'll show you a little demo of here
traffic manager works at the DNS level
so if I'm using it in front of a web
application the way it's being used is
my web application when I type in Stata
uncom my web my web browser goes off and
says oh dear network what is the IP
address for SCADA uncom and it's
ultimately answered traffic manager that
would answer that question and say oh
here's the IP address and it will answer
based on some rules that I program into
it let's take a look at that real quick
so I have a couple copies of the music
store that are out there
I have asp.net core music store one of
those is in deployed in a data center in
the east US so PS - music store - East
US is the URL for that and if I look at
the bottom of the browser here I have
the layout page read an environment
variable that actually tells you what
data center you're in or what ad your
region your hand so it clearly says East
US
and I have another version of that
running in lefty you just make sure that
is up and running so PS - music store -
West EU - your website snap but of
course I don't want my customers to
necessarily know there's a West Europe
version and an easier version what I'd
like them to do is just go to some
custom domain or something like PS -
music store Traffic Manager dotnet see
if I type that correctly and that
particular request actually went to a
short Traffic Manager I'll show you the
configuration here in just a second but
as your traffic manager in this case
returned the IP address of actually the
West Europe deployment because that's
the way I have traffic manager
configured it's going to send me to the
data center that has the least latency
so the URL there PS - music store
traffic manager net how would I set that
up well inside the portal I would go to
let's come over here to all resources
first I am well first of all we come in
and I would say dear Azure I want to add
a new Traffic Manager so if you haven't
been in the azure portal lately you
would come in and click the plus button
and you would say that you want to set
up a Traffic Manager profile that's what
I already have set up let me just show
you the configuration for there's really
just two key blades of important
information so here's the Traffic
Manager profile so first of all is the
configuration itself in this
configuration I can first of all set the
routing method for traffic manager
performance is a way of telling traffic
manager send the request to the data
center that has the least amount of
latency so if I'm over here in Europe
chances are I'm going to go to the West
EU or from in the United States chances
are I'm going to go to the east us
there's also Geographic so that would be
a way to say that I want all EU
customers to
stay and go to an EU data center that I
have configured and all US customers to
go to a u.s. data center so it with
disregard latency would just strictly
route people route requests based on
where they're geographically located
there's also priority so priority so
first of all performance is a good way
to say I just want to give my customers
the best experience so I have all these
things set up and running of spared no
expense I have app services and
instances running everywhere just send
them wherever has the least latency
wherever makes them happy I could also
do priority which would be a way of
saying okay I have one data center this
is one thing you can do with it I have
one data center where everything is set
up and highly-tuned and I have
everything scaled out and that's the
number one priority but in case that
goes down here's a number two priority
that I have set up in another data
center somewhere and you can start
directing traffic over there if you've
detected that the first priority wasn't
good there's a bad shape and then
there's weighted the weighted routing
method and that's actually an
interesting one because I could say for
example will send 90% of my traffic to
East us and send 10% of my traffic to
West EU and it could be any two
deployments but that one can be
interesting because you can set up one
of the situations where you have a new
release you want to you want to put it
out in the public you want to see how it
behaves you want to see what angry
customers actually do to this thing but
you don't want everyone to see it
because if it has a bug or something it
could cause bad press so let's push that
out and we'll have Traffic Manager set
up to just send let's say 5% of our
traffic to that new deployment over here
and we'll evaluate it and monitor it
make sure it's actually going to work
out in the wild so once you've chosen
the routing method based on what you
want to do you can also set the DNS time
to live so remember this is a load
balancer that worked by responding to
DNS queries you know what is the IP
address for this domain it will hand
something back to the client you might
want to make that shorter you know if
you're doing failover testing and things
like that but the default is five
minutes so if something falls over your
one of your customers will have to wait
at least five minutes before getting a
new IP
address and going to a new data center
and then there's also monitor settings
and traffic manager this is how traffic
manager determines if a service is
operating normally or if there's trouble
so basically I'll show you how we we
plug in endpoints in the traffic manager
which is a way of saying here's the
different places to route traffic and
what traffic manager will do is go to
each of those endpoints and make a
request on this port using this protocol
and at this URL so I could change that
to something else and if that request
does not return an ACD status 200 code
traffic manager assumes that there's
some sort of problem there and if it
does that I believe three times in
traffic manager says ok no more traffic
over there there's obviously a problem
and it has to be a 200 response not a
302 response or anything else has to be
a 200 response so in this case I just
set up to hit the root of the website
make that reclass see if you get a 200
back but you can do interesting things
here too so for example if I had a
specific URL for the traffic manager
probe then the logic behind that URL
inside of a controller or somewhere can
actually do some system checks and say
oh you know we're having trouble with
sequel server maybe we should return of
a 500 arehere for the for the check and
get some of the traffic manage of
traffic away from us that's or to say so
in here that configured them as
endpoints so yes discard some edits this
is basically where I go in and I tell
traffic manager hey I have this new
endpoint it is sorry for the scrolling
it's an azure endpoint or an external
endpoint so you could use traffic
manager to route to stuff in your own
data center if you wanted to it doesn't
have to be inside of azor an azure
endpoint or nested endpoint that's that
you can also set up nested traffic
manager profiles which are interesting
so maybe the top-level profile weights
based on geography and then once you've
out to the next Traffic Manager it looks
at the requests and does some
calculations based on priority or weight
or something like that
so my app services I would say hey I
have an azure endpoint give it some name
the target resource type to be an app
service or apps or the slot or a public
IP address that is assigned to one of my
virtual machines and then since I chose
app service here the portal would give
me a list of app services to choose from
and it's really just that easy
so now Traffic Manager is running and is
probing this two different endpoints and
routing traffic there is appropriate
load bottling so another aspect of high
availability is what happens when I have
customers who are calling a service and
some of these customers I may not trust
they might be doing something odd they
might have a bud bug in one of the
back-end services that's calling my
service and it's running in a loop and
issuing a million calls a second and is
overwhelming my back-end service what
did you do in a situation like that
there is a technology in Azure called
API manager that you can set up that
will implement a number of features that
one of those features would be to
swaddle calls one assists from a
specific client or on a specific
subscription or who's using a specific
access key so that they don't overwhelm
things in your system I'll just show you
API manager real quickly let's go back
to all resources
and I'm looking for Oh was at the top
this API management service so here's
the way this works mmm
someone could spend a couple hours or an
afternoon describing
excusing everything you do with the API
management service but the basic idea is
that I have a set of services that I
need to manage and that I need other
people to be able to access and sign up
for and I need to do this in an orderly
fashion so I can use this API management
service and Azure to wrap all of my
services across disparate things I could
implement them and functions and put
them in virtual machines and just have a
collection of services everywhere but
provide a cohesive single endpoint for
my customers and business partners to
use which would be PS music stores er -
API net and I can provide them a
developer portal and I can provide
myself and other users in my company a
publisher portal which will allow me to
manage these API s so the developer
portal that Azure gives you API
management is one of those typical
experiences that you would have as a
developer when you want to go out and
use Google Maps or Bing api's or any of
those things you know you have to sign
up somewhere you have to get a
subscription key and then you have to
plug that key into your application as
your API management will basically give
you that developer portal where you can
hand out this URL to other developers
and they come here and they can sign up
and they can manage their keys and they
can start working with your API and this
is also where you can provide
documentation for all of your API s
there's a place where developers can
open issues against your API s and so
forth and then there's the other side of
the house which is under person writing
the API and I need the ability to see
what API is I have published
there's no music store API I need the
ability to import a new API that someone
just wrote so I can go in I can say hey
I have this API endpoint
this other application is exposed to
swagger documentation so I'm just going
to plug in the URL and this API gateway
will read the swagger documentation
figure out all the operations that are
inside and just produce some nice
documentation pages and so forth but one
of the important one of the important
things that you can do with API manager
here is that we'll cancel this policies
and when it comes to things like making
sure you're highly available and making
sure that people aren't abusing your
application in some way
things like policies can help because
not only can you require authentication
require authorization require specific
headers you can also go in and say you
know what I have this music store API
people have not been treating it very
nicely
these are making too many calls against
it so I'm going to limit the rate call
per subscription on this so add a policy
let's use limit rate call per key and I
realize that's probably really small but
there's some XML everyone loads XML
so let's now you can put in a file that
says things like I want to limit the
number of calls we're going to count the
number of seconds where people are
making calls and we're going to make
sure that they limit their calls to I
forget what the action where the actual
number is stored in there but you could
do that a good bottle things let's talk
about service fabric anyone using
service fabric it's curious here service
fabric
so service service fabric is interesting
um service fabric with two perspectives
to look at with service fabric first of
all I want to tell you that service
fabric is one of those technologies that
Microsoft itself uses to implement
things in Azure so if you look at
document DD or cosmos DD or whatever
they call it this week and you look at
as your sequel database those are
implemented using service fabric and
there's two perspectives the first
perspective is the runtime service
fabric is a runtime environment that
allows you to manage services in a way
that will make them available and
resilient and reliable and that runtime
isn't time to Azure in any fashion so
there's a service fabric SDK that I can
install on any machine and there's a
runtime that I can get installed on any
machine so I can have a service fabric
cluster in Azure or in Amazon or in
Google or I could take you know the six
laptops that I have laying around in my
basement and put make a service fabric
cluster out of them the other
perspective of service fabric that I'll
show you also briefly is the programming
model for service fabric so there's also
an SDK and an API that you can use from
Java and c-sharp to create service
fabric reliable services that service
fabric will manage so here's the idea
and say I have the traditional
monolithic application the movie store
application it's a website it's a web
service it's all these things if I were
to really want high availability and I'm
looking at service fabric as a way to
give me that high availability and
disaster recovery what I would probably
do first of all is break movie store
down into there I say something like
micro services but something where the
components are a little more thinner and
identifiable and that I can refactor out
and separate from the rest of the system
so yes there's still a web UI portion
for the movie store but I may take all
the rules and things related to credit
cards and put them into another
component let's call it paint
and I'll show you what it looks like to
implement a reliable service and service
fabric and here's the deal with service
fabric service fabric is all about
managing a cluster of machines and then
you tell service fabric about the
application that you want to run and
what services are part of that
application and service fabric will make
sure that that application and the
services are distributed around that
cluster in an effective manner and
there's a couple different types of
services that you can run so for example
this a service appears on four different
machines
maybe that's the web UI portion of my
application maybe I just want to install
that on all the machines it's pretty
much stateless any state that it uses it
stores in sequel server or some external
persistent store service fabric also has
the concept of a stateful service which
is very interesting for a lot of
different applications a stateful
service and again I'll show you the code
here in just a second there's a service
that service fabric knows you're storing
state and it's going to give you some
role glue some reliable collections that
you can use like a dictionary and a cue
that you can put stuff into and it's
transactional so it looks like
programming with an in-memory dictionary
but it's transactional and it's actually
persisted and it's durable somewhere and
service fabric will make sure that that
state is replicated around in this
cluster so even if one virtual machine
is completely destroyed that service
will still be available and be able to
run and it hasn't lost any data let's
actually just dive into the code for
this so for here before I get to that
let's go over to this version of Visual
Studio sorry this version of Visual
Studio
so when I create something for service
fabric I could do individual studio 2017
I could say file new project I'm going
to do a cloud project and I'm going to
do a service fabric application and what
that would give me as a solution with
two projects inside the first project
describes my application that would be
this project here on building a movie
store but this has no executable code
inside of it it's really just metadata
and parameters that I use to describe my
application service fabric and then also
to publish my application into Azure
which is relatively this simple to setup
and there will be XML files that say
things like this application Looby store
it consists of a couple different
services one called a counter service
which I'll show you here in just a
moment
one which is the web service which is
the web UI for the application and
you'll notice things like instant count
which is basically a way where I can
tell service fabric I want to run this
service and I wanted to have three
instances or I wanted to run on every
node that is in the cluster in which
case I could set instance count to minus
one not just self-service fabric run it
everywhere and they also has this
service this counter service this is a
stateful service so it's actually using
these reliable collections and storing
stuff and a dictionary that has to be
persisted and one of the things I can do
with a State will service is provide a
partition count so partitioning is
important for scalability if I have so
much data that it doesn't fit on a
single node I have to partition that
data across multiple nodes and we do the
same thing with sequel server except
they're charting and MongoDB have
starting and everyone has charting or
partitioning ready to break up Dena so
partition count is a way of saying in
this case I have this range of numbers
that I might use from negative nine
something to nine something
if I wanted to say have three partitions
to cover that range service fabric would
just take that range and divide it up
into three categories and now there
would be one node in the cluster
responsible for the lowest range one for
the middle range one for the high range
you know a simpler example would be an
address book I have the range A to Z and
I could tell service fabric I want a
partition count of two so one service
fabric might set up one partition that
handles all the state or address book
entries from A to M and then a second
partition all the address book entries
from MD D that sort of thing so here
we're using a numeric identifier that's
the application project that describes
the application and the services that
are inside and then there's the services
themselves so the easiest one to
understand would be the web service
which is the web application and this is
an asp.net core application that is set
up to run in service fabric so it looks
a little bit different than an asp.net
core application that doesn't know
anything about service fabric and I do
want to point out that service fabric
has the concept of a guest executable
which means you could write a program in
any language Java Ruby whatever nodejs
and you can have a managed by service
fabric as a guest executable and that
that program that application doesn't
have to know anything about service
fabric but if you're willing to take a
dependency on service fabric you can
allow service fabric to more effectively
manage your service you can also take
advantage of service fabric features so
this is an asp.net core application it's
going to build a web UI but it is
plugged into service fabric it just
taking the dependency on service fabric
so typically when you go into an asp.net
core application and you look at the
main startup method you see things
related to spinning up kestrel and
spinning up the web server and all that
stuff without doing any of that here now
when we're launched we're launched by
service fabric and service fabric is
telling us to start up and one of the
things we have to do when
we were programming a service fabric if
we have to register the service so we're
saying I am the web type service and I'm
going to right now create a new instance
of this service and I'm going to pass in
your context service context that you
provide to me to give me some more
information about my environment and
we're often running so where's all the
asp.net core bits about kestrel and
configuration and all that stuff it's
actually tucked away in this class web
so if we open up web this is where you
would see the code specifically this
code which says I'm a new web host
builder using a web listener or using
kestrel I want to configure some
services I want to use of content
through the startup class application
insight etc etc and once I do build I
now have a server process listening to
port 80 or whatever port it's configured
to listen to it will now take HTTP
connections right very exciting yes but
this is now in a class that derives from
service fabrics stateless service which
means when I execute this code I'm
executing that code to build my web
server and response to service fabric
telling me hey it's time to go out and
create your service instance listeners
so if you need to listen for something
over the network you override this
method and that's where you create
something not well Services has to
listen over the network some services
might just listen for messages on our
service bus queue or a storage queue in
which case they wouldn't have to
implement this exact code right here let
me just show you over in the azure
portal
what happens when I right-click this
thing and say that I want to publish it
out there somewhere
we would go to distort cluster well so
first of all you know I've ran this Web
API and everything else inside this
project is just regular asp.net core
there's controllers there's razor views
there's layout views all that stuff
jQuery scripts whatever you want in your
web UI now I'm ready to push it out into
Azure I want to run this server I want
to run a service fabric cluster in Azure
and of course with Azure there's a way
to click and say I want to add a new
service fabric cluster and you fill out
about four pages of configuration it's
not too terribly bad and the azure
portal will just spit out a service
fabric cluster for you and that consists
of a public IP address so people can
reach your cluster from the outside a
load balancer that will accept let's say
a cheese HTTP connections to that public
IP address and then distribute them
across machines that are inside of the
cluster maybe all of them maybe just a
subset of them there's a resource
representing the cluster itself there's
a virtual machine scale set so this
virtual machine scale set has or should
have five nodes inside of it if you
haven't worked with a virtual machine
scale set it is a resource that you can
create in Azure where if all you want is
just five or five hundred identical
virtual machines and Azure what you want
is a virtual machine scale set you want
to create one of those and it'll just
manage all those instances for you and I
can see I have five virtual machines
running inside of this virtual machine
scale set and traffic will reach them
because of the load balancer that was
configured for me everything else is
just storage accounts and virtual
networks to keep those all on the same
subnet let's look at the service fabric
cluster itself here
and I'm going to open up this thing
called the service fabric Explorer where
I've already deployed this application
by the way and the service fabric
Explorer because I'm the admin on the
subscription that will allow me into
this Explorer to see what is actually
happening inside of the cluster any
moment now
but I should be able to see that I have
a servic service fabric cluster up and
running and has five nodes it'll have
this web service as part of the
application and we'll be able to
actually see what nodes that is on okay
so the service fabric Explorer are one
application deployed that one that we
have on visual CEO I have five nodes in
the cluster I could add more if I wanted
to five is the recommended minimum
speaking of upgrades them upgrade
domains and Fault them in Azure put each
of those nodes into the unique fault
domain so there's a total of five
different fault domains there and also
five different upgrade domains so if
something blows up or something needs
upgraded I should only ever lose one
node I have one application running
inside the movie store this over here is
telling me that I have two services
inside of the application one is the
service that provides the web user
interface this is the instance of that
service that is running and if I keep
expanding things I'll basically see it's
running on all five nodes so it's
running on every machine there so even
if one virtual machine goes down someone
should still be able to get to the web
UI
now that web UI depends on a counter
service but it's just a file new project
thing that I put together with service
fabric all this all this does is this
counter service does is incremented
counter but it's interesting to look at
the code because
gives you an understanding what service
fabric can do this is the world's
quickest introduction to service fabric
by the way this particular service is
implemented in counter dot CS you can
see this is a stateful service not a
stateless service and it has a method so
here where we create service replicas
listeners will create something known as
a remoting listener we don't want the
service to listen over the public IP
address we don't want just anyone
contacting the service we just want
other services inside the same cluster
to be able to communicate with the
service and I'll show you how we do that
in just a second essentially we want
other services to be able to call get
count so we'll return the current value
of the counter that the service is
tracking where do we compute the counter
I'm going to skip down to the bottom
first I can run a sync this is another
method that I can override when I'm
using that service fabric API and
deriving from stateful service what I
can basically do in here is set up a
loop or you know start my listening to a
message queue whatever I need to do what
this particular method is going to do is
say let's go out to the state manager
which is part of the service fabric API
and please give me an I reliable
dictionary with the name of my
dictionary so this is a reliable
dictionary things that I put into it get
persisted in the store and I can
actually back up that state move it
around if I need to and then we're just
going to set up an infinite loop or at
least we're going to loop until someone
raises a flag on a cancellation token
but we're going to create a transaction
if anytime I use those reliable connect
collections I need to have a transaction
in place inside of the transaction I'm
trying trying to get to a value called
counter and down here it's not the
prettiest code we're basically going to
update the back counter the entry with
the key of counter we're going to update
it with a new value so the increment
value put it back in the dictionary
commit the transaction and then go to
sleep for a second so the service is
just constantly running in the
background
you know just setting a counter new
value for the counter just incrementing
it one time after another and eventually
someone's going to want to come along
and get the value of that counter so how
does someone call get count I think this
is interesting so from the web
application if I wanted to call this
method on the stateful service what I
would do is get ahold of a fabric client
first of all but first I need to
describe what I want to get to so I'm
inside of an applications of trying to
reach another service that's in the
cluster I don't go out and say hey give
me the thing that's listening on this IP
address in the cluster or go to this
node with a specific name now there's a
URI scheme with service fabric where
there's a layer of indirection so I'm
basically saying I'm going to go into
the fabric and in the movie store
application I want to find the service
with the name of counter and then
service fabric will figure out how to
connect me to that thing so I don't have
to know too many details since this is a
stateful service it can be partitioned
so when I connect to the service I am
also have to indicate which partition do
i1 so there's a little bit of code here
that's saying go out and give me all the
partitions giving a list of all the
partitions that are available for that
particular service I can do that through
the fabric clients query manager
property then I can ask the service
proxy to give me an instance of
something that implements I counter
service it should have this name should
have this URI and using this partition
key and at that point I basically have
an object where I can invoke a method it
will call the method in the other
service and I get the count value but
the important part of this all really is
that service fabric is the one that if I
go out there and let's say I look at a
node and let's see if we can find
actually let's do this let's open up
counter and I can see it's been
partition three times
our staple service will always have a
primary into secondaries so the data
will be replicated and I concede that
the primary for this partition is
currently on primary - if I go out to
this node primary - and there's actually
a programmatic way to do this - if you
just want to kill off nodes and see how
service fabric reacts I could click on
the little context menu there and say
let's restart this thing and we'll see
how this behaves today but I should
eventually see that that instance will
be moved off of primary - little instead
one of these secondaries will become the
new primary and then there'll be a new
primary place somewhere else I just
might have to up the refresh rate here
but since I'm running out of time you
might just have to trust in vacancy
service fabric already kind of detected
Oh something's wrong there we'll
question mark and eventually is going to
have that counter service running
somewhere else so I can completely shut
down a node will never never lose any of
that state will never lose the counter
value or anything else that we're stored
in there so service fabric is very
appealing from the viewpoint of writing
something that's reliable and highly
scalable because you can have a cluster
with hundreds of nodes running with
these services and the last few things
I'm going to point out are that it's to
be highly available you need to
thoroughly automate everything so
automate never you need to teach
developers to never use the azure portal
to do anything
always use a resource manager templates
or scripts or you know provision things
and update resources and Azure in a way
that's repeatable you need to do some
load testing because part of being
highly available is making sure that you
can handle load and the things don't
fall apart and you always discover
things that you would never expect to
fall apart sometimes fall apart under
load testing make sure you have
monitoring in place and I think that
pretty much concludes this presentation
if you want a more slower perhaps more
guided tour of everything that I just
went through I do have two Pluralsight
courses coming out in the next four to
six weeks let's cover all these topics
instead of spending ten minutes for
service fabric you can spend an hour
with service fabric and it'll be a
little more washable perhaps but I thank
you so much for coming out today I'll
hang around for questions enjoy your
conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>