<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Real World HoloLens Mixed Reality Development with Unity - Be Part of the Future - Rene Schulte | Coder Coacher - Coaching Coders</title><meta content="Real World HoloLens Mixed Reality Development with Unity - Be Part of the Future - Rene Schulte - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Real World HoloLens Mixed Reality Development with Unity - Be Part of the Future - Rene Schulte</b></h2><h5 class="post__date">2016-08-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/L_DR9L12jNI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey welcome my name is van is shorter
and director immersive experiences that
identity mine and I'm also Microsoft the
MVP for Windows development and I have a
background in computer graphics and
watch reality and augmented reality and
those kind of things since many years
actually I have a few open source
project one of them is an AOL to kid
port and what is it like a few weeks ago
when I had the five year anniversary
when I was putting this a or two kid
poured over to Windows Phone manga does
anyone remember Windows Phone mango yeah
you're my man that's there was a good
time right so they finally added like
camera access API so I could do some
computer vision with that stuff but
that's history now we're talking about
whole lens and today I want to speak
about whole lens development we have
been doing since actually last year so
first I want to set some terminology
straight what is VR what is a or what is
M R because it's often mixed up and then
I will tell you a bit about the hololens
device have the current development kit
here with me and then i will tell you
how you can develop for it we will do a
nice demo using unity air from scratch
and build a nice little game or app if
you will yeah and then at the end I want
to spend some time to
okay
yeah and at the end I want to spend
quite some cool awesome yeah I give a
good hand for the technician so yeah
let's get started at the end I want to
talk a bit about the experience and but
the things we learned while building
Holland's absence if a couple of months
or since last year actually if he works
about the company I work for identity -
headquartered in Seattle in the United
States but I will she I actually work
out from Reston in Germany identity mine
has always been at the bleeding edge of
technology so I've been working with
those big multi-touch tables there's
PixelSense tables we're doing xbox one
Xbox 360 apps Kinect for Windows we have
actually Kinect for Windows solutions
deployed and the real role not just for
you know tryout but actually real use
cases and we were also caught by an
earlier program from microsoft hololens
so we're developing fall and since last
year here's a quick video I want to show
you about the company it's just one
minute it shows some of our whole and
stuff we're building so I think it's
worth it
okay enough of the marketing let's let's
talk about some content so what is much
reality what reality is a fully
immersive multimedia solution which
means you're fully inside a
computer-generated world and you don't
see the outside the real world anymore
and what reality is around since many
years actually since think about flight
simulators many decades actually but now
we have consumer devices that are
available for the masses and we have on
the low end like Google call pod and in
high end spectrum we have devices like
the oculus rift and HTC wife which are
not too expensive compared to what we
have like five or ten years ago and they
provide a very good quality and with
quality I especially mean latency
because latency is a big challenge with
those VR headsets because what you want
to do is you want to Randall the scene
very realistically and on the other hand
you also need to provide it as a fast
feedback so when the user rotates the
head you need to quickly render a new
updated frame and sent it back to the
device to show it to the user so this is
the latency and width we all were
talking about three milliseconds so
actually anything that's taking longer
than three milliseconds can make a user
sick they can throw up they can get
headaches and all the things you
probably want to avoid for users also
important is we humans don't just have
eyes right we also have ears we have
hands and so on so spatial sound is
something that those devices already
implement and they're also like those
data gloves which have those little
motor holes inside where you can give
haptic feedback and there are even
experiments with virtual smelling which
could be fun but could be also very
awkward I'm I'm thinking about the VR
for that there right this this will
happen I'm sure so what is what is a or
what is augmented reality and what is
mixed reality on the other hand it's not
fully immersive this is this is clear so
you'd still see the real world you don't
just see the virtual world you see the
real world which is augmented with
virtual objects so this is the main
difference compared to VR here and the
microsoft hololens i have here is called
a mixed reality device although it's
actually also an augmented reality
device it seems like everyone has its
own deaf
finishing what mixed reality means right
so but I just want to tell you why
Microsoft calls it mixed reality in
order to diversify from at the existing
existing augmented reality solution we
have these days so you probably all have
a smartphone with some kind of a our app
on it with an augmented reality app and
what they do usually they take the
camera stream the analyze the camera
stream run some computer vision on all
of that and then augment it with virtual
objects and you see it the real world
not with your own eyes but through
another screen you see that also in my
new scope because the camera is Monas
copic it's a cyclops basically and just
went one eye so you don't see a
stereoscopic and you see it through
another screen but if the hololens you
see it with your own eyes it's mixed
basically because you have those nice
sifu lenses here where you can see the
real world with your own eyes and then
the virtual objects are faded in there
so in Microsoft wants to diversify kind
of so they call it mixed reality
challenging it's even more challenging
to have low latency because you also
need to analyze the real world you don't
just generate virtual objects but you
also need to run a computer vision
basically so even more challenging to
keep a good frame right there and of
course you want to seamlessly merge the
real world with your virtual world and
this is yeah quite challenging if you
want to have realistic rendering cool so
let's talk about the hololens and i have
to get the current development edition
here with me the whole lens is a mixed
reality head-mounted device from
Microsoft and it has a bunch of sensors
integrated so it has an IMU unit an
inertial measurement unit which is
basically used for the head rotation
right and then it has a bunch of cameras
like those here those are environmental
cameras which basically scan the room
they are used to provide a special
mapping of the room so this is nice we
will use that in a demo later where we
use the special mapping of the room so
tube and some physical objects of it has
an RGB camera and has a depth camera
what else
microphone array so it has a bunch of
microphones here which will use for
speech recognition it does a very good
job for speech recognition yeah and
everything is self-contained so the
device contains everything inside here
it's actually computer it's not just
their display it's a computer there's a
CPU in here a GPU and a new Co process
or Microsoft called stage bu the
holographic processing unit which is
basically responsible for the
the spatial mapping speech recognition
gesture recognition and so on yeah
and another cool thing is it actually
multiple people can wear a whole lens
and they can see the same Holograms so
if you would all have a hololens you all
get one from me I'm just kidding so if
all were whole ends we could basically
see each other right this is also a nice
diversification factor compared to V or
because you're just we are just on your
own world with Ariel still see the real
world so multiple people can rather
hololens they can see each other they
can still see the room and they could
also see the same holograms so this kind
of multi lens collaboration futures are
very very nice unique a point of the
whole lens and it's Windows 10 device so
you can even pin your Microsoft edge
browser taps on your walls and stuff
like that cool so what is the input and
output paradigm of the whole lens the
whole lens uses the so called ggb input
paradigm which stands for gaze just
shown voice so when I wear a device like
this and I rotate my head this is where
I'm looking at right so this is where
I'm gazing I'm basically setting a gaze
vector array and this this ray is giving
me the interaction focus so this is
where I know okay this is where the user
was looking this is where he's
interested in so think about in a
desktop world as a mouse move basically
and the next thing is there are tabs so
I can have a couple of different
gestures and one of them is the app tab
just room so I held my index finger up
like this and then I tap like that so
this is in your desk to blow the mouse
click basically so you're gazing setting
the interaction focus then out tapping
to trigger they actually action like a
mouse click if you'll and since this is
quite limited in terms of input voice
and speech recognition is a very
important natural user interface input
mechanism for hololens as well and it
does a very good job it's actually
better than Xbox more in speech
recognition it's it using Cortana inside
its Windows 10 like I said for output
you have those two sifu lenses here each
of them has one megapixel resolution and
it's actually not just the simple flat
screen or flat flat lens it's actually
multiple layers so the camera avetis
dream is split up into two channels like
red green and blue and each of them is
feed into each layer basically and
spatial sound and we will talk about
spatial sound in a second later those
tiny speakers provide a very good nice
spatial sound which is an important
addition to the visual aspect so how can
you develop for lens first of all you
can just use dyke 3d directly tag for
the 11 with c++ or even shot the X which
is a wraparound tag 3d and C sharp or
you can use a middleware like unity and
unity is not just a 2d or 3d game engine
where it comes from it's actually also
used to build applications and you have
a ton of amazing components already
available like a very good global
illumination system where you can have
very realistic shading and also physics
and so on you have this nice high
efficient workflow you can be very
productive with unity and develop
quickly using unity and of course it's
cross-platform
they have right now 28 platforms they
support as target output which is pretty
impressive
they have build and we also support
since a few versions so the oculus sdk
is integrated already so you don't need
to install separate plugin to unity you
can just use it out right out of box and
unity is a first-class citizen poland's
development that's that's for sure
because microsoft puts out all the
tutorials in the holographic academy
it's all built using unity basically and
actually a few of the applications you
find in the Windows Store for hololens
are also build by Microsoft and they are
also using unity R for a few of them so
you can build some real cool stuff and
yeah it's still free for personal use so
I was at the United Front last week in
Amsterdam and they announced a different
pricing model for the professional and
plus version but they still keeping the
personal version for free so you can use
the free version for non-commercial use
case and you can download the SDK for
the hololens the emulator and unity and
all of that for free and get started
it's pretty nice cool enough talking
let's let's build something right what
we will do I will start with a fresh new
unity scene and I will configure it so
it can be deployed to the hololens and
we will implement the things we just
talked about like gazing just shows
spatial mapping
and speech input and we will do this
with a nice little setup of physics
subject so we will have a nice plane we
stuck some cubes on top of that and then
we can shoot a sphere from the camera
which is the useless head with the whole
lens basically and we can shoot the
spheres into the scene and table bounce
of physically correct basically let's
switch to unity so this is unity you
probably have seen it I don't want to go
too much into detail about a unity here
Brian had a nice session yesterday about
it so but this is basically your scene
view you have the hierarchy of the scene
here and this is the game view and this
is the assets assets few where see all
your files so first thing we need to
configure for hololens is the main
camera because if we have a fresh new
unity scene the camera has a set
position of 0 1 and minus 10 which can
cause an offset if we use it for the
hololens it's like I said if the user
walks around with the device the
position of the device is mapped to the
main camera and unity right so we don't
want to have an offset here so we set it
to 0 not 10000 and also the head
rotation is mapped to the camera
rotation okay so we set this the next
thing we need to set as a solid color
black as a clearing flag because every
frame we want to clear the frame and the
hololens uses additive blending which
means the virtual objects are blended
into the screens using additive blending
which means black is transparent you
cannot see black with the device so we
clear it to black and we set the near
clipping plane to 50 centimeters because
we want to avoid that the users eyes are
crossing basically so if the virtual
objects are rendell too close to the
camera the users eyes will start to
cross which is also uncomfortable so we
want to clip off the rendering at 50
centimeters or 80 centimeters and we
also don't need 1000 meters we need 10
minutes and you notice it's all in
metric space right this is also nice
with the whole lens all the spatial
mapping is already in metric space and
in the real world scale cool
so we have our scene set up here and now
let's also enable the player settings
for hololens you can find them here and
we need to switch to this tab which is
Windows Store applications or universal
Windows platform and we need to enable
justice checkbox so the Windows
holographic SDK is included in our
target output another thing we need to
set is the publishing settings and if
you've done you WP apps before you
probably are familiar with those
capabilities and we need to enable some
check boxes here Internet client because
we want to use the emulator also for a
bit which is connecting with the hosting
operating system also microphone since
we want to use speech recognition and
spatial perception since we want to use
spatial mapping okay so we have
everything set up let's actually add
some objects and what I usually do I
create an empty game object as container
why it can place objects that are
belonging in the same physical location
together and group them and I will tell
you at the end why I do this and why it
makes sense so reset position and set it
a bit full away from the camera so we
set it like 50 centimeters lower and 2
meters in front of the camera which is
usually a nice way when you saw the
application that the Holograms are
studying 2 meters in front of the users
view them cool so let's at the plane I
mentioned this is all plane a bit too
large so it's reduce the size here and
like this and yeah we want to have a
different color so we created we create
a material and since we're good good
citizens we create a folder for that so
let's add a folder for materials and
let's let's go with something's not
right try again in a little bit ok wait
a bit Colten ah I'm good
so let's lose a green field green plane
drag and drop that's so nice of unity
it's so easy to build that stuff yeah
that was funny a cube so we will add a
cube on to our plane like I mentioned
also reduce the size of this one with it
like
and pull it up a bit yeah like this this
is good and edit rigidbody component -
physics rigidbody component from unity
basically it tells the unity runtime
that it should run rigid body physics
calculations here and so we just
duplicate this one place the other one
here and all the one on top like this
and then we can hit the play button and
see if all of physics stuff works they
should fall down on the plane once it's
compiled yep that works okay nice so we
have our basic scene set up let's add
some actually a whole lens functionality
so we will add another folder for that
for our scripts basically because we
will write some C shop scripts for
accessing the hololens api so let's call
it cannon behavior because like i said
we want to shoot spheres from the camera
into the scene so like a cannon like a
cannonball and we can attach that script
to our camera i just drag and drop it to
the camera here and you see it's
attached here and then i double click so
it opens in Visual Studio 2015 I really
like that with unity 5 they finally
added a full Visual Studio support so
you can use Visual Studio for editing
your scripts and you can actually also
debug your scripts which is very nice
you just set a breakpoint in Visual
Studio attached to the unity process and
you can debug your scripts very
convenient if your windows developer so
that's nice and you can use c-sharp
scripting like I said and also
JavaScript actually so that's that's
nice we were using as you saw here
that's it's my preferred language for
this kind of stuff ok cool so this is
the basic unity scene at sorry the basic
unity script it is generated for us we
have to stop method for initialization
which is called once the script is
installed it the first time and the
update method which is called every
frame basically so let's remove that
stuff because I have some code snippets
already prepared which we will plug-in
and then we talk about those so let's
add the gesture recognizer for the art
tapping and I have this class here which
I instantiate and gesture recognizer and
like the name implies this is in whole
and specific API which was responsible
for
recognising gestures it has a few events
like this tapped event is of course
fired when the usual does this tap
gesture and then we can also define what
kind of justice we're interested in in
this case we just want to listen to our
tapping - single tap so that's fine for
our use case here but you could also set
a few more gestures of course and then
we call sergeant capturing gestures
which is nice because we can also start
and stop this kind of captures gesturing
yeah yeah just recognition like we want
okay let's add the tapped event here
this is the event handler which is
trigger once the user taps we call the
shoot method and what the shoot method
does is it creates a sphere and code so
just like we created those cubes in
unity editor we can also create those
dynamic clean code and since we attach
that script to the camera I thought
those spheres are like you're shooting
out your eyeballs right that's why I'm
calling the variable eyeball so let's
set the scaling a bit lower so it's not
a huge ball but it's smaller and we
attach the rigidbody component to it as
well to have a rigid body physics
calculations we give it a mass which is
a bit lower and we said to position the
initial position of the rigidbody to the
transform dot position and this
transform in the script is the camera
transformation since we attached script
to the camera so in transform position
is basically the camera position right
and then we give it an impulse into the
direction the users looking so transform
that forward is the vector all the users
looking and we multiply it with a
constant factor which is defined up here
as 300 Newton and if you have never done
unity you might wonder why I define
public fields here you will see in a
second once we switch back to unity so
let's save it here and we can switch
back to unity give it a few seconds to
update and there we go and you see that
field is now here so this is nicely done
without any attributes or something just
so face here on the unity inspector and
they do this public fields because they
also serialize it and easier lies cool
so this is our basic setup let's let's
build it for the whole
so build settings and we select Windows
Store
since it's a Windows 10 device we should
like you WP 10 we want to run load tag
3d because we want to have 3d holograms
and then we hit the build button select
a follower and let it create the build
output and what it does now it basically
generates a new visual studio solution
for us which is the hololens
applications because the whole lens
application in the end is au WPF is
that's what it is with a dyke Freedia
rendering so it generates it for us and
I did this before so it should be a bit
faster now and I already opened the
other solution so once that is done you
will see this reload all here so we can
hit reload all and yeah reload it and
then we launch it in the emulator so you
can select release I usually just go
with release mode x86 because the other
lens is 32-bit processor so we use x86
and then you can select the target
output so you can go with device if you
have the device connected via USB or
also removed machine if you have the
device if you have an IP address you can
deploy over there basically that's very
nice and we will just go with your
hololens emulator for this case so just
hit the button here and already open the
hololens emulator before so it's a bit
fast on and you have the nice Windows
Start menu here and you can simulate
gazing basically with the mouse just
left mouse button down and mouse move is
gazing and right mouse button down is
simulating in our tab so that's that's
very convenient
so our app launches now and there we go
we have our cubes on the plane and then
I can hit the right mouse button and
shoot at those guys right but you see an
issue here it's very hard to aim it's
very hard to see where I'm actually
shooting so I don't have an indication
of my gazing so I probably want to have
some indication of gazing so let's fix
that lets edit gaze curse on okay stop
it here switch back to the editor
yep yeah all of that let's edit a screw
so plug it in what I define here is
another reference we will set up from
our unity scene so we can create another
game object and just plug it into our
script here and you will see that in a
second so we use another game object for
showing the gazing for showing like a
cross or something you can see while
basically looking at okay and I will do
the gaze update in in the update method
so this update method like a set is
called every frame by unity and what I'm
doing here I will do a ray cast all
which is a built-in method by unity so I
can shoot basically a bunch of rays into
the scene from the camera position in
the direction the users looking at and
where those rays hit and virtual object
I get a ray cast hit back so and I
swapped those by distance because I'm
interested in the one object that is the
closest to where I'm looking at so and
then I get this first hit basically and
I used that ray cast hit the position
where this was hit to set the position
of our gaze closer to that place and
also I want to orient the gaze go so
nicely to the surface of the object so I
used the raycast hit normal as a forward
vector for our gaze Curson okay let's
save it switch back to unity R and add
our actually gaze cross the game object
so what we will do here we will add
another cube just just a simple cube we
call it flat cube because we will make
it a bit smaller let's go with yeah like
this
and this a very small flat cube and and
you see if we would just use the white
color and on the white cubes we wouldn't
see while we're gazing so let's add a
different material to this one and I
also went to remove the box Collider
because I don't want to have like
physics interaction with the gazing it's
just useful visual indication add a new
material create material reds and there
you go and just apply it so we have our
gaze cursor here another thing we need
to do of course we need to add the
reference here so I just can drag and
drop the game object from here to there
so our script has a reference to this
game object basically cool so but before
we deploy it I want to add some more
features I also want to add spatial
mapping so right now we have our tapping
ref gazing now I went to have spatial
mapping and spatial mapping is quite
complex to implement or multiple lines
you would write and this is a session on
its own actually but I made it a bit
easier and there's a nice project by
Microsoft called holo toolkit it's up on
github open source project and they have
a bunch of very nice scripts and if you
like for spatial mapping as well so you
can just use them because all someone
already implemented it for you very nice
so I created a custom package which I
can just import here here you go so this
is a unity package where I just
extracted the stuff from the holo
toolkit which we are interested in like
the spatial mapping kalila and we will
talk about this in a second so let's
import those
I have a prefab here I just plug it in
to my scene and this has the two scripts
were interested in already attached and
there are two scripts from the holo
toolkit which I'm using here when it's
the spatial mapping collider which
basically generates a spatial mapping
collision mesh so which we can use for
physics interaction right so we can use
the real-world spatial map for physics
interactions and this has a bunch of
properties so I can say what kind of
bounding volume I'm interested in and I
set it to a sphere of five meters so
everything that's five meters around me
that's what will be part of this spatial
mapping mesh and then you can also
define level of detail low medium and
high which basically means higher
position but also more takes more
processing power of course and then the
default for time between updates is
two-and-a-half seconds which basically
means this is the time the especial
mapping mesh will get updated so if lots
of people are walking nearby fast they
won't be part of it but if I place the
table in here and move it out it this
will be app all of the updated mesh yeah
and there's also the spatial mapping
renderer which is similar to the spatial
mapping Collider but this is used for
visualizing the spatial mapping mesh and
I can use material here so I have a
wireframe material attached to this one
which is just like a wireframe looks
like a net and you will see it in a
second how this actually looks like and
I could also say occlusion so if I said
occlusion you won't see the actually
mesh you won't see the spatial mapping
mesh but your virtual objects will be
occluded they will be hidden by a
real-world object right so this is
probably something you want to do okay
cool so we have gazing gestures spatial
mapping let's add less thing for the
session let's add speech recognition and
we'll do this with a nice little script
so I have a c-sharp script another one
and I just call that one
speech handler there you go let's remove
that stuff and plug in some code
I prepared yeah
it doesn't yeah resolve Oh doesn't show
me what - what namespaces I should add
so let's let's switch here and copy
those in there you go yeah it's Windows
speech I always forget this one it's
Windows speech so the namespace is here
okay cool this is the keyword
recognizable this is similar to the
gesture recognizer but of course it's
used to recognize keywords right and the
nice thing is you just give it an a
string array basically so you can see I
define some string variables up here
these are just normal SC shop strings
right just a normally shop string like
height plain should reset seen and I
passed out it passed those into the
constructor of the keyword recognizable
and if you have none speech recognition
before with other platforms you'll
probably have to define some XML and the
grandma and whatnot and this is really
what I like they made it very easy you
just you see shop strings and they go
and once it recognizes one of those just
shows a tree goes dead on phrase
recognized event and we have an event
handler here and this gets in passed the
arguments and the text which is
basically recognized string so I just do
a very simple string compare it's it's
so easy it's yeah really simple so I
compare it with my height playing
command
something's not right again in a little
bit it's all good katana calm down so
high plane commands I can set the plane
deactivated for shoot I'm shooting the
cannon and for the reset seen command I
am resetting the scene I'm just
reloading it basically cool saving it
always important to save the scripts and
then switch back to unity we have our
script here I just attached it to the
cameras well drag and drop there you go
and then we plug in the references like
the plane
and the cannon the cannon is attached
also to the camera so I just attached
this one here and yeah let's let's build
it again for the emulator but we will
also see it on the hololens Lela okay
let's let's build it for a second and
you notice it's taken quite some time to
build like the deployment cycle is quite
cool it's taking quite a bit to test
some stuff and at the end I will share
some best practices how you can avoid
that and how it can be more productive
and faster inside unity without having
to weigh it always for the for the build
time and did a deployment cycle cool
launch the emulator again with our
updated stuff and every level is really
nice it's included in the SDK and you
can actually also load different spatial
maps so you have this room tab here
where I can basically load a spatial
mapping I created before so I can take
the device and save them special mapping
of a room and then loaded into emulator
pretty cool also for testing okay cool
so you see the gazing here I have this
little flat cube this is now shown at
the position I'm looking at and also
nicely orient to the surface of the
object I can also use speech commands
hide plane you see and you see another
thing they bounce off in the real world
and if you look closely you might be
able to see it so far here so this is
this a default room that comes with the
whole lens emulator okay cool so this
damn you Leila let's do something very
brave let's switch to device ooh and you
should be able to see what I see now
let's put it on and you will get the
code from the demo all in github so I
put it all the sample code up and get up
good let's let's disable also audio and
yeah there we go okay it's opened hold
in stop menu I have the finished app
already pin here so I open it place it
somewhere like this and then it loads
and like I said you can get all the
source code from the little demo at the
end of the session it actually also has
audio collisions so we're special sound
as well there you go I have a different
gaits crosswalk you see I made two
little cubes and I have someone else
here this is me as free scan hologram
yeah I know it's weird yeah and and
since we are in the Scandinavian region
I fall instead of shooting his fears
would do something cooler so I thought
we shoot some meatballs like skip Ola is
it is it correctly pronounced I hope so
so let's shoot some soup Ola
this guy I hate him let's shoot him away
hide plane see that and you can all get
your Bolin for London's there you
go and you see them they bounce off here
in the real world right this is pretty
pretty amazing with the whole lens the
whole lens can do this special mapping
right and so we can yeah we can shoot
here and yeah I can do this the whole
day basically and it's almost lunchtime
right we didn't have his script balling
yet yeah okay cool
glad that worked out cuz sometimes this
does work right with the Wi-Fi but I
actually have my own Wi-Fi access point
so it's good cool
let the demo worked like I said you can
get the source code of this maybe not
with the script polling texture but with
the rest you can get all of that and
github later on and you can play around
with that cool I also had a video
prepared just in case it doesn't work so
this is the video I recall I recall it
at home so and you see I call this one I
actually use eyeball textures and yeah
the nice thing is you see how it adapts
the spatial mapping adapts to the
environment so those it feels plain
those fields you see how it bounces off
and rolls down the stairs so this is
recognized by whole lens yeah good fun
okay cool so shooting eyeballs and soot
pollen is really really fun by the very
small nice hey so let's talk about some
real-world applications and we have we
have built a couple of apps right now so
we did an engagement with your museum so
we tried out some different stuff what
you can see in a museum so we put some
dinosaur models into the office which is
very amazing actually
because you can see those dinosaurs at
the rear world scale and you actually
can figure out in metric space right so
you can figure out how big they actually
are really really nice experience and we
also did some stuff for automotive
industry for construction as well and
plane maintenance in a couple of
actually real business use cases I want
to spend a bit of time to talking about
an app I was working on last couple of
months it's called hollow flight and I
want to share some best practices some
learnings and stuff with you from the
applications as we build it so hollow
flight is a real-time flight data
visualization basically and we take the
real flight data and visualize it in 3d
and if you think about air traffic
controllers what they use these days is
they are looking on flat 2d screens
right but planes and flights are
actually flying in 3d they have an
altitude so we figured let's put them in
the hololens and visualize them as
Holograms so that gives you another you
know relationship between the the
flights because you see there nicely
stereoscopic so you get the relations
between the different flights and we can
also visualize a usable invisible
information like flight trails and so on
here's a quick video showing you the app
yeah let's turn down the volume a bit so
then okay that's a bit slow let's
restart the video
okay that's better I think cool so you
can see the Hawaiian Islands here and we
have two flight space of the wine
islands basically visualized and you can
gauge that those planes that are flying
like I said this is real-time flight
data and you can see flight information
like callsign altitude and whatnot you
can also hear our traffic controls
conversations and spatial sound and we
can also visualize the airport web
information' wind speed wind direction
all of that stuff and we use spatial
mapping of the hololens so the user can
beso upin those information panels on
different places like on different walls
on the table and whatnot and basically
layout the workspace we also have
different level of details from terrain
the terrain is by the way built using
Bing Bing Maps API data so this is real
topographic data yeah like I said we
have real-time flight data but we also
have our own as well back-end where we
can cache the data so we can play it
back a different speed basically what
you can see here we played back faster
and then you can see those flight trace
visualized so in flight information you
usually don't see so that's that's nice
and adds another value of course okay so
what were the challenges first of all
usually flight information is visualized
in 2d but if you visualize it in 3d you
also open another dimension of arrows
basically which you don't see in 2d yeah
you also need to be careful with the
holographic frame size because you
cannot put too much information in front
of the use law because flight
information is very dense you have a lot
of information you want to show a lot of
information but on the other hand you
need to be careful that you're not
putting too much stuff there a special
mapping of the hololens as you have seen
it's really amazing it can scan
basically the room so you want to use
that in some way in your app gazing and
selecting at small objects like those
tiny planes can be very hard to gaze at
them and select them because they are
pretty small so we have to fix it as
well and yeah you want to make it an
awesome immersive experience and spatial
sound is also one important factor there
so how did we solve those first of all
finding the right flight data and we
partnered with a company that provides
us the flight data in a nice rest based
Web API so we get the data as JSON easy
everyone can pause that
but then we noticed a bunch of arrows in
the data actually which you don't see in
2d like I mentioned for example we had
some like crazy altitude drops like and
this happens just a few times but not
that often like we have seen in the in
the data so there were definitely some
data glitches right because if it
happened so there's we have CNN no one
would fly anymore right so we had to fix
that and you can basically go it with
two approaches you can do it offline or
you can do it online since we wanted to
do real-time we did it online so we
invented some algorithms there to fix
the data to smooth out the data to you
know make more sense out of the data
basically and avoid those issues then of
course you need to visualize it in 3d so
you have geographical coordinates and
you need to map that to an unwrapped
planar rendering and we also want to map
the altitude and if we would map to
altitude linearly like the thirty to
forty five thousand feet of flight space
we would waste a lot of rendering space
with uninteresting flight information
because the most interesting flight
information is in the first like three
to five thousand feet close to ground
right so we do we use it on nonlinear
mapping basically of the altitude so we
give them more interesting flight space
also more rendering space then we get a
bunch of positions for each of the plane
and the flight space and they can even
have different time stamps or a lot of
different data basically so we need to
normalize them so they make sense when
we visualize them all together in one
flight space rendering and the right
size is important of course because we
are visualizing those planes and if you
gaze if you're stepping with fall away
they can become very small so you just
see a few pixels actually so what we do
that we use level of detail to swept out
that plane model with just the cube and
you don't notice it is because you just
see a few pixels anyway and we also
makes sure if the user steps even
thought away the cube will stay at the
same size you can still see a few pixels
there in the back and you know there's
something going on there yeah you I we
had a couple of iterations for the eye
to make sure that we were like I said
not polluting the holographic frame not
showing too much information there and
this was the first iteration that's my
fault it's developer yeah so super ugly
and next one was in place blue board so
we had those little mood boards which
was shown directly at the plane position
so he could pin and enable them multiple
billboards basically and you see the
issue in the screenshot after is over
drawing after overlay issues also not
very nice then we figure out let's using
central let's use a curved screen UI
which is nice because human had is also
bit curved so this you have all the
scroll TVs right so we figured let's use
a curved piece but this thing then kept
on growing and growing we wanted to show
more information like flight information
and weather information and whatnot so
it was also not fitting holographic
frame anymore so then we split it up and
we polished it to what you have seen in
the video and we have those independent
panels basically and the user can pin
those separately in the room basically
and layout this workspace and we're
using the spatial mapping of the
hololens to allow the user to place
those cool yeah size mellows also for
ray casting like I said those planes are
very small and if you want to gaze if
you want to select them the gates reg
will often hit nothing basically so we
figured let's do something else and we
use basically just a simple sphere
Collider so we use this field collision
volume which is a bit larger and you can
also think about your dynamically adapt
it so if the usual stepping fall away
you can grow it a bit to a certain
amount and if the user is getting closer
you can make it smaller and then
actually switch to the real mesh
Collider of the plane the mesh add a
circle arrows also nice performance gain
because testing array with the sphere is
really cheap it's really easy and
doesn't cost a lot of performance yes
special sign does some some crazy crazy
experiments out there you can see on the
slide really my I'm sure there's a super
amazing special sound like with all this
but it might also be a bit heavy on the
head so I rather prefer this one and the
hololens has those tiny speakers up here
right you have those small speakers here
and of course they don't provide a very
very best sound not not very low
frequency but what it does with those
tiny speakers is super impressive it's
really good
and yeah it's perfect brain trickery
actually because you know where the
sound is coming from and in hold of
flight we actually use it for this so we
use that to playback air traffic control
conversation and those are not just
adding value from the atc conversation
itself but also from the specialness
from the spatial sound because we're
playing back that sound in 3d at the
plane position so even if you're not
seeing the whole around like rotate in
your head you don't see the hologram
which is somewhere here but you hear the
spatial sound and your brain knows where
to turn you know where the sound is
coming from right and this is what they
have really done very nicely with the
whole lens and they have a very good
algorithm there to compute their spatial
sound really good and if you have use
case in your applications make sure to
use spatial sound it's really adding the
topping on the cake if you will cool
some sort of best practices so use
fading and transitions is an important
one because in the real world objects
don't just appear or disappear
maybe ghosts if you believe in such
things but I don't so
and what so objects you can just enable
disable them right you can just make
them appear or disappear but of course
you want your virtual objects behave
like real-world objects so you need to
fade them in move them in grow in size
shrink in size and so on and a free
short clips here where I want to
visualize you the differences of all
those free approaches what you can do
basically and so we have all our wine
Islands here and then we want to switch
to just one island right so we see all
Islands just switching to one island
without any transition so this is not
very nice very awkward then you could do
a cross fading like an alpha blending
between the terrains so you can do this
which is a bit nicer it's a bit smoother
but on the other hand you're losing the
context so as usual you don't know we're
looking at you don't know which island
you're actually zooming into right you
see all Islands in just see one island
but which one so what we did in the app
is basically scaling up the mesh of the
all islands sarios going up the texture
of the whole island mesh and then do
cross fighting to the just the one
island
so this is a bit nicer and it's a nicer
transition for the user to actually know
where to look at them cool so let's talk
about my my top ten whole lens
development recommendations and first of
all you can be really good developer you
can be great 3d develop or best one in
the world well if you don't have any
free content to show well you don't have
anything to render right and identity
mine we're lucky to have a bunch of very
talented designers and also 3d artists
who can make very nice and good 3d
models and also with a reasonable
triangle count and it's very reasonable
because hololens is a mobile device
right hololens is a mobile device and
compared to watch reality headsets like
oculus rift and HTC vive with those they
are just this place basically with a
bunch of sensors but the computing is
all done on a computer in a full-blown
desktop pc right for the oculus rift you
need a really high-end PC which of
course can compute like amazing scenes
can compute really nice renderings but
you're always connected with the cable
you're basically always on the leash
right so with the hololens i much prefer
the hololens approach because it's
self-contained everything this is
computer right it has everything inside
here and they don't need an extra cable
I can just put it in and walk freely
around and can interact with other
people as well so this is a much nicer
off but on the other hand you have
limited computing power of course
because you cannot put in your
full-blown test of graphics called in
that device will probably get very hot
if you would do that and this is
something you want to avoid as well so
limited computing power and what we know
it is this that the hololens is mostly
full rate bound basically so you can run
the tens of thousands of triangles like
50,000 67,000 it's not an issue to
ramble those but it those all rendered
closely to the eye if these were ended
close to you to camera and take up a lot
of pixels and you have a heavy pixel
shader running for each pixel then your
performance will drop you get really
quickly running into issues there so you
want to draw your pixels very easy and
for example don't use the unity standard
shader
because this is too heavy it's
doing too much and what we noticed this
you can actually get away mostly with
vortex lighting so you can just use a
simple lighting model for vortex based
lighting and have a super cheap pixel
shader and another thing you want to
avoid is like over drawing this this is
just with every mobile device basically
you don't want to have like mashes
models multiple of the one one often
another which would cause like multiple
pixels drawn multiple times right you
want to avoid those oval drawing issues
also large transparent objects or issues
so you need to be careful there because
you want to Randall with sixty frames
per second this is really important to
render your Holograms of sixty frames
per seconds otherwise they if they're
dropping to fully frames they can become
unstable so you have seen they're pretty
stable in the room right even if I told
my hat they will be at the same position
but if the framerate drops steak can
becoming you know unstable and drift and
this can make you sick actually
Microsoft did some user research there
and some people really get headaches and
they can throw up so you really want to
go with sixty frames and optimize
everything still the whole lens has a
multi-core CPU and in halle flight we
use that for the data fetching the data
clearing and you know all the processing
is done in a background thread basically
so we can keep the UI thread
UI thread free from that work and UI
thread can keep up with the rendering
loop yeah if you have never done a 3d
programming or it's been a while you
probably want to brush up some of your
math skills as you have seen as a bunch
of stuff going on like with vector
algebra and so on you don't have to
implement all yourself unity or whatever
game engine you're using is helping you
a lot they have everything built in but
of course you need to be familiar what
that means you know where's the
transformation what is a matrix
calculation right so this is important
anchor your Holograms so the whole lens
has an API which is called world anchor
and if you remember the demo I grouped
this those cubes and the plane in one
container right in the scene hierarchy
and what I could do I could apply a
world anchor to that container and then
the hololens runtime would basically
give that world anchor its own
coordinate system right it has its own
coordinate system which then results
that the hololens and makes those world
anchors very stable so even if I leave
the room and come back into the room the
hololens the holograms will still be at
the same position this is done with the
world anchor basically and the coolest
part about it is you can actually
persist those so you can save them in a
global storage and the whole lens you
can save the world anchor with an idea
and once you reload your app or restart
the device and then load the world
anchor position there will be at the
same location and when I'm flying back
home hopefully tomorrow I don't know I
heard some really interesting things
about a strike something well and if I'm
flying home hopefully here I will see
some holograms in my office at the same
position when I left them so this is
done with the persistence right so you
can persist those world anchors pretty
cool
yeah leverage level of detail like
you've seen you can save some rendering
time there as well you don't need to
show a high detailed model when it's
just like meters away their gaze crosses
importance as you have seen like the
main input paradigm of the whole lens is
gazing and gestures so we want to make
sure that your gestures sorry that your
gaze cross flow is very stable and very
nicely done because you the user will
see that most of the time basically and
one part about it is to smooth it
basically because the gazing is based on
head rotation right this is based on the
IMU unit which means this is based on
sensor data and like every sensor on the
world
this contains noise if you use the raw
data which makes the gaze cross of
jitter always slightly move and this is
something you want to avoid right you
don't want usual to like always have to
jittering
thing in front of it
so you would want to smooth it basically
and the hollow toolkit I mentioned it
has a bunch of reusable scripts and also
smoothing algorithm implemented so you
can use that one we actually developed
our own because it gave us a bit better
results with less legging it's a bit
faster to react and also the one we
implement it has some prediction so it's
just giving us better result another
thing about the gaze cursor is the hand
ready state so the Yuuzhan is our typing
right and the cameras if the whole lens
of course need to see the hand so if the
users out tapping here won't work
because it doesn't see the hand right it
needs to see the hand somewhere here or
there are also works but not there so
you want to give that feedback to the
user that it's now basically seeing the
hand and it can interact with the with
the piece the user is gazing at and what
most apps do and also the hololens Start
menu they show an open ring gaze cursor
when the hand is in view to tony user
okay you can now interact and they show
a flat circle something like this when
the hand is not in view right to give
the users of the information okay open
the cursor you can interact the auto
correspond nope
those details really mellow they make
out a good experience and you probably
want to have a good experience so pay
attention to those details yeah use
animations and transformations to let
your modular objects behave like ruble
objects and yeah so this is also a nice
one so if you have a bunch of unity
projects you probably also have a few
reusable scripts and the naive approach
would be to copy those script files
between all the different projects but
this is not nice because when you have a
butter or something I want you to change
something you have to copy all the stuff
so what I did instead I created a
central see shop solution where I have
all the reusable scripts in one central
place basically and I can just then
build out a DLL and copy that into a
special folder into your unity project
so the unity project have that assets
folder right and there's a special
folder you can create it's called
plugins where you can put in dll's
and then you can just use those scripts
as well from the DLL and this works out
quite nice because I also have a post
build step in my C shop solution so I
just hit build and all my projects are
updated with the latest stuff there's
one thing one got job the unity editor
uses motor runtime which is actually a
five or six year old version of the mono
runtime so it's quite outdated I think
this is being fixed now since Microsoft
acquired as a marine so I was at the
United Continental II announced it on
the roadmap that it's planned to update
the mono run time they're using inside
unity here which is great because you
don't have TPL of anything when the
unity editor right you don't have task
pal a library so but on the whole lens
it's running Universal Windows platform
so it's running you WP donate core
latest one so what I have to do is
basically you have to see short projects
one I'm building for the motor for the
unity editor
mouna runtime the other one I'm building
for you WP and I'm just sharing the
scripts and sometimes you have some pre
compiler directives like if if unity on
the school I did or blah blah blah do
this right those kind of things but
anyway having a central co-op solution
with all the scripts in one place really
is it is a huge benefit for maintenance
yeah
avoid the long deployment cycle as you
have seen it's taking quite some time to
deploy something to the hololens so
first of all you need to you know change
your unity scene then build the visual
studio output and then you know build it
again
and then deploy it to the emulator or
the whole lens so this is really foo if
I want to take some if I want to test
some small stuff and what I did here I
wrote a custom script where I can
simulate the gazing and just was already
inside unity so I can do the same like
the ami little does I can use my mouse
for gazing I can use the mouse click
right mouse click for app tapping I can
also use different gestures but I can do
this already inside unity and for every
little change I don't have to deploy to
the device or the emulator it's a huge
huge time-saver
right so the whole lens also has a bunch
of other gestures I didn't mention like
the app tab I said then has double
tapping
and tap and hold so you can do like
scrolling the nice thing is
three-dimensional right not just this
but also like this so there is justice I
can also simulate already inside unity
with the custom script I wait there yeah
so you can stay productive most of the
time inside unity but of course you also
want to test it on a device if you're
lucky enough to have one because nothing
comes as close to this device as the
real device itself you know it's
performance and spatial mapping and so
on but anyway the emulator is really
good it as you have seen and supports
speech recognition I can load spatial
mappings I can basically test most of
the stuff already with the emulator cool
so I think next step though if computing
is really happening right now and it's a
great time to be a developer it's really
good to be working in that space again
yeah and the whole lenses is one of its
kind of thing we have a bunch of virtual
reality devices and the whole heads off
can compare to those but it's actually
of course it's not a VR device as yours
alone it's actually a mixed reality
device so totally different and it can
also do things which are just not the
difference in you know showing the
rendering of the pieces like augmenting
the real world but actually it can do
the spatial mapping for example you can
have the collaboration features and so
much different things and I'm pretty
sure the hololens
it will change how we interact with
computers it really has a huge potential
and you can even run a 2d apps on it so
you can develop the universal Windows
platform applications and they will also
run in the whole lens but they will run
in a window basically so you can have a
2d window then you can pin it here or
there or whatever it's also nice but of
course with the stereoscopic 3d
rendering and Frida is King basically so
you went to a free content and you can
do this with going straight with Dyk
free D 11 which we also do by the way or
you can use unity right and unity is a
nice tool you can get quickly stalled it
be very productive and there's a use
case for every piece right for example I
wouldn't build Skype with unity right I
would build it straight with back 3d in
C++ that's that's what
right but for four really nice proof of
concept unity is great and not just for
that like I said a bunch of hololens
applications in the store built by
Microsoft actually build using unity
yeah you can get a few links here so the
whole lens SDK that's you can download
it for free it has the emulator inside
and also the special unity built that is
supporting hololens development holo
toolkit for unity is also available it's
on github it contains a bunch of scripts
prefabs shadows so they actually have
some nice optimized trailers you can use
like fulfill ticks lighting and so on
pretty good stuff so you really want to
grab a copy of this one once you want to
install the SDK and yeah on my blog you
can find the top ten development
recommendations longer right up with
more details and also the slides and the
demo code I will put the link for the
demo code on my blog as well and the
demo code is actually on github cool so
we just have a few seconds left and I
don't want to over run because there's
lunch I think we're all hungry shut
Poland what is just Holograms
so they're all real hopefully you have
some real ones I I need to try them
anyway so you can shoot me an e-mail or
tweet to me or I will stick around here
a bit and just ask me there if you want
to ask some questions and with that I
thank you for attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>