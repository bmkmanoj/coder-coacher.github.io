<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A developers guide to Machine Learning - Tess Ferrandez-Norlander | Coder Coacher - Coaching Coders</title><meta content="A developers guide to Machine Learning - Tess Ferrandez-Norlander - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A developers guide to Machine Learning - Tess Ferrandez-Norlander</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hjpUHZY5-18" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Hey so hi welcome to machine learning
101 or a developer's guide to machine
learning if you guys were in Barbara's
class like her Barbara session yes
before this one was she talked about
neural networks and deep learning being
indistinguishable from magic
this is kind of like the totally other
way around this is machine learning it's
basically a pattern matching glorified
pattern matching in big chunks of data
and if any of you seen me before talking
about debugging this essentially what
I've done my whole life he's looking at
big chunks of data trying to do pattern
matching and finding like errors and mmm
problems but let's talk about machine
learning so firmly enough she also
mentioned this particular problem which
is a huge problem in computer vision or
in machine learning distinguishing she
was from muffins or rather
classification of images so East
Raisa pants like so we got a chaiwalla
and a muffin who of you thinks that this
is the Chihuahua okay pretty intelligent
crap wouldn't so think for a moment and
think about how did you know that that
was the shoe or WA because that butt is
the correct answer by the way either
either you've seen enough dogs to kind
of know that this is a dog and this is
not or if you're like me you've seen
enough muffins to know that this is a
muffin and that is not and you can kind
of distinguish them pattern and same
dogs like they got I stick at errors to
get nose and a bunch of stuff like that
and that kind of distinguishes it from
the paper cup of the muffin so if you
were to write an application that
separates muffins from chihuahuas you
would probably write something like this
and this was sort of like the rule-based
engines that were wrote like when I
started AI in college like 20 years ago
and it's also kind of what we've been
dealing with before machine learning so
apart and like I said this code has a
couple of issues I can't spell Chihuahua
which is of course an issue but not the
biggest one the bigger one is picture
contains it's an insanely hard piece of
code to write unless you have machine
learning to back it up and even if you
did and you wrote this code you would
get kind of lost when we're messing with
something like this because some of your
rules yes don't imagine it so what do
you do well you kind of go back to the
way humans thing because that's sort of
what machine learning is based on and
one thing that you must do is draw from
their experience so what you do to solve
this problem instead of using those
heuristic questions instead send a lot
of pictures of chihuahuas and say these
are all she Wallace and you send a lot
of pictures of muffins to Sadie's or
muffins and you use one of Barbara's
cute neural networks and the new your
network figures out or is a pattern and
she wore was it's not present in muffins
and it kind of distinguishes between the
two now this is a problem that's already
been solved so you don't necessarily
need to solve it it's solved in many
ways one of those ways is through
Microsoft cognitive services Google has
a lot of other companies have some but
if this is the problem you're trying to
solve you can just call into an API and
be done no machine learning needed and
you can of course fix up like if you
wanted to distinguish between Chihuahuas
and I don't know other breeds of dogs
you might have to train it a little bit
but you can still do that is through API
calling and it will tell you things like
it's a dog it's a thing it's cute I
could knows that everything I'm sorry
and but either way it's yesterday API
and if you want to use it just go to
Microsoft cognitive services
and endure a lot of issues like that the
Dardis odd like things like the general
case of face recognition or something
like sentiment analysis is trying to
find out if someone is smiling or if
someone is sad or if you you can do
sentimental now to see some pictures but
also on text so if you send out a phone
and want to know what people thought
about it you can kind of examine the the
feed and see if people thought it was
talking about or not but there are some
issues that haven't been solved and they
haven't been solved in a general case
because they require very specific data
so we're going to take a look at one of
those problems and we're going to kind
of go through the process of machine
learning and understand what's involved
in it so in Sweden it's kind of a
national sport to go and see houses like
I heard some people who from Sweden and
you know this is true so at one point in
my life I went to like six showings a
weekend and then my husband had an
intervention with me and I can no longer
go and see houses anymore so I gather a
lot of data about houses because that's
ultimately what I did when I went to see
showings and instead now I use it for
science so I take that data and I'm
trying to use that data to figure out
how to predict the price of a house
given certain parameters that I gather
so what I've done here these are real
houses in my neighborhood and I have a
list of the area of the houses and
square meters and the price and thousand
crowns so 1 million crown is this
approximately hundred thousand pounds so
I gather the data and then the next
thing I do is I try to visualize it to
kind of get a feel for the data to know
if it's linear or if it's like follows a
curve or have some other weird pattern
to it so I I do that through a plot like
this race dot out the different kind of
value pairs
and this is called a scatterplot and
it's extremely useful in the era of
machine learning or data science and
then I go through and I do that for all
of these dots and now my task the thing
that I want to predict is how much is a
house that's a hundred 50 square meters
so any takers how much do you think it
was worth yeah six and a half million
give or take how did you know that yeah
see it right if you squint it's kind of
like a line and this is what machine
learning is so now the line is something
called linear regression this is one of
the algorithms and machine learning is a
straight line as East mathematics and
fitting it to the data then it's like
moving it so that it has the right slope
that's called fitting the model and the
actual line is now called a model and
this is what you used to do predictions
and Roisin learning so now we can use
this line and say okay hung around 50
yeah 6.2 6.3 somewhere around there so
good stuff now what kind of halfway
through in the machine learning process
because the next step is that machine
learning is just predictions like you're
not actually calculating real numbers or
predicting like or it's saying that this
is for a fact the absolute truth
we're just doing a guess and the
question then is how good is that guess
well what we can do now is figure out
something called the confidence interval
which tells us how good our guess is and
in a case like this we can you just use
a marker and do this so we kind of do a
marker around all the dots because not
all the dots are on on that straight
line and this will now tell us how far
off we are at maximum so we're give or
take like I don't know half a million
crimes off like 50,000 times off which
can seem kind of excessive but we can
fix that by giving more parameters
because obviously not the area's not the
only
we used an aura not the only factor in
how much of houses and so we can fix
that in a number of ways that's one way
to fix it we can also fix it by having a
better model
maybe that fits the data a little bit
better now one thing I left off here was
in the course of writing things down I
wrote some things wrong so I said 40
million instead of 4 million so I had to
go through a process of cleaning up data
as well
and this is the whole machine learning
process from like gathering data we're
actually asking a proper question and
asking a sharp question is the first one
gather data clean up the data do
visualizations figure out sort of the
the patterns of your data we didn't go
through transforming features we'll go
through that in a little bit selecting
an algorithm the straight line linear
regression we train the model kind of
fitted a line to the data property and
then use the answer and then one step I
kind of haven't liked once ever left
office obviously the world changes all
the time
so with new laws and when you parameters
happening like in the world the model
may not fit anymore so you have to
retrain it and yes to continuously like
get new data yeah yeah we'll get to the
sharp question and yes like everybody
yes it is so glad you asked because here
is the answer so a sharp question is
something that yes has the straight
answers so it turns out that there is
like five questions that you can ask
machine learning really only five like
well five ish five types of questions
one of those questions is how much or
how many like a number so you have to
have a proper number that's the answer
this is called regression and remember
Laverne was collinear regression so that
comes from regression so in this case
you can ask things like what's the price
of the house what's the temperature in
in London on Tuesday which we don't need
machine learning but you could and/or
how many people will buy my product
based on these parameters you could also
for example ask what's the stock price
going to be but a stock price is
something that's very volatile and not
something that humans can normally
predict so it's very hard for machine
learning to predict you because it's not
only based on past performance so that's
one question and then a next type of
question is is this or that like so it
can be and this is called classification
so given an email and how it looks is it
spam or ham like I said spam or not spam
or dispersion given all these symptoms
is he going to have diabetes or not and
it can be two classes or it can be multi
class so if you look at like a news
article you can say which type of news
article it is music technology sports
whatever but it's still classification
the third type of question that you can
ask is another type of classification
and that's called anomaly detection so
anomaly detection basically has a lot of
normal cases and then it has a few
anomalies so it might look like
classification because it only has like
two classes but it's not because the
anomalies are so few that we have to use
totally different algorithms to
determine anomalies from the normal case
anomalies being things like fraud
detection like if you look at a lot of
credit card statements or in a weblog
you can see if a request seems very out
of the norm
all of these are supervised learning so
supervised learning is something where
you have a lot of data from before this
is given these features this was the
answer given these features this was the
answers so you have labeled data like
things that you know the input and you
know the output and you can just learn
from that then there is a state I'm a
supervised learning which is clustering
and this is essentially grouping things
but you haven't decided on the groups in
advance so you might group products but
you haven't decided that you're going to
group them by phones and tablets and
computers or whatever instead you say
give me some similar stuff and what
might you want to do this may be when
you want to recommend something to
someone for example so a recommender
system is good for clustering so you say
these people are kind of similar they
probably like the silent same type of
movies or these movies are kind of
similar so if you like this one you
probably like this one and you don't
decide in advance exactly how you're
gonna categorize nummis like romantic
comedies and drama whatever it might
disc lustre like all the Brad Pitt
movies together and all the zombie
movies together and kind of like the
zombie movies with Brad Pitt one not
another huge cluster but it could be so
this is unsupervised learning because
you haven't decided what it's going to
classify and then finally there is
another one it's called reinforcement
learning and reinforcement learning
basically works in another aspect of the
human brain which Hays penalty and
reward so let's see your kid and you
touched a stove and he goat like that
and you don't want to touch it anymore
because it was hot and was red and it
hurt so you got a penalty being that it
hurt and you're not doing that anymore
on the other hand if you're cleaning up
your room and your mom is going like
good good continue then you might keep
doing that and reinforcement learning
works kind of the same way you have a
minimize or maximize ur function that
tells you if you're further away from
where you should be or closer to where
you should be and this is used in things
like trying to get a computer to learn
chess or in this case a vacuum cleaner
that figures out if it should keep
vacuuming or go and go back to the
shorter or
each system that should race or lower
the temperature so these are the five
types of questions that you can ask
obviously the one we asked was a
regression problem because we asked it
how much the price of a house was gonna
be then you go through and collect the
data and collecting the data is usually
a huge ordeal in figuring out exactly
what data you need to collect for your
for your machine learning to work it's
an even bigger problem but I want to
just give some hints if you're you
starting out in machine learning because
the data collection can be kind of
tedious there are some shortcuts so you
can go through kaggle calculus a website
for machine learning competition so you
can make a lot of money go into cattle
and national learning machine learning
but you also get the datasets for free
then you can work on UCI is another one
it's a website would stale data sets
like data sets of things that happened a
couple of years ago or like basically
donated data sets that are now kind of
not interesting anymore and a sure
machine learning is another place to get
data so lots of good places to get free
data from like there is like predictions
we can make predictions on Titanic who's
gonna who's gonna die and who's not
gonna die based on a bunch of parameters
their wine quality your UFO sightings I
have no clue what they're gonna do with
that but there are plenty of what of
data sets and various more and more and
more the more you look independently of
how you get your data so your data
normally comes from a lot of different
sources but you kind of want to put it
together in sort of an excel like format
or just a square where so this is for
supervised learning for supervised
learning you then have a target so this
is what you're trying to predict and
then you have a bunch of features so
features are the input parameters to the
function that machine learning is going
to create and target is the output and
the features
or sorry the the row is called an
observation or an instance that is
basically the features mapped to the
output now so the features can either be
numbers and numbers are or consider
something that you can multiply or add
or subtract basically something you can
make an mathematical medical operation
on and then you have categories like
this one is the type of house a detached
home or is it a town house and since
math can't deal with categories you'll
eventually have to turned into ones and
zeros or or some kind of binary thing
and then you have sorry many other
things like the sick code where the zip
code kind of looks like a number but
it's not because if you would add zip
codes or if you would subtract zip codes
it wouldn't tell you like how far away
they were or or any good information
about them so you have to then turn them
into categories and eventually turn them
into numbers again I'm saying this
because this is a very important part of
like how machine learning works in the
sense of machine learning it's only math
in the end even if you don't know the
math you need to understand that it's
math so how do you figure out what what
kind of data to gather for for machine
learning well there are basically three
good rules for this it has to be
relevant it goes without saying so it
has to be data that actually tells you
something about the problem so how far
away from the water of my house is
number of chickens in Alabama at the day
of the sale maybe not so relevant so not
something I would pick up and then you
have some that you kind of don't know if
they're relevant or not it's the the
broker is that going to be important for
the sale or or maybe the broker is used
like you don't know cause and effect
because maybe brokers just pick more
expensive houses and they don't actually
make the house is more expensive so
sometimes there's difficulties and
figuring out whether or not I should use
something and
in your methods because you don't know
if they are the cause or effect so
relevant is one the next one is
independent so if you have the area of
the house and square meters
you shouldn't also have the area of the
house and square centimeters why well
because math is going to then double the
effect that this has on the outcome like
if you have the same information twice
it's just going to sort of double the
importance of that and this might seem
kind of like yeah of course I'm not
gonna have it both in centimeters and
meters whatever but what if you have the
area of the house and the number of
rooms those are still extremely
correlated so those will still give the
same answer see how to be worried about
features that you bring in that have a
strong correlation so can you calculate
the area of the house roughly from the
number of rooms then you shouldn't use
both of the best input parameters and
this third one is simple so if you have
the GPS coordinates for the house that's
extremely useful because you know
exactly where the house is but how do
you use this in math like there isn't a
really good way instead you would then
transform this feature into something
that math can use which is distance from
the water or a distance from schools or
distance from transport systems or
something like that if you can minimize
or maximize so good deal of machine
learning classic machine learning is
actually taking features that you have
and transforming them into something
more useful for the model now I have to
say what I'm saying all this this
particular one and the independence one
those are things that deep learning
takes care of because deep learning kind
of figures out these relationships for
us innocence
a little bit not not fully but that's
kind of the magic of deep learning and
that it figures out their relationships
so now we have all these features and
now it's time to explore the data and
you can do that in a lot of different
ways Excel being one
our machine learning has a lot of good
ways to to explore data and like do
diagrams of things but mostly you would
do that in something like Python and
this is called a Python notebook so
Python if you haven't used Python Python
is a really cool language in the sense
that you can run it in the browser
interactively and you can do something
like this when I show you the notebooks
in a bit but what you create basically
is documentation Wilier code so you
write some markdown and they know you
write some code and some markdown and
suddenly have like the full story that
he didn't give to anyone else in the
project and they'll understand like how
you came to the conclusions that you did
and the other reason why you do
something in Python or an R they said
both of those languages have really good
machine learning modules and also for
example tensorflow from google to do
deep learning works with Python and C
mtk from Microsoft and a lot of the
modules that are created work with
Python so during the exploration phase
what you do is usually you take a look
at what do I have and you try to figure
out like all the parameters are a are
they strings categories numbers do I
have to convert them do I not have to
convert them and that kind of stuff what
do I expect from them to expect this to
have a high effect or a low effect
because then you can kind of validate it
later and a lot of machine learning when
you create your algorithms and you want
to refine them or about humans then
going in and saying you know what I
think we'll treat this because this I
know that this would be a factor if I
looked at myself so that will probably
be something that we want to make more
prevalent in in the machine learning
process and then you'd look for things
like missing data and outliers so and a
few other things so duplicate
observations duplicate observation says
if I have two of the same observations
so in my case the how
prices on some sites where I would
gather data I would find out both when
the house was sold and when it was
finally transferred and it would be
listed as two different observations and
we have two observations like that and
sometimes conflicting they will skew
your data so yeast won't have the one
observation and you want to look for
irrelevant observations so if I want to
create a model of townhomes then I don't
want the detached homes to be in there
so if you have something that doesn't
necessarily fit your model then it
should not be in there outliers is
really odd
data so we'll see that in a moment whoo
and look at the actual data but for
example we have the really read a big
house that doesn't necessarily fit the
trend of houses that will then skew my
model and make the line go more like
this instead of like this so we want to
remove those and missing data we always
have to have all the input parameters to
get the output parameters so if we're
missing some of the information like if
we're missing the area of the house or
something else then we won't be able to
make a prediction so in that case we
have to either remove everything the
tasks missing data or do something like
guess the missing data like try to
impute it before we take it to the
training process and sometimes you have
things like structural errors so if you
write things down you might have
sometimes wrote and written down yes and
sometimes why and sometimes the data was
missing and then you have to kind of
clean it up so it's in a format that
looks similar so that's what you do and
after that you can then go ahead and
create like much of diagrams so this is
another reason why python is popular and
are because they have very good
visualization techniques okay and then
you go through and clean the data so
cleaning the data and this is a super
important data set it takes care of
figuring out if a superhero wears a cape
or not based on a few in particular
meters so we have first-name lastname
one day
or and height and if you have things so
it kind of looks nice but it's actually
a mess we look at the birth here for
example we can see that sometimes it's
written down with dots and we have some
before cries and some dashes I mean
sometimes it looks like some people were
and cracked wouldn't I wrote this but
yeah we need to clean it up before
before we deal with so we simply go
through and clean it to the point where
it's nicely formatted and BC can turn
into a - that's okay yes as long as it's
in the same format then we have the
height and height is one of those
parameters that's similar to a GPS
coordinate because us is it imperial
though um you can't calculate them with
a calculator that's it so what do you
have to do then is you have to transform
it to something that you can actually
calculate which is like the number of
inches so in that case we'll just do a
quick transformation and turn it into
that then we have the birthplace and the
birthplace is a categorical with a few
with a lot of categories actually and
it's okay to have unknown like if you if
you're missing a few values or if you
don't know they valued so key to have a
category that's called unknown for the
identity we clean it up - why yes no
same thing with if it can fly and
sometimes you then have to go back to
the person who gathered the data or to a
domain expert and figure out what does
this actually mean like this weird thing
that you wrote down here we have three
categories good bad neutral because
Selina Kyle doesn't want a line and then
where's cape again yes and no this is
what we're gonna predict all of this
takes a lot of time it takes like eighty
nine two percent of the whole machine
learning process this is called shaving
the shark like it's really difficult to
but it does take a lot of time and
sometimes actually when you're done with
this
and you've done your visualizations and
everything you might not even do machine
learning because you were too gained
enough insights around your problem that
you need kind of use like for example in
the case of the Titanic survival thing
you find out very soon that it was women
and children first
and you don't have to do machine
learning and create a model didn't you
say if it's a woman she survived
specifically if she was like less than
20 years old she definitely survived and
if not didn't survive and then that's a
Google it's a good enough model that you
don't have to do and go through and
doing a lot of training and everything
but of course that's not always so in
that case you would then go through and
and do a Jupiter notebook and do some
just say you're not here so this is how
the notebooks worked so you can either
run them on the web at Kegel you can
actually run the notebooks at a kegel
website or you can run them in Azure ml
Sascha I'm Alice Microsoft's machine
learning studio on the web where you can
for free and do like experiments up
until a certain point where you have to
start paying for per hour of training or
whatever but I had they have you paterno
books that you're gonna run and a lot of
other things or you can install Python
locally and run this as a web server on
your system and and the nice thing about
this is is you can then write code run
it and write some markdown and run that
and this is sort of like the
documentation I was talking about so
intermingle like text and pictures and
an actual code so here I opened a file I
looked at combs I list like the five
first elements stuff like that so used
to show you some examples of the clean
up face in this case I ran some commands
to figure out if I had null values in my
data set so I gather the data
from one of the housing sites in Sweden
where most people list their houses and
all the yellow ones here are missing so
with a visualization you can kind of
quickly see that there is like a toggle
state here so if the house has land area
it doesn't have a monthly fee and the
other way around so this particular
thing happened to be that the ones that
had monthly fees were condos booster set
in Swedish for my Swedish friends over
here and then you don't own the land so
the land area was listed as zero so you
find things like this and you find it
quicker through visualization even
though this is kind of like stupid
visualization you find patterns like
this pretty nicely one is give an
example of how it looks when this is
Python by the way if you've never seen
it before
it looks similar to any other language
except for it doesn't have semicolons
and it works on indentation as far as
figuring out when the next statement is
but you would do things like cleaning up
the region so the region is kind of
messy because the realtors will
sometimes write down you know Tribeca
and sometimes write down like triangle
below canal and then you have to figure
out is that the same or not and they
kind of have to put them together so you
then write a function in Python to
choose to clean it up and then convert
them so similar to what you would do in
any other language really and I'm gonna
go down here and see so this is one of
the diagrams for area for example which
shows that the area for the houses we're
looking at they're mostly around 120 or
most of them are actually between 110
and 160 but then we have this dot up
here saying that there are some odd
values like one of the houses was 500
square meters and then you start looking
into why was that house really really
big and you find out that there was
actually an error
whoever liked the realtor just wrote it
down it was it wasn't supposed to be 500
and you can also see those kind of
outliers in here and then you clean
those up so it's the process of kind of
going through those duplicate values and
outliers and everything and cleaning
your data in fact here I had some other
ones that were really really expensive
and then I looked at the houses on
Google Maps and found out that they were
by the lake I didn't have information
about by the lake so but obviously that
now helps me to say I need that
information to make an accurate
observation about these so try out cube
their notebooks if nothing else to see
where we are yeah okay so we've cleaned
up the data and now we're ready to
transform some features or create new
features so for example from the GPS
coordinate we can then create like these
how far away it is from from all these
different points of interest or for
example if you're looking at how much
ice cream you're gonna sell and you
might have the date you might want to
turn dates into whether or not it's
winter or summer because that might be a
better estimator for you than the actual
dates or is this a holiday or is this a
weekend or something like that so trying
to make as much useful information as
possible out of the data that you do
have and now we're kind of getting ready
to the actual machine learning so it's
been a long stretch but here we go
so there are a couple of different
algorithms in machine learning
one of them being the linear regression
like the straight line and we've seen
how that works another one is called a
decision tree so this is a data set
where we're trying to figure out if
people are going to play tennis or not
you can see the business case might be
how many people should be staffing the
tennis store
I'm taking this because this is kind of
one of the quintessential like training
data sets that is in every single book
but it's a pretty good for explaining a
decision trees so a decision tree it's
kind of like asking 20 questions so I'm
thinking of a person and someone needs
to like ask 20 questions and figure out
who I'm thinking about what's your first
question that you would ask mayor FEMA
yeah exactly
that divides like the sea of people into
two very big chunks so if you say yes
you know that would lost all these and
if you say no that we lost all these
right so it's a very good way to
separate out and this is kind of how
machine our decision trees works well so
if we're gonna figure out if someone is
going to play or not and we'll take a
look and see is there something in here
that we could use to actually tell
immediately if if they're going to play
or not and the answer is if it's cloudy
then they will always play no matter
what the other parameters are so we now
have kind of like a decision tree like
what branch is to say cloudy yeah they
will say so now we can remove all those
sets some days and then we try to find
out of pattern so if it's sunny and high
humidity then they won't play and if
it's rainy and not windy then they will
play and you get like these trees
basically if else statements in the end
and you do this on a whole set of data
and this becomes your algorithm now
neither linear regression or decision
trees are usually used alone and because
they're kind of crude and specifically
where decision trees if you do this
you'll end up with something that learns
things by heart so you don't want to end
up with
because then you're not going to
actually be able to predict anything
from the new like from any unseen data
that you have so instead what you do
where decision trees is you create
shallow decision trees that maybe have
only two or three layers and then you
have them vote like so you can create
like a thousand shallow decision trees
and you have them vote and say well you
thought this I thought days together
will think this or you can boost and do
like a decision tree that feeds into
another decision tree so there are ways
to kind of augment simple algorithms to
do something more spectacular but
decision trees is as one of them one of
the ones and decision trees can actually
use be used both for classification and
for where you figure out the number
another one is called nave base and this
is an example where we have a bunch of
emails some of them have spelling
mistakes like so some of them are spare
and some of them are not spend the red
ones have spelling mistakes so now the
question is if you have a spelling
mistake what's the probability of this
being a spam email I won't ask anyone to
do math but it's and then you go through
and you look it's okay so if it contains
the word sheep then what's the
probability if it can if it doesn't have
a title if it's from this country if
it's sent to these many people you know
all these properties that the email
might have and then you calculate all
these together you get a probability and
you use that probability to say this is
spam or not spam so naive basis and then
we have another one for a classification
citizen's apples and oranges based on
sweetness and acidity think about how
you would divide the apples and the
oranges maybe like this yeah so we're
back at a straight line again which is
why we call it a regression
but in this case is called logistic
regression
so logistic regression says are you
above or below this line and if you're
above then you're something and if
you're below you're something else and
we can see similarly to when we try to
figure out a number for the house this
is also imperfect like this Apple will
be classified as an orange and this one
too and the further away you get from
this line the more probable AJ state
you're actually classifying things
correctly but you will like it's still a
guess you will
we'll still have artifacts like this so
those are some base algorithms and then
you have neural networks and I'm not
going to go away and deeply into neural
networks if you weren't at Barbara's
session go ahead and watch it online
afterwards it was really good but we
have a set of inputs and an output and
what we do now is we create we take for
example the size in the bedrooms and
that becomes like the family size for
this house or walkability or school
quality like we create all these new
features in layers so this is one layer
that transforms this into this and this
is one layer that transforms this into
this and eventually you'll have like a
large number of layers that you don't
know what they do like so the key here
is while it may look like we're actually
creating this layer and deciding what
this layer will do or what this small
algorithm will do it doesn't like neural
networks the medical neural networks is
that it figures out these layers for you
so you figure self like the
relationships between the features so
you don't have to yes
yes oh yes and no and this is kind of
like the the problem of machine learning
as it stands today actually because deep
learning is super nice and my you can
usually get pretty good results with it
but traceability is horrible so and if
you don't have traceability and you
don't know why it guess what it getting
did then you might be doing it on
totally false premises so a typical
example is one algorithm that was
supposed to figure out if I have
something like a dog was a husky or a
wolf and they had really really good
accuracy until it was found out that it
was only looking at the snow in the
background and making that
classification or something like it in
the underground figuring out patterns
that's like how many people are actually
on the underground and it was again
doing a really good way like a really
good classification but it turned out it
was just looking at the top left corner
where the clock was so those are kind of
funny but in some cases it can get like
you're classifying because of all the
wrong reasons like race or other things
that you don't necessarily want to
classify on so the answer is a little
bit like so in imagery like if you're
classifying images which is what most
neural networks are used for then there
are ways to visualize it so the layers
will typically be like first you have
all your input pixels and then the first
layer might find sharp edges and the
second layer might turn those sharp
edges into a square or a circle or
something like that and then in the next
layer you put together the squares into
circles and it becomes an air for a dog
or something like that and you can kind
of visualize that with different tools
but a big deal in machine learning right
now is trying to create traceability and
trying to create
transparency to the point where you can
actually see what's going on and it is a
huge problem because so I was involved
in in this project where we were looking
at retention so figuring out if someone
was going to leave the company or not
and we got super super good results in
the beginning like we were classifying
like ninety-five percent correctly and
when you do that you should kind of
think and say yeah maybe that's that's
not that credible so it turned out that
it was looking at and the performance
evaluation of of the employees but
problem was that for a lot of the people
that had left a long time ago there was
no performance scores so they were
defaulted to zero so it could easily
tell that everyone who had a Syrah score
would leave the company and obviously
that doesn't help anyone but yeah
traceability is a big big deal and it's
very hard sorry for the long answer yes
yeah yeah so if I urge you to look at
Barbara's session because she was
explaining it a little bit better than I
would be right now but yeah you have all
the pixels and depending on if you're
using like a convolutional neural
network it it still keeps like the
relationship between like where your are
in the picture and figures that out so
in that case you will do like processing
on very small parts of the picture and
putting things together there's a really
good YouTube video that's called trying
to figure out friendly a friendly
introduction to neural networks he's
really really good so you should look at
that sorry
it will be a very long answer but it
okay so what algorithm should you use
and the answer is not always neural
networks because sometimes you need that
traceability but there are plenty of
cheat sheets this is one from asharam
owl that you can use to figure out what
to do and the reality is that you will
take for example with a two class
classification then these are some
reasons why you might want to use this
one instead of this one but in reality
what you'll do is you'll try all of them
and then like at a smaller subset of
your data and then you figure out okay
so this was really good so I'll keep
kind of improving on that and optimizing
something we call hyper parameters which
basically for example if we say we have
linear regression but or pulley Gnomeo
regression where it's like a curve like
this deciding on whether we should have
like a two degree curve or a three or
four or whatever we should have so you
keep tweaking like the algorithms after
you've decided that this is a very good
fit so used to kind of talk about it a
little bit like so logistic regression
is good for things that are easily
separable bialon where a decision tree
is good if you have like a little square
pockets off of data like that and
usually like the ones that will perform
the best are normally some kind of
decision tree based algorithm okay so
now we get to train the model and
training the model kind of looks like
this
you feed into data set you clean the
data and you split the data into a test
set or sorry into a training set and a
test set and the reason for this I'll
let you picture the reason for this is
because we could technically have a
really good model we could have a model
that looks like this and said every
single one of the dots but when we get
new data we wouldn't necessarily be able
to predict anything at all
and instead if we would use a straight
line we might not fit everything for the
training set correctly but we we have a
much better way of fitting like new data
so doing this orange thing is calling
Oatley it's called overfitting now if we
make the model too simple
that's called under fitting and then we
might need to to move it into multiple
degrees but figuring out if you're
overfitting or under fitting it's a
matter of first training the data and on
a train set and then testing eight on a
test set and this is done like sometimes
you do it straight like this in the
beginning but eventually you will want
to use most of your training data if you
don't have that much so you use
something called a cross-validation fold
we have techniques for being able to use
like four parts of your data test it on
one and then use for other parts of your
data and test a time on another and then
once you train the models so training
the model is basically saying I want to
use this algorithm create a model for me
and then you go back and you score bits
of you take this data set and you
predict writing and say what are the
values here and then you evaluate and
see how far off where you given a lot of
different criterias so it looks like
this in reality now you can do this in
in Python just using code or in our and
Barbara was showing how to do it in
Python wait your networks and tensorflow
this is kind of like a drag-and-drop
environment a sure machine learning so
it's a very nice getting started
environment it has some limitations when
you get up to doing really complex
things but it's a very good prototype
deal that you can at least get started
and kind of figure out what what your
models are going to be like so you
dragon
and draw basically is placing random
things on here I wait to see what I did
I guess I didn't drop anything at all
but so what I've done here is I brought
in my data set that I cleaned in Python
and if I go in and look at it is yes
then Excel format with all these
different parameters and the price is my
labor no label and I can do things like
visualize let's say I have the area I
can then compare that to price and I can
get a visualization showing like this
scatter plot right forth and then I went
in and said when meted edit metadata as
one of the modules in here I went in and
said to some of my fields where
categorical so it will automatically
turn them into ones and zeros and select
whatever comes I want from the data set
there are modules in here and in Python
that will help you figure out which ones
are the best parameters to start with
because sometimes you have ten
parameters but you might only want to
start with five and see how well you're
doing with that and see if you're
actually increasing or decreasing
precision would by adding more and then
splitting the data in this case we split
so we're training on 80% of the data
testing on 20 and then you start running
this so in this case I have like nine
thousand rows so it takes like 30
seconds or less to to do this but you
then end up with a train model and I
just wanted to show like a decision tree
and how it looks so you can see there
are many different decision trees all of
these are decision trees that will then
kind of vote with each other to figure
out what the correct answer is but this
is a particularly if I
let's grow up okay so in the beginning
it says if the land there is over six
hundred and fourteen I could go this way
if not it will go this way
this basically is an if-else statement
thing and then you end up with a train
model that you can then apply to to the
test data and when you test it it will
then look like this so we have the prize
okay we have the price right here and
then we have a skort label so this is
what it thought the price was going to
be for this house and obviously there is
a difference here and that's the
difference we're trying to minimize and
then we have Indiana and evaluation that
we can look at and see for these two
that I was testing like okay a decision
forest and a linear regression which one
was best
so 0.77 is the coefficient of
determination that kind of is a
coefficient between want to see Ron one
that will tell us how good we're doing
and this is one of the things that we
can try to improve or monitor and that's
essentially what you do to do the
machine learning now we talked about
looking at how good your model is and
and this is for classifications so again
looking at ham or spam or not spam and
in this case we have I guess a thousand
emails some of them are spam some of
them are not spam out of the ones that
were spam I send 200 to the spam box and
170 to my Inbox
on the other hand out of the ones that
were not spam I unfortunately sent like
30 to my spam box and 700 to my inbox
and now the question is how good is this
model and what actually matters so there
are three different ways to calculate
this one of them being
accuracy
Sakura C is basically how many did I
actually get right 800 out of thousand
right but in the case of spam the
question is is it more important that I
actually identify all the spam or is it
more important that I never send an
email from my grandma to my spam box and
the answer is you shouldn't send
non-spam to your spam box so what you're
trying to figure out is a way to
minimize this number so in this case we
have 130 that were sent to spam and I
managed to get like a 76 percent
precision on what I was sending to spam
on the other hand you might have a
problem where you're identifying people
that might have cancer and in that case
you might be want to send a few extras
to the doctor for an extra check in that
case you want to look at the recall and
said which is the other line of the true
positive or the true positive or true
negative things so this is true positive
this is true negative false positive
mwah
well yeah either way this is called the
confusion matrix for good reason
and this is part of what you use to
determine what you want to like how to
optimize your machine learning models so
and again decide on what you want to
optimize and then start working from
there now figuring out like the
parameters for the algorithms you can
also do through code by specifying what
you want to optimize and it will it will
kind of do a grid search and figure out
what parameters will do best so and then
finally use the answer using the answer
in annasher ml you can basically go
through and create a website or web
service out of this when you can pass in
input parameters and it will give you
the answer or a batch processing where
you can send an excel file and it will
automatically like rate everything for
you
but at this point we're kind of at the
end except for except for that we also
should continuously retrain the model
because things will change and like AI
and machine learning is kind of like
it's been through a few rough times with
because it started off kind of like a
heuristic system and and now we're kind
of at a sweet spot where we have a bunch
of data and a bunch of computing power
that actually lets us do a really good
machine learning so these are used to
statements that I think are kind of
interesting data is the new oil kind of
referring to that day days whatever
fuels the system and data is now
becoming like a very very strong
commodity so if you look at big
acquisitions that companies like the big
companies for example like Microsoft or
Google or other companies make they
don't necessarily make acquisition
estate will will give them a immediate
benefit as far as like how much money
they will make but they make
acquisitions based on how much data at
that company owned so data is a very
strong commodity the AI is the new
electricity by Andrew and G he's a guy
who's new who doesn't like his big name
and deep learning and his statement here
is essentially he's like electricity
transformed like essentially every
industry AI is now on the verge of
transforming every industry as well and
I think I want to leave you with with a
few thoughts because it's transforming
every industry we're kind of at a place
where we have to take responsibility for
the way it's transforming things so real
like AI is really really good at doing
things that humans can do in a split
second or less for example looking like
if you're a really skilled doctor and
you look at an image of an MRI scan you
can figure out if someone has a tumor or
not but it takes skill that skill that
machine learning can learn so even real
this skilled like even professions that
require quite a bit of skill can quite
easily be automated with machine
learning so we have to take
responsibility and figure out like sort
of where where we want all these people
including ourselves because a lot of our
jobs are also going to be automated how
we're kind of going to continue and make
use of the real power in our brains and
that kind of stuff and the other thing
was something that John was talking
about like the traceability and figuring
out like the first off how did we come
to a decision and also is that a
decision that we really want to make so
if you look at something like using
machine learning for hiring and you've
always hired white males wood beard to
do programming is that gonna be your
future because if you're using machine
learning and looking at your past that
will be your future because machine
learning will only learn was whatever
has happened before another example of
that is something that you see a lot and
then I'm Facebook made a big statement
his recent play saying that they will
now give you more power over your feet
but if you have a particular political
view or if you have a particular
interest you probably notice that every
time you do a search or every time you
look at your feed that interest or that
political view is kind of like
continuously it's given to you back over
and over and over again no one what kind
of dispute your facts except for maybe a
few of your friends that that have a
different opinion so you end up in this
bubble of confirmation bias where
everything you see is something that
you've already decided that you wanted
to see and if you're not challenged by
something new your views will never
change so we have to kind of look at
that and take that into account when we
think about machine learning and an AI
and how we want to do this so thank you
that's it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>