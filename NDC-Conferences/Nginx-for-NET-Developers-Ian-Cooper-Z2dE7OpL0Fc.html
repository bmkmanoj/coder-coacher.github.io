<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Nginx for .NET Developers - Ian Cooper | Coder Coacher - Coaching Coders</title><meta content="Nginx for .NET Developers - Ian Cooper - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Nginx for .NET Developers - Ian Cooper</b></h2><h5 class="post__date">2018-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Z2dE7OpL0Fc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so
talk today about nginx and this talk is
mainly aimed at people who are not net
developers particularly moving to that
dotnet core running things on the UNIX
platform who need to understand what
nginx is and what it can do for them for
generally if you're not developer P once
know about nginx we will probably cover
enough of what nginx does without you
having to know anything much about what
that actually does to feel comfortable
you'd get something out of it and it's
designed to really be if you like an
introductory talk we will cover off some
of the basic scenarios that you need
reverse proxy load balancing there's a
lot of depth or something like nginx we
can spend three or four hours talking
about all the options but hopefully this
will get you started and getting you
enough to basically be able to deploy
into production environments and then
you can start tweaking things to your
heart's content
Who am I most of this slide is really
quite unimportant the only bit you
really care about I mentioned at the
bottom is this kind no smart guys rule
those of you by the way the Bacchus you
know there are monitors are there
hopefully you can see stuff but I will
read bits on the bomb and slide just has
no smart guys only the guys in this room
there is my challenge to you guys is I'm
not smarter than you I just happened to
be confident enough to come and stand
here and speak and I did that because I
overcame my lack of confidence by just
speaking starting out with he's on
lightning talks and user groups etc and
all of you could be up here doing what I
do and I encourage you to have a go and
participate and it's really good for I
think you as a developer because it
helps you to structure and organize your
thoughts a lot about what you're about
your practice and I think the other
thing to bear in mind from people say I
haven't anything to talk about
that's because you talk to your peers
and they know what you do but many
people don't know what you what you know
and any of you are not you know more
experienced developers you have wisdom
and understanding to pass on touching
your developers we need people out there
still doing talks about the basics of
TDD and those kind of things for new
guys to learn right so please do speak
that's where I work you don't really
care apart from the fact that another
consultant I have nothing to sell you
right and this is just a plug for bright
so how many of you done that developers
right so this I mean if you've heard of
brighter okay fine so this is a CTRs
framework I work on it also does
basically tasks use I'm messaging
basically between your web app and a
worker process so that you can offload
work to work a process and you can use
it for communication between micro
services via basically messaging and we
build it built in a huddle and we've
open sourced it and people outside
huddle as well work on it with us so
it's alternative to things like and
service bus and mass transit so please
check us out you can find us by just
searching for practicum and on Twitter
or go searching for brighter command on
github
and you'll find us what's today's agenda
agenda first of all what I'm going to do
is do a little demo right and we will
start with dog demo of a little web app
and I will show you it running without
any kind of proxy in it and then I will
show you running with a proxy and expect
to be a bit confused
I'll show you basically the engine etc
running without explaining everything in
detail but I want to give you a feel for
the hot walk were actually building to
what understanding and then we'll start
digging a little bit into some detail
we'll talk about cache turtle what it is
and what it isn't I'll talk about nginx
itself what it does for you how it works
and then we'll talk about some key
scenarios reverse proxy and load
balancing and we'll come back to doing
demos and we get to reverse proxy and
low back sink run show you a whole range
of different scenarios okay then we have
a kind of Q&amp;amp;A at the end and I'll
probably hopefully have time to mention
a few things we didn't cover the nginx
we'll do for you
I don't know given the amount of time
it'll take to run through all of that
what we'll have much time for questions
in the session but we the police prefer
to cache we turn it down afterwards
okay so let's do this a quick look
right so this is a s pertinent core
application and it's based Web API so
we're going to basically create an API
that we can serve this is the world's
most over complicated hello world
application and what we can do you this
is basically far what we can do we can
say I can post a given greeting to the
send point say basically with my message
hello world and then effectively I can
go and get my list of greetings back
okay and I will show you a little bit of
how it works but we're not really that
interested in that
so you can see there I've got HTT p-- o
stand what we're actually using is never
once see by the way anyone need it
bigger good okay just shout if basically
things are not visible or not audible
shouts and I will fix it
okay so you actually using brighter and
darker here or the framework see I can
explain to very quickly what's happening
so the command processor just says we
are sending the command to do something
here we're going to add a greeting and
the query processor says it returned us
and results and what happens is they map
over to handlers and this is what lets
us do is build a pipeline such that we
can insert things in between for things
like retry semantics circuit-breakers
logging etc we document all of that else
why but just go to ports and down here
for example you can find a greeting
command we just basically spin up a
framework or do be context and we just
basically add everything into their
database obviously we're all a sink our
way through obviously nowadays please
try and be a sink if you can and this
for example is a greetings by ID query
and we'd go into that context and great
to me you can see some attributes there
query rich reliable and you can see some
here that's essentially those actual
pieces of the pipeline were inserting to
do some work beforehand that's not the
solution of this talk but you are going
to have a look at what we're doing a
bride to that'd be interesting okay so
let me just switch this and get help to
a standalone version all right
sir this version is standalone all right
so no other we got here is a docker
compose file who's familiar with docker
mostiy okay those either are I'm
probably going to not do a digital
introduction of the docker as part of
this hopefully you'll be able to follow
anyway so what we're saying is we're
going to build two services here
basically the web service which is been
a bit gonna be our actual web
application that we want to launch and
essentially a database we're using my
sequel just because it's a lighter
footprint in darker than sequel server
for Linux which requires a lot of memory
on my box we're spinning up with those
applications okay and then this tests
HTTP file will let us fire some requests
against that application so we're going
to go over here I don't know whether to
keep what setup assistant it's a very
disturbing thing to see in the middle of
the demo but I'm let's hope there's
nothing there that this one's let me
just get this bit bigger okay and then
we're gonna just do docker compose up to
bring up basically all those servers
actually pretty why should just check I
had need to do first
our this technique to build it first
sometimes do and I might as well do that
first rather than get an error so what
we're doing is building and publishing
to an out directly so effectively we're
not using the whole of the SDK and then
we're fetching when we build our docker
container we will take the build a lot
effects from there out directory and
load them into the container that runs
the web app just like watching paint dry
and then we're going to bring those
servers up
and you'll see we basically pull down
images we've actually got them more
locally and I'm just spending on the
various containers and you should see a
final message at the end after my
sequels finish doing its thing we will
see a final message that says basically
we're listening on port 5000 which is
kind of the default for an expert in it
some of you can't see that maybe they're
back we're just saying I'm listening on
port 5000 don't worry about that okay
and then I can send a request
whoops-a-daisy I don't know why that's
not running on standalone the minute it
used to run all the time yeah me just
have a quick check if I can fix it if
not we'll move on to the next bit of
demo and I'm running on five thousand
right okay sorry dude it did to do I
don't know why this file is they won't
just shut that father visitor from a
book okay right so the only trouble with
switching between github files and
changing addresses and stuff is if you
get too close that you're wanting it so
complete their own location right so
this is the one we want to point to if
initially there's a few heart-stopping
moments while basically it figures that
had to do that so I've got nothing in
there currently send a message of
operating in there and I can pull back
that great thing I can send another
message in another message and I just
played like a whole other greetings all
right so I booked this fabulous hello
world application all I need now is an
initial coin offering and I will
effectively be rich okay but maybe I've
read some documentation and it says to
me you know you don't really care as an
HTTP server but you don't really want to
use kestrel as effectively the edge
server for your application because it
although it can will actually do HTTP
processing it lacks maybe the features
you and I might want for example
compression it lacks essentially ability
to do proper proxy cache URL rewriting
it does do SSL but not really in a sense
that you would want to rely on for your
actual application so I want something
to stick in front of that and that's
essentially what nginx will deform
injects we're sitting in front of that
and let me
deploy my application to live so I'm
gonna choose this one because it's
fairly clear and easy so what you'll
notice are than this will shut these
down you don't really care about the
code anymore right now what I've got
here is an engine X folder named engine
X folder now I've basically got a docker
file for building nginx it's just very
straightforward the main thing to note
that is what just a copy over a
configuration file and the configuration
file and nginx is the heart of what I'm
essentially trying to do here which is
to provide by city labor to an edge web
server so you can see I'm doing a few
things in here don't worry about them
too much I'll do some things to modify
TCP I'm doing some stuff to provide some
compression and I'm turning off server
tokens to security I'm adding a few
security headers but the key thing
basically to have a quick look at is
what I'm doing an exposing effectively a
server and that server is listening on
port 80 and the location is listening
out effectively as root so it's getting
all my requests coming in to local host
on port 80 and then it's saying when you
get those do a proxy pass same pass that
across to a group basically HTTP app
servers what HTTP out service well you
can see how to find some called upstream
lab systems actually you use for load
balancing but we've come back to that a
little bit later but in there I'm saying
fine basically on port 5,000 something
running with basically your DNS actually
matches a web and direct the traffic to
them okay as a lot to take it at once we
will explain all this more slowly to go
through I wanted to show you running
into end but we're going to essentially
be able to talk to us and it took this
thing now effectively on a different
port so it might docker compose file
you'll see what my reverse proxy doing
is saying while I listen to basically
externally on port 8080 to the docker
host internet like port 80 but I don't
listening on to put 8080 you can see the
web
no longer has a ports it's no longer
possible to access that web container
from from outside the docker host we do
have an expose which says the container
is exposing 5,000 to other applications
is in the network that composes crating
but you couldn't get to this directly
you have to now go through my proxy okay
I should really have shut still so I'm
gonna bring up this one what we're gonna
do is get the test HTTP file open again
and you'll see now rather than 5,000
we're actually hitting on 8080 and we'll
run that and get back you'll notice now
we've got a few extra few extra headers
it tells us the server is nginx we so
server than nginx so you can see we've
got some so many security as we talked
about so we're saying same-origin four
frames don't sniff content-type
practically except tradition XSS
protection encode this basically is a
zip right so we're compressing the
content on the wire effectively in some
other settings so we've added a few bits
of robustness to the HTTP sub we got out
of the box with kestrel okay and we're
just proxying from the call to nginx
over to our application server okay what
we'll do now is we will go back through
that and a little bit more slowly so
that you guys can figure out what the
hell we're actually talking about and
most of that okay so I wanted to give
you a feel for what the whole thing
looks like and now explain all those
concepts bit by bit won't blow it up and
put up on the ways to understand it so
but the end everything I showed you
there should be really clear to you so
let's talk about Kestrel Kestrel is a
HTTP server their cameras basically
their speed are not cool
and essentially what Kestrel will do for
you is listen on a poor parlours
basically the content coming into that
TCP port into a HTTP request and then
essentially spin up a hand
manage code and direct effectively that
particular request to your handler and
then send the response back out right
but it doesn't do all the other things
that we may come to think about and
being needed effectively form a web
application and certainly if we're used
to using something like is it doesn't do
any of the things that we've come to
suspect out of I asked like you know URL
rewriting caching etc okay so um guess
was built around lib of sola bob is
essentially the library that also drives
things like no js' and what happens with
lib of is we say what I want to do is
listen on a port for say a tcp/ip
connection when I get one what I then
want to do essentially is execute some
code and I will execute that code until
such a point effectively is that piece
of code decides it's going to do
activity some kind of i/o which point I
will then say right while I'm waiting
for you again then I will execute with
that original thread some other requests
I see there's now waiting okay so what
happens is you can use one thread to
successively serve requests that are
coming in and every time essentially
that's read pauses to do some kind of
i/o or other asynchronous work use that
thread to service the next request in
sequence and that leaves you with one
thread a several large number of
requests and that is the underlying
technology become behind say no js' and
why when it came out it was a kind of
bit of a revolution in server
performance okay
it's cross-platform so one of the big
things about espadrille net core is
essentially we can run on multiple
platforms in particularly you're
interested at a lot of cases and running
on the Linux and effectively either a
Linux server or effectively a container
within docker and all that yes it's not
cross-platform so you're not gonna be
able to use our is in those scenarios
and essentially it's the production of
surf right has been a cool okay
so Kestrel basically uses Liberty valve
but it actually uses multiple level
above lips so what it does it says
effect Felicity well the trouble is I'm
only using one throat and they have
multiple threats
so what something like no Jess forces
you to do is run multiple no Jess
instances on the Box to use the Box
resources what Cashville says is well I
just basically use multiple threads and
each one of those straits will affect
you on a lip of leap for you the only
thing it's doing on the lip of loop is
i/o right so essentially it's the
requests coming in and any i/o you're
doing otherwise you're a managed code so
it's basically calling into managed code
stuff switch shots and manage codes
right to execute your code right let's
optimize for low number sis calls and
it's fast right kind of anything you've
seen these kind of this is detecting
power plane take plain text benchmark
and as you can see this is basically yet
done that call enix that's pretty raw
slower with basically when you put the
NVC routing in but it's right up there
at the top basically of the list and you
can see just behind some of the very
fast ones like Nettie and it's way ahead
of things that fall come which is a
really fast Python server many of you
may have seen a lot of interest recently
in the c-sharp language in things like
span and refs essentially vertical thing
a lot of the motivation as far as I can
tell for that it's around reducing the
allocations and enabling you to work
with effectively memory in such a way
that essentially they can push the
performance of kestrel up to the same
levels as nettie okay that's really your
gold standard because Nattie is what
drives a lot of Java
micro-services for example because it's
very very fast okay well she said the
problem is it's not a fully featured web
server so you've got to stick it behind
something which is where is comes in if
you're on Windows or nginx
or a patch if you're in the UNIX world I
don't really know it's about Apache so
I'm just going talk about your next
today
zangetsu puffle whistling it comes on
Windows as well if you want if you're
interested and it sells content response
to HTTP or HTTPS he also has some mail
features I don't even wear any one that
actually uses those but it will support
doing mail for you so it's a webserver
standalone out-of-the-box it can serve
static content so if you have static
HTML files you can effectively use nginx
basis web server to serve those up for
you right but it can also call your
application right so it can do that by
doing proxy parse it just takes the HTTP
that comes into it and it basically
forwards that HTTP application it can
also do things that we call fast CGI so
if your application supports fast CGI it
can pass to you ever file CGI and that's
quite common in the Python world a lot
of Python bases platysanic or new WSGI
people basically passed it over it's a
TCP level basically to the other server
generally speaking what you're doing
with nginx is essentially you're always
doing its respective doing i/o right so
it's not an application container in the
way that I is is it doesn't host your
code anyway what it does essentially it
says as I receive an i/o as a web server
and I know how to service that I'd
rather read a file off disk and serve it
to you or I will call another IO
endpoint be that your application
essentially or something else and get
content out of that so it treats
essentially reading your content your
application because that's not really
something from disk just no operation is
performing okay
as a result it can be very fast by
optimizing around basically the fact
that it's only delivering i/o okay and
when we talk about I think like nginx
one of the things to understand a little
bit I think it's helpful is a comparison
between the world say of IIAs and the
world of nginx
so typically what used to happen to us
when we were using is on Windows was we
get a request in from the user to it to
be sis and basically we get Windows
activation service basically spins out
and says I can look in the application
host config and get a find an
application to deal with this request
and essentially what it does is it looks
a list of the detail in there and it
says I'm going to spin up basically an
app pool and it's put up a worker
process to service that up or I'm going
to handle your request and under is 7 or
later with the integrated pipeline
everything goes through our aspect on
that including trust with static files
and this and it's essentially important
to think about are is less as a web
server than as an application container
right it's a it hosts your application
this is the water docker right in da
core we have the docker host which runs
a docker demon and what we do is then
say ok I'm gonna go and basically to
find some containers and I put an image
out of a registry somewhere I'm going to
basically build that container and I
create a network they can all talk to
each other and have some data volumes
they can all share ok and that model is
increasingly a lot happy by deploying
give them a bit darker directly be
communities running basically on AWS
that's becoming increasingly common
model and the problem is is doesn't make
sense in that scenario and there are two
reasons for that
one is essentially quite often and then
I'm going to run something like nano I
don't want to run containers they're
basically are windows because they are
huge
I want something small and I want
something is essentially a sandbox wrap
around a process is what containers are
and second this is a container model
there is no point to be running a
container model inside a container model
right putting is on my container does
not make any sense because I'm putting a
container model inside of container
right so as soon as I get here I want
something else I want something
effectively that's going to deal with
that issue basically of serving the web
for me without bringing in there need to
be an application container on top so
advantage of nginx is I separate these
concerns I'm not being application
contained that and a web server and just
being web server
all right quick discussion we basically
have web servers work nowadays so you
might think and it's not an unreasonable
search and how the web server would work
is I receive a request from web browser
says you know here's a connect to
connect to you I accept the connection
and then I use a thread to say okay hate
connection
give me your data I will then use that
thread to go and process your request by
creating some connections to be handler
and I always wanted to request with you
okay the problem is that's pretty nigh
even wouldn't Ashley scale what happens
is when you receive a certain number of
connections you can spend all your time
switching between threads and context
switching because you're going to run
out by Steve capacity on your server
you've only got 70 cores once you see
the number of cores you're gonna start
doing context switching and eventually
your website would spend more time
context switching it does actually
serving requests and eventually you'll
run out of memory as well because each
request these thread will take a set
amount of memory and I bet effectively
eventually you'll start queuing
requesting your circle then will start
spitting out 500 hours cuz it can't take
any more low okay so we don't build web
servers that way we actually a bit more
complicated so this is the way that a
modern web server works I receive I
accept the connection coming in and what
they do is I actually say right what I'm
gonna wait on effectively the operating
system telling me effectively its
buffered all basically the bytes that I
need to read so I basically puts it 50
to make of a completion dispatcher which
says hey I'm gonna wait on you being
done when you're done signal to me and I
will then process the work okay and you
have a kind of single thread here for
something like Lib of but you can have
effectively multiple threads if you have
a selector from model so cache was
effectively using multiple threads that
multiple live are loops yet what will
work by search engine experts here a
second
so affect you when something complete to
spin up a thread and then I say go away
of actively and handle that now down
here you can see as I talk too much to
be handler I may find there are points
where I'm doing IO in that process I'm
reading from disk I'm talking to the
database and what I can do there is
essentially the same thing I can say
effectively right while I hand control
back and essentially I'll wait till
you're complete
I use this strategy going Serbs another
request and what that means is an
individual thread can their service
multiple requests right about that only
above model effectively you know Jess
but all web service now they tend to
work this way right so you're constantly
handing off control when you're doing IO
so you have a smaller number of threads
so we can do then is see kind of how
nginx is built this is nginx diagram
since this is a master processes that
supervisors are quite a worker from
patent mustard spins up and says I'm
they're running nginx and there's a
spread up number of worker processes and
there's worker processes are things that
actually handle requests for you and it
is as a kind of model of saying right
when I'm going to listen basically on a
completion port when things are ready to
be dealt with that thread will deal with
them and it will deal with them until
they do some IO again and then I get
queued on the completion pool and then
effectively our service them when
they're ready again right so a small
number of threads can service a high
number requests because in theory most
web applications tend to be i/o bound
rather than CPU bound suffer here
basically I'm a talk to a property cash
so effectively nginx comes to support
basically in processing I will show you
all that later okay you can a mixture
for CGI HTTP so essentially if I'm not
doing i/o or seven from the cache I can
go and get it basically from your web
server I can also talk by far CGI to
something but we're so there's support
for plugins to talk directly things out
Redis and then cache so rather than
necessarily having to talk to you have
to do that I can actually say I'll go
and get the results basically for this
thing live in Redis so I can use those
as distributed caches I won't show you
that today but it's worth thinking about
if you basically use those technologies
okay the heart of nginx
is essentially a configuration file and
we saw that earlier I showed you that
nginx conf right so you need to learn
how to write one of those that's really
that we need to understand in order to
use nginx in your environment and then -
essentially its configure files went for
a series of directives so you got a
syntax analysis for all units for a
comment and essentially we have a
setting and a value and a semicolon and
we just set hole in our directors and
NGH is modular so we won't show
had to do it today but you can
effectively build nginx some source and
you can change the modules that you have
in your nginx deployment and NEX that's
you each module that you bring in
essentially brings in the Signet and you
said directives that you can use to
manage our nginx so and then directives
can be grouped as a block structure so
you can crib directives so done I'll
come back explain what all these groups
are you can see we've got placing HTT
block a server block and a listen block
within those blocks we are setting
directive values okay so your config
file effectively looks just like that
okay so the three modules that you
essentially always get when you build an
engine X are effectively call that
basically says where's my master process
how many workers have I got how am i
listening for things events and that
says basically I want to listen for
events on completion port what's how am
i doing that you can control the actual
technology used it varies between the
operating system which one is best
there's a default and called poll but
you may want to use the e polar or cage
depending on your operating system and
the configuration configuration I want
to show you much today but just so you
know it exists effectively when you get
a big config file I'll show some big
config files because it's easy to have
one file to show you guys you can take a
section of that put another config fun
just include it so you can essentially
break up your config file and make it
easier to read there are a few key
models we want to think about using and
I should try to explain to you today
that are really key to using this web
server to be cool and that lets us
basically configure what's called
virtual hosts so a virtual host is
really just your downstream that we're
talking to rewrite and let you do you
are rewriting upstream so basically
that's used to do load balancing that's
saying actually services request with a
pool or servers and proxy module which
basically doesn't need load balancing at
all but that's just access basically
like a star bless your act function is a
reverse proxy so she do things like
caching practica tetra fioor so okay you
can build it from source if you want you
options there's pretty good instructions
out there essentially it's all about
creating a using configure to say what
modules I warn that generates a make
file you
build the makefile right there's
instructions out there if you do that
Sherman if you want too hard or you want
to basically add additional modules that
don't come and say the apt-get or doc a
default set you'll want to do that we
don't do that today okay
and that modular system means it's very
extensible so third parties create
modules that you can then plug into
engine exit additional things there's a
protocol open rusty he said open rusty
basically brings in a lower module into
nginx and that lets you convert Internet
into an application server because you
can then write lower code that runs in
response to HTTP requests and on top of
that people have built something called
calm so Kong is an API gateway built
above nginx that uses essentially
modules in nginx and and lower to
effectively help you do things at the
Perimeter for example rate limiting
authorization under parameter
constructor concerns okay
people have a calm or over rusty okay
open Trustees our module that you add
you build intervention acts - lets you
do lower Kong is essentially a set of
modules that basically let you use open
resti to effectively produce an api
gateway so effectively it's all nginx as
its engine at the bottom but effectively
it's just a set of modules to which have
been built by that project to let you
use it in that particular role we said
okay so every nginx config file has a
kind of like the route if you like
something that's basically not with
another block and generally at the root
of the file are you're defining how
nginx is going to run so you're saying
I've got nuts Daisy
saying saying I've got a number of
worker processes right so I'm just
saying for you can choose auto generally
you want the number of cores that you
have if you say auto will figure it out
for you and how I'm gonna listen so here
I'm saying I'm running on UNIX so I'm
going to use a poll those two comps see
at the bottom it just says you use a
poll and worker connections 204 right
I'm limiting the number of connections
that's essentially that I will run
basically on one give an engine X in
this instance to 204 right the next
station put the most important one to
understand is the HTTP block HTTP block
says I want to listen for HTTP requests
socially the pretty fundamental thing we
want to do with our web server right
within the HTTP block we define a number
of virtual server blocks and the virtual
server blocks essentially will tell us
how to respond to requests so a virtual
server block is basically a listen
directive listen on this port and a
server name on this particular object
adjust to this host okay so I get a
request in to address to a host and a
port on that host and I listen to those
requests okay and within that server
block and then we have another number of
location blocks so the location block
says after that portion of the URI there
may be additional parts of the URI and I
can use those two routes essentially the
incoming HTTP request to a given
provider so maybe I just serve that
big file I'm subbing straight off disk
but I may be passing that essentially
violation to be proxy to your
application or far CGI to an application
to actually build instead okay
locations can be internal or external so
external just means this is a URI coming
into the nginx from basically the
outside and the internal means I've
actually done some rewriting in tonally
and nginx and now created basically a
mapping to an internal location which
tells you how to handle it
but see example that is the second okay
so this is a typical HTTP block you can
see that I'm saying effectively that I'm
going to I've got somewhat I got some
things here to say I want to optimize
how TCP works the TCP is a protocol is
designed for things that other than HTTP
and so in this case we just say actually
we want to really basically modify the
way we're listening to make sure that
we're listening in a way that's
effective for HTTP not for SMTP cetera
okay and then I'm saying effectively
here I want to listen on port 80 I'm on
localhost because nothing has to find
John 17 and I'm a double doors aren't
become something that the example combo
18 and this is the sake I'm at the root
location and the root location I'm going
to use this thing called try files it's
actually rewrite directive and it says
try things in this order first of all
just try serving that address off the
disk is a static file I'll try something
off the disk basically with a slash
after it's at a default it says she bit
further down right or send them to thing
called Kestrel weather and kessaris and
then another location are defined in
that location essentially I'm saying
proxy to pass this through to basically
app 5,000 writes basically is my Kestrel
hosted s Bennett not Cora right the
request comes in and port 84 mm I'll
become I look in silicon serve off disk
he's a maintenance page here as well
right so it's quite common thing is to
put slash maintenance or HTML that
exists you go to the maintenance page if
not it doesn't exist you get you keep
going down the line
right so you can put easy to put up my
mind to this patient say I'm under
maintenance before you get to it I think
I'll show you that later on and then
essentially a passing up the proxy
all right
let's have a look at reverse proxy
shutdown before I get confused reverse
proxy
let's fix that problem when I've altered
something we can easily fix that right
what are you thinking you're going to
overwrite to be announced directory
somewhere mm-hmm
let's just kill that for a second
Jesse did all the cashing in it reminds
me right it's happy again while we talk
about that let's just build this yeah
I've got this one open that's why so
just complaining I think because I had
that open out of locked it wasn't
killing it for me so I'm just gonna
rebuild this server so why it's just
shutting down so here we have a basic CV
docker compose file again we've got a
reverse proxy here you can see you are
basically saying to nginx basically
exposed 80 ATM did expose we mentioned
that kind of earlier and we're saying
and a push through here
and and the nginx config file I've got a
lot more in okay
so we talked about this earlier what I'm
doing here essentially is saying I want
to optimize House and Senate file works
in UNIX so I want to talk to my son the
TCP for HTTP this is saying I want to
limit the number of requests that I am
receiving from any one IP address in our
window of time and I'm limiting some
stuff about how much space you can use
to store that history of requests and I
can also basically allow forgiven to
place you forgiving bursts of activity
right that's just the example of the
kind of thing you can get out of a proxy
like this and doing compression and
compression for all those given types
I'm turning seven tokens off so you
can't see the version it's really hard
to stop nginx reporting that it is nginx
but the best you can do is turn the cert
and turn the version officer people
can't exploit given version based
exploits I'm doing some basic security
staff in terms of the headers I'm
adjusting some of the defaults around
the keeper lives and the collections and
I'm defining basically here a proxy and
I'll come back and talk about proxies in
a second mister saw me having to
basically switch things but the key
thing to understand down here is I say
right listen basically on port 80 you're
all requests than the words once at the
root location so there's an order which
we match locations it's worth reading up
on nginx I'm not there trying to run
through that with you today but
an order in which location blocks are
matched given that they're effectively
could overlap and there there's there
are given directives you can in location
blocks you can use things like regular
expressions you can use things like
equals to say exactly match this you'll
want to go away and have a look at the
documentation and nginx around that to
understand both what your options are in
terms of matching and how it figures out
when there are conflicts which one to
choose okay then this is try files line
which basically says have a look in
citizen maintenance HTML if so serve
that if not see if you can basically
serve this request by looking up a
location on disk if you can't pass it
through basically to this location
kestrel that's an internal basic
location internal location kestrel just
says okay pass it through basically to
greetings we have 5,000 so I'm just
effectively using the internal DNS
basically on the docker host to look up
greetings to a web and fine bye see
their port 5,000 and I'm setting a few
headers essentially on the proxy port
okay let me just build this
what we'll do is we just talk a little
bit about some of these cache headers
will come to torment and what I'm build
in there we'll talk about caching in a
second you can see here where the fourth
defining something called proxy cache
path so we're saying essentially cache
some stuff and put it here so any
annexed by default will cache on disk
for you right but you can do stuff to
basically offload caching this after way
seeds or things like um ready so
memcached and here we're saying set a
number of headers up how long the cache
should be valid proxy these up these
settings proxy revalidate all means you
should always essentially check with the
origin server if I turn that on with
saying if modified since header or if
none match header depending whether you
using kind of last modified @e tags to
the time whether the content is stale
minimum uses down here says only cache
after a certain number of hits you stay
Lehrer says in the event that the what
the origin server returns you basically
one of these errors like a 500 just
serve results out of the cache very very
useful and background update says when I
get a requesting and the cache is kind
of basically invalid then serve the
stale results and then go to the origin
server and refresh so this first person
gets the stare result subsequent
requests get refreshed in the background
and lock on says if I receive a whole
burst of traffic asking for the same
page which is cashable then what I want
you to do is to lock take that the first
person go to the origin server first
request bring back by see the cached
result serve it out basically and then
everybody else then hits the cache and
serves the cached result that stops you
otherwise continually revalidating your
the cache and say everyone now a whole
burst says I'm gonna get it replaced one
in the cache I'm gonna get a place to
run the cache that doesn't work so what
you want to do is it use the lock on
okay right let me do this stuck a
composer so we're gonna bring up docket
compose at this point okay
so the most basic use of this we kind of
showed you earlier which is around the
idea effects when you're saying well I
want to basically hit 88 year other than
5,000 oh that's no longer exposed we
just reminds you of that and then we're
talking a little bit about caching so
this essentially is going away
effectively and hitting our endpoint
that gets us multiple greetings can you
see those the sunlight killing you guys
okay so you can see here I've got a
header back and the header basis you've
got a lot more information in it now
it's talking about basically things like
cache control price protections that we
identified and we're selling everything
back but you can see we're hitting 188
okay so you know jump away for that
slide for a second how comfortable are
you all about HTTP caching who thinks
they know law about it who thinks they
don't know much about it comfortable
okay and we'll do some slides on this
way okay
and then wish I had that reverse caching
stuff works
right so intense it's often described as
a reverse proxy what does that mean that
means it sits in front of your server as
opposed to a forward proxy that sits
essentially say in the enterprise and
sits between clients and the enterprise
in the outside world right in most cases
effectively it disguises effectively
where those requests originally came
from but also has mainly used for things
like caching so you're for proxy server
let's your administrator cache the BBC's
website so that essentially you're not
continually using bandwidth when
everyone at lunchtime decides to have a
look at the news right the reverse proxy
server lets you cache basically your
responses from your application so that
you're not constantly hitting the origin
server and you can then effectively
scale more easily right the thing to
understand about if your caching is
essentially the way it really works is
you cast downstream whether your
application server and a lot of cases
you'll see things like a speed on that
core having middleware that talks about
doing caching in the middleware right
and it's useful essentially in your
application to cache things like I don't
want to walk to the database etc but the
way that the web sky Atlas is to cache
basically using proxies or your browse
that you cast downstream from the origin
server and what we're doing is we're
trading performance for the risk of
becoming stale right so we say I'm gonna
get a faster response if it's in your
forward proxy server or my browser it's
much closer to me and the latency is
lower right if it's in my online the
providers basically site it's
essentially provide pred to be having to
go to the database acceptor effectively
so the latency is still okay well
trading opt for consistency in some
cases essentially we may find that
basically we are have more availability
through using a cache in other words we
can get some results even event of an
error but we may be inconsistent may be
serving stale results the browser cache
is very low latency because it's where I
am so nothing basically that's cached in
my browser makes me look very responsive
but it's not shared so everybody has a
copy okay and a reverse proxy cache is
shared because essentially everyone that
goes to that proxy can effectively use
the cached version so typical kind of
thing we used in caching
saij so what we're saying here is when
we make a browser request we use a cache
control header with a max age of 30 and
we say essentially well look at the
response initially we say max age of 30
on the get response right and so what
we're saying is this now lives with
basically 30 seconds when you request it
from the browser in future you can skip
going to the origin server provided it
was in 30 seconds the last window that
you got that right so the first request
says put in the browser for 30 seconds
this essentially responds and then the
next 30 seconds if you do another get
request just surf within the browser
it's so important that it expires and
what then happens you didn't necessarily
have to go all the way to the origin
server and get a new version it can ask
the origin server why I basically have
modified since or if none match if it's
still valid and continue to extend that
time slice and serve it expires is very
similar to max age rather than giving
your window it just says there's a
specific top point in time when it's
going to actually expire last modified
so what happens is this effectively when
you get response from the server it says
this is the last modified date otherwise
this is basically when it was created
and you can then essentially pass across
a if modified since from the browser
that says this is the version I have
traction on this date do you have a
later version if the proxy and the proxy
server can say there's no later version
basically right so if actually you have
the current correct version you can also
force revalidation so you can tell the
proxy server you to ask the origin
server if the version you've got it's
still current and what happens is you
can say with a last modified date across
the what if I'd sent star to the origin
server if you support it in your code
base hey is this still valid and you can
return a 304 which basically says it's
not modified to the proxy cache is to
say you can serve this version back out
to customers right this is where the web
scales and that's the way that the web
coats for temporal coupler and temporal
coupling is the notion that it needs to
be up in order to basically serve
requests but actually if I can hear a
proxy I can offload that right we can
always use the cache control header for
privacy so public means your proxies can
store it private means only the browser
can store it sensitive information right
I don't want anyone store it in between
no store means nobody can
a copy right please don't keep it it's
too sensitive even in the browser
because someone might get onto that
machine and basically looking at it
looking the cash no cash no cash is
normal people think no cash is not
intended to be no one gets a copy
no cash is intended to be that you must
revalidate the copy you have with the
origin server virus modified since or if
not match unfortunately no cash has been
badly implemented by developers and so
no cash has actually come in most cases
to mean the same thing as no store so
most proxies and browsers now don't keep
a copy if you say no cash so if you want
the behavior that no cash used to have
you start using things that mastery
validate instead right just they we as
developers Google got all got that wrong
this is a typical basically property
cash statement so I'm saying at the top
basically an engine X define my proxy
cash and I'm saying further down
essentially what I want you to do is
property cash to my cash and I've turned
them by our settings I want you to
revalidate with a server minimum number
of uses surf style surf stale pages for
errors update your cache in the
background and only the first person in
a whole burst of traffic goes to the
origin server to fill the cache okay let
me show you caching so this version
essentially has caching in it so if I do
this what I can see you want to do that
particular response is a whole load of
IDs for these particular greetings so if
I grab that and go to here so this is
request a specific ID and I'm all
fingers and thumbs that isn't really
what I wanted is it
driving a lectin with a trackpad is not
the best skill in the world that I own
come on in a minute I'll invite one of
you have to drive for me okay great so
oops
so again sorry yeah oh yeah thank you
very much okay so I can send a request
for that greeting to the back end and
say give me that one specifically okay
there we go so basically what's happened
is this has gone over to the browser can
you see there's a head of there which
basically I've asked the proxy cats to
give us and say did you conserve this
from the cache or not and first round
time around
obviously it's saying miss it's saying
that wasn't in the cache I went to the
origin server to get it you can also see
over here in the output basically from
docker that effectively I've gone to the
origin server okay next time around when
I request it because I've asked it to
cache and I'll show you how we do that a
second it's going to change that status
to hit so this time around it's said
that directly out of the cache rather
than going to my application server and
you can see probably those of you that
it's at right at the bottom the line
unfortunately but you can see it's just
calling basically straight to the proxy
those you can actually see that line at
the bottom from docker all you have to
do to really enable that is on your
controller in within let's just shut
that down that life and a better way for
you
right so you can see here that
effectively there is a response cache
attributes basically placed on that get
this one says never because to get all
of them so that will keep updating and
I'm not doing any kind of cache
invalidation and this one and get
greeting says basically what I want you
to do is cash with a given profile we
set up the profiles in this file and you
can see effectively and it's basically
built into Web API
you can see I'm setting up basically one
that says hold this basically for sixty
seconds and one that's essentially
saying don't hold it at all and with
that on essentially I will return the
correct cache control headers when I'm
cold which are picked up by nginx which
then essentially will basically serve
those results from the cache that we
just saw okay make sense slides whatever
time great okay
let me show you a little bouncing
okay so the other thing which we showed
we kind of leave it at the beginning was
well I may want to do essentially is
behind my proxy not just have the one
application server but the one instance
of my application what I may want to do
is have multiple instances of my
application and so that's where
effectively we come to this upstream
directive so we defined by see this
block called upstream and an upstream we
basically put either the IP address
listing them one by one of every app of
every application server that we have or
essentially were using something that
can be picked up by basically domain
name servers to say okay go away and
find me basically that particular app
server and I will then push request to
that collection map service and you can
generally it defaults to round-robin
but you can use a vote you can set a
variety of ways in which nginx will
basically talk to that pool so you can
say things like basically lease
connection etc okay I won't show you
today but you can basically use health
checks there as well to determine which
servers basically are viable just about
to load balance across T okay let me
just stop this one okay and I'm going to
move to use this one I think so I can
show you that okay so in its nginx calm
file you're gonna find essentially you
don't I need to rebuild that you just do
I've got a little trick here so the
resolver here and the set upstream so
resolvers basically saying to engineers
well where should you go to find the
names you don't have to do this all the
time but this is a trick for Dhaka and
this essentially the address refers to
basically Dockers internal domain name
server that it's built basically to use
of that network and what this means is
that as you add new containers in
because I've got a variable that gets
evaluated at run time set upstream here
it will always go to the resolver and
say well what service do you have
available in order to resolve that
request okay otherwise you may find that
docker where were some doctor winners
and doctor for Mac when you add a new
container it automatically knows you
don't to do this but he's a docker
toolbox
to do this because it doesn't pick up
the fact the better than you container
otherwise alternatively you can just use
an upstream group to do this as you saw
in the slide deck okay now what I'm
gonna do is do docker compose app but
I'm going to add a scale parameter so
now docker compose I want three
instances on my web app hang on the site
they will clear this screen so you guys
can see that at the back
so Saint bring up three inches of my web
server when I built that what I'm gonna
do is then run the same sonars been
running all along basically to follow
HTTP but what we're gonna do is see
which web servers handle those requests
well I just seen that to run because
running close towards end of time all
right so a couple things I haven't I
haven't shown you you can do your
writing I don't have time to show you
yet show you that today essentially it's
a regular expression based and
essentially you can say I'm going to
take this location when I get the result
into this match send it to basically
this address instead right so you saw
those we had external blocks and
internal blocks from the location so if
I match location essentially with
external block I can then say well
they're sent to this internal address in
other words effectively rewriting that
URL to be posted elsewhere okay so if
you direct expressions to pick out parts
of that given URI and repass them
through to another uri health checks we
haven't talked about monitoring is the
other thing you can do a lot to actually
monitor and the debug locks are pretty
comprehensive and we turn them on
they're very handy particularly for
doing things like URL rewriting and you
want to understand why isn't it working
it will tell you basically what process
is using to try and match the URL you've
generated against stuff okay this is
inordinately slow for some reason that I
don't understand which is worrying what
are you being so slow that time alright
let me just show you a couple of slides
while that filters through I don't think
I have time to do SSL so surly
straightforward you just basically
generate certificate I need to tell
basically nginx where that certificate
lives and it will do as a cell for you
right it's very straightforward you just
had enter a reverse proxy I do not know
why this is taking so long to bring up
three web servers it never takes this
long
he said the demo gods have decided to
hate me well there we go
okay great so you can see I've got some
color coding there and I've got
basically at least two web servers if
seem to have appeared
no I happens web server number three
let's figure that out later when I send
this request what you'll see is
essentially that was served by web 2 and
then if we do it again a few times
hopefully what we'll see is that
eventually we said enough times to see
different yeah so you can see in the
color coding different web servers are
now responding to that request so have
essentially load-balanced across that
collection of web service that we set up
all right well I'll also just show you
is if you're not on docker toolbox
the simplest syntax for this is just to
use this upstream group right so it
figures out that basically web is saying
look in the DNS for something called web
so there'll be a greetings where one or
get to it that's how they see docker
crates them and it will find one of
those round-robin to basically on port
5000 all right I have two slides more to
show you I am probably slightly over
time apologies right so I'm fine to me
I've got some other but there's some
bits of demos like if you if you're
desperate to see it come and follow me
and I will sit down at table together
and I'll show you things like I've got
an SSL demo and are you are re writing
down where you can see so that's really
important to you just find me during the
day and we'll sit down at a table
together and our charity or questions or
things didn't show you today we didn't
share the build from source that's
something you're probably gonna want to
do when you want to harden the nginx
using fuel production servers we didn't
show you health checks which are useful
essentially where things like load
balancing you effectively define a
health check endpoint or a page
essentially for your app server and say
talk to this to determine whether or not
effectively that that web server is
viable for you read to it
we showed you what every talkback
include speak to break up the
configuration file take a whole section
like for example the gzip compression
section put it in a gzip confident just
to include gzip comp in your master
helps clean your files up make them
easier to read and it lets you reuse
standard configuration elements in your
organization we didn't show you passing
through requests to register memcached
but you can do that you can essentially
say serve this thing at our Edison
memcached right and we didn't show you
how to do monitoring it comes with a lot
of options monitoring and there's also
basically commercials and go to nginx
Plus which is the nginx incorporate to
do and they have our whole range of
commercial tools built-in type of nginx
for things we have talking about
monitoring and also using this in a
micro services environment as a proxy
for a micro cells which is nginx a human
I think all right thank you don't forget
to vote on the way out feedback is very
helpful thank you for coming and I
appreciate you can tell you came to see
me and not Scott building Q medicine
Raspberry Pi I thought might have nobody
hopefully that gave you some idea of why
you what nginx does how you use it why
you might want to it's a there's a bit
of a learning curve to it start simple
right just start doing by saying here's
my website what I want to do is stick a
proxy just to pass straight through to
it in front and then I would and then
build up more and more at a time that's
the way I would do it because you will
find things go wrong and investigate how
you basically which you saw the line
once but you pretty much don't remember
how you put in an error log with debug
on it and you can start a debug version
docker comes with a pre-built nginx
debug you can use which will let you
essentially build debug output and that
helps you trace things that you are
already writing and get your
configuration file right it doesn't come
with an engine it's debug if you want
Energex debug deployed actually to you
know an on docker environment you have
to build it yourself unfortunately okay
thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>