<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Top Ten Worst Repositories to host on GitHub - Carlos Martín Nieto | Coder Coacher - Coaching Coders</title><meta content="Top Ten Worst Repositories to host on GitHub - Carlos Martín Nieto - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Top Ten Worst Repositories to host on GitHub - Carlos Martín Nieto</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c3Ciaz5X8FE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hello everyone I'm Carlos I
work at github and today we're gonna go
on a tour of a few educators that and
unusual situations that happen when you
provide kid hosting to the world at
large so just in case a bit about github
with the repository in the world with
over 67 million projects and 30 million
users right now and with this numbers
you can you can be sure that we see our
share of situations that we didn't
expect when we initially wrote the
budgeting services so I work in the kid
infrastructure team we're the ones
providing the gate service both to
internal and external users so that's
the website the API any github pages and
in general customers and also any
external customers such as any of you
that runs git clone or even if you run
subversion checkout we also are in
charge of that one we're also handle
we're also the ones handling the support
escalation for any technical issues or
questions about key performance or don't
come performance that kind of thing that
that all gets back to us and then we
also provide input on technical aspects
in support replies so given how popular
the website is we run into a fair share
of unexpected use cases and people
having you know ideas about what service
we're actually trying to provide
sometimes it's just not obvious at first
glance that something is actually going
to cause an issue because like I said
people make assumptions about how a
service operates and sometimes it's
that's not quite correct
now I'm gonna name a few reports during
this talk there there's the ones that
are public so necessary when I have too
much on them specifically these are some
of the ones we have to intervene on and
definitely not the only ones that are
behaving in an expected ways and most of
them aren t been doing anything
particularly that we call inappropriate
it's just that they we didn't expect
them when we only should build some of
these services and you know sometimes
the the solution to some of these issues
is just social or educational right
where we just have to tell the user okay
well this is how git actually works or
this is how we build github with these
assumptions in mind and you're breaking
them so let's let's figure out a way not
to do that
so let's start with our own repository
get up get up we deploy to about 600
machines only to deploy and we deploy
maybe a hundred times a day in a typical
day and this is one that we're very
proud of this is something we've spoken
about publicly quite a bit and we
definitely want to keep enabling the
dynamism that such a deploy rate allows
us but as a machine account increases
that puts more load on the servers that
are hosting the github github and now
we're using kubernetes so the number is
fast here but this one hasn't become any
any smaller so this kind of issue where
everyone tries to fetch at the same time
this is what you call a thundering
health issue in systems so let's see
well all of the machines that are the
last deploy and they all want to update
to the new commit so they all need the
same objects here is where get due to
several technical reasons can't
guarantee that it's always going to
produce the same data for the same
requests so by default you can really
cache responses but we so we had to
build our own little tool that sits in
the middle called lariat because it
deals with hurts and when we detect
multiple identical requests coming in
like the second or third one will will
actually detect that it's concurrent we
can then cache that response in a way
that we know that we can actually replay
that one so this allows us to only
create two or three times this back file
and then we were able to cache it and
then send it out from disk instead of
having to ask it every single time for a
four-for-one and this allows us to
actually perform the deploy fairly
quickly and service aren't overloaded
anymore and this is something that we
also could give the customers in the
on-prem version so that because this is
also very typical in an internal tool
someone in somewhere in the in a big
organization also tries to deploy all
constantly
we see it with a table us our customers
also see them with their clients so this
is something that we're we're happy that
we can provide to anyone even if they're
running get up on their own machines so
this the second example is the
kubernetes k8s cube however you
pronounce it this is a project that
makes very extensive use of pull
requests and which is good right we like
pull requests but every time that you
update the base branch the any tab where
you have the pull request page open it
will notice this and then it will ask
the server hey can I paint the merge
button green now and that you know from
every tab from every person that's
opening it and that's a lot of requests
and that requires a bit that we do a
test merge and a test rebase and it's
not easy to coordinate all of this
because they all have completely
different jobs with no notion that any
other job is actually happening so this
isn't so much of an issue most we are so
small so the actual merging processing
isn't that bad but because we run a
distributed system we need to reach
consensus on reference updates this is
where when you say three-phase commit
because it's proven it works
but it's a bit slow so the this is the
system we call spokes and as part of
that we are limited to maybe a couple of
reference updates a second when you
update the base branch and all of the
tabs ask at the same time that can be a
few hundred and that's when it starts
going wrong because we didn't build it
with with enough slack unfortunately to
actually deal with this
so the bottleneck becomes actually
creating the reference update so this is
the ref spool namespace that you might
have seen that ci uses this is something
that we're still working on so part of
the the issue part of the solution is
already there right so that's grouping
together whenever possible but also the
client knows that it's okay to retry
this is something where
we're also building more smarts into
into the code so that we can avoid
denying any updates to real users and to
be able to detect when it's a robot
doing the update and then we can tell
the robot okay try again later it didn't
work out this time because the robot
doesn't care and it's probably some
tapped someone opened an hour ago and
they've completely forgotten about it
anyway so the human is even looking at
it so the you know just the robots don't
care about them and then if you don't
care about robots the scalability issues
become a lot simpler usually so from
some there are sort of more of our our
issue with the increasing scalability of
the or scale of our customers here's
something be interesting so a user wrote
to us and usually these are people who
are too conscientious to actually have
any particular effect on our service
they're usually not much of issue but in
this case they were asking if it's okay
to have a github raffle in their
hackathon so they wanted to have the 300
people or so that we're going to the
hackathon to push to a particular
repository all at the same time and then
the winner of the raffle will be the
person who pushed last before a
particular timestamp now they asked us
if it was okay for them to do this we
tell them no for multiple reasons one of
which just being that keep time thumbs
are completely fake able so they won't
even know who won it and we have not we
don't have the capacity to deal with 300
pushes simultaneously we told them they
did anyway I think they changed a tiny
little detail about what was going on
there but yeah they just went ahead with
it and we saw this right so this is what
that looks like the red bar you can
barely see at the bottom that's that's
the usual alert level and this went way
beyond that so they try to do 7,000
pushes in an hour they fail obviously I
mean even if we didn't have our three
phase commit system it would this
there's no way we would work
so that's you know at least you know the
far from this being just a weird
situation this allows us to at least
know that hey this actually failed in
exactly the way we predicted and no one
else was affected right because all of
these requests rate and update rate this
depends on this hangs of a particular
repository so no one else not even any
fork from that repository would have
been affected by but by them not being
able to commit because they're basically
abusing the system so this give us a lot
of confidence that you know the system
we had built we knew how how it behaves
next up this name this is a repository
with a bunch of it's called who's on
first data venue us so this is about
this New York data it's not too big for
a repository holding data and they
actually even use multiple nested
directory so that it actually performs
well so all the trees and all of the
objects internally are kind of small so
it's not too heavy maintenance but
that's almost a million object optimism
your files in each commit and that adds
up because the way we we do updates via
the the web UI is the same way that get
does it what it does locally is reads in
all of the files in the into the index
updates one and then writes it all back
out now locally does it works mostly
fine just because you keep that file
around this is not something we can
actually do on the server side because
we don't we have thousands of users that
are acting on and everything so after a
while we're looking okay and figure out
what the difference was now here yeah
this is just on us so we made the code a
bit smarter right so we read only the
the affected paths this were about six
files in and to directories at the root
so like why are we and why are we also
reading everything else well that's how
git does it basically so we built a
bunch of code that you know is smarter
than just plain local git and this made
everything better for everyone so that
was a really great case to run into
because it just made us it let us make
it better for everyone who updates via
the API so this is both the the UI which
uses the same API as to update things
and also any changes via our API TOCOM
and the code is in the lipcott to
library which we're using to to create
this tree so anyone can actually use
this code if they are if they have a
similar need on their system right next
up we have one which you know it's
actually a bit big about 365 gigabytes
I found it as one of the largest
repositories in one of our partitions so
I wouldn't seem to investigate like well
what could possibly could you possibly
be doing that requires normal 400
gigabytes and as first I can tell turns
out mangog this website where people put
images think they're funny they used to
have a feed and then they changes to an
app that showed you ads so people got
angry about this and built their own
fields without ads now one of them was
this but this wasn't just the feed this
was also all of the pictures and videos
so this user was up pushing three to
four hundred megabytes every 10 minutes
every time to happen at the RSS feed now
the reach of they were forced pushing so
the reachable size for the repository
whenever you cloned it this wasn't this
was about you know half a gig a but
because we've we've built github with
the assumption that you actually care
about the data you're pushing to us more
than 10 minutes so we want links to
objects PRS all that kind of stuff to
continue working so we don't actually
delete objects this is how we get to 365
gigabytes of repository in this case the
solution is just to tell the user please
don't do that because this is just no
it's not
not useful to anybody so yeah that was
they stopped which is good right
I think they're still using us to host
the RSS feed but using ec2 or something
to to actually host the images so you
know it's nice when the user says okay
yeah we're doing something silly and I'm
gonna stop the next one you might have
seen in the in the news I see where this
is cocoapod specs so cocoapods is a
package management package manager for
Objective C and Swift and these
repositories where it stores the it
specs right that's where it shows the
manifests what report what packages
there are what the versions are how to
find them so you know they have a lot of
them and they update them quite quite
quite often so there's about 260
thousand commits which is about half of
the github repository and we'll be
working on it every day for you know the
last nine years so you know it's a lot
it's a multi-year so we're except they'd
reach it in just like one year or
something and they have across all about
175 thousand files now similarly to or
rather I mentioned before with the New
York data to actually have multiple sub
directories that are so hashing just
like you would in in a file system set
before as well in this case they didn't
they had a single directory or a tree
ask it has it with every single package
that they had so these were like you
know several thousand so every time
anybody updates any of the packages get
needs to rewrite anything leading up to
that tree which means the base tree with
all of the different projects it also
has to update it which is a huge for you
know tree stand that's huge files and
huge obvious the kid needs to repack
whenever we do maintenance and needs to
find used to infect them check for
compare against other big trees and see
if there's any differences to so that I
can see compressed data
this was failing maintenance constantly
and they the rate of commute is about a
thousand commits a week mostly from
robots as far as I can tell it just
meant that they were constantly failing
maintenance however how it really came
to tow attention properly was that every
cocoapods user whenever they run an
update so that they can update their
current they're sorry the local list of
what versions of what packages there are
they fetch from github now this is every
single user whenever they run an update
command and programmers love to have the
things the registry at least up-to-date
so this this happen quite a bit and we
have a monitoring system that actually
intercepts and says okay well you've
actually this repository has there been
fetching constantly for the last week so
I'm just going to stop it well not
completely stop it but it what we do is
delay fetches or a bunch of other
commands until the quota which is a
certain level so this they the uses of
cocoapods
complain to them and hey this takes
forever and so it's just times out so
then they they come to us and say hey
our users are unhappy so and that's when
we went go into investigate so there's a
couple of posts in this issue from
engineers in our team explaining how
they cocoapods had managed to almost
make every decision to make it harder
for us to host so they have this huge
trees users constantly fetching from it
and also they had decided to do shallow
clones so this is where you do get clone
- - depth one for example initially this
can actually help to reduce how much you
download however due to the way they
were merging the pull requests they had
and to do with the graph ended up they
were end ended up always fetching all of
the history anyway but because they had
done this depth option in the beginning
that meant that our lariat caching layer
was unable to cache because whenever you
all of this the shadow clone a lot more
computation has to happen server-side so
now not only could we not use the the
caching the actual fetch that was
happening was more expensive than it
would have been otherwise and to both
they weren't actually serve it saving
any space in their clients machines so
yeah this is a fun one so you know we
asked them to do the the fetches more
efficiently and to use smaller and
nested directors right this is just
called shouting this is what you do for
a file system as well right a file
system with a directory with thousands
of entries doesn't perform well it
doesn't either
because it's basically a representation
of a file system so I think it still it
took a while for everything to get you
know getting land because then you know
people had to update their clients and
then undo the the DEP thing but I mean
it's it's gonna ride however one of the
other things we did because that would
still been pretty pretty expensive
we also spread the load around all all
machines so we we have distribute this
this really case of each repository and
we generally read from just a single one
so that we can make use of the caching
in each machine so that the file system
is cached in RAM and then everything can
be a bit faster but that means that the
quotas are also applied on per machine
which means if if all of the cocoapods
users are going to the same machine they
fetch from the same machine right and
all of the quotas are being applied to
everyone from the same machine so we
spread it out currently they're on six
different machines that we read from
which means that we almost never
actually apply any quotas so I mean we
definitely decide not to like you know
banned them or anything because it was
well this it's it's a it's a big useful
project so we definitely wanted them to
you know and their users to be able to
keep using their cocoapods packages this
is where I start cheating a little bit
and at halves for honorable mentions
this is for a it's a couple of things
that have happened or reports that we've
discovered since
I last wrote the talk so this Hubble is
a security compliance stack it contains
configuration this repository in
particular contains configuration for
this however they had set up their
client similar to to cocoa pods to test
every minute whether to try to update
every minute write test whether there's
any any updates across all clients so
they had you know thirteen point seven
million features in a day that's three
percent of all fetches across all of
github so you know we were not amused
however the monitoring layer code days
so we were able to actually so so we
will to avoid the servers being actually
overloaded so you know they had similar
issues where there are clients would
just stall because the the way we we
enforce this quota is like I said we
stopped operations from happening for a
while until the quota is back edge and
to an acceptable level and then we don't
throw any more because then we want the
clients to have the the data as fast as
they can and then you know go away so we
kept you know throwing for a while and
then you know contacted the the owners
of the project and said hey what are you
doing can you maybe fetch less often so
they changed it to to think twice a day
and they're also looking moving to
something that's more that's actually a
CDN rather than us which is sort of
accidentally a CDN for anyone who wants
to push data to us we also did like with
cocoa pods move the replica to move the
reading to all six replicas so now we
barely enforce the the quotas and you
know this is what that looks like
you can hopefully tell without the arrow
when we spread the load around to the
six replicas so you know everyone's
happy for a while hopefully we don't
have to you know do this again for for
them
so here's
this next one's a bit of an interesting
case because no one's really trying to
do anything
and usually subtly very very helpful
right so this is Barry Clark jerkle now
this is a repository that you can fork
and then you have your own get a page a
site that's set up with Jekyll
a lot more than if you just create a new
repo locally so the issue here or one
issue here is that each fork is
someone's website which means that they
all have different contents and we've
built github and git itself is built in
the with the assumption that you know
when you fork that's gonna be the same
project so most things are going to be
the same most files are going to have
the same content and then you know
someone's going to do a small change and
then you know maybe people request or
not but it's going to be all very
similar this is quite the opposite from
the point that they fork everything is
completely different this means that
it's it's more expensive to store it's
more expensive to perform a chrome for
each of these repositories but it's not
too bad I mean if it's fine as long as
you you clone every once in a while you
know whatever some updates we clone it
through our github pages workers and
then render it however we found that
there were a group of servers that
attempted to clone all of them at the
same time because we serve forks all
from the same machine for the caching
reasons I mentioned earlier it got a bit
busy right so this is our 32 core
machine with a load average of 400 and
that was they because of the particular
way they had done of this cloning they
had actually managed to bypass the
quotas a little bit and because we know
we were already past the applying quotas
we let all of them run run wild and try
and get as quickly tried to get done
with the clone as quick as possible now
in this case there were so many that we
could barely do any progress and here
well this we did we saw this is a small
group of IPs
in the same data centers
we figure maybe this is no actually a
user that's really interesting in all of
these people's blogs so we just banned
the IP addresses and everything when
when well in immediately like okay so
yeah we've actually been playing around
with expanding the scope of the quotas
from from the monitoring but this is
something that happens very very rarely
and it has no knock-on effect on other
networks where really there are actually
multiple very active Forex adrenals
colonel for example so we just figured
well sometimes you just have to burn a
few a piece and move on so that's what
we did this next one was an interesting
interaction so we had this repository
that we're pushing multiple times a
minute which is fine we can deal with
that on the reference update side
however that means that we were
accumulating small chunks of changes so
we want to run maintenance on this so
that we can package everything up
together all right this is the same
thing that gets us when you fetch a lot
and after a few commands get would say
oh hey there's too many objects here I'm
gonna run a garbage collection run we do
kind of the same on the server just a
lot more controlled there's a lot more
data now this meant but so this
repository because they were constantly
pushing like all the time for days
straight
it means that whenever by the time we
finished maintenance there were already
too many incremental updates that so
that we had to run maintenance all over
again and to make it worse one of the
Machine one of the replicas fell fell
behind and that one could never catch up
because by the time we managed to sync
everything over to the new machine again
we had already accumulated so much data
that they could just never keep up so we
and especially with with the name
hey CI logs okay what are you doing here
so we email the the unearthed repository
asking them hey were you doing like
maybe could you not they they come back
and say well yeah we tried this
experiment like 6 days ago we turn it
off mmm are you sure about that can you
doubt like double check and they're like
yeah sure enough this was like a an
academic IPA range so we figure it was
some some grad student that had not been
careful enough when they thought they
shut down their experiment yeah so you
know once they I double checked turns
out they had a process pushing to github
it's also empty Kermit was also extra
weird and then that process was for some
reason spawning other processes which
were also pushing and also spawning
other processes so at one point it just
reached like you know hundreds of of
processes all trying to push at the same
time to get up or a bunch of empty
commits I don't know Academy experiments
I guess not sure this big risk there's a
big lesson to you know get out of this
just you know make sure you turn off
your things and speaking of commit rates
from academic projects this tool is from
a university group as well believe in
Canada we found they were pushing about
four commits a minute it's not a
problematic rate but you have to wonder
what they're actually doing
alright so discover this I was I was
looking through backup blokes and I said
okay turns out every hour when trying to
run some garbage collection there's like
over 200 incremental updates from this
right which means 200 commits from this
particular repository what are they
doing so you know I go look in like okay
this is like a weird contents and you
know there are two accumulate 110,000
commits which is multiple years of an
actual software development and yeah
this is one of those cases where you try
locally to use git as a database and it
works great because you have you know
five things and then you try and use it
in production and it breaks down
unfortunately this time we broke down on
our servers instead of tears so they had
no idea
you know that is actually causing people
to have to run maintenance manually and
actually you know distract him from
actually trying to to you know make the
system bearings instead of trying to
deal with this so as far as I can tell
that what they were actually trying to
do a workflow cue essentially a job
queue and this was slow of the metadata
so instead of using a database like hey
it helps kind of like a free database
for us so you know we email them and hey
you know it's cool if you want to do
this but like not on our servers and
please stop recommending on the readme
for the tool that you usually don't get
hub because that's not gonna go well so
you know they're okay fine sorry and
then you know they stopped which is good
and I think hopefully they can actually
they're now using natural database with
database semantics instead of trying to
you know to push get into being a
database right this is not the same
issue that that happens with cocoa pods
and something else we'll see later on
you know so this was finally said okay
you're doing something silly and they
stopped so it's fine so here I have a
standing for a bunch of like any large
network so for example surface index is
about one gigabyte to clone roughly it
has 18,000 Forks as you might guess we
don't use 18 18 terabytes on it because
we just ran out of this space if we did
for every popular fork so network report
so say network I mean repository network
this is whenever you fork all of these
Forks are called reported net work and
then what we do there is to duplicate
the the actual contents of the data so
this is using fifty five gigabytes the
reason it's not you know just just over
one is that the people do have folks and
actually have so there are companies
starting still having their own thing
they fork from turbos Linux and actually
apply their changes on this to their
Fork of the Linux kernel so which is how
it's supposed to work right but this
means that yeah we're usually about
fifty five gigabytes to store multiple
copies of the one gigabyte
now yeah some of this is just internal
bookkeeping that we're also trying to
improve we keep like lists of all of the
references from all forks which will
seal it now in IntelliJ how that's a bit
of an issue so IntelliJ is about three
gigabytes one point eight thousand Forks
but it's about the same size now the
report itself is bigger but it's not you
know enough to make up for a difference
now the interesting thing about IntelliJ
is that they really love tags they tag
everything so they put a the the main
forked a isn't just brings a community
that one has 52,000 tags and now when
you fork a repo we copy our of attacks
well that's a good idea is a different
discussion but that's what we do which
means that for the one point 8,000 Forks
were doing bookkeeping for one point
eight thousand times for two thousand
times so we have a master list of all of
the references here that's about seven
gigabytes out of the five gigabyte 55
gigabyte sighs now get being get this in
order to fork get needs to read this
file into memory multiple times which
meant that earlier earlier this year it
actually became impossible to fork
IntelliJ and so we apply the small
optimization and this is only we're
still working on because it's still way
too big and maintenance is still way too
expensive in this repository and
something to really show how I mean it's
still more efficient but still not as
the features a bit like how they this
this bookkeeping that we do on the whole
fucking thing so the octocat spoon knife
that's if you've done any training with
us you might know about it it's what is
the one we use to teach forking pull
requests so you know this one has ninety
three thousand Forks this is also an
issue on the on the website I think if
it shows just five thousand plus per
requests because it's too expensive to
just even count them and the database
and yeah so you can see this is a
hundred sixty
k5m if think is a single file or 260k if
you clone it we're spending men
gigabytes of it for it in its own just
for the network or anything this is just
well we have a lot of Forks which means
we have a lot of different changes real
change that people have made all of the
pull requests have extra bookkeeping
costs and here is just something you
always have to be on the lookout for
small t mutations because they when you
run at the scale of all these files I
mean gigabytes maybe let's write it a
little less that that immediately helps
a lot so my last example here is an
interesting one it brings up this sort
of Oh weird use cases and things you
definitely didn't envision this one
where we have three hundred forty five
thousand files overall or in each commit
I mean right 150,000 commits in a few
weeks and 26 26 thousand individual
authors in their repository so this is
weird right there's like 26,000 people
for a small I mean say small hello first
but it's it's not a big it's not know so
I'm like IntelliJ where all you have a
lot of or the current where you have a
lot of people contributing to it right
this is an unknown thing so turns out
this is a repository for a companion app
to a programming learning website and
they were in repository for their users
to read about what they'd learned so
this is like a like a blogging thing
like it's tiny thing right but what they
were trying to do is is commit in each
user's name so that it would actually
appear on their contribution graph right
that's the different shapes of green
boxes that we shown the profile and
their idea was like well if they if they
if they block with their app and then
they push it up to our servers that will
count them as their contribution and
then that means that they'll actually
get a green square and those stay
committed
right so the commits here for
can tell refers to being committed to to
learning
now everything putting everything the
same place has some of the similar
situations that I mentioned before where
maintenance is really really expensive
and this also meant that we had for the
contributions were also very expensive
because there were a lot of contributors
in the same repository constantly
obtaining again they had 26,000
contributors so they have 26,000 entries
in a particular directory
that we have to keep updating so other
things we've already seen before and
here like ok maybe you want to teach
people about programming you want them
to commit but this isn't this is really
not not the way to do it like please
don't do this we're we're happy to host
their blog or whatever if if they want
to have it right like I mean their app
can even create the reporter for them
and commit on their behalf they just
need to authorize it right but we're
this is so beyond what we're what we
expect people to do this is not
something that we're and there's a bit
of way right so this is not something
where this is my use case that we're
gonna be supporting here they also got
to write a blog post about how they
broke it up which I mean fair enough it
they broke their bit of github I mean
thankfully we're still after that it
didn't actually affect the other
repositories so just a bit of a
conclusion at the end what do we what do
we take from all of these right well
first off when you're running this this
kind of service for everyone your users
are going to surprise you and that's ok
right part of the job is to learn from
what users are trying to do with your
service right they they come in they
learn about what you do and then the
deterrent push the edges that's ok and
the job is to learn from that and try to
make it work as long as it's within
reason some of these use cases were just
not a natural extension to what get up
the collaboration side does and
sometimes you know just reach out to the
user ask them what are you doing why are
you doing this and you know a few places
okay well we work with you to make it
that actually behaves the way that
github that github actually behaves so
that you actually make use of get up in
a proper way and we're not you know
having to wake up three and to fix your
repository now yeah there's always
something new and you're going to be
learning a lot if you just let people
give you data and just push to you thank
you
right repeat the question for them okay
so it's about the custom fixes whether
wish we dad's taking all that to us
some of them do some of them are quick
fixes a lot of this stuff however is
working with upstream get and you know
so Larry what we're trying to fix up the
the patches make them prettier for
upstream so that we can actually send
them for example the thing for the
process that creates the pack files for
you when you clone there's a few
performance improvements we have that
we're slowly you know sending back
upstream there's the thing with the New
York data that you know make it more
efficient to create new trees out of old
ones that stuff that's upstream now and
you get to and you can do it now if you
have that nning you can you can use the
library to do that kind of thing
sometimes it's just how we run things
like the whole network thing it's it's
just how we do things so it's not it's
not adding anything it's not it's not
adding any other versions from upstream
because it's just what we do
and I think so and because we have this
seven-year but foul and you know Google
also has these issues were in working
with them to provide to have a new
database format essentially for for the
references so that we can you know be a
lot more efficient server-side and
there's also some patches that will
apply even to the current system for
everyone like like the original idea was
to M map so to directly read the files
from disk where we store all these
references that never actually happened
so what kit does is really doll into
memory and then pass it so we're
creating someone else in my team is
creating patches to actually run a map
on that file so that we can actually
read straight from disk and only the
bits that we actually need and that
should be making it upstream at some
point soon hopefully after we've tested
out on our machines yeah yeah so the
questions about the 9gag repo where it's
three hundred and sixty five gigabytes
whether that's about the binary size
yeah
so that was because of all the this this
user was uploading it was pushing to us
all of the files or all of the images
all of the videos so those add up and
because we don't actually eat them when
you first portion overwrite the history
so where it just keeps accumulating and
that's yeah that's how it gets to three
hundred I mean I discovered think last
night another hundred odd gigabyte repo
with a name like backup something
something so like it's private so I
can't look into what's actually inside
but I email the user asking hey what's
up yes the questions about these large
files and whether that's why LFS exists
yes one of the the things we keep
hearing about when we show up to like
games conferences is like developers are
like yeah we love gates but like we have
gigabytes of you know just designs that
the like models pictures you know that's
you know things you have in a game so it
looks pretty so and that's basically how
it came about say okay well now we're
gonna have a tiny file and then push it
up and then the the real file actually
exists somewhere else that's one of the
main reasons for the LFS is the the need
to have large files in in some trees
that you don't necessarily want to put
in to get
well if there's no more questions then
I'll take my leave</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>