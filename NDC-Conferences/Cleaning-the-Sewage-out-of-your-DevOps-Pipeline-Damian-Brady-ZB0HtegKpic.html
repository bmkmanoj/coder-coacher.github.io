<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cleaning the Sewage out of your DevOps Pipeline - Damian Brady | Coder Coacher - Coaching Coders</title><meta content="Cleaning the Sewage out of your DevOps Pipeline - Damian Brady - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cleaning the Sewage out of your DevOps Pipeline - Damian Brady</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZB0HtegKpic" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for thanks for coming along how's
the morning been so far I haven't seen
anything it hasn't been good awesome
that's silence I'm going to assume that
that means it was amazing so welcome
today I'm going to be talking about
cleaning the sewage out of your DevOps
process I'll talk about what that means
in a sec but um some of you have
probably noticed I talk a bit funny I
thought I'd start with telling you who I
am so my name is Damian Brady I actually
work for Microsoft now as of literally
this time last week I've been home for
two days since this time last week so I
haven't really got everything set up so
I'm really hoping that this laptops not
going to lock me out at some point so
don't have a bad yet
I am a cloud developer advocate at
Microsoft and part of Donovan Browns
self-styled League of Extraordinary
cloud DevOps advocates which he seems
determined to to see if I can catch on
so we'll see how that goes so I'm that
guy who looks a little bit like a
hologram because that photo wasn't very
good we're going to redo this photo at
some point but that's me prior to
Microsoft and most of you would know I
work at octopus deploy if you know me I
used to work it up to us deploy so prior
to that I started about 15 years ago
with software development and I started
in government so I know how the
government system kind of works at least
in Australia I moved on worked in a
couple of different finance companies
building net solutions for internet
banking and things like that and then
some loans software and things like that
I've moved from there into consulting so
I ended up visiting a lot of places and
finding out how they did their DevOps
process although it wasn't called that
then it was just the process and then I
moved on to octopus deploy and so as
well as being in a product company
I also saw how people did their
deployments and things like that so I've
kind of got this varied history in a lot
of different environments where people
have done things very very differently
especially in the case of government
versus octopus deploy where we could
deploy a new version to production about
an hour with a couple of lines of text
in slack so yeah it's changed a bit I
originally come from Australia around
not too far away from here this was a
little bit early on this time last year
so that's winter in Australia by the way
it was it was about 30-something degrees
I think so not very nice so I come from
around this part of the world
there's a lot of other speakers from
about the same place actually Troy hunt
leaves about 45 minutes south of where I
was but Christmas Eve last year my wife
and I actually moved to Toronto which
was a bit of a shock we kind of
exchanged Vegemite and dropped bears for
maple syrup and real bears I think
that's a trade up I guess the big
difference though was this is Australia
in summer which is around Christmas and
this is Canada around Christmas right
this was the first time that my wife had
ever seen snow I know you have snow here
but we had never seen it before really
so that was a bit of a shock we are
heading back home at the end of the year
so I'm kind of looking forward to
getting back to the beach as well yeah
so that'll be exciting when we get back
you let the run twice anyway so today's
talk is really all about the scenario
where there's a lot of organizations who
have jumped on this DevOps bandwagon but
have found it's a lot harder than what
they think and it doesn't really work
for them is is usually what we hear now
that doesn't necessarily mean that
DevOps itself is a bad idea it's usually
that the implementation of it is not
working so well and we saw the same
thing with agile when that became a
thing a lot of people trying out scrum
and finding you know scrum doesn't work
for us it's not that scrums broken it's
just that the implementation didn't work
so well so now it's DevOps right and
people are saying well we tried to do
DevOps but it didn't really work so
today's talk is all about the most
common things I see in people's DevOps
pipelines that makes them think that
DevOps is just not working and is not
for them and how to fix them how to
identify them and how to move forward
and start getting it to work
so it would not be a DevOps talk if I
didn't have a definition of DevOps
thankfully my new boss
Donovan Brown has the definition of
DevOps that Microsoft uses which is the
union of people process and products to
enable continuous delivery of value to
our end-users I won't dwell on that too
much
I think people by now kind of know what
DevOps is the general idea is you're
delivering like real value to people who
are using it and that's the idea behind
it in terms of an actual pipeline this
is kind of what I'm talking about here
this there's a number of phases and it
actually does move in a circle you need
this feedback loop rather than just
requirements all the way through to
production and then that's done there is
a feedback loop that needs to happen and
the idea is basically to move through
this pipeline as quickly and as safely
as possible I'll talk about that in a
sec but as developers we don't have a
huge amount of control over the top two
number one and number four their
planning is often out of our hands
and monitoring and learning yes we can
add some monitoring we can collect this
data and things like that learning what
we need to do next in a lot of
organizations is kind of not our job we
get told what stuff to write and so on
so I'm going to concentrate a lot more
on the bottom part of this loop but I
just wanted to mention a couple of
things at the top because this is this
is one of the big problems that people
have with DevOps if they don't close
this loop the idea of monitoring what
happens in production and then learning
from that is hugely beneficial from a
DevOps point of view and something that
a lot of companies don't do it's the way
you learn about you know what bugs there
are what new features you should be
working on and any rework that needs to
happen if you've ever developed a
feature added it to a product and then
found that people aren't really using it
or you're not getting any queries about
it and you're wondering why it turns out
it's actually really hard to discover or
something like that by monitoring what's
happening in production you can be
alerted to that much earlier than then
um you know if you're waiting for your
customers to tell you about stuff and
remember like the work that you should
be doing day to day fits in really
nicely with this definition of DevOps
it's about continuous delivery of value
right
if you're working on brand new features
that nobody really wants and I know
there's a lot of software out there that
seems to do that they work on features
that nobody asked for rather than fixing
the problems they do have then you're
not really delivering value are you so
the idea here is to actually look at
what's happening in production what
people want what's going wrong and
working on that because that's going to
provide the most value so here's the
number one thing that I see at least in
this fear of something that's going to
get in the way of a good DevOps process
and it really just comes down to this
that there is no feedback loop the
developers will write the task the move
stuff from the left-hand column of the
board to the right-hand column
eventually it gets to production but
that's the last the devil ever see of
that task right they don't know whether
it succeeded from the point of view of a
user because there's no monitoring
nobody nobody really knows whether
somebody's using this new feature or not
and so this is a problem to do this like
to fix this problem is obvious you just
create a feedback loop right you add
exception handling that notifies you if
exceptions have started climbing in
production you look at feature usage are
people using these features that we've
got in there even performance monitoring
stuff like if I deploy this new feature
have we suddenly like pegged the CPU and
something's gone wrong there's not
really an excuse anymore for finding out
about this stuff from your customers or
from your users it's something that you
should be able to notice as much as
possible before you get to production
but even if it is in production you
should notice this stuff before somebody
comes screaming at you and I have been
in those organizations where I got a
page at 2:00 a.m. and somebody's yelling
at me over the phone rather than letting
me go and fix the problem but that stuff
happens there's not really any excuse if
you're a dotnet dev application insights
for example is pretty much free like
once you start using it in a lot bigger
volume then sure they'll make you pay
for it but um sorry we will make you pay
for it I got used to that
um I personally will make you pay for it
if you start using it and then you can
even tee it up to like power bi and
stuff like that to give you your
managers nice charts about who's using
stuff and what the performance is and
things like that outside of the
Microsoft stuff as well there's a one of
my favorite tools
use Raygun built by a team out of New
Zealand as well which means I think
because it's out of New Zealand I think
that Australia gets to claim it if it's
successful how it works if you're in if
you're in mixed session earlier Siri log
&amp;amp; Seek as well awesome ways of managing
the like exceptions and the stuff that
happens in your application it's like
structured logging that means that you
can find the information you want really
really easily so all of these tools are
available to you there's not really any
excuse for just finding out something's
gone wrong because somebody gives you a
call like I said I don't want to focus
too much on that so let's just jump back
to our DevOps pipeline and we'll start
talking about the stuff that we can
control a little bit more as developers
one of the things that is important
about this is that it is a feedback loop
and so the kind of goal of DevOps is to
move as fast as we can right to get this
cycle time fast but safe there's kind of
two guiding principles there is move
fast and make changes safely so in other
words we want to make it really easy to
push good changes out to production
really easy and fast to get good changes
in production and anything that's bad
anything that causes problems or causes
bugs we want this pipeline to stop it
from getting to production so move fast
but make changes safely and not let
anything bad get to production in
practice from the pipeline point of view
this really kind of equates to
automation and testing so automate
everything you can so that you don't
have to sit there and click buttons and
drag files around and queue build
yourself and things like that
but also make sure that your pipeline is
really robust so the good stuff can get
to production really easily and the bad
stuff gets stopped before it becomes a
problem so let's look at this bottom
part here the develop tests build and
release kind of part which is what as
developers this is the bit that we have
a lot more control over so if we can
think of this part of the pipeline as a
bit more of a pipeline from one side to
the other develop test build and release
now this is obviously kind of a gray
area as well you want to be do testing
at different points you want to release
to test environments do some more
testing and so on I'm actually going to
swap the test and build around because
development obviously you're going to
run tests locally as well we don't
really care about that from a from a
pipeline point of view we want to build
our software and then we want to test it
and make sure it works and if
everything's okay then we can release it
to production so just to make it a
little bit easier to walk through this
process I'll just swap those around so
let's start at the start obviously which
is the development part how teams work
and the writing the code part of it and
the problems that I see in organizations
that feel like DevOps isn't working for
them what what you can you do about that
from the development point of view the
number one guiding principle here is
that des should be writing code most of
the time right in an ideal scenario they
should only write code you write code
you commit it if there's a problem with
it you find out immediately and you just
fix the problem so all you're doing is
writing code you're not involved in the
actual deployment process of dragging
files around you're not pulling down
everyone's changes and building to
create a release candidate you're not
you know running through manual testing
processes and things like that you're
hired to write code so that's what you
should be doing so the biggest problem
that I see here in terms of something
that goes wrong with pipelines is that
you have a pipeline that is waiting on
other people to do stuff so what I mean
by that is that imagine you have made a
change to your to your code and it works
is beautiful but you still have to wait
for this other team who has broken the
build and they just need to fix that up
right I can't release this change that
I've just made until somebody else fixes
their problem like we you're getting in
each other's way and there are ways to
get out of each other's way right this
being in each other's way and having to
wait for another team to do their job
before you or your code can go out means
that that cycle time just slows down in
fact it just stops until that until that
happens
getting out of each other's ways there's
a couple of ways that you can do this
and this is really where we come up with
branching strategies so you know having
feature branches or team branches and
things like that it's kind of a way of
getting out of each other's way if
you're doing work and it's not finished
yet it shouldn't prevent anybody else
from pushing their changes through
feature flags kind of goes a little bit
better than this especially if you're
using it in the context of like dark
launches of code provided your code
compiles and it doesn't break any tests
if it's behind a feature flag that is
turned off that can go all the way to
production that code won't run in
production which means that
half-finished code that's behind a
feature flag that's turned off can still
be deployed to get a different feature
out or to get a different change out and
I can keep working on it until it's
ready to go
it's harder to do in practice safely but
it's definitely a really powerful thing
if you can wrap all your changes in
feature flags and just leave them off
until you're ready to actually deploy
turn them on briefly make sure
everything works and then just remove
the old code that's a really powerful
way of making sure that you're staying
out of the other developers way
branching strategies though they can
come with their own problems
particularly if you get to a scenario
where you have what's called continuous
isolation so as opposed to continuous
integration continuous isolation
basically means that the code you're
working on gets further and further away
from the master branch or from the main
branch of your code if you're working on
a branch that hasn't had any merges from
master for a period of weeks that merge
is going to suck right getting your code
working with everybody else's code is
the hardest part of any branching
strategy the merge is the worst part
so continuous isolation really describes
when you have these long-running
branches and you really really need to
avoid that as much as possible trunk
based development is a great way of
avoiding these these merge pains or at
least working out what the problem is as
early as possible so everybody working
on the same branch is a
great idea if you can do it and you'd be
surprised how many people do this I'll
show you in a sec but Microsoft the team
that actually writes Visual Studio Team
Services basically does trunk based
development and there's a lot of
developers working on that but if you
can't do fewer like trunk based
development it's a little bit hard
there's too many teams working on too
many different things then at least to
make your feature branches short-lived
short that the definition of shorts
going to completely depend on your
organization how much work happens how
quickly but if you have you know one or
two developers who commit maybe once a
day twice a day maybe you can go for
weeks without the modes going to be an
issue if you if you have hundreds of
developers committing constantly then
you really want to be close to master
you don't want to let that stagnate get
isolated for a very long period of time
so short live feature branches another
technique I've actually seen that gets
around this quite well is the idea of a
what if continuous integration build in
other words if we're working on a branch
that's fine but have a continuous
integration build that means that every
time I commit code to that branch
there's a build that merges it with
master and does a compilation and
identifies the problems that are going
to occur when we actually do that merge
later for one thing if it doesn't merge
automatically you know that you're going
to have to do some manual stuff when you
merge that later on and it's a good idea
to know that ahead of time or if code
does merge perfectly but now this really
unexpected test breaks at least you know
before you have to go through that
process of merging particularly if
you're in a scenario where you merge
your code just before you start doing a
deployment which happens in a lot of
organizations so this kind of what if
build is not a bad idea either now I
mentioned that Microsoft does pretty
much the trunk based development this is
a screenshot of the actual visual studio
team services codebase so they're using
git the thing I want you to notice
though is how like clean relatively
clean that that path is there's not all
of these feature branches coming in from
all over the place and this is like five
hours worth of commits roughly five
hours worth of commits there's also
about 25 distinct people committing in
this as well so it's not like
you have three or four people who are
working very closely together there's
teams working on very different stuff on
very different code and the clashing is
not really happening because
everything's more or less trunk based
development now they still do pull
requests they still do topic branches
they call them but they get merged into
master really really quickly so the
merge conflicts with a huge team are
kind of kind of gone so this is it is
possible in a large organization with
kind of a large number of developers so
I just want to summarize this
development stage of our of our pipeline
there's a few different things that can
go wrong that we see number one waiting
on other people and so branching
strategies and maybe even using feature
flags and things like that is a good way
of getting around those problems but
with branching strategies you've got to
be careful that you're not doing
continuous isolation in other words
you're merging really really constant
really really consistently the word the
term continuous integration really means
integrating your stuff with everybody
else's stuff so having a CI build that
just builds on each branch individually
and doesn't merge that's not continuous
integration that's continuous isolation
there's actually a really good website
that talks about the practicalities of
trunk post development and what that
actually means in organizations as well
that's worth having a look at so that's
the development stage let's move on and
talk about the next part that I want to
talk about which is the build part so
the number one guiding principle I think
for doing DevOps properly from a build
point of view is that builds should be
automatic right you shouldn't have a
person in your organization who pulls
down the code does a build manually and
then send it out it's kind of similar to
the previous point in that developers
should be writing code they shouldn't be
trying to work out how the builds going
wrong or builds working and things like
that the build also gives you an
opportunity to do like a first pass
filter so if your code doesn't compile
having a continuous build for example
means that you know that immediately
it's not going to be three weeks later
when you've forgotten what the code
looks like that you work out that you've
introduced a bug or you've introduced
something that can prevent it from
compiling on the production server or on
the sari on a build server with
everybody else is everybody else's code
when it gets merged the idea of finding
out these problems as early as possible
is what people who talk about DevOps
talk about shift left so this is the
idea that you remember our pipeline
where we had developed build tests
release you want problems to be
identified as far left in that pipeline
as possible everybody knows that fixing
a bug while it's on your machine like
while you're writing the bug fixing it
then is so much cheaper than fixing it
when it's in production so shift left
refers to the idea of trying to notice
and fix these issues as far left in that
pipeline as you possibly can so this is
what we talk about when we talk about
shift left the build server gives you a
great opportunity to do that or the
build phase gives you a great
opportunity to do that because you can
immediately tell if your code is not
going to compile and you want to know
that straightaway not a week later so
the biggest problem with DevOps that
people who find this this issue have is
the idea where they they build locally
ready for a deployment so the biggest
problem with this is that it's not
automatic so you want you want to know
when the build is going to break like
immediately right but you also don't
have to rely on somebody pulling down
all of everybody's changes and compiling
and making sure that they all work
together
you really want that to happen
straightaway if you're relying on a
particular person doing a build for a
deployment then you're going to have to
wait until that person is ready to do it
they're working on their own stuff right
just before you do a deployment they're
like yeah let's grab everybody's thing
we do a compile and oh there's weird
stuff that's gone wrong
and this often happens with these
deployments that happen on a Saturday
when everybody comes into the office
right you need to work out how this
stuff fits together first you want to
know this straightaway so like number
one the easiest thing you can do is set
up continuous integration with a build
server so continuous integration is
basically every commit there's a
compilation it gets merged with
everybody else's changes and there's a
compile you end up with an artifact at
the end of it that is your compiled
application it means that you can come
it and be told that you've broken the
build within a minute or two often
depending on how long your build takes
if you're not doing this if you don't
have a build server that's doing this
this would be my number one suggestion
to improve your process like there's
nothing easier to do then install a
build server and then you never really
need to worry about whether your code
compiles or merges with everybody else's
again it will tell you straight away
right when the code that you wrote is at
the forefront of your mind
not a week later when you've forgotten
it the good thing about this as well is
that especially if you're a dotnet
developer build servers are pretty much
free like teamcity unless you use it at
a particular volume again teamcity is
free Visual Studio Team Services
actually has a good build engine now the
old zamel one was atrocious but the new
one is really good and it's free unless
you have like huge volumes going through
and presumably by that point you're
making enough money hopefully that you
can pay for it so you can just you can
just start using this stuff um anyway so
yes build server continuous integration
that'll at least get you moving fast and
mean that your problems have been
shifted left the other problem that I
see with this pipeline is build that
also do deployments and this is a really
common strategy that people do where
they have a development build or a test
build and then like a staging build
which will recompile deploy to staging
and then a production bill which will
recompile and deploy to production now
this is dangerous because even though
you may do these compilations in a short
period of time there is always the
chance that something is going to be
different between them so when you
deploy to your test server and you run
through all your tests and then you
decide that that's all going to work and
you do a staging build for example you
are now pushing different bits to
staging different files they may
technically be more or less the same
because nothing's changed but you can't
be sure the last thing you want to do is
push some bits to production that have
never been tested and that's exactly
what this kind of strategy does
even no matter how confident you are
there's always the chance that something
is going to be different so as much as
possible you want to build your
application once build your binaries
once and deploy it many times so deploy
it to your test environment deploy it to
your staging environment deploy it to
production and so on now commonly people
will say yeah it's not realistic like
stuff's not really going to go wrong
like it's a build on the same build
server we don't patch in between our
releases stuff like that
imagine having a build that is like your
test build you deploy the test run
through all of your unit test all of
your integration tests everybody runs
the thing it looks like it's fine so you
do a new build ready to go to production
and these tests wouldn't pass anymore
like there's bugs cropping up things
that worked in staging just don't work
in production you only built these
things like a very short period of time
like away from each other you can't
figure out what's going on so this
actually happened it's net four point
six the initial release of four point
six introduced bugs when you did a build
with the optimized tag turned on so this
is what people were doing they were
doing a deployment so doing a build for
their test environment like a debug
build with optimization off deploying it
running through all their tests and then
rebuilding for production with the
optimized flag turned on and methods
were getting like the wrong arguments
passed to them in the underlying code so
if everything had been built once and
then deployed many many times you would
have at least seen these problems in
your test environments first and you
could have dealt with it then so this
was a bit of an issue this is just an
article from the register I think it's
2015 so this stuff can happen right not
to mention you know if you patch your
build server then maybe the binaries are
going to come out different when you
compile again so is something to think
about to play one so I build ones to
play many times which means that your
configuration needs to be outside your
build you shouldn't need to recompile to
deploy to a different environment for
example the other idea is a bit more
general which is just make sure you're
using the right tool for the job
build servers a good at building
deployment there's deployment tools that
are really good at deployment
optimist to play for example is still
definitely my favorite deployment tool
right you should it encourages it
encourages you to do the right things
things like building once and deploying
many times you don't have a new package
to deploy to each environment you deploy
the same package to all your environment
so the tooling can help you do this sort
of stuff but you want to use the right
tool for the right job come that's just
a generally good idea anyway and again
pretty much free until you get to a
certain level of volume octopus
certainly is release manager I think is
free ish until a certain point again
same as the build tooling so from the
build point of view there's a couple of
things that I mentioned here things to
avoid building locally and to do that
get a build server and and continuously
integrate your code right but also make
sure you're avoiding continuous
isolation there so yes
builds that also deploy your build
server should be building your artifacts
and then use a different deployment tool
to actually deploy those artifacts that
way you are testing the same bits as are
going to go into production and you're
deploying them the same way ideally
which I'm actually jumping ahead of
myself but it means that you know that
this code is going to work because we've
tested the exact same compiled
assemblies that's really important
all right so testing phase so we've
we've worked out the problems with our
development process getting in each
other's way and slowing each other down
we've got our build engine which means
that we can find out whether we've
introduced problems immediately we're
building once and making sure that those
binaries are going all the way through
to production rather than having
unexpected failures occur because we've
rebuilt everything so now we want to
make this pipeline really really secure
and really robust meaning that anything
that we do that's bad is just not going
to make it to production so in the test
phase and this is going to seem really
weird but the test papers the number one
thing is that tests should do testing
right this sounds ridiculous
but it actually occurs a lot where
people will write tests that don't
actually do what they're supposed to do
in other words tests don't fail when
something goes wrong right and you
actually see this a lot the I think the
reason that you we see this a lot is
because code coverage became a big thing
with testing this it might be
controversial and a lot of people
probably disagree with it but I think
code coverage is one of the worst things
that we've done for testing point of
view because not only are you not
worrying about whether this test is
serving its purpose which means it fails
if something is changed and it breaks
all you're worrying about is have have I
exercised all of the code have I gone
through it all which means that this
test here for example it's a legitimate
test it will contribute to our code
coverage but it's not really testing
anything if you haven't noticed that's
the assert at the end of it right so we
have code coverage but if something if
we introduced a bug in this it's not
going to break all right
code coverage gives us the false sense
of security that we've got tests over
all of our code so we don't need to
worry about it anymore but if the test
is not written well then it's not
actually serving a purpose and it's
making us feel safe when we're
absolutely not safe so by the way this
is um code from Ben Cole the guide who's
doing the beer for Thursday night I made
him change the assert he's not actually
doing this just in case you ever use any
of his products he does have real tests
in there that do real asserts anyway
this is obviously contrived example but
it's very easy to see if you go looking
in a lot of companies people who are
whose tests basically amount to create
some mocks and then call the thing and
make sure that the thing is calling the
right functions in the money in other
words create mocks and then make sure
the mocks are being called that's not
really going to fail very often in a
real words world scenario it's not
testing what you needed to do it needed
to test one of the techniques which is
really valuable in this is the idea of
red green refactor everybody heard of
red green refactor yep okay most hands
the general idea is that you see a by
and instead of just fixing that bug
straight away you write a test first you
make sure that test fails because
there's a bug in it then you fix the bug
and make sure the test succeeds so it
goes red you fix the bug and it goes
green and then you do any refactoring
and changing things around securing the
knowledge that if you break it again the
test is going to go red you've seen it
go red when it when it breaks so often
this is like red green refactor red red
red red red damn it revert and come back
but the idea is you have the test in
place so you know when something breaks
that the test is going to alert alert
you to that so this is a really good
strategy test-driven development is
along the same kind of lines as well
where if you have a new requirement you
write the test first before you've
written your code and then your brain is
just verifying that you wrote the code
that you just thought you wrote you
write the test first to set those
expectations it will go red because you
haven't written the code yet and then
you write your code to actually
implement that that function and make
sure your test goes green so same idea
you know that you have a test that's
going to fail if something goes wrong
it's serving its purpose the other thing
which you see a lot with tests is people
ignoring tests as well so saying things
like oh this test actually fails in
staging because we don't have this
back-end database thing or something
like that just ignoring that that's a
problem or saying yeah your test fails
but that doesn't matter in production
because it's slightly different
something like that this is a really bad
habit to get into a lot of the time you
know people will comment out comment out
tests as well because they're like oh
this is failing at the moment it's just
temporary while we're doing this stuff
I'll comment it out for the moment or
even worse you just comment out the
assert because then you get the code
coverage but even the deployment process
as well so if something goes wrong
during deployment then you want to make
sure that your pipeline can handle that
and will let you know if there is a
problem in the future so imagine a
scenario where for example you deploy to
a staging environment but it fails
halfway through because there's not
enough disk space it's really common to
say well productions got tons I won't
worry about that well let's just clear
up some disk space run the deployment
again oh yeah it worked great
and now you deployed a production it
would be much better to apply those kind
of red green refactor techniques to your
to your entire pipeline there so rather
than just saying oh that's that's fine
it's a different scenario in staging or
something you actually add something to
your pipeline to check for that scenario
check that there's enough disk space and
only let the thing continue if there is
that way one day when production runs
out of disk space when you do the
deployment you can find that out before
you actually do it it's going to stop
you from making a mistake in the future
the same kind of thing with integration
test system test things like that don't
ignore them if something pops up and you
say oh that that's not a problem in
production don't worry about it no put
the add to your pipeline to make sure
that if it does become a problem in
production it's going to catch it before
you get there
so don't ignore these tests the other
thing I see in the test scenario that
that breaks DevOps a good DevOps
pipeline is the idea of extensive manual
testing now are there any testers here
by the way no one okay I apologize in
advance computers are better at running
through scripts than people are right so
if you have testers or a test team that
needs to run through this gigantic
spreadsheet of scenarios where the
instructions are more or less go to this
page click on this link type in your
name press ok and then go to the next
page and it should have this text on it
that's a script right and if you tell a
computer to do that script it's going to
do it the same way every single time
people aren't as good at that as
computers are so let the computers do
those scripts now the best testers that
I know are the ones who are automating
themselves out of this terrible part of
their job you know they're clicking
around and just data entry style testing
if you can automate that process as a
tester you are extremely valuable it
means that you don't have to keep
clicking around all the time but that
test is going to get run it'll get run
faster and will get run more
consistently than a human could do now
I'm not saying that that
kind of clicking around exploratory
testing is not valuable what I am saying
is that do those but they shouldn't
block your deployment process or your
DevOps process this actually works
really really well in combination with
feature flags particularly from the dark
launch kind of perspective if you're
writing some code you wrap it in a
feature flag that's off you can
continually deploy this to production
maybe you don't really know what tests
need to be run at that point so your
testing team can flick the flag on for
themselves
do some exploratory testing find the
bugs and then give it back to you before
you flick the flag on for the general
public all right this is this is
actually the way that the V STS the
visual studio team Services team works
as well they do do a lot of testing they
actually have 50,000 tests I think that
run in six minutes they do a lot of
parallelism it's insane but they wrap
everything in feature flags they push it
out all the way through to production
through their arm through all of their
different environments and then when
it's in production they're like ok we
need to work out whether this actually
works let's turn it on for a very small
subset of people like just our internal
team do some exploratory work do some
exploratory testing are these things
going wrong let's add that test to our
pipeline make sure it won't go wrong
again fix the bug push it all the way
through ok we're good now after that
point the automated test is going to
pick up any kind of any kind of instance
where that that bug comes up again
so it gives you an opportunity to do
this exploratory testing work out what
needs to be tested what's going to go
wrong before you actually get it in the
hands of everyone so exploratory testing
great but don't let it block your
process all right
terrible scenario if you have developed
all of this code it's all ready to go
and then you have to wait a week for the
test team to go through and test
everything manually in the meantime all
of your work is piling up you're
forgetting what you did so when it does
go to the test team and they say there's
a bug in this like I don't remember that
code and then you do a get blame and
find out that it will you two weeks ago
yeah anyway
I may have done that once or twice
before um okay so there are a few
different points here in the test phase
as well so number one which is the most
important thing is that tests don't fail
when something goes wrong you really
need to make sure that your tests are
serving the purpose they're supposed to
serve which is not code coverage so red
green refactor test-driven development
is really useful for that and apply the
red green refactor techniques to your
whole deployment process your your whole
DevOps process if a bug gets through to
production that's not just a failure in
your code that's a failure in your
pipeline your pipeline should have
stopped that bug from getting the
production so add something to your
pipeline to prevent that from happening
again and then you don't have to worry
about it from that point forward and
then finally any kind of extensive
manual human driven process that stops
the deployment of your code is not going
to help you from the DevOps perspective
it's going to make you feel like this
isn't actually any faster than it was
before and now we're just waiting all
this time
so this pipeline the idea is to get from
one end all the way to the other not one
end halfway through and then you know
then be held up by by people so I think
I've said before that a pipeline with a
human step in the middle isn't the
pipeline anymore it's two pipelines
joined by a dumb human alright
so try and get the pipeline from end to
end that's really important alright so
now we've got a pipeline that's that's
testing effectively we are now at the
point where we want to deploy all the
way through to production so let's talk
about that this is kind of my wheelhouse
I think the deployment stuff at the
moment at least so number one thing for
the release phase is that your
deployments should be easy to do and
very predictable in other words it
shouldn't be your every six months
everybody comes in on a weekend spend 48
hours like hating each other and then
goes home at 3 a.m. on Monday morning
because the deployment went horribly
wrong and so on they should be really
really easy to do I'm ahead of time so
I'll tell you back when I so I moved to
Canada
the rest of the octopus deploy team is
in Brisbane Australia where the circle
was in the map there's no time zone
overlap between those like no business
our time zone overlap which means I
didn't really get to talk to the team
that often what that ultimately meant
was that I was working on peripheral
stuff I wasn't working on the main
codebase I was working on integrations
with VST s and stuff like that um so I
hadn't actually pulled the latest source
code down for maybe five months I woke
up one morning and there had been a new
deployment a new version of octopus
deploy that had gone out to production
and there was this really weird edge
case where somebody was going from like
four versions earlier to this current
one and it didn't work so they upgraded
octopus and it wouldn't run anymore and
they've actually since fixed this
problem so you can't do that anymore
it'll tell you before it's too late but
it was enough of a problem for us to say
well we can't let that one stand so we
took that download off so nobody else
would download it keeping in mind the
rest of Australia the rest of the
company is asleep and I haven't touched
the code for five months and I needed to
fix it so I pulled down the code from
github I worked out what the problem was
and I fixed the problem and I committed
the code and pushed it to github all of
the continuous integration ran all of
our tests ran everything looked okay and
I'm like okay let's do a release so I
went to slack which is our chat tool and
I typed in a release but release octopus
deploy three point one point four or
something like that and then I just
waited for about 20 minutes and then
there's a new version in production I
hadn't touched this code for close to
six months I wasn't actually sure
whether I'd fix the problem properly
like it looked fine to me or my local
testing seemed okay but I was confident
enough in our pipeline to know that that
if I hadn't made a mistake and something
had gone badly that wouldn't make it all
the way through to production at least
not worse than what was already in
production um
so that pipeline was going to prevent
any bad stuff from happening but the
deployment itself was so easy I just
typed in a line in slack and I knew it
was going to work because this process
is predictable I know the deployment
does the same thing every single time
and it's worked every version before now
so it's going to work now
so this that was a deployment that was
easy to do and really predictable that
gives you confidence it means that you
can deploy more and more often and
whenever you feel like deploying
whenever there's something to deploy you
can just do it because it's easy the
biggest problem with this once again is
that there's a human in the process so
the deployments that happen where
there's somebody who has to do one or
two different tasks that really really
slows down your cycle time especially if
you have sign-off processes and things
like that and I'm not saying that those
are completely useless just that they're
not that useful I guess so you want to
take the humans out of the deployment
process as much as possible right out of
the whole pipeline but the deployment
especially which means automating
everything right automate all your
deployment and there are some pretty
good deployment tools that do this work
once again octopus deploy but if you're
not in a dotnet scenario there's like
salt stack I don't know there's a bunch
of other ones there is there's no
shortage of deployment tools and they're
built for that they're built for
deploying your application in an
automatic way but there are some things
like sign up processes that you can't
automate you really in a lot of
organizations you need a manager to if
not add any value by signing something
off at least be the person who is the
full guy if something goes wrong which
is often what these sign off processes
are the manager will insist that they
have to sign this stuff off but they
have no idea what you've done they'll
basically say did everything pass did
all the tests pass does it look okay you
say yes and they're like okay we'll sign
the thing alright that is a process that
does not need to happen right now this
actually putting this into effect is
harder in a lot of organizations than
just saying hey manager your job is
useless
that's probably not going to get you
where you need to go but at least you
can optimize this process and make it
less of an issue at least speed things
up for you I mentioned that I worked in
government as my first job at a
university there was a occasion then
when a DBA got a sequel scripts to
update the schema of the test database
and he accidentally ran it against
production so he ran the script against
prod which brought prod down so it took
about 10 seconds to to get a production
outage it took us about 10 minutes to
work out how to fix this and to give him
a new sequel script to say can you run
this and it'll fix it but then it took
us six hours to get all of the sign-offs
necessary to make a change to production
right the reason is this the policy at
the time was that for a production
deployment or production change to
happen every department that could
potentially be impacted by this change
needs their manager to sign off on it
one of them was outta lunch another guy
wasn't there that day you know six hours
of process just to undo a change that
occurred now that's not because what I
mean it was because somebody made a
mistake sure but that process shouldn't
need to take six hours so you really
need to optimize these kind of processes
the best way I've seen it done as well
is a lot of octopus customers used to
just have the deploy button like on an
iPad and they would hand it to their
manager and say we're ready to go and
the manager will go oh sure
press the button and it's in production
which is really good if he doesn't know
what the button does and you're at the
pub and it's Friday afternoon which
happens so optimize those processes that
require humans really work it at getting
those as quick as possible the next one
with the deployment process is really
the idea that production is special and
you can't do your deployment to your
test environments the same as you do
your deployment to production in other
words yes you can continuously deploy
the tests and maybe we'll do automatic
deployments to staging and stuff like
that but production that's different we
all need to come in on a Saturday every
six months and work for 48 hours
straight to get stuff working
production should just be another
environment as much as possible this is
why we have staging environments that
are supposed to look exactly like
production so that when we run the tests
we know that it looks the same it's
probably going to work in production as
well why are we not applying that to the
deployment process we have this
automated process to put it in a test
environment and to put it in a staging
environment but then when we deploy to
production it's the first time we have
ever done that with these binaries and
that's really scary the first time you
ever test something should not be when
you put it in production right so as
much as possible your production
environment should should be the same as
all the others deploying to it should
not be any harder the key to this is
making sure that you have the same
process for all of your environments now
again tools like octopus and a lot of
the other tools make sure that or make
it really hard to not do this so you
have one deployment process in octopus
and you have to run through that
deployment process in all of your
environments it's hard to make a
different process for different
environments so you want to make sure
that not only have you tested those bits
because we only build once we deploy
many times we've tested the bits but
we've also tested putting them on a
machine so when we do it in production
we are pretty confident that that is
going to work the next deployment thing
that I keep seeing is the idea of kind
of patch based deployments in other
words every release is a collection of
differences between the last release and
the current one changed files here's for
sequel scripts that we need to run over
the database in production to bring it
up to speed to the latest one now this
makes for relatively difficult to
reproduce deployment processes there's
so many things that can go wrong here
for one thing all of your test
environments need to be exactly the same
as production now so if you're running
scripts the schema needs to be exactly
what production looks like so iterating
really quickly make is quite difficult
as well as that if something happens to
production then how do you deploy the
whole thing again like production is
gone now how do we how do I put it back
right it's it's exploded and all they
have is the changes from the last one so
the idea of patch based deployments
makes it really hard to get this
repeatable process going the solution to
this really is to deploy is to deploy
everything every time now that's
generally well that's often not
practical I know
SSW which is a consulting company I used
to work for their website they kind of
had a policy of never deleting any
content from their website ever for
various reasons I don't know what they
are so that meant the content of their
website was about a gig now if you try
to push a gig
to a u.s. VM and an Australian VM every
single time you want to make a change to
a page that's going to be really really
slow but tooling can help you do this
so I suppose had a had a feature called
Delta compression which basically means
if you have two one-gigabyte packages
but they only differ by a tiny amount as
long as that target machine had the
previous version you just send that tiny
amount will rebuild it on the other side
so the tooling can optimize this the
important thing though is that you could
do that same deployment to a fresh
machine or to a machine that had the
last version on it as well in practice
of like database upgrades and things
like that that means using tooling like
I was a reggae ready roll is one of
those ones there's things like howdy be
up for migration based ones energy
framework migrations is also good for
that so excuse me or other tooling that
looks at the schema works out what needs
to change and makes that change you can
apply that to an empty database but you
can equally apply it to a database which
is one version behind so making the
tooling help for you so you're not
actually deploying everything every
single time but you can write so the
release ones number one problem is that
there's a human involved in that release
process you want to automate your
deployments and optimize anything you
can't automate don't treat production as
a special environment it should just be
another environment it's something else
you can deploy to the other advantage
for not using patch
based deployments as well is that if you
want to just try something you've
written some changes and you want to
just give it a go on a server spin up a
new machine deploy to that machine
because you've got the whole deployment
process deploy to that machine test it
out tear it down you're done really good
for load testing as well you can spin
something up and then just hammer it and
then tear it down when you're done so
avoid these patch based deployments -
all right so we've gone through this
develop build test and release process
and a few things on each of those that
have that can cause problems and make
DevOps not work I would just want to
talk about one other thing briefly which
is actually implementing these changes
or implementing DevOps or doing DevOps
the idea of the talk was really just
that you know people are saying well we
tried to do DevOps it didn't really work
for us a lot of the time that's because
they went and tried to do it all in one
hit in other words they did their DevOps
implementation in one go everybody
stopped writing code now and we're going
to do DevOps for a month and then we can
start writing features again we learned
like a while ago back when agile became
a thing that big bang like
implementations of anything doesn't work
so why would we do it when we're
applying these new principles and these
new ideas to the way we work we
shouldn't we should really identify like
one problem at a time what is the
hardest thing right now that we are that
we have implement a fix for that
automate a certain part of the problem
and then move on to the next one so
iterate continuously look at whether
stuff is working for you and if it's not
fix it and then move on to the next
thing the important thing here though is
that you need to expect to fail pretty
regularly right you can't just implement
DevOps and then that silver bullet now
means that you never have to worry about
how stuff works anymore that just is not
practical right you need to expect to
fail there's actually a really good
report that comes out every year done by
an organization called Dora which is
oops well I'm going to I'm going to get
this wrong DevOps research and analysis
group run by a woman named Nicole Falls
grin as well as jazz humble who you may
know wrote the continuous delivery book
and another guy whose name has just
escaped me who wrote the Phoenix project
said he would he remember Jean Kim that
was one Thank You Jean Kim they put out
this report on the state of DevOps in
different organizations and basically
they classify all the different
organizations they interview about
27,000 organizations and they classify
them in terms of how mature their DevOps
process is usually based on things like
how often they deploy or how quickly
they can deploy their cycle time things
like that but various other metrics and
they classify them into low medium and
high performers what they actually found
in the 2016 state of DevOps report was
that as you move from a load performer
to a high performer the medium
performers actually have more more are
change failures so changes they
introduce fail more often than high
performers and even low performers which
basically means as you're starting to go
as you're starting to do DevOps better
it's going to get worse before it gets
better so you need to expect to fail now
the 2017 state of DevOps report actually
came out about a week ago I think the
reason I'm using the 2016 state of
DevOps report is because it paints this
picture the 2017 one actually does this
so but the point is it's not always that
easy to get to get this stuff working
properly you need to expect failure I
think the reason for this is probably
just that our understanding of how to do
things more effectively is better now
and the tooling probably stops us from
making the mistakes that people used to
make so expect to fail but it's not as
bad as a better story as as you as it
has been in the past all right let's
just quickly jump back to our devops
pipeline we kind of talked about all of
these sections in differing amounts
particularly the ones at the bottom that
developers have a lot of control
/ and there's a lot of different areas
where the things can go wrong and make
you feel like DevOps just isn't working
for us right from the feedback loop that
we have all the way through development
build testing and the release cycle as
well and even the implementation of it
the companies that say right we're going
to do DevOps today let's do the whole
thing and then it just doesn't work
often it doesn't work and you feel like
it's not not going to work in this
organization you're just going to
abandon it and go back to the way that
things were right so there's all of
these different things that I would
encourage you to look out for and
hopefully if you can make some of these
changes and notice these things
happening in your organization you can
start fixing the problems before they
become too much of an issue and start
doing DevOps a little bit better and
that is all ahead thank you very much
now I think we have about five minutes
if there are questions so right maybe
kind of make sure by the way if you're
on your way up make sure you do the
feedback it's really really valuable not
just for the speakers but for the
organizers if you're confused just grab
a green one put it in instead of a red
you could just grab a stack of greens
and put them in as well yeah are there
any questions yeah
so so the question paraphrase I think is
what's my opinion on differentiated CI
builds from the production builds I
don't have a problem with that at all I
think your CI build should really just
be a first pass does everything compile
do our unit tests run that one should be
fast especially if you have a lot of
developers developing like writing code
against it as long as you know if you
introduce a compile error or something
that emerge is going to break you need
to know about that as early as possible
if your CI build is going to take four
hours it's not really going to let you
know as early as possible so CI build
that does really really fast basic ones
we just run some run just running some
unit tests and then a separate build
that you do to create a release
candidate I don't have a problem with
that at all as long as your release
candidate build is going to pick some
pickup problems as well right if your CI
build is just kind of your first pass
then you have a separate build to do a
production ready release in fact even
having one in the middle that does like
a nightly release isn't a bad idea
either it does some more work I don't
have a problem with that at all I think
it's a good idea yeah any other
questions the difficulty with this room
is that there's two lights here and so I
can't see anything which is kind of good
because I'm just assuming that it's
completely packed any other questions
we go yeah I'll go here first
so how do you sell the business on
implementing these principles I actually
did a talk on that about a week ago
called something like doing DevOps as a
politically powerless developer there's
a lot of different ways you can do it I
think number one would be trying to make
it very clear to the management that
they are the bottleneck so the CIA tools
and things like that you can probably
install them on your local machine it
still does a CI build you can still
notice things are ready to go you've put
octopus on your local machine it's not
ideal obviously but if you can if you
can put yourself in a scenario where
your manager says hey I want to make
this change you bright the code in like
20 minutes or an hour or something like
that and then have it running in a test
environment say hey try this URL does it
work and the manager goes yes great
that's good how long can we how long
until we can put that in production and
you're like well based on our current
process it's six weeks it's going to
become pretty clear to management that
they are the bottleneck not the
developers right I think it's just a
matter of pushing pushing this stuff and
keep just keep churning away at it until
they notice that that's the case the
other thing is the change in
organizations really often comes from a
analysis and then an acknowledgement
that something's wrong and then some
adjustment so this like analysis and
acknowledgement adjustment phase you can
do a lot about the analysis stuff if you
can measure the mean time to push
something out and say to your manager
here's how long it took to put a
production build in production and most
of that was waiting for sign-off waiting
for the test team waiting for all of
this stuff there are better ways to do
this can we talk about that I think but
it is a hard problem like that's the
people problem which IT people are not
generally that good at I think but yeah
make them feel like they're the
bottleneck and then give them give them
data to prove that this is not working
very well I think of the two - I would
recommend and there was a question there
that was the exact question cool would
you like me to answer it again
no okay was there somebody up there nope
cool I will be hanging around here after
this if you want to have a chat a bit
more privately as well but if that's
everything thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>