<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ASP.NET Core Kestrel: Adventures in building a fast web server - Damian Edwards &amp; David Fowler | Coder Coacher - Coaching Coders</title><meta content="ASP.NET Core Kestrel: Adventures in building a fast web server - Damian Edwards &amp; David Fowler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ASP.NET Core Kestrel: Adventures in building a fast web server - Damian Edwards &amp; David Fowler</b></h2><h5 class="post__date">2016-08-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kej3YJDMAW4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh yes hello
testing testing there are a lot fewer
people here than they were this morning
because we're up against Dominick and
other very capable presenters this time
and we're building we're talking about
something very esoteric yeah so it's
good that you all joined us I'm very
happy that we get to model our way
through this we are going to admit right
up front that there are some things in
this talk that we do not fully
understand but we're going to attempt to
explain them to you anyway
there are people on our team who
understand them notably Caesar and
Stefan who I've called out here who did
all the hard work putting this
presentation together merging for
requests emerging for requests and we
are going to do our best to explain to
you how some of these things work some
of them we understand a lot better yeah
but others are quite a struggle to get
our heads around if you've done native
development before dealing with CPU
instructions or game development or
vectorization and sim D and memory pool
Matt I don't know like I've already lost
sir I'm building fast servers is really
hard that's what it all comes down to
building naive servers that just work
it's pretty straightforward building
ones that work well and work fast it's
hard so introductions again I am Damien
Edwards so I work on the HP net team at
Microsoft live a fellow I work on the
esmad team Microsoft and Microsoft as
well yep
and we're gonna talk about Kestrel which
is the web server
managed web server that we built for a
spinet core that runs on Linux and OS 10
and Windows obviously and it ran on a
Raspberry Pi at one point yeah not the
moment but it will again I'm sure of it
and then we get back round of fixing it
users libuv which is the native
component that's used by a nodejs
to do i/o and file stuff and a spinning
things and threading and loops and yadda
yadda yadda and yeah it's been quite a
journey and so it kind of all started
with this this was in July last year so
we're not even talking about a long time
so July last year kelly summers who's
well known in the community tweeted this
she's been
working on this cool little web server
project called haywire which I believe
started out as an academic experiment
she likes to do things in the open to
learn to teach herself and she was
writing an HTTP server in C because why
not and her server it's called native
compiles down to annex C like there's no
way to add handlers you have to get the
code and this is right and change the
code and recompile the server whenever
you want to like change the title of
your webpage
I don't think I'm lying I'm pretty true
that someone might build an extension
frog by now and so when she ran that on
an ARM processor on her Raspberry Pi
with a 100 megabit network she was
getting 72,000 requests per second which
is pretty good and then she ranked
kestrel at this point which is a spin it
cause super fast via beta 4 or 9 bid
five on her jolzi on eight core two
Gigabit Ethernet network server and got
750 requests per second and so yeah that
was me a cop-caller bear yeah I think
she another tweet used the word
embarrassing which I can understand
early her so we wanted it to be fast we
had that this wasn't the only motivation
and there were a bunch of reasons why
she got such a terrible result I think
she ran it again and got like 75,000 or
something but we were getting hit by a
bunch of networking 101 things like act
delays and Magelang and other stuff that
we eventually figured out when we went
back to the whiteboard and drew sine
waves that represented like electrical
pulses on the wire and learnt how I'm
not even kidding and I like we one of
the guys in the team schooled us on how
networking actually works when you talk
about electrical pulses that go across
copper and then how network drivers are
written and what Nagaland does and what
act lasers are about and how Ethernet
does now TC pieces on top of that we've
learnt all this great stuff and then we
fixed stuff so then we looked at the
competition and this is a results from
the latest round of tech empower who's
heard attacking power the second power
benchmarks okay most people half people
maybe there's a bunch of frameworks here
you've never heard of which is fine
there's a bunch of frameworks here that
you probably have heard of that are very
well known this is the top this is the I
just
screen capture the very top of the list
here if we go and look at the live
things this is round 12 this is the
latest round this is running on their
peak hardware which is a fairly beefy
Dell server 44 20 cause hyper threaded
so it's 40 virtual CPUs with 10 Gigabit
Ethernet and you know what do we got
this thing called rap boy rapido rapido
which only showed up in this round in
the last round rapido we'd read oh then
it gets a lot 7 million requests per
second on this hardware but you can see
it drops off really quick so there's
like a cluster the top 5 and then it
like it nearly has like by the time you
get down to like number 8 like we're
down to 3 million already which is still
amazing it's incredible and Nettie is an
interesting one because Nettie runs on
Java s does rapid Lloyd actually Java
amazing but Nettie not any runs on Java
but is very popular like is using a lot
of products is shifting a lot of white
box products you don't even know it's
being used it's been around for a while
it's a bit like WCF really isn't it yeah
yeah yeah very very very bad awful but
but better yeah not well but different
different different but yeah okay javis
so and we can scroll down here you might
start to see some things that you under
like jetty is also pretty well known we
have to go down a bit further to see
this go not quite
cracking the million yet go pre fork not
sure what that is maybe a lysis that's
Larsen what's Colossus I don't know oh
says akka runs on alcohol okay um what
else anything down here just a raw jetty
servlet rather than running jetty
outside of a servlet just a jar I guess
wicked what's wicked that sounds good
yeah I think it ya know I still might
say it with rack rack okay that's the
thing that really Ruby this JRuby that's
not real we're just faster than Ruby and
then here's note I do the cart right
that's not bad we're dead like four plus
four point six percent of the number one
result like where wait I'll use it the
tail is enormous here and here's nodejs
all the way down here which I think for
a while node was considered like this
really fast web server and it does a lot
of things really really well don't get
me wrong it's it's architects
certainly very good cookin for
concurrency but when it comes down to it
it's not fast right when you put it
against a lot of these other servers it
really not fast 18s not in this list at
all you have to go all the way back to
round nine which isn't even shown here I
have to go to previous rounds for the
last time that they ran the Windows
tests cuz the windows test broke
unfortunately and you see asp net in
this round here it is a spin yet
straight hundred nine thousand requests
per second it's a good name
just pretty lame I think we got up to a
hundred and fifty thousand yeah in a
later thing and I've done you on Windows
oh so this hasn't run since and here's a
spinet not stripped down here seventy
one thousand where's NBC isn't here I
always in be seen here oh that's on mono
then we use mono on a server this is why
anyone this is why you don't any one
gentleman it's okay so we'd like to do
better than that in a minute Corp um I
remember looking at these results maybe
I don't know
March last year or something banging my
fist on the desk oh my god we faster oh
we don't care we've never cared before
do you want to care yes I'd like us to
be fast I'd like this to be in here okay
let's do it let's make it fast and so
that was kind of the beginning of the
journey and so I'll hold the number til
the end of the talk to tell you where we
think we're going to land in the next
round we're going to submit to taken
power spider based net core we're
working very hard on that now and we
have an idea roughly of where we're
going to land we have a number and we
think we know where we're going to land
for RTM and I'll reveal that at the end
somewhere in the middle of the thing
hopefully it'll be above the fold here
top of the foot you'll see us in the top
here alright so let's talk very good
with the very high-level concerns that
you have when building extremely
performant code in net especially with
the types of numbers that we're talking
here reducing garbage collection or GC
pressure is absolutely paramount okay
it's well nearly the most important
thing you have to worry about GC will
kill you if you don't think about the
memory that's being allocated when you
do are doing something a million times a
second then it'll just come round and
slap you around the face and say no I
have to take a break right now and then
your server will stop for four
five seconds and guess what your
throughput goes to a big fat zero when
that happens and then you can go back up
to allocating gigabytes per second so we
need to do a lot to manage our own
memory so I know dotnet is great if
you're building your WPF app or your
normal system web app or just not really
doing anything normal other than trying
to build a fast web server you generally
don't have to think about this stuff but
when you have to do this type of stuff
you have to stop just going all I can
allocate the garbage collector will take
care of it no you can't so you have to
pull memory okay and as we have a custom
memory pool that we wrote for kestrel
it's that lis MVC in a spinet core also
uses memory pools now which are provided
by system dot buffers yeah for pool yep
in order to do a lot of the similar
thing razor like razor rangering and
formatters you have no types of crazy
stuff so we've taken a lot of the work
that we've done in kestrel and reuse the
same ideas in other parts of the asp net
stack this isn't just about benchmarking
ok it's not just about looking good on a
chart it's about real applications that
render large HTML pages and making that
as memory efficient efficient as
possible strings are problematic strings
are very special in the CLR and they do
a lot of tricks to make them fast and to
make them sort of easy to use and to
have them interact with other things in
Windows so the utf-16 by default so they
just cast straight into what is the
thingy B string B string I can use it in
comm it's all goodness right but the
problem when building a web server is
that what is the HTTP 1.1 spec written
around well everything's pretty much a
damn string right you get sent a line
it's a string get some address some HTTP
version bunch of headers they're all
asking encoded strings so it's not the
most efficient protocol for sending
things across the wire and then to
represent those in memory a million
times a second if you're trying to do a
million requests per second you have to
allocate a buttload of strings in order
to represent the header and the verb and
the device and the query string and year
to year to year data and all that stuff
is immutable which means that once
you've allocated it you can't change it
you have to allocate it to use it and
then it has to get GC at some point in
the future okay so it's cost cost cost
cost cost
so the other thing is one of consuming
came coming data as soon as it's
possible because of the way libuv works
and it's this single loop that runs best
it's push based right it wants to push
you bytes as they come we can't just
wait for your app to say I'm ready four
bytes
I'll read the bytes because that'll
potentially stall the libuv loop so we
want to make sure that we're reading
stuff a delivery as it's ready to give
it to us okay as it comes off the
network card Internet into memory and
then into libuv and it says i've got
memory for you we need to be reading
that even if your app perhaps isn't
quite ready for it so that results in a
slightly more complicated architecture
that would otherwise that second line is
just some comps are rubbish we don't buy
not primarily focus on asymptotic perf
yes it's all event so it's fine that's
fine yeah what really matters is real
numbers we don't really look at thing go
well that algorithm is n log squared we
go let's just measure it and find out
okay yeah let's compare it to forty
other things and we'll pick the thing
that seems to work the best and as I see
when you're doing one microsecond per
request every nanosecond that you take
to do anything ends up you know
mattering and so we do a lot to do that
so this isn't this is just a random
screenshot of code why just here so
there's a bunch of bytes that are known
known values for header values and
header they're all values actually at
this point your value so if we know that
it's a caching closer keep alive we have
a hard-coded static by array payload for
those things we never allocate these
values ever for a request we all kind of
once once per servers like with the app
so we know there's a sub 1.1 there's
content length 0 there's rn rn there's
chunk to keep alive so for the common
pill that that that that we have in the
in the am server we hard cut a bunch of
bytes statically across the entire
server this is about reusing string
cashing it right so we know these never
going to change we're going to do a
bunch of comparisons against these we
can have to write these out to the wire
or read them off the wire so there's no
point doing string comparisons and
allocating your strings every time we do
this the web is byte thin strings more
strings
byte so it's kind of every web framework
on the planet does basically a super
complicated way to write strings right
strings a little socket at all it is
like the web is full of strings until
you download a file and then we send
application/octet-stream blah blah blah
but when you write a web page
everything's a string and so all the
crazy web forms rails know but don't
know what you do what framework you use
you're just putting layers on top of
writing strings out and strings are
expensive it turns out we have to do a
lot of tricks to make sure that it's not
as expensive as it would be so what is a
memory pool so we build this structure
called a memory pool and we shove it in
the large object keep so quick refresher
in dotnet garbage collection we have a
generational heap design all right so we
have a gens Eero heap which is the first
one Jen - Jen one zero Jen - and then we
have the large object heat and it's a
simplified version of how this works is
that when you allocate something under
the heap
it goes into Jen zero to start with
right and stuff goes into Jinzo goes in
Jin zero then at some point memory
pressure occurs and doctor goes all I
need to do some cleanup and it scans Jin
zero and goes okay well you're still
referencing their survival it survives
I'm going to move it into generation one
and everything that was in gender that
is dereference I'm going to free up
right get it off the heap it'll get
collected and then that continues and
you keep going and then eventually Jen
one runs out of space and they do the
same thing for Jen one okay then
anything that was referenced in Jen one
moves in - Jen - okay new yada yada yada
I go go go - and then in June Jen -
happens the world stops Jen -
collections are tend to be pretty bad
but server GC is a little bit better and
so then when Jen - happens ultimately
the things that end up in Jen - have
been referenced for a while okay their
long thing cymatics statics things that
took more than seconds multi radix I
mean if you're into is kind of a bad
sign generally yeah yeah although in
things like the client applications like
wind forms and as we BF a lot of things
end up in Jen - because you run the app
for a long time okay then there's a
special heap for things that are bigger
than what 80 something 45 5 some number
some magic number the dotnet decided was
a magic number those things always go
into the large object heap because there
are efficiencies when you're doing
collection of memory for things under a
certain size and when they're over a
certain when they get certainly a bigger
they go wow we're going to treat these
special we're only going to do they ever
collect a large object heat I don't
think so I think it stays rapid gets
comparator not in there defrag like
literally you can suffer from memory
fragmentation with GC like a thing that
I used to worry
and dose 5 I'd run because let's kite up
on my hard drive you have to worry about
memory when you're doing a lot of memory
allocation right so we create this thing
we stick it in the large object heap
it's called a memory pool and then we
pin it you stick a pin pennant and what
a pin in we're not pinning memory
doesn't net prevents it from moving
around so the other thing that GC does
when it does this little arm scan of a
generation is that after its removed all
the stuff that's not being used it
compacts the heap moves it all back into
contiguous memory space all right
that'll leaves a nice big empty block at
the end pinned memory cannot be moved
why would you want to pin memory well if
you've got native code that isn't
playing in this happy GC land and
relying on the pointers being always
up-to-date then you have to pin it so
that doesn't move while the native code
is working with it ok so when you're
doing i/o memory ends up getting pinned
because you alternate up to call a
native OS platform API in order to get
that flushed out to the wire or read off
the way yep so we take a big slab of
memory we shove it in the large optic
heap and we tell the GC pin this don't
move it I'm going to be using this for
the life of the application ok that way
the GC can go up don't worry about this
memory it's pinned move on to the next
thing makes it more efficient ok we
break up that large amount of memory
into slabs all right so we have a big
block of memory then we say one slab
here one slab here one slope here and
then those slabs are further broken down
into smaller blocks pretty basic the
slabs are there to reduce the number of
things we have to allocate if the rather
than just allocating a block at a time
because every allocation is a cost it
ends up being a reference we say let's
allocate a slab and then as people ask
for memory we sort of divided it up into
blocks all right
and this gives us larger portions of
continuous memory in physical memory
space
so therefore memory address here to
memory address here we have one big slab
of memory and then we can lease out
blocks from within that slab as people
need it as it come have to come and go
yep and the size is 4k for some reason
to fit pages what to pay the numbers the
numbers that we chose were based on a
Ben Ben Ben Adams dissin research and he
said he gives a big write down about how
this works
I was he used Camry in linear chunks for
the word size I'm for key with optimal
for
we measured debate atoms as a community
member who's done a lot of work on
Kestrel Rio and Kestrel and stuff and so
he basically read the stuff some stuff
and learned about highlighting some
details and then said do this truck yeah
and we did it as fast as faster it's
faster than what we had sorry for though
so do this measure and make sure matches
your own okay open so try this it's at
the very first request that comes in we
don't do this upfront we wait up the
first request to come in and then all
this magic kind of appears and laughs we
have memory to use now great we've got a
bunch of memory and then a next request
that comes in obviously just reuses
don't you don't pay that cost again and
you just reuse the memory that's already
there okay so that's good memory pool
pretty basic oh okay awesome
so who's used async/await right good and
you know how I think awake work works
basically you will wait some method that
returns a task right in net and the task
is magic does stuff does something now
gives you back a task some time in the
future the task will finish and then
magic happens it rejoins threads and
stuff and your code keeps going and it
looks like it was sequential okay you
can write your own tasks effectively you
can use the awake keyword with stuff
other than a task if you build a custom
a waiter and because of this design we
talked about before where we have libuv
which is running its own threads right
and then we have net using its own
threads like thread pull threads we have
to have a way of marshaling information
between Lybia V as bytes coming off the
wire and your code dotnet code right
running your application and so we have
to be able to say well Libbie V is
finished some work I need to do some
work over in net or vice versa net is
ready to do some work I need to get that
work over into libuv so we've got all
this hand holding and handshaking of
async work that goes back and forth
anyone use Irish before all right all
right reactive reactive extensions yeah
okay if you feel libuv is currently rx
you say start reading and it says here
are some bytes
here are some bytes here some bytes and
it's your up to say I have some bytes
I'm going to take and turn into a stream
alright so and.net their streams and
that is pull on oliviers push so we turn
a bunch of push calls into a stream by
by catching them in a bigger list of
blocks and then saying
the consumer can read these blocks as
they come in from the TV so we have one
thread producing and what they're
consuming this is this arm waiter so on
allocation we allocate a block from the
pool the TV rights to it and then we on
complete I guess if you hit next
the next complete on read we say it we
read this much bytes and then the
consumer can say I'm going to read more
bytes from that makes total sense yeah
yeah made sense right producer consumer
so we could have tried to turn this into
a stream which is what most people
in.net when they use these types of API
so overall I owe you generally get given
it as a stream right request up buddy
socket blah blah blah read now remember
each dream writers those are type of
things right and as David said that's a
pool API when you're ready to read from
the stream you call read you give it a
buffer you say please read into this
section of memory or byte array right
and then it fills it gives you back a
result that says I gave you this many
bytes go off and read that many bytes
and the thing that you gave to me okay I
also read I'll get a task per call to
read or read a sink allocates a task
every verse is saying here's a call back
coming back whenever you read data
that's one call back for the entire
server for all things right so slice
because the callback is allocated one
person ethics and it gets cold over and
over and over and over again there's no
data appears as opposed to read async
being a one-shot affair every time you
call read async that will return exactly
once it has to get allocated a task for
that invocation then once you've
consumed what you read you have to call
read a sync again and then allocates a
task completes it when there's more
memory you do that and then you call
read I think again because oh no no
whereas a callback
you'll get once and it just calls you
over never never it pushes it to you
okay so yes memory would have to be
copied if we did this yet every time you
call read we'd have a copied from the BV
ensure buffer copying is expensive it's
more allocations etc etc it would work
but we decide to do it a little
differently and this is where things get
a little gnarly okay so the socket input
class which is we're going to talk about
a little bit socket input is this thing
in kestrel that manages bytes coming in
off the wire and feeds them from the
boovie into your app it's it's a queue a
channel a link between the producer and
consumer so the DB is right into it and
the consumer is reading from it okay
it's a giant list of byte
and so just like ask you if you have
ever done any type of dispatch already
called distributed sort of async stuff
someone's publishing someone's consuming
let's knock it input is that yep okay
it's simply the thing about it and so
you get your socket input you will wait
it and then once that returns it's ready
for you to start reading yep right
episode then I say I'm going to start
consuming that data that's it gives you
an iterator that's iter
right because you start a special
iterator special iterator and it's magic
you'll consume your bytes over that
special iterator which we'll talk a
little bit in a minute and then once
you've finished it you'll say I'm done
consuming the bytes in this iterator and
I read this much data and I know this
how much I finished yet okay all right
so we got there so okay so then we have
so the producer calls incoming start to
get a block and incoming complete to say
on there's a memory pool with a block
there's a pool and it says incoming star
and your deficit you produce i'm
producing i'm the i'm libuv right so you
throw by Sammy you throw bytes and I'm
catching them damn you sitting coming
complete so your incoming start now you
send incoming - I said incoming stuck
yeah from the pool so you cool which you
gave me earlier pool right the pool is
this is a pool okay get in the pool yeah
all right get a block I got the bite all
right and you say incoming complete
write down and the thread is like
waiting here so there's a bunch of
memory just sitting here mhm and I said
I'm gonna consume okay so the Oh waiter
returns because it can please because
you you mean bytes right right and now
I'm consuming start read read read read
read parse my just chilling out our
sparse part no no you're late throwing
 oh there's more stuff coming in
throwing stuff right as fast as you can
I'm reading reading reading reading
little bitty thing I just keep throwing
stuff into me complete okay I'm done and
then you say okay I'm keep going so
let's keep going and at the point when
you say complete if I'm in the middle of
putting something here all right
synchronization it pauses and it's like
a ref throwing keeping and I says right
and then we keep going
then the consumer says I thought this
much and you're gonna say it doesn't
matter because I have more stuff okay he
keeps going over and over now ah so okay
so let's do this again a little simple
okay so some bytes came in cleaned up
right and then you start reading yep and
then while you're reading I give some
more bites I feel all is bites all right
and then you got back to the beginning
and went oh hey I come or I'm not gonna
fin
 cuz there's more bite since I
started see keep going okay then then
you say complete yep and then I put more
bites in after you've said complete get
it all starts again yeah okay
make sense so that the fast path is if
he is throwing bites
as I'm reading and when I say I'm done
reading if he's been throwing and I
haven't consumed all the by side and
read so far then then the await returns
immediately so never awaits if there's
more by since I have been reading the
first advice I am a sense so I keep
going pay the cost of going a sink you
know exactly like you stay on the same
thread reading off the hot cash over and
over and over if you throw embrace as
I'm reading I see and I've always that
the look in the stack trace at that
point it's almost like I'm just staying
on the same while the tight well like a
tight while loop that's important for /
so you can keep your method can keep
reading up bites off the wire yep
asynchronously to them coming in off the
wire correct right so you've got it is
actually happening in parallel I am
processing bytes while they are coming
in yep but I'm not paying any boundary
cost there's no sis call cost there's
nothing because the bytes are coming
into a memory block that is shared and I
can just keep reading in a tight loop
yep with basically zero costs while
that's happening that becomes important
especially we talk about benchmarks if
it's a little later on okay Zack clear
this might help this is a tool from now
on so imagine this is another diagram to
try and explain this a little bit better
all right all right so this is a block a
memory block out of the pool right the
bottom is like a couple of blocks
doesn't really matter if it's one or two
the matter doesn't matter the iterator
will go over multiple open right so it's
a bunch of memory that is shared between
the consumer and the producer
okay so there was Green as what was in
there in the beginning and I call
consuming start or incoming start down
here right I put the green blocks in yes
and I called incomplete see you wrote to
the alright so green was done by
incoming start incoming complete the
first line consumed start was called
here now at this point while this person
is iterating across here new bytes came
in off the wire and so incoming start
was called again and they append to the
tail of this block while the
is reading from the head of the block
okay so then I keep going like you're
going this person just keeps consuming
they didn't even realize that originally
when they started it had ended here they
don't get the next byte in the iterator
in the numerator basically and they go
next by it next byte and this could even
span blocks I believe like it even bike
spans physical spaces of memory so we do
we link blocks veiling lists so each
each block is 4k yep it's for killing
lists so the head and tail and then at
some point this code will decide it's
finished like it may not need the next
byte yet if go again I'm done here and
then this thing will stop when that
packet or that series of packets is
finished being read off the wire okay
and then that just repeats add finished
and forever for the life of a single
socket so this is all happening the
space of one TCP connection because what
are we reading off this HDPE stuff and
that's what we'll get to next so we now
we understand data how data is coming in
off my copper or by fiber in through the
little metal bits and through the
network card in through the buffer in
the card in through the driver blabla
blabla when Windows stuff Linux
Tunnock's then up into shared memory and
net through libuv
and then we've got two things one coming
in here one coming in here and they're
moving cursors room okay super fast now
we've got bytes what we going to do with
them person we're going to pass them
remember strings as we said before
magical and everything in the web is
strings most the time but strings aren't
really magical that is bytes right there
just bytes with a particular encoding
who's ever had to write
really understand to do anything with
character encoding like text encoding
only a few hands otherwise people it it
hurt it hurts doesn't it it really hurts
like ASCII I can get my head round good
got it
anything beyond that and I design I'm
gonna call that circus characters then
bite all the marks and unique a double
with triple with ribbon little bit item
so at least in HTTP all the headers need
to be one one anyway
all the headers are asking so that makes
it a little bit simple but eventually
there's a bunch of bytes coming in and
we know that there are some special
bites that delineate points in the life
of an HTTP request so for example the
end of the headers or end of each header
is marked by a carriage return line
feed right unless there was an escape
character before and then the end of the
head is is marked by two carriage return
line feeds all right it's pretty basic
actually so is if you hit enter twice in
the terminal window where you were
sending your HTTP request because that's
what we all do right and then you send
the body and somewhat you've told it a
deport eighty at some point right yeah
typed in get space blabber Blair and you
make a request so there are very
efficient ways of looking for those
things so that we don't have to do the
naive thing which is just go byte by
byte by byte by byte these are very well
known things in HTTP right there's a
connect these are verbs HTTP things and
verbs and versions and blah blah blah
blah so these are actually the verbs
right these are the very first things
that will appear in any request that
comes over the wire connect or delete or
get or head or Pat or any of the verbs
and then we know the version will be at
the end of the first line before that
first carriage return line feed ok these
are well known we're going to reuse them
over and over and over again what's
interesting about these strings is that
they're all eight bytes or less just
luckily okay for the ones that we know
about you can have custom verbs but the
ones that we know about that we care
about the most or all lesson eight bytes
which means that they can all fit into a
long you can represent every single one
of these strings hmm as a number you can
probably see where this is going so if
you can represent the more the number we
can start doing some clever tricks to
read and interpret these bytes off the
wire without ever having to treat them
as strings because remember strings
result in allocations and dotnet which
results in GC pressure and doing things
with encoding and comparison is
expensive dah dah dah dah dah and death
right what and that yeah that is GC
beaucoup you alright so when we pass a
request start line we're going to read
the first eight bytes as along about
that much okay this is good because at
this point I know what to get but I
don't know what to get till I actually
compare those bytes to something that I
know to be a get we check against a pre
computed LAN for no method names are we
going to show this code I think it's
various crazy and then when we if it
matches one of those we just reuse that
string rather than allocating a new
string forget
take the existing string and we stick it
on the object so that when you type
request verb that string get hadn't been
allocated for that request it pointed to
the one that we statically assigned in
the Kozyrev onst all right got it yes
they all said one say yes if you include
thee
now they're all your bites yeah
yes yeah we will tell you show you how
be helping our family avoid that we
avoid the bug the potential bug so like
you know basically every character is a
bite that's exactly right and so eight
bytes gets us every dance is a seven
right so eight fits every single one
here but none of these are eight right I
think the biggest one is seven
no ACP 1.10 is another it all these are
eight right and so eight bytes equals 64
bits which is along in net and so we can
represent every one of those as a long
let's just point it out this one isn't
that like this is has other stuff in it
so how do we how do we mask and compare
so that that doesn't happen
well this is how this I'll let you read
it for five seconds okay let it thinking
so this is statically assigned this
represents the long the number that
represents the string that the ascii
spear with a space followed by four
nulls guys that slash 0 is a null ascii
string right so this gives me back a
long that turns this into bytes which
means cast to a long and I store that
long that long represents that string
okay got it
beautiful then we create a mask to match
that long that we can use to do a
bitwise operation between that string
get space HTTP and this string all right
so we have three bits of data we have
the comparison string we have the input
string then we have the mask three bits
of data and the result of that is this
beautiful witch
oh my god so we have an iterator which
we talked about from the memory pool we
peek a Long's worth of data from the
memory pool block which is our value we
bitwise and that don't ask me what a
bitwise and is I'm a web developer and
but I know that's what that character
means with the mask that we created over
here right so what's that that's a bunch
of nulls and a bunch of not know odd
bits that all bit set right put that by
all bits set for that bike right and
then if that is equal to this gets piss
right get space then we know that that
input was started with the word get okay
understood and we do that for every
single one of these known things okay
and then we even have some other code
that before you even get to this I think
compares lengths and things and does all
that's for the head of a yeah yeah okay
all right good all right so I talked
about peak there so all memory blocks
are read by iterators we have a peak
operation which you can peak a certain
Langille bite yeah right a peak in this
case it peaked along right give me a
Long's worth of data
what does take to move it to move it one
so peak is peak one and but don't move
the cursor cursor okay
take his tick when I move the cursor
okay and then sequence crazy passes some
stuff and that's what we're gonna do
more than what we're gonna do next yeah
new stick next okay I'm gonna stick okay
all right so hold on hold on hold on
this I you drink of water
all right all right
seek do you want to drive this no okay
well no okay so we're just talking about
the start line still think hard by the
way this is okay focus
so remember this is the start line
that's a start one that's what you see
the first the beginning of any h-2b one
one request is this line followed by a
carriage of ten and then the headers
start okay all right so we call it
wonderful times to consume the start
line and I guess that's because of the
size it can be either 1 or 4 depending
on how big the URL is like suppose it's
later this late says four so I believe
yes the beds that were the slide 5 so
header finds the first kind of elements
of 1 2 3 of any given character so you
basically seek for this right yep and
then it will tell you whether it found
that in that part of the memory pool ok
and if it doesn't then you go on to the
next part of the memory pool and call it
again that's why it's n times 1 to
whatever many times
for example find spaces find new lines
find question marks because if we're
decomposing a request we care about
finding spaces here and here we care
about finding new lines because there's
a new line here and there's a new line
after every head up and we care about
finding question marks because we might
want to split up this URL between the
quarry string you know the query string
and the first part of it okay
so we'd like to be able to say for a
given set of memory find that remember
we're doing this all to avoid turning it
into a string and then calling index off
because that's really simple and you
should totally do that in all your apps
now all of them do this catalogs mask
the property record and it's like
compared with ends and then assure your
job security yeah by having no a no
comment has no comments either that's it
and then to make it even faster and I
make it even more complicated yeah and
we have to explain this now we use
vectorization and sim D or single
instruction multiple data operations in
the CPU to allow us to look at more than
one byte at a time in a single CPU
interconnection
okay and I'll explain a little bit about
what that means in a minute we'll take a
look at one character version for
simplicity okay let me just talk about
vectorization very quickly so I had a
thing in my computer he started Eddie it
was on the Intel website it was this
really good stuff yeah yes nasty oh
sorry not XD you're thinking about forms
man I am here we go
this one so advanced vector extensions
are everyone remember was it SSE to see
SSE Pentium 2 all right yeah
and add the multi oh and MMX multimedia
extensions that came with opinion to
memory I was big bad baby that's what
gave us the first 20 frames per second
postage stamp video over the Internet I
remember this style do you Oh old much
older than you
AVX is kind of the newest version of
value up to AV x3 now right these are
basically extra CPU instructions that
are built into the CPU to allow you to
do super advanced stuff if you write
your code in assembly in dotnet you use
the numerix namespace system dot numerix
which is a package and you
you'll use the newest jitter the Riu jet
which came in air for six or in.net core
and it can emit byte code that uses his
stuff so what this basically means is
that with the SSC and evx 128 128 bit
types you can perform operations on sets
of data that are larger than the
registers register size in the CPU write
make sense so the register is a 64 bit
we have CPUs now right okay yeah right
but now I can do 128 bit operations via
these special extensions awesome or 256
with avx2 and I think it's 512 with a
3x3 and I think the architecture scales
all the way to 1024 bit and so for
example we can fit four floats into a
single instruction or two doubles or
sixteen bytes is what we care about
because we're operating on bytes and
Long's
okay so we care about bytes and I think
along is the next one above this so it's
too long right it's right there oh it's
right there but I could just like read
it down here 64-bit in 64 is along right
so two Long's or sixteen bytes I can
operate on sixteen bytes at a time
rather than looping over every byte as
it comes off the wire that's what it
comes to a super saloon what's doing it
everybody at a time for loop sir if for
loops are easy to understand but not as
fast okay so what does this look like in
actual actuality here is a series of
bytes with hello space world and a
carriage return line feed after this is
meant to represent a single header all
right we're - making it very simple here
right this is one header line and these
are the hex values that represent every
character or every byte in this string
okay let me go okay here is our here's
the bytes that represent 1 vectors worth
of data so the vector size is different
on every CPU it's called the stride
length writer I think the stride I think
we caught the stream and on my machine
for example it was 32 and on yours it
was 16 dinya parallelize parallelize I
didn't know it was and I think I'm even
more modern machines it you also might
be high it might be 64 even hundred 28
if you have a Xeon and so we take that
much data we operate on whatever the
means vector sizes at a time so in this
case for the example this is going to be
8 right and then we have this vector
here which is our mask going back to the
first example right / RS and this is
just a bunch of / R's 0 D is carriage
return all right so we have our value
and we're looking for carriage return
and so we just have a full set of
carriage returns then we say is this
vector equal to that vector and then you
get back as the result a third vector
with bits or bytes set that tell you
whether each byte in the input and the
comparison were equal or not make sense
so these are all 0 because none of these
were equal understand and whereas this
one see these two are equal none of the
other but these two are because this
one's a carriage return so the result is
a hit green hit good we know that in
this 8 bytes right there is something
we're interested in so rather than
having to loop through and that would
have taken us 1415 operations we did it
in 2 so we've gone for 15 to 2
straightaway to find the carriage return
good right that's what vectorization
does for you if you're on compliant
hardware that can do that all right
huh and this was a pull request but this
was a pull request yes just so you know
that split I don't we go then we go to
the next step okay thank you thank so
all we've figured out here is that this
block contains some of the thing that
we're interested we don't actually know
what slot it in yet
it says green here but it's lying we
actually know that yet all we know is
that one of these is a match we know
it's not zero we know it's not zero
right we don't know which one it is
we know the whole thing is not equal to
zero it's more than zero yeah right
because remember this is just a long
really does a number right so now we
have to figure out where which bit which
bite in this set is actually the one we
care about that's what this method comes
in you're going to explain this one know
so so so it's kind of like a binary
search you don't know it okay hit the
arrows are you anyone know what a binary
searches yeah okay good good I'll do
thank God
not everyone you don't have to know
whether have no other binary this called
a method on a radar binary search in
your phone okay
so sorry that's lovely we need to loop
over the entire vector and just find the
one that's nonzero that makes sense but
instead that would be too slow whatever
that we take me seven in this case
operations yeah what if we could just
mask our way to success and keep going
so so first everything see how the right
hand side of this mask is all F and the
left hand side is all zero my simple
more amazing what the mask that's a mask
right my simple mind says well okay
though that basically says is is the
thing we're interested in in the right
hand side and if it is it'll be greater
than the other way around right because
it'll say no and then it jumps down to
that the other statement okay there okay
well we know it's in the first half
right we know it's in all these zeros
which is backwards from what we're
showing down the bottom yeah for some
reason so then we go to the next half
which is this set here because we know
it's in this first set so now we're
going to do the right hand side of that
first right and in this case it's going
to be true no it's false again else
because I got that wrong because I get
this bad as the lazeric retina versa
that's fine right besides I'm sure
correct so now this is a worst case
search actually yep so we've now done
what three operations we would have done
seven to find it's awful so now we know
it's in one of these two so it's either
going to return us six or seven I don't
know why that's good cuz reasons and
then F something six so we found it yeah
okay because you can see like lots of
F's then we jump down to here half a
number of F's jump down here another
half the number of F's but it wasn't
that one it was this one so we know that
were there yeah final research half
every times good good my web developer
brain can understand that so this point
you're thinking come on is this really
worth all this insanity well then with
this test probably not so like if you
just if you just isolate this one little
test this is what we got when we render
through benchmark now the naive thing
which is just looping through every byte
took 893 nanoseconds
with a standard deviation of 61 the
median was give quite a bit faster 654
nanoseconds but look how much tighter it
was because the worst case now is so
much better than what the worst case was
for the naive thing right because the
naive thing is you might sometimes it
the first one or the middle one or this
one or sometimes it the last one whereas
with the binary search the worst you're
going to do is what log n log n right
and I'd pretend like I know what that
means log in next and that's the naive
version just loop over the bytes right
really really simple easy to understand
easy understand what most people would
write is that who this in your that's
what you should do unless you have
reasons not to but when you amplify that
up to actual calls to seek with real
data like 4k of data you give it a full
memory block of data is coming off the
wire and then you say I've got to find a
carriage return in this 4k of data now
it goes from taking four seconds on
average right or four sorry 4000
nanoseconds which isn't quite false okay
yeah forth by a clock by a couple orders
of magnitude it takes 4000 nanoseconds
average versus 863 nine seconds alright
we'll end the standard deviation again
it's much much less so this adds up as
you start using real data which is why
you do micro benchmarks as well as
bigger benchmarks all the way through to
full end-to-end testing okay crazy stuff
to get crazy okay you were doing this
one really no I'll do this one you want
to do this one after you'll stare so
there's some yellow hang on hey nice and
green please ever looked at the
waterfall loading chart in like the
chrome developer tools right oh yeah
good good comparison all right good and
so you know that the browser opens like
six connections to the server so if you
request 20 images you'll see six loading
in parallel while the other requests are
all sitting there waiting pending
pending and then when those six complete
the next six will start you get like a
stagger diagram same thing this is
exactly the same thing but we're talking
about you writing memory out to a socket
and they're actually being written to
the socket because a slider hand going
on here so we lay to you when you call
response dot body dot write async okay
because it's IO right so it's supposed
to be async that's what we've been
drilling into you all since dotnet four
and
also dot there four or five yep we lie
to you a little bit because it's faster
if we expectedly fun angling its
effectively it is Negley but we do it
asynchronously yes listen it's yeah the
right ish yeah let me just explain yeah
we'll see what make sense okay so the
red is when you call right alright so
you're calling right you give us 16
kilobytes of strings you'll raise a page
calls right ultimately you may not call
right but you types and raise or net
calls right the yellow is the task that
we give back to you as a result of
calling right async okay so that
represents an outstanding operation and
left to right is time so they say yeah
you're fine you're done so this one is
marked as completed right is finished
and it's in quotes because it isn't
really we told you it was completed what
we did is we stole the data that you
gave us and we shoved it in memory
listen and then we wait a little a
little while so little bit a little bit
and then you can see here that we
actually then say libuv we have data
remember that pub/sub thing it works on
the way out as well libby v we got data
for you oh so you're lee harvey right no
I was a beauty before so you're the app
okay and now you're throwing data to me
16k hey and then I say I'm done oh wait
I turn around I go but really done quick
quick wait for you yeah what good no I'm
riding out I'm mowing event okay yeah -
late you're having me stuff an alley
craps and I'm telling you that yet I'm
done
give me your data and I'm not really
neat not really complete complete and
that keeps going completely that's the
first four calls because we have a
buffer of about 32 kilobytes we'll let
that go on all right until you call
right async one more time and then I go
I can't keep up on I'm sorry
and I need you like Lane and I give you
back a time stop that is marked
incomplete the arnold one i say look
giving me too much data
i have to wait please stop app dot and
then usually go please if you're being
well well well behaved if you call a
wait if you just call right a sink
without awaiting it and ignore the
compiler when it screams at you say go
wait you're cool don't do that please
there you just blow memory up right
forever
because that that clutch gets broken
that safety mechanism goes away all
right so now I've given you a task I go
right ah I've done then I go done tell
you I'm done and I sit here and then you
give me more the next call yeah that's
this one down here right yep and same
thing happens I open over and over again
okay so that's how we write data out to
the client it's decoupled from you
calling right and that's called right
behind yep
right I think no does the same thing it
does it does right that limit I think
it's just completely arbitrary and you
can't change it you can't change it so
don't ask maybe in the future we'll make
a comparator we'll make it you know plug
a figure below or not don't know why
this slide is here but the benchmark
biplanes oh the benchmark does pipeline
so I did mention before that this is a
lot of this is very worthy when you're
doing benchmarking so every browser on
the planet doesn't do this but HTTP
requires that you support it and so what
it means is that if I am a client
talking to a server I get one socket one
connection right let's just focus on one
let's not worry about the six parallel
or anything like that one and then over
that connection I send you a request and
then I wait until you send me the
response yep and then then when you've
sent me the response I send you another
request and it just ping pongs back like
this that's without pipelining
pipelining says I'm gonna give you three
requests like at all and then I will
wait do this and then you agree to give
me back three responses in order in
order yep and then once I've received
those three I'll send you another three
requests or two or sixteen whatever it
is right and how many that is is what's
called the pipelining depth now browsers
don't do pie plate I think opera had one
build where they had an experimental
version of pipelining and they turned it
off because all the servers in the world
broke my opera technically to support
HTTP you have to support HP one one
requires that you support pipelining and
the taken power plaintext benchmark uses
pipelining it's the only one that does
in their full benchmark suite because
that benchmark is designed to test how
efficient is your server at reading and
writing HTTP we don't care about being
nice and waiting for you we're going to
give you sixteen requests at a time and
you better be fast enough to deal with
it all right that's why the numbers on
the plain text benchmark are so much
higher than the next simplest benchmark
which is adjacent because it doesn't
pipeline so that thing we talked about
before where we're basically doing a
while loop on the same
on the same thread and we just keep
reading from memory as it's being
asynchronously piped in this is why that
matters when you have scenarios where
data is coming in off the request off
the socket and the app is processing it
while the data is still coming in
pipelining is a great example where that
happens now if you're doing large
uploads it also happens you have a large
request you can start reading the body
while the body's still being strained by
the client okay if you write the right
code given so you'll still get the same
benefit if you're doing things like file
upload all right all right how do we
coordinate libuv thread in the
application thread with magic and with
smarts people who are smarter than us
solidity is very specific structures
libuv you have to run all the code on
the UV thread so we have a bunch of
threads in system one per core by
default well ice one per half number
cores because that was fast for some
reason I think I'd reg halt to mod 16
divided by 2
what I don't know no looking it's 1/2
numbers you don't laugh it's hot the
blacks less for the max of 16 divided by
6 yes there's some threads that run the
do your change that that one is good
figure we can figure it out yeah and to
do anything a write or read you have to
be on that thread so when updating code
calls write async from a top thread pull
thread get to actually hop threads from
the main thread it's just like just
about different voting WPF yeah right
just like getting on to be your wife
right come to the UI thread leap UV is
the UI UI thread and everybody's been it
is your background worker if you have to
dispatch back or some work yeah right
door right and it Dobbins yeah so we
have a bunch of things to make that
easier in our in our stack we post to
our threads whenever you do a write or
use all these crazy types like I mean
you've all use a lock statement right
but the monitor don't try enter to do
stuff and blablabla but that if it fails
we do more stuff whose work item on the
background v this plus block um same
with memory box when we finish using a
memory block obviously we have to give
it back to the pool and we do try and it
in a very safe fashion we have checks in
our code so that if we if anything ever
gets finalized rather than getting
returned to the block we crash the
server get when it's in debug builds I
think so we so I don't know we do that
in the release build right no don't
I mean no we don't we might accidentally
be pretty funny now and we all rush the
bad No
so when we run it with a debug build we
can find mistakes if we are actually
leaking blocks all right yeah
I blocked little UV thread so this dis
architects auto boat we did we thought
about running your code on the UV
throwed nard runs your card on the Libby
vase right
but nothing is synchronous in node most
things aren't synchronous a node most
things are something that your code
might be synchronous we thought about
doing this and then we thought dotnet
people made like dude wait and then
servers dead the entire circle the
entire server the entire server the
Intel eight not just your thread the
entire servers did not that request no
more call the whole server right that's
how bad it is so we run your code on
triple thread and we post work to the UV
thread on our cadence not yeah control
by you so we do the work to make sure
that libuv thread has never blocked you
don't have to worry about it but we did
contemplate both designs and we chose
this one alright we've only about five
minutes left so let's run through this
one because this one's pretty crazy
quickly finding known B headers from
bytes
this just shows us the cases it's arson
are you wanna show some code yeah just
wanna do that let's show some Libya we
do a lot of crazy stuff to generate no
one header fields oh my god my computer
is like dead after login
all right no mass another guy never go
alright that good size beautiful temple
brains all right so we saw in Damien's
super ghetto server that he wrote
they've been for me this morning this
morning if you wait for morning haze
phonetic also a bit of a inform server
that implements features having a
feature allocated seems stupid so let's
just stuff all the types statically
across the entire server because type of
is super slow don't you type of in like
for request code because that's horrible
the features actually all implanted on
the same object so we have to send to
this and this reset this assigns the
feature to this object to reset them by
they were before um it looks very
repetitive yeah so we hire a guy that
writes these if statements and correct
something it's all code gen we don't I
just go to generate a lot of this um is
that yeah so you know how in c-sharp
sticks you can interval it strings this
is kind like a razor but for a c-sharp
so we interpolate and generate a whole
bunch of stuff for known header this is
our it generates code this is the code
that we use to generate the coding
kestrel that knows about headers so meta
ones that we use has meta code right so
here it is season own head and on header
that we all of his heart with a bunch of
header and this generates all those
crazy masking comparison stuff right
using Co top string interpolation it's
great yeah that's just as understandable
that's yeah so dictionary it turns out
pretty slow I mean not for normal use
but for this so what if when you called
try get value it was a field lookup
instead of being actually a table
dictionary lookup cuz hashing is super
slow right so like why even bother
hashing about hashing was fast
uh-huh for you okay not for Castro not
if you asked for a numeral we actually
conjure up one on the fly cuz it's part
of the contrast we wanted to cut the
contract alright more code so to know if
a header value is set so the interface
for headers is a higher dictionary which
is a regular regular dictionary internet
that has try get value and you know
count on those kind of things
but it's two sort of dictionary so what
if we just stored the fact that if a
header was set the bit was on if it was
not set it was off so we store a giant
long for all the headers that are known
and we just do these checks header is
set so we said before that we stored
individual values is numbers nine string
yep you're telling me that there's why
every header that we know about all the
way that represented by a single
sing-along yep great what do we do with
that single number we just check to see
if the bit is so we have a bit preheader
this is that is for that header so cash
control is one header question is to etc
etc etc etc okay if you set a header
it's a reference to this field and if
you get a header same same thing
oh so we store one giant long that has
all the bits sets like a giant array
that has wasn't zeros okay so for each
one there's some hair that we have
mapped to some I say field I see um and
let's Foggs actually let me do this okay
to these okay I think I do understand it
these turn into offsets inside that
giant no no and then I get bits from
there and I've no letters it 11 close
not even those so does all this does
okay it's kind of like if you if you ask
for this specific header yes we know to
return either the field or I'm not set
based on if the base or not
I see like Oh got it the bit for that
for that header value okay this file is
wait what yeah that's better 10,000
lines of Awesome all right so let's look
for the next piece of magic if I had to
step through this code no I think it's
header values I'm kind of impressed that
you can navigate it on its head yeah
it's pretty impressive
I never get bigger and bigger because
lungs are like super long um what was
that huge case statement facet to try
get value for a dictionary wouldn't it
be awesome if we didn't have to look for
Strings unless we knew the length of the
key was when we knew for our known
headers so 413 we know that
cache-control is 13 long and content
range so don't compare strings unless
the length matches first that may
actually faster yeah okay probably and
if you
see that phone exception if it's not
there because that's what you're doing
dictionaries there's another thing yes I
swear it's in here so I got one more
thing oh I get one more one more
everything that's in a pull request
what's that look at the PO request now
because the pull request is super
awesome the new pull request that oh my
god is freakin insane
that's not c-sharp there's too many
asterixis and X's in the numbers dude
I'm like hey Ben I'm a web developer so
it turns out we realized that most of
our output buffers we actually copy a
bunch of stuff from us so we actually
allocate for online headers one giant
static byte array for everything I mean
you say responds all right if a header
is set we said that is section 417 in
this header bytes array it's an offset
yeah offset and lent for each known
header flip that out but it turns out we
can actually just put the constant
longer represents the payload for known
strings so HTTP 1.1 like this string is
this text so so and it's 5% faster for
the benchmarks so instead of copying the
bytes that we pre-allocated right so
like h-2b one instead of copying that to
the output buffer so I can get flushed
out yep you say car has to that constant
number which I know equals HP 1.1 yep to
a long pointer and then dereference it
and then assign the value to that value
it's a move versus a cop and that value
is pointing to a space in the memory
block yeah because why call a function a
just s fine too like memory right there
how did someone find that measuring
measuring measure measure measure
measure measure and then leak but there
wasn't like a JetBrains
like a profile I said hey you should
cast an unassigned this is a long point
yeah really 5% faster jebra said you're
spending a bunch of time copying stuff
how dare you copy stuff ok how do you
use mem copy ok yeah that's low but I
have memory here I need to copy it to
here I thought the logical thing to do
would be to call mem copy but whatever
pausing to along with they're all non
strings where's the stranger like fixed
and never change my copy
okay because there's a string and I need
to get it there to flush it out this is
unreadable game so it's better okay so
that is it we are at a time I hope you
we've peeled the layers a little bit
here and you get some appreciation for
what we didn't do but some crazy people
on our team and in the community which
is the really big about this is it ban
and Thomas and a bunch of other guys in
the community have done some amazing
work on Kestrel and continuing to when
we're never going to finish the number
that we're looking to hit for RTM is
five million requests per second on the
big RTM and on the big iron we've
already hit back so in our big perf lab
on the 48 the 24 core server 48
hyper-threaded we can do over five
million requests per second for the
plaintext benchmark which will submit to
tech and power for the next round and
we're not we're not satisfied by nettie
does eight million I think on that
server so we want we're going to keep
chasing we want to get this as fast as
it it's impossible be yeah faster so and
not just Kestrel but not just Kestrel
but MVC and razor and EF and all the
things we're trying to make as fast as
possible so I hope you appreciate that I
certainly do should copy this good and
you should copy all this code into your
wind farms app and no comments and make
your button clicks lightning fast and
with that I thank you very much and
we'll see you again next year buddy
Creek</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>