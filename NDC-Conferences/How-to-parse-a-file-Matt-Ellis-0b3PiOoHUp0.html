<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to parse a file - Matt Ellis | Coder Coacher - Coaching Coders</title><meta content="How to parse a file - Matt Ellis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to parse a file - Matt Ellis</b></h2><h5 class="post__date">2018-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0b3PiOoHUp0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay then I think we might as well crack
on um thank you all for coming this is
all good um you do realize a Rob
Connery's down in door room on he's
doing a talk about Cassini and space and
all that kind of so yet don't blame me I
would too but sadly you've got me and
it's okay we're gonna have a bit of a
talk about how to pass a file gonna have
a look at some sort of structured
parsing kind of stuff and some things
that maybe you've had a look at but
maybe not since University or something
if you haven't looked at all um and
we're gonna start kind of nice and easy
and simply and say don't don't pass
files it's not worth it it's a this
isn't your problem this is somebody
else's problem we've got lots of
general-purpose programming languages
it's not your speciality it's not your
business it's not your the thing is
gonna deliver value to your customers so
generally speaking don't pass fast use
XML use the animal UCS for use anything
don't do it yourself that's my top tip
so plenty of time to go and catch Rob
Connery is good but finally if you do
want to stay and everything we can sort
of say why would we want to pass a file
there are still some reasons for doing
the parsing yourself and I can think of
a handful really the kind of the first
one really I think would be sort of
speed or efficiency perhaps the
general-purpose parsing you've got isn't
fast enough for your use case perhaps
you've got a really big file which takes
a long time to parse all we lost in lots
of small files which is then uses a lot
of memory or something you know so
that's a good reason but you got to sort
of think about the trade-offs that are
involved the maintenance cost that
you're taking on by writing your own
parser and building your own parser
another reason might be to reduce
dependencies perhaps you're working in a
resource-constrained environment you
know if you're a mobile or something you
don't to take on another dependency on a
general-purpose parser and again that's
useful thing custom or simple file
formats if using a custom file format
you don't really have much choice you
have to pass something you have to do
something with that if it's something
nice and simple it's possibly quick and
easy enough to to pass yourself and
that's a good going but similarly you
might want to then sort of switch to a
more general-purpose language and just
use off-the-shelf components as a way to
doing that the next one's a really good
reason I think is something that isn't a
file that isn't something that can't
really be represented nicely in a
general-purpose language something like
a DSL a domain-specific language
and there's plenty of sort of examples
of this you know things like make files
so make is a good example there you know
that's got its own custom file format
but then there's sort of the flipside to
that you've got things like cake which
is just c-sharp you got fake f-sharp
rake which is there's a theme here isn't
that this Ruby so you know you again
those don't need custom parsing that's
just a using a language even Gradle is
using groovy the things like a
command-line options HTTP headers
perhaps but again there's likely
general-purpose languages sre libraries
out there which you can use for that
kind of thing parsing standard out is a
good one if you just want to get the
output of a file and see what's happened
capturing the output and handling that
or other things things would like might
be a natural language command so
something which you can just type into a
application straight on so you track for
example issue track you can just type
work ten hours and it'll add it on it
knows what to do with that kind of thing
the other reason though is is kind of
where I come in it's it's where we're
just as interested in the structure of
the file as its contents so not just the
the actual date of this in there but how
is how its structured how its formatted
so we'll just whine back a bit and this
is me hello that's what I looked like on
the internet work for JetBrains I'm a
developer and developer advocates and if
you don't know jetbrains we build a lot
of ID's if you've got a language there's
likely an IDE for it and the way we
build IDs is the interesting bit because
we build a parser for every single file
type and we understand the structure of
the files and we use that to build
features we can use that structure and
the semantic knowledge from that to
build navigation seems like highlighting
refactoring inspections and so on
and in fact if you kind of did a back of
the napkin architecture diagram of what
an ID JetBrains IDE looks like you'd
come up with something like this you've
got a base platform at the bottom which
is kind of like fire handling UI that
kind of thing project model wife are we
working with lot of compilation options
and I've got this nice section here psi
program structure index that's where all
the fun stuff happens we parse all the
files we get our semantic and structural
information we cache it and read all
that kind of thing and then and this is
the key bit
big thing at the top features that's all
the features we have those are all the
inspections the quick fixes the
refactorings the navigation everything
and the key thing is that it sits on top
of the psi it's really important so if
we have a PSI if we have this structural
information of the code we can build a
whole load of stuff but the flip side is
if we don't have that we've got nothing
so it's a it's an interesting sort of
model that's going on there and where I
come into all of this is that I've been
working on the unity support in rider
and unity for ever know what unity is
yeah brilliant
it's absolutely massive it's powering if
you've got any game on your phone good
chances are that it's written in unity
it's got its own file format for shader
support for doing things with the GPU
and that's about as much as I understand
it things with the GPU and I wanted to
add that into into rider and resharper
and once you've syntax highlighting for
that and code folding and all these
kinds of things and to do that I need
that psi level and so I wanted to pass
shader lab files and it ended up being a
lot of fun and so I thought right if
it's a lot of fun and interesting to me
and a lot of interesting challenges
interesting problems then I'm gonna go
and tell you that as well misery shared
it's great so what am I trying to build
what do I want out of this I've already
said that I want to know about the
structure and the syntax of the file and
we end up with a tree like this which is
going to represent all of the data that
we've got I can show it to you in in
real time on the Left we have a shader
file from a unity project
there's no syntax highlighting in this
one unfortunately that's the way it goes
and we kind of have blocks at the top
which describe everything we can have
properties here with colors and so on
and it's a sort of a structured file
format it's a sort of hierarchical thing
and we can map it on the right-hand side
there to a past version of the file and
so this is our sort of syntax tree that
represents everything we've got here so
we have a shader lab file at the top
that breaks down into a shader command
and inside that we've got a keyword for
the rich maps to shader and then shader
values and it kind of drops down and
goes in sort of hierarchical order from
now and given this information we can do
lots of fun things so for example we can
reckon
colors and highlight them and do fun
stuff like that so how do we do this how
do we build this how do we pass a file
in brackets for an IDE so that's my use
case I want to build it for an IDE
um by the way if anybody has any
questions any time please shout down we
can address those so we can start simply
we can have a naive parser which is
incredibly naive and it just kind of
walks through each character in turn and
sort of looks at that and compares it is
it an S okay is it an H is it a D er
okay of match j2 brilliant now do this
and yeah fine it'll work and everything
but you don't to do this this is not
well architected not well designed not
useful not maintainable it's terrible
what we want to do is is really TechEd
it we want to split it up a little bit
split up the responsibilities we want to
be able to read our individual
characters and identify the specific
building blocks of this file format
things like keywords identifies that the
punctuation of the the operators or
semicolons the open brackets and so on I
know once we got all that we want to use
our information to recognize the more
complex constructs the blocks the if
we're defining a property the sort of
the name of the property and the value
of it it's a color what type of color it
is and so on and then do something with
that so is that what we want to do
depends on our application for for
myself as an IDE so I'm going to be
building this psi low it layer from our
architecture diagram but if you're doing
something else you might want it to to
write out values at that point you know
so you're streaming things as you're as
you're passing your file or you might
want to sort of save that information
build a new different kind of data
structure to save at the end but we want
to split up these responsibilities and
do something with it
fortunately somebody's already done this
for us somebody's already got this
architecture set for us and we can go
back to the archetypal example of a a
parser and look at the compiler the
compiler is a very structured parser
that we've got here and it's not us so
the compilers kind of split into two
parts a front end and a back end and the
front end is the bit which we're
interested it does all the actual
parsing the back end is the bit which
generates code and
does all fun stuff like that so have a
look at the front end bit this actually
maps nicely to what we want to do in the
IDE as well so that's also useful will
be split into three things we got
lexical analysis syntactic analysis and
then finally semantic analysis which we
can then use for our parsing we got a
lexer a parser and we end up with the
result in our case in my case sorry the
result is going to be some semantic
analysis so what Alexis and parses so
lex's we'll start with that this
performs lexical analysis and lexical is
something that relates to words of
vocabulary or something so this
recognizes key words recognize that
identifier it breaks down a big block of
text into smaller chunks which we can
manage which we can do something with so
it's going to take our file is gonna
take a big block of string of text and
convert that into a stream of tokens so
we're going to generate a token to
represent each individual thing in our
file format and I can be an identifier a
comment it can be whitespace it could be
string literals or it could be any kind
of operator or punctuation
I want to key things with Lexa's is that
our tokens are lightweight generally
it's just an integer value all you need
to do is be able to identify this is a
comment this is a bit of white space so
there quite often just an integer value
in my case with resharper in the IDE
they're actually singleton object
instances because it gives us a bit more
information on there but once we got our
tokens passing them becomes a lot easier
because all we need to do now is match
instead of having to sort of figure out
what this block of text means we've just
got a list of tokens we just have to
match against these tokens so for
example with an expression if you had
integer followed by the plus operator
symbol followed by an integer you know
you've got an expression so that's a lot
easier to pass them the digits 1 2 3 + 4
5 6 and because of this then Lexa's can
actually be harder to write than pies on
the pies it can be a lot easier because
you're just pattern matching over this
stream of tokens so what do we get out
of a lexer so we have were say we're
gonna get a stream of tokens and on the
left here we got an example of the file
format itself this is the structured
file format with the keywords with
string literals with braces and so on
and on the right then we have a stream
of tokens that matches this so we start
off with an end-of-line comment and it
tells us what the value of that is we've
got new lines we've got shade a keyword
we've got whitespace a string literal
and so on so it's just a stream of
tokens yes so the question is what is
the difference between a lexer and a
posit because Alexa is we're just
showing that Alexa is going to pattern
match on characters to produce tokens
and in the parser is pattern matching on
tokens to produce bigger constructs and
in that respect they are conceptually
similar however it's just what level
they're doing it at and it's splitting
up the responsibility so if the lexer is
responsible for converting a big block
of text into something smaller and
manageable that we can work with and
that the smaller manageable thing is a
stream of tokens and the parse that then
just needs to work with the stream of
tokens rather than having to posit each
individual character as it goes through
yeah and so again the question is could
that be could there be other layers on
top of that which will take the output
of a parser and do something useful for
it yes what's this place so is every ok
with lectures and stuff so far
awesome right next thing that Lex's
there are solve problem they've been
around for absolutely ages and they have
been written for us you can use we can
work with this so you can use a little
extra generator basically you don't have
to write one yourself you can use
electric generator things like Lex the
granddaddy of them all which is nearly
as old as I am I'm not a granddaddy hang
on oh if we've got flex which is a
slightly better version of Lex then
there's things like C s Lex for C sharp
FS Lex for F sharp J flex for Java and
so on it's basically been ported and
improved and made different for each
different platform that you're working
with so what does it look like what is
it input of this look like traditionally
this Alexa is Alexa file is split into
three parts
you've kind of got an intro part at the
top which is just user code and this
just gets thrown into the output then
you get a middle path for declarations
where you define directives such as the
the namespaces you're going to use the
class names the interfaces and so on and
they need to clear some regular
expression macros declare some States
and it finally your last part down the
bottom is a whole load of rules and
actions and those rules are are the
regular expressions we got to match and
then the action that we're going to do
and the action is most frequently
returned a token yeah regular
expressions so everybody who has told
you that you don't use regular
expressions to parse HTML has been lying
to you so don't quote me on that let's
do ever ok so let's have a look at a
lecture class so this is you can
everyone see that one okay we write that
good stuff so this is a lexical flexo
file for the shade lab file format at
the top there we just got a bunch of
using statements these will get copied
to the output file just as is there's a
bunch of definitions for example there's
a namespace there that's going to be the
namespace that gets generated
the name of the class that we generates
more boring stuff like that and then we
start to hit our regular expressions so
here's one for example this is defining
newline characters and these gonna be
the newline characters we're going to
match and the the braces around that
referencing other macro so we've got a
file which would bring in which just
includes a whole lot of characters for
us so that we don't have to use
backslash our blacks backslash n
everywhere but these are defining all
the characters that represent new lines
so we've got standard character and live
feed but also Unicode stuff because we
also need to deal with Unicode we need
to remember to handle that we can then
build those up into a slightly more
complex regular expression whereby it's
we've got an optional CR CR question
mark optional carriage return followed
by a line feed or it's just a scarier
term by itself or it's just I forget
what n L stands for but one of the
unicode line break characters or page
break Kara graph break parrot characters
and so on and as we go down we get kind
of more interesting regular expressions
as well so we can define a decimal
digits or we can have an integer literal
whether that one integer literal which
is a minus symbol followed by one or
more decimal digits and so on so great
for expressions and finally we get our
block of rules which match against
actions and so on so for example here we
have an equals sign and the action we do
just there is just return just return
the token which is equals so it's nice
and straightforward it's a really quite
straightforward thing we have the real
expressions which just describe
something I show something which
actually is a regular expression so
there's our integer literal for example
and then there's the token that we
return at the end of it so we're
splitting up our code based on regular
expressions
sorry we're splitting up our input file
based on regular expressions and
returning back tokens is everybody happy
with that in a quick overview of what a
file looks like
okay so how does oh sorry about a
question now it's so the question was is
out file format specific to a particular
lexer or are they all using the same
file format they're not using the same
file format but they're all very very
similar so they all base if they're all
pretty much based on Lex from 1975 and
so they're all very very similar they
will have different capabilities with
the regular expressions they will have
other different things with the
directives but they'll be very similar
and they pretty much all follow that
pattern so it's very similar alright so
how does it work the lexer when it's run
on that input file will generate some
source code so it's not generating
binaries or dll's or anything like that
it's just generate some source code and
that will be included in your project
and it'll be built and it'll do the work
for you
all of the rules which are regular
expressions get converted into a single
finite state machine and that means that
all of our regular expressions are
combined and matched at the same time
then that's really cool
you don't believe me but it is this
thing gets encoded into state transition
tables which means everything is all
done by lookups and the lookups are
based on the characters and the current
state of where we are in the transitions
and that means it's very fast and it's
very efficient so we're not allocating
loads of memory we're not running crazy
regular expressions we've got a limited
subset of regular expressions that we
can do and it's all done through input
look-up tables it also means that the
code is not very maintainable so if we
look at the generated code off the back
of this it gets very very scary very
very quickly so these I mean I'm just
still scrolling no small arrays oh there
we go and then there's there's the code
it gets scary quickly it's not easy to
maintain it's not easy to terribly easy
to debug but it's very good and it is
very efficient so it's a bit of a
trade-off there with maintainability and
there isn't any and efficiency
so I'm gonna kind of one of my reasons
for doing this talk was looking at that
file and going what on earth is
happening in there and so it was like
well if I submit a talk that means I
have to find out what all that greys are
doing so I'm going to tell you we'll be
able to take this as an example regular
expression so is the simple regular
session a and followed by a B or C
followed by zero more D's followed by
one or more ease so nice and
straightforward we can represent that as
a state transition diagram so we start
at state zero we can then have an a we
move to state one we can either have a B
or a see and remove the state two from
there we can have a D and we stay at
state two so we could have a zero or
more DS and then we can have an e which
moves us to state three at which point
we can have extra ease so we've already
got a one so of more ease going up there
and we either terminate or we keep going
it at the final state does that is that
clear to everyone awesome
we can represent this as a table and
this is effectively what's going on
inside that file with the index of the
arrays effectively map that table and
the table has elements for all of the
characters that you can match and and
something that happens at that point so
we can either match it and move to new
States we can accept the the pattern and
we've that means we're at the end of our
regular expression or we've got an error
nothing matches so we start at state
zero and let's say we hit we get the
character B state zero with B is an
error so that's no good we don't match
that but if we get a we match and we
move to state one I know we're at state
one if we get a we get an error doesn't
work but if we go to B we match we can
move to state 2 at which point if we get
a D then oh right well that's our zero
or more so if we get a D we stay on
state two and we loop and we can carry
on looping but if we get an e we match
we move on to state three a state three
is our final state and if we get an e
again we're looping here because it
stays of state three or if it's anything
else well then we're at we've hit accept
States and we've matched and there we
are we've
we've matched the token and then at that
point the LexA will do the action we've
got return the token we've matched does
that make sense
awesome because it gets better we can
match two rules at the same time with
the same table so with the diagram there
we've added on an extra bit the first
state we can either match a as before or
Rica match any other characters between
0 and 9 that will take us to a new state
which at that point we can match 0 to 9
again and it stays at state 4 or we end
so again we start at state 0 if we got
our a we would just go the route we just
went and everything's fine well or we
could get a 0 to 9 character and at that
point we then we match state we match
and we move to state for our new state
down at the bottom at this point if we
might get another 0 or 9 then we've
matched and we stay at state 4 and we
loop otherwise anything else says right
ok that's it we've accepted and we've
matched and so that allows us to match
multiple regular expressions at the same
time with a table based lock-up which is
really cool and really efficient but
don't make me write those arrays by hand
I've got no idea so um it's very cool
we've got lookup in the state
transitions table it's very fast it's
very efficient there no allocations
going on there which is also a really
cool thing but the trade-off is then the
size of the tables I mean you saw from
the file that massive amount of tables
you need one entry in each the table for
every character you going to match
because it's just a lookup but that's a
fixed cost and you know you that's a
good trade-off to take the other thing
you need to do is make sure that you
match everything you need to match so in
your rules for your lexer they you make
sure you're matching your newline
characters matching your unicode
characters as well so that if someone
sends you a Unicode file you can work
with that and the other thing is that
you need to handle an expected output so
they could have a catch-all rule which
says no bad character I can't do
anything with that and it throws an
error and it can't handle it alright so
we're gonna move on to pausing before we
do is everybody happy with lexing
excellent
I feel like setting your homework now I
feel good oh I'm too far
so what's parsing that so pausing
performs the syntactic analysis so once
we've got our stream of tokens we've now
going to verify that the the file our
inputs matches the syntax that we expect
from the file so we've now no longer
dealing with character level we're
dealing at something slightly higher
we've got a higher level abstraction to
work with and all we need to do is
pattern match on our stream of tokens
which we've got from the lexer we've
also got other information from the
lecture as well that's given us token
offsets and we can also look at the
contents of the text as well so we can
see what that block what that identifier
is what what the value of things are the
syntax is described by a grammar and if
you're lucky the file format you're
trying to pass has a well described
grammar and if you're not looking there
isn't one and it's just a bunch of text
which is harder but a grammar is usually
represented as a recursive hierarchy of
rules so the top-level be the whole file
and then it kind of gets broken down and
composed hierarchically into into other
structures and finally down to tokens so
for example with my shade of file given
a sort of an abridged shader file on the
right hand side there the grammar to
describe that on the Left would start
with shader file which is a block which
has got a keyword a string literal an
opening brace I can have an optional
properties block an optional tags block
some more stuff which is abridged and in
our closing brace and that's it but we
recursively fight define things so the
properties block is defined on the
properties block can be a property's
keyword an opening brace stuff abridged
so for example in that one I've actually
got a property star which is 0 or more
properties and in the closing brace same
with a tags block tag keyword opening
brace and content 0 more tags and braces
and it builds up and it gets nice and
big from there I can show you
the parser as well so this is actually
this is a bad idea because there's a lot
of extra boilerplate in here let's go
find a file okay so there's our
top-level rule which is shade a lab file
and it's got shade of command in here
which again okay so there's our shade of
command rule and the shadow command rule
is a shader keyword followed by a shade
of value and a shade of value as a block
itself and that's got a shade of value
name which again isn't recursively
defined rule a token there which is left
brace so the way this one works is there
anything in capitals that start there is
actually a token anything in lowercase
is a is under the rule another block
we've got left brace of properties
question yes yeah so the question is
we're looking at structure in the pars
of right now in the structure of the
file does that is the lexer concerned
about structure as well so if you get an
invalid file in the lexer does that fail
does that go wrong and the answer is
usually no but and it's also there
that's a good difference between a lexer
and parser because the lexer doesn't
care about the ordering of the tokens it
just cares about identifying tokens and
so you could have you know 23 left
braces followed by a comment an
identifier return keyword and everything
those are valid tokens but they're
invalid is an invalid program so the
generally speaking the lexer doesn't
necessarily care about the structure of
things it's not quite true because the
lexer can have states so it can
recognize the start of a token so for
example there might be regular
expressions in your lecture which only
match under certain conditions I'm
struggling to think of something now
there might be yeah so you can see here
I've got these things on the left here
these are different states so when I
enter it a bracket I have to pass things
a little bit differently sorry I have to
Lex things a little bit differently much
different
expressions for a couple of things
because they're only valid in certain
contexts and so you can have different
contexts in a lexer but it doesn't
really care about the overall structure
of the file only the parser itself does
another question yes the parser is
ignoring whitespace and new lines great
question
love it we'll come back to that yes so
okay so go back to this we will come
back don't worry parsing is unlike
lexing parsing isn't these kind of a
solve problem to be honest that there's
lots of solutions to the problem there's
lots of different ways of writing
parsers and generating parses there's
two main styles really top-down and
recursive descent and botton up
recursive assets so I'm going down one
going up so top down is you match the
whole file first and then you
recursively go down and to smaller and
smaller things bottom-up you match
tokens and slowly build things up from
there so if we're talking top down
parses so imagine a c-sharp file so
we're probably all familiar with c-sharp
files
hopefully the the file is split into
things like you know using statements at
the top then you've got like a zero zero
or more namespace declarations namespace
declaration is written to zero more
class declarations class declaration is
written there more method declarations
method declaration so on you know so you
see the connect collapses down into
something smaller and smaller constructs
but recursively so that's top down a
natural cursory descent and sort of
pseudocode it would be kind of something
like this so you start with passing the
shade lab file the first thing you would
try and do there is parse the shader
command when you're trying to do that
the first thing you would do then is
match the token the shader keyword token
that would pull in the token from the
lexer and match it and if everything's
good we move on we then pass the shade
of value that recurse is down into a
method which will pass the just the
shader value name match that match the
string literal for that match the
opening brace and then look at the next
token it might be a properties keyword
in which case go and pass the properties
it might be the substrate a keyword go
and positive
sub shader or it might be a tax keyword
don't pass that so we've got sort of
optional recursive descent in that now
I've shown you that in pseudocode
because the generated code has a lot of
boilerplate in it as well
so this is the pass file method and it
kind of it takes up the whole fire whole
screen because it's doing extra stuff
it's doing error handling and so on but
we can see that the first thing it does
here is powers shader command and in
part shade command itself we'll do a
match against a shader keyword and then
it will do power shade a value here and
so on so it does the same sort of thing
but there's a lot of boilerplate there
so we're not gonna look at it moving on
just does that make sense for top-down
pauses yes question there yes yes I
shoulda said that so I said the way it
works with resharper we do have a parser
generator yes so that is a an input file
to passage and rate to its custom I'll
come back to that in a sec right so
bottom-up pauses are an entirely
different beast and they start at the
bottom and work their way up instead of
starting you know saying match I'm gonna
match the whole file and then match
smaller and smaller pieces they smart
start small and end up matching the
whole file so this works with what's
called the shift reduce algorithm I'm
not going to go over this because it's
difficult to to show to demonstrate and
to debug as well it's it's it's tricky
but the idea is something like you know
you match a token this could be an
integer and then what you do is you
shift it onto a stack and that's the
shift of the part of the shift reduce so
you might have something which has got
an integer the the plus operator symbol
and you've just matched another integer
you've shoved that onto the stack you've
shift that onto the stack you then
reduce that and you say ah I recognize
that pattern that's an expression so I'm
gonna reduce that down to an expression
and then you get your next token which
maybe it's another plus symbol and it
might be another integer and you shift
those on and you can reduce that again
you get another expression and you build
things up from there might be a variable
assignment so you kind of reduce it down
to a variable assignment then we reduce
it down to a method declaration reduce
it down to a class declaration so you're
building a bigger structure from a
smaller thing to start with okay so I'm
gonna as I say I'm gonna skip over this
you free yes
benefits and drawbacks of top-down and
bottom-up pauses there are in some
respects yes there are some grammars
which can be best handled with different
types of parses and it's all to do with
recursion of something so if you have an
expression we should defined for example
as an expression plus expression all of
a sudden your recursive there because
your expression could to match that you
have to match expression but to match
expression you have to match expression
and so certain parses are better at
handling that than others and that you
can go in an entire rabbit warren of
Wikipedia pages on nois it's a it's a
lot of fun there's some definition of
fun building a parser you've got a
couple of choices here as well you can
do a hand rolled parser you can write
one yourself or you can use a parser
generator if it's actually easier to
write a parser than you'd think you know
the lecture is the harder think to write
building a parser especially if there's
a top-down recursive descent parser it's
a very mechanical process pars file
parse command passed keyword passed this
it's it's mechanical process is fairly
straightforward to build it's also easy
to understand if you're reading it
debugging it it's easy to follow it's
easy to understand you can also use a
parser generator such as yak and bison
yak stands for yet another compiler
compiler there was a companion piece to
Lex bison was a newer version of yak
probably named because it was yak you
know an antler have no idea it's an
acronym of something and ah perfect
another tool for language with
recognition because I was thinking
antlers and these like Bisons don't have
antlers they have horns but they usually
bottom up so they normally were their
nominee table-driven that generated
their table-driven and work as a
bottom-up recursive descent parser and
they are also harder to understand
harder to understand Hardesty but to
debug as well so in resharper we've got
that follows showing you there it's a
custom file format and we generate a we
actually generate the same sort of thing
you would do as a hand-rolled
top-down recursive descent parser it's a
mechanical process just gets generated
there's another type of parser that I'd
like to to mention though which is part
of combinators these is an interesting
initialling way of doing things where
you build a positive by combining other
smaller passes and so you can have if
this is more of a sort of functional
programming style of doing things you'd
have a function which say is to say
parses a string and then you have
something which is going to pass a
bigger construct and you combine
positive lists of the string parts of
function and mash things all together
it's kind of similar to how link works
in the you know sort of functional its
compositional and all of that but you
know it's also very easy to use but
there's a sort of similar cost
associated to that because you're
building your parser at runtime and so
you're combining all those things at
runtime and so then there is a cost to
that that might be absolutely fine we do
have some usages of parser combinators
in resharper but we actually save it for
our build tools rather than for as you
type analysis so an example of this is f
parsec for f-sharp this is an example
from fill trail Foods blog and given a
couple of sort of primitive functions P
string to parse a string P float to
parse a float and space is one to pass
one or more float sorry one or more
whitespace characters you can define a
new parser pass forward which will match
the string FD or forward you can see
that the sort of angle brackets pipe
symbol would be or matching or followed
by one or more spaces followed by a
float and if you match that do this
function which is going to be creating
an instance of this you do a similar
thing with left and right and generate
two you've now generated three pars of
functions there and then finally we've
got P come out which combines those all
together and it says match either the
forward or left or right as we kind of
combine those all together to give us a
final part of which we can then invoke
and parse things that
Parviz there's a similar thing for C
sharp which is using links in tax
personally I find this less readable
than the F sharp version but it's
available and it's there and it's
interesting we this is the library we
use in in resharper but that's doing a
sort of similar sort of thing parsing
many white spaces parsing one character
or a digit or white space and
concatenated everything and building up
a parser at the end of it so I'm not
gonna spend too much time I'll ask a
question yes it is actually yes that's a
very good point this is doing the lexing
and the pausing as that one is at the
same time as well yes very good point I
hadn't kind of tweaked that before after
splitting everything down and
responsibilities and everything yes so
this one will be parsing whitespace and
it'll just do those particular things
there because it can actually build
things up from smaller values right so
that's kind of an overview of lexing and
parsing so we're all experts like this
now and the what then comes in is some
of the interesting problems that you
kind of encountered the gotchas and
everything and as was mentioned before
whitespace and comments oh it's a good
one so we'd expect this to work so we've
kind of got a grammar on the left-hand
side there the shader block there it's a
keyword followed by a string literal an
opening brace some stuff which is
abridged and a closing brace and that's
what we're seeing on the right hand side
and that looks like it should match but
the actual input we get is usually very
different we get a whole bunch of
carriage return line feeds white spaces
weird line feeds we've got comments and
we've got all sorts of stuff and if we
Lex that we get a whole bunch of stuff
including our comments and new lines
which doesn't match what we said it was
going to be in the grammar there's an
easy answer to this we filter things out
we have a lexer which just returned so
our lexer returns a stream of tokens
what we do is we create a new lecture
which kind of decorates that decorate a
pattern and it filters out our
whitespace and comments so resharper as
I said has its tokens instead of being
integer values they are singleton object
instances one of the things we've
there isn't is filtered property so it
makes it very easy to filter things out
so yes we just wrap that and we filter
out any tokens that we don't want to see
so then the parser when it's looking at
its stream of tokens it doesn't have to
worry about comments it doesn't have to
worry about whitespace it just says do I
have a keyword followed by a string
literal followed by a left brace some
stuff and a right brace and that's a lot
easier to match but the question we need
to ask is is it safe to lose the
whitespace and that kind of depends on
what it is we're building why we're
doing the putt this parsing in the first
place if we just want to get stuff out
and we're interested in the contents of
the file you know because it's a custom
file format or whatever that well might
well be yes but my answer is no I'm
building for an IDE and I've got a
different set of constrict constraints
with this so I'm trying to build sort of
code editor features syntax highlighting
code folding and so on error
highlighting inspections where you put a
squiggly in the the right place
refactorings and formatting and so on so
I need to work with both the contents
and the structure of the file because
the contents gives us our semantic
information for navigation and so on and
here's an identifier and what that is
and so on but the structure allows us to
report all our inspections so if I know
what the structure of the file is I know
where to put the highlight so I know
where the squiggly line goes so I know
how to rewrite stuff I know where to put
in new lines if I move formatting code
so I need to represent the structure of
the file as well as the contents so
that's gonna be syntax trees so if we're
using a syntax tree accuse inspections
to walk the tree I can refactoring it
just by rewriting that tree and then
sort of basically re serializing that as
text so who thought about stats syntax
trees yeah no static syntax trees the an
abstract syntax tree is abstracts that
see the Glee it doesn't represent the
contents of the file it represents what
the file does effectively so these two
nodes are the same the the brackets on
the left are implicit in the structure
of the of the tree itself because the
brackets are used for precedence well
we've already got that because of how
the nodes are laid out what we need to
use instead are concrete parse trees and
this is just a data structure which
includes everything all of the new lines
or the whites
all of the comments and everything we've
got in there so basically we need those
filter tokens I'm trying to build a
syntax tree of a concrete parse tree but
I've filtered out a whole bunch of
tokens I need to get those back in and
so I have to add in an extra process at
the end of my parsing thing to reinsert
those missing tokens and add all the
white space and comments back in and
that's like it's boring it's a
mechanical process basically you walk
your tree and if there are any gaps in
the the tokens that are there you
compare it with the original excerpt and
put them back in and it's a mechanical
process is fairly straightforward and
it's good to go but then I hear you say
what about what a significant whitespace
so if we just strip out a white space
when we're dealing with with code how do
we deal with something like this with F
sharp F sharp doesn't have well in
depending on configuration it doesn't
have end of scope markers there's
nothing to tell you that the method has
finished
apart from indentation same with a for
loop and the if statement so we filtered
out all of our whitespace but we need
significant whitespace so how do we deal
with that
and the answer is to actually insert
tokens we're in control of the lexo we
can do interesting things there when the
lexer sees that there is an indent we
can put in a new token which is zero
width it doesn't have any particular
size so it's not gonna affect the output
and just say his yeah here's an indent
and when we're coming out of the other
way and it's shorter we can put in a
token which is out stent or an indent is
it out dent or anything nobody knows it
says is tricky or we can call it block
start and block into to sort of save
things there and then if the parser can
then match these tokens in the grammar
so that the parser can say you know gone
if if token some stuff then match indent
batch of whole bunch of stuff and match
finally outdent
and carry on and this then kind of it
was a bit of a not exactly a lightbulb
moment a reminder of how flexible code
is and how good everything is because we
can
interesting things with the lexer and
with the code and just because it's
generated doesn't mean that we can't
work with it so other examples are
things like with f-sharp the if you see
two followed by a full full start
character that's a float but if it's in
the contract of square brackets 2.0
that's now a range operator so you've
kind of got to figure out now how to
match whether it's a float character or
an operator and what we can do instead
is well let the lexer match in taught as
a specific token and then we can put
another decorator around our lexer and
say whenever you see in dot split that
into two tokens so we kind of
post-processing this token stream and so
who is the gentleman asked about extra
things on top of the lecture and the top
of the pars and post-processing and
things this is where that starts to come
in we've got flexibility with the lexer
a similar example is rigor expressions
might not capture everything we might
have to do things differently
shada lab has nested comments and you
can't do that with regular expressions
so when I said that people who've been
lying to you about HTML and regular
expressions they're telling the truth
really don't do it nested things in
regular expressions it doesn't work and
you definitely can't do it in the lexer
so you do things differently instead you
have a rule which matches the start of
the comment and the action for that
instead of just returning a token the
action finishes off flexing that comment
and ask custom code you could just put
whatever you want in there and that code
can then count the number of start and
end comment characters to handle nested
comments so that a lot of flexibility is
kind of I don't really know how to sum
this up succinctly but it's you don't
have to blindly follow the rules with
these things you don't have to blindly
follow the pattern of just return a
token make of just forcing everything
into a regular expression you can do
extra things you can do more complex
things and you can get it to work work
with you another interesting problem
which encountered with these lashade lab
file format is that some tokens can
actually just appear anywhere and a good
example of this is c-sharp files as well
so if you have hash if debug and hash
and if they could appear absolutely
anywhere
but if you can have tokens that appear
anywhere how do you put those into the
grammar how does the deposit deal with
that because the parser couldn't even
deal with comments or white spaces so
how does it deal with surprise tokens so
in shade lab we've got this cg program
token here about halfway down and
everything else is sort of shaded green
behind that that can appear anywhere in
the file format which makes things
interesting it's essentially a
preprocessor token to make things more
interesting with not going to delve
dwell on is the block inside C G program
is actually a different language and can
be positive can plead differently but
that's an entirely we're not going to go
into that but the way to deal with this
is is to pass passing you kind of you
have a preprocessor you treat it like a
preprocessor and you have a first pass
which finds these preprocessor tokens
parses them does something with them and
then gives that to a new filtering lexer
which can skip them I think your parsers
normal and the grammar just doesn't see
it the parser doesn't see those surprise
tokens at the end of that though I need
to put those back in and so it's exactly
the same process as the missing tokens
in Sirte and they just get shoved back
in and everything's good error handling
that's not right that's more like it
error handling is more of an art than a
science error handling is is kind of
tricky is there's also surprisingly
little written about this as well
there's about three or four papers and
and things articles written about how to
do error handling with when parsing why
do we care why do you care about error
handling if you're parsing a custom file
format you want to have some useful
error reporting and you also want to be
sort of minimal error reporting you
don't want to have to say you know if
there was a missing token the rest of
your grammar is going to fail you know
because you can't match something it's
all gone horribly wrong
then you need to be able to just sort of
say there's an error here rather than
saying your entire files messed up in
the IDE we especially need to do that
because as you type your code is broken
you don't type incomplete constructs you
type in you know
characters and so for a large part of
the time in the IDE your code is
incorrect and you don't want the whole
file to be highlighted and shown as an
error so you got a couple of choices you
can fail fast you know if you're if
you're pausing a custom file format you
might just want to say oh there's an
error right there and leave it as that
but you're not going to be reporting any
following errors you don't reporting
once and somebody could fix that and is
like oh there's an error there now as
well honey it's not great or you can try
and recover you can try to error
recovering so what happens when there is
an error in my case the parser is going
to add an error element into the tree so
instead of having an element which
represents a method declaration or a
block or a keyword or whatever I have a
custom element in my tree which
represents an error the error spans
everything has been passed so far which
could be the incorrect token or the
thing we're not expecting and in that
case for myself with my shade lab files
that means highlighting that in the
editor is trivial basically I just look
for an error element in the file
if there is one put a highlight for that
whole token how do we find an error
sounds kind of obvious the starties is
obvious you don't match something the
parser is trying to match a particular
token and it doesn't find it so the
either the token is missing or it's
something different and it's and there
we go but where does the error stop
that's the tricky thing because if
you've just kind of got one token
missing or one token extra everything's
shunted along and it's gonna be very
hard then to find where to stop so you
need to try and recover but how what are
the options for you and it comes out to
sort of three things basically you've
got three options you can go into panic
mode which is a great description for it
this is what the papers called it's not
just me and that is basically you eat
tokens until you find something you
recognize so if there was an error in a
method declaration for example you would
go into panic mode and say oh quick
something's gone wrong I need to find a
closing brace and as soon as you find a
closing brace is half fine and carry on
parsing it might work you might not
another option you can do which works
surprisingly well is
token insertion or removal a
substitution it's kinda like playing
what if he's saying oh I've got that I
wasn't expecting that
what if I did get it and the other
option you got is actually encoding the
rules into the grammar so panic mode as
an example in my shader file here I'm
trying to pass a property here and a
property is one of those lines and it's
supposed to be the name followed by
stuff in brackets equals and then a
value on the second line there
I've actually got something going wrong
because I don't have any brackets and so
it's gone into panic mode it is said I'm
not expecting that I was expecting
brackets I'll panic until I finds the
end of that particular expression which
is the end of light and so it just eats
everything so that's that's fine we've
kind of recovered nicely but if you look
at the second example where somebody's
put in an extra bit of text in there you
have a syntax error it tries to tries to
match up again and finds the next thing
it can and the next thing it can find
actually is then the start of a new rule
which is going to be an identifier for
an X property but that's not the
identifier for an X property that's
still part of this line and so it gets
it wrong and so all of a sudden now
we've got two errors and it's it's kind
of messed up so panic mode will kind of
get us part of the way there but it's
not brilliant token insertion however is
a good a good chance and it kind of gets
a lot of it does capture quite a few of
the issues so example here is we've got
a missing closing brace at the end of
the word color there at the end of the
color key word and what we do in that
case is we assume we got it so we said
what if we did get it here and I care a
try and carry on parsing from that and I
say well okay I'm I'm expecting a right
parenthesis character I didn't get it it
should be followed by an equals
character let's pretend I did and we've
actually look at it here now we see
there is an equals character so it's
like fine and he carry on parsing and it
actually recovers very very quickly
there and we've got a very small
localized error if that fails we can do
the opposite you know perhaps somebody's
type too many characters and we can do
total token removal so for example if we
tried to concerting a character and it
didn't sink back up we can then try and
say well
perhaps as an extra character there and
I was expecting equals but I got another
right parenthesis what if I didn't get
the right percent parentheses what would
happen next look at the next character
isn't equals it's correct and we sync
back up again and we move on and the
final one then is is initially one in
that you can tell your parser to expect
an error and you can encode it directly
into the parser if there's are going to
be a common place in the file where
there's usually an error or way is
obvious or you expect that something
could go wrong you can put something in
there to catch it directly so a good
example is an empty block so if
something which has got a left and right
brace character and you know it's going
to be empty you also know that
somebody's going to put something in
there because that's what people do so
you kind of code something in you you
actually put in a rule there to say
match me something but if you do match
anything make it an error you know it's
just kind of that's like this I think
fine I'll deal with it so um just start
to wrap up now then really there are a
couple of other things which are
pertinent only really so to an IDE which
are also very interesting though
incremental lexing and parsing and the
idea of this is really that you don't
want to parse the whole file on every
change so field a nice big file and you
hit one character you don't have to
repass the entire file relax the entire
file so anyone who just repass the
subsets the smallest subset that
encloses the change you also want to
avoid relaxing it as well and that's
actually easier than it sounds you just
go walk up to find a block the stealing
there you've already you you have to
cache the lecture output first of all
but you've already got that information
and you know how to parse and you just
start parsing from that point on until
you've finished you close the block and
basically resync everything up and
you're carrying out it's a surprisingly
sort of straightforward mechanical
process and the other problem then is
composable languages and there's there's
several ones of these that can have
things like injected languages so that's
like the cg program problem that shader
lab has where the cg program block is a
different language again and you can
solve this in a couple of ways your part
you can extend your parser so it expects
that and it
as effectively a whole extra set of
rules embedded in it to deal with that
or you can have support for that too so
the ID for example can have support for
that saying okay well I'm gonna treat
this as a separate file and you have
just a different person or a different
thing treated as a separate file you can
have things like inherited languages so
typescript is a superset of JavaScript
for example so when you're typing when
you're parsing your JavaScript sorry
your typescript files
that's what can also work with valid
together right around when you're
pausing a type script file that can also
be valid JavaScript and the way you can
do that then is it's useful if you are
using a hand-rolled parser or a a
mechanically pies are generated top-down
recursive one where you can override
certain parts of it so you can extend
the parser class just like you'll extend
any old cut any kind of class and then
the other ones would be interesting are
nested languages things like JavaScript
or CSS nested inside HTML or raised or
in c-sharp and you can sort of switching
from that and that one again you kind of
have some sort of inheritance type thing
going on or a mode switching which can
sort of multiplex as it were between the
two razor and c-sharp is a particularly
interesting one because they can be
embedded in certain inside themselves
several times which gets messy so to sum
up then how do you par as a file
don't just just just avoid it please
it's not worth it
stick tej so it's a xml six of CSS six
the am will stick to a general-purpose
language but there but if you do there's
a lots of fun there's lots of
interesting things you can do with it so
finally some some links then if you want
to see what's going on with the shader
lab file it's all open source it's and
resharper unity github page on JetBrains
account and then a couple of links there
which are pretty much the only things I
could find on error recovery for for
pauses which is you know considering the
size the internet three or four articles
is is a pretty impressive and that
brings me to the end and really so if
anybody has any questions please let me
know otherwise thank you very much for
attending we have a question yes I'm
sorry miss that yeah so the question is
if the tokens are integer values how do
you get to the contents of the token and
that's actually a when the the stream of
I kind of lied to it when I said it's a
stream of tokens it's a stream of tokens
plus offsets plus well tokens plus
offsets and given the offset you can
then get the text there so yes somebody
else have a question yes yes so the
question is that some of the things are
recognizing things based on where they
are with the neighbors so Z that is not
far removed from convolutional neural
nets
the my answer is I have no idea I don't
know
but that'll be cool you know artificial
intelligence file pausing that's cool
you know there desi PhD right there tell
you it's anyone else nope
cool very good thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>