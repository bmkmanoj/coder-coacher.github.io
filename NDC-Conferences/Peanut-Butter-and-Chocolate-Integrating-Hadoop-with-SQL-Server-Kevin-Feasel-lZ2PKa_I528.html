<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Peanut Butter and Chocolate: Integrating Hadoop with SQL Server - Kevin Feasel | Coder Coacher - Coaching Coders</title><meta content="Peanut Butter and Chocolate: Integrating Hadoop with SQL Server - Kevin Feasel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Peanut Butter and Chocolate: Integrating Hadoop with SQL Server - Kevin Feasel</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lZ2PKa_I528" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey good afternoon everybody we're gonna
talk about integrating Hadoop with
sequel server so my name is Kevin feasel
I am a manager of a predictive analytics
team in Durham North Carolina also a
Data Platform MVP and I've got a blog
curated sequel curated sequel is all
about trying to find interesting blog
posts five to ten interesting blog posts
per day in the space of database
development database administration
Hadoop power bi our Python this security
this ever broadening ever-expanding data
platform space so it's curated SQL comm
I'm going to assume for the purposes of
this talk that you have a Hadoop cluster
if you don't that's no problem I'm just
not gonna show you how to set it up I'm
also going to assume that you're more
familiar with T sequel or with c-sharp
or some dotnet language than you are
with the natural languages of Hadoop
like Java Scala and even Python and
finally I'll assume that you want to
work with Hadoop so those are the three
basic of some assumptions I'm making
they're not as strict as you may think
but yeah that's my audience
the reason integrate comes from a book
that's 241 years old now it's basically
this is my my sneaky way of trying to
fit economics into this discussion if I
want to talk about it from a product
standpoint think of it this way if you
have a small company then you can
probably fit all of your data in one
relational database and it works then
eventually the company grows you have
additional things you have additional
resources some of those things don't
necessarily fit nicely into the mantra
of what a relational database is and so
you expand out you have other teams
working on other things
maybe you eventually hit a point where
you need a Hadoop cluster and you go out
and you put one together but there's
still a reason to tie the data back
together the specialization and division
of labor both so we specialize by
putting certain data in sequel server
versus in Hadoop
that stuff that fits really well in
sequel server is the stuff that if it's
wrong you're fired
we can take advantage of properties like
consistency that don't necessarily exist
in a lot of other data platforms we also
have decades of experience we have a lot
of people who know how to use this model
we have ways of getting that data into
applications very quickly we have a lot
of people who can tune those queries so
there are some good reasons to put
things into a relational database by
contrast there are also really good
reasons to put things into a Hadoop
cluster if I Magra gating a huge amount
of data that's kind of the classic
reason for Hadoop the newer reason for
Hadoop is almost the opposite I'm
streaming a lot of tiny data so I have
millions of messages that I'm sending
out each message is is itself very small
but it's just a sheer throughput of
messages that drives a larger solution
then when it comes to things like
warehousing on my sequel server side the
warehousing solutions tend to be tell me
what I need to know so in other words I
know that I have to fill out these
reports and give them to the sec every
quarter I need to fill out these reports
and give them to different regulators I
know the CEO needs to see these reports
the board needs to see these reports my
manager needs to see these reports so we
build the warehouse with those reports
in mind we build it with an end goal in
mind we are answering known questions
the Hadoop cluster is about spelunking
it's about trying to find the unknown so
we're digging into a data set that we
may not necessarily know what's in there
we just expect that there is an answer
there so it's two different use cases
and yet we're going to want to tie these
together we're going to want to take the
data that lives over here take the data
that lives over here integrate that data
together because from both of these we
can get the full answer
so I'm going to show really five methods
of tying together Hadoop and sequel
server scoop is a method that I will
briefly discuss but again I'm assuming
that we're more interested in the dotnet
side of things so we'll start with
Ambari and barri is just a web framework
for managing a Hadoop cluster this comes
by default with the Hortonworks
distribution and I should probably note
that I am using the Hortonworks
distribution I have my portable Hadoop
cluster right over here and I have my
network router that will inevitably let
me down right here and I'm going to be
working with those today so I have
Hortonworks Hadoop installed on that
thing a couple other important
distributions there's cloud era which
was the original distribution and
there's map bar which is the third
distribution they have their own
management interfaces if you happen to
have a cloud error cluster use its
management interface it's basically the
same there will be little differences
here and there but you know nothing
nothing too big to get worked up about
this is perfect if you have one file one
time that you need to move over
somewhere or I have to pull a file once
a month and I'm willing to do that
instead of automating it so let's take a
quick look at that I have an bar set up
right here so this is my Hadoop cluster
and it's looking on port 8000 so that's
my embargo and barri gives you a quick
overview of the health of the
environment you can see I've got a bunch
of stuff that's currently down that's
because I rebooted the machine before I
came down here over here I have a menu
and inside that menu I get to see things
like the files view so this is where I'm
going to be able to look at HDFS the
Hadoop distributed file system if I
select files view it will take a moment
because the cluster just started
acting up and responding really slowly
to all of my requests so it knows that
it's on right now it knows that you're
looking at it it's very it's very shy
inside here I have a temp folder inside
this temp folder I've got other stuff I
can create a new folder if I want and I
have now an indc folder notice that it
has unix-style permissions owner group
and the the CH Ahmad 777 set up
speaking of Tremonti seven of course
we're gonna give everybody read write
and execute on it because who cares
about security right so now anybody can
go in here and modify this file I can
hit refresh and I get to see yeah
everybody's got full access to this
directory now everybody can do stuff in
here eventually we sober up and realize
that was a bad idea so I'm just gonna
delete it so it's gone let's go into
here OTP inside here I've got some files
already I'm actually going to delete
this one because we're gonna recreate it
later there we go and I'm going to
upload some batting ratings so all of my
data today is baseball and I have a
batting ratings file it's a very small
data set but it serves the job so we're
going to upload this data set as it
slowly uploads eventually we're going to
see it there we go now we have the data
set I can hit open and that will give me
a brief view of the data its comma
delimited data where the first row is a
header and then we have some basic
information about a bunch of baseball
players so now that I have that data I
can go in to hive hive is sequel on
Hadoop inside this hive you I have it
open over here on this other tab I have
a database
hive is a set of databases and they
don't have schemas as such it's
basically database table I can create a
new database but I'm gonna use default
because I'm lazy let's take a script
this script here will create a batting
ratings table so we are dropping a table
if it already exists and if it doesn't
exist we're gonna create it with one
column it's called placeholder it's a
string we're gonna load the data from
batting rating CSV into the table
batting ratings so let's execute this
and I'm gonna cross my fingers that it
will actually work here because hive was
giving me problems just a moment ago so
this there all right it worked I go over
to default and you know we can see down
here that the query did in fact succeed
life is good
I can go to batting ratings and hit the
menu option right here and that's going
to select the top hundred rows from the
batting ratings table I can click on the
batting ratings table itself and it will
show me a list of the columns but I'll
be able to see them soon enough over
here batting ratings dot placeholder and
in fact these are the individual rows so
I have two problems here problem the
first my headers are in the data problem
the second my data is not really
separated nicely for Hadoop that's okay
this is semi structured I'm saying dump
the data however and I'll define that
data when it comes time to query it so I
can write functions that will explode
out this text it'll split it on comas
like maybe I want to get all of the
first baseman who are younger than 25
years old so I can go find what the
position is for first base I can find
the position for age I can break out
that array and do a an if statement and
check that out
or I could do it the right way and the
right way is by creating a table where
we've actually defined the data types so
here I've already created this table I
don't need to run it again but I've
defined all of the individual data types
in my data set this came from a
structured data source somewhere
probably and so I can take advantage of
that structure when I load it into hive
if it came from a strictly unstructured
data source I'm not able to do this I
could create different views of the data
using hive and like look for things
where the structure looks like this or I
may end up having to create functions
and explode out the data set and do the
nasty work that I'm trying to avoid
anyhow I create the table here it's
fields terminated by comma lines
separated by a new line stored as text
input and it has a particular output
format but what's important is skip
header line count equals 1 so that first
row is a header throw it away and I can
see the end result of that up here in
batting rating is perm so I run this
it's going to pull back a hundred
records and those records will have
appropriate names so now I can write a
simple select query that says select
star from batting ratings perm where age
less than 25 and position equals first
base and I get all of those players
another example of what I might be able
to do is to write out some right
fielders so run this and say 1000 TP
output I'd better make sure that
directory exists I'm going to terminate
fields with a comma and I say give me
all of the right fielders so first let's
make sure temp OTP output exists so
we're back up here in an Baris files
view and I see that there is no folder
for that so let's do
Oh TP output and I think I can leave
this as current permissions if we get an
error then I'm just gonna grant it
Jamaat 777 because again security not my
problem I'm a developer that's only like
40% facetious okay so it succeeded and I
have an output file output file looks
like this it's just a bunch of zeros
that's because each of the individual
nodes of my Hadoop cluster can generate
its own output file and then it's my job
at the end to stitch them all together I
can open this output file and see the
results where yeah these are my right
fielders from the data set so with that
I've got some data I've got some files I
can also do one last thing that will
help me out a little bit later in the
talk and that is I'm going to create a
table called second basement so this is
a new table off of an existing table
create table if it doesn't exist as a
select statement so I run this and it's
going to give me back 777 results in my
second basement table so while that's
running not a big deal let's talk about
scoop scoop is a console application it
has two major benefits or it has two
major use cases use case number one is
to retrieve data from a relational
database and put it into a Hadoop
cluster use case number two is to take
data from a Hadoop cluster and wait for
it put it into a staging table in a
relational database so I had to specify
staging table because scoop does not
like to play nicely with existing data
if it has data here
and if you want to insert all new data
it's happy with that if you want to
update all existing data its it's ok
with that if you want to do come a
combination of inserting some rows and
updating rows that already exist whoa
does not like that at all so my
recommendation if you are using scoop
take data from Hadoop put it into a
staging table and then write your own
code to merge the staging table into
your real tables also if you do use
scoop
you'll have to install the Microsoft
JDBC driver if you want to talk to
sequel server that is because sequel
server has concepts of things like
schema that don't exist in hive or in
Oracle at least not in the same way if
you do want to check out scoop I have a
whole set of scripts on what you can do
with scoop and it will include things
like listing all the databases listing
all of the tables we can take a table as
is and just dump it right into Hadoop we
can write queries to join together data
and dump that data into Hadoop and then
vice versa we can take data from HDFS so
just some text files from HDFS and dump
them over here or we can query hive data
dump it over here there's also some
pretty good security options in here
where I can store credentials securely
within my Hadoop cluster so I don't have
to specify a username and password and I
think I'm hesitating on this because I
never actually got it to work but you're
supposed to be able to use Active
Directory to connect to a sequel server
from a Hadoop and completely bypass
credentials I've not gotten that to work
but it's supposed to be possible all
right so let's talk about some of the
fun stuff let's write some dotnet code
if you install a particular nougat
package microsoft hadoop mapreduce
then you get the ability to perform file
maintenance this sounds like a really
boring
super power but it's actually pretty
cool we're going to take some data from
Hadoop pull it down in a sequel server
and then we're going to have some dye
net code write that data into a database
I would use this method if you're
already comfortable with writing ETL in
dotnet if you don't use other tooling
for this or if you have some very
complex work that you need to do and
also if you're not very comfortable
writing Java so let's go check that out
first step I have to talk about NuGet
packages I have two interesting new get
packages first one is Microsoft hadoop
mapreduce that is a dotnet API notice
that says MapReduce functionality so
originally back in 2013 it did MapReduce
because Microsoft had this cockamamie
idea that they would put Hadoop on
Windows thankfully that died so if
you're using hdinsight it actually still
has an option for Windows please don't
do that
never do that but back in 2013 they had
this idea that all right you're gonna
write some c-sharp code to write map and
root reduce jobs just as the world was
leaving map and reduced jobs because
writing hive queries Pig queries was a
lot better using spark way better anyhow
ignore the map introduced this is all
about file maintenance it will install
Newton soft JSON because every project
must have Newton's off die JSON and I
installed F sharp not dated a sequel
client which is a type provider that
will give you a micro arm in F sharp so
yeah this is an example that uses F
sharp if you're not familiar with the
language don't worry about it the syntax
here is gonna be really similar to C
sharp and we're gonna walk through it
step by step
so what I want to do is load my
libraries and I create a connection
string to my database the database is
OTP and I'm using windows authentication
I then
a statement here to insert into the
player that second baseman table and I
have my columns that I need and I have
the values so I'm going to build a an
insert statement and I'm going to pass
in some values and it's going to insert
a row into that table so pretty easy so
far by the way let's show that I have
nothing up my sleeve I'm going to drop
the table if it exists and recreate it
this is syntax that exists in sequel
Server 2016 this drop if exists if you
don't have 2016 then you have to do a
check to see if the table exists
yourself fortunately I have at least
2016 so we have an empty table I can say
selects star player that second basement
I run this I get 0 results back because
I just dropped the table in my main
function I'm going to connect to my
Hadoop cluster on port 5 0 0 7 0 this is
the web HDFS port so this is a web
protocol that will allow me to work with
the Hadoop distributed file system I can
add files I can retrieve files perform
that file maintenance I'm gonna connect
to it with a username of admin and a
password of null because every time I go
and do something secure I then go and
mess it up in a real cluster you would
have a real password so let's say hey
who do cluster now that I'm talking to
you now that we're in speaking terms
I would like to copy a file from you to
me specifically I want to copy this file
second baseman CSV to a local path in my
temp directory then I'm going to read
the file now this is a very small file
so I can get away with using read all
lines that's going to read everything
into memory if I can't do that
then I'm going to open up a file stream
and I'm gonna string the file in which
is
what I would do with a data set that was
larger than 777 rows second basemen you
can see is a string array so each
element of the array is a line in the
file I open up my connection to sequel
server and then I begin a transaction
and I say let's take that file and first
throw away any lines that have a length
of zero there's a new line at the very
end let's not confuse anybody as just
throw it away
then for every row in this file I want
to take that element of the array and I
want to run split against it so I split
on commas so now I have a new array this
thing is called s so this is my string
array that's going to have five data
points it's going to have the first name
last name age bats and throws then I'm
going to create an instance of that
insert statement using my micro arm I'm
going to execute that statement and
notice I never created an execute method
that was generated for me it also uses
sequel server metadata to go look up
what those data types should be so age
is actually an integer in my database so
I need to pass in an int so it correctly
figured out that I need to give it an
int which means that if I did not give
it an int I would get a compiler error
right here and it says that yeah you
should pass at an integer here so let's
make the compiler happy and I'm gonna do
that for each line create a new object
to insert run the insert statement once
I'm done I'll close the transaction then
let's take a file locally so hey cluster
I got something for you let's copy it
from local up to HDFS it's called
pitching ratings
so we're going to set this as the
default project because I didn't and
then we'll run this so it says that it
loaded the table and in a moment it's
going to say that it successfully
uploaded the new file while that's going
on let's confirm that it did in fact
download second basement CSV 816 at 1:25
a.m. yah mahn Eastern Time eastern US
time so it's 1:25 a.m. right now
hey it uploaded the file ok
so we can go back to here and
double-check and let's make sure that in
OTP pitching ratings is as of 1:25 a.m.
which it is so we've got the new files
one thing that we can also check let's
delete those lines so I don't
accidentally run it select star from
player dot second baseman we have 777
rows as I expected we have all of our
second baseman in the data set so great
at this point in 50 lines of code I took
a file from here I loaded it into sequel
server I took another file and I loaded
it up here I've just created a fairly
basic ETL engine you can add a lot more
to it you can start moving around more
generic sets of data but this is the
core this is the basics so I'm going to
hit enter and close that so this is the
meat of you know what I would do if I my
dotnet developer and I'm working with
both sequel server and Hadoop this is
probably one of the two methods that I
would be most inclined to use now
suppose that you are a big fan of sequel
server integration services suppose
further that you have sequel server 2016
and Visual Studio 2015 or later in that
case you have some components that will
help you in your quest
with Hadoop because you will be able to
connect to the Hadoop distributed file
system as a source or a destination in
integration services if you're using
older versions you're very limited
access I wouldn't even really talk about
it in that case because here you can do
one thing and one thing only it's only
how if you normally write integration
services packages for sequel server then
2016 you can do this with SSIS let's
walk through an example what I have here
are a couple of Hadoop connection
managers we're only going to look at
connection manager number one the other
one was a test I created that by saying
right click on connection managers and
say new connection manager so I do that
and as of SSIS 2016 I now have a hadoop
connection manager so I can select that
and it will give me a screen that looks
roughly like this that's actually
connection manager one that was my test
that was going to add your HD insight so
it looks like this there are two
connection methods with integration
services 2016 web h-cat and web HDFS we
talked about web HDFS already web h-cat
is the hive catalog so I create hive
tables and then I can expose that data
on the network but I don't want to move
around hive tables I want to move around
data files so I'm going to connect my
hosts on my web HDFS port I'm going to
give it a user name and I don't have a
password for it because my
authentication is so basic you can use
Kerberos and actually have serious
security but this is a demo
so I create a data flow task inside this
data flow task I have an L a DB source
this data source is my OTP database
again and it is top salary by age so
this is a very small database table that
has two values an integer and a numeric
value it's a decimal value so very small
table but that's okay I can still write
it to HDFS this is one of the problems I
have with integration services with high
resolution monitors it's not supposed to
look like that in fact it gets a little
bit worse than this because there are
some values under here that you have to
set so if you have a high res monitor
like like this or if you have a 4k
monitor well it doesn't work that well
if you're still on 1080p then you'll
actually see all of the values and one
of one of the things that you're missing
is you have a data type well this is a
flat file this is a text file cool
what's the separator oh it's a comma
okay coming back here I can tell you
yeah it's a text format what's your
separator somewhere down here
fortunately I did this before I migrated
to a high res monitor that said once you
have the data loaded what's nice about
integration services is sources and
destinations are just sources and
destinations they're all abstracted out
to be the same general principle I don't
care if the destination is a sequel
server or a flat file or if it's HDFS or
if it's Oracle or if it's anywhere else
if I have a destination component it's
all going to work the same way
I have input columns and destination
columns and I can map them appropriately
so I can write this data and as I
execute this package it will take the
gigantic 22 rows of data and it will
write it to my I do cluster meanwhile my
Hadoop cluster is saying you
you realize that the minimum block size
in HDFS is set to 64 megabytes right and
I say yes and I'm still going to give
you 1/3 of a kilobyte this is big data
people so let's let's look at the top
file we're gonna open it up and it's
going to show us that we do in fact as
soon as it actually opens we do in fact
have all of our data in here I'm not
going to wait for it to open it's being
sassy with me I will sass it right back
so let's continue on to retrieve from
HDFS and just trust that the data is up
there all third of a kilobyte of it we
have our data flow and we can take data
from HDFS once again not worrying about
the screen resolution issues I select a
file path pitching ratings this is again
a text file that is comma delimited so
in integration services you hit columns
and it will show you the columns and
when you connect to a flat file or to a
sequel server database or like an Oracle
database oftentimes this metadata is
filled out for you but it's not filled
out for you here the reason is we're
using web HDFS and web HDFS when it
retrieves a file does not have any type
of method of saying hey here's what the
columns actually represent here's what
the data actually means so integration
services is just taking a file and it's
saying I know that you have let's find
out how many columns it has 61 columns
in here so you have 61 columns in here I
don't know what they mean I just know
that you have them and so it gives you
automate it default names if you want to
clean up the data it's pretty easy to
create your own transformations and I
decided to add some new columns those
columns are named first name last name
age vats and throws
and they are unicode strings including
age which we know is an integer that's
pretty suspicious but I've put on a
little watch icon to help us explain why
I did it this way no points for spoiling
it so we're going to connect a Hadoop
maybe there we go okay yes it retrieved
the data and if I scroll all the way
over to the right we have our data we
have our columns and then we have our
lines of data that include the headers
so pull down the headers unfortunately
integration services for Hadoop does not
understand the idea of a header row with
a flat file you can say use the first
row as a header and it will take those
values and it will replace the headers
with those values and then if I could do
that life would be good I could make
this an integer and we would have a
pretty decent data set but I can't do it
here so that is a limitation that
currently exists in integration services
when dealing with Hadoop given the set
of limitations I'm not sure that I would
recommend using SSIS if you're moving
data back and forth with hoop I mean I
guess maybe if that's all you do then
it's an option but I would probably just
write some net code it's not that
difficult
so up to this point we have looked at
ways of integrating data indirectly
I've been shuffling files around and
I've been assuming that people down here
will understand what to do with this
file people up here will understand what
to do with a file but now I want to say
over here in the world of sequel server
how do I actually get data from
somewhere in Hadoop and do something
with it and the first method is using a
linked server link servers have been
around for decades and they allow
to connect external resources to a
sequel server so that external resource
it could be another sequel server
it could be Oracle it could be an Access
database it could be an Excel
spreadsheet could be a flat file it
could be a hive table this is going to
be your main option prior to sequel
Server 2016 and let's go check out a
demo in order for this to work I must
have the Microsoft hive ODBC driver
installed on my on the windows side on
the side that has sequel server on it so
the Microsoft hive ODBC driver and I
have a link to that let's configure this
driver this says please connect to my
cluster on ports 10,000 this is another
configurable port that's the hive
integration port I actually have a
password here so for once I did it right
ish so then I also have to hit Advanced
Options by default the default string
column name length will be 32 K well
what happens here is that sequel server
and the the hive o it uses the hive ODBC
driver to take string data and it says
how do I translate this okay well I'm
just gonna take this value and make it a
varchar' of that size so if it's set to
32,000 blah blah blah then sequel server
will say I don't know of this 32,000
character varchar' that you're talking
about that's an invalid data type error
so you do have to set it to a value
unfortunately there's no such thing as a
max or a 0 or a negative 1 or anything
to indicate I want a max column meaning
that you're limited to 8,000 characters
per column when you're using this hive
driver
a couple things to think about there so
I have my linked server I can or excuse
me I have my ODBC driver and I can
create a link server
pretty easily I say my server name is
called cluster Ino
it is going to read a database called
hive and I just named that myself you
can name it something else it will use
da SQL providers that's pretty default
the data source is my hive ODBC
connection the one that I created and I
just showed you user ID and password
fill in your own real password don't use
mine that's not my real password
it's admin one no so then anyhow we then
create a link server login and I can
make sure that the query actually works
I've already created this link server as
we can see in server objects link
servers I can see it right here
that creates a hive database and
remember in hive I said that the
database was called default and the
table was called second baseman or
batting gradings perm so in sequel
server it takes the hive database and
turns that into the schema and I defined
what this hive database was so took a
few seconds gave me 777 rows they are
the 777 rows I'm expecting the execution
plan says this is a remote query all of
the work is happening outside of sequel
server so it has relatively little
information to share with you except
that it thought you were going to send
it 10,000 rows why 10,000 because that
was a number that somebody plugged in
and is the default for sequel server
2017 for 2016 and before I think it was
like one row so basically database
engine is saying I have no clue how many
rows
coming in let me give you a number
10,000 that sounds like a good number
and it takes in all of the data and it
does this little compute scaler thing
what that does is I'm taking the columns
that came in first name last name age
bats throws and I am converting them to
sequel server data types so that's what
this is doing and then returning those
results so that took a few seconds but
what we can do once we have that link
server in effect is we can join sequel
server tables with hive tables so this
is a sample query and I'm selecting the
top 50 records from my second basement
table and then I'm doing something that
does not exist in hive I'm using the
cross apply operator so this is in
sequel server T sequel this is not in
hives hql and what I'm saying is okay
for the second basement if you are more
than 30 years old
you are officially old in my data set
this is a cruel cruel thing to tell
people but we're gonna modify it a
little bit by joining to this top salary
by age and what that is is what how much
money did the highest-paid second
baseman of your age make in the year I
think this was 2016 so I'm going to
point out that this table is in sequel
server this table is in hive we're
joining the data together and we're
going to get some results and it's going
to take a little while
first of all Hadoop clusters being shy
second of all lint servers suck I'll get
into more about why link servers suck as
we go on but I just want to point out
that Ben Zobrist is old but I would
happily be called old if I got me if I
got to make seven-and-a-half million
dollars this year that's a trade-off I
am willing to take
so this is link servers prior to 2016
it's probably your best option
outside of shuffling around using.net
code with 2016
Microsoft has poly base poly base is the
latest offering it was originally
introduced in 2010 in a product called
parallel data warehouse in 2012 they
improved upon it later on they renamed
parallel data warehouse to ApS analytics
processing system I believe basically
this is a really really expensive
appliance that is a combination of a
Hadoop cluster and a sequel server
instance jammed together really
interesting but base price I think is
upper six digits lower seven digits so a
lot of companies just they're not gonna
buy this thing poly base was a key
component in that in that process well
in sequel Server 2016
Microsoft released it to the masses by
the masses I mean anybody who can afford
Enterprise Edition so not really the
masses but compared to PDW or ApS as a
much broader audience so if you have
Enterprise Edition you can take
advantage of this sequel Server 2016
Service Pack 1 changed a lot with regard
to what you were allowed to do many
things that were previously Enterprise
only now we're available in standard
edition in Express Edition so you know
down to the free version poly base is
not really one of those it's still an
enterprise only feature with a caveat
and we'll talk about that caveat poly
base has three big advantages over Lync
servers number one is the ability to
push down predicates I can take a filter
like part of a where clause or maybe a
grouping operation and push that down
and make the Hadoop cluster do that work
my sequel servers are very expensive my
Hadoop cluster nodes
nearly as expensive so if I can take
work over here and force it down that
way I'm making the less expensive stuff
do more work and it's gonna return back
to my pretty expensive database instance
and I'll return that to the end-user the
way that we figure out whether or not
it's worth pushing down and having
Hadoop do the job is with statistics in
the linked server case I showed you the
estimate was 10,000 that number is
pulled out of a hat it's just the
default with Polly base we can actually
collect stats on how many rows are in
that table what's the distribution of
values in that table just like a regular
sequel server table and that will give
you an idea that well if I'm filtering
down and I'm only gonna get back one row
from here maybe I should have the Hadoop
side do that filter give me back one row
but if I'm getting back 95 percent of
the rows just stream that data to me and
let me do the filtering the third big
thing and this I think is more of a big
thing for the future parallelized sequel
server so let's draw out we have a
Hadoop cluster we have a name node on
the Hadoop cluster and the name node is
basically in control of a bunch of data
nodes so each one of these data nodes is
responsible for holding data it's
responsible for running code that the
name notice signs to it on the sequel
server side you know historically we
just had a sequel server but with poly
base and specifically for poly based
queries we have this head node and the
head node can farm out work to what it
calls compute nodes
so I have these smaller databases down
here these are my compute nodes these
can be standard edition head node must
be Enterprise Edition what this allows
me to do is say ok I have a pretty big
query that I know is going to be
distributed out I have three compute
nodes that are available to me there are
some number of data nodes over here just
for simplicity sake let's also say three
as part of the poly based engine there's
actually direct communication between my
compute node sequel servers and the
Hadoop data nodes what that lets me do
is send data back into these smaller
compute nodes pull it into temp tables
basically do as much of the processing
as it can here and then send up the
small data set up into my head node
where I finish processing where I do all
final work and I send that data out to
the user in other words this is a
massive parallel processing for sequel
server it only works when you're working
with a Hadoop cluster but cross my
fingers I'm hoping that the poly based
team is able to get this to work with
other queries so that sequel server
becomes a massive parallel processing
system just like a new cluster would be
there are a lot of problems that that
help solve but it's not there today
this only works when you're working with
a Hadoop cluster poly base is not
perfect this is something that was
designed in 2010 that means that it has
the limitations of something that was
designed in 2010 the stuff that has
happened into Hadoop world since 2010
poly based kind of ignores so it's
MapReduce only there's no tez tez is a
directed a cyclical graph generator that
helps reduce this the
sighs of mapreduce operations basically
it it reduces the number of times you
write to disk and that makes things
faster it's almost a free improvement
it's built into hive now but is not
available in poly base there's no spark
spark is to me it is the current
database in or the current data engine
for Hadoop that has replaced MapReduce
this is the default now because it is so
much faster than MapReduce no spark
support that said poly a base is still
really cool and this is my preferred
method of working with a Hadoop cluster
when working also with sequel server so
let's take a quick look at that in the
last few minutes here I am going to want
to create a poly based table in order to
do that I've got to do some prep work
first the first thing that I need to do
is create an external data source that's
saying where does the data live data is
not gonna live in sequel server sequel
server is gonna have a pointer to the
data but it won't actually host the data
the data will be hosted in a Hadoop
cluster and the location will be this
now I left this in this is the version
that's available when you grab the
slides and codes and demos because I'm
assuming that you download a Hortonworks
sandbox and they by default hard code
that URL sandbox Hortonworks com in
reality mine is cluster II no it's on
port 8000 that is the MapReduce port
it's a configurable port but that's
default it's a standard with Hadoop the
resource manager is necessary for
predicate push down see what predicates
push down does essentially is it decides
I need to push down this predicate in
other words I need to generate a
MapReduce job over here
the alternative is I don't push a
predicate I just stream the data back
this way
well in order to create MapReduce jobs
the poly based engine needs to
communicate with a resource
manager yarn so it does so typically
that's the default port eight zero five
zero the resource manager location is
not required but if you do not include
it you will never be able to do
MapReduce jobs you will only be able to
pull data I have a data source I need a
file format as well this is how I'm
going to start defining structure comma
delimited and it's I'm just calling it a
text file format so delimited text
delimiter is a comma I have a location I
have a format I can create a table an
external table the first half of the
statement up till about that point
except for the word external it just
looks like a normal table so dbo that
second baseman this is not a remote
server this is just another table and I
could put it into a schema like Hadoop
cluster dot second baseman or I can just
make it a regular dbo table here's where
it gets weird
the location is my poly base file or
folder so I'm going to connect to HDFS
and I'm going to look for that file
called second baseman dot CSV now I can
scratch this out and just say tempo OTP
and in that case I would look for
everything in that folder typically I'm
gonna look for everything in the folder
because then I can have processes that
will create new files regularly and I
don't have to update my table statement
I define my datasource and file format
which we already know the last bit is
rejection this Hadoop cluster
semi-structured the data that's in there
doesn't necessarily fit the structure
that I'm trying to apply down here in
sequel server so I need to be able to
have a safety valve let's say that a row
is bad do I want to stop the job the
answer could be yes it could be I need
every row
to be correct or else we fail in that
case I would set the reject value to
zero but realistically I probably want
to have some slack I probably want to
give it a few values so I'll let five
Records fail before it gives me an error
now that there is a trade-off here
because let's say that I pick one record
then I'm probably gonna get a lot of
failures because oh there's two rows
that are bad but let's say that I pick a
million rows million rows have to be bad
before this thing gives me an error
probably not gonna get an error very
frequently but if I write a query where
like every row is bad for some reason
then it's going to have to read a
million rows before it gives me an error
message that says hey dummy
you probably didn't mean this the worst
case is let's say I pick a million and I
have a hundred billion rows and it turns
out that the one millionth error is on
row number one hundred billion it is
going to go through the entire data set
it is going to try to process everything
you're gonna see things and then it will
give you an error message it says oh
we've hit the threshold sorry something
went wrong let's just discard all of
these results that have been piling up
on your screen so that's kind of
disappointing because typically the
error message does not tell you why the
values were bad it usually just says
well good luck with that you got a
hundred billion row file you can use
grep or something right okay so we've
got that table we've created the table
and I'm gonna show you here that I
already have it in place in the OTP
database there's this external tables
folder so you need management studio
2016 or later to view this but their
external tables here's D biota second
baseman that was the table that I
created I can right-click
on it select it and it'll take a couple
seconds to spin up but it gave me back
all of my records 777 of them so let's
run a couple comparisons I'm going to
very quickly compare the sequel server
version using poly base to the sequel
server version using linked server query
I'm going to turn on actual execution
plans and I'm going to run these while
I'm running this let me note that this
query is just a rewriting of the linked
server query that I showed you and what
I'm trying to get at with this rewrite
is trying to tell the optimizer that you
really want to push down on second
baseman you really want to filter to
include just 50 records you really want
to do as much as you can against the
remote resource and then I do the same
thing down here with dbo that second
baseman so again that's connecting to a
poly based server or excuse me it's
connecting using poly base to my Hadoop
server I get back the same number of
records 50 for each I get back the fact
that Ben Zobrist is old and that he's
probably making a big chunk of change
but what I what's most interesting to me
query cost 99% versus 1% and the major
chunk up from that is I have to pull all
of the records from my link server and
then do all of the work in sequel server
and then sort and then join to my top
salary by age table so notice the
relative thickness of the lines fat line
fat line
in line these are 700 some rows that's
50 rows by contrast down here
my remote query I'm already filtering
out all but the 50 rows so technically I
did not push down the predicates
technically I did not run a MapReduce
job but even then I was still able to
take the data stream it down filter it
down to the 50 rows before running the
sort and then joining to my table so I
was still able to get a pretty big
benefit just from doing that and from
also taking advantage of the better
statistics where hey we don't have
10,000 rows coming in we have 50 rows so
I have a quick comparison table if
you're interested want to figure out
which to use there are other methods as
well one method that I've really grown
on is using Kafka which is a message
broker think of it like a service broker
or Microsoft message queue or a rabbitmq
celery 0 mq one of literally dozens of
other options but you can take that and
push messages individual messages from
one source and draw them into the other
actually I have a whole talk on that
there's a link to it in my slides so
wrapping everything up plenty of ways to
integrate Hadoop with sequel server I
recommend that you mix and match however
suits your needs figure out what works
best for you and go with it if you want
to grab the slides the demos the code
links to additional resources there at
CS more dot info slash on slash Hadoop
SQL if you have any questions at all
please feel free to reach out to me my
email address and twitter twitter handle
are up here and i'm gonna be here all
week so thanks everybody
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>