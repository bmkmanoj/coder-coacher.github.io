<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How one team built their first microservice - Jim Pelletier | Coder Coacher - Coaching Coders</title><meta content="How one team built their first microservice - Jim Pelletier - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How one team built their first microservice - Jim Pelletier</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ta3N93ce5tQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright looks like it's time to go how's
everyone feeling last session on Friday
spent rip brain fried okay that's good
um I'm just gonna be telling a story
today so hopefully it's a nice easy out
for you guys I'm gonna tell you a story
about how my team not just any old team
but might the team I was on I'm built
our first micro service so first of all
just Who am I that's me - beard my name
is Jim Pelletier I've been a dev for
about 10 years down in Melbourne working
on teams have been playing the the tech
role on the tech lead role on teams for
about five years or so and the moment I
work for a team called a company called
page up to make a HR product suite and
my team specifically makes a performance
management product within that suite of
software so those yearly reviews that
you have with your managers that
everyone loves
you know you get reduced to a number
every year for everyone to look at let's
start that's the product we make a few
other things about me just to a bit of
background I run a I use a group the
ultimate user group down in Melbourne
and involved in the DDD conference down
there if you know of it I used to do a
bit of training for what was then called
in-service bus that's now the particular
software stuff so I've been building
distributed message-based systems for a
while sort of since before they were
called micro services so yeah what I'm
going to talk about today is like I said
literally just a story so what my team
set out to achieve and why we decided to
go about building our first micro
service how we plan to go about doing
that and the decisions we made before we
started what we actually built at a
really high level anyway or some some
key parts of what we built and then how
things turned out at the end of the day
who I'm expecting you guys to be so this
is where our avoidance participation to
check a few things I am expecting you
guys to sort of have at least a passing
familiarity with what a micro service is
and some of the domain driven design
concepts that are often associated with
it like aggregate routes and bounded
context
whose roughly familiar with all those
okay well we do cover it a little bit as
well so hopefully we don't lose you a
long way if I do I'm more than happy for
you to stop me but I'm expecting most
people probably haven't built one am i
who's built a micro service okay perfect
sounds like I've got it about right okay
cool so what i hopes in this story for
you it's mainly a little bit of
inspiration a little bit of reassurance
that this is a journey you can embark
upon and what it might look like if you
do given the knowledge that we've just
established because most of my team also
only had a passing familiarity with some
of that stuff i knew a little bit more
but a lot of them had never done a lot
of this stuff before some learnings from
the specific mistakes we made everyone's
going to make specific you know their
own set of mistakes and it's very hard
to learn from other peoples but I'll
tell you mine and you can take out of
them what you can as well as some
examples of how we used some concepts
out of demand-driven design in order to
make sure that we manage the complexity
in our software as it sort of as its
split and as a group because of the
nature of the of the talk I'm giving
it's a very high-level overview of a
long journey it's about six months worth
of time that elapses in this so I'm not
really going to dig down into any
details unless you guys put your hand up
and ask me to at which point I'm happy
to so I'm time permitting I'm definitely
willing to take questions along the way
so please do do shout out I those sorts
of interactions anyway so let's start
with what we wanted to achieve so why we
went about building this micro service
and making some of the decisions we did
and first let me introduce you to the
page up dev team this isn't actually
them but it's about the right size so so
when I did a Google image search for 40
people this is what came up so this is
this is the page up dev team there's 40
of them and these are the Wahoo's the
Wahoo's is the name of my team
it's named after a fish I've got no idea
where teams are named after fishers
that's not actually them either but it
again it is the right size
that guy's beers not big enough to be me
and this is the size of the teams that
we compete against so not the page up
team but the the wires themselves those
five people we saw on the last slide we
compete against people that make
performance management products with
teams this size some of our biggest
competitors are people like Oracle and s
ap and those kinds of people who who
make really really big performance
management products and the way our
performance management products often
sold into an enterprise is basically you
get compared on a feature matrix right
so big list of features who checks the
most boxes wins so when that happens and
when you're compared on how many
features you've got versus them it
becomes really important to be able to
get new features out quickly when
they're uncovered that someone else's
gotta check in a box that you don't
check yet so time to mark it's becoming
more and more important to us as our
markets getting more and more
competitive so this being a developer
conference the part of the time to
market I'm going to focus on is this you
guys is everyone used JIRA at some point
in time awesome well even if you haven't
basic it's a pretty simple concept
stuffs not done then at some point you
start doing it and then at some point
it's done so just moving cards from left
to right and today for the purposes of
the talk that I'm doing and a lot of the
goals I'm going to be talking about time
to market it's just gonna mean that so
that's cool we sort of define what time
to market is and we need to improve it
and it's a little bit hard to see on
this but this is actually a screenshot
out of JIRA and it's something that
comes out of the box with JIRA and it'll
do it by itself if you if you just focus
on moving the cards left to right jr.
will make this graph for you what this
graph is it's called a control chart and
what it tells you it tells you the cycle
time of your work at any period of time
so what it means is on the 9th of
October it was about one and a half days
from someone to take something from not
started to done and you can do all sorts
of things like those little check boxes
here to decide the cycle time between
two columns so you can see how long
things I'm taking
to move between columns and how long
they're moving overall and I'm not this
is just when I pulled off the internet
so we'll come back to the control graph
later but what I did was able to do
because of this feature of JIRA is build
this and this is got a value stream map
value stream maps are tool that sort of
came out of lean and and Toyota and I
think that kind of area but it basically
is a graphical representation of how
long it takes for us to do work and the
different stages it's moving through and
how we add value along the way so you
can see if I if I include everything
from the moment that we select an idea
is something that we want to do and sort
of elaborate it into a bunch of cards or
stories in JIRA so we can start tracking
the time on it then we do a bit of work
on it
we test it and we deploy it and you can
see here most of that times actually
it's sitting there waiting for people to
pick it up so that's not really useful
where you need to address that in other
ways that's not got to do with how
quickly we can build software so I'm
gonna focus on basically if I blow up
just this section here we get something
that looks like this so now we're just
looking at a representation at the time
it takes when the developer you know
everyday sitting at their desk picks up
a card and gets it to done and I'll
explain a little bit about what's going
on here
um there's all I'll get to the absolute
numbers as well at the end of the talk
but for now it's just interesting to see
the the relative sizes of things so it
took you know a fair chunk of time for
the developer actually write the card so
think about what they want to do write
their unit test all the stuff that goes
along with that and then they have to
wait for a little while because we've
got a code review there and someone else
has to look at the code and someone else
has to be really look at it and all that
sort of stuff now get another big period
of waiting and then we actually test it
and then another big period of waiting
before it's in the users hands so the
reason for all this is is got to do a
lot with our current architecture so the
current architecture is it's a it's a 15
year old codebase it's a model it's got
a lot of different technology
in there it was definitely written well
before unit testing was a thing so we
need to make sure ensure quality and to
do that we have a test environment that
we were released to and developers can
manually test their their code during a
production like environment now it's
quite a big deployment as well it's sort
of a whole server farm full of Windows
servers to take a while to stand up and
at the time of this it's gotten a bit
faster than by the time that we embarked
on this journey to about four hours for
us to deploy and you serve a farm into
production and when something's taking
that long to do it sort of gets painful
to try and do it on demand so what we
ended up doing is we get a release train
going and we deploy once a day on a
nightly build to our QA environment for
the developers to test in so that's
what's going on here once the code
review is done we wait for a day for the
next day for it to be built and we test
it and then once it passes that it takes
another day to get out to production
that's what's going on there and that's
sort of what's some of the forces that
are creating this value stream map which
sucks right
waiting is one of the most wasteful
activities you can do there's no value
being at it you know the only time that
we're actually adding a lot of value
here is is during this period all this
stuff's waste so we really want to get
rid of that so we want to move from this
to this and we can do that really easily
just with going to continuous deployment
right so being able to deploy stuff
straight away verify it and then get it
into our customers hands this is one of
the main reasons we chose to build a
micro service because we can't fix a lot
of the model this stuff but if we build
something separate we can at least avoid
all this wait time we can get to this
continuous cycle of getting value out to
our users is that to really be part but
there are other reasons as well
sometimes I know that writing code feels
a little bit like this you remember
there were these guys before they're a
really small lean team right we can we
can do things we can move fast but the
thing is that every time we write code
these guys kind of get in our way right
because we're in a shared codebase we
have shared components that they might
change
affect us and we might change stuff that
affects them and worst of all we might
have to actually talk to them who's
talking to people takes forever and the
more people you have to talk to they're
actually the lines of communication grow
exponentially as well so it's a really
time-consuming thing the more people you
have to work with as well you can kind
of think of this there's a good analogy
we use when we're inducting people at
page up talking about sort of teams of
mountain climbers right you sort of got
all these guys climbing up the mountain
some going fast some going slow until
you realize actually this guy up there
is not moving any faster because they're
all tied together they can only ever
move as fast as the slowest guy so we
want to cut that dependency that we've
got between the teams they're not going
to fall off the mountain necessarily but
then we can all climb at different
speeds we can all go as fast as each
team is able to and of course by that
logic you know the wahoos will be the
people up at the top by the time we're
done right so cutting dependencies
decoupling these are terms you hear
about people talk about a lot when they
talk about microservices I'm gonna just
introduce a different term for it today
and I think it's important I hope I hope
I can convey why decoupling if you
completely decoupled something it
actually becomes useless right the end
of the day things need to be coupled
together to either receive information
from other places to make decisions on
or to give information back to other
people to make sense on if you can't
receive all or send information out of a
service it's it's useless it actually
can't do anything so what I'm going to
talk about today is autonomous and I'll
go into hopefully along the way of that
I'll explain to you why autonomy is an
important property and when you think
about it that way you'll make some
different decisions than when you're
just trying to reduce coupling as much
as you can I guess it's sort of time to
talk about exactly what we were building
this is our product this is a sort of
the main the crux of the interaction
between a manager employee and a
performance review in our system so you
can kind of see basically there's a goal
there and it would have some description
of what the goal is and then these two
people basically come in of the year or
along the way they're supposed to
actually
update with progress manager right here
and the employee writes there and they
do that along the way and then at the
end of the year or whatever the period
of time they're using is they give each
other a rating and then that's used to
calculate your overall score and what we
heard from our users was that people
weren't using this all year people were
forgetting about it all year then at the
end writing a big long essay trying to
justify why they deserve four stars and
they really hadn't done anything towards
the goal so we were trying to actually
affect our users behavior through the UI
design and basically just took a page
right out of social media and turned
what was too big essay documents and I
know you can't see it as well here this
is a little bit washed out this
projector but each of these is a
individual comment in a feed so what
we're going for is a back and forth
conversation between the manager and the
employee so when we look at how that was
implemented in our system it's sort of
what's going to look like this right so
we'd have sorry I'm not talking about
that yet what I'm talking about now is
once we've got these two systems is how
they want to talk to each other so one
way we could do it like I said a manager
and an employee commenting on this goal
and you don't want other people to be
able to comment on it basically so
that's one of the conditions we have if
you're commenting on this thing I need
to know that you're either the manager
of the employee on the review so we
could do it like this where basically
every time someone tries to put a
comment I call over there and check who
the manager is I get a response and I
check who the employees and I get a
response and we definitely did not want
to do this if we do this if we need to
call another service in order to be able
to fulfill our capability so in order to
be able to let people comment on things
we needed to talk to another service
then we can't operate if they're not
available so if for some reason that
thing goes down if that changes if they
change their interface and we haven't
updated yet if any of these things
happen we can't operate and that's when
the different distinction between
autonomy and coupling comes in so I want
to be able to operate no matter what the
state of this and that's why we
definitely didn't choose synchronous
stuff
I'll talk about what we did in a second
before I just want to talk about another
reason we chose not to do this sort of
synchronous coupling it's bad if you've
got synchronous coupling between two
things but if I have to talk to this
service to get information in order to
do what I need to do and it needs to
talk to that and it needs to talk to
that then the latency is gonna build up
across all this I'm gonna become really
slow I'm also going to have with each
extra service in the chain a higher and
higher chance that something's going to
be wrong whether that be that service is
down all that service has changed its
contract or something like that
it really sort of starts to compound as
you have these synchronous calls
rippling all out through your system so
though it's bad to have it between two
services but the reason you don't have
it between two services is because if
you have it between two then you'll have
it between more than two when these
things start to expand so what we do
instead basically is we we're going to
use a pub sub model over a bus where all
of the services that do things in the
world will publish information onto this
bus and we'll sit here sucking the
information off the end and it will get
it as soon as it's available and then
we'll have it when we need it is the
concept instead of asking for it at the
time we need it so if we've got our
existing monolith over here and we've
got the new micro service over there and
a bus between the two that's great but
it's gonna take us a really long time to
build that bus we've got to stand up a
bus and understand all the properties of
hosting and running a bus and all that
sort of stuff so we're actually not
going to do that we're lucky that when
we were just building two things we
don't actually need to do real
publish/subscribe between stuff so we're
just going to run two queues and these
things can put messages on each other's
queues for each other to process so
that's great we've sort of we've decided
that we want to build this thing in
order to deploy continuously we want to
make sure we we don't couple
synchronously to anything it's gonna be
something that allow us to operate
autonomously and move faster but just
working in a Greenfield thing that's
going to give us a bonus to write just
new things are easier and it allows us
to do
I hold a lot of stuff we can't do
already when things are small and when
things are new they're necessarily
faster we haven't had time to accrue
tech debt yet if it's small enough that
one person can just fit it inside their
head and immediately reason about the
whole thing
then they're going to be able to make
decisions about what to do quicker there
are some downsides so our existing
monolith has got a lot of plumbing right
it's got a lot of infrastructure support
there with it there's things like you
know authors multi-tenancy in there
there's really reliable logging there's
a whole host of stuff in there that
we're going to be giving up but we made
the decision that we thought by giving
it up even though we'd have to rebuild
some of that stuff we'd still get again
and because we wanted to just use all
the new cool things and it you know it
might sound like you know this is just
what developers always want to do but
we've got a really in the might in our
model this code we're tied to a really
old version of an RM that has some real
implications on the kind of code we can
write and how productive we can be and
it's really going to be expensive for us
to change because it's used in so many
places so sometimes just even being out
to use new libraries is really helpful
but specifically we were gonna choose
these three docker AWS elastic container
service and then dotnet core on top of
that and I'll explain why like I said a
big part of this change was us moving to
continuous deployment and when you move
to continuous deployment a few new
things become true basically because
you're deploying so often you need to be
sure that each change will work in a
automated in quick way so one of the one
of the the things you'll need to be able
to do is ensure it works in a production
environment which is why we chose da car
we chose docker because docker allows
you to develop and run locally what is
very close to your production
environment by running inside a
container so everyone familiar with
docker yeah cool all right awesome so
that's why we chose docker first of all
as well as the fact that as we deploy
more you want your deployments to
faster and faster if deployments take
four hours and you're deploying ten
times a day you're gonna actually be
growing the queue of code that needs to
get out and it'll take forever to
actually get out to production and
you'll eat up thousands and thousands of
server farms that are constantly
swapping out so deployments also have to
be quick
and we chose darker because and this is
but well before Windows containers were
available but a container on a Linux
container or Linux host spins up like
that whereas standing up a Windows VM
can take forever sometimes so we chose
that because it will allow us to deploy
more quickly as well and then once we've
chosen docker and Linux dotnet core sort
of your wet wet on it we're done that
team so don't their core is just sort of
where we go from there so that's why we
chose those sort of three main pieces of
tech at the same time we deliberately
did made some choices not to choose some
things so we are going to do dde we're
going to use events between our services
publishing to each other's queue we're
going to you see QRS inside our service
and we're gonna use all these things to
manage our complexity deliberately
decided not to use event sourcing I
thought the team has never done a lot of
this stuff before let's you know just
leave it with that for them to learn for
now and and we'll worry about event
sourcing later I don't I thought I
thought we could we could get away with
not using I didn't think it wasn't
necessary we also decided to use jQuery
on the front end it's not a huge front
end it's not a really big deal so jQuery
adding web pack to it to allow a little
bit of composability and we sort of
stuck with that so a balance of new
technology for specific reasons as well
as some pragmatic choices in order to
try and get things out so I'm gonna dig
into a little bit now what we actually
built and how we laid it out I'm only
going to do diagrams I'm not going to
show any code so new it was a last
session on Friday but there are a few
Givens when we sort of started
architecting this solution one is that
we're in AWS so we're working with AWS
services
as a as a development team in general we
believe DDD is a good way of managing
complexity in our software so we're
gonna sort of get that for free HTTP is
is sort of the level at which we're
willing to couple things together at
everything good - okay CDP so when you
when you talk HTTP you're not limiting
your choices in any other way it's like
we've got some stuff that runs in
elasticsearch and Scala and if we had to
chosen any kind of other technology
perhaps they couldn't have talked that
and we have got our existing monolith as
well so that model is not going away
anytime soon we've got some really
valuable code that our customers like in
there that's written in VB script right
it's not going away we're not going to
refactor that anytime soon so we need to
sort of keep into account that the
bunless gonna stay for a long time so
before we started it looked the part of
the product that we're talking about
looked roughly like this there's a
review and a review has a bunch of goals
hanging off it and those goals have
comments on them
and then there's a manager and an
employee assigned to each review as I
think it's an MVC front-end a bunch of
code in between that implements their
business logic and what we want to do is
turn it into this like I said before
right so they've got the old stuff over
here and the morale lifts and you stuff
over there with queues the browser's
going to compose the two together in
order to make it all work so we're going
to take this stuff here I'm going to
take that goal in the comment and we're
gonna page up new project and create
this thing over there this is a template
that we've developed amongst our teams
every time this is it's the default
choice when people make a micro service
now at page up
basically you get a bunch of terraform
scripts Tara forms a DevOps tool that
allows you to create infrastructure in
code and deploy it and so we we spin up
an instance of this infrastructure and
needs each instance and then you also
have a bunch of c-sharp project
templates that go with it they're pretty
pretty simple ones a Web API the other
one is just a little console app that
runs and reads off a queue so we've got
our SP AR jQuery SP a there we've got
our API but it's going to talk to we've
got the process it's going to read off
the queue and in dynamo we're just going
to
use the key value store there to store
this goal and then once we do that the
idea is that we can iterate on this
really quickly to implement new features
so we're not just aiming to go to this
this feed of comments the idea was that
this is one of the core interactions of
our system and over time we want to go
to iterate on it quickly and build new
rich functionality in there with why we
we chose to do this part of the project
so I started trying to lay out how all
this looked in AWS and how elastic
container service worked and all that
sort of stuff and it got a little bit
crazy and I don't think it's all also
that interesting I think we can learn a
lot for a lot about roughly how this
stuff works by just looking at our build
pipeline right so a big part of this is
doing continuous deployment so I'll just
talk to you about how a continuous
deployment pipeline works and hopefully
that gives you enough insight into how
elastic container service works we've
got two repositories one for the backend
one for the front-end basically whenever
a developer pushes code up there and
then merges it to masters we'd deploy we
start deploying Travis watches that
repository for us and has a little bash
script that it runs in order to build
our code so it just run some unit tests
and starts building a docker container
from it for us from an image that
Microsoft provides that Scott dotnet
core on it once the containers built it
pushes into the elastic container
repository elastic container
repositories a docker image repository
hosted by AWS on your behalf
you just push your image up there and
then you tell the elastic container
service that there's a new version of it
and then the cluster of machines that it
runs just automatically go off and start
pulling in new versions of this image it
manages rolling Bluegreen deployments
for you it basically will make sure
there's a certain number of your tasks
running at any one time and you specify
all this by configuration you'll get any
questions about that before I go on a
sort of breeze over it pretty quickly
cool the front ends very simple to just
push to a repository Travis web pack
build into it
a file storage bucket that then updates
a CD and it's all pretty straightforward
on the front end so that's cool we've
sort of seen how the backend looks and
it's going to communicate now and on the
front end we've got this screen and we
need to process a command so I'm going
to show you what it looked like to post
a comment how we structure the code news
Dedede in order to try and maintain our
container our complexity when I click
that button sorry it's going red there
basically what's going to happen is the
browser after having whoo having a
little space here the browser's going to
load the user interface and then at some
point that single page applications
going to post a request to our API the
API just performed some really really
high level validation on it to make sure
that it's not a garbage request and at
that point we sort of say that it's a
validated command and it pushes it up
into this command handler thing so we
used the the mediator library in there
at Jimmy Borges mediator library to sort
of do a lot of this stuff but what
happens in the command handler and this
is sort of gonna happen in anything that
the model itself on CQRS is first we
load the current state from dynamo we
just process that command through our
domain and then that's going to produce
some new state we need to say back there
as well as a bunch of events so one of
the events will go out through these
projectors into a view model that view
model is another representation of the
same state that can be used by the front
end in a very simple fashion as well as
it's going to go back into the monolith
and what happens when it goes back in
the monolith is these things remember
when I showed you before they there were
tables we took out of the modern list to
put our new microservice it needs we
need to put the data back in there we've
got all sorts of places where people are
running custom sequel queries through
you know direct access we've given them
from you know customers and stuff and so
we can't decommission those tables we're
going to project all the information
back into there as an array only way
so that we don't have to update all of
that at the same time so the point of
all this though is that this is a
seekers pattern the pattern of the plan
of it is that each one of these things
becomes very simple all this API does is
know how to validate commands all this
thing does is know how to pull in save
state then there you've got your whole
domain model that encapsulates just the
business rules so that's a really
valuable part and this view model here
is in exactly the state that this guy
needs it so he just goes get and select
star from and it all becomes very simple
these are the patterns that are supposed
to make this quick to moving to do that
we need to get the right state in here
in the first place and we do that by
this is now the events between these two
bounded contexts so one thing is sending
events to another in order to make sure
we know who the manager or an employee
is when we go to add a comment to this
thing so you know at some time some
state changes here the managers say gets
updated and we publish an event over
there when that event comes over it's
going to be called something like the
manager on review changed right now
something we did that was really
important to making sure that our code
remained clean was introduced this
anti-corruption layer this thing over
here this code doesn't know anything
about reviews at all right we don't want
to have to let it know anything about
reviews because it knows anything about
reviews it's gonna need to know about
users it's gonna need to know it like
and it's just gonna grow and grow and
grow and the amount of concepts we're
gonna have in our domain will become the
monolith itself so by doing this
anti-corruption layer here we transform
that and we transform it into something
like manager changed on goal and now
this thing understands it perfectly and
we've maintained and made sure the size
of this code stays really small so
that's how we keep the backend
autonomous and small simple parts the
front ends got some patterns were used
as well what we've got here is the UI
but this is to you eyes we've got some
that's loaded from the model it
and that's coming out of MVC and then
we've got our little chunk of single
page application hosted in the middle
there now that's cool there's no
interaction between these two things so
those things do things that are
completely decoupled and that's fine
they just talk to their own backends um
but like I said before it gets a little
bit more complicated we want to enrich
this feed with a whole lot of other data
so we've got this thing in our system
called journal I won't go into what it
is or where it is but it looks like this
basically I can pop up a list of
journals that have been created by me
and other people and I might want to put
this into my feet of stuff to show that
I've made some progress towards this
goal and the complicated thing from here
is that that's in another micro service
as well so we've got the purple stuff
coming from the monolith we've got the
red stuff coming from the micro service
we're building we've got the yellow
stuff and that's coming from an existing
micro service so we want to be able to
do this in a way that all these things
are actually deployed updated and
operate completely independently and
autonomously and the way that we do that
is something along the lines of this so
when the monolith first loads up it
loads first and it loads itself into the
browser there's a little bit of
JavaScript that's sitting in there and
when that loads it loads our single page
application this other things going to
load as well but it's going to be hidden
in the background for now and we load
our state from our service so these
things are paired and those guys are
paired as well and once we've loaded our
state someone's going to click that
button click the button to pop up the
other UI and we're going to fire a Dom
event right we've got events in the back
end let's use events on the front end as
well and that events going to going to
say hey someone clicked this button and
when they click that they clicked it on
a goal of this ID and then this thing's
going to the the list of journals is
going to show up and it's going to know
what state to load because it's been
passed a little bit of initialization
state through that event and then
someone's going to select which one they
want to link and that sends a command
back to its micro service and when that
happens we publish events out to all the
other services that might be interested
in it that means that when we go to load
our feed of stuff back up the thing will
be linked it in there except not really
now we've introduced a little bit of a
race
addition right because all this stuff's
happening asynchronously those two
things might happen in any order so the
way we get around that without having
the tile is back up together is again
with just another event so we just fire
a Dom event back into the window and
then other things gonna listen for it
and make a little cheeky update to its
UI so it can do everything a little bit
of time to catch up success no not
really
so at this point we've managed to get
something deployed any into production
and that's great it's you know that's in
treatment itself and doing all this new
stuff fantastic that's not what we're
really trying to do so let's talk about
how things really turned out and what
winning really looked like for us in
this situation so remember the start of
the talk I was saying that really what
we're trying to do here is move fast and
we've got to keep up with you know our
competitors here so let's check JIRA
right remember the control graph that's
what it actually ended up looking like
this is the whole year the whole of last
year and we started the project I think
somewhere around here and then finished
it somewhere around there so it's a
little bit hard to see because of the
scale of this of this graph but this is
what happens so the average cycle time
the average time it took us to get
something done before we started was
about four days and then we started at
work and we started doing all these new
things and using all those new things
and standing up terraform the elastic
container services and docker and dotnet
core all that sort of stuff and our time
went up dramatically it went up to about
eight days so about doubled at this
point in time it's taking us twice as
long to get stuff done there was a bit
of a support spike in there as well we
have a every time of year when people
work trying to close off their
performance reviews at the end of
financial year it sort of we had a lot
more support cases come through so it's
a little bit inflated by that but most
importantly once we got over all of the
learnings we did get down to two days so
that's sort of we doubled the speed at
what which we could get things done by
doing this
to sort of blow that graph up a little
bit and give you a better idea of the
scale of the improvements we made over
that time looks like that so that's the
four days the eight days in the two days
now if you remember when we looked at
our value stream map we kind of could
have guessed that would have happened
right I mean we're going from this to
this it's pretty much half I would have
been surprised if if by going to
continuous deployment alone we would
have really seen the full hundred
percent so I think it's probably about
my guess anyways it's probably about
eighty percent due to the fact we're now
continuously deploying and then maybe
twenty percent due to the sort of the
DDD and the other patterns and stuff
that we were able to employ during that
so that's all well and good but how did
it actually go down with the business
and I would lie if I said to you the
during that time when things were going
up that everything was going smoothly
there was definitely some friction with
the business as we did this you know
things like this is taking longer than I
expected were not directly said to me
but I heard secondhand through other
people and that sort of stuff as well
and we clearly lost a bit of trust and
there was a bit of a communication
breakdown there and I think that like
that was definitely our fault I think
it's really important to make sure that
you tell the story about why doing this
why things are taking a bit longer for
now you know I've just shown you all
these really pretty grass but I only
made them once I knew I was gonna come
and talk to you guys about it it would
have been really good if I had to made
those graphs to talk to the business
about it now I couldn't have shown them
the JIRA stuff before it happened but
those value stream maps would have been
a really useful tool to explain why we
needed to make the change that we did
and now that I have made these graphs
and I have shown them to some people in
the business they're really happy with
the changes we've made and they
understand why a lot more now so I think
the biggest learning and the biggest
takeaway that I got out of all this was
to make sure you tell that story and
make sure you've got people along the
way if you don't want to you know have
that those trust issues come up
now all the things so we've docker
remember we're docking we had AWS
elastic container service and we had
dotnet core how did they all go oh we're
all terrible
none of those helped at all
so that idea that docker gives you a
production like environment in order to
test your changes early on the
development machine everyone once needs
wouldn't be a studio right they want to
hit play so no one ran it locally on
their development machine and then when
dotnet core change their cryptographic
libraries at some point we deployed a
version that couldn't decode our I war
tokens anymore and everything went down
so we definitely didn't realize the
benefit from that
we reuse the last two contain service
and one of the reasons reasons we're
using containers on Linux was because
they would deploy quickly the way we set
it up because we didn't really
understand what we're doing we pushed an
image out to each region and then it
took basically an hour and a half to
push all these images out I think we had
a structure differently we could have
made it quicker but it was a really slow
deployment process who was still hours
and hours to get a together change out
and if you remember that period of time
that I was talking about there during
last year dotnet core was you know
changing underneath us every day and so
elastic container service was new as
well so there was breaking changes
underneath us all the time and that
really hurt us a lot as well so yeah
life on the bleeding edge wasn't fun as
well as some of the properties that we
chose those things weren't great not
only when things broke did we have to
fix them we had to work out how I spent
a lot of time in github threads on
the.net core project trying to work out
why my stuff was broken because no it
was broke it it broke today and people
are just realizing it's broken no one
even knows why it's broken yet so that
wasn't that wasn't cool but we did have
a lot of wins um you know the
anti-corruption layer worked really well
that was one of the the things that went
up when we talked to other teams at page
up who tried to use dynamo who tried to
do this sort of stuff and just ended up
with their own little mess or they
couldn't even persist their stuff
properly that seemed to be one of the
big wins that we had it made our domain
model really
more and easy to reason about and the
way we structured our events and just
using events made things really easy and
of course read stores again were a
massive wind
I'm gonna call jQuery and web Packer a
half half like it was cool it was fine
before what it didn't I think we did
make the right choice or not taking on
something else new but I'd probably want
to rewrite it now but I probably can too
because it's really tiny so but we had a
couple of really big issues not
transactionally publishing and
processing messages so that we could
guarantee our system was eventually
consistent I'll talk a little bit in a
second about what happened there and and
how you might avoid it but the biggest
one that I'd probably like to do over on
if I could was not event sourcing things
and why that became really important is
because because we had problems where
things would get out of sync between our
monolith and our micro service basically
what was happening is we lose updates
too when the managers changed then the
real manager wouldn't be able to leave a
comment on the review and that's that
was bad on its own but without event
sourcing things became really hard to
get that back into what's right state
we're not running in sequel we can't
just run a sequel script against dynamo
got to manually run code against it now
so it became really hard to repair the
state that we did have when things got
out of sync and thinking back on
everything we went through now if we had
to chose an event sourcing and just go
on with you know elastic Beanstalk or
something on AWS that was just running
dotnet 4 or 5 and sort of ran out of the
box for you I think we would have would
have had a lot nicer time so let's talk
about how things were getting out of
thing for us basically I said before
sometimes the manager would get changed
inside of our monolith and at that point
we needed an event to come over and tell
us that that was the case basically this
would fail to happen for some reason
sometimes it was just a network issue
between us and SQS and then sometimes
we'd have a programming bug in the
anti-corruption layer somewhere and we'd
sort of lose the update of something
like that
what's supposed to happen there is it's
supposed to go back on the queue and
we'll retry it but because of a program
bug didn't even get on there so I
actually knew that this could happen
like I said a builder of distributed
systems before and I knew how important
the properties of these systems were but
I thought about this team who hadn't
done distributed systems before and I
thought about what it would take to go
through all it's quite difficult to get
things to be guaranteed and eventually
consistent like that so I thought it's
fine we're not going to lose that many
messages right like it'll be fine
managers don't change that often during
the year and then we had one of our
biggest clients go through a restructure
a week before they ran their performance
reviews and when they got into the
system and realized they couldn't leave
comments anywhere
we couldn't rebuild the state quickly
because we were an event sourced so all
those assumptions and all those
decisions did compound to give us some
real problems so I think it was probably
a really bad assumption of mind system
to think that that would be ok I
definitely didn't run any numbers on
what the likelihood and what the impact
and I didn't consider how would recover
if it did so it's very least you're
gonna make assumptions about the
guarantees your system has backed them
up a little bit better than I did
so this is the overall score we're
nearly done now so you know we had a lot
of a lot of failures a few wins but this
gave us everything we needed at the end
of the day so purely just moving out to
a micro service for continuous delivery
purposes was worth it for us and that's
it we're about 10 minutes early but I
figure that's ok on a Friday yeah yeah
well yeah yeah so let me get the slide
yeah I'll repeat the question sorry when
I've got the picture to sort of help
explain it so the question was the data
moving back and forth between the two
if basically if all the data is moving
back and forth between the two won't
they both continue to grow all right so
we're not just keeping all the
information about comments over here the
comments are actually coming back
through to here as well the answer is
when we continue to grow it'll stay how
it was so it won't get simpler so we
were pushing the comments back this way
and into the table that already existed
and the reason we had to do that was
because there are a lot of other places
reading from that there was reporting
being run off it like anyone could wrote
it right an ad-hoc query against it
there are actually other things like
there sometimes has to be a comment on a
review before it can be approved for
instance so we're kind of with the way
we conceived of that is that this is now
a read store for the model list to
operate off yeah
but it will never get any more
complicated yeah yeah sure so what it
looks like
it's it's there right so it's it's the
first thing that reads off the queue and
like I said so the event that comes in
there is called for instance manager
changed on a review and it's got a
review ID in the manager ID the problem
is that we want to update the the
manager on this domain model which is
just a goal and we don't know what goal
is things for so we keep a bunch of
mapping tables in dynamo there and all
these are as a table is a review ID and
a list of goal IDs and then so when that
particular event comes in it gets
translated into n number of goal
managers changed on goal of
and now it becomes really simple for
this it just gets the goal and says
here's your new manager it doesn't this
thing doesn't have to remember which
goal is we don't have to do a
complicated query across this data set
in order to know which goals are the
right goals because all of those places
in the places where the complexity
starts to to come back in cut things get
coupled together
does it make sense a bit ok yeah it's
it's just a set of classes it's just a
it was a folder with event handlers in
it that the event took an event of one
type and it published an event of
another type on a little internal bus
yeah yeah yeah look we're gonna stick
with it we know what we're doing with it
now and I think we've probably learnt
all the lessons we need to in order to
use it well
so we'll use it differently next time
also we're all using it all the teams
are using docker so it's something we've
kind of standardized on I think we just
went there too early too early like in
the global sense of it wasn't ready and
too early for us in the sense that we
weren't ready to go there yeah yeah hmm
no we didn't I wasn't interested in it
because so streaming is a feature of
dynamo that basically publishes events
every time you save the event to
publishers in the same format as this
right so it'll it'll basically publish
my domain model every time and that's
not what I want to expose to the rest of
the world because that's basically
giving everyone the wholesale access to
my to my object model it's gonna allow
everyone a couple to it I wanted to
explicitly publish an event with the
right semantics and the right level of
information in it yeah
I'm hoping twos better
we still haven't properly upgraded this
to the latest version of one before two
came out it sort of go to a point where
we were done and we went okay we'll just
put it down now and step away for a
little while
look there were no showstoppers in there
there were just a lot of surprises going
through that we had a little bit of
trouble with their certificate support
for a little while I don't know if
that's something everyone had trouble
with but we had almost a you know we're
to do something a different way because
of that but otherwise we would just have
to roll back to the previous version and
spend time working out what to do next
so it wasn't that it was necessarily
unreliable in you know we do strange
things in production it was just we
didn't know what when it would break and
then we didn't know how to fix it at the
time consuming yeah how do we put
together would ah so yeah right so for
instance right so this thing is but it's
a div it's just a div so there's a piece
of code that sits in the monolith right
there's a script tag when that script
tag runs on page load it looks for
there's a div in this monolith code as
well with a special tag on it and it's
also got data attributes about the ID of
it so this goal has a specific ID so
there's a div somewhere that's like
class goal I and and data dot goal ID
and then the idea the goal and so the
the the JavaScript grabs this div and
then knows how to load and talk to its
back in to get the state to populate it
yeah yeah we're putting the auth token
through the data ID as well so that's
something basically we've got a we've
got a less-than-ideal
identity server implementation that's
been upgraded since so we've got a new
solution for that now not quite sure
what it is to be honest though we'll be
changing that one so shortly yeah
yeah yeah we had docker compose I think
that the main reason that people didn't
do it local is because I wanted the
visual studio editing experience so
you're sort of would have to you'd have
to edit the code and then build a new
image to get it to run so there are ways
you can do it where you map folders
inside the container and all that sort
of stuff but we we sort of a little bit
beyond us to go that far to get that
live code editing experience inside the
containers on our local system and so
that that was the friction that stopped
us using docker properly I think yeah
they call yeah right so the question was
sorry for everyone else how are we
finding the Dom events and and the
contract between them is that is it
stable enough yeah yeah well I mean it's
just like the the contract and our
events it is a little bit different
because the events the way I like to
model my events is that if you make them
named after the business concept they'll
be stable because the business is stable
and if they're not if the business
changes then you can incur the cost of
changing them
yeah we've not had much change between
the two yet but I get like they are a
lot granular they're sort of like button
was clicked so you know if that button
goes away then you need to change the
other one as well yeah yeah exactly so
there's still it there's there is still
a shared contract between the two but
yeah yeah
yet so the question was how did you
choose which version of.net core to use
and and were you using the latest all
the time we weren't using the latest all
the time but for instance so we started
unlike preview 1 or something like that
it was one of the beaters that we
started on back when D and VM was still
a thing and that sort of stuff and so at
that early stage it made sense to be
upgrading because they were adding new
features and then we sort of stopped for
a while and didn't didn't upgrade
because we were on something that was
stable then they started deprecating
things as well behind us so we sort of
had to start moving to catch up with
them and then there was some specific
versions we had to move to correct
between because of that certificate
support issue that I talked about again
so we weren't just trying to keep up
with the head that we went that's not
why we were doing it but yeah there was
still a lot of necessary change at that
point the question was how we move to
event sourcing since no but I think we
will either for the next project or we
might rewrite this one at some stage it
is pretty small end of the day and I
think we can rewrite it quickly now
we've got the contract of the event sort
it out
one thing we are going to be doing
really soon is going and fixing all the
synchronization issues and making sure
everything's guaranteed delivery and
that sort of stuff cool all right we'll
have about five minutes back and get to
the pub with a little belly thanks
everybody
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>