<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sherlock Homepage - A detective story about running large web services - Maarten Balliauw | Coder Coacher - Coaching Coders</title><meta content="Sherlock Homepage - A detective story about running large web services - Maarten Balliauw - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sherlock Homepage - A detective story about running large web services - Maarten Balliauw</b></h2><h5 class="post__date">2016-11-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qPBMY6eI_W8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">awesome can everyone hear me properly
perfect well welcome to the graveyard
shifts of the first day of NDC thanks
for being here really appreciate it it's
for me coming from Belgium it's quite
later quite early I actually have no
idea what time it is but it's always
nice to have people at the last session
slot of the day especially in this
session I think it's a nice one nice
title to its Sherlock homepage I
detective story about running
large-scale web services it's kind of a
disguise title for a war story on stuff
we did on the nougat server last year
when we had a lot of trouble with
frequent downtime frequent slowdowns of
the application frequent errors that you
may have been seen and this is basically
a talk on fixing those bugs and then
falling into the next bug in fixing that
and going into the next bug and fixing
that again so let's let's see if we can
dive in this site is unavailable this
was one of the first items that we had
when when when the site didn't really
want to work so the site wasn't
available and we were saying that our
primary location where we were hosted in
Azure was unavailable so no servers were
responding there and we could not it's
the public weapon points of the server
in that data center so that's not ideal
and kind of scary as well but luckily we
are smart and we have two locations so
we had a secondary location as well
which we load balance to using traffic
manager a measure which means if the
primary goes down the secondary you
should take over based on dns change it
will basically fail for 30 seconds or
something like that where it will fail
over to the secondary location at least
we thought because we saw that the
secondary location also was away so our
website was gone from the internet and
that is really really really scary if
you have a lot of users so as an end
result our website was down now we
started looking into it and starting to
investigate why or what could be
happening and we thought
things so the public endpoint of the
website was unavailable dns worked but
we just could not hit hit the website
true the load balancers in each of these
locations on azure the funny thing was
that the machines were available
individually so if he would hit the
specific tcp port where we expose every
single machine on to the public internet
we would see that the website would
respond normally and we could generate
traffic on it and everything was working
so that was really really weird the
azure load balancer didn't seem to be
flowing traffic true and we were not
seeing results for typical users that
were not using these special tcp ports
so what could have been going on well
turns out we were using a custom load
balancer probe in Azure so what you can
do is basically tell a sure where to
query every couple of seconds to see if
the website is up and running and if
that endpoint returns a 200 okay status
codes then I sure will think everything
is fine and the load balancer will keep
the machine in the whole balancer
rotation the fun thing is if you start
returning a different error codes or
different status codes like a error 500
or something I sure will take your
machine out of the load balancer which
is interesting and awesome because you
actually want to do that you don't want
to keep our faulty machines in the load
balancer rotation the fun thing happens
when all the machines start responding
with an error code 500s and all the
machines disappear from the load
balancer rotation which basically means
that your status check is taking all the
machines out of rotation and making sure
that the website has just gone if you
hit it through the load balancer so that
was what we were seeing we looked into
that and we removed the probe because we
saw that all the different machines were
responding individually so we removed
this check to basically have a quick fix
in getting a website up and running and
the downtime didn't really last long but
we yeah we found out that there was some
service not responding properly and some
service was basically causing this error
500 to happen and taking the machines
out of the load
so this is the implementation of that
status service that we were hitting and
we saw that one of these services would
be down and causing this error so I'd
rate was our gallery itself which was
gone either it was our sequel database
could have been our storage system could
have been our search service and could
have been our metric service so one of
these services was down but we decided
to not look into it at the moment the
website was done and gone from the load
balancer we decided to fix the load
balancer probe and just make sure that
we were up and running again and we were
happy because it got fixed the website
got running again and we were happy our
users were happy and the fire that was
raging on Twitter at that time I was
cooling down again which is an awesome
thing now how did we find the issue why
did we come up with oh we have to maybe
check the load balancer probe there well
there's actually a great website called
heights high scalability calm and I like
reading that one because it has
technical articles on the big websites
out there but it also has fun parallels
with other industries and other
disciplines like for example FBI
profilers and there's actually a quote
from a book that the guys reviewed and
it says that if you have a crime scene
going on you have to put yourself into
the shoes of of the victim you have to
know what is going on you have to learn
and and feel what is going on with the
server or with with the service and know
what it was like and try to build on
that experience to find whatever was the
issue so we did that and thankfully we
knew our service so we work on the
service so we knew what's happening
there but even then each one of our team
members had a lot of experience with
Azure so we immediately thought that if
the website was available on the single
instances but not truly load balancer
the only thing that could be wrong is
the load balancer so you have to know
your system you also have to have some
logging and telemetry of course but it's
the combination of all those things that
makes you appreciate your system know
your system and fix whatever when
is wrong with your system so with that
thanks for being here and thanks for
joining this presentation as i said i'm
from belgium I just took a nap before
the session which is awesome but I don't
think it's a good idea to have done that
because I know I don't know what time it
is right now if you look on the map I'm
from Antwerp which is a nice city in the
north of Belgium we have awesome
chocolate we have lots of years and it's
a great place to live I work in the
microsoft and the nougat team so I'm a
developer on the server site there and
as I said in the introduction this talk
is going to be mostly about how we fixed
the nougat websites so let's give you a
bit of history and context on our
application so you also have the
knowledge of how we how our application
was built and structured at the time we
had all these issues happening so the
nougat websites if you don't know a
nigga to doesn't know when you get okay
good we serve dependencies for net
developers worldwide and pretty much
every developer is using us continuously
during their day-to-day job installing
packages updating their dependencies and
so on so we get a lot of requests and at
the time all these things happens we
served around eight million to 10
million requests per day on our computer
notes alone because we also have a lot
of things that are just served by
storage and CDN so we have quite quite a
number of users on computes during the
day nougat was built for 25 years ago
and at that time it was really hip to
expose your data as as an odata service
and o data is basically an HTTP protocol
that allows you to expose agreeable
interface over HTTP and as Microsoft's
released Oh data they started to do a
lot of samples and all those samples
were basically just exposing the raw
sequel database over HTTP and nougat of
course did that as well because it was a
nice sample and a nice showcase of the
technology problem with that was new get
started to grow and a lot of things have
to happen over the years to make it make
it perform because pretty much every
developer was just hitting our database
and running queries on our database so
that's not the best technology to work
with
it's a monolithic sites back then we had
our web application so if you go to
negative dork I want to publish a
package or search in your browser that's
the exact same application as the
application that is serving the 0 data
queries that visual studio is using to
to install the packages in Visual Studio
some improvements have been made so the
sequel database is no longer it for
every single query that you are making
to this or data endpoint but yeah we had
some rough times transitioning from that
period of time to where we are right now
and that's this session so the
architecture back then was we had some
front-end servers into regions to Azure
data center regions and that front end
web site is hosting the MVC application
as well as the odata service which is a
WCF WCF service those front end servers
are backed by a sequel database and a
database contains all the package
metadata so if you search for a specific
package the package I did a package
description tags and things like that
would come from this database the
database is also used to track it down
load counts so whenever you do it
download when you get package nougats
woods rights to the database saying okay
just a package has been downloaded one
time it was this specific client version
was this user agents this was the IP
address in the geo location of the user
just have some metrics on who is
downloading everything so that means
that if you would do a download you
would query the database to find that
metadata and serve that metadata but you
would also push an update into the
database because you were increasing the
download counter there now we did some
improvements because searching on the
database for every single query on
nougats was not that performance so we
built a search service which is a
leucine based search service that
contains all the metadata of the
packages and if you use visual studio to
search for a package you're actually
hitting that search service directly
with visual studio 2015 indirectly with
visuals to you 2013 because we basically
linked the dub CF odata service to this
search service so that whenever you
you would hit the search service on the
websites you will actually hit this
loosening next to take the load off of
the database now of course the search
service has to get its data as well and
that data is on azure storage just raw
blob storage we have the leucine index
running there and that thing just uses
that to search to serve the search
results and serve things like download
cans and so on now of course the data
has to flow between those things and we
have a number of background jobs as well
and these background jobs basically read
the database build an index whenever you
publish a package for example the
package gets attitude is index and just
flows through that entire system to this
search service so that's roughly the
architecture back then last year around
March to May time frame so did we solve
the crime did we actually solve this
issue we had with the load balancer in
the beginning of this session well we
didn't really do it we removed the load
balancer probe to get the servers back
into rotation but we didn't actually fix
or find the root cause of what was
happening here so flash back to our
issue we had the load balancer was
taking each server out of rotation
because our status service was
responding with a error 500 status codes
and that status codes was there because
of one of these services now we could
rule out one of them because the status
service was also checking itself and if
it could check itself that means that
the status service is up so it's a
stupid redundant check but we we
basically found four possible root
causes of this issue where the status
service was returning an error 500 so it
could have been sequel could have been a
database could have been storage it
could have been our search service or
could have been our metric service which
basically takes all the download counts
and does this increments by one whenever
you download a package on you get so one
of these services was causing this issue
so we looked at all our logs for example
the sequel database locks and we found
the database had just been running
perfectly fine we think because we
didn't have all the logs back then so
that's that's another
nice thing to discover when searching
for the issue we found we have not no
detailed logs on whatever was happening
on the database back then so we fix that
along the way but we were fairly
confident that the sequel database had
not been acting up and had not been
causing trouble especially because all
the different individual machines were
still reachable and we could run queries
on them so that basically meant that the
database was fine and just returning
results as we were expecting storage
since table as well we had lots of log
in there as well and we saw a hundred
percent up time no latency issues no
dropped requests or whatsoever so unless
our logs were lying to us Storch was not
the issue there we looked at search and
search had just been running for a
couple of months just fine no issues
with it and in the logs there we also
didn't really discover anything
significant that would explain the
status service to be down we looked into
our metric service which is like the
steam the simplest REST API you could
have it's just posting data to an HTTP
endpoints and let the HDP endpoint would
increment the download counts in the
database so nothing really special going
on there and we didn't really find
anything obvious in any of the locks of
these different for services that we
were calling so we started looking
around at the crime scene itself we
started pulling is locks we started
looking into Event Viewer we started
looking into a lot of things we even
started profiling on our production
servers because we were stunned that
there was no information in any of the
locks and we just wanted to find out
what could have been the cost and could
have cost his down time so we started
profiling the website and we did that as
I said on production which is something
you probably don't want to do but it was
our last resorts and we had to dive into
that so we did some profiling there and
we saw a couple of things first of all
let me start zoom its first of all we
saw that there was a little bit of
blocking garbage collection like two
percent of the time we had blocking
garbage collection it's not too bad it's
not good either but
nothing special going on there we saw a
couple of methods there we saw that the
consuming to there we saw that the
hijack method here I was hit nine
percent of the time so it turned out
that our are hijacked that we do when
when you hit the OData endpoint and we
redirect you basically to the search
service nine percent of the time was
spent in this hijack and we started
taking in there and we saw that hijack
was called quite a lot of times which is
as expected because we we redirect to
the search service because we wanted to
offload the database and we didn't
really find out anything interesting
here yeah there's this garbage
collection that's happening but it's
it's not out of the ordinary and it's
not something really really special so
our gallery was definitely not the cost
there was some room for improvement we
actually went into this profiling
reports and we found a couple of things
that could be optimized but nothing that
would explain the down time that we saw
so yeah that's nice but we were still
left with nothing because we didn't know
why our website was down the previous
day so it could have been searched
because we saw all of this traffic going
to this hijack methods which would
redirect to the search service but again
the logs didn't show anything and this
gallery profiling on the web servers in
front of its didn't really show anything
special either so yeah we continue to
chain and start profiling our search
service as well and our search service
proved a little more interesting if you
look at these things lock contention for
example we had sixteen percent of the
time our search service was waiting for
Lux to be released that's something you
don't really want to have in your
application because it means your users
are waiting a lot of the time for Lux to
be released if this is in a request path
for example another interesting thing
was that we had fifty-eight percent of
the time blocking garbage collection
which means that our server was doing
more work in garbage collection than in
serving requests and responses back to
our users that's not something you want
to have in
so let's dive in there and select this
lock contention and see what's happening
so an interesting thing we see here in
this chart is that the lock contention
basically happens when we are not
garbage collection are collecting so
this this brown charts is the garbage
collection happening and it happens here
as well so either we are garbage
collecting or either we were waiting for
something to happen or waiting for luck
to be released so this is not a healthy
application and we got really worried
about this because the search service
looked like the culprits and the thing
that was actually causing this issue
that we had previously so we went in
there and looking at the top methods
that were being called we saw this
searcher manager gets and I can't sue to
that corner of the screen which is
really really funny but I'll do it like
this so we saw that these get methods
was being called a fifty nine percent of
the time and a release method was called
thirty-eight percent of the time
interesting because those two methods
were basically whatever our search
service is doing so whenever we do an
index reload whenever we load the search
index again from storage these two
methods would be called the gates would
be called two loads the index and
release would be whenever the index has
a newer version and we release the old
version back to back to the system so
these two methods were actually causing
either garbage collection or waiting for
locks to be released so we started
looking into the codes and we this was a
deployment that happens a long long time
ago and we didn't really have the codes
anymore so we had to download a
decompiler to look at our actual
production code that was deployed at a
time and we started digging into that
and we found a couple of interesting
things the searcher manager that we use
which calls these get methods to load
the index and release the index
afterwards was checking lusine index
freshness which is perfectly fine you
want to check if your index has been
updated and if it has been updated you
want to reload it so you can search the
new results that are in that index
the the big issue we had there is that
in the stark reopen method we had when
when you would start reopening the index
from storage and actually have a link to
get up from that time we had this
happening so we had a lock there and
while the reopen was happening we would
just wait until the index was reloaded
and then continue and free this luck and
make it available to the search service
so this thing was actually causing our
downtime because loading our index would
take around two minutes at that time
because it was quite fragmented and
stuff like that so we would just wait
for two minutes in a request pots for
this luck to be freed and I was that was
horrible so we fixed that we took this
guy out of the out of the request part
so we we created a background job that
would reload the index and swap the
index that was being used to serve
searches but we learned the hard way
that this is not something you want to
do in your request box now was this in
the request but for every user that was
hitting the search service know where we
were doing this where we were checking
if the index was still fresh was in our
monitoring so our monitoring would call
a specific end point on the search
service the search service would then
check if the index was to be reloaded it
will start to reload wait for two
minutes and then free the lock problem
is that same luck was checked in the
request pot so if our monitoring would
take long request because the lock was
not being freed and people just have to
wait for another luck to be freed again
so we fixed that we took that out of out
of the request part and move it into a
background job so that the lock would
never never hurts production traffic or
never would hurts any HTTP requests
coming in so awesome now back to the
profiler we also saw this garbage
collection issue we found out a lot of
this garbage collection was costs by
loading the leucine index it would just
reserve a big chunk of memory in our net
run time and then free it again but we
also saw that Newton softs Jason Jason
botnets was actually causing a lot of
this memory traffic
yeah yeah we had a lot of objects that
were really causing an issue here so
what would happen if we did an index
reloads was that we would also reload
the number of downloads packages hat
because the package download can
actually influences the search results
so we have to know how many times your
package has been downloaded now if we
would increment this number in the index
that would mean we had we would have a
really high index churn because you
would actually have to delete and
reinsert the documents into this search
index and you don't want to do that for
every download or every 10 downloads or
even every hundred downloads so what we
did was instead have a background
process created a big twelve megabyte
JSON blob which contains package ID
package version and the download counts
and whenever the index would be reloaded
we would also reload that specific blob
and loaded into memory and make sure
that we could use it problem was as we
saw later in a memory trace that's this
loading of these download counts was
actually causing a lot of memory traffic
it was causing the garbage collector to
have a lot of work garbage collector had
to do a lot of work and why was that
well turns out we were loading all this
data into two dictionaries so we had a
dictionary that had a key of the package
ID and value of the list of versions and
their download counts but we would
whenever we loaded stats that JSON blob
we would recreate that entire dictionary
in memory and just to start the old one
basically give it back to the dotnet
runtime so the net would have to reclaim
a couple of million objects in memory
every time we did that reloads and that
was causing this garbage collection
traffic memory traffic in the garbage
collection issue so what we did was we
were we started reusing that dictionary
so instead of recreating the dictionary
whenever you would DC utilize the JSON
data we would just reuse the dictionary
and update all the existing values in
the existing dictionary because the
package IDs typically don't really
change the only data that changes
the download guns and even that doesn't
change that often for most of our
packages so we basically decided to
stream the entire JSON blob and just
update the download counts in the
existing dictionary if the package was
already in there if not we would just
add it and not cause a lot of a lot of
memory traffic another interesting thing
we saw was that there were a lot of
strings loaded in memory as well reason
for that was that we were also using J
object up bars which means you load the
entire string the entire block into a
string then this utilize the entire
thing into a j object and then use that
to populate a dictionary well turns out
that's also not a good good thing to do
if you have this 12 megabytes blob
containing json data better thing is to
just stream over the entire file and
just loads basically package per package
package version by package version and
reuse these objects and not instantiates
objects all the time I in there so we
fixed that and we were happy because we
found a really nasty bug in our search
service and we thought okay we're good
to go for a for another year well turns
out that was the case but not for a year
but for a couple of weeks in those weeks
we did a couple of things because we
discovered we didn't have all the
logging we really wanted to have so we
added additional monitoring additional
tracing we started looking into a pin
sites as well I don't know if you're
familiar with that product it's
basically sort of like a mini continuous
profiler on your application that
collects request data couples it to to
stack traces couples into exceptions and
so on so you can search in it and
basically troubleshoot your application
more easily so we started investing in
debts they'll have changes there and
we're confident that whenever something
would happen we would have the data to
investigate it and we did we would no
longer have to recite to profiling on
the on the production system so as I
said a couple of weeks later we started
seeing another issue on our services we
were starting to see internal server
error so whenever people would do packet
restores search was working fine if you
had existing packages and would just
want to check for updates
example that would work perfectly fine
the only thing that was not working was
if you open up a solution and you don't
have the packages on your system and you
would want to download all those
packages to your system that would fail
with an error 500 as well what we were
seeing in app insights was that the
response time would go up the number of
requests would actually be stable or go
down even a little the exception rate
would not really go up it was just the
response time that was going up so yeah
that's an issue so what we did was we
looked into the traces we had in epping
sites we started looking into the events
that were being logged in Windows Event
log and we saw that is was crashing
during these time frames so whenever
this response time would go up we would
see at least one of our servers having
an is crash restarting the application
pool and returning error 500 for a
couple of seconds so that was what we
were seeing there but what was the cause
of that so we started looking into all
of our logs and we didn't really find
anything significant except for the fact
that we saw that is was crashing on us
we saw that on our starch accounts we
saw the crash terms of is being written
so I saw quite a number of of crash
dumps on there so that confirmed that is
was crashing but didn't really tell us
what the cause of that crash was we also
started looking into the HTTP dot sis
locks on our web server HTTP dots this
is basically the HTTP subsystem you have
in your windows machines so whenever you
use is or even a self hosted web
application you're actually using HTTP
dot sis as the web server and is is just
a fancy expensive nice wrapper around
this HTTP Nazis now is has logs but HTTP
dot sis itself is kernel module handling
HTTP traffic also has logging and we
looked into that one and we saw a
tremendous amount of connection
abandoned by request queue a type
errors so if you look up that error you
will see that it actually means that the
application pool just stopped working
and expectedly which typically denotes a
crash has happens so all those things
combined told us well the the root cause
of this thing is going to be is crashing
out on us and we wanted to start looking
into those crash dumps so we loaded
those crash dumps on to our machines and
started looking into them now hardcore
debuggers will typically use something
like windy bug or something but if you
have a crash dump you can actually open
that crash dump in visual studio as well
and that's a nice thing because it gives
you a number of things in terms of
information I don't know if you're
familiar with crash dumps you basically
have two different types of crash dumps
one of them is a full dump that contains
stack traces exceptions and whatever is
happening at the time the crash dump is
created the other one is a mini dump
which just contains the stack traces but
not whatever is in memory at a time
Asher for us was creating mini dumps so
it meant that we had stack traces we
knew which threads were running which
methods were running and things like
that but we didn't have the actual
values of the variables in memory still
quite interesting because when we open
this crash dump we saw that the treads
tried to read from or write to a virtual
address for which it does not have the
appropriate access that's a nice error I
don't know if you've ever seen this one
I hadn't seen this one before and turns
out it's quite an interesting one to
debug as well now if you open a crash
dump in Visual Studio you can actually
tell visual studio to act like it is
debugging connected to the process that
is running this thing at the time this
crash dump was happening so let's debug
with managed only and if we do that we
will typically wait for symbols to be
loaded and i found out that australia
apparently is a far as far away from the
microsoft simple server so let's see how
fast this goes this is ask this is
actually not that I did this in the room
before and that was that was terrible so
this is quite ok
won't be long should not be long the
thing we will see is that the exception
that happens at the time the Crashdown
postmates will just open into our face
and show and show the cause of the error
on the server so let's wait for symbols
to be loaded there we are we see the
exception this time we see the exception
with both the message as well as the
physical address where this happen
happens the module in which this
happened happened and so on so let's
break and if we break we can actually
close the traits window we can actually
look at the disassembly of this of this
code so let's disable just my codes and
let's look at the disassembly and
apparently the issue was this move
statements in memory awesome but here
what was going on in the application so
an interesting you can do is if you have
a crash dump and you have the symbols of
the application and the Assemblies of
the application like it was built on
your CI system for example you can
actually load those symbols and look at
the source codes of of his entire thing
now the problem was that at this point
in time we discovered our build server
was not storing symbols which meant that
this deployment we had crashing on us we
did of course have the assemblies
because the assemblies were serving the
application on the server but we did not
have the debugger symbols on our build
server which means that the only thing
we were left with was this disassembly
and that didn't really tell us a lot so
luckily for us we still had the
Assemblies of course and JetBrains has
an awesome decompiler and you can load
the assemblies in there and look at the
code so for example I loaded the
assembly which crashed on us and we can
look at whatever is in there we can look
at all the different MVC areas we can
look at the controllers and search to
debts but a more interesting thing is
that the peak also comes with a simple
server so if I enable
this little button there it will host a
simple server for us and actually mimic
the Microsoft symbol servers or whatever
symbol server you would have decompile
everything on the fly and serve that
data to visual studio essentially giving
us the source code of the application
that was running at the time so let's do
that let's dive back into visual studio
and this time set the cymbal pads
includes our dot peak as as a decompiler
also that one was disabled and if we do
this we can debug again and this time we
will get much much more information
because visual studio will reach out to
the peak which will be compile the
actual assemblies that we had back then
and will show us the source codes of the
application as it was when this error
happens so here we are we see the
exception and we actually see the line
of code where this happens now be warned
this is not the actual codes that you
wrote and that God compelled into the
application this is of course d
compelled codes and it's compiler
optimized and it's not exactly what you
wrote but it should give you a good idea
of what was happening in the application
and apparently our post download
statistics methods which would track a
downloads whenever you would install a
package was failing us now as I said
before this is the metric service and
that metric services really really
stupid it's the simplest service you've
ever seen so we were kind of surprised
that this method was crashing on us even
more you see that we do a task dot run
and just fire-and-forget basically to
increment the download counter so you
were not expecting that at ascot run
would actually cause ious to crash
because we would simply fire and forget
and even if the download count wasn't
really tracked we don't really care that
much about those download counts at
those moments in time so we would expect
this to just work and never never never
never crash our is in production but
turns out it did crash is in production
so we started searching the internet
being didn't find a lot of things
because
yeah being is an awesome search engine
for some things but for technical topics
it's not always the best turns out
google was also not really helpful
because we didn't find a lot of
information in there as well we searched
on the error message where we had this
the actual error message that gave a lot
of results but nothing that related to
our codes nothing that related to the
center the scenario that we were seeing
until somewhere on page 50 of some
search engine we found another person
with who had an unobserved task
exception when doing this fire and
forgets type of operation in his
application so we looked at that link
and we started reading and basically
what the guy was saying was that if you
do a task but run and just fire and
forget the operation in there and that
operation crashes the exception will
actually bubble up and if no one catches
the exception your process will die and
that was what was happening we did not
really catch that exception because we
were just expecting it to always work
and even if it would not work we did not
expect that the exception would bubble
up yet it bubbled up and basically
crashed is whenever updating the
download count went wrong so turns out
the fix is quite easy all you have to do
is tell the task scheduler what's to do
whenever you have an unobserved asked
exception and tell it's that the
exception was handled so we added this
portion of codes handle the unobserved
asked exception we log it we track the
metrics in it and then set it observed
and IOUs was perfectly fine with that
and with no langer would no longer crash
on us awesome case closed another issue
down and we thought were good for
another year there was that was a
colleague of mine by the way who had a
funny witty remark on that anyway so we
thought we were good for another year
but apparently a couple of weeks later
we saw another issue arise on our
services we saw high response times on
the web server in general so basically
any requests that would happen which
would have really high response times
so what we were seeing was high response
times lots of CPU usage on the servers
we saw that Asher was auto scaling so
back then we had a sure auto scaling
enabled based on this on on the cpu
percentage which would mean if the cpu
would max out we would just add more
instances hoping that this would be able
to serve or handle all the traffic of
course it didn't it would just spin up
new new service and also makes their CPU
out so we were seeing that and when
profiling it using App insights we did
not really she see the root cause of
what was causing these high response
types so we basically started looking
into our locks we started looking into F
insights we started looking into all the
logging we had we started doing Google
searches on whatever could be causing
unexpected rises in response times and
what we found was what we thought the
root cause of this issue and it turns
out it actually was a root cause of this
issue turns out that if you are using is
to serve your traffic and you have lots
of requests coming in is will handle
them just fine ious does have an issue
which is that if you have burst requests
coming in which means you have a low
usage of the service at one point in
time and then all of a sudden the hordes
would come in and start hammering your
server ious does not really like having
that reason for that is is has this
concept of worker threads which do which
end all the requests that are coming in
but also io treads which do things like
communicate with the database or
communicate with the storage system and
stuff like that whenever is starts your
application pool it will fire up a
number of worker threads to handle HTTP
traffic coming in and by default it will
start I think to i/o trips per CPU in
your server to handle things like
database communication and Men storage
ins and things like that whenever
traffic comes in and traffic increases
ious will dynamically detect that it
needs more threats to handle all the
traffic and it will for example scale
210 I otras to do data based
communication and stuff like that the
only issue is that is has to learn about
what is happening and it has to see that
the traffic is slightly increasing and
slightly going up but if all of a sudden
you get this burst of traffic ious will
simply add one treads or two traits but
not the 10th rates or the hundred
threats you would actually need at that
point in time so I asst would eventually
scale up to the point that you wanted to
scale up to but not immediately and that
was actually causing these high response
times that we had there and turns out
the fix was actually quite easy we would
just have to change the default settings
in is to spin up more work effects in
your backgrounds so that even if we
would have burst traffic bursts coming
in in the requests ious would have
enough threats to handle a database and
storage and stuff like that so the
results we had up inside and you would
see that the application with spike in
requests times we turn down this setting
and immediately you see the drop to
something that's almost a flat line in
response times and you can really see
where we did a change and where we
optimize this is minimum I oh dress nice
side effect was that our server response
time went down in general as well why
well we had auto scaling we had a number
of servers so not all servers added the
exact same amount of i/o traits so you
would always have a couple of servers
that would be slower or responding a
little bit slower but by changing this
setting all the servers were on the same
page whenever they start at the
application pool and we immediately
basically saw that response times would
drop to a constant level on all machines
at all time awesome now Asher is quite
nice if you are hosting on web roles you
can actually RDP into the machine and
make changes to is to for example change
this mean io traits setting problem is
if you do that the next time you do a
deployment or the next time Asher
decides to kill your machine and reboot
it's on a different host those changes
will be gone so you have to somehow make
it permanent and the way to do that in
Azure is by creating a startup script so
what we did was add a few lines of
command-line codes to our application so
that on
every deployment it would call into the
command line is configuration through
and set these worker threads to to a
reasonable number instead of the default
awesome case closed we're good for
another year well turns out we were not
good for another year it turned out we
were good for another four hours and
after that's a little more but still it
turned out that we were seeing package
restore timeouts so we optimize the
number of things we were seeing that
response times would drop to a pretty
much constant level yet we were seeing
that package free stores were timing out
and again only package restore so
bringing down the packages whenever you
don't have them on disc everything else
would be just working fine so here's
what we were seeing on the v2 based feet
so basically Visual Studio 2003 2013 and
the first versions of 2015 they would
hit our dub CF odata service that one
would hit the get method where you would
get by ID inversion to get the metadata
to see if you can actually install the
package and get the download URL to the
package and what we were seeing is
packaged restores on that specific call
in the service we were seeing this every
7 to 15 hours so not continuously every
7 to 15 hours and it would last for 10
to 15 minutes and afterwards everything
would work just fine again so very
temporal very yep not really easy to
debug and not really easy to trap what
was going on nice screenshot from app
insights if you were doing package
resource we actually had requests take
up 21 minutes to serve a package which
is insane for something that just
fetches meta data from the database by
ID in version so primary key look up and
would give you a URL to where you would
actually download the package so one
minute for that operation is just crazy
we didn't really see an easy way to
reproduce this issue it was happening
only on production we did our best to
reproduce this issue on our development
in our staging environments we weren't
able to do it why because we weren't
really sure what was happening
it would only happen for a couple of
minutes every couple of hours so it was
really hard to reproduce and we already
beat into our production systems and we
basically had someone on the team look
at this at the task limiter for the
entire day looking at when the cpu would
spike to trap the issue and try and get
some information about what was
happening in the process at the time and
what we were seeing was high cpu load
whenever this would happen but we had no
way to continuously profile this because
profiling generates a lot of data so it
was really difficult to trap whatever
was going on there so we looked at what
we did recently to fix things and we
thought okay so we change this min I
otras value to make sure that's whenever
requests would come in ious would have
enough dreads available to handle things
like database and storage communication
and the slow code path is fine packages
by ID so we started looking into what
this method actually does finding
packages by ID and what it does is it
reaches out to the search service which
is a rest based service over HTTP to
basically fetch the metadata of that
specific package and serve it back to
the user so what could mean I owe treads
and Max I'll traits in is have in common
with HTTP calls being made in the
application to a different service we
did a lot of searching again in a lot of
research again and we found an
interesting article that basically
benchmarks HTTP clients or the dotnet
HTTP client that you can use to call
into any service out there and we found
out that if you use HTTP clients and
it's a sink model of programming that's
HTTP client doesn't really limit the
number of concurrent requests it makes
on your server so what we were seeing
was if people would start doing package
tree stars there's typically a lot of
requests coming in because every package
in your solution would be checked if
it's still on yigit's and where it could
be downloaded and typically whenever the
u.s. wakes up for example you would see
a lot of traffic coming into the service
and evidently this service would make a
lot of calls to our search service in
the back so what we were seeing we
thought was that dhcp client would just
hammer our search service our search
service would be perfectly happy with
that and surf traffic but still somehow
it would block on making those requests
at a certain point in time reason is
that HTTP client does not limit the
number of concurrent requests it makes
to an upstream service combine that with
the fact that the number of TCP ports
you have on a machine is limited and
combine that with the fact that you are
running on a web server which means that
you have TCP ports in use to serve your
clients and you have TCP ports in used
to basically fetch data from this
upstream service makes that you are
actually using a lot of TCP ports on the
system you would think those requests
are quite fast and they would free up
again after the TCP port has been used
by the operating system or by little net
rotten time well turns out that's every
TCP ports that you used to make an
upstream connection on a downstream
connection is being held for about four
minutes by the operating system to make
sure that whenever a request still
returns or there's traffic returning to
your service to make sure that doesn't
confuse the run time because the request
was already handled and some router in
between is resending packages that have
already been received so combine all
that information and if you do a quick
calculation you will find that you can
pretty fast you can starve the number of
pcp ports you have on a server pretty
fast actually so we looked into widest
what's happening and what we could do to
limit the number of concurrent requests
this HTTP client could make to upstream
services and we found that if you don't
set the service point manager dots
default connection limits to a
reasonable value you would just starve
your tcp ports on your system so all we
had to do basically to fix this issue
was set a value to that thing and the
net would simply respect whatever value
we sit there and have more than enough
TCP ports and reuse them
and bourke fine so that's what we did we
changed a few things in our coats and we
set the maximum number of concurrent
requests that could be mates now when
reading all these blogs and articles and
internal documentation we also found out
there's a couple of other optimizations
that dotnet does for you that you
typically don't want to have if you have
a server-side application that makes
upstream calls to a really tiny service
that returns yeah small small amounts of
data so another thing we disabled was
neg Ling an eggling is an optimization
in the tcp/ip protocol that basically
combines several smaller requests into
one tcp packet so that you would not be
flooding the network with small packets
and instead would just put a lower
number of packets on on the network so
we disabled that because it didn't make
sense to do that because all of our
requests would typically be smaller than
then a number of TCP packets so we
disabled it to make sure that yes the
network would be flooded a little more
but traffic would flow faster back and
forth between our search service and the
gallery so we did that and we also
disabled expect 100 continue on this
thing to basically make sure that the
TCP connections or the connection would
be freed up faster on the server there
some charts after optimizing this so
remember that we had one minute of
response time for a package restore well
when we enable these changes you could
really see a drop in a response time
back to a normal level you could see the
number of requests actually go up a
little bit so the server was able to
handle more traffic by making this
change and response times would drop to
their normal levels around three seconds
for some or at least the highest number
of the highest response time in there
still not really really good but much
much better than one minutes of course
awesome case closed good for another
year
yeah so we thought that again and we
started thinking like okay good for
another few weeks probably and yet again
by fixing a bug we actually uncovered
another bug in the system so we were
seeing memory pressure what we were
seeing was on all the different servers
we were running we would see one hundred
percent cpu usage being developers and
being being developers using a cloud
service we thought okay we are
Microsoft's we actually pay these
services internally at internal rates so
let's go for bigger VMs so we increase
the size of the vm and what we saw was
that the bigger vm was also spending one
hundred percent of its time one hundred
percent of CPU of memory awesome maybe
we don't have enough memory in those
bigger machines so you end up another
level of machines so we basically did
our memory on each instance but times
for and what we saw was again one or a
percent of sleep of memory usage in
there so if you do that a couple of
times and you keep on doing that that's
just stupid so we thought okay we did it
twice and we saw the same result again
so probably the application is
misbehaving here so what is eating that
memory so to discover that you actually
have to do some memory profiling and you
have two places where you can do this on
production which we did not really want
to do again or try and reproduce it on a
test server and profile on that test
server so we decided on the letter and
downloads all our power is locks and
replay traffic onto a server that are
onto a spare server and attach a
profiler on that machine so we did that
and we downloaded all of our is locks
and our is locks where do we have it you
can actually load those into a nice tool
called called jmeter it's an open source
tool which is written in Java and it's
very high DPI friendly as you can see
but it's actually an awesome tool
because it allows you to read
web server log specify a number of
threats to cycle through that entire
lock and replay traffic onto a different
server so what we were able to do is
just take actual HTTP locks from our
production machines and replay it using
jmeter you can actually have multiple
agents putting traffic onto your server
so we were actually able to reproduce
the exact amount of traffic in the exact
traffic flow that we had on production
on to our onto our development server so
we did that and attached a memory
profiler now before we dive into the
actual profiling of memory I just want
to give you a quick introduction into
memory management in Nets who have you
dare say they are really familiar with
how the net does its memory management's
like I guess no one really everyone
knows a little bit and I also know a
little bit so let's let's yeah put out a
baseline there so when you allocate
memory in.net what is happening is that
you are not doing something like C++
would do ask the operating system for a
portion of memory in in its address
space and use that what you do is
actually you start the application and
the net will reserve massive chunk of
memory for your application and not net
will simply allocate your variables in
that block it gets from the operating
system and we're not using that that
memory anymore the net will free that
memory for you that thing is called the
managed heap so whenever you allocate a
new object whenever you create a string
or create a new object or a person or an
address or whatever objects that you are
using you're actually allocating that
objects in this manage to keep this dot
net memory memory space now it's really
fast to do that because the net doesn't
have to go to the US to ask for
additional memory it just has a block
and it's can freely allocate itself of
course that metal so you use some
unmanaged memory for for doing things
like you're hosting its own memory
hosting the garbage collector and things
like that but generally you have this
managed heap and all your stuff goes in
there of course all your stuff has to go
out of there as well if it's no longer
being used in C++ for example if you ask
memory from the operating system and you
no longer need it you would give it back
to the operating system so it knows it
can distribute it to another application
or maybe again to your air same
application dotnet does not do that
stuff net has its own memory space that
it's freely distribute san delegates for
your objects in the.net space and it has
a garbage collector instance so what
will happen is if your variable is no
longer being used that net will simply
scan the entire managed heap look at the
objects that are in there and if it
finds something that is no longer being
used it will kill it and update it in
its own index basically to to be able to
allocate that memory for new variables
in your application again now this
managed heap is divided into multiple
generations you may have seen Jen 0 gen
one gen 2 and stuff like that reason for
that is a lot of objects are really
short lived typically you have a methods
you allocate a variable and at the end
of the methods the variable is no longer
being used so there's no point in
keeping that variable around or keeping
the memory space around and Jen 0 is
basically we're all objects are
allocated whenever you allocate
something in the dotnet memory typically
garbage collection is really fast
because if the net wants to free up some
memory it will simply scan that Chang 0
space and remove everything that's no
longer being used if something is still
in use whenever that garbage collection
cycle happens dotnet moves the objects
into gen 1 and same thing happens there
it's the net will scan jan-14 objects
that are no longer being used and if
that's the case it will remove the
objects if it's still being used it will
go to Jenn to where it will just keep on
living or eventually be cleaned up again
if it's no longer being used now why
these generations and why this moving of
objects between those generations well
the reason for that is typically if you
have short-lived objects they don't have
a lot of pointers to them they are being
used in one methods and there's only one
pointer so it's really easy to free it
up by the garbage collector in the
garbage collect
can simply scan the entire thing and
free up the the memory there now of
course if you have an object that lives
longer in this manner cheap then there
may be more pointers to this specific
object and the garbage collector may
have to do much more work to find out
who is actually using that specific
objects and to see if it can actually
free up the memory or not same thing
with gin too if you if you end up in
gentoo you actually have an object that
lists quite long and the garbage
collector has more work to check if that
object is still being used or not so
what that dotnet does is it keeps these
generations long-lived objects go into
Jen to medium leave the objects go into
gen 1 and short-lived objects just
remain in jan 08 get cleaned up in there
that's one thing to basically partition
the memory like that the the second
reason for that is the garbage collector
has a lot of work to basically check all
these pointers and it checks gen 0 more
often than jen too because dotnet is
smart enough to know that's in gentoo if
your object ends up in there it's longer
lift and it will not have to check that
every couple of milliseconds or every
couple of seconds it's usually good to
check it less often to see if those
objects can be freed or not so that's
what happens there so in a nice diagram
you basically allocate an object in jan
08 still being used it moves gen 1 if
not it should it gets freed in gen 1
garbage collection cycle may happen and
the object may move to Gentoo if it's
still in use if not it gets freed and
perfectly fine if your object is in gen
2 and a garbage collection cycle happens
there in the object is still in use it
will just stay in gentoo if not the
garbage collector will free it up now
when does the garbage collected run
there's some documentation but not all
the documentation is as clear as you
would like it to be generally when does
the garbage collector run typically when
you have an out of memory situation
where you have memory pressure and your
net runtime needs additional memory to
host
application to allocate variables that's
typically when garbage collection would
run after you do a lot of allocations
the garbage collector will typically run
as well remember where we were reading
all this JSON data and making sure that
we could deserialize it into a
dictionary well that was a significant
allocation because we were allocating
millions of objects in memory typically
the garbage collector would run on such
a situation you can attach a profiler
and tell the garbage collector to run
you can have the garbage collector run
when the application moves to the
background so there's a number of
reasons why the garbage collector may
run what I'm saying may run because the
garbage collector is not guaranteed to
run so it may well be that you just
allocate memory and if there's memory
enough or none of these conditions
happens dotnet will simply not run the
garbage collector and not free up memory
because it's not considered necessary
why not well the garbage collector takes
some time it pauses your application it
actually causes a small slow down
whenever it runs so when it decides it
should not run it does not run just not
have that boss in the application so
with that knowledge let's dive into our
memory pressure we were where we were
seeing one hundred percent memory usage
on a small server and on a larger so
what we did was profile reproduce
traffic on a development server and
attach a memory profiler to its and we
took several snapshots during that
process now I'm going to open this
specific snapshot snapshot number six
reason for that is that it turns out
that the garbage collector ran here so
we see that this snapshot has two
million objects less than the previous
snapshot which means somewhere in
between the garbage collector has run
has done its thing and this snapshot
would contain the objects that were not
being freed and that's what we were
interested in because we saw memory
pressure which means something sticks in
memory and it's not being freed up so we
want to know what that is so let's dive
into that one and we can see a number of
things like the largest size of objects
that is in memory
we see a lot of strings in there see a
lot of string duplicates and things like
that but nothing out of the ordinary and
this profiler doesn't really give us a
clear indication immediately on some
common things like if antenna lyrics and
stuff like that what we can see though
is that if my zoo it works properly what
we can see is that gentoo is actually
quite full there's a lot of objects in
gentoo which means that we have a lot of
objects that are not being garbage
collected so let's dive in there and
look into what is happening in there so
gentoo awesome let's dive into that one
and see which objects are in there and
what we can see is that there's this
ninja dots activation not cashing that
cash which has its really annoying that
so it doesn't really work properly stri
it like that what we can see is one of
those objects small objects but it's the
object retaining the most memory in our
dot net memory space this thing has some
other things but yeah we're using a
dependency injection container so it's
probably fine that we are doing this an
interest a more interesting thing is
that our entity framework contexts is
around in memory for eight hundred and
twenty seven times that's not too bad
because we have a lot of traffic on the
server what is bad though is that we are
looking at gen 2 so this means that our
entity framework contexts are long-lived
and are not being collected by the
garbage collector that also is not that
much of a problem but keep in mind if
you are using entity framework that
entity framework is tracking changes to
the objects that have been loaded so
this basically means that all these
entity framework contexts are dragging
along domain objects from our
application because it still has a
change tracking enabled for all those
things so this means that we're using
quite a large number of memory in gen 2
by just having objects that are probably
no longer being used because we have a
request response cycle and you typically
don't end up engine to
during that so let's let's dive into the
entity framework context and what we
want to see is which object is holding
on to this entity framework context so
what we can do is group by similar
retention and we can actually get a nice
diagram telling us where the entity
framework context is being held in
memory and if we look at that diagram
you can't really zoom in properly let's
try again using this one if we look at
the diagram we can see that it's being
held in memory in a dictionary somewhere
and that dictionary comes from a cache
and that cash if we dive into that one
is our dependency injection container
awkwards our dependency injection
container is keeping around entity
framework context for a long period of
time and they don't get collected and
memory simply fills up interesting so at
the time we were using ninja taz our di
container and we started looking into
the source codes of ninja to see why see
if we could actually find an explanation
why these entity framework context were
being held in memory so we found out
that there was this garbage collection
cash pruner object which is actually the
object responsible for freeing up the
memory that's ninja is using and if you
look at the code of that thing it's
actually quite clever or at least the
implementation needed is quite clever
they have a weak reference in there and
indicate the reference which is a simple
object and they use this indicator
objects to see if the garbage collector
random or didn't run how does that work
well you have a weak reference and if
the garbage collector runs it will
simply disappear from memory so you know
if this object is gone the garbage
collector has run and then you can
reallocate it and wait for it to be
deallocated again and see the garbage
collector run now ninja is actually
checking nets using a timer and every i
think it was 30 seconds it's actually
checking if the garbage collector ran
and if the garbage collector rent it
would prune its internal object cache
and basically
free up the memory that it was holding
on to and free of the cached objects
that it's instantiated previously now
remember when we looked at how the
garbage collector works it's not
guaranteed to run which means the week
objects or the weak reference we have in
inject here would never be freed and
inject will simply keep the dictionary
of cached entity framework context
around forever until the garbage
collector runs damn so we basically
found out that ninja was holding on to
all of our entity framework context and
there was no really good solution we
found a couple of other people having
the same issue so we decided to get rid
of this one for our application and
switch to a different one we did that
and we found that memory usage dropped
to a normal level and we actually fix
the issue there so by changing the DI
container not to blame the DI container
but just to blame the way it's its
cycles and garbage collected memory we
found out that by simply changing that
and changing the way it behaves we could
actually drop the memory pressure again
and we are now still stable at around
four gigabytes of memory on our web
servers so yeah nice and this time we
close the case and this time we we're
actually good for a year and two months
now so awesome so in conclusion there's
a couple of things you have to do you
have to know your system when you have a
system running you have to know your
runtime you have to know all of the
different components that you are using
and I really like the reference that's I
scalability thumb gave us where they
told like profiling and debugging an
application is typically similar to the
work that an FBI profiler would do on an
actual crime scene you have to sort of
gain insight into whatever is happening
in your service get to know it get to
know the life of your service to find
out what may have happens in a specific
case and I used to not really like
debugging and finding out what was
happening with a service but by doing
this I
I found out that I learned a lot of
things about the dotnet runtime about
our own application and things like that
and it's actually a good thing to do so
if you have a service running and you
are like me not really liking debugging
it and doing things like that dude I've
in because you will build experience and
you will gain insights into finding the
source of of an issue the next time much
much much faster with debts thank you
for being here and thanks for joining me
in this sacrilege</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>