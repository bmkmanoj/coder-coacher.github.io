<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deploying Docker Containers on Windows Server 2016 - Ben Hall | Coder Coacher - Coaching Coders</title><meta content="Deploying Docker Containers on Windows Server 2016 - Ben Hall - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deploying Docker Containers on Windows Server 2016 - Ben Hall</b></h2><h5 class="post__date">2016-09-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eIOxJ2C6Fmk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone
so for many years now the Linux
community have had all the fun they've
had Dugger they've had great way than
deploying packages
they've had all of this cool technology
and it's people winning Windows Server
haven't had quite so much fun well
thankfully with Windows Server 2016
that's all changing and if it wasn't
subject to my talk will be about its
describing what Windows containers are
and how you can start implementing them
on 2003 16 so my name is Ben hall I am
the founder of Oslo up war we spend a
lot of time helping companies adopt
containers and docker and how they can
potentially improve architecture with it
we also build our own products
internally one of which is brownback
which is aiming to be like Netflix for
developers so aggregating all of the
best conference videos for example on
here and representing them in a very
nice easy way to find them the user is
cat coda cat's coda is an interactive
learning platform for software
developers we basically have a theories
of step-by-step tutorials we've combined
with interactive browser-based
environments so you can start learning
docker kubernetes mesosphere or directly
accessible from the browser all for free
so today what I want to do is start with
an introduction into what dr. what
containers are in the third place I
don't want to talk about how that
applies to the windows world and how the
changes are being introduced we then
look at an example of actually deploying
an iOS and asp.net application as a
Windows container and then how that he
looks by running it in production and
what are the tooling of what support you
need in this new wilt and then finally
we will look towards the future and
where I see the trend of containers and
especially Windows containers going so
as I said a beginning
containers have like they're all the
rage at the moment they are the simplest
way to get through the top of hacker
news they're a great way to get retweets
when you blog anything they are what
everyone
talking about but what Anthony is a
container so if we take a step back and
if we look at the physical real world
every day billions of items are sent all
the way around the world in these huge
big metal shipping containers and these
shipping containers have certain key
properties they are certain size and a
certain height and they have the door in
a certain location but most importantly
they have everything the corner they
have a hook and this hook allows them to
be connected to other containers and
that's a lot how they can build if those
strong coherent unit which can be part
of shipping containers and sent all the
way around the world but actually what's
on the inside and what's inside the
container didn't really matter that much
it could be cards it could be t-shirts
it could be most of the time we part
built products the most important is
that they were the same spec the same
standards the same approach without that
they wouldn't work we wouldn't be able
to connect them together and we wouldn't
be able to utilize and get the capacity
which we have and software containers
have taken a same mentality so if I
container thought about based on how do
you start on how do you build them how
do you win them making sure that they
can all connect and communicate with
each other but at it what's on the
inside doesn't really matter that much
they could be a java application and the
JVM they could be a golang binary or now
there could be a Windows container
winning iOS that is no important what's
important is a consistent way to start
and build and win and this is what
docker brings us so the container starts
as a image the image have got everything
with your application needs in order to
be able to win so it's got any given
time dependencies for example fourth a
sharp application it will be built and
it'll be compiled it will have all the
new get dependencies which it needs it
would have any configuration and who
also have the donut framework and core
the core sell are if it's one of the
newfangled applications and if if Java
all look the same they will have the JVM
set up with where the particular version
it needs I'll have the application jar
and they will have everything to launch
on top of underneath that divert winning
on top of
or the docker engine no the document is
offering kernels virtualization so if we
think back to traditional ways of their
driving applications and virtual
machines they will run in on top of
hypervisors you have ESX or then
managing the communication between your
gift wrapping system and your virtual
machine to your underlying host
operating system and advert the
underlying hardware but this
virtualization and the traditional way
of doing it off
also in cooler also includes a lot of
additional overhead because your
application and your guest operating
system need a third term of wait until
you have to pre allocate a certain
amount of RAM it has the auto
performance been impacting that
translation and because it's being a
gateway it can't maximize the i/o and it
can't maximize the CPU use it as it
would if it was just winning on war host
operating system itself this is where
Dockery significantly different docker
is offering kernel virtualization
basically if taken advantage of the
kernel from the host operating system
and making it available to each and
every single container it's this kernel
virtualization which is also offering
the protection and the isolation and the
security mechanisms if a strict him what
the container can do and what could the
container can see darker with managing
that layer and then below that we have
been a host operating system which would
expect so we anbu to Thantos Red Hat
Windows Server 2016 and then we have our
bare metal super easy to machine
wherever we would normally have and so
what you can think of container like
virtual machine today I have got some
significant different properties they
have their own process space so when you
go into a container and you ask it what
is actually winning
it will only be say what it actually had
been started so if you're only winning a
bash prompt it will just say the only
thing winning is bash if you're winning
a done application it will say the only
thing winning is this done application
even though the host operating system
may have thousands and hundreds of
thousands of different processes and
different containers from the containers
viewpoint it's the only thing winning on
that host itself each container have a
node their own network interface
have their own IP address and so you can
open port you can send packets and you
can operate and deal with it and manage
it I think it was any physical virtual
and physical machine they have their own
wit directories and so if you list all
the files on a directory you'll commonly
think these things like the user
directory a home directory a bin
directory everything which you would
expect from an underlying operating
system but this operating system is
completely independent from the host and
so you can completely delete everything
inside of a container and a host will be
infected and this is because it's
completely sandbox it's completely
isolated and shielded and so any
security is mitigated and protected and
the host is always secure they also have
a lot more benefits in what virtual
machines do so because we've got live
kernel virtualization and because a
container is communicating with
everything that either kernel everything
is a lot quicker and a lot more
performing than what you would expect
otherwise and so we have things like the
native CPU performance and native memory
but most importantly have a native i/o
until when we've written big big tasks
into winning databases or our core
databases or my sequel server it not
been impacted by having the
fertilization layer which we
traditionally had with virtual machines
instead we're getting the maximum
performance that the host can offer and
also we don't have any pre allocation
because we don't have this operating
system sitting in a way to manage
everything we don't have to pre allocate
that 8 gig of ram and athens we can get
much better capacity and much better
utilization of the hardware and a
service which we have available because
the only thing which is actually being
used is the memory and if the CPU cycles
that the process is winning and what the
process requires and I think we can now
start packing off over and actually
getting the maximum return on our
investment and when it container all
containers doing is literally just
launching the process as if we were
running on a host operating system and
that's it everything takes milliseconds
to launch because the whole stability
there the kernel Authority where they
are waiting the only thing which needs
to start is the process itself and so
everything becomes milliseconds and I
think she can change how your
application works and operates and
scales you can move workloads around a
lot quicker and
more seamlessly one what you could
before because everything feel much
quicker to be operating with in terms of
containers they have been around for a
while
they got created in FreeBSD because the
developer of FreeBSD needed to be able
to develop future versions of fee bsd
and so we needed to be able to win on
side by side and this is where the
concept was introduced we then had
scenarios and bones and jails which are
very similar kind of concepts and
containers and if with a slightly
different way in linux the alex c
functionality with introduced and Red
Hat and Google implemented a lot
additional to Paul in order to make that
happen and make that secure and
performant all of this was there it been
ready and waited and company they have
been taking advantage of it for a number
of years but it was actually very
difficult in order to be able to build
applications on top of these containers
you had to understand the internal
workings of Linux networking and bath
scripts and the kernel in order to be
able to take advantage and so if the
barrier to entry was significant this is
why the everyone loves dakka dakka have
different moods a barrier to entry
they've made containers accessible and
approachable for our everyone and so
it's now a very simple process in order
to take advantage of all the existing
benefits in a very nice clean structured
way with a great tool set and a great
community but more importantly if not
the tooling is the fact that they've
actually got us to agree on something
they've got us to agree on what it means
to be a container how containers have
started how containers are built what
did the images look like how are they
shared how are they progressed and if
you think back to how the metal shipping
containers if that wasn't a standard
they wouldn't work and docker starting
to increase its platform and if standard
for people to adopt and build her around
and this is why we've seen a huge
ecosystem starting to pop up because we
have DIF no standardization
doctor cells have got some great tools
though if you go a hell on tadakuni
download the tool sound to your laptop
you'll get the toolbox the toolbox it's
going you know three different tools to
solve different problems so the octopus
is docker compose this is a way to
define multiple contain
if a single one file definition and then
you can use that definition you can
commit it to your source controller and
then that can launch all of the
containers which you need how all of the
dependencies such as various and my
fecal as one single file you don't have
a document machine which it's the tank
and whale and this is for managing your
30 machines on your local laptop so
created new environment very much
quickly upgrade them spin them up just
manage your local dev infrastructure
it's also got hooks into there is a
cloud providers and so you can tell it
to launch and as your machine or
digitalocean droplet and it will set up
docker and have everything working and
configured for you one with whales plus
containers is swarm
so swarmy the way to do dokko are
multiple different hosts so manage
container is managing where they're
running ensuring that the wall if
available or I hope even if you lose a
host and managing the network and the
service discovery and how you can
actually start doing working with docker
at scale but Dockers appreciated and
respectful that it can't solve every
single problem even though it is try to
it knows how every company has a
slightly different way that they neither
network configured or they have
different requirements for their storage
drivers and a search their mentality is
that the batteries are included but
removable if you want to replace how the
networking works that's fine that the
whole ecosystem around it which supports
it so you just simply swap it out and
when you've launched the container you
just tell it to you the different way of
running the network and then it will
just dock or go cool we'll just do that
instead and so it opens up and it makes
it actually easier to adopt and migrate
you existing infrastructure over to
docker because if you have special
scenarios you can actually still start
taking advantage of it
so when we have containers or containers
of started based on a docker image this
image is basically a layered file system
and of all other changes which are
required in order to get your
application to a certain configuration
and so at the bottom this is the history
of building a docker image at the bottom
you'll see certain folder being added
we're doing things like a map get an apt
app installed for installed installing
things like mono in this example and we
progressively enhancing our base
operating system and adding more and
more functionality up to the top where
we have our own application which we was
one of the original versions could have
to even KPM and it that'd be an if
daughter not being deployed if this
layered file system which have given us
a great advantage in terms of reuse we
don't have to keep rebuilding an image
from complete scratch per application
instead we can have a base layer and a
base image which has got for example
mono installed and then all we need to
worry about is adding our own
application on top of it
and all of our model-based dependent
applications can reuse a same base image
when we need to roll out security
updates we can do that very simply
because we can just change the base
image and all of our applications can
get that benefit get that advantage and
so it makes it very simple to start
managing how things are being deployed
internally how this actually looks from
the applications point of view the
application doesn't feel like a layered
layered file system instead it will just
see a normal disk notice the other
normal directory it doesn't make any
difference it internally docker with
managed in this drive storage drivers
until we were in your layer in your
image if they've been overridden by
later versions or later layers then the
application will see that latest version
it's very similar to how it works and I
get repository will work your master in
your head will be the latest version but
you can go back and you can move around
and can catch checkpoint and branch in a
very similar way and in order to launch
a container we simply use the docker CLI
or use our API and we do docker run to
tell it to launch container in
fifteen that we ought to need access to
some ports so if they open up or six
three seven nine on our host and that's
all thought what should be communicated
with the traffic and a protest winning
inside the container and six three seven
nine and then start the image called
Venice by the food this looks up at the
docker hub which is a registry where
order of the images are available it
looks for the official one called Venice
if it's not on your local machine will
download it and then you have the Redis
process with it this is exactly the same
process which you would have if you
installed it natively if you downloaded
thor's bill to compile there and have it
winning and so now you can just simply
have a ready process and it's the same
process which you would do for my sequel
Postgres all of these other applications
that's ready available and thinking to
start downloading and running them
inside containers and so just from a
very simple development time experience
you don't have to worry about versioning
you don't have to worry about how it get
configured you don't have to worry about
different developers and different
machines have been slightly different
versions which may become our sync which
may or may not introduce certain
problems instead you can have it all
winning either docker container
everything will process in dependency
and everything can be consistent and
shared amongst your developers but more
advantage under development time
dependences because it removed a lot of
the pain of installing and fetching up
and actually winning the processes you
can start getting much more reuse and
taking advantage of work which is a
people have them for you so for example
I wanted to do some data science and
some data analytics in a very simple way
cuz I don't understand it and I wanted
to use are because that thing like a
thing choice but I didn't want to go
through the stress and pain of
understanding how to set up our I'm a
laptop and actually I didn't want to
Croton around and hanging around when I
didn't even know if I'd be using it in
two weeks so instead I just do download
it as a container so the the our studio
which is like a IDE
you've written a container it's got
everything which everything set up it's
got our installed it's got everything
configured correctly and you open all
the poor and then you can access it via
a web browser you don't have to worry
about how it works internally you don't
have to worry about how to come
it's just they're better I'm ready for
you to start playing and start
experimenting and start to gain the
value which you want a third place not
how to configure it so that's how Linux
works and that's how containers have
worked for the last two years but what
we have now is that docker didn't want
to be just focused on necks docker wants
to allow you to deploy your applications
anyway
Fiat no matter what operating system no
matter what device and so if support
advice reprieve and arm devices and our
MVP use for the last year and now with
Windows Server 2016 we now have support
for Windows Server so what did that look
like so at the moment it's currently in
technical preview five I am I believe it
will be out our ignite in September I
believe there will be another
announcement whether it be an RC or some
further updates for the technical
previews but you can go ahead you can
start playing with the technical preview
you can run it on a shore download the
ISI and start experimenting and winning
containers today and what uber 2016
brings is four key components for new
components we've got Windows server core
and Windows Nano there's our two new
operating systems for Windows we then
have Windows containers and we have
Windows hyper-v containers significant
difference but operate and work in a
very similar way and how a nightly look
and feel with how dock opens our Linux
it's got a very consistent very portable
way of how it's adopted and how if moved
to the windows wilt and again docker
doesn't want you to have to relearn
different things or worry about
different architectures they wanted to
feel transparent and that's what they've
actually did a very good job of doing
and so we'll have our new operating
system we know service
2015 would have the windows kernel which
we have already but with the added
benefits of container awareness and
container based features which we didn't
have before will have the windows
version of the docker engine which works
and looks in a very similar way and then
on top of that we can bring containers
but now we can win containers which are
actually winning Windows binaries so it
seems like faecal server things like msn
q io yes and asp.net locations taking
all of the benefits and only the
advantage which we had in Linux
but bringin onto the windows wilt in
terms of hyper-v containers
I hope of a container to add an
additional layer of security and
protection around the container itself
and so by default containers are secure
they are they have been locked down they
have an additional security and I don't
know from Catco de people do try and
break out and they are there have been
no reported breakouts in the last two
years
however certain companies and certain
approaches like PCI compliant kind of
add additional restrictions and so
having that shared kernel could
potentially introduce some problems and
for bifid where hyper-v containers come
in hyper-v containers basically add an
additional layer of security around the
winning container itself the additional
layer if they very many very lightweight
virtual machine and so when you do da
cavern launched hyper-v container what
it's doing is actually launching a
virtual machine which will give you a
different brand new isolated Windows
kernel and then on top of that your
container is winning it's all
transparent if all managed for you and
it recovers there's very little overhead
if only adds around 200 milliseconds to
be talked so again the timing isn't that
far off but we have split the kernel and
so from a security point of view it's a
lot more secure in terms of news cases
generally for most companies and most
problems Windows containers will solve
everything that you need however the
ones which kind of where hyper-v kind of
comes into its own one is said hosting
so if you are ashore and you have a
burning malicious potentially malicious
code which is currently going to attack
the Windows kernel adding that
additional layer now
not sharing the same kernel between
different clients and different hosts
protects everyone else on system and so
even if they do manage to break out of a
container they'll only get to the
virtual machine inside hyper-v and they
can't they'll have to then break out
hyper-v and it becomes a lot more
problematic for them the other one if
multi-tenancy so where you want to have
more instructions and more control over
which machines which tenants can talk to
widget of the tenants and so this is
where the scenario is where you'll be
using hyper-v containers over windows
containers for the most part windows
containers will solve the problem so
that's how we've got new features which
win containers on Windows for to new
operating systems enable the process is
winning inside the containers themself
so Windows server core is it's kind of
like Windows Server it's got all of the
same or nearly all of the same API so it
works and feels and looks like Windows
and it's got all the things which would
expert from windows so you can remote
desktop into it you have Windows
Defender running if you really really
want to for some strange reason and so
it works in a point and handles Windows
Server the benefit of this is if your
application is currently running on
Windows Server it should be very easy to
port over to Windows server core because
the API then the way it operates in the
way it looks is very similar Windows
Nano is the new those streamlined very
quick down version of Windows if it has
been designed with containers and
cloud-based claimants in mind and that's
been Mike's of focus especially because
they needed something lighter for as
well and so this is how they came down
with Windows Nano everything has been
stripped down apart from the very bare
essentials for example networking
storage drivers the kernel itself and a
few other bits and pieces like hyper-v
and the core to your lab and so you're
talking like if 125 decides it's like
it's really streamlined it takes
milliseconds to boot up and as such this
is a foundation for your container image
which people you can look which you can
use to win your own applications
if you think that kind of like if you
compare these two new operating systems
to Linux we're not - of course it's like
the big proper operating system like a
red hat or and Mbutu or CentOS it's got
all of the libraries all of the API is
it got legacy backward compatibility and
they were in pretty much most
applications Windows Nano is kind of
like the new cool Alpine Linux it's been
designed without to my vision it
designed to be small if designed to be
lightweight and effort if you're porting
applications you may or may not need to
make changes like it'll make a best
effort and if you're not doing anything
too weird and wonderful it will probably
work but your migration may vary
especially as you progress to older
older technologies and so that's how I
feel when people are going to be
migrating people will probably start
with Windows server core get something
working and then sample and experiment
with Windows Nano to see if they can
make it more lightweight and more
streamlined and then make changes where
either when they need to when they can
then how do I do you do this how do we
get started and how do you start by
using everything so
and once you've got Windows Server 2016
I look there they new windows feature
called containers funny enough to
install and that opens open and sort of
hooks and all of the API which docking
needs to be able to deal with Windows
they're there to help scripts which you
can download which installs everything
and it will just run and configure a
host to have docker and containers and
everything set up which you need Ida
script can take a while to win but it's
it's kind of like it's it's not that bad
nobody downloads everything you need it
fed supportive services and downloads
docker and it also did some really
interesting things what I was getting at
prepared such as download in the n SS m
which would like that's a bit weird I
had not seen that before one of the
little bit concerned I want to know
what's winning since how it's done for
the normal sucking service manager which
was like okay when you need when you
don't want to be dealing with Windows
service manager I can understand you
want to get them to another door and so
this is the approach which the team took
is like let's get something working and
let's get our feedback instead of making
or fixing fixing all of the
potential issues which may arise I have
been told that in the future virgins it
will win a popper Windows when it is
said I won't need this but it was a nice
way that the team are approaching the
problem of getting containers working on
Windows is do whatever it takes and
we've got containers and once you've got
duct setup it kind of it looks and feels
like it's on an X it looks and feel
they've got the same api's and
everything is consistent and so we're
dealing with it will kind of look and
feel like this you'll have your dhaka
client which will be like winning on
your Mac Linux Windows machine and then
that can talk and communicate with
Windows hosts or Linux hosts and then
the doctor demon will talk to the
underlying kernel or know how to launch
processes and how to interact and do
everything which you need again from the
viewpoint of how the docker images are
structured and how they're layered and
how that built is identical it's the
same with taking the same experience and
adapted it to the Windows Foundation or
the windows willed and so who will have
the ministry we'll have our container
base image and window serve core or
Windows no no and then on top of that
we'll have whatever applications and
whatever configuration we need be it
sequel server or iOS and everything
which we need and then from the
containers point of view and when you
start it you do on the C Drive and it
will have Program Files about the
windows directory or like 1532 and will
look and feel like a complete Windows
host and that's because it is if this
running inside of a container you may be
familiar with the new Windows Linux
subsystem this was introduced in Windows
10 and it's kind of like the call you
can win bash on Windows this is
completely independent of Windows
containers windows containers are native
they're winning native applications not
a shell color like shim layer going on
and for this it all work inside the
kernel and the Windows kernel team had
been trying to get it working for a very
very long time and so it's only recently
that they've actually made major
breakthroughs what they needed in terms
of the research in order to be able to
get it in and I think the advantage of
docker being around has kind of made it
John
a few priority lists in marks off
schedule so what is a Windows docker
image and how do you actually build and
ruin these docker images so when you
start after you didn't store that lovely
script you will have two new docker
images ready and waiting you love them
no no server and you'll have Windows
server core we've darker image is
everything is immutable so you won't be
running Windows updates you won't be
applying security patches to the winning
windows version which you would actually
expect if it was a real virtual machine
instead you'll be downloading and
pulling the later versions of Windows
server core and then you'll be using
that as the base for your application
and redeploying
your redeploying your new docker image
and so when we start our container again
we're using docker Vern like we did with
Venice Redis with a winner windowsill
core and start a command prompt so in
the blue we've got my host running from
PowerShell you can see you've got the
install script and which I downloaded
and then inside our container we do a
list and it's just it's just completely
isolating it looks and feels like we
know the machine but it's completely
separate also note that it's renamed a
UI so in Linux is a bit different
because after how the display port works
but in Windows a container can launch UI
processes which will make for
interesting future opportunities so how
has it actually been being built so if
we start container we can start making
modifications and we can start
installing things which we we need for
our application to rip so let's
configure in feta pie I guess though we
can win whatever commands which we
normally would burn on Windows for
example IP config we get given back the
network card information on the IP
address so it's actually winning and it
can do whatever you need it to do and
then we can use whatever features of
Windows which we'd normally use in order
to be able to configure a box so for
example install windows fee to web
server so this will install iOS inside
of a container and then we've like that
seems
like a good feature that's brought our
container and our Windows machine up to
the level which we require until we can
exit very go back to our hosts and then
we can use docker commit to save that to
save that customized container and
convert it from a container back into an
image we can then use this image and
share it around and promote it from our
definition to stage into production we
can then also win it so we can take a
newly built newly built iOS Windows
container and we can start it and so in
this case we give it a a we're exposing
port 80 instead of with videos we expose
six three seven really no we didn't port
80 we're giving it the name of the image
which we just built in this case windows
server core is and I typically work
around of we need to tell it something
to learn in the foreground could the
content background jet but that will be
fixed soon we tell it to win CMD but in
the background now we'll bring up the
container that will bring up I is and so
we can access it from a web browser and
we can see the eye is subpage orbiting
inside of a container which we'd expect
however doing docker commits it's kind
of a complete anti pattern in the dock
wilt because it's not repeatable if
you've manually going in and turns in
containers and changing them the next
time you have to upgrade for security
fixes you have to repeat those steps
again and the tenses are you will break
something or forget something or lose
the original script and have to start it
all again so what we actually have is a
docker file the docker file is a list of
instructions which define what commands
need to be ruined in order to build your
docker image in the first place and so
in this example I may have example going
forward we've got a a docker file which
says win install feature
webserver and then we can use and we can
update that script with everything which
we need in order to build a tower docker
image
but a lot of the time you don't even
need to build an image yourself a lot of
time images are already available and
already accessible on the docker hub and
so Authority or not for things like
sequel server it's already been built
and are they available some samples
which mugs off to produce for example
Redis for the windows build some window
burners a MongoDB my sequel iis which
we'd expect all ready and waiting so you
can just take these images and you can
extend them or you can just read them in
your system if you're happy with the
defaults and so again right with the
Linux world we now have that same
features in Windows and if you've an
example of the docker file I was talking
about so a we can specify our base image
which originally would dock of moon in
order to start it we now specify that's
our starting point we can give it some
nice metadata so we know actually what I
contain with winning or more particular
versions and then we can define whatever
commands which we need in order to build
that docker image and bring it up to our
spec in this case launch powershell and
add for web server so in order to build
the image instead of doing darker
commits which commits a live winning
container and turns it into an image we
use docker build we give that a nice
friendly name in this case i is and we
give it a tag in it case 10 this tag
allows us to give it a particular
version and so we can start visioning
our different container images and as we
upgrade and we have i is version 11 we
can roll out that new version and that
new tag and then when we do dock a list
we now have our original images plus the
one which we are just nearly built based
on the instructions in our duct file in
terms of actually doing this for a real
application we can't just install is we
also need to be out to install our own
source code and so the simplest way to
do that would be with using docker win
and so what we can do with window
command simply echo hello world to the
index.html at Parv our build step and
then when we rebuild our application i
it will take the iOS which we had
previously built at the foundation
so if Humes the docker file assumes that
I is have been figured has been set up
it's got everything which we need we
build up and so we now have our is base
image we have the customizations which
we added for our own application in this
case coaching the index.html and then we
can win that newly created container
again opening up the poor and then it's
accessible from from our web brother
which we expect and we've deployed an
iOS application winning on windows
containers using docker and if we wanted
to add I additional level of security I
not win it other windows container but
we're in a other windows hyper-v
container we simply specified the
isolation level and we say isolation
equals hyper-v and so now when we launch
that it will launch that very mini
cut-down virtual machine and win our
newly built darker image other container
inside of that virtual machine and not
on our host kernel yes the question
whether it is something like breach
docker no so this is if something which
myself to do which I'm not internally
familiar with because it's if not open
source however why I believe it is doing
if it's all of doing it basically the
Windows Server kernel that's what is
beating up and so it's just a very very
like cut-down version and it doesn't
even burn if not winning docker it's not
running and he hooks it's the window
server host knows how to use that newly
built kernel in order to be able to
start process and that's what the
process is communicating with via or
communicating fire and so yeah it's
different to the boot docker stuff you
don't know so again you don't because
it's not a proper virtual machine you
don't have to pre allocate the memory
you don't have to give it to you give
them it's just like a proxy for the
Windows house kernel
that's probably the best way of putting
it I'm heating the hyper-v technology no
sir I was saying all of these docker
images should be immutable because we
like immutable infrastructure and we
don't like thing violently changing and
so when we want to upgrade our image we
want it to roll out security patches a
new version
we'd reuse and the duckbill command
would give it
we'd execute the document file docker
file we went all the commands and ethics
would have a newly built image which
we'd then deploy onto our system and
throw you will see things like this
you'll have iOS and then you'll have the
app and then you'll have 1.1 1.2 1.3 and
then sometimes people like to label have
multiple different tags for the same
image so in this case we've got one
point one point three with a certain
image ID which we've also tagged at
latest and some people when you want to
know what the latest is you did simply
do docker win latest and then I will
pull down whichever particular version
is and so you don't necessarily need to
be tracking the version numbers in a
different place it does always say like
just always - probably the latest
version and then I will always make sure
it's it'll tag
pull down whichever one is being tagged
- when you start containing if you also
a lot customizations you'll have a lot
of different details a lot of different
options and to configure out how your
application may be required and so for
example you can specify the MAC address
of the network interface of the
container itself useful if you're
dealing with licensing because it's all
the license maybe a test and the MAC
address and as such that's how you
identify the Machine and identify the
container and so you can start
customizing that you can then also
customize the ports so we didn't we
don't necessarily always one port 80
beam up to port 80 we can split those we
can have multiple different things we're
on different ports on our host but
internally they could all be mapped to
port 80
useful for load testing testing
scalability testing things side by side
and making sure it's bit in that
container container configuration and
host configuration other things like
persistent data is quite important
you don't want the data which has been
saved for example from sequel server or
my sequel living inside the container
can when you like move the container or
you were greater container the date or
goal with it and add search or Luthor
the data which I did and it's
embarrassing and you don't really want
that to happen so instead you can
specify when you launch a container you
can specify a volume to my pin
Ethan the - V and so if they point a
directory on my host to actually be in a
directory inside the container and so
anything gets wrote to slash data is
actually being written to my host in OP
docker elasticsearch when we launch
newly updated containers when you point
it back to the same directory and we can
ensure that the data is consistent and
available across across multiple
different independent versions we can
then also start locking it down so we
can start adding additional security
restrictions to running processes and so
we can say that in this example this
certain container is only allowed 20% of
the CPU shares for example and so we can
start adding restrictions and adding an
additional security and quality of
service to ensure that highly available
highly important applications have
higher shares than less background
willing processes and although I'm sure
that won't contain a concrete completely
saturate the host but the fundamental
aim of all of this is to simply simplify
deployment simplify how we can automate
the lifecycle so how actually looks and
how everything looks in terms of a
pipeline so we have like our git
repository or Team Foundation for the
tree Adair developer comes along Nathan
change commits into team foundation that
will send a web hook and now we'll kick
off a bill task that bill tasks instead
of did building the binaries were like
we do at the moment it will also build
and package those binaries at any docker
image if then leaf docker image which we
will promote and/or grade to the various
different environments and we will want
to reuse that same image from testing to
QA to production
can we want to ensure that we relief in
the same components and the thing which
we built is actually what we're
releasing and testing and pushing it
through the way we can deal with that
and the way we're connecting work with
docker I personally like the darker CLI
because that's why I'm familiar with but
for people who like PowerShell or like
to automate and combine it with large
scripts there is a power shower API I
find it a little bit more the boss but
if it's fixing with the PowerShell
community and there's lots of permanence
so this is command lit for launching a
new container again another kuvira's if
using the same API of docker win
it's a fits and feels more familiar for
the windows admins who like power
shopping so what's actually happening in
the covers and what's going on
internally so the way that Linux
containers and of security and activist
tricks into three groups and namespaces
the groups were created and built by
developers at Google and they limit what
a container can or a process can access
and utilize and so this is where we'll
say if one process can only access CPU
is one to four it can only ever much my
50% of the CPU cycles it can only ever
use 512 of memory and it's secretly to
making and enforcing those restrictions
namespaces control what that process can
fee and what I process can interact with
and so this is how when I said at the
beginning where you ask the container
what it's running and it says just me if
you could if in namespace or by itself I
know the processes belong to that same
namespace the Windows kernel has
introduced this and I've in streets the
same concepts and the same principles as
what have been in the Linux kernel and
so they will look and feel very similar
even though they're completely
independent and completely separate as I
said but we know 13 and 1/2 days for a
while they did draw bridge which would
like an early prototype there's insecure
OS operating systems which is like kind
of cool and on but now it's all
available in all in to the point where
if if they might have led initiative
Microsoft reached out to doc and said
let's work
together honest and let's make a very
consistent standard API instead of
docker going to Microsoft and going like
we really wanted this and so if the mics
of that thing and so that's why I think
we'll see a lot weight being put behind
it as 2016 comes out and I think West's
and gods forward and everything it is
open source or large parts of it is open
source
the most important part kind of is to
shim between how did docker and how did
the Windows Server kernel kind of
communicate and well it's quite
interesting go code and it's quite scary
in places it is all open source and so
you can kind of see how doctor
communicates with the shim and how this
shim interacts with the Windows kernel
and so if you incline and you want to
know what's really happening this is a
great place to go and the Microsoft get
opera in terms of adding building
containers I said that the Hope had got
lots of images already people there's
also lots of documentation around how
you can take have 30 requirements of
features which you need implementing and
doc files would describe how to learn
and how to build those particular images
for example asp.net applications Ras my
sequel and if the dockerfile with all of
the instructions which you need in order
to be able to build and deploy what your
application will require in terms of
things like doing nginx and geniux
you'll see everything will look and feel
very familiar because the docker file
didn't have that many instructions it's
quite streamlined and filled with few
things like there's a form there's
always a base layout some kind for your
docker image you'll see color like
metadata and labels to give it some
description you'll see bring commands
which in previously we just ran power so
it will install feature web server this
is kind of doing something more complex
but fundamentally the same process and
then we can set things like and the
working directory so that when we start
the container or fuge commands or
waiting from that particular directory
and then we can give it like way the
default command in order to launch this
process so if we don't specify anything
assume that we want to launch the nginx
executable and sort of it all baked into
the image
if it sings like Java it looks and feel
the same in this case we are adding
because of licensing and Oracle you have
to go and download certain files from
the Oracle website copy your domain to a
certain directory and then manually
using the add command add them into the
docker image as it being built feel same
in terms of actually the docker image
because document now they've
cross-platform thing doing docker run on
Linux should also look and feel the same
as if you do it on Windows and so doc
hood made certain changes in order to
make that possible so a docker image now
knows how to win itself on different
architectures and so it's got a manifest
file and so when you do docker Paul
Redis on Windows it will download and
pull the microsoft windows all the
windows related binaries and the windows
related docker image when you do to
think amanda linux it will do the
appropriate thing and download Linux
binaries and windows and when you do in
the army will download your 32 bit
concepts and so when you're moving
around and when you're winning docker
containers on different architectures
and different machines it would look and
feel the same and docker will do the
right thing you won't have to worry
about prefix in it with Windows or
having 13 different versions or 13
different namings that will all happen
internally and into the coffers for you
but what about developers I feel great
having windows containers on Server 2016
but like no one's gonna be running that
on their local laptop and though what do
you actually do so thankfully I have to
have also realized this and have come up
with a story so Windows we know is 10
the like the insider release has windows
container support built it and so you
can download it from here download the
inside of leaves and you can get quick
start fundamentally it looks and feels
like Windows Server 10 Windows Server
2016
you simply enable a feature you know
hyper-v you download and invoke some
particular scripts and executables and
then you have windows containers waiting
on Windows 10 do we work way it manages
to do this and the way it works is by
using that hyper-v isolation so it's
taking advantage of that isolation level
in order to execute and boot a different
offer different Windows host kernel and
it just happened that this Windows
Server kernel inside the isolation is
one which have got windows container
support and so that's how they've
managed to very quickly bootstrap
Windows 10 in order to be able to have
it to ruin and build docker images by
using hyper-v which means perfect sense
and now we can just bring docker on our
local laptops we can use that to then
build our images and then we can use
that to promote and put it into
production where overnight windows
containers on Windows Server 2016 and we
can just work it operating the same way
Visual Studio has additional plugins and
so you can build docker images for me
within Visual Studio and add some nice
to pour and add some of those shortcuts
and just make everything feel a little
bit more together so how does this
actually look when you're winning it in
production how do you hide your own
containers in production so for those
who may have for my talk this morning on
kubernetes kubernetes is a great way of
managing containers at scale across
multiple different hosts it will give
you very nicely the ability you'll do
how checking it will do self-healing it
will manage auto scaling it will have
all of these awesome features which you
kind of need when you're winning
containers that a particular scale and
if you've got things like it understands
how your application works and how it
operates and so will when you say deploy
this particular thing it will deploy it
onto the most appropriate host depending
on what other things are happening in
your sink so you don't know you no
longer even need to do capacity planning
because capability is it doing the
capacity planning for you so that's
pretty cool and that's a button called
pin packing and I think what Google do
internally people match pirating memory
requirements or memory applications
which require low CPU could
together on fem Sheena's high CPOE
applications which require very long
memory because they are not constrained
and could not fight another thing
resources your Google understands how to
really maximize the utilization of a box
because it's understanding the metrics
of the application internally and it is
what will be coming to keeping edges
hopefully one day we'll be administering
service like this and we can actually
start flying through and seeing all of
them in pretty lights which we another
one which I really like if my Sofia D
cos when you are using assure and if
you're you think is your container
service this is actually what is
powering that myself have partner with
Azure partnered with mesosphere in order
to be able to win it and listeners kind
of like if you've also got really cool
features which will probably come to
keep netted where our whole
fingerprinting so when you've got a
cluster of different hosts or different
types
methis will understand which host can
win which types of workloads and so when
you execute a a Windows container and
put into a cluster of multi hosts it
will understand that it can only win it
on windows containers or containers
wholesale particular functionality and
so it's not looking like this where like
a linux cask will only ever burn on
linux for Lex
example whether it's Luna Tony with
windows basket a little bit more blurry
no because windows communalism ah
windows so Linux based tasks and so that
will go into mezzos and marathon and I
will get scheduled on to the Linux only
operating systems things like Java which
is portable and course language will be
deployment to any available host which
ever had the most resources free and
then when things like windows containers
will obviously only be deployed on to
hosts which support windows containers
and so this will be all managed from the
viewpoint of the administrators and the
developers they don't care they just say
win this container and then mezzos and
matthan will manage which hosted the
most appropriate whether it's Windows or
Linux
so this drain and so that's all
available now and we
can start winning and building windows
containers and shipping applications
inside of them but what by the future
and what did that look like
so docker will be on IOT devices they're
the cool startups like resin which will
manage deploying docker images onto
arm-based devices at scale so they've
got really great demos where they've got
3000 different devices that do a docker
push and it pushes to every single image
and can see it we're doing a rolling
update across IOT devices or using the
docker api which is kind of impressive
and kind of better than any alternative
ways of deploying software unto IMT
think we're over the container now think
so like why do we have to go through the
ten pages of tick boxes just install
sequel server when we can just do a
docker pull and duck a burn and have a
default version of sequel server willing
and like we don't have to go through
that stress and that pain but if we're
doing that for Windows Server and when
Windows 10 now has container
functionality then what can we do it for
things will studio as we saw earlier
when those containers can launch you is
they can anoint a command prompt so why
can't they launch other applications so
why can they learn Visual Studio why do
we have to download two gigabytes of
installation files to spend another
three hours of them installing and
optimizing themselves when motes of
Khedira or forests ship as a docker
image and then we dictate docker in
Visual Studio and everything just works
and you can alter inversions
independently and side by side and they
won't have versioning conflicts and when
a new version comes out we just do a
docker pool visual studio colon T and it
will download and pull the latest
version and again we can bring them side
by side so that's something which I
would love to see and then we start
talking about everything under container
we deployed applications bri OT devices
be it our desktop applications
everything even england deployed as DS
when a DS docker images and of these
docker containers and they can be
deployed anywhere because everything is
independent and it knows what can win
where we ship it into the cloud
and it just will run on what is the most
available it's appropriate hardware and
it will be deciding that for us and so
we won't have to we can then focus on
more important things like yeah if that
mind containers and the ecosystem and
docker are the hold of lots to learn
it's all very new and there's lots to
try and get your head around and so try
and help that out one of the motivations
why I built cutter Kota if they give you
a centralized place where you can go and
actually take a step by step guide and
understand and learn things which
actually need and so I I would love to
hear your feedback I'd love to hear your
thoughts if you have questions then
please do reach out and contact me with
that in mind we started off by looking
at the physical world we started off
looking at what containers are in the
physical world and then adapt UNAM and
how they apply to the software and how
we can have these very consistent ways
of building images using docker build
into the api we're in an arm using
docker Vern and making it feel and look
identical no matter whether it's Redis
or iOS what's on the inside didn't
matter
Windows Server 2016 brings together four
key components about how containers will
operate at how containers will win in
production we've got Windows server core
and Windows Nano which are the new
operating systems need base layers to
build and deploy our containers from the
containers would then be running from
Windows containers and then if you want
an additional security layer Windows
hyper-v containers everything is
immutable so once that container have
been built it didn't change it just gets
promoted and then when you need to make
an update for example security updates
or new agent of your app you'll rebuild
that container again promoted foo and
everything will be automated and
scripted for you and now we have it on
Windows 10 we have the same level and
the same support for docker and winning
containers
what we have our window theater 16
available to developers who are needing
to build and operate and win these
containers and get them ready for
production next steps I recommend
obviously cat coda
but if you want
this play with it on as your if not very
expensive and it save you downloading
like huge amounts of iOS and docker
images and it's very quick and very easy
to get started with and then if you want
to play with the developer editions and
the developer windows support download
the Windows 10 insider release and it
will be there ready and waiting for you
with that thank you they're made for
your time and thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>