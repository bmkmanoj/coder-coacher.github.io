<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling Docker Containers using Kubernetes and Azure Container Service - Ben Hall | Coder Coacher - Coaching Coders</title><meta content="Scaling Docker Containers using Kubernetes and Azure Container Service - Ben Hall - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling Docker Containers using Kubernetes and Azure Container Service - Ben Hall</b></h2><h5 class="post__date">2017-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jvdInWOlt3U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone at the moment
we're at a great stage within the
container revolution it's getting all of
the excitement and everyone's
enthusiastic about it but what we're
also seeing it how it can actually start
working and how it can actually start
working and adding value to our
companies and to projects in production
and delivering back on the promise of
how these containers can deliver great
amazing things and this is kind of what
I want to go into today I want to talk
about how we can actually take
containers remove some of the birds and
buzzwords and excitement and actually
get down to how do we start delivering
value and how is we do start adding and
supporting containers in production and
at scale using toolings like kubernetes
so my name is Ben Hall I am the founder
of company called cat coda and cat coda
is an interactive learning platform for
software developers we give free
hands-on interactive tutorials and
everything is in the browser you don't
have to download you don't have to
configure everything and we have topics
and content on kubernetes on docker and
open shift and we take you from getting
started all the way up to running and
scaling in production and for today what
I want to start with is why I personally
consider containers being important why
I'm so passionate enthusiastic about
them and then I want to walk you through
the process and some of the tips and
sort of the trick start picked up about
how you take you from building and HP
that net core application running as
containerized application deploying that
on to kubernetes explain what kubernetes
is and then using Azure to help scale
that out help win that in the cloud and
then explain some of the advantages some
of the benefits that kubernetes can
bring and what problems it can solve and
so if we start and look at the ecosystem
at the moment we're in this really
amazing explosion of technologies we
have also many languages which are
appearing such as HP that met but also
seems like elixir and F sharp and some
of the functional things
starting to become mainstream and this
is great but also it becomes a problem
especially women too many to maintain
all of these different languages and all
the potential applications all as a big
data solution to all of the most equal
data storage offerings and if we always
have to reinvent the wheel if we always
have to figure out how we need to
install how we need to manage how we
need to backup all of these interesting
complexities start and come into play
and this can have a huge impact about
how quickly we can innovate and how
quickly we can start dr. new
technologies if we keep needing to learn
everything every single time and we need
to change how production works and
operates into in order to support these
technologies then that's going to add
barriers that can add blockers and it's
just going to remove some of the
advantages that they can bring and then
once everything's running we still have
to maintain it we still have to worry
about how do we do upgrades and how do
we make sure that we don't have any have
any downtime in the meantime and so this
is life in containment kind of have huge
advantage because it can help simplify
and help remove some of this complexity
it can help remove some of the
uncertainties about how applications
should operate and how an application
should scale what's required from an
application in order to run effectively
and properly and securely and by having
everything at the container we
introduced this standardization where
everything becomes a very consistent
experience from building applications
using different languages to run in and
deploying them at scale and top on top
of technologies like kubernetes and I'm
sure many of us have seen a container in
operation we have the docker command
line and this allows us to go off to the
docker registry which is about all of
the docker images and these docker
images have everything with the
application or the process needs in
order to be able to start it's got any
operating system dependencies it's
getting Winton dependencies such as the
done it framework or the JVM and it's
got all the application configuration
the default configuration needed for the
application to launch successfully and
this allows us to do things like doctor
give it the image name which we're
interested in in this case Redis the
particular version so we're not just
using the latest greatest we're actually
been very targeted because the latest
tag is amazing for those whom I've
female but you just got a winglet
reddish and it will pull down the latest
version and you get the latest and
greatest version available but once you
start depending on this latest version
the problem is that can change the
latest tag is a moving target and so one
day you'll be running registers and
three and the next day you may be
running with this version four and that
could industry some unknown side effects
so when I run out images I always I'm
very distinct and very concrete with
what version actually want to run just
so I don't get any unexpected side
effects and so in this case it's three
dot zero dot three and to make that
container accessible I open up the port
so I make the port available on the host
six three seven nine so any traffic I
send to that machine that puts it and
gets passed to the container running
process and now it's as if from a
development experience as if I've got
Redis installed on my local machine and
so I can use and interact with it as a
development dependency without ever
worrying about how that needs to be
configured in other covers and as this
container gets container technology gets
adopted within more interesting use
cases and more complex applications
starting to be containerized so now with
some of the amazing work that the
Microsoft team have done sequel server
the official full-blown sequel server
wins on Linux they've managed to migrate
the Windows NT kernel into the Linux
user space using things which I do not
fully understand it's a magic but it
seems to work very nicely so
congratulations to them but it works and
Winnick Linux and you can execute all of
the sequel commands and all of all of
the operations that you would expect
from equal server on Windows but because
we now have it on Linux it means we can
now run it effectively inside of
containers and so in this case I'm
launching the latest CTP or sequel
server 2017
and as it launches out you can kind of
see normal operations which would expect
from sequence survives creating the
master it's going to model we've got
some temp DB initialization happening
and when we launch a container we have
to pass it certain environment variables
because it's a big blonde system things
like we need to accept the end-user
License Agreement and so we need to pass
it in as a environment variable and we
have to set an SI password something
super strong and secure such a dis
beautiful password of mine but the
benefit as you can see is digit take
this is everything which is required to
win sequel server we don't have to
download huge massive installation files
and I ourselves anymore we don't have to
sit there and going through ten fifteen
different screens and ticking boxes
which we're not quite confident if
that's a box which we take the last time
we try to install it or if it's the same
configuration my colleague next to me
has and so now this container introduced
if standardization about what sequel
server looks like and health equal
server operates and of course we can go
in and can we can tweak and modify and
add all the extensions which we won but
we've got this consistent baseline to be
starting with and because we don't have
to go for the installation this is where
things like the accept and user License
Agreement becomes a little bit different
because like how do you take that box to
verify that actually I do agree to these
terms and so this is where you see
things like the old way of working and
the traditional way kind of being
exposed in a containerized system and so
this sequence server started from an
image that millions of images available
on the docker hub and um these can ever
be used as kind of like foundation and
you can start those containers and win
them within your systems or you can take
an embrace and extend mentality you can
use these as a foundation for what your
application looks like and what your
application desires in this case things
like the dotnet core and then use that
the foundation to deploy your
application on top of and all of these
images are based on layers and this is a
really important concept within the
docker
because when we deploy our application
and they depends on dotnet we don't want
to have to deploy its own unique version
of.net every single time if we running
100 200 different services if they all
had their own installation have done
there don't just become a maintenance
nightmare and also we need much bigger
disks a much bigger hard drive space and
it would be much slower on the network
and everything was just eventually going
to a halt and we couldn't do anything
apart from wait for the done that
framework to be downloaded and that's
not very motivational it's not very
productive
so instead by having it as a layer we
can actually start reusing and
repurposing the files underneath they're
all sandbox and all of the containers
are in completely separated for a twin
time purposes but in terms of files on
disk they can share they can reuse and
there can be much become much more much
more effective in how they repurpose the
different dependencies that they require
looks on the application viewpoint it's
all the same everything gets flattened
and they just see the application
hierarchy and the flat can emerged file
system but in the recovers you've got
the layers underneath and now he's got
these containers and some of the
benefits are they've got the run poser
space so they kind of interact and
interfere with other applications which
are winning they've got their own
network interface so you can open ports
and send traffic to it through
directories so if you ask what's running
inside of a container it will have
ordered files which I expect from a full
virtual machine or a bare metal machine
such as user directories bin directories
a Windows program files and system32 etc
but they're completely sandbox and
completely isolated and so in many ways
they deliver some of the benefits which
we would traditionally get from virtual
machines that containers a signet
significantly not a virtual machine the
MS more lightweight the mid more are
flexible and you can get a lot more
benefits from them in different ways but
for me one of the main things that about
the performance because we're not
depending on its hypervisor which is
managing the relationship between a
guest operating system and our
in the line host machine we get all of
the mated performance and in particular
we get the native i/o which means that
containers are great for winning big
data workloads winning sequel server in
the maximum performance possible where
traditionally VMs could have an overhead
through that bottleneck and through that
data path but the other benefit is
desktop containers don't need a guest
operating system containers operate
against a host kernel and we use a weak
take advantage of what's already there
and so because we don't have a guest
operating system we don't need to wait
10 minutes for it to boot up we don't
need to pre allocate a VM 8 gig of ram
in order just to run our know process in
isolation instead we can take advantage
of the SEM boxing and the nature without
pre allocating any hard disk or space or
CPU etc and so what's actually running
on our machine is actually what's
required and so we can get much better
resource optimization and capacity
planning about how our systems work and
because we don't have this free
allocation step everything takes
milliseconds to launch docker is
fundamentalist a process manager it
doesn't do anything apart from launch
and secure processes that is
responsibility and that's all what's
happening in the de covers it does this
via kernel virtualization and so when
you run the process it's taking
advantage of the kernel from the host
machine and that touch gives you the
performance and the capabilities and the
recovers but the three main concepts to
keep in mind when you think about dark
or anything about it container
processors
you've got the brilliant process itself
which is the container this process but
launched by a docker image and you can
think of this as like a layered zip file
if you exported a container image it'll
be exported as VIP and that VIP which is
have multiple folders inside of it it's
reflecting the layout and the coverage
and then you've got a docker registry
which can be public and hosted on the
cloud or can be privy and hosted inside
your data center with all of that in a
moment
literally important to remember that
docker
the only game in town Dhaka isn't the
only container win type we've got some
really exciting technologies such as the
container real-time interface as
particle even it is we've got green sea
which is a really low-level way of
starting containers got container D and
rocket and they've all got their slight
opinions and their slight variations and
some of the different advantages that
they bring to the party but they are all
to hearing and working towards a common
standard and that allows us to try and
move around and experiment with
different technologies and repurposing
and reusing the same images under the
covers and so we've got the container
window we've got the open container
initiative and that defined
specifications so the wind time spec
which is actually how these start
containers and how do you talk about
them and the image specification which
is how do you build an image and what is
an image look like and so we can have
different technologies which all build
and all go into this overall system and
this is what allows us to do things like
rent equal server and a consistent
reliable way we start our container
based on the image and that's what
brings up but the containers aren't used
just for background processes we can
also view them for normal everyday
command-line tool in too and so when I
work with a jewel I don't necessarily
want visual command-line installed on my
laptop because I'm ensure that the
dependent is requires my laptop at the
moment is working quite happily and I
don't really want to mess with that but
I still want to take advantage of the
tooling and a still one access and to
the azure command line interface and so
now I can use all of this as a container
I can launch a container I can pass this
flag IT to make it interactive and work
at the tour terminal and then it works
and operate in exactly the same way I
can set my credentials I can learn to
manage order my Azure machines and then
when I'm doing I simply exit the
container am i I'm back on my hosts and
everything is still exactly how I left
it and so now I don't get conflicts
between things upgrading in the
background which may have an impact on
how I actually win my applications and
how I actually do my daily development
work
and all of these images are based on the
hub which I've mentioned and they're all
official and so you can look at 90% of
your dependencies I'm sure have an image
already available and you can play down
you can start that or you can simply use
that as an inspiration and build your
own container and configure it how you
need and these images are great they
give a lot of value and means that we
don't have to worry about dependencies
but what about our own applications
while how do we deploy and how do we
build our own applications using
containers so what I've got is a amazing
application built with a feeling air
core and it's got two classes and this
is the fundamental things required the
first program file I'm sure many of us
have seen and if we've been
experimenting with a bit on a cool these
boot strapping that hem self hosting
HTTP server and the most important bit
is we're defining the URL which we want
to listen on but in full sometimes
applications like to listen on 170 2.00
that one which means it will only
respond to traffic from the same network
interface ie localhost but when you're
in a container and running in a
containerized world only listening to
local host doesn't really help you
because Nissen else is running inside
that container which means you will not
listen to anything apart from your own
messages which doesn't really help you
scale until when you launch up casing
you need to be very explicit and make
sure that your binding and listening to
the outside world and you do that by in
this case with a speed on it pointing to
zero zero zero zero so all of the IP
addresses on the machine and in this
case listen on port 5,000 so we can
respond and accept traffic and then
wouldn't a request comes in no matter
what request it is we simply respond
with the hostname of that request which
in this case is also the whole thing
maps to the container ID because that's
by default how it works and coverage so
this is our application and if you what
would be using our advanced as our
example and then we need to turn it into
a docker image a docker image will then
allow us to win out of the container and
an allowance to deploy
Quebecers and so to start I'm start
building these darker images we need a
darker file this docker file lives with
your source code it can be checked into
so your source code repository such as
get and it can be maintained and
supported with the rest of your
application and the first part of the
doc file is defining what your
application needs in order to be able to
learn effectively to what dependencies
today require so this could be nodes
version 6 node is an 8 or in this case
and net 1.1.1 and because we've done
that we need the build tools we need to
be able to take our source code and
build and compile the binaries download
the new get packages and perform other
things until we say that we need the SDK
tooling as part of that and this comes
from the hub and the different versions
available depend on what you require so
if you want to go ahead and you want to
start experimenting with and asp.net 2.0
there's an image available for that and
you can go ahead and download without
worrying about how that will impact you
know how that will affect your machine
so the next stage is once we've got a
dependency we just need to define how do
we deploy our application and thankfully
with the word which one sort of doing it
becomes a very standard streamline
process depending on without too many
switches between different variations
and applications so the first part is we
need our application and we need it we
need to give it somewhere to live so we
make an app directory and we set that
the working directory we then copy our
CF pod file from our host machine from
our source code and pull it into the
container and into the container image
itself and so now we can use that file
in future commands such as running
dotnet we store and so no towhead when
that gets executed it will download or
the NuGet packages and all the versions
have been defined in the project file
and so we now have that foundation for
what the application needs the next
stage is we need the application itself
and then we need to build and publish
the binaries as a result the reason I
split these two is quite important
because we don't really want for every
single build we don't want to have to go
back
to Newgate and download order packages
if missing changed if our CS pod file
lists exactly the same dependencies and
the same versions of what it was the
last time we built the container then
listen to the changed everything to be
identical so I haven't go back and
download that maybe five Meg or maybe
fifty mega dependencies it's just going
to add a lot of time to your build
process and it's just a lot of overhead
which you don't need until part of the
process docker has caching built in and
so docker is aware of the files and add
versions and if they have changed or not
and so when we do this copy if that file
hasn't changed Dokka will just refer
back to the cache and use the cached
version of the run commands from the
previous build and so in this case if
the project's haven't changed don't
every store will give you the cached
version and so we don't have to go off
and redownload our dependences so we get
much quicker build time our application
source code is likely to have changed
and so that will invalidate the cache on
the next line and as such it will then
rerun and we Bill we execute the real
instruction I said we get newly built
compiled binaries as we thought and
these kind of structure in a thinking
could have a huge impact on build times
especially as you start scaling out to
different projects and different
approaches so application will now
define how we build we think we male
need to say how to execute and what is
the metadata around our containers so
what ports is our container listen on
and how to use that application start in
third place and by defining this within
image this gives us that standardization
and that standard way of launching
containers without worrying about what
technology it is on the inside if every
time we got an image which we needed to
play the first step was we needed to
look inside and figure out what tenant
technology was and then look up how we
actually launched it that's going to add
a lot of overhead that's kind of having
manual interventions or requires
scripting and automation by having it
baked into the image all of the
information which we need to launch is
that and so we don't have to go off
think about how that works and then how
that operates so we've got this docker
file
it lists our dependencies so we need to
build and run that and so when we do
Tucker build we give that nice friendly
name of cata :
we give her a version number so we know
what build was we're so into and we know
what build we want to push out in this
case v1 and then data will run through
every single step in our docker file
execute the commands and we will
eventually get our build image which has
got everything our application needs so
the dotnet framework it's got all of our
new pet packages and our built binary as
we thought and then that launches in
exactly the same way that we was running
things like Redis or sequel server this
is the advantage that having a
standardization and a consistent way of
launching and stuff in containers really
add value because whether it's have done
that application whether it's a mode
application whether it's an f-sharp
application we still have this
consistent experience from the outside
and a consistent way of managing them
and it really simplifies the build
pipeline to because we don't need our
build agent to do anything apart from
know how to communicate with docker and
how to run docker build the build agents
don't need to be configured with
particular versions are particularly
works we don't need to start
ring-fencing build agents to win that
legacy application which hadn't been
touched but we need to be able to build
and release it and it wins on a
particular old version of.net The Smiths
announces that can all be encapsulated
and defined by the dock file and when
you build the docker image all is that
processing all of that version is
happening with in containers itself so
our CI CD pipeline just communicate
through ducking out calling things like
Dugger a build up of runs when our unit
tests and verify that everything happens
successfully and we'll start to see
these consistent experiences and this is
how we define node we still see the same
pattern defining our base dependencies
copying over a package.json to define
while dependencies are installing and
building
and running in a consistent way but one
thing you'll see is these images start
to get quite large especially now with
bill I'd done that application but it
also included all of our SDK tooling
now this is great from a build point of
view really exposes a security risk when
we're winning in production because we
don't necessarily want a GCC compiler on
our production servers because if anyone
did manage to find an exploit that have
all of the tool in which they need in
order to be download source code
compiler and win it and launch it on our
systems so docker has realized this and
they've never corrected it and added
this concept of multistage builds and
this allows us to take our existing
occupier which we all know and love this
will build our dependencies on
applications but other side effect is
quite large like roughly 900 Meg and we
can improve it and so we can take a
docker file and combine that with
something which is much more streamlined
and so we can add this second stage to
the process and the second stage as a
way to optimize how we want our
container to be built and operating and
winning in production so we've defined a
second base image in this case we only
need the win time dependencies of net in
order to launch so this have been
clicked down all of the additional risk
and all of that additional attack vector
because we don't have to build two links
in there again we define things like
working directory is and how that
container image launches and then when
we need add a binary we can copy it from
the first stage the third stage which
had always built all engine to produce
opus in and created out directory can
copy that from the third stage into the
second stage and that is what it will
launch another what we push into
production and as we thought what you'll
see is you'll take a 900 mega docker
image we've got everything and that can
be clicked down a couple be streamlined
do something which is still on the large
side but it's a lot more streamlined a
lot more performant a lot more easier to
deploy into built in at 250 Meg roughly
and we'll see that now because how do
you are prey and how do you start
putting these things into production now
we've got our image build and we're
ready and we want to scale out so the
first thing is it needs to go somewhere
it needs in order to be format build
agent or formal definition and pushed
into somewhere there's accessible farm
production and so you do this using
bucket push the push will push it into a
registry the registry which is where all
of the images are stored and so in this
case it's pushing into the public
registry and making the image available
publicly great for open source projects
or community ideas which you want to
share and make the mediums clay one
important thing to notice is it only
pushes the layers which our application
has changed and this is where that
layered file system really starts taken
to account and benefit we don't need to
push the entire data framework because
it's already on the registry that's
where it came from and so docker is
aware of this it checks to make sure and
so only they pitch in the 14 Meg and
which is our application and our
dependencies and everything else is just
taken care of from what we came before
and then when we do new builds and we
create new versions will push those in
giving them a different version name and
again it will only push the layers which
have changed so that's where we get can
get a lot of value from having these
optimized settings and if you go into
the public registry go into calf coder
you'll see the beautiful application
running on the system and form a build
CIPD pipeline we kind of walk through
these steps we went from darker build
and with clay to the images and then
when we started the release process that
involves a dagger push and then when we
went to our servers would do a docker
pull which would download everything and
make it available for deploy but the
people at image the public registry
isn't very good for companies it's not
very good because we don't really want
to release our source code into the open
world and so this is where you have
hosted private registries you can
download them and win them locally on
your machine or you can use things like
a short and let the Shore and Microsoft
manage and make the veg
available for you so the container
registry it's very cool very easy to
work with and it fits natively and
naturally into the rest of Asher so when
you go in you go into the portal and it
ask you a few questions mainly what URL
would you like
what do you Ness name would you like and
where would you like the location of
your images to be so whichever data
centers you are particularly used to
deep pointing and then you create a
registry and then you'll get given the
DNS name which we selected in this case
cat coded demo reg one bit of a weird
name but it seems to work you're going
to have a login a username and a
password and so this allows us to from
our command line login to the registry
using the DNS which we entered using the
username and our super long super secure
password which as you're provided for us
so now we have this as your as your
container registry which is private and
only people which we've explicitly given
access to can access our images and so
in order partial images we need to tell
that image where to go and where that
image should live and so but fulle it
goes to the public registry which is
darker Daioh but we can change that and
we can prefix the image name with a
particular registry URL in this case our
private one and as your and when we do a
docker push it will now push into our
private registry using the
authentication tokens which we get
logged in and then the image is
available and we can see it in the UI
and that's now our process so we now
have it in Azure so now we need to
deploy and this is where kubernetes and
complexity starts to be induced because
go into every single machine in your
cluster and willing Darwin isn't the
most effective way of deploying
applications this can introduce a lot of
uncertainty entities a lot of manual
processing and it introduces a lot of
our risk
so we need a way in order to capitulate
that risk and reduce it and have a more
streamlined automated way of processing
winning it and this is where container
orchestration comes in there's many
different orchestration so you may have
heard of things like duck swarm which is
Dulles native one things like DTS things
like kubernetes and their responsibility
is making sure that your containers are
running in a healthy reliable consistent
way across multiple different machines
and allowing them and enabling them all
to communicate and act as you have one
big compute plane and making that work
and so let's look at what that actually
looks like in reality so ma one of my
personal favorite technology is
kubernetes kubernetes defines itself as
an open source system for automating
deployment scaling and management of
containerized applications so it's kind
of solving our production level problems
and making it making it easier for us to
manage our applications which we have
just built in kubernetes was born from
inside of the lessons which Google have
learned Google have been running
containers for the last 10 years they've
run everything from databases to that
big data workloads to their third
engines to Gmail to Maps everything
inside of Google rinse as a container
and if you can imagine at Google scale
they have built some really interesting
really unique ways of managing this
across like 40,000 different datacenters
and however many machines they have now
and this system with called Borg and to
build it held Google rinse containers
and they produce of white paper which
define how they operate some of the
lessons which they've learned and how
they might have managed that in reality
and while white papers are really
interesting to read they're not as easy
to implement in reality you have to do a
lot of effort to go from a white paper
to have something running and so in
order to be able to capture that and in
order to be able to make an impact
Google decided to launch kubernetes and
start this open source project and so
they took the lessons which I've learned
they made it applicable to a wider
audience and wider vision applications
and approaches and then released it to
the Linux Foundation and a new
foundation called cloud native
foundation and so it's not driven by
Google Google don't have a incentive to
control it they've just given all its IP
all of the copyright all of the
responsibility to a third party
independent nature and what this means
is other vendors and other companies are
much more willing to adopt and work with
the foundation because they're not
driving Google directly they're driving
the community as a whole until you see
comparing the companies like Microsoft
and red hat pin a love way and a lot of
effort into kubernetes because it is a
community driven project and it has that
approach Google is still very much
involved if you go to the Google cloud
platform and you say want to win
containers in the de coverage is all
kubernetes that you're dealing with in
that you're managing and so Google is
still very passionate still they infuse
eyx enthusiastic and still a major
driver behind the project it's just that
they've invited other people to the
party which is great from our point of
view because we get to build build on
the shoulders of many amazing companies
and if when sponge ability is to take
this ocean of containers you have data
leaks so we may as well have oceans of
containers too and take all of these
containers they all come into
communities and cuvee Nettie's
responsibility is figuring out what the
best most optimal healthy way to win
those containers across multiple
different hosts and multiple different
machines and with that comes with some
really advanced really cool features
things like role based access control so
who is actually responsible and who's
allowed to deploy and manage containers
running in production but not just who
but also what context are they working
in making sure that team a can interact
or interfere with team B's containers by
mistake making sure that you can start
defining quotas around what what teams
are allowed to deploy and how much
resources are allowed to be
allocated and he could start seeing
companies build similar things to like
platforms other services and cloud like
nature within their own company and you
can say that like team a is allowed 20
gig of memory and allowed to deploy
thousands of outpatients while team B is
more constrained because it's a test
environment or a staging environment and
so they're only allowed a maximum of two
gig so if something does go wrong or a
process goes rogue it didn't take down
our cluster didn't interfere or interact
with other things winning on the system
at a host we see things like auditing so
if you need processes and you're
tracking about what's being deployed
within a system for like financial
regulations for etc then kubernetes is
keeping at the track and it's keeping a
list of all the events that are
happening and you can see when versions
are being upgraded when systems are
being scaled up and scaled down also
really helpful more formally or to
debugging and just an awareness of
what's happening within your system can
see who did what at what point in time
and a lot of other features which are
amazing one more time a particular fan
of a batch X batch execution wheel in
containers it's great most applications
will be long-lived that we databases or
they'll be web servers and we want them
to learn and if we deploy them we want
them to be stay running and that's
that's their aim and that's our
responsibility in life but sometimes as
we enter into a world of big data
we don't want long run in Big Data
workloads what we want is short small
optimized workloads but we want
thousands hundreds tens of thousands of
them and we want to be able to learn as
I manage and process this chunk of data
and when you're doing just simply stop
your execution and let give up your
space to the next process in the queue
and this is what communities is designed
to do cubed s is is designed for
managing and scheduling workloads it's
not just about long running web servers
about what workloads require execution
and so we can have our big data
processes we can send them to cuban
entities and kubernetes will manage what
resources
are available on our cluster when a
resource becomes available it will
deploy the workloads and we're not
finished and resources are available
again it will deploy the next workload
in our system really powerful when we
start to think about things like GPU and
how I how we can make GPUs available for
Big Data and GPU will sit there in a
server server may have eight of them
depending on how passionate we are about
our workloads and kubernetes will say
like we can win a maximum of 8 workloads
at any one point in time so will
allocate those 8 they'll use a GPU
however they need and when they finish
the next workload will be done and will
be processed and we started building
this queue and we don't care where it's
willing we don't worry about how to make
it run and how to allocate that resource
explicitly instead we just want to make
sure that has been run at some point in
time so keeping it is really advanced
and really cool and this is where as
your fits into the picture so the usual
container service is the new offering
for Microsoft and it's responsible for
winning containers but the way it does
this is by bootstrapping and deploying
clusters like kubernetes or swarm or
DCOs and it does work as your is amazing
app so starting machines making them
available ensuring that they're in the
right availability sets configuring
things like load balancers assigning IP
addresses but it doesn't do any more
than that it then lets kubernetes do
what kubernetes is awesome at - which is
managing workloads and scheduling
workloads and making sure that our
containers keep running and so it's
really a best of both worlds and that's
why I'm really excited about why
Microsoft is heading in this direction
and so when we deploy our when we deploy
kubernetes on top of the shore it will
tell us and define what the default
configuration and setup is that we need
so we need a master which will manage
all of the scheduling and manage all of
the hell - making sure that what we've
asked to win is actually winning and
then we'll have nodes or agents running
and now we'll actually be the ones which
do workload out do one where the
containers actually winning and it sadly
executors none and this is all
structured with an availability sets
until we can start adding reliability if
we lose them at your region then the
containment surface can cope with that
and make sure that that's been defined
correctly and one of the really nice
things about winning initial and running
with kubernetes is it all integrates
naturally it's aware that is winning
inside of the azure data center and so
it knows how to interact and configure
things like load balancers until we
don't have too many go in and assign IP
addresses we can let the process and
that competitive manage that and that's
what we'll see so the three the three
current container orchestrations
available docker swarm which is a native
default and farm docker which is very
good it's got some really nice features
got kubernetes which also one I'm
passionate about and DT OS which is
awesome but you need to be at certain
scale I think or I feel in order to take
maximum advantage of it and since why I
think kubernetes is and sweet spot
between both alongside kubernetes as a
project which i think is awesome too
called OpenShift openshift is created by
Red Hat and it takes Hooven it is with
kubernetes image covers but they've
added their own opinionation opinionated
views on top of it and delivered a
cleaner out the box experience and to
all manage things about your entire
workload process from start to finish
and so it integrates with Jenkins so you
can start building at container images
it will then from Jenkins integration it
will push it into its own private
registry internally so that's been
managed and maintained for you and then
because it's running on top of
kubernetes or scale out across multiple
nodes multiple datacenters
and keep your application win it and
then it comes with all the security or
the routing everything which you need in
order to be able to win effectively and
in is because it's just kubernetes and a
lot of features which we feed like
role-based access control
originally started within OpenShift and
I'm Google so then Red Hat Postum
upstream are made them available to the
rest of the community which I think it's
cool
so I'm hoping that that will appear on
ideas or container service soon so let's
say we want to deploy kubernetes let's
say we want to go and we want to use a
shorter Beach drop it and so within the
portal there's the actual container
service and the first option which we
ask you with what do you actually want
to deploy so drop down menu select
kubernetes we're all good to go the next
stage is we need to configure as a
master so we need to configure the
machine which is responsible for or
managing all the workloads within our
system so we give a DNS name so we can
talk to it we give it the public key of
SSH so we can log into it and then we
deliver this strange service principle
which I'm not 100% confident about but I
do know if you win those four commands
then it will create you the username and
password and from my understanding the
wages have used is it's used in order to
be able to configure the azure load
balancing and configure the ensure as
your environment within Kluber natives
and create that barrier and create that
integration with the azure cloud trail
we don't need anything stored we've got
it all within the browser which is
awesome until you can take these
commands and it will produce the
information which we desire and the next
thing which we need before move on is
the master camp so three on one three
five nine masters just for reliability
purposes and depending on how large your
managing I you're anticipating your
cluster to be will depend on how many
masters you expect
I'd recommend winning three if you're
winning across multiple datacenters or
multiple regions and because if one goes
down if you lose the region your system
will still be or it will still be
healthy and still be managed but for the
purposes of experimentation one more set
is great and it'll get you there and so
for problem the next stage is then
configuring the agent to find how many
agency ones are how many nodes will be
processing the workloads depending on
you've got so you can start with wine
any contains it later which we see and
define what machine type or what size so
the standard is to machine in the drill
and then what operating system so far
yeah talk about a month and then
Microsoft and sure we'll go off it will
deploy and start all those machines
which on city so it's very good at and
then deploy Cuba net ears and configure
all and boot up all of that commands and
this was building for the portal but
with the like it's a sure there's an API
to everything and so we can do it for
the command line if we wished and so by
using the jewel go into the ACS and
saying create we give it what type of
Orchestrator we want in this case
kubernetes and that will deploy our
cluster we can then install command line
tooling from managing and interacting
with the kubernetes cluster get the
credentials required and then that's
originally need and with the cloud shell
willing on the iPhone we can do that
from an iPhone everyone on our train on
the way into work and then by the time
you get to work it or been happily
deployed and everything spinning for us
which is pretty awesome
so we have our agile has now bootstraps
kubernetes for us and so we can either
SSH in based on the public key which we
gave it or if you went for the
command-line we have installed the
client tool which is called cord to
cuddle and can we we can remote
administrator and from our local machine
either way perfectly acceptable and
desired I'd probably recommend
downloading it locally because then
you're not a citation into boxes but the
choice is yours
so the third thing that you can ask is
out though you reached up my cluster so
what are you aware about and you can say
things like given actives get nodes or
you could all get nodes and all this the
master or live list the agent which is
aware of and what their status is so are
they down are they being configured or
are they ready to be have workloads
deployed on top of them which in case
case they are so now deploying workloads
and a lot of the complexity about how
operating
have kind of been hidden for us it's all
available and it's all happening in the
covers if we want to go low level but
from a high level we can simply say
could could have learn so there is able
to do Korean give it the name of our
deployment and what the project is which
we we want to win and he image which we
want to be deployed in this case our
done net example which we deployed
deployed in earlier in the presentation
or built earlier in the presentation and
then we can ask kubernetes certain
things about what the state of our
crystal and what the state allow
deployment are things like get
deployments so this will list everything
which is winning on our cluster and it
will list the version numbers which is
why within the desired and current state
how many of diversion or how many of the
containers look to date and winning
particular datum we won and how many
available and so we get some insight and
as you start scaling up we can see what
that state is but in the recover is
fundamentally it's all been in
containers that's the process of what
the relatives doing when we say cube
could organ we are scheduling the
workload with scheduling this image to
be deployed in front of our cluster
kubernetes will then go like oh cool
I've got something which I need to do it
will find the most available node which
would work I'll needs in the case of
this we've only got one agent or one
node so that's where the workload will
be scheduled on it will tend to how that
node that it needs to do something in
this case Lourdes container and that's
what we've netted is managing and
wrapping up and make an available for us
so now we'll have our contains started
but we need to make it available to the
outside world and we do this with who
could all expose we tell it what
deployment we want are the information
about ports so our application is
running on port 5000 so we say that all
traffic all of the incoming requests
should be targeted toward pipe 5000 but
we can say actually from the outside
viewpoint we want to hide out so we're
not everything on port 80 and so when we
quest come in on port 80 send them to
our application send them to this
cluster and all the traffic should go
into port 5000 and finally we say tight
low balance
this means that it will go off to a
Shore and configure the low balance of
forests define an IP address make sure
that's all willing in a secure nature
and then give it back so we say get the
services internally or have a know its
own IP address but you'll see that it
pending waiting for the load balancer to
be configured and waiting for that IP
address to be started and allocated
until after a few moments the IP address
will appear in this case 40 that 1856
and we can now send request it up we can
update our DNS we could add a cname and
give it a nice pretty URL and that will
now start responding to requests from
our application this would go to as
you'll go to a given X plus go to the
most appropriate pod and that is what
will get processed and now everything we
have more requests everything will be
responded to from our container but
ridding one process isn't very
interesting so we can now start scaling
up and scaling our workloads so Cupido
scale and say we need three replicas we
need three instances of this application
we're in our class our cluster and in
the recover that game kubernetes will go
I need to schedule from workloads find
the most available nodes if you had
three nodes within our cluster it would
deploy one instance under each and a
spread mentality to try and make it as
available and available as possible in
case we lose that node or an application
crashed for example and then we'll have
three pods three containers winning
always our own unique names and in the
de coverage this is also updated and
figured as well balancer it's aware of
what the state of our cursor is it's
aware of what's winning and it's aware
of how we hold it to expose our service
and so when we started in this IP
address using curl will get round
webbing to each other available pods in
our system now okay from a system
so with these men's we could take as
your container service are winning
kubernetes cluster deploy our
application with the particular image
exposure and make available with us or
load balancer and then you get service
to get that excellent IP address and
find out what's been allocated or we
could go through visual portal it's all
listed and it's already find it in there
and try to make it available and now we
can start scaling it up Scott and start
scaling down our application but as
we're doing this like we've only got
only got one agent so what happens if it
goes down right how is our system start
responding and the tensor we want more
than one agent and we want more and the
one thing on our system and so fuzzy as
your portal agile as I mentioned is
managing the infrastructure order is
managing and making sure that these
machines are available so we can set the
agent count would be whatever we need we
can scale it up and scale this down as
we quiet so in this case we're saying we
need two rooms has been saved as all
will start spinning up and in this an
additional machine and they'll it also
knows how to configure kubernetes
so add that new node into the cluster
and that node will then get configured
and iterated them initialize and after
two minutes or so we'll have that
additional agent available so makes it
really easy and simplified to add
additional workloads and additional
effort what we need in order to meet our
demands when that demand has gone we can
scale it back down again and so now when
in our system and now we've got this
additional agent we can scale our
application we can say we need fig clip
because and then it will start issuing
up workloads initially also like we've
only got three available because image
hasn't downloaded and Hasmik started yep
but you can see these are that mixture
between what we desert we desire and
what we actually have available after a
few moments winters they initialize
you'll see everything deployed
the cube will go from three to six and
our system and our load balancers will
operate across two different versions so
now is our application working what
about deploying new versions and again
as your as you land kubernetes is there
to manage this for us and so in our
system coop could all set image give it
the name of what the deployment we want
to be updated give it the new version of
our image which we want to be deployed
and then kubernetes will do a rolling
update of our application and so it will
make sure that we don't lose capacity we
don't lose any downtime with our
deployment instead it will go to each
pod and it will replace it it will stop
the previous version start the new part
start the new container and then launch
it and make it available in the load
balancer and so as we're doing sending
requests you'll see it being pop it
popped and kind of spread across these
different versions until it's completely
rolled out until seamlessly from the
users point of view but what about
visual registry at the moment we just
said deploy the ones on public images so
where does usual container registry come
in and so we need to first tell it about
some secrets we need to give it some
user name and password so it can go off
to the registry and actually get access
to our images and so kubernetes has all
of the secret management and all of the
security built in until the store in
under that highly secured way using
command line or you think different
parties but in this case we can say
create a secret I'll type docker
registry and this can be our username
and password in order to be able to go
off and pull our images and those
equivalents as I said you can use the
command line or you can do it the
low-level way by default this is all
based on Yammer and you kill you can see
things about how that application is
configured and some of the conventions
with kubernetes have put in place so how
many replicas do we need we're coming
out six but we could go in and change
this up and down was the rolling update
strategy so when we say update our image
town
to that work and how does for that
operate how quickly should we change
each image should be wait a certain
amount of time for the application star
and become available before we move on
to the next one and then we can point it
to our image and so I saw private
registry we can tell it how to access it
using image pool secrets so it knows
which authentication and all which
username and password to use and now
when we deploy this it will go off to
our private registry but work and
exactly the same way that it did before
and kubernetes now allows us to do some
really interesting work and particularly
around things like the evently and
managing our deployments so we've got
this camel which is the definition in
the covers but it allows us to issue and
schedule different types of workloads so
let's say from a monitoring point of
view we have a monitor which needs to be
deployed unto every single agent within
our cluster and so that we can track and
monitor things successfully so now with
kubernetes we can say this is a type of
a demon set a demon set is a container
which deployed unto every single host
within our system until when in a jaw we
said added that new agent the demon set
would have automatically been deployed
and autom automatically been made
available within our system and so we
wouldn't have had to explicitly go in
and configure and bring up certain
workloads it would have just happened
for us based on what kubernetes
is aware of what our desired state
should be and this is where we start
getting some really interesting
approaches and some really interesting
capabilities we also now have windows
containers like we have Linux containers
at the beginning but with sequels
Windows Server 2016 we can run Windows
containers within a dark file when they
get built they'll do things like you can
define dependencies but like Windows
dependences so I is we can install
Windows features like the API on that
framework we can configure how iis works
and operates in this case creating a new
website called new dinner and just like
we did with Linux and Linux containers
we can copy source code in
in this case the merge dinner from May
2010 the traditional asp.net 2 example
application but we can now build it as a
container completely untouched
completely independent using the same
tooling and same approach that build but
now we're targeting Windows and we're
targeting and configuring a Windows
container and windows functionality such
as IAS can be launched and it can be
operated and we have our beautiful AP
done application running ad containers
for more importantly it can be operating
it can be worked in exactly the same way
within a filler we've got another
drop-down say in the operating system
and we can set windows and so now we are
Callisto can start operating and working
in a hybrid fashion they can manage our
new shiny awesome application built with
asp.net 2i if you like cool or elixir or
anything running on Linux in exactly the
same way it operates and manages our
traditional applications running on
things like iOS and asp.net 2.0 and
having a seamless seamless transparent
experience between the two and using the
same tooling the same conventions and
same porches and in order to scale those
up and scale those down and I think
that's a really interesting proposition
about where this can and start heading
and I think this is what we're going to
start seeing with containers as we move
forward we already have sequel server
winning as a container now and so we've
simplified how that looks and how that
feels and how we can operate that and
both in development and production but
what about development time what about
things like visual studio as a container
like why do we download and install
download 5 gig at the installation files
to spend an hour having it set up and
configure itself to then only need a
middle of day or override or interact
with other visual studio installations
already on our system and so you can't
work with things are besides if those
operated as a container that would be
completely independent
they wouldn't interact it with the
registry and we can win different
versions seamlessly side-by-side without
them interacting and when an update disk
applied would simply download the image
download a new layer and that would look
that's when enough get started and we're
now finally starting to see this come
into interaction so this is Eclipse
the new version of Eclipse which are
significantly better than the old
versions and it significantly changed
about how it gets deployed when you go
onto the Eclipse website the getting
started and the installation is docker
run Eclipse the entire IDE and the
entire installation experience is based
around containers and based on how we
can keep things sandbox and isolated
they still win in a really interesting
desired workflow state so just to
summarize we have containers we have
containers which can simplify our
deployment process via a clips or be a
sequel server or something like Redis
we've got this consistent experience
rather how they can be launched how to
Camille and how they can be operated and
so we don't need to configure build
agent anymore because all of that
complexity fits within the docker file
itself and that's where we define what
our application needs and define how our
applications are built once we start
scaling this out we've got toolings
like kubernetes which can help manage
and scale these applications cost
multiple different agents configuring
the load balancers as required making
them available making them secure and
ensuring that if they do crash or if an
OU goes down that workload is
rescheduled somewhere else to make sure
we're not losing availability or
capacity by building this and deploying
it on top of us or we can take advantage
of what a joy is really good at which is
starting and configuring and
bootstrapping
infrastructure and then let kubernetes
and our workflow fit on top of that
without us having to reinvent the wheel
or use differences party technologies
and then when it comes to things like
monitoring kubernetes can manage that
workload we can deploy these demon sets
which had executed across all of our
nodes and so when new things happen and
when new agents are deployed they
automatically fit within our monitoring
system because automatically they have
the right configuration and the right
settings being deployed onto them and so
we can monitor our
squash the entire thing and so is that
I'd like to thank you very much for your
time and for attending I'd love to hear
your feedback about if you're playing
with kubernetes and you get started what
you think
how does it work have it work for you
and if you have any questions please do
feel free to reach out and send me an
email or drop me a treat or catch me at
the drinks later and with that thank you
very much I hope you enjoy the rest of
the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>