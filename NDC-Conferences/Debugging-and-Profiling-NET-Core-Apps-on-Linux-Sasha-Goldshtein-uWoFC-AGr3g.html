<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Debugging and Profiling .NET Core Apps on Linux - Sasha Goldshtein | Coder Coacher - Coaching Coders</title><meta content="Debugging and Profiling .NET Core Apps on Linux - Sasha Goldshtein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Debugging and Profiling .NET Core Apps on Linux - Sasha Goldshtein</b></h2><h5 class="post__date">2017-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uWoFC-AGr3g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is going to be a debugging and
profiling Dartmouth core application
and hopefully some of you have already
looked into using dotnet core at all
we're using dotnet core on Linux
specifically and if you have you know
the story is not quite what it should be
doing dotnet on windows for ages I've
been using that net since 2002 it's it's
got a very large echo system of
different performance tools profilers
production tools there's a lot of stuff
built into Windows itself which has
dotnet in mind or was tailored for
dotnet specifically over time so it's a
lot easier to do this sort of thing on
Windows and getting dotnet ported to a
different platform is not just a matter
of you know running console.writeline
hello world it's also a matter of this
whole ecosystem of tools techniques best
practices for doing performance
investigations for doing debugging for
doing production work and most of my
work actually is performance
investigations and production debugging
stuff for customers I'm a consultant so
that's a large part of my work and I've
been really interested in Linux and
Dartmouth cordis combination and this
talk is kind of a set of practices a set
of tools that work as they tend to say
conference talks are not for really
learning how to do every single thing
but more about what's possible so if you
want there's a bunch of references for
you to explore I've also written a
couple of blog posts about this with
more detailed instructions and some of
this stuff keeps changing so just also
that to bear in mind it's all fairly new
and still being changed by Microsoft and
by the open-source community so my name
is Sasha I work for a training and
consulting company back in Israel it's
been quite a trip
here to Australia but it's I've enjoyed
it really much so far my wife is also
speaking at the conference we've been
we've turned this into a three week
holiday so that was pretty awesome
so again this is a talk on debugging and
profiling dotnet core apps on Linux and
again it's it's it's worth just stopping
for a moment and just sort of admiring
the fact we got here that we can
actually run dotnet core apps on Linux
and we can talk about profiling them and
we can talk about advanced topics
related to dotnet core and Linux this is
pretty incredible so the five things I
would like you to take away is profiling
CPU activity
which is pretty obvious you want to make
sure you're not overloading the CPU and
if you are what is taking my CPU time
which functions which stack traces which
threads we're going to talk about
visualizing these stack traces for
example visualizing CPU activity using
flame graphs which are incredibly
popular on Linux less so in the windows
world although we are getting some
traction but we'll talk about flame
graphs specifically for Linux on data
core applications we'll talk about some
Linux tracing tools which can be used
with dotnet core processors as well
although it takes some tweaking and
we'll talk about the tweaking necessary
to make it work we'll talk about dotnet
core run time events if you're familiar
with CLR etw events on windows things
like garbage collection events
just-in-time compilation events that
sort of thing that you can get out of
the CLR this is also possible than Linux
with again some tweaking and we'll talk
about LT T and G which is the Linux
equivalent for etw that microsoft has
chosen to use and finally it will also
talk about generating core dumps and
again for production use on Windows
hopefully you are familiar with core
dumps or dump files incredibly useful
for any kind of post-mortem
investigations and production crashes
memory leaks a bunch of different
scenarios so these the same techniques
are also applicable to Linux and we'll
talk about getting this stuff out of
qualifier core dump files on Linux as
well so that's the general story I've
told you this already a lot of these
stuff changes I've sort of tested all of
these tools and scripts with Dartmouth
core 1.1 and 2.0 but if something
doesn't work is probably my fault and
you could definitely get in touch a lot
of this relies on scripts I kind of
hacked together so it's not exactly the
recommended way of doing things it's
just sort of the only way of doing
things right now and hopefully some of
these things will improve now just to
draw parallels between different
operating systems this is a Linux talk
so we'll only look at the Linux side of
things but a lot of these things have
equivalents on other platforms so that
meth core obviously runs on Windows it's
a lot easier for it to run on Windows
Windows has for example etw which is
very very powerful instrumentation
framework built into the operating
system which can be used for CPU
profiling it can be used for tracing
interesting static events like garbage
collections JIT compilations assembly
loading that sort of thing so atw
equivalents on linux for example would
be perf and LT t and g and we'll talk
about both of these tools today
there's also another cool thing that you
can do on linux but you can't do on
Windows easily anyway and that's dynamic
tracing we'll look into that as well
just basically choosing any arbitrary
function and tracing its execution
without having to recompile or even
launch your code again so that's
something we can do on linux but not so
easily on Windows we'll talk about dumps
and again there's tools for Windows for
this and there's tools for Linux and Mac
OS and we'll analyze dump files as well
and again there's tools for Windows
including visual studio and there's
slightly less user-friendly tools for
this for other platforms so there's
roughly mostly equivalents for things
we're going to do on all platforms
our focus is Linux again if you want to
talk later about how to do the same
things on Windows for example please
feel free to reach me I'll be happy to
explain and show you examples one final
thing before we get started with
different tools and techniques for this
presentation is the overhead and if
you're used to development time
profilers like the visual studio
profiler or maybe the Intel VT own
parallel studio that sort of tools the
JetBrains tools like dot trace and dot
memory all these development time
profilers usually have an overhead which
is not really acceptable for production
use and I would expect most people using
Dartmouth core on Linux to actually do a
lot of their investigations on
production because your development
environment might actually be Windows
with visual studio but then you want to
put your app in the Linux docker
container and run it there and so a lot
of the investigations we'll be doing
will not be using JetBrains
dot trace on windows it will be actual
work in production on Linux and there
they overhead there's a lot
more considerable so a lot of profilers
will just tell you upfront by running
this profiler we will slow down your app
by two thousand percent and it's sort of
okay for development I guess if it slows
down your app by two thousand percent
not so much again for production it's
not even very good for development time
if the app takes a very long time to
load now or a certain scenario takes
seven hours to go through but that's
sort of acceptable for development not
so much for production so for a lot of
tools we'll talk briefly about overhead
and we'll even have alternatives so this
one has more overhead that one has less
overhead but it's it's got a different
set of requirements which might be
harder for you to satisfy so we'll talk
about overhead as well
now there's two major categories of
tools that we'll be looking at and this
is not Linux specific it's just the
general observation about performance
investigation tools there are sampling
tools and tracing tools some of you
might already be familiar with the terms
just the brief overview sampling is for
events or for things that you want to
monitor which happen in a very high
frequency so you can't possibly record
or inspect every single occurrence of
that thing you're monitoring a good
example of this is CPU instructions in
many cases you want to figure out like
which functions are doing a lot of CPU
work in your program but you can't
record every single CPU instruction this
the processor is executing and then make
a tally of which ones were more more
active it's just going to slow you down
by not 2,000 percent probably two
million percent so instead you do
sampling you look at the CPU execution
in in certain intervals for example
every certain million every million
clock cycles or every couple of
milliseconds and then you make a tally
of which functions or which threads or
which processes were active in many of
those samples so you don't record every
single event
you only take samples once in a while
and then you try to draw conclusions and
interpolate from these samples tracing
on the other hand means you record
everything you look at every single
occurrence of a
an event and then again you analyze
post-processor to draw conclusions so
tracing works for lower frequency events
like disk accesses or garbage
collections that sort of thing which
happens maybe tens or hundreds of times
per second
sampling works for higher frequency
events say millions of events per second
you can no longer record every single
one so that's just a couple of
techniques will be used so dotnet core
on Linux what are the different tracing
components were different moving pieces
that we can put together and get extract
information from so some of these are
not really a dotnet core specific if you
have a Java app running on Linux if you
have a C++ app running on Linux there's
a lot of stuff from the core operating
system that you could already be using
so looking down at the bottom of this
diagram the CPU itself and this is not
even OS specific that's also available
on Windows the CPU itself has a bunch of
performance monitoring events that we
could sample and record and that helps
with identifying heavy CPU consumers it
helps with stuff like cache misses
branch mispredictions various kind of
pipeline events inside the processor so
if you're optimizing your algorithms for
CPU performance the CPU itself has a
bunch of performance information and
Linux makes it very easy with freely
available tools to extract that
information and display it on windows by
the way we have the Intel tools mostly
for this and they're not really free
quite the opposite of free on top of the
CPU we have the operating system kernel
which has a bunch of interesting events
as well there's two kinds of events so
one on linux is called a category of
events is called software events and
these are like low-level OS things such
as page faults so when you access a
memory location which is not in main
memory or core migrations when you have
a thread migrating from one core to
another the scheduler moving a thread
between course so that sort of thing can
be traced on the kernel level there's
also several hundreds of static trace
points in the kernel as well in the
block i/o subsystem so stuff like
writing to
quitting from desk networking events
like a packet being received for a
packet being sent there is events and
the scheduler there's events and the
interrupt processing layer so everywhere
across the Linux kernel there's hundreds
of static trace points that we could
trace and again for our dotnet apps it
might be relevant for our Java apps as
well for Python as well so that's not
really special specific for dotnet core
in userspace again there is a set of
libraries which are not dotnet specific
such as the Lib C library which is the C
runtime C++ runtime the P thread library
a bunch of other libraries provided by
Linux provided by your Linux
distribution which again have some
tracing data which have some tracing
events inside that we can record and
these vary across the different
libraries so Lib C has a lot of those
leap P thread has a lot of those other
libraries might only have a handful but
there's a lot of different things in
these libraries that again we could be
tracing so again drawing the parallel to
Windows that's kind of like etw events
which you might have embedded in user
space which you might have embedded in
Windows itself in the kernel finally
dotnet core itself has two major
components so we have the core CLR which
is written in C++ and that has a bunch
of interesting events which are also
available on Windows it's kind of a
cross-platform eventing implementation
so if you have a garbage collection
started event on linux you also have a
similar event on Windows you also have a
similar event on Mac OS you just have to
use different tools to record them but
the event itself the payload will be the
same so there's a bunch of these events
which we'll look at and the Linux tool
for recording these events is called
ltte and gee that's a decision Microsoft
made so they could have chosen from a
variety of different tracing frameworks
and they chose to use LT T and G for a
bunch of reasons we'll look at LT TNG
later it stands for Linux tracing
toolkit next generation so it's it's a
fairly modern library it's a fairly
modern toolset it's it's pretty nice and
clean record high frequency events using
that and finally your daughter
core application itself can also write
custom events so kind of like log
messages you might be familiar again on
Windows with the event source class
which is part of etw it writes out etw
events so the same class is also
available on linux and you can use it to
omit tracing messages at runtime enable
them disable them record them or just
ignore them whenever you want so that's
kind of the different pieces of tracing
that we can record so the first thing
we're going to be talking about is
recording CPU samples so taking a dotted
quarter plication which burns a lot of
CPU and trying to figure out where which
functions are most active which
functions are taking up lots of CPU time
and the official story for this which I
want to share with you first and that's
not what we're going to be using is a
script is a little bash script not so
little actually it's probably a few
thousand lines of bash called perf
collect and so the official workflow if
you have a dot and core app running on
Linux maybe in a container maybe just on
a bare bone VM the official story would
be the following you download that bash
script which is just a bash script and
then you run the script with the install
argument which installs a bunch of
prerequisites and there is tailored for
different distributions so the script
detects which distribution you're on
downloads the different dependencies
install them that's all good then you
run collection so you run perf collect
collect and that records a bunch of
stuff it basically records everything
there is to record it records CPU
samples so you can do CPU profiling it
also enables and records all the runtime
events so
garbage collections exceptions being
thrown assemblies being loaded methods
being compiled everything is getting
recorded and that produces a zip file
now the workflow gets a little more
shady you take that zip file and you
copy it over to a Windows machine
because you can't analyze it on Linux
using perf glass so you take the zip
file and you copy it over to a Windows
machine then you download perfume which
is really
great tool for Windows it's free it's a
Microsoft developed tool for analyzing
performance traces for recording etw
events really really useful and then you
open the trace in purview and you can
look at the events and you can figure
out which functions are taking lots of
CPU time in your dotnet core on Linux
application now there's several things
wrong with this and hopefully it's not
going to remain the official workflow
for long but that's the story right now
just a couple of things that come to
mind there's a very long turnaround time
here so you have to record this
recording get a zip file copy it over
somewhere else open it there do analysis
maybe fix your code go back to that
machine record a zip file again copy it
over keep doing that the zip files can
easily grow to hundreds of megabytes for
very short periods of time so it's not
something you'd be copying around like
super easily especially if it's a cloud
VM somewhere and the Windows machine is
in your office or that sort of setup so
that's one problem the turnaround time
the second issue is you obviously can do
any real-time monitoring like this right
so you have to make a zip file copy it
over open it you can do real-time
monitoring on the linux box and of
course I mean there's no control over
exactly what gets collected so you
basically say perf collect collect and
it decides for you which events to
record goes ahead and record them
there's some tweaking you can do but
basically it records everything so
that's the official story and they were
not going to be going with the official
story because what the perf collect
script actually does is not that
complicated and we can totally replicate
parts of this ourselves
so we'll be using perf and perf is what
perf collects perf collect users as well
perf is a Linux tool for performance
investigations it's not doesn't core
specific at all it's just the go-to tool
on Linux if you have any kind of
performance investigation to do its part
of the Linux kernel so I mean it's
developed as part of the lunar of the
Linux kernel tree so there's always a
build of perf that matches the build of
the kernel you have and if you upgrade
your kernel you also have to upgrade
perf along side it's available of course
for all distributions because again it's
just part of the kernel so for example
an Ubuntu you install it from Linux
tools common on redhead you'd install it
from the perf package so it's just out
there you have to install it and in some
cases it might even be available out of
the box now perf is a command-line tool
with a gazillion different options it
can record it can display it can process
events it can do a bunch of different
things
one thing we'll be doing is just
recording CPU samples but first did the
general architecture of what proof
events look like so the basic premise is
you have different event sources so for
example here on the left you have some
kernel events like page folds cache
misses contexts which is a bunch of
different things happening and you can
also have some events in user space for
example memory allocations in a in a C
application so all these events if you
enable them go into the perf events
mechanism in the kernel which writes
them out to a file or a shared memory
buffer so there's two options for you to
work with the default which most tools
use that's also what perf collect uses
is to go through that perf dot data file
so you write out every single event
occurrence to a file and then you
analyze that file a little later you
might analyze it on a Windows machine
using per few or you might analyze it on
the same Linux box using perf itself but
you do the recording into a file and
then you analyze it later and that's a
pretty generic architecture for an event
collector DRD the ability to work with
memory buffers rather than files is
pretty important because it means you
can build real time tools so you don't
have to go through a file you can write
events to a cyclic buffer in memory and
then just analyze those and discard them
so you don't have to record everything
for weeks or so so that's the general
architecture so let's see how to use
this thing I was planning to do demos
but I don't have my computer so I do
have screenshots of everything that's a
sort of mock was my backup plan so we're
going to see how to do a perfect and how
to get reports out of it
but we do have to talk about symbolic
information first so if I were doing a
demo I just show you what happens if we
don't take care of it first but here I'm
just gonna have to fall back to the
slides so you know that on Windows as
well
we need symbols we need pdbs right we
need PDB files to do debugging that's
the most common scenario when you
encounter pdbs but for profiling you
need PDP's as well you need them to
figure out the relationship between
addresses and function names in your app
in your source code so the same thing
exactly happens on Linux you need
symbolic debug information you need
debug info files in order to match
addresses to function names so for
dotnet code if we did a proof recording
and then we asked Perth to visualize the
results we'd get a bunch of addresses
like on this slide so all of these great
numbers here this is where our code is
this is the stuff we care about but it's
just shown as addresses and on windows
there is actually a fairly elaborate set
of different moving pieces which work
together to make sure you get symbolic
information for your dotnet code and
Linux it's a little trickier and that's
part of what the perf collect script
takes care of for you so what do you
have to do to get symbolic information
on Linux for the different moving pieces
so on the left you have a call stack of
a dotnet app calling console.writeline
on a Linux box and it goes all the way
from the bottom where there's the thread
initialization stuff going through the
core CLR and then we have the green
frames which is managed code so the main
method and program dot foo which is some
function and that goes through to that
and framework implementation
console.writeline and so on all the way
down to the Linux kernel so each of
these different functions all on the
stack they need symbol information they
need debug information to be resolved to
function names and the source of that
debug information is different for each
of those so for example if we're dealing
with native frames so C or C++ code on
Linux the story is pretty similar to
what happens on
those there's debug information files
which ship alongside with the binary so
you can install them separately or you
can just get them as part of the binary
package itself and these files contain
all the symbolic information you're
going to need to translate addresses to
function names but for managed code the
story is a little different because
managed code gets compiled at runtime
right we have the JIT compiler which is
great so we don't have upfront
information about code addresses it all
gets created when the code gets compiled
at runtime so we need a different set of
information sources and indeed there's
this weird convention on Linux that
whenever you have a JIT compiled runtime
like the JVM nodejs
dotnet core there's this convention
where the runtime would write out
symbolic information for any methods
that are compiled to a text file which
is stored in a certain location by
default it's under the temp directory so
it just goes there and then performance
analysis tools can just pick it up from
there and use it so that's the
convention and Microsoft decided to go
along with the same convention so you
can turn off of a turn on a flag which
we'll show you in a moment and then the
dotnet just-in-time compiler will write
out symbolic information to that text
file which all the performance tools
will be able to pick up so here's what
it looks like just gonna skip for a
moment here to show you the these are
the screenshots I was talking about so
just want to show you the switch we need
to tune so there's this oops there's
this comp last perf map enabled
environment variable which when turned
on will instruct the CLR to write out a
text file with a line for each method
that gets JIT compiled so there's going
to be a line there with the method
address and the method names and perf is
going to be able to pick that up now you
have to turn this on again in advance so
before you launch your application that
environment variable has to be on it
does have a certain overhead because it
means every method getting compiled also
incurs
right to a temp file and the file can
grow pretty big as well but it's in the
temp file system so hopefully not
nothing to worry about but you
definitely need to turn this on if you
plan to do any debugging in production
if you plan to do performance analysis
you just have to turn this thing on in
advance I have a slide at the very end
with a checklist of all the different
things you need to prepare if you plan
to do performance investigations in
production okay so assuming we have
debug information let's finally move on
to getting CPU profiles and we'll
visualize those we'll put them on the
screen using flame graphs which again is
a concept a lot of linux people are
familiar with but on windows it's only
getting traction recently and a flame
graph is a visualization method for
taking a bunch of stock traces a lot of
stock traces like millions of stock
traces and putting them in a single
diagram now if you use the profiler
before you've already worked with stock
traces so you might be used to various
kinds of trees where you expand the tree
to get through the stack trace or maybe
lists of functions or other kinds of
views fan graphs are really really
useful and again even on Windows they're
getting traction now even as part of
Microsoft official tools like the
windows performance analyzer and that
sort of thing so what basically a flame
graph does is put in a single diagram a
bunch of stock traces where each
rectangle and I'll have a bigger example
to show you in a moment each rectangle
you see on the screen is a function the
vertical axis is color Koli
relationships so if a function is on top
of another function it was called by
that function so if a is on top of B it
means that a was called by B the
horizontal axis is just alphabetic
sorting so it's not a time line or
anything it's just alphabetically sorted
the wider something is the more common
it was the more times that function
appeared and that's pretty much it
the colors are usually just randomly
assigned to make it look pretty that's
mostly why we have different colors it
does help sort of differentiate the
different functions
you could also assign colors by some
other metric like maybe c-sharp
functions would be green and C++
functions would be yellow and kernel
functions would be orange or whatever
there's obviously a bunch of stuff you
can do now the diagram itself is usually
just an SVG file and SVG's are just XML
files and xml is text so you can do a
bunch of stuff on that like grep and
zoom and and remove stuff out of it and
filter and that sort of thing so let's
take a look at what the workflow looks
like for dotnet core app on on Linux
again I'm sorry it's screenshots but
it's it's the same workflow exactly it's
except it's not going to be interactive
so on the Left we have set that
environment variable compassed perf map
enabled equals 1 and then we ran our
Dartmouth application giving it a
certain set of things to do so just
running there and printing dots on the
console in the right on the right hand
side I rent Pitt stat which is I went
Pitt stat which is a built-in tool that
shows you CPU usage for a certain
process and it shows me that my CPU
usage for that buggy process is pretty
high so user CPU is time spent in user
space system is time spent in kernel
space and it adds up to sometimes a
hundred percent of a CPU so that's a CPU
intensive app then we run perf record
and perfect chord is an incantation that
tells perf to sample the CPU it's a
certain interval and record each
individual sampling into a perf data
file now the interval is over here at
the end
- F 97 that 97 means 97 times per second
so sample the CPU 97 times per second
and record samples and then you just hit
control C to stop and perf record says
captured and wrote that much data to
perf the data 720 samples were captured
overall you could do this for more or
less depends on your scenario then you
use
perf report which is not actually shown
on the screen shot to get this
illustration here on the right and this
is a coal tree in text format it shows
you which functions called other
functions so if you start from here the
main function called the fetch function
and then the fetch function called the
process result function and so on so you
can read this thing like a tree and it
has percentage points assigned and it's
kind of like exploring in a in a GUI
except if you have a lot of stack traces
if you have a real application you might
have like 5000 screen falls of this to
go through so you don't really want to
work with the text format it's just an
illustration that you can do a lot of
the stuff on the terminal but you really
want to turn this into a a chart so
that's where we use perf script and pass
it through a bunch of scripts that
generate a flame graph again all the
commands are available online and the
presentation as well and that just
generates at the very end in SVG and SVG
fun the application by the way keeps
running in the mean time so we didn't
have to restart it we didn't have to
stop and start it again it's just
running there we record a few samples
and we analyze those samples on the same
machine the only thing we had to do in
advance is this environment variable
because otherwise we wouldn't have
function names like main and fetch and
process result we wouldn't have those so
what does the flame graph look like this
is the actual flame graph generated from
this exercise so again remember the
wider something is the more prominent it
was the more time we spent there so the
more prominent flame seems to be this
one right this one right in the middle
but there's also some other flames like
this one here and this one here which
have a few samples if you hover over the
different rectangles if it wasn't just a
screenshot you'd have a tooltip saying
exactly the percentage points so you
don't have to just trust your eyes now
looking on the right here if you can see
the function names because the zoom
doesn't work either but if you if you
look at the function names this is a
garbage collection thread right the
function names you
we're gonna read that for you GC heap
BGC thread stub BGC is background GC GC
GC heap GC 1 GC scan that sort of thing
this is just a tread doing garbage
collection in the background and that's
that's okay we recorded that of course
this is not c-sharp code that's just C++
code inside the cell are our c-sharp
code is over here so this is program
main and there's a bunch of functions
leading up to that point in the course
alarm but this is our code here program
main program fetch program process
result and then we can see some of the
ways it goes from here so for example
one frame that we have a bunch of set of
functions on is framed allocates trained
and then slow allocate string so I mean
it's not slow deliberately it's just
there's probably a fast path where you
can still allocate from a local buffer
and then there's a slow path where you
can't but regardless we have function
names for our c-sharp code as well as
for the course CLR the Linux libraries
and so on all the debug information is
available and we get a full fidelity
flame graph now again coloring could
maybe help like color the c-sharp code
yellow green whatever that's definitely
something you can do as a
post-processing step so that's just an
illustration of getting a CPU profile of
your app where you're spending lots of
CPU time what kinds of things you were
doing there's nothing super interesting
in this scenario itself is just really
allocating a lot of stuff and in the
process it's also causing lots of
garbage collections to happen in the
background so it's interesting that we
have a full view of what's happening in
the process not just the c-sharp code
every everything using the CPU in that
process
I'm sorry again no no this so this is
this hiraman yeah that's that's core
sealer yeah and this is our C sharp code
it's it's not marked meaningfully right
it doesn't say C sharp or C++ next to it
but you could figure out by the function
names just if you recognize your code
yeah okay so that was CPU profiling we
do have a bunch of other scenarios to go
through so I'm gonna move ahead and I'm
going to talk talk briefly about BPF
which is a different set of tools for
doing performance investigations on
linux which i want to talk to you about
in addition to perf there are some cases
where the BPF approach is gonna be
giving you a lower overhead and will be
more suitable for continuous performance
monitoring so it's worth mentioning even
though perf is kind of the go-to tool
like I just showed it so this just
illustrates the two different approaches
to doing performance work on Linux the
one on top is the perfect roach which
we've been going through so far and the
one on the bottom is something new so
just go through the bar to the top part
first this is what we've been doing so
far we have the perfect mechanism in the
kernel which receives events from
somewhere and writes them out to a text
to a sorry a binary file perf dot data
and then you have some user space
scripts analysis processes visualizers
whatever generating some sort of reports
from that the report could be a bunch of
text the report could be a flame graph
the report could be anything but you go
through this data file you go through a
major major big file on this the other
approach which we'll be talking about
here is the BPF approach where inside
the kernel there's a little analysis
program that we install and run so
rather than putting all our events in a
file and doing analysis after the fact
in userspace
we do analysis as the events arrive in
the kernel using this BPF technology now
I wish
could spend more time talking about BPF
itself BP f stands for Berkeley packet
filters so it sounds like packet
filtering technology and indeed it was
used for packet filters many years ago
started in the early 90s was ported to
Linux from BSD so Berkeley packet
filters but today it can be used for
performance tracing for performance
analysis for performance investigations
as well again the key difference you
have to remember is that with BPF you
can do aggregations you can generate
visualizations charts histograms that
sort of thing right when the events
arrive and not by inspecting a huge file
that has millions or billions of entries
so that's going to be the key difference
and that enables continuous performance
monitoring it enables higher frequency
event collection because you don't have
to store every single event to disk
there's a bunch of things opening up and
indeed if you look at the pipe to user
space the pipe instead of a file becomes
a data structure which is called a map
in BPF terminology and that map is just
exactly the set of data you want to pass
you want to pass through the user space
it's not every single event occurrence
so that's just the general story very
briefly now BPF itself is a kernel
technology which has a bunch of
different front ends available in user
space Python C++ Java go a bunch of
languages that can interact with BPF to
a set of libraries the set of tools I'll
be using is called BCC the BPF compiler
collection it's an open source project
that I've also contributed a couple of
tools to and there's a bunch of people
working on it from Netflix from Facebook
from planned grid from VMware a bunch of
people collaborating on building
gnu/linux performance tools that use BPF
now before we go there just one minor
thing you want to be you might be asking
why would we even use perf anymore right
so if we can instead of going through a
big big file if we can just process the
events in real time
why use peripheral why use what I've
just shown you the in the first 20
minutes and the answer is BPF is fairly
new so BPF is
generally available in Linux kernel 4.1
and onwards which means roughly the last
two and a half years which means if you
have a super old Linux distribution like
cent OS 6 for example or Ubuntu 14 or
that sort of thing that you have to keep
working with you simply don't have
access to new enough kernels to use BPF
if you have a bunch of 16 if you have
Center 7 if you have fairly recent Linux
distros which keep updating with recent
kernels you'll be able to use BPF
probably today if not then maybe in a
couple of months but for older
distributions perf remains the tool of
choice because perf has been available
since linux 2.6 it's just there so
that's roughly why we want to go into
BPF here's a small subset of the tools
we have available today built on top of
BPF there is something for everyone here
if you're doing low-level work on like
filesystem networking that sort of thing
there's a bunch of tools for you if
you're working at higher levels like
data tracing database accesses tracing
trade events tracing garbage collections
there's a bunch of tools for that as
well
for different managed runtimes dotnet
Java Python a bunch of different managed
runtimes out there I'm going to show you
just a couple of examples of what you
can do with with the BPF based tools
some of these things can also be done
with perf but with an higher overhead
and going through a big file that you
have to pay for writing and reading
whenever you do a performance
investigation so just a couple of quick
examples I'm gonna just probably skip
this one here because it's not super
interesting when I show the other one
because we're a little short on time so
this super long invocation here on the
right which I don't expect you to
memorize basically says I want to trace
all the functions inside the core CLR
which have garbage collect in their name
now the core CLR is open source so I
don't really have to guess function
names you can just go to the source and
see what there are but I don't remember
exactly so let's say anything that has
garbage collect in its name I want to
trace
so we're just running this tool when
a.net process is active on Linux I can
get a report that says for each
individual function how many times it
was invoked so this function here was
invoked once this function here was
invoked eleven hundred and seventy times
and so on
now the function names look mangled and
that's just what the C++ compiler does
to your functions if you are if you
compile C++ code it would mangle the
function name slightly there's actually
a simple tool called CPP filled which
would revert this which would give you
the original C++ name back but we didn't
run this in in this example so basically
I can draw conclusions from this even if
I don't kind of like the decorated
format it looks here like a garbage
collection started function and a
garbage collection finished function
both were invoked the very large number
of times it does seem like the started
callback was invoked more times than the
finished callback which is curious but
we're talking about thousands of
invocations anyway so I suspect there's
a lot of garbage collection going on in
this process and this is not by using
the core CLR static events it does have
events for garbage collections it can
emit this is just by looking at
functions arbitrary functions inside the
runtime so that doesn't require any kind
of cooperation from the application that
you're tracing and it's applicable to
any language it's applicable to JVM apps
running on Linux or Python apps running
on Linux or whatever as long as you have
the function name another example of
what we could do is actually get stack
traces invoking a particular function
frequently kind of like CPU profiling
but instead of profiling just arbitrary
CPU work I want to see who's calling a
particular function frequently and so I
can run another tool which is not shown
here and get a stack trace and account
at the bottom here and that means that
this stack trace was calling the
function I'm interested in thirty one
hundred and fifty five times so it's
just a slightly different way of
performance profiling rather than just
looking at arbitrary CPU work there's a
certain function I'm zooming in on and I
want to know who's calling that
particular function and so that gives me
again
I managed stack as well and so these are
functions were familiar with like main
and fetch and process result and they
are causing lots of garbage collections
by doing lots of allocations which is
again not surprising if you recall the
flame graph but we just got there in a
slightly different way so we can use
this for garbage collection tracing for
example now I want to show you the
static events as well and so I'm just
gonna jump over again I've told you in
the beginning that the equivalent for
etw events on Windows the equivalent on
Linux is called LT TNG that's the
framework Microsoft opted to use for
static events coming out of dotnet core
both the core CLR which is the C++ part
and your dotnet code your c-sharp code
that wants to emit run time tracing
events using the event source API so LD
T and G again Linux tracing toolkit next
generation is fairly similar to what
perf does in in spirit and by that I
mean there is a kernel component which
aggregates events from the kernel
there's also a user space component that
aggregates events from user space like
garbage collections events assembly load
events whatever you emit as tracing
events and these events can go to memory
buffers or to files so again kind of
like perf kind of like any performance
tracing framework out there you can
either work with memory buffers which
are limited in space or in addition to
you could also write out to a file on
disk and analyze that later you can do
real-time processing or you can look at
the whole file after the fact with a
bunch of different tracing tools one
advantage this architecture has compared
to perf is that file format is actually
standardized so it's not the random
binder
we perform at its CTF which is the
common trace format and there's a bunch
of different tools from a variety of
vendors that can write and read CTF
files it's kind of a common log binary
format which a lot of tools would
recognize so that that quarry itself
again has a bunch of different events it
can write out through LT TNG so I'm
going to show you again a demo of some
of those so the general process is as
follows for the.net process to emit
events you need to enable yet another
environment variable which is called
comp plus enable event log if you don't
there's not gonna be any events it's
just quite as simple as that
you can't enable just the subset of
events you either turn the whole thing
on or the whole thing off and then the
CLR will emit or not emit any events you
run your application and in the
background you can do whenever you want
you can create a recording of these
events getting emitted so there's a
bunch of commands you have to run for
this which again no one is supposed to
memorize you can obviously script it out
so LT T&amp;amp;G create a trace at context
enable certain events you're interested
in start the recording stop the
recording destroy the session and then
you have a file on disk you can analyze
later now the file again is in binary
format but you can inspect it with a
bunch of different tools the one built
in that just comes with LT T&amp;amp;G if you
install it from your package
distribution is called
babel trace so it reads an arbitrary
common trace format file and just prints
out a text listing of all the events in
that file so you can just see it's a
fairly simple format you can just see
individual events now these aren't super
interesting but let's take a look at
slightly more interesting events so here
I've just used grep to look for
exception thrown events and now you can
see a bunch of lines here each
indicating an exception that was thrown
in that trace recording so they're all
pretty much the same dot at runtime
exception thrown it gives me the process
name
exception type which was HTTP request
exception the exception message I got
and a bunch of other stuff one notable
thing missing is the call stack there is
no call stack right and when you have an
exception event the call stack might be
kind of important and interesting
unfortunately probably the only drawback
that LTTE angie has right now is it
doesn't support stack trace collection
so whenever you have an event you get a
lot of stuff in there like the process
name process ID time stamp and so on you
can't get the whole stack embedded
inside the event automatically and this
is something a lot of other tracing
frameworks actually do support but that
was a conscious decision Microsoft made
the core Solara team made they opted to
use Alta T&amp;amp;G in knowing that they won't
get stuck trace support the LDT energy
team is working on adding stack trace
support that some future may be release
it's an open source project you can also
contribute but right now you can't do
meaningful stack trace collection for
core CLR events so for GC events for
exception events for assembly load
defense you can get the event
information but not the call stack on
Windows you can on windows etw supports
stack trace collection for ages it's
just not something available on linux
and it's really painful I'm only
mentioning this as a drawback because it
is a pretty painful drawback so a couple
of other things you can do with these
trace files and these are just examples
there's obviously a lot more so this one
here is a plot that a guy called Alex
Avery shaggin has created from the GC
events this gives you a plot of your
dotnet memory usage over time so it just
looks at the garbage collection events
in that trace and draws a plot of time
and amount of memory in your dotnet
heaps now if you have ever done this
sort of thing on windows on windows it's
like super trivial to do this with
performance counters it takes five
seconds to get this kind of graph on
Linux unfortunately the CLR doesn't have
proof
counters and there's no such thing as
performance counters on Linux so the
next best thing is getting out those GC
events and analyzing them in real time
to produce a plot so that's a certainly
doable but someone else has to do it
this example over here is just a bunch
of shell pipes that look at all the
allocation events in the runtime and
give you statistics over which types
were allocated more frequently so again
just another kind of thing that you can
do so 39 allocations of system byte
array 20 allocations of system string
and so on these are sampled allocations
so it's not every single allocation that
gets recorded that would be way too
expensive it's just every 100 kilobytes
you get a an allocation event for for
that sample so there's just a couple of
examples of what you can do by analyzing
those recordings of the CLR events that
I wanted to show you so the final thing
I want to discuss with you and we're
yeah it's gonna probably take a little
more than four minutes but I hope to
finish on time it's the very last thing
I wanted to talk to you briefly about is
core dump analysis and again I wanted to
show you a nice demo but I do have
screenshots covering the whole thing so
hopefully core dumps again you're
familiar with a core dump is a snapshot
of a process of a running process on
Windows they're usually not called core
dumps they're usually just called dumps
or mini dumps or dump files on Linux its
core dumps but it's pretty much the same
thing you take the process you suspend
it for a moment you write out the whole
thing to a file and you let the process
keep running and this can be useful for
crash analysis if you have a process
just crashing under you you can record
its state at the moment of the crash and
then let it crash and you can analyze
the crash later or it can be useful for
just live analysis memory leaks
accumulating over time or maybe you have
a deadlock and your process is stuck and
you want to figure out what's happening
so that's where core dumps can be useful
now remember again we're dealing with a
slightly more production angle than
development so you usually can't attach
a debugger in production you don't
usually even have the debugger installed
in production
and core dumps are kind of the next best
thing so how to generate kadam's this is
again not really dotnet specific Linux
has a bunch of knobs that you have to
tune to make sure you get core dumps on
crashes for example there is a file
which is in progress so it's not a real
file it's a system setting that you can
tune to modify word the core file goes
where the dump gets written and if it
gets written in the first place and
there's also a limit that you can set
per session that controls the core file
size and in some environments it might
be set to zero by default so the maximum
size of the dump is zero bytes which
means you won't ever get core dumps so
you do have to tweak those if you do if
you just tweak those two you will get
crash dumps you will get core dumps on
crashes if you want a core dump on
demand there's again a bunch of tools
that you can use the easiest one being
just G core which is part of the gdb
debugger G core just takes a process
attaches to it writes out a dump file
and detaches it's just giving you a dump
file on the map now what do you do with
the dotnet dump file so the only
debugger currently capable of opening
dotnet core dump files on Linux is LL DB
you can't take that core file and open
it in Visual Studio you can take that
core file and open it in GDP the only
debugger that works is LL DB and you
need a specific version of LD P which
matches the version of the core CLR that
you happen to be using currently it's l
DB 3.6 even though l DB 3.9 is now
available but you do have to stick to
3.6 so you open the dump file in l DB
and you'll get a prompt it's a text
based debugger so forget Visual Studio
nice tool windows and then you run the
BT command which stands for back trace
that's supposed to give you a stack
trace of where you crashed for example
here's where you crashed these are the
frames you care about these are the
managed functions which caused the
exceptions if you look at the bottom and
the top of the stack there's function
names here but these are unmanaged C++
functions in core CLR
the managed frames the c-sharp frames
gone now again you might expect a map
file like with the perfect proach a map
file would be somewhere and then you
just feed that map file to the debugger
no LOD B doesn't use the same approach
you don't need a map file what you need
is a plugin which Microsoft provides it
builds alongside course LR itself and
that plugin is called Lib SOS plugin now
if you used windbg on windows you know
there is SOS on windows which is a
plugin which is an extension for windbg
that you can use to do managed analysis
in windbg same thing here it's the same
source code so Microsoft just ported the
plug-in to Linux and it's now called
Libre so as plug-in ur so you load that
into LD B and then you get a bunch of
commands like show me the dotnet stack
show me my objects on the heap
show me the contents of a particular
object or array so you get a bunch of
text commands for analyzing the state of
your process it's just to walk you
through a couple of screenshots just
very quickly here we ran a dotnet
process and it crashed and that's what
it printed to the console so unhandled
exception system application exception
task crashed and the call stack it
doesn't look meaningful it says here
tasks exception holder finalize task
scheduler publish an observed task
exception and then handle and handled
exceptions so you don't really know
where the original exception came from
you might have run into something like
this with your own code if you have
tasks throwing and handled exceptions
but it says here core dumped good so we
have a core file to analyze now opening
that core file in l DB 3.6 here on the
right just prints out a bunch of stuff
which is totally unimportant just ignore
this whole thing it shows you the state
of each single thread but then if you
run bt it shows you the stack we saw on
one of the screenshots the one that has
a bunch of missing frames okay so we go
ahead and load the SOS plug-in and then
we can finally use a command called
print exception which prints the
exception information and the call stock
but this is still the same bad call
stack it's still the same course that we
saw in the terminal which has a task
exception holder finalized and published
an observed task exception but what's
nice is that if we actually look at the
tread call stack the publish unobserved
task exception function and the handle
and handled exceptions function it takes
an event args which has the original
exception inside so it's not super
interesting I'm just gonna skip through
it real quick you can look at the stack
you can extract the object out of there
you can look into the object you can
find the original exception like you
would in Visual Studio in about three
and a half seconds but here it would
take a bunch of spelunking through
through text-based commands so we look
at the stack and we look at the event
args parameter which is over here and
then we ask to print out that object and
that object has an exception property at
this address and then we print that and
that's an aggregate exception object and
it has a list of inner exceptions here
on the bottom at this address
and then we grab that and we print it
out and it's a read-only collection and
the read on the collection has an array
inside called list at this address and
we print that out and that's an array of
exceptions and the only element in that
array is at this address and then we
print the exception and here it is so it
was an invalid operation exception which
happened at this call stack okay and
that's what we were after
so the original exception that caused
the whole sequence of events to unfold
so somewhere in program update async I
had an exception thrown so again just an
example of why you'd actually go through
all these commands because the terminal
output doesn't show you what what you're
after
so that's again some of the things you
might have learned about using windbg
and SOS on windows would certainly apply
here as well and again currently you
can't grab the core file and just open
it and visual studio like you're used to
on windows so for now this is the
workflow we're stuck with so here's a
quick checklist that you'll be able to
take a look at later for what you need
to do to
per your production environment so it
has all the different things like the
perf map enabled variable and event log
variable and the setup you need to do
installations and so on this is things
you need to do in your production
environment so you'll be able to
replicate some of my examples so I hope
to show you in this really quick tour
how to profile CPU activity in dotted
core applications and how to visualize
them using flare graphs we saw a couple
of linux tracing tools that you could
use with dotnet core like perf and the
bcc tools we talked about core doesn't
quarantine events like garbage
collections exceptions getting thrown
and how LT t ng can collect them and we
took a quick look at generating an
opening core dumps as well it's all
super painful right now and it's all
just hacked together from different bits
and pieces I've blogged about this
there's a bunch of references you can
take a look at here the slides are over
there and there's also a bunch of labs
for linux tracing if you're interested
in this I have a bunch of labs on Java
and node and dotnet as well available on
github for you to take a look if you
have any questions we have a long one
hour break now so please feel free to
stay here and just ask whatever you'd
like and if you want to go please go
ahead thank you very much for coming I
hope you enjoy the rest of NDC and sorry</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>