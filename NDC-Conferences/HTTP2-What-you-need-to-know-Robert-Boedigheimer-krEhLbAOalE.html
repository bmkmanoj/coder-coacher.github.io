<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>HTTP/2 : What you need to know - Robert Boedigheimer | Coder Coacher - Coaching Coders</title><meta content="HTTP/2 : What you need to know - Robert Boedigheimer - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>HTTP/2 : What you need to know - Robert Boedigheimer</b></h2><h5 class="post__date">2016-11-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/krEhLbAOalE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right I think we'll get started
and we're going to talk about HTTP to
what you need to know my name is Robert
Bunning hammer that's my Twitter
information I'm a full-time web
developer for an e-commerce site in the
US I'm a Pluralsight author as well I've
got my email Twitter and blog
information I'll show that again at the
end I also have the slides and I don't
have a lot of code for this one but the
slides in the code are online so you can
get a link at the very end ok if you
have questions feel free to ask as we go
we're gonna start with HTTP what they've
now dubbed 0.9 so this from 1991 I gave
you a link to where you can read about
what was there I was created by Sir Tim
berners-lee who calls himself just a web
developer if you at the keynote this
morning it was a text-based request and
response protocol and you could do gets
and you give back get back HTML that's
all that the 0.9 was available to do and
it would close the connection after
every response so his original intent
was really for sharing physics documents
between people at CERN later people
started extending it and adding
different things so they created this
RFC it's actually an informational RFC
it wasn't a standard but they just
people were implementing stuff all over
and it was kind of busy so they wanted
to get something documented so people
had you know if you wanted to implement
it they had something to go look at as a
compilation of best practices they added
the ability to do request and response
headers so we're gonna see that and you
could get any type of response back so
you could start getting images text
files CSS whatever else you needed to
serve that was added in one dodo then
finally in 1999 they came up with HTTP
1.1 some of the big things they added
were persistent connection so they keep
alive for your underlying connection
host headers so we have a lot of
subsidiary companies and it that was a
nice one for us because we used to host
you know 30 internet websites and we
burned an IP address for every single
site so being able to do host headers
just allows you to share
single IP address and the host header
gets sent up from the client and says I
want that website and that's how it
knows which is site I needed to run huge
success right I mean I don't know
anybody if you have any of your software
that's been running you know what 17
years now
so there's things that we don't like
about it but it's been a huge success I
mean all the things you're doing on the
web it's been a very successful protocol
I talk briefly about fiddler I'm going
to show it a couple times but - tracing
tool built specifically for HTTP shows
the complete request in the response
which is why I like it
if you've used like net Mon or Wireshark
you kind of have to reassemble the
packets to figure out what got sent and
what came back so that's why I've been
using fiddler for a long time it's a
proxy it's free Eric Lawrence was the
original author of it and then it was
acquired by telluric in 2012 he went to
work for them full time and he has now
decided to move on to I think he went to
Google like a year ago or so so it's now
telluric product they promised they'll
keep it free so hopefully that will stay
that way one note on it the reason I put
it on here is it's built using dotnet
and so it's dependent on the.net
framework for its functionality one of
the things we won't get real in depth
with this but part of the handshake for
HTTP to over an HTTP connection uses a
LPN which is just a shortcut negotiation
it's not supported by the got net
framework so right now today fiddler
does not support HTTP - so if you're
gonna use it to actually watch traffic
because it's a proxy it'll actually
downgrade you to 1.1 so just be aware of
that I sent to Eric I think a week ago
to see if he's heard anything he said
Microsoft does not put it in the
framework yet so I don't know why if
that affects all of our uses of HTTP
classes don't support HTTP 2 yet which
is disturbing but see you where a couple
of the problems with hdw 1.1 it really
wasn't designed for today's webpages so
if we go out to
if you've been out to HTTP archive they
check a lot of the top sites and just
keep statistics about how the web is
changing and the one I find interesting
since I said I've been doing web
development forever so it just freaks me
out the average web page is about 2.5
Meg now and so they said I don't know
how many weeks ago that that's as big as
doom was if you ever used to play Doom
so that's kind of shocking to me
102 requests so just to see one page
you're making an average hundred two
requests getting back about two and a
Half Men
so that's part of the problem for the
protocols it just wasn't set up to be an
application platform like how we're
using it today right it was designed to
serve documents originally and that was
pretty much it so the four main issues
with it it requires multiple connections
it has what's called head-of-line
blocking which I'll show you
it has a lack of prioritization so out
of all those requests I have to make I
have no way from a browser perspective
or as a developer to specify I want this
to be more important than something else
and lastly it's got very verbose headers
so we'll look at all of these so the
first one that requires multiple
connections and so you making single
active request and response on a one
connection so the original spec said you
can open two connections to each host
right
so if you come to my web site you're
coming back to the same host you can
open up a second connection so you can
basically get two responses at once
right
didn't take browser developers too long
to figure out will cheat will be faster
by adding a few more right so most of
them today do about six connections a
host and I'll show you that when we talk
about head-of-line blocking problem with
that is they use resources for each
connection and if you know how a CP
works TCP has a three-way handshake
every time it needs to establish a
connection and it also has what's called
tcp slow-start which this means it
starts not allowing you to send a lot
and then if it's got a good line it'll
slowly ramp up so the bad news is you're
not taking full advantage of these
connections
and you have to go through that hassle
on six different connections to that
host okay so that's one of the main
problems second problem is called
head-of-line blocking and all this means
is once I have a single connection and I
issue a request on it I can't do
anything else with that connection and
until that response comes back so
effectively if I needed to get 50
different files from my one host I can
go in blocks of six because I send out a
request and if that request happens to
be responding really slowly everybody
else stacks up behind that one that's
what's called head-of-line blocking so
I've got a demo of how that would work
so I've got a page just HTML if I can
get it open here I've got a bunch of
different image requests I added this in
here for a freebie for you I'm using is
I wrote a delay module which just does
asleep on each request and it's helpful
because you can actually target when
you're developing and say if this
request were slowed by 10 seconds or 15
seconds what happens to my page right so
you can also use it put it all in on
like Thursday and your site will be
really slow and then you can tell your
boss you worked all weekend and you
fixed it
I'm using it here because I want to show
because I know there's six connections I
want to show that as I make all these
requests ideally the browser could just
issue all of the image requests and get
them all back you'll see the first ones
are actually delayed by 10 seconds so
we'll see what that does
I'm going to open up fiddler here
because I just want to show you what the
head of line blocking looks like
of course I already ran it so let me run
it again so you can see here in Fiddler
that these six requests are pending you
can see the minus one and there's no
status yet and you'll notice besides all
the other garbage that chrome is doing
for me here the other images you finally
get the other three images get requested
right here that's because again you have
six connections and if those are not
responding you're stuck and you can't
issue any more requests so that's what's
called head-of-line blocking they tried
to work around that if you're familiar
with they try to HTTP pipelining which
was a way that you could send out
multiple requests on a single connection
and it turned out to be extremely buggy
and nobody actually has it implemented
right now the modern browsers all
disable that so the pipelining turned
out not to be a good answer so we'll
talk about how that gets solved there's
a lack of prioritization so there's no
direct way for me to specify the desired
order of responses so browsers need to
basically decide it's a guessing game
and they say they keep changing what
they want to do because it's it's a game
of how they're gonna win they know they
have six connections so they look and
they're parsing your HTML and they see a
new resource they want to get and they
try to decide this is an image I'd
really rather get CSS first maybe a
stylesheet will be coming up next so
maybe I should just hold that connection
and not issue it yet hoping I can use it
for CSS so they play all these games
just because there's no no
prioritization built into 1.1 talk about
verbose headers
there's no header compression so even
though you turn out in compression which
I recommend you do for your web sites we
cut our bandwidth in half just by
turning on HTTP compression so do that
however the header portion of the
requests in HTTP 1.1 are not actually
compressed so you end up seeing
especially I highlighted the cookie here
the user agent you send up 50 requests
back to the same host that stuff gets
repeated with every single request
because HTTP is stateless by default so
it has to reissue all those things over
and over so that's the two problems
really
you keep sending it with every request
and you don't compress it so those are
kind of the problems we'll talk about
this a little bit bandwidth I went to
the hardware store to make this
beautiful picture for everything they
were wondering why I was laying pipes on
the floor but it's relatively easy to
add more bandwidth right get a bigger
pipe spent a little bit more money
that's something that's reasonable to do
latency however is actually based on
distance and speed of light and
everything else there's not a lot that
we can do generally to fix latency
problems okay so as we talk about it
since it's hard to remove the focus is
going to be to try to avoid latency
especially when you get into mobile and
you're using mobile devices the latency
is a lot higher typically so you want to
be able to deal with that in a better
way so I love how things change on the
Internet supposedly the lure is two
people at Google we're debating this
which one's more important latency or
bandwidth they must add a bar bed or
something and they went out and said
we're gonna do a controlled test of
changing the bandwidth but leaving the
latency the same and then leaving the
bandwidth the same and changing the
latency and let's see which one of these
has more impact on the average website
okay so the average websites the
important part of this they ran it
against a lot of the top web sites and
this is what they came up with for their
graph so if you notice as I add
bandwidth it helps for a while and then
once I get to about five megabits per
second it really plateaus you're really
not getting much more improvement on the
average web page by adding more
bandwidth obviously if I'm talking
Netflix or streaming video or something
more bandwidth will help but the hundred
requests kind of scenario that's normal
for most web pages they don't benefit a
lot from bandwidth latency however if
you notice as they reduce the latency
they pretty much got a linear
improvement by fixing the latency
problem so that's why they and I've got
a link down here we can go read about
that study and how it worked but the
important thing was that really changed
their mind when they were about to start
talking about how do we do the HTTP
protocol differently they wanted to
really focus on how do we eliminate
or cutout latency we can't fix it but we
can try to avoid it that make sense
so they started Google started speedy
back in about 2009 it was meant to be
experimental it was not something that
they wanted to become kind of a de facto
standard they said we think there's some
things we could tweak so we want to do
speedy we're gonna let people use it so
you know other people outside of Google
started to use it with server
implemented implementations and clients
but it modifies how the requests and
responses were sent over the wire they
didn't want to change the semantics of
HTTP they said if we change the rules of
HTTP like to get the post the status
codes it's going to take forever to roll
out something new so we're gonna focus
on just the plumbing of how do we send
this stuff over the network without
altering the other stuff they happen to
require HTTPS and so there's a big
movement for you know HTTPS Everywhere
part of that was their philosophy but
the thing they really noticed when they
turned speedy on they would I think is
20 or 30 percent of their traffic would
get screwed up by what they called
middleboxes proxy servers and the other
thing is in the chain that looked at
speedy and said that kinda looks like
HTTP so I'm gonna tweak this or I'm
going to cheat and so literally twenty
or thirty percent of the people that
tried to connect to a speedy server
ended up failing so they basically said
we need to tunnel it through HTTPS just
so these middle boxes don't play with it
so that they just made that a rule that
we're going to require that because
otherwise you fail their features where
they wanted a single connection so
rather than having multiple connections
per host they wanted a single connection
they wanted to get header compression
they wanted to add request
prioritization and then lastly they
wanted to add the ability to do server
push which we'll talk about so as I said
they did that experimentally on the side
then the IETF said we wanted to start up
HTTP two and improve the protocol so
they started their own working group in
2012 it ended up they did an RFP they
got different recommendations in speed
II came in as what they wanted to base a
gp2 on COC http/2 basically took where
speedy was and then made slight
modifications to it to become http/2 so
that's kind of the history now that HTTP
2 was standardized May of 2015 I gave
you the two links I actually printed out
I've read them before but I printed I I
had a 15 hour flight to get here so I
thought I wouldn't actually read it but
if you want to see the details it really
is not that big I mean you've got both
of the specs here the bottom one is the
header compression the other one is HDPE
- they're really not that long as a web
developer reading that and understanding
I've gotten a lot of answers by looking
at the RFC's before that I just wouldn't
have been familiar with otherwise now
that HTTP is - a standardized google has
actually their deprecating speedy so i
think they just shut it off like May 26
for Chrome where it's no longer like the
client for Chrome won't support speedy
anymore they're just they want people to
move away their intention was not to be
a standard they want people to adopt
HTTP - now let's go to http - goals
you'll see they're very similar to what
we saw for speedy they really want to
minimize the impact of latency avoid
head-of-line blocking use a single
connection I put per host because it's
not meaning that you'll have one
connection to all the places you go it's
just one per host and again they said
we're gonna keep the HTTP 1.1 semantics
the same and that's an important choice
and people have come back and said why
didn't you fix this and why didn't you
fix that we're doing this first it's
difficult to change core things that are
built in to all the servers and the
clients that we run and so they said we
won't be effective if we do that so they
chose to leave all that stuff alone and
the good news of that is you don't need
to change your application code to run
effectively switching from one one to
two you can pick up your whole site drop
it over there it's going to run it's
going to run pretty decent with a caveat
that we've done some tweaks best
practices hacks whatever you want to
call them to deal with HTTP 1.1 that you
probably want to dial some of those
I'm gonna go into detail on those
towards the end of the talk so we'll
talk about the major features of http/2
first of all it's now binary it's not a
text-based protocol anymore okay
bothersome people were they like oh I
used to be able to use telnet and I
could tell that to a server and I could
read it did any of you ever do that the
way it is now with HTTP and compression
and everything else I need a tool to
effectively read the traffic anyways I
mean I've been using fiddler four years
or Wireshark whatever is your favorite I
need a tool anyways it's not a big deal
it's a lot easier for people to
implement the protocol using binary than
it was tech space there are just so many
edge cases with the tech space they said
we're just switching the binary we'll
talk about streams and how you do
prioritization and dependencies we'll
talk about a fully multiplexed single
TCP connection we'll talk about header
compression and then lastly I put server
push in italics cuz it's the one that
kind of freaks me out the most I'm not
quite there yet it sounds great but I
haven't seen I'm gonna use it exactly so
I just wanted that's the more
experimental wing we'll call it of HTTP
too at the moment okay so the first one
again it was previously text-based now
it's binary there's like nine different
frame types they're all in the spec if
you want to get to that level and look
at all the different frames and how they
work and stuff go ahead and read it in
there the two that you'll see the most
are the header and the data which is
just the request or response header and
the actual data payload those are the
two frames you'll see the most will
mention a few other later talk about
streams so this is how they are dealing
with priorities so a stream in the term
of HTTP two is a single request and
response yeah you're gonna have a
bi-directional series of frames for your
stream and the order of the frames is
significant each stream is given an
integer identifier you can use what's
now called client priority hints and so
what I've drive got kind of a tree here
I can say here's my root request
let's say I want to make sure I have a
certain file first let's say I want my
CSS file first the browser can say
that's the highest priority until that's
actually downloaded use all the
resources you can to get that first then
after that's done I want you to get
these other two resources not only can
they make them dependent on their parent
but they can also do waiting so you can
see that out of these two B is twice as
important as E and then after B is done
you've got same thing DNA so the
browser's have started implementing and
they've been changing a lot I mean
they're learning as they're doing this
where things are slow so they come and
tweak with this is pretty much a browser
developer thing right they're deciding
for us what is most important here and
how can I specify these in a way that
hopefully will maximize our connection
and get the page to render in the
fastest way okay so again this is part
of the prioritization we didn't have
before what's cool about is it can be
updated at any point okay so a couple
notes and some cool things that they're
not doing all of these yet but how many
people have heard progressive jpgs
before a few yeah like so it was cool in
the modem years you know when I had
black hair still it was a hot thing but
with a progressive jpg instead of
drawing like a if you've ever seen on a
slow connection how you get one line at
a time for a jpg a progressive will
bring it in the whole image fuzzy and
then improve the quality so what they're
toying with is if if we switch to
progressive jpgs which I want to do on
our site once I can get that work
allocated I want to switch over to all
progressive it'd be nice browsers have
been talking about why don't we do the
first request get the fuzzy part of all
of the images then deprioritize that
stream to let the other stuff come in
first and then we'll slowly ask for the
the rest of each image that's about you
know in the fold so to speak and then
we'll ask for the rest they could be
that complicated if they want to be if
you use progressive jpgs the other thing
they could be is you could be Midway
downloading a page and you suddenly hit
a different
on your browser it could D prioritize
all the requests that you had on the one
tab and up the one on the other tab so I
mean these are all things they're
they're learning they're playing with
their algorithms and stuff but this is
stuff you just couldn't do before okay I
think by far the biggest change is this
having a single TCP connection solves a
lot of problems so like I said in the
past you'd have just six connections
you'd have head of line blocking you'd
have to choose which one of these do I
want to put out now now what happens
instead is I have a single connection so
I can issue a whole bunch of requests to
the server and I'm just showing you that
it just interleaves those streams on
that single connection so you can't have
head of line blocking anymore because
let's say the data for seven is taking
forever you don't care because it slips
in your data for stream three it's just
able to multiplex it's supposed to do it
obviously based on the priorities right
and what the when we talk about
priorities if it turns out the server
supposed to get a certain resource like
C but sees just not happening for some
reason on the server the server does not
have to do what you're telling it it's a
hint if C's getting delayed too long
maybe it'll start working on e and B so
it'll start interleaving those streams
on that single connection so the all the
advantages of this a lot less resources
the three-way handshake and all the
stuff you have to do to create a
connection isn't there six times it's
there once it's going to be more
efficient more quickly you're gonna get
through that tcp slow-start and again to
me the biggest important thing is the
head of line blocking is just totally
gone
so whatever is available and what the
browser asked to be prioritized will
start getting multiplexed on that single
connection I said to me that's the
biggest change that they've made I'll
talk about header compression they
originally started with doing
compression using gzip like they did for
everything else and they ended up having
some I think it was called crime there
are a couple different compression
related security flaws
with using that particular compression
algorithm they actually had issues where
they could find stuff out about the rest
of your connection so they had to back
up and say we can't use that we're gonna
create a whole new header scheme for
compression from scratch and that's what
H pack is and that's this other RFC so
the two things they're trying to solve
why do we keep sending the same data for
every single request to the same host
let's add a little bit of state into
this connection so we don't have to
transmit the user agent 50 times right
the user agents not changing between the
other 49 requests and then the other
part was they wanted to do compression
so I showed you part of the table they
have a static table where they say
instead of saying get let's say 2
instead of saying you know index.html so
they basically went out and scoured the
web and said what are the most common
methods used statuses files let's make
those be just a single number index into
a static table so you don't have to send
long stuff anymore and then they also
have the idea of a dynamic table they
know there's headers that aren't going
to hit this particular table so they
want to be able to go and add your own
dynamic stuff so I'll show you the what
it would look like so here's your what
do you day right I'm doing a get I'm
doing HTTP I want the home page here's
my user agent I want to do compression
down here I happen to implement my own
custom headers right I can do that on
HTTP 1.1 I'm just sending something from
the client what this gets converted into
for each pack get is just two HTTP is 6
asking for the home page is for user
agent is 58 but this value we know is
going to change right the user agent the
actual value is going to be a lot
different for all the different user
agents that we have so they just
compress that you like my compressed
version cool
anyways if I want gzip I say sixteen and
then you keep going through this list so
the idea is I'm sending a lot less now
on my next request over that same
connection I'm not gonna resend things
that didn't change okay so like the
user-agent is the same I'm not even
going to bother to send either one of
those two it's gonna use what it had
from my stateful connection before if I
need to override that for some reason
it'll go and take the new value and send
that to the server that makes sense
so again I think I have and they do have
some limits on the dynamic size so
they've thought through people attacking
it and trying to overwhelm your server
by sending it a bunch of headers and
stuff there's only a certain amount but
again out in the RFC they have the full
definition for what is the static table
I think it goes up to sixty one of the
different common known values that most
people are going to send and then they
talk about the dynamic table and how
they do their compression and everything
else so again just the big advantage of
not having to send that stuff over and
over
talk about server push it's the idea
what server push is if if you request my
home page I'm gonna give you the HTML as
soon as you get it you parse it you're
gonna turn out and ask me for the CSS
file the JavaScript file all that stuff
right the idea with server pushes if I
had a smarter server I could know what
you need next and I could start sending
it with the first response down your
connection okay so again you come to my
home page I always have site dot CSS or
whatever and there's different ways the
server figures it out which is kind of
the mystery part of to me and the part
that's a little experimental today yet
some servers I've seen actually watch
the refer and they keep track of every
time somebody asks for this HTML page
they keep asking for the CSS and
JavaScript some servers do that
automatically
others no I don't think I've the thing
in here I've got in the actual code
asp.net 4.6 as what's called a push
promise so as a developer I can actually
influence which resources do I want to
try to push to a client okay so like I
said the idea with it is because the
HTML request came up rather than have a
whole new round trip this is part of
that latency avoidance right rather than
wait for it to get down to you and parse
it and for you to come back and ask for
the CSS I'm just gonna start to push it
to you once you start pushing it so it's
same origin restrictions I can only push
stuff that's from the same host they
call it better inlining so if you've
been watching a lot of web performance
lately some people are saying you should
embed the top part of your CSS file into
your HTML right or you should take
images and make them into data URIs and
shove them into your HTML if you're
familiar with that doing in lining the
problem of that is when you inline that
stuff it's not cacheable right it's
mixed with your HTML it's not shareable
because now I'm putting it into every
one of these files so they really call
server push a better way to do inlining
okay I'm gonna push you some stuff but
rather than shove it into the HTML I'm
gonna push it down the pipe if you don't
want it you can reject it you can send
as soon as you start seeing it you can
send a reset stream I don't you know I
already have that cached in my browser I
don't need that so that's where I get
kind of iffy on it right the game I do a
lot of browser caching and stuff I use
expirations versioning all that kind of
stuff
I'm not sure how much I want my server
trying to figure out and pushing stuff
if I already know I'm keeping it in your
browser cache I'm actually wasting
bandwidth if the server starts pushing
it so this is the area that still people
are trying to figure stuff out you know
how are we gonna use this do how would I
influence it like I said I can actually
and I think I have the code in here if I
still I do not
but it's a push promise and it's part of
the I wanted to see if I had on my notes
I don't it's built in days p.net 4.6 so
if I do want to take advantage of it
literally as a developer in asp.net I
can say I do want to push this to you
and and I'm controlling that rather than
letting servers do that for me okay like
I said this is the experimental part I
move affion there's a big fight about
whether they should require HTTPS or not
and so within the actual standards group
they went back and forth and they didn't
get anywhere so they finally said we are
not going to require it as part of the
spec so that in the RFC it does not say
you have to use it does say if you're
going to use HTTPS you need to use at
least TLS 1.2 and they have a blacklist
right in the RFC of these are ciphers
you can't use with HTTP to tweak ciphers
that they don't want in play anymore so
that's kind of how they dealt with it
have you ever done standards within your
own company like what should where
should the braces go I've been in those
that's not fun I can't imagine being in
like a worldwide standard over this I I
can see where they get to the point
where they said we're not we're not
gonna require it we're done argue about
let's move on
however the browser vendors said we are
so Chrome all of them all the major
browser vendors at this point they've
come out and said we won't implement
HTTP 2 over non HTTPS so even though the
spec talks about how you can upgrade and
do it they're basically saying I don't
care
so effectively for me as we run an
e-commerce site with all sorts of
browsers I'm gonna have to convert to
HTTPS I'll talk about some of my
strategy at the end I won't have a
choice because the browser vendors are
making that choice for me if I want to
use HTTP to claims aren't gonna work
unless they have HTTPS and again there's
two reasons they avoid problems with
middleboxes and proxies that they learn
from using speedy
and also just to elevate general
security on the web
talk about browser support so if you've
been out - can I use before anybody
familiar with this not too many I went -
can I sue once you can't you might not
win but you can definitely see you in
the US so I can come out here and what
they do is they track how well
implemented the css3 and html5 features
are so I can come out here and type in
this case HTTP - oops and it's gonna
show me current statistics on how well
it's implemented across all the
different browsers so it's really got
pretty good support already from major
browsers talk about some implementation
things so there's actually a link if you
go to that link you can see a list of
all the products you probably use and
they're the ones that currently support
HTTP - so you can go out there and say
like we use f5 load-balancing for
instance I can see that their product
now supports HTTP - there's also for me
IIS 10 so I'm an asp.net developer I is
10 now supports HTTP - so I'm running
Windows 10 on my own machine here I can
use HTTP - if I want to do it on a
server that's gonna be Windows Server
2016 I don't know about your ops groups
but I won't see that for years so
indicators so you can actually go get
for both Chrome and Firefox if you're
interested you can get some extensions
you can install which tell you what
server support is so if I go out to for
instance Facebook I didn't mention it
major websites have been running speedy
for a really long time so one of the
advantages of HTTP 2 is as its come out
as a protocol it really has been really
well supported and tested before it even
came out because a lot of the core stuff
was done in speedy so what these
indicators do if you go look at right
here I can hover and it'll tell me
Facebook is already
converted http/2 thank God and look at
some others I think WordPress switched
over not long ago so since chrome
supports it it's gonna show me that so
if you want to go see what a particular
server is running that's a quick way to
see you know what version are they
running and are they doing that or are
they still doing speedy so talking about
some expectations for HTV - isn't the
magic web performance pixie-dust
right you can't just drop it and expect
to decrease by 50% this is Mark
Nottingham who was the head of the HTTP
working group he basically said you're
not just gonna switch to two and
instantly be 50% faster to see your
expectations are set it should help the
most when you're in high latency
networks with lots of requests to the
same host right if you think about
here's the advantages of the protocol or
avoiding latency and if you make a lot
of requests to the same host you now
don't have headline blocking issues
everything's multiplex on a single
connection that's the sweet spot where
if your site runs in those sorts of
environments you're more likely to get
better performance potshot from what
some people have said about 5 to 15
percent performance improvement I've
actually seen some major implementations
have been putting out lately about how
they've been doing and some have
actually been getting worse right so the
things I claim and tell you here always
take with a grain of salt the
recommendations are the standard you
should go that direction but don't do it
without testing because there were some
sites that had tons of images on them
really large images and they switched
over to HTTP 2 and it actually got
slower for them so they had to back that
out and they have a whole article about
why is that and they did a lot of
research to figure out which piece of it
was causing the problem but it's just
important when you're gonna switch to no
test your stuff right see if it's
getting better or not
so now let's talk about all the
performance techniques that I've been
preaching for years that you don't want
to do anymore probably okay so the first
one is we bundle JavaScript and CSS
files we did that couple years ago so we
minify and we bundle them all so all of
my JavaScript files get bundled into a
single minified javascript file I put a
version number on it and I tell it to
keep it in your browser for a year that
way you come to my site once first page
you get everything you need for
JavaScript you never have to ask for
JavaScript again and I compress it okay
so that's good I'll give you one little
warning on CSS files we bundle all of
ours work great about a month later we
added a whole new piece to our site and
worked everywhere but I so we're like
why why does this work in IE
turns out IE only lets you have I think
it's 4096 unique selectors per CSS file
yeah I just crossed that that's all
thank you very much so be aware that if
you do have a lot of CSS you so we
actually have two separate CSS files now
that we've done our redesign we went to
sass and cut down our CSS a lot so we
don't have that issue but when you're
about to bundle be aware that so like I
said I cut 50% of our home page
performance as 50% faster just by doing
that took a couple hours so it was a
great idea in one that one we'll talk
about now the penalty for having
multiple requests is not as high as it
used to be because you don't have that
waiting for a TCP connection
head-of-line blocking having a single
multiplex connection really reduces the
impact on you of making multiple
requests because there's bad things
about bundling right i bundled all my
stuff together somebody says well I got
this one problem in this function in
JavaScript we got to change it great you
know now change it you have to download
a whole new bundle just because I made
that one change I can't individually
cache things like I would really like to
that's really an artifact of something
we're working around that 1.1 didn't do
very well yeah I've seen some people and
recent implementations saying don't go
all the way over to the other
and do 200 or 500 separate javascript
files i meanthere's don't do one don't
do 500 separate figure out for yourself
that good sweet spot for how many you
should minimize and combine but it's not
an obvious choice anymore like it used
to be because you don't have that impact
anymore
for the same reason don't do CSS sprites
if you've seen sprites before you take
all your small images bundle them into
one image and you index in using CSS so
we did that on our home page we cut like
half the images we had they were in a
sprite it's a single request great again
there's not a request penalty so the the
pain of doing them you know they're not
super hard but they're also not
intuitive and things that you just want
to do for the fun of it right I don't
want a bundle and do sprites if I don't
need to so that's something you want to
look at avoiding domain sharding is
another one so today you knew you've had
six connections per host so if I had CDN
comm let's say that's my domain name I
would create CDN one com CDN to calm now
I've got 18 connections Wow great right
so I'll just keep making and there's
trade-offs because every name I make you
have to do DNS resolution and and so
they kind of did research and said the
sweet spot for most people is two or
three different domain names seems to
work pretty well I actually tested it
for us for our site it would have been
about 25 percent faster but it's pretty
entertaining getting web developers to
do what you want them to do right from a
standard standpoint and once you do
domain sharding let's say I took logo
gif and stuck it on cbn.com and another
developer on a different page to use CDN
to calm everything I did about caching
you know keeping it in your browser
domain sure just went out the door
because it's sitting in the browser
cache and he's making you go get it
again because he used a different name
so I just decided we never did it
because of that hassle but again now
that I have a single multiplex
connection I'm not gonna benefit as much
by splitting across I'll probably be
worse off doing domain sharding because
I don't get a lot of advantages and yet
I'm paying
the DNS lookup now two three four or
five times so they recommend not doing
that lastly is inlining so doing data
URI is putting your CSS and JavaScript
directly into your HTML to save that
extra request not being made today
once server push has proved out to be a
great idea I'll consider I don't do
inlining anyways but if I did I'm gonna
wait to see that server push works out
like I'm hoping it will that it's useful
before I tell people to stop doing this
so at least test that if you want to do
it on your web server and see if that
works for you or not but that's another
thing you could avoid when you have HTTP
to things you should continue doing that
you should have always done for web
performance there's really three golden
rules right make fewer HTTP requests I
put that in italics because now that's
not as big of a penalty but I leave it
on there because don't make requests you
don't need right I mean we we get to put
a lot of third-party tags and our web
pages and stuff I'd really like less of
those I just I don't want requests I
don't need to make send as little as
possible so use compression minify you
know sending less down any pipe that's
gonna do faster so keep doing that send
it as infrequently as possible if you
can use the browser caches there's
nothing faster than the browser cache
the user already has your stuff in right
it doesn't use any of their data plan
doesn't use any my server bandwidth it
puts no load on my server use that and
take advantage of that so I listed the
four things you should still do still
minify you know you probably not gonna
bunt
bundle but you should minify absolutely
turn on compression I mean this is I
think IAS has had it standard for static
files for the last I don't know three or
four versions turn it we turn it on for
dynamic to so every one of our asp.net
pages we do the compression and send it
to the client it's worth it because some
of our pages turn out to be kind of big
some people say well that has a CPU
impact they've done some measurements
you know two to five percent if you're
within two to five percent of having a
problem with CPU any website
right you got other problems so I
recommend just turning that on doing
expirations is just setting and saying
this folder I'm gonna control that if
you ask so for instance we do this for
our bundled files today we say it's good
for a year because I have a version
number on it so it'll actually put it in
your browser cache and say don't even
come and ask me for the next year if the
HTML still references that same file
with the same version use it from the
browser cache if it's not there it'll
come and get it people freak out a
little with expirations and say well
what's going to happen if the file
changes right like so I have a where
food site right I've had a picture of
unbrided chicken breast fillets for five
years in marketing says we need a new
picture you know sales are gonna go like
somebody gets the old chicken picture
for 29 more days so take advantage of it
if you're forced to do it just change
the filename so if it used to be logo
dot gif name it logo one dot gif now
nobody will have that never our cash and
you get the best of both worlds so as
long as you have control of the name
that's why you see a lot of the files
have version names built in now to get
around that kind of stuff but you should
do that lastly having a CDN content
delivery network we did this a long time
ago because we were limited on bandwidth
into our data center so we actually
offloaded all of our images so now all
of our static files get served from a
CDN so a CDN is just they have servers
spread all over the US and all over the
world that are closer to my customers
than my data center is so you get the
asp.net HTML from me
but I'm referencing all the rest of the
files from the CDN so it calculates the
closest CDN server to where you are if
I'm lucky somebody's already been to our
site all that stuff is sitting there
otherwise it comes back to our server
once keeps it now everybody else who
comes to my site will take advantage of
the CDN so it saved us a lot of
bandwidth and it's like it was 25%
faster for us and these days they're
dirt cheap because they're all competing
with each other to get your business
it's easy to just RFP and get your costs
down having a couple years so those are
still worth doing
so some strategy from my perspective of
what I can do in the short term anyways
I'm gonna continue to use a CDN because
I want to avoid latency a lot of major
CDN vendors have now implemented HTTP
too so you can go out to that list I
gave you and see if your CDN you're
looking at supports it or not all my
static resources sit on a CDN they're
minified today they're bundled we use
HTTPS with our CDN they're capable of
doing that they'll host your
certificates and serve images and stuff
on secure pages that works just fine
so for me my strategy the best case for
me if I looked across Tower who uses our
site and I haven't done this yet but
based on other people I would say
probably 60 to 75% of my users probably
support HTTP too so I can make a choice
right now today and just say I'm gonna
stop bundling you know I'm gonna stop
doing some of the stuff I'm doing to
make the HTTP 2 better by 5% well
potentially hurting my HTTP 1 people by
50% right so that's not a choice I'm
gonna make what you can do and I gave
you just an example is I made a request
to my web server the web server served
HTTP 1.1 because I was doing it on port
80 I wasn't doing SSL I could just
change all of my CDN references to use
HTTPS right I don't care about if I use
HTTP - between the browser and my data
center when all I get is my asp.net page
it's a single request you know I'm not
gonna roll out Windows Server 2016
anytime soon and I really don't care
because it's not optimized to fix that
one request problem right it's meant
when you have multiple requests however
if I point to all my resources using
HTTP 2 and the CDN supports it then
those guys could all take advantage of
that if that makes sense and then I
could do so I'm trying to decide in this
middle world keep minifying and bundling
and also have another version of it that
lets me point to use just HTTP - that's
kind of where I'm at trying to decide is
that worth the effort or not based on
how many customers I have and who I want
have it be faster for so again you can
detect protocol version so some of the
things we talked about for detection if
I turned on HTTP to my load balancer
currently supports HTTP - even though my
web servers don't so I could add when
your request comes to my load balancer
it can negotiate HTTP - I could add a
header then into the request the load
balancer sends on to the web server to
say this guy can do HTTP - then I'll
only use all the resources using HTTP
and they'll get everything fast using
HTTP - first as I just leave everything
else alone and do bundling and
everything else and still support the
people that do HTTP 1.1 that makes sense
so it's kind of a migration to make
everybody happy and then hopefully and
what four or five years when everybody's
using HTTP - and my server supports it
in my production farm then I can just
switch it and get rid of all this
bundling and minification and everything
else so quick summary it really is ready
for production I mean because it got you
so much with speedy and if you go out if
you get that indicator and hit major
websites Twitter's Facebook's all
they're all using HTTP - I mean they're
using it in production for a long time
some of the features again binary
streams I really think number three is
the biggest win being fully multiplexed
on a single connection really changes
the game around how we should approach
things why you don't want to bundle
anymore of those kind of things
header compressions gonna make a
difference and again I'm kind of waiting
on server push to see how I feel about
that some good resources so you can see
different links the MN ot is mark
Nottingham's so if you want to follow
what he does for being the chair of HTTP
- you know the ilya grigorik works for
google does a lot of web performance
work he has a free book on HTTP - so if
you want to read his book
high-performance browser networking is
just good in general but he has a free
chapter with that second link that goes
into more detail about the HTTP - stuff
that's really good
if you're still gonna be doing 1.1 for a
while this happens to be a really good
book on on 1.1 that's email Twitter blog
that's the link where I have the slides
and the code again there's not much code
but from a slide perspective and we've
got about 10 minutes if you have any
particular questions you want to go
through okay that's good question so if
you reference another words right so yep
so I'll pick like jQuery so jQuery has a
JavaScript file today they have their
own CDN and that's what you're referring
to and what would I do as a
recommendation for that so in an HTTP -
I'll tell you the first one and then
I'll add on these so I am in general I
don't like to use third-party CDN stuff
and because it's another DNS lookup more
likely for me it's another point of
failure I mean people say oh these
things they do they fail so I don't want
to add when it comes to jQuery I put
that today in my own bundle off my own
CDN because then I don't have a
dependency on whatever jQuery is using
for their CDN so that's how that's how I
would deal with it today so nothing
changes for me from one to two pointing
to it one to two really shouldn't make
any difference than either because
you're probably likely only making one
connection to them so them going from 1
to 2 you don't get most of the benefits
you'll get small on header compression
and stuff what I'd say pretty much be
the same from 1 to 2
why you funneling stuff like that
because the yep so the question is the
counter-argument to that is if your
customer happens to have already been to
jQuery CDN and already cashed that I'm
making them download that over again I
don't consider that well it's possible
that that could happen my complete
bundled I should look again my complete
bundled compressed javascript file is
like 150 K maybe which jQuery adds when
it's compressed maybe like 20k I have
images on the home page there tons
bigger than that I don't want that on
the detrimental effect of their CDN
going down then the slight advantage I
would have by leaving it separate and
hoping they'd been to that same CD
right so that yeah and I've had people
ask me that before and some people live
on the other side that they like to pull
that from the other CDN I just I've had
I've had my own CDN when I just using a
previous CDN the day I took my bundled
files and put it out they had a failure
and all my pages were blank I learned
how to do fall backs that night so you
can actually detect that your CDN didn't
actually download jQuery and force it to
pull it from your own server so I I'm
just really I really care the site works
first because I'm not gonna get called
about the extra 20k I'm gonna get called
that uh near Jake where he's working
it's kind of where I land but so the
question was am i aware of any libraries
that automatically detect to and choose
not to bundle I haven't I haven't seen
or heard of that with any other ones
that would be a cool thing because it
would save me from having to do some of
that but I haven't seen anything in
particular that does that
so well so I think today most people for
http/2 are using wireshark this wire
check was one of the very early
implementations of being able to
understand it and do the decoding and
all that stuff yeah I still want
Fiddler's to work
I mean I'd you know I've used that one
I've used Wireshark I don't like
assembling packets at that that level of
detail I cared about one day in my life
when I used to write protocols for
communication and stuff and then I don't
want to see that anymore so I really
want a Fiddler's type tool to abstract
all that up into a request in a response
and hide that stuff very rarely like
what I wanted to see how much do I
really save with compression I've gone
into a Wireshark or net mom to see how
many less packets and stuff I have but I
really want fiddler 2 to work so I
really want Microsoft and I mean the
side effect of that is all the other
calls I'm making to Web API is today
aren't gonna support to until Microsoft
fixes as anyway I can't I honestly was
surprised when I heard it still hadn't
been fixed so that's even probably more
important to me but I like fiddler 2 so
that kind of bothers me
so the question is could you use server
push to replace other stuff um like
signal are you can I mean it's a general
it opens up two-way conversations it
doesn't necessarily require that like
it's not gonna scan through your HTML
and notice that you're pushing something
that you didn't actually have in your
markup so you can push whatever you want
to push the clients can choose when they
establish the connection there's
actually part of the they set them like
a settings frame with the connection to
say I want this or I don't want that
anyway so they can shut off server push
so you just have to be aware I don't
know what percent are going to do that I
would guess if people see failures with
server push some people might start
turning that kind of stuff off and then
he'd be stuck but it's a general
surveyor pushing it to the client sort
of protocol so yes
yeah so the question was the server
prioritize a or the stream
prioritization stuff do you believe that
we'll ever be exposed to a web developer
I hope it will be I haven't seen like a
JavaScript API for any of that for me to
be able to influence and say I really
care that you get this library or the
sandwich first I haven't seen that yet I
really hope that that will be there is I
really I would prefer not to
I'm glad browser vendors are
implementing some of this for me but I
may have a page I might leave at 90% of
the time just let it do what its gonna
do but if I have a page with a very
specific resource that I want to
influence I sure hope that I have an API
to do that yeah yeah so he said
something like an async or something
that you could put on the tags trying to
remember if I saw something about link
preload that had some influence on that
so let me know mind I'll pop out because
I think I put something in one of the
slides about that no no if I had it on
the streams
okay I'm thinking is something else yeah
sorry I guess the general is I hope for
that I haven't seen that yet
any other questions yeah yeah so the
question is if you had two tabs open in
a browser that will they share the same
connection I haven't heard what they're
going to do I would if I were them I
mean I would literally keep I want each
I want to limit the number of
connections I need in general and if I
could share those in multiplex those and
with stream prioritization I could
actually do that de-emphasize as I
switch tab to tab if I were the browser
vendor I would do that but I don't know
if they dream
did we request for something it's I said
backwards a little sunny you can use the
style whatever that you have your
attention but there's another one coming
next time I don't think I know that one
yeah stop up afterwards and we'll see
anything else okay otherwise thank you
for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>