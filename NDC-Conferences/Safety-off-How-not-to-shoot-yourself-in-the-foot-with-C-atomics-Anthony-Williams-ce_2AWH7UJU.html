<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Safety: off --- How not to shoot yourself in the foot with C++ atomics - Anthony Williams | Coder Coacher - Coaching Coders</title><meta content="Safety: off --- How not to shoot yourself in the foot with C++ atomics - Anthony Williams - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Safety: off --- How not to shoot yourself in the foot with C++ atomics - Anthony Williams</b></h2><h5 class="post__date">2016-08-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ce_2AWH7UJU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so you're using C++ Atomics or
Atomics in any language then you're
really going for the low level and it's
far too easy to do something wrong
shoot yourself in the foot so we're
gonna try and show some sort of
guidelines that are going to help with
approaches when you're using the atomic
to try and avoid shooting yourself in
the foot
but I said to start with we're going to
go through what the types are that we've
got what operations you can do in C++
and then we're gonna do some worked
examples and then lead up to know these
guidelines by the end so I'm just gonna
say before we actually get into the
things that fundamentally most of the
time you're using atomic operations
rather than blocks because you were
hoping that it's an optimization and
like any other optimization you're doing
in your system you want a profile
particularly in this case because if
you're skipping to the lower level and
using no lock-free data structures and
Atomics then there's so much scope for
getting things wrong in a way that won't
manifest in test because it's
multi-threaded code that you really
really want to be sure that it's that
it's going to be a benefit because the
cost in terms of maintenance complexity
on code complexity is so high that you
need to be sure it's worth it and wait
so when you're profiling you need to
think about the sort of things that you
care about for your application is it
the throughput know the total number of
baited items processed per unit time or
is it the latency how long it takes
between when you get some input before
you get some output or is it something
else now something that's specific to
your application some no measure that
you've got specific so you need to
decide what your major is and then
measure it with a simple reliable
mechanism or easier to use mechanism of
using locked locks and so forth then
implement your hopeful optimization
using Atomics and then profile
and check that you have actually
improved the thing you're trying to do
so the atomic types we've got in C++
fundamentally we're looking at by
standard atomic of T class template and
for this you can store any type which is
bitwise copyable if you can use men copy
on your type then you can stick it in a
standard atomic even though if it's a
struct or plane value and whatever you
can do it if it's a one of the built-in
integer types then you get an extra set
of operations on it but and but you can
you get the basic operations whatever
your type is and one of one of the
operations you want that you might want
to do is they compare an exchange and
for this then we also need it to be
bitwise comparable so if you can use mem
come on your type to compare the compare
values then you can do your compare
exchange and of course if you use but it
can be true for any type whatever type
you put in atomic T it might not be
locked free on any given platform it
might actually use an internal mutex
under the hood so this is one of the
reasons why you might want to profile
because you're replacing a nice high
level of mutex that you're in control of
with a low level one that they're
compositing control of and you just made
your code more compacts so no you don't
don't want to do that if that turns out
to be the case there is only one type in
the C++ standard that he's guaranteed to
be lot free and that is standard atomic
flag which is the most cut-down boolean
flag you can possibly imagine and it
only has two operations on so we'll look
at isn't just SEC but you can it is have
sufficient to build a spin lock and
therefore everything else on top of it
in the concurrency TS we've got two new
atomic types atomic shared pointer and
atomic
weak pointer shared pointer and weak
pointer not only they're not you can't
copy them with mem copy or compare them
with mem come because you've got to deal
with reference counting so we've got
special types to deal with those again
they may not be block free in any given
implementation so you need to hope no if
you want to use these you need to check
but know if it does turn out to be lock
free it can greatly simplify your code
so yeah what why are we talking about
atomic well the point is that it's a
simple irreducible operation so
operations on atomic types can't be
divided up any given thread will only
see either before the operation or after
the operation now there's no possible
middle ground no scope for races on the
actual value no data races no undefined
behavior for your atomic types okay so
what operations have we got well for
everything but we can load and all
values we can store new values we can
exchange swap no one value for another
and if they're bitwise comparable we can
do compare exchange which is I've got an
existing value if and then I've got an
expected value what I think it might be
and the new value I wish to put in and
if the it will compare our expectation
with what's actually there and if our
expectation matches what's actually in
there it will exchange our new value in
and this is a single atomic operation so
you don't have to worry about doing the
compare and then somebody slipping in
them and changing the values in the
meantime but like again that requires
that it's a bitwise comparable type
there's two variants is weak and strong
the strong will only fail if the values
were different if the comparison failed
whereas the weak might fail due to
running on a weekly or that architecture
like our more power and if the OS
scheduling meant that the operation got
interrupted because the those platforms
don't have ice though
things like x86 have a single
instruction for this so you can say I
want to do an atomic compare exchange
ban the compiler just does it the
processor just does it whereas other
architectures don't and because it's now
multiple instructions your thread might
get be suspended by the scheduler or
various other effects across multi cause
might cause the operation to fail by the
time it hits the final instruction so
the compare exchange strong then has to
loop whereas the compare exchange weak
just says well I failed for no apparent
reason so you will have to then decide
whether you wish to try again or adjust
your values before you try again we have
assignment and now most types when you
do assignment then you're assigning from
the same value but with the Tykes you're
assigning from the contained right so
atomic of T you're assigning from T
atomic event you assign for an int if
you can't you can't copy assign an
atomic event because that's now would
require a topic or Association as to
values and most implementations can't
manage to do that because they've been
set for places in memory so you need to
explicitly say I want to load from that
one and then assign to the other one
if we've got integral types or pointers
we can do arithmetic so you can know do
a fetch an ad which is added value
atomically to the value the values
that's there and return what the old
value was or a subtract which does the
same
then we've got operators that do this
we've got the plus + or minus - both
prefix and postfix appeareance
and now plus equals and minus equals so
you can do atomic operations these are
sort hands for doing a compare exchange
with the relevant values if you've got
an integral type rather than a point and
you can also do bitwise operations which
again no you say you've got and an or
and exclusive or and then the operator
variants yes at that they'd all be
equivalent to the strong compare
exchange so it's possible that on any
given platform there might be a nice
atomic operation to do it so if the
compilers being smart on x86 if you're
not using the return value then fetch ad
might use an in construction a locked
variant of the in construction to
increment the value but if you need the
return value it can't do that because I
actually has to do the exchange and then
of course on atomic flower we only got
two operations there is test and set or
clear you can't just set it you can't
test and clear it you can only test on
set and you and if you want to test what
the value is you have to set it there's
no no query it without setting so
there's only these two it's the bare
minimum that was guaranteed to be
supported us across all the platforms we
wanted to support C++ C 11 on and it's
enough to build the higher-level
structures with an internal block so of
course always thought might not be
locked free but these ones are there is
an an is lock free member function on
your atomic types which you can use out
runtime at startup in your application
to test
any given atomic of T is lock-free on
the current running platform
there are also some macros that you can
use a compile time which will tell you
whether it's always locked free never
lock free or sometimes and the reason it
might be sometimes is because you can
compile a single process that a single
application that you might be able to do
the same binary or multiple different
CPUs and depending on the properties of
the CPU so for example old x86 on 64-bit
platforms don't have a 16 byte compare
exchange whereas new ones do so you
can't then have 16 byte Atomics on an
old version of an x86 platform but you
can on the newer ones and when I say
newer I mean anything that can run
Windows 8 so we're not talking epically
new but it is still something there are
no systems in the wild that don't have
that operation ok so when you're doing
your atomic operations then you can put
constraints on them they've out the
memory ordering now there are six
different values that you can put you
can the the default if you don't specify
anything and which you get if you use
any of the operator variants you don't
have any choice because operators you
can't add extra parameters so and it
always uses sequentially consistent
which is the top one the the strongest
ordering constraint if you use one of
the function operations then you can add
additional ring constraints which are
increasingly relaxed in the in the
requirements that they put on you and so
you can have to do acquire and release
they so acquire ease for load operations
to say no I want to be able to see
changes that were made by the threads to
other variables that have been that I'm
acquiring the synchronization through
this load releases the store part of
that so I'm when I do a store with
release then that saying the changes
that I've made to other variables I'm
publishing to anyone that doing this
synchronizing through an acquire on this
value
acquiring release is the
read-modify-write operation version so
if you're doing a compare exchange which
is both the lowdown the store then you
use a car I release to do both halves
but you can only use only say I'm only
synchronizing one way I only wanted to
release or only in the choir and then
relaxed which is fixed versus only is
don't do any synchronization other than
this specific value so the specific
value that I've got my atomic of T that
I'm using then that value is going to be
safely accessed but I'm not going to put
any ordering with regards to what other
operations are visible to other threads
so and when you put these on your on
your operations it it controls precisely
the details of the instruction that it
generates so now whether or not there
might be additional fence operations and
on x86 then if you do a sequentially
consistent store then it will use a
stronger no an exchange operation rather
than playing store in order to guarantee
that the global synchronization
properties are met okay now down here in
tiny type which hopefully you can't
actually read it says memory order
consumed now this is a special variant
of memory order require which only
synchronizes some values and there are
probably only three people who know how
this works
and they keep disagreeing on what they
what what it actually does so debt let's
think it's shaking his head I think he
thinks there might not be anybody who
knows how it works well if it if it
works how some people think it was that
it would also be applicable to some
other systems of an alpha but there is
disagreement so yeah maybe nobody
actually knows but so really really you
don't want to use it but it's there just
to confuse you
well if it is then that's great okay so
if you're doing sequentially consistent
ordering then sequentially consistent
operations they work how you might think
things work which is really nice you can
in your mental image think I've got all
these operations across all these
threads and I can I'm going to rearrange
them into this happens first then that
one over there then that one over there
then this one down here in some nice
order and that's what we get with
sequentially consistent operations is
some single global total order of all
our operations across all our threads
and obvious you don't know in advance
what orders going to be but when you
look at the your variable operations and
you look at the values you get you will
be able to construct that order the
compiler and the processor and the
runtime work together to ensure that
there is such an order and then you can
reason about it so if we've got four
threads we've got one thread over here
that is storing a value to some variable
X and another thread over there they're
storing a value to some variable Y then
we've got two threads in the middle that
are reading x and y but in different
orders so this one reads X and finds
that it's value 1 so it must be after
they store so that's great this value
reads what this thread reads Y and sees
that it's value 1 and that must be after
that store so that's great but over here
with the second order low we're now
loading from X if we load 0 here now
we've got a single global total order so
this load here must be before that
before the store 2x in that order
because if you've only got the one store
then if you read the value that was
there before it well that's really must
have happened before the store and so
then we can think well we've got a
single global total order so we can
follow the arrows round we've got our Y
store two Y is before a load from Y
which is before our load from X which is
before our store 2x which is before a
load of X so when we load Y over here
we must get one if this thread reads Y
is 1 X is 0
we must get ik and Y is 1 if we get X is
1 over here though if you've got single
because sequentially consistent
patient's there is no other way that
this can work so if you're doing release
acquire synchronization then the
synchronization is a bit more limited
it works on specific variables and the
operations that are visible to given
threads so there's no longer this global
single total order but a specific set of
operations and things that were visible
to one thread become visible to another
so the specific case that we will start
is if we've got a store to some variable
X now this one doesn't have to be atomic
I know for this synchronization to work
we've got a store or to some variable X
and then we've got a store to some
atomic variable Y with a release
operation but memory order release on
the on this other thread we've got a
load and we're reading that value of
from Y with a memory order acquire now
if the value that we see is 1 is the
value that we stored over here then we
have a synchronization relationship here
if we don't see that see the value that
was written then we haven't got that
offer at that synchronization it's not
there and so then you can subsequently
you will you will see that this value
was stored over here it was before the
store with the release and so when you
do the load with a choir then I've read
this done afterwards we'll see the value
from up here you can follow the arrows
and you get the value that you expected
the difference from sync sequential
consistency is that unrelated reads do
not synchronize so we've got our same
four threads as we had before we've got
a store 2x and a store two why we've got
two threads doing
of X and load of Y in different orders
but this time we're doing release
operations on the store and acquire on
the loads if we get our load of X is
zero over here
you it doesn't now have to be a global
total order so from the point of view of
this thread here the store 2x hasn't
happened yet but from the store to from
the point of view of this thread over
here it doesn't care well that thread
thinks it thinks the store to act has
happened but the local but the store -
why hasn't so you can now whereas before
we had that we if we get Y is 1 X is 0 X
is 1 means we must have ways one over
here now we're allowed to have y 0
now X is 1 y 0 or Y and Y is 1 X is 0 so
you can get inconsistency from the views
between threads so yeah you really need
to think about what it is you're trying
to synchronize if you want to relax your
the operations even from sequentially
consistent to just acquire release of
course if you're using relaxed
operations than anything can happen now
even with just the two threads no you
store to X and store to Y and if you
load from Y with relaxed ordering then
when you load mix I mean in this case
that would if it was if X was an atomic
there would be a race so assuming X is
atomic then you can get any value from
correct because it doesn't there's no
ordering the the store to Y the load
from load from white doesn't apply any
ordering at all we've also got fences
two kinds of fences there's a thread
fence and a signal fence now the thread
fence is what one will I always think of
as a fence and the signal fence is
specifically for signal synchronizing
between a thread and a signal handler in
the same thread which for most most
purposes you can probably completely
ignore I've just put it on here for
completion if you're doing signal
handler
and using atomic operations then you can
read up about signal fence but otherwise
just ignore it and in C++ whereas in if
you're looking at processor
documentation then fences might have
global global sync properties in C++
then specifically fences modify the
ordering constraints on the surrounding
atomic operations so if we've got a load
with a relaxed memory order and then we
apply a thread fence with a choir then
it is as if we've said load with memory
order requiring no fence whether the can
process that generates the same
instructions for the two cases it may or
may not the compiler might decide that
actually no I need to if you specify
explicitly the fence you will get
different instructions so it's not
completely the same but it can implement
it identically if you can see that
that's what it's doing and likewise if
you have a release fence before a relax
store then it's equivalent to a memory
order release on the store so no moat
mostly young no I would say mostly you
don't need to use the fences but there
are a few cases particularly if you end
up if you think there's only conditional
synchronization I want to do so you
might only conditionally do your fences
okay for completeness I mean and acquire
release fence is both an acquire fence
and O'Reilly fence so it will deal with
stores before it and stores after it and
loads before it and sequentially
consistent fences they fit in your
single total order of sequentially
consistent operations so they provide a
little bit more of an ordering
constraint but it's can be quite hard to
make it what to make it work right and
if you end up really relying on where
that fits in your ordering list then
you're probably doing something wrong so
know it's not necessarily a good plan
but it is there
okay so let's look at some lot free
examples now why would you use atomic so
I mean obviously the simple case you can
use it all right is you've got a single
variable and like an atomic like a bull
variable that you're protecting with the
lock and you say well I can get rid of
like I'll just have an atomic bull
instead so if you're in Andres talk
short time ago then rather than having
synchronised of an int or synchronized
of a bull then you could just use atomic
of an inter atomic of a bull but if
you're doing larger scope things then we
talked about knock free code a lot free
data structures so we've got three sorts
of guarantees that the no researchers
like to talk about with your lock for
your things first is obstruction free
which is that if we've got lots of
threads accessing our data structure and
we pause no the OS decides that they're
all going to stop apart from our one now
maybe we're on a single core machine we
were using lock three data structures on
a single core machine so everything
stops except this one can it finish its
operation and if it can if there's a if
it can finish in a bounded number of
steps it's obstruction free and that
doesn't mean wet matter regardless where
the other where in their operations the
other threads are suspended if no if you
want to step up on that and you say ok
we're not going to suspend in your bath
threads they're all going to access but
they and so they're all going to try and
compete on my data structure but they
might get in each other's way so no
we've got compare exchange compare
exchange might fail because another
thread has swapped in the value you were
trying to solve to try and put in so
then you have to loop and so if you've
got multiple threads and they emit
within about the number of steps one of
them is going to actually get to the end
then it's now a lock free data structure
but you don't know it might any given
thread might have to keep looping it
might be really unlucky and that know
you that that one thread will have to
keep looping a million times
whilst I know a million other threads
get all the way through their operations
without it without interruption but
that's still locked free and that's the
most common guarantee you're going to
see in lot free code because it's the
one that ideally you you you're going to
want because most of the time your other
threads are not going to be paused
they're not going to be stopped by this
edge you know
so obstruction free will mean that
you'll actually end up with things
getting in each other's way you'll get
live lock whereas the strongest
guarantee at the bottom wait free is
really hard to get weight free
essentially means there are no unbounded
loops in your code and every thread that
is accessing the data structure has to
be able to get to the end in a known
upper bound on the number of steps it
might be that if it's the only thread
accessing the laser structure you can
get through in three steps but if
there's other threads and now takes 20
but that's okay because we know there's
a top limit of 20 but if there's no
known top limit then it's not weight
free and if you're doing real time stuff
where you've got hard time boundaries
then you need weight for the operations
because then you can know that no my
operation is going to complete and
within a certain time quota though I can
then put into all my calculations of
whether or not I can do things so in
order to know demonstrate some examples
let's talk about queues and court queues
are a core facility for communication
between threads there might be many
types and you might have single producer
single consumer couse multiple producers
and single consumers multiple producers
and multiple consumers or the other way
one producer and then loads of threads
reading values off the other end you
might have bounded queues where there's
a maximum number of elements that can be
in flight in the queue at a time or
under and the queues where you just keep
allocating off the heap and then as long
as you've got heat memory then you
there's no bound now you might have know
various properties FIFO use LIFO queues
priority queues unordered
and again they might be intrusive and
non juicy so if you want to put it in a
queue you've got to actually do
something put in intrusive to your data
structure or otherwise and they call to
multi-threaded stuff and they're quite
good for demonstrating lots of the
issues so let's start with a know a lock
based unbounded multi producer
multi-purpose consumer FIFO queue and to
do this well we're going to Rampur span
the queue object with a mutex in the
condition variable so to push back we
just lock the mutex push our value onto
our map queue and notify anyone who's
waiting that there is now a value and if
we're popping in front popping off the
front then we can lock our mutex we can
wait with a concur on that condition
variable using now know with our lock
and we're checking for the case that in
the wrapped queue is not empty and when
they go back to queue is not empty we
can then take the front value remove it
from the queue and return okay so can we
do this lock free okay well let's start
simple we're going to have one producer
for it one consumer thread and bounded
operations so there's only a specified
number of items in a queue if we get too
many then it's full and somebody has to
wait and then we're going to assume that
we can copy our values with our
exceptions because that just makes
things easier and if you look now a
queue of into a queue of pointers that
is the case so
no it's not a daft assumption okay so
we're gonna have an array with some
entries our entries we're going to have
this is a basically this this conflict
bit at the top align storage we're going
to say we're going to create a buffer
that's big enough to hold and
appropriately that aligns to hold the
values that we want to put in a queue
but when but we're not going to put the
values there yet it's just in storage
and then we've got the atomic bull to
say is it initialized or not is there
actually a value in this given entry so
when we we're going to push values on
then we're going to have a position
where we're pushing now that we've said
that there's only one thread that's
pushing so we can it doesn't need to be
atomic
we just kate so we take the current
value we find the entry in the buffer if
it's already initialized then we're
going to throw that with full because it
means we've wrapped all the way around
and the threads haven't been taken out
yet value something taken out by the
consuming thread if it wasn't full then
we're going to increment the value but
with a wrapped round and then we
populate the value and sit and say yeah
this files ready okay so just an aside
know we this thread doesn't actually do
any waiting it just says if the queue is
full then you're going to throw and
which means that if you want to if
you're trying to push on a queue this
full then you've got to do something
above it because you could know you
catch the exception and try again
but then you end up essentially with a
busy wait loop so we'd like to avoid
those if possible but so you probably
might if the if there is a danger of the
thread being the queue being full
because your producer-consumer threads
are running at different different
speeds then you need to think about
waiting the only case for I know where
you really want have a sort of loop is
like if you got a compare exchange week
and so you just you're just looping
around and you actually want that loop
to be really small most of the time
because you don't want it to be
interrupted so you want to make sure
that you complete your operations but
otherwise you need to use an external
weight mechanism external to what we're
showing here like your condition
variable which does add the complexity
but you might be able to use something
that's a bit lighter weight if you know
depending on your platform okay so that
skip over to removing values so we again
we've got a non-atomic position that
we're popping from so we're going to
check that buffer we're going to check
we're initialized in the buffer and if
that if that if the current entry is
doesn't have one
then the queue is empty so within the
throw
otherwise we're going to cast our
storage to get a pointer to the the T
value that's there take the value out
destroy the one that's stored clear the
flag and check move the pointer it's
nice and straight forward but it's
single produce a single consumer if we
were going to try and extend that to
have multiple producers multiple threads
being allowed to push onto onto your
queue at any one time then things get a
bit more tricky you might think that you
could just make it an atomic on your
push position and so we load the value
and then we sort of try and do an atomic
increment but we couldn't just use fetch
and because we need to do with the
background so we do in the compare
exchange wheat loop you might think that
this would be good but it isn't it's is
too naive and this is one of the first
things that you find is you trying to if
you're doing a lock free stuff then the
simple case is all nice and good you try
and extend things to more threads and
then if things go wrong it's still
broken
the particular sequence of operations
that can get things wrong we've got an
empty queue pushpa z0 so thread one
calls pushback it gets a position of
gets the position is zero and increments
the position to one nicely the compare
exchange just works there's no other
competing threads it checks the cell is
empty and get suspended then now suppose
we've got another thread which pushes
back until we've got full buffer so the
push position has now wrapped all the
way back round and is now pointing our
first entry again and thread two pushes
then one more item so it's actually
trying to push our no buffer size plus
one item on to the queue but the first
rate hasn't actually put an item in its
slot yet so thread two gets the same
position and checks that the cell is
empty and it is it is empty because the
first way it hasn't done anything yet is
guys it was allocated the cell but it
got suspended before it could put a set
put anything in the cell so thread two
checks at the service empty puts a value
in in the cell and it's done but thread
one is then woken up and this is what I
just checked this fell this cells empty
so I'm going to put my value in the cell
and now we have unsynchronized access to
a value which is being accessed by
multiple threads and we've got a data
race and the queue is broken and even if
that was an atomic value that you're
being then you would have overwritten
the value that hadn't yet been removed
so the naive attempt of just making a
push position atomic doesn't work
so you might think well that only
happens if we were trying to push no
buffer size plus one L elements because
the buffer was already full can we
prevent this by checking the size for a
full buffer can we add an atomic size
now we'd load the size if the size is
the buffer size then we keep looping
we'd put some sort of busy weight in
there otherwise we're going to increment
cuz we're gonna push we're adding a new
value but now we're not even obstruction
free because as soon as the if the if
you get thread no in there in our
example if thread 1 was suspended with
it's waiting to put value in itself the
buffer is now full and no no thread
would now be allowed to populate that
but that cell because the
retrieving thread will will get blocked
because the cells empty and the other
pushing threads are blocked because the
buffers full so if you suspend all but
one thread unless the one thread that's
awake is the one that's that hasn't
there's got a cell that it's trying to
populate that it's holding everybody up
unless that is the one that's awake then
nobody makes progress this is not even
obstruction free okay so can we fix it
now and to do that we need to think
about what the problem is that we're
actually trying to address and that
thing is that so pushing a value has
three steps we need to find a free slot
we need to construct the pushed value in
that slot and we need to mark that value
as available
so whichever threads going to consume
that consume the thing so and if we get
those in the wrong order then we don't
even get obstruction free or we just end
up with beta races so what we need to do
is we need to publish the availability
of the data at the end of step 3 rather
than acquiring our cell at step 1 and
now that cell is now blocked from the
rest of all the other threads in the
system
we need to now if we do that as step
three at the end then think no then we
can do with that so is there a way that
we can manage that and to do that we
need to sort of separate the buffer
ordering from the queue ordering so we
need to redo the steps and so you might
think okay can we hunt the buffer for a
fruit free slot and then construct the
push value into the slot and then link
that entry into the queue is there a way
of doing that you might think and
fundamentally the answer is well you're
going to have to end up using some form
of link list it makes it the now if you
stick with an array and you hunt through
four free slots then you end up with
ever such a lot of complexity and then
you still now got okay well that was the
slot that I added but which water was it
in the queue and now I need to put it on
a queue to show which order of my
buffers were in no so let's just use a
linked list so legal easy
no you just stick a list on the tail and
pop them off the head but we still got
two locations to update to the next
point node so we've got linked list
which has a node with a pointer to the
next node so we've got the title pointer
that says this is the last node in a
list we want to have the new node
we need to change the table pointer to
point to a new load and we need to
change the next pointer from the
existing one to pointer to a new note
there's two things to update and
whichever order you try and do them then
you could end up with a race so it's
still not straightforward
so one possible option is actually to
say I'm not going to update the next
pointers I'm going to update the next
pointers from the one and only pop
thread and instead I'm going to make it
a doubly linked list with a previous
pointer because that we can now safely
do so we have a new node and we say well
the previous node was the one that's
currently lost and if we then
successfully change tale to point to our
new node then we know that that was the
case because we're changing tail from
what we think that the previous node to
our node and if it doesn't if the
compare exchange fails then we can read
we've got the new tail that we can try
again
so we allocate our entry by some means
we know crate put our object in their
storage we clear out the next pointer
because we don't know what it's going to
be we say the previous one is the
current tail and we repeatedly try and
update the tail to point one you know
now if this fails then actually the
compare exchange actually reloads the
first value so we don't need to
repeatedly call table dot load okay so
then when we're popping then if we find
that where there isn't a value at the
head of the head of the list then that's
because the then that might be because
well we haven't set up all the next
pointers yet so let's go and chase start
at the tail and chase down for the in
the next pointers we've only got one
consumer thread so we don't have to
worry about races on that so we'll look
at how you do that in a second but once
we want to be have got the no chase
chase through that the tail then we can
then load the next pointer get the value
destroy the old and then we cycle a node
back onto now ready ready for now ready
for reallocating for the next
I said chasing the tail he's quite
straightforward so we we take the
current tail and we're going to exchange
it we're gonna swap it swap it for null
there's no end of a list because we've
just taken it on so we take an ownership
of that and then if there really wasn't
anything we're going to return a null
and and make sure we deal with that in
the in the pop code but then then we can
just go through from our tail and
allocate point though we've got a node
which is pointing back to a previous
node and then we just say we'll take
them the next pointer this one and make
it pointing that way and now we've
relink them back in the correct order
and so then the pop can now proceed with
just facing leaving this that way so
this Q is obstruction three because
there's no ones one thread that can be
but that will hold up all the others now
if they if any of the push pushing
threads gets gets interrupted then it's
not going to cause a global stop and if
the one popping thread gets interrupted
it's not going to prevent any of the
pushing threads operating so it's it's
definitely obstruction free but is it
lock free
now if it's full then we're gonna have
to wait so now you you're going to know
so one thing is that then you you might
want to use a locked reallocator instead
of fixed buffer and if you've got a
fixed buffer size then we've got issues
on fullness and likewise you've got
issues on emptiness but generally if
you're thinking about lock-free versus
weight free then you can disregard those
if you've got just got a test no so if
it's full then I throw I have returned
in a bounded number of steps of a higher
level up I might have to put some high
level waiting in but that's not know so
but but we do have weighting and compare
exchange loops and there's no upper
limit so it cannot be weight free and
then just so but then but we also are
using compare exchange week well would
it if you're using any compare exchange
then if you are on an implementation
which doesn't do it as a single
instruction then that compare exchange
might be an unbounded series of steps in
itself and so then actually it's not
technically lock free if that's the case
and we can only be obstruction for you
because no there's not a there not never
an upper limit on how many threads can
make progress and so the whether things
can ever be locked free really does
depend on their implementation of
compare exchange week but there's an
algorithm then it's a lock free
algorithm assuming that the steps are
individually yes okay so one thing to
think about then is how does it perform
know and know one of the big issues with
lock free stuff is cash ping-pong where
you've got one processor is accessing I
shared verily over here I know we some
on some cache line and then the other
processor wants to modify either the
same atomic variable or one right next
to it that's on the same cache line so
it needs
to grab ownership of the cash line over
to the other processor and then the
first one tries to do it back again and
so it had to take the cache line
ownership back over here if you've got
different processes in the same system
with different cache or even if they're
on the same core but they've got a
different level two cache or no then you
can find that that can be a real issue
and the cache the cache line is
continuously shuttled back and forth
between your two processors and you've
got no cash ping-pong it's called and
there's the this can have a big effect
on your performance
now this can happen if you've got the
same atomic variable that you're doing
but all alternatively others on the same
cache line a Mac that that can be a big
issue because cache lines might be 64
128 bytes so if you've now got a load of
atomic in so right next to each other
then actually they're all on the same
cache line and you might end up having
no having performance issues with that
okay so this cue that we were looking at
has got many threads that connects it
the core push back and one more look and
that can be calling pop front so if we
look at what the data that we've got
we've got a know a push position we've
got our head pointer we've got our
atomic list of no the tail which is our
head of our link no the tail of our
linked list and then we've got all known
our our buffer know that we were we were
using now the hidden tail right right
next to each other here so they're
probably on the same on the same cache
line but they're accessed by different
threads so know even though access is to
head and tail are not going to race in
any way because they're accessed by
separate threads it's going to shuttle
the the cache line back and forth no the
the pushing threads are going to be out
updating tail and the popping thread so
in the updating head but know that the
- line shopping back and forth and
there's quite a few no potential ways
places in this data structure that could
be no though if you're if your entries
are small enough then each of the
individual entries might end up being on
the same cache line and then the entry
accesses can be it can also cause cache
ping pongs and so forth and the way that
we deal with cache ping pong if we if we
don't want to change our structure is
just a stick some padding in great big
wad G's of padding no charm arise with
we're not going to do anything with but
that about your data structure so now
there is a whole cache line between each
of the values and you're not going to
get a cache ping pong do two accesses
two separate values obviously know the
threads pushing values on organ or
accessed tail so that's going to have to
be have to be shuttled back and forth
for that but there's no that's a core
part of the algorithm whereas the the
ping pong do to the popping from the
head then we can avoid that by sticking
the padding in but the and this trades
memory space for performance yes not
currently but there are proposals for it
I think the the there's going to be two
values which a if I remember rightly is
yeah constructive yes so there's there's
two values for constructive interference
and destructive so it's like the minimum
value of spacing that you have to put
things together they won't touch and the
maximum value the size that you can put
together that guarantee will be on the
same cache line because sometimes that's
important that you want things on the
same cache line so there's two values
for the two things
yes oh if you want if you want cache
line boundaries things on the alignments
of individual values will be nicely
aligned server the van the accesses work
so it doesn't matter what the padding
size is for that but if you want if
you've got thing things like this next
to some other value in memory then you
want to make sure that the it's dance on
its own cache line and doesn't share
with something that's previous in memory
so you might want to take the whole
structure and make sure that it's a line
to a cache line size but yeah so if you
don't have the nice new constants then
picking a value like 128 for your
padding size is pretty likely to be good
enough for most systems if you know
where you're targeting then you could
look it up but cache line size is do
vary because the processors and the
memory architectures no change all the
time and they say oh well actually no
sometimes no 64 bytes works really well
no 256 that's what we need and depending
on the details of the processor then a
might vary so know you if you know what
you're targeting you can look it up if
you've got a compiler that provides the
new constants you can use that otherwise
and take a punt on 64 128 or something
like that say okay so if we stick those
in does it make a difference yes we
turned I'd I ran they ten thousand times
10 million times
pushing values on three threads and
popping value on a four thread because I
got for call system I was testing it on
we timed it against the lock variant
that we had right at beginning so with
no padding no we had sort of comparable
times to the lock based one but if you
stop the padding in then it was really
no quite a significant difference so
that cash beam problem is a real effect
it's not just marginal no that's half
the time
so okay so far we've all been using
memory order sequentially consistent and
I'm just going to say you've really
really ought to unless you have a strong
reason not to it's possible to edge out
some corner cases of performance by
relaxing your constraints but no I don't
be if you really really want to profile
and profile some more and review your
code no for like three weeks solid even
if you've only made one change no can
convince yourself that it's correct and
then profile some more and get somebody
else to revert know if you're going to
relax constraints you need to be sure
and also test on a weekly ordered
architecture if you're testing it on x86
the memory underneath is really strong
test it on arm or power or if you've got
access to an alpha then use that but the
if you're relaxing your memory ordering
then it's very easy for things to work
in tests and then fail in production so
yes so on x86 only the store really is
affected but power on arm then actually
you can affect all the operations so
yeah make sure you test on on the weekly
audit system if you're going to try and
relax the ordering constraints okay
quick dive through stacks and you've got
a few minutes left so it's going to be
speedy stack is a simple data structure
like you it's great for examples but
mostly you're going to reuse it in
multi-threaded code because if you got
multiple threads accessing everyone's
piling on the head there's only the one
point of access accessing the top of
stack but I'm going to use it for a
specific demonstration of a specific
problem the ABA problem
so assume we put we've got a stack which
it's got those nodes with next pointers
we've got a topic value to hold the head
head of a head of our stack stick create
a new node compare
exchange to stick it on we can just loop
round no yes it's nice if we've only got
one consumer popping it's all nice we
can just take the head and do a compare
exchange if it works then we take the
value and delete the old head
there's only no the new values are
pushed on there's only one thread
popping it off there's no particular the
pushers are not accessing the contents
of the nodes so we can safely delete
them and but it's a single consumer for
a reason and the answer is a B a problem
the ABA is you've got a value in your
data structure that holds value a you
read it it's a it's a some other thread
then does your thread get suspended and
other threads do stuff it gets changed
through some other value B and then
change back to a again but now that a
has a different meaning from your data
structure and so when you look at it
it's a still but it now has a different
meaning and when you then make your up
make your operations based on that you
do something wrong so in this particular
case if we then said multiple threads
could pop what's going to happen okay
thread one calls pop it reads the old
head and gets a value a it reads the
next pointer from its old head and he's
then suspended thread two pops two items
head now has a new value which you're
going to call B and then we push to two
items but because we just freed some
values and then the allocator know says
well I'm going to give you the same
object that you just freed because often
it does that if you calling you and then
called delete and then calling you again
with an object of the same size you
might well get exactly the same point of
value and so no we've got when we stick
the items on the queue but the new one
that we just allocated has the same
address as the old one we just freed but
it's a new note so head head now has the
original value of a but
it's a new note a and not the original
so when a first thread resumes its
compare exchange will succeed because
it's got the same value that it thought
that it's expecting but it's now
referring to a new node so the next
point of that it loaded up here is now
pointing off into a deleted node and
you're no you're you're no we when we
change the data structure to point to
our deleted note here which is no just
broken data structure now so rather than
having the correct setting that we
wanted we've just missed everything up
and that's the general setup we have no
value changes from A to B in back to a
but the global state means that that a
now is based on a wrongheaded a or a
wrong view of the global data structure
and so everything goes wrong quick
solution is to use a struct so we stick
an account value you increment your
count every time and then you've got a
know you know that it might have the
same point of value but it's got a
different count value so that's okay we
can spot that the difference is there
alternatively you can use ensure that
things and not recycle while they're
still accessible if you've got an atomic
shared pointer implementation that you
can use then use that if you've got if
you can use hazard pointers do that and
it's possible to do other things but it
can be quite tricky there things like
this and it can still be locked free so
on no like I said at the giving on
64-bit x86 platforms this is a 16 byte
structure it may or may not be lock free
depending on precisely whether your
library supports it and whether or not
the process that you've got supports it
on on different different platforms than
whether on a lot free will vary okay so
what guidelines can we pull from all
this
first one I'd say is don't use Atomics
unless you have to profile your code
before and after make sure that this
really is a beneficial change that
you're making know firstly what is it
the bottleneck if your code that you're
actually wanting to update if this data
if the data structure you're trying to
update isn't actually the bottleneck
don't do that do something else but if
it is then profile it before profile it
afterwards make sure you've actually
improved things make sure you test on a
weekly order of architecture such as
para NARM
and again don't use it on Excel if you
have to and as a final one which isn't
on the slide just a reminder then things
like the previews if you pad things out
where there's contention then that can
actually have a dramatic performance
improvement so more general guidelines
if you think Incans in transactions then
you could know then that can improve
things if you do your work off to the
side and commit with one atomic
operation there's now only the one
operation to think about so now that can
have a big benefit in terms of making
sure that things working correctly if
you if your operation is big then if you
can split it into smaller steps which
can in themselves be done as an atomic
operation now possibly off to the side
and then committed then that can make
things work it can help you retain if
you can manage to retain the data
structure invariants between each step
and that can be good there's so there's
various things you can do with no insert
inserting marker marker values that says
I'm not a real value yet but I'm going
to be here as a placeholder so if you're
if you're a reader thread just skip over
me for now
and then then when the value really is
then set as a separate operation then
obviously we tall limit your use cases
if it only works with the single
consumer thread say it only works with a
single consumer thread and just declare
that as part of your documentation and
then if people try and use multiple
consumers the last they're tough and but
then if if that's something you need is
the multiple consumers then you can't
and you've got to think no a new
approach oh and watch out for the ABA
problems I mean the chances are that it
won't happen in testing as ever because
the things have to align just right for
you to actually get the same value back
again but if you're if this is a heavily
used thing then it will happen at some
point and undoubtedly with any of these
things it will happen at the worst
possible point now you're trying to
demonstrate demonstrate to a crucial
customer and it will all go wrong
avoid your cash ping-pong with having
use sequentially consistent memory
ordering unless you really really really
have a know a strong idea of what it is
you're trying to achieve and you really
want the performance gained and then
package up your whole lot free structure
and then know use it from the outside
through an API so it's just a cue as far
as anybody else is concerned and then
you can fiddle with the implementation
make the implementation thoroughly
tested and thoroughly reviewed and then
switch and and but from the outside
world you just use it as a cue and it
might as well be a lock based one aim
for lock free code rather than
obstruction free if you need it then
wait you go for wait free code but it's
really really complex because trying to
avoid any unbounded loops is actually
really hard and don't use busy weights
unless you have to so I've got questions
where we're over time as it is but if
anybody does have any questions then
feel free to talk to me so stop that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>