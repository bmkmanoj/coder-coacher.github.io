<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Better Code: Concurrency - Sean Parent | Coder Coacher - Coaching Coders</title><meta content="Better Code: Concurrency - Sean Parent - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Better Code: Concurrency - Sean Parent</b></h2><h5 class="post__date">2017-02-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zULU6Hhp42w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Shawn parent for anybody who wasn't
at my previous talk I'm a principal
scientist at Adobe I've had a number of
roles they're good at Adobe now about 22
years depending on how you count because
I was there 15 years and then I went to
Google for about a year and then back to
Adobe I started my career at Adobe on
the Photoshop team and later I ran
Adobe's software technology lab that was
a research group that spun out of our
desk Knology group I ran that group for
about eight years and now I'm actually
back working with Photoshop team again
so I've had a number of roles
my clicker here so I mentioned this
before a few years back I gave a talk at
going native conference called C++
seasoning and and it was well-received
and I got a lot of requests to write a
book and so since then whenever I get
asked to give a talk I try to flesh out
a chapter of the book so this is kind of
my working outlines you see for each
chapter here I've got a goal most of the
goals I need to redo the first one now
that one's also stated in the negative
are stated as a negative as far as their
what not to do and there's a reason
there the the word goal was chosen very
carefully because what I talked about is
is not prescriptive things they're not
rules they're things to strive for but
they can be difficult especially when
you're dealing with a large system to
achieve these goals so they're not rules
are not guidelines they're not things
that should be held up in a code review
or people that John said don't do that
what Sean said is think about that
really hard and and the reason the other
reason why they're stated as a negative
is because I can't give you all the
answers as to as to what the
alternatives are so these are things
things to try to avoid and I'll talk
some about better in different ways but
sometimes it just requires more thought
so these are all tend to be open-ended
problems so today we're talking about
concurrency
alright some common themes in all my
talks are our that they're all about
managing relationships and what I mean
about that is relationships between
components and software understanding
the fundamentals which i think is
incredibly important learning how to
write simple code and learning how to
how to reason about code through local
and equational reasoning right so local
reasoning means that just looking at a
piece of code you can reason about it
without having to understand the whole
system and equational reasoning means
that you can you can describe what
something is in terms of something else
a is equal to B right a plus B is equal
to C all right so you're able to go
through those transformations
now this talk started because I was
working on what became this product
right here this is Lightroom for web and
and this is the complete Lightroom
rendering engine running inside of the
Safari browser with no plugins okay so
for people who don't know Lightroom is a
product that adobe ships that
professional photography rendering
engine and it's a parametric model so
it's a non-destructive model you you
edit your image and you can come back
later and change your edit and readjust
it so to make this work over the web
means that we had to have full parity
with the desktop rendering engine okay
and this is a piece of code now that's a
decade old and heavily tuned on the
desktop to give you some idea this is a
woman in the London subway and as the
train was pulling up I I glanced
backwards and snap the photo and so
that's the the unedited photo and that's
what it looks like edited so this was a
big problem taking a 10 year old
codebase that's heavily threaded
getting it to compile with the M
scriptum compiler which is a compiler
that Mozilla puts out to compile C++
code into JavaScript removing all of the
need for threading in it because in
JavaScript you have no threads you have
web workers which is the equivalent of
having separate processes but you don't
have threads and getting the code to run
well and to behave in an interactive
fashion so that was the challenge so
this got me thinking really really hard
about concurrency and the reason why I
got me thinking is because going forward
this code base needs to be shared so I
needed to leave the system in a state
where we had a code base that developers
could work on that would continue to run
in run performant in a heavily
multi-threaded environment and the same
code would run very well when you had no
threads right so I had to think about
what is the actual programming model for
how you write threaded code okay so
first some definitions here right
concurrency is when tasks start run and
complete in overlapping time periods
where parallelism is when two or more
tasks execute simultaneously right so on
a single threaded system you can have
concurrency and you can have threads and
it just means things are time sliced
okay but parallelism is when you have
multiple cores right or something like a
sim decore where you can do multiple
things at the same time now the reason
why we want concurrency is because
concurrency is our first building block
to enable parallelism right right if I
can't run run with multiple threads on a
single core then I certainly can't run
with multiple threads with multi-core
the other reason why we want concurrency
is because it improves interactivity
right in the previous thing there right
when I'm sliding my my slider I'm
receiving user events coming in and I'm
updating frames and if every time I
update a frame if that slider stops that
the users sliding that makes for a
pretty
at the user experience right so we want
to be able to slice things up so that we
give give good interactivity down goal
for this talk is no raw synchronization
primitives so what are Ross
synchronization primitives well there's
these things they're mutexes there are
tommix a lot of people don't don't like
me putting Atomics on this this list but
they're certainly Atomics and i'll
explain more about that later but things
like semaphores
memory fences and conditional condition
variables okay the number one reason why
you don't want to use these things is
you'll likely get them wrong how do I
know that because I've been doing this a
very long time and I still get them
wrong okay so here is a class that I
wrote once upon a time its renamed it's
the copy-on-write class and it's
abbreviated here
it's renamed as bad cow and the reason
why I renamed it is because giving this
as a as an example of a bug then
somebody immediately when it was just
named copy-on-write
somebody immediately copy and pasted the
code from my slides and put it into a
working system so not actually looking
at the rest of my presentation where I
showed the bug so who can find the bug
in this code right I'll even help you
the bug is between those two lines
does anybody see what's wrong here time
of check and time of use problem
absolutely
okay so and I'm using an atomic here
right so what happens is I check to see
is my count equal to 1 what a count of
one would mean is that this object is
the sole owner of some shared data okay
if this object is the sole owner of some
shared data then I know that it's safe
to assign directly into it right okay if
it's not one right then I need to make a
copy here this is my copy on write okay
when I make the copy then I need to to
decrement the count
of over this one here okay so so what's
the problem the problem is if I wasn't
the sole owner on the first check then
by the time the second check comes
around I might be the sole owner and if
now at the second at that decrement if I
am the sole owner it's my responsibility
to free the shared state okay and I'm
not doing that here right so the six is
when I do the decrement I have to say
okay when I do this decrement was I the
sole owner and now my count went to zero
and if so delete the object right so
this piece of code shipped in a running
system and probably ran for something
like five years okay and it wasn't that
bad
right you have a very short window there
where you do that new where somebody
else can release ownership and if they
do then you'll leak one block okay okay
but it was very vexing because
occasionally we would see these blocks
leak and couldn't figure out why or
where they were coming from
okay even with these slides I've given
this talk now a couple times I've
received five email messages where
people tell me my fix is is wrong okay
it is correct I'm quite convinced I can
write the proof fix is correct the five
people who wrote to me all what they
believe is a bug all boils down to to
one of two things in those five emails
one is they assume that my bad count
implementation is an atomic type it is
not an atomic type right right if it
were an atomic type then when I was sole
ownership somebody else could come along
and and claim ownership of it okay so
the account could go up it's not an
atomic type in that a copy-on-write
integer is just as thread-safe as an
integer right it just means that I can
have separate threads each with their
own instances with shared data okay but
I can't share a single instance of the
cow right the cow refers to share data
the other thing is people don't
understand what - minus X does on an
atomic and that's that it's an
equivalent of effect such subtract okay
of an atomic such a subtract which means
the subtraction and the fetch of the
value happen is a single atomic
operation and sometimes people think
that because I'm doing the - - and then
my compare okay that that between the
subtraction and the compare something
could go wrong
nobody caught I fixed it in the previous
slide that my count was uninitialized
okay neither did I so neither did the
compiler which Frost's me so originally
when I wrote the code and it was in a
working system I didn't have this
problem and it's because I wasn't using
SPD Atomics because the code long
predates STD Comics I was using a
Atomics with different behavior but an
STD atomic of an int if you just default
construct it right without giving it any
value is an uninitialized variable just
like an INT is okay so I actually
introduced a bug and nobody caught that
okay so so a Tom even a Tom X can be
very difficult to reason about why do we
want performance through parallelism
right this is a slide that one of my
colleagues Russell Williams on the
Photoshop team put together awhile ago
but it's very true today so this is your
typical desktop computer right and this
is looking at where the nips are where
the gigaflops in a desktop computer okay
that's the bar that's the GPU right
right so so this is looking at an AMD
Radeon which is a little outdated that
bar these days is getting bigger if
you're running on mobile it's a little
bit smaller okay to get into that you
need to get into OpenGL OpenCL cuda
direct compute c++ am DirectX metal pick
your thing all of those things tend to
be C or C++ derived programming
environments with constraints tailored
towards parallelism okay that's how much
of your performance is in the
vectorization unit on the machine okay
so to get to that you're going to have
to be writing with Auto vectorization or
like the Intel compiler we'll just some
degree of that but you have to know how
to structure your code to utilize it
OpenCL can target it or your writing
with intrinsics which is where most
people end up if you want to unlock
multi-core as of C++ 11 we actually at
least have a threading model there
that's how much of your processor is
tied up with multi-core and if you're
talking about just one core the scalar
unit that's the slice right there that's
what most people think of a straight C++
and if you're writing in JavaScript or
Java or something like else something
else divide that by a factor of 2 to 5
these days okay so that red line at the
end there is point 0 to 5 percent of the
performance of your machine
okay now this is am dolls law Kevin
mentioned this in his last talks you
were in here what AM does law says is is
basically if some amount of your machine
is serialized okay then you cannot
accede to that in overall performance by
adding parallelism okay
so in this graph here you can see if the
the Purple Line is at ninety percent so
if just 10% of our app is serialized we
can never perform better than 1010 X
increase in performance because now that
will be the bottleneck is that that
cereal is serialized piece okay but the
interesting thing about this graph is
that you don't hit something
approximating that 10x speed up until
you get out to a thousand cores okay the
bottom scale on this thing is
logarithmic where the top scale is
linear and I hate the graph for that
reason I think this is a very misleading
graph okay
so but this is what you'll find on
Wikipedia or in most textbooks is people
draw the graph this way because
otherwise you can't really see the shape
of the curve okay but the more telling
graph is to just look on linear scale
down at the access so each line here
represents just 10% of serialisation so
if I just have 10% of my application
that's serialized and I'm on 16 cores
I'm only getting a little better than a
6x speed-up
okay so Amdahl's law sucks
right right it bites us okay and
serialization doesn't mean locking on
mutex utilization can just mean sharing
the same memory right or sharing the
same address bus for them for the memory
if I don't have a Numa architecture okay
or sharing the same cache line right
anything that's shared within the
processor
starts to bend that curve down and it
bends down rapidly any synchronization
anatomic bends that curve down okay
and most people think about threading at
least most people you know coming from
kind of a C or C++ background in this
horrible way they say I've got this
object that I'm sharing and I got a
bunch of threads that are banging on it
and so I'm going to stick a mutex on
there and we're going to keep rotating
that around okay that is a horrible
horrible way to think about threading
don't do that we don't want to stop okay
no stops minimize locks now a lot of
people that I find when I talk about
mutex and I asked them what is a mutex
right can't give me a good answer say
give me an equivalency to a mutex right
right give me a mental model for a mutex
I added this section to the top because
somebody who I work with came to me and
said I'm trying to optimize this piece
of code
where I've got this mutex and I've got
this lock over this big block of code
and I've tried to minimize the amount of
work I'm doing under the lock to the
absolute minimum and it's still huge so
now I'm trying to optimize the locks and
I'm playing with like adaptive spin
locks and stuff and I'm like ah you're
thinking about it all wrong okay so
let's take a look at a traditional
little piece of code here right I've got
some registry class that I'm going to
write and it's going to be shared so I
want the operations to behave in an
atomic fashion so I'm going to have a
set and a get right where I can set a
key and a value and it's just going to
store it into a map and because I'm
sharing my map I've got that under my
mutex and I'm going to get it okay and
so I'm going to lock lock my mute my
mutex and just pull it out
at the same time okay so I've got this
this little piece of code right now even
here although it looks like I'm doing
just a little bit of work under the
mutex right and I'm using an unordered
map which is a fairly efficient data
structure an unordered map is a hash map
the amount of time it takes me to hash
my key depends on the length of the
string so the amount of work that's
being done under the locks here is
actually fairly unbounded right it
depends completely on the lengths of
length of my string so it's probably
typically small but it could be big okay
there's this line of the C++ standard
and you'd find it in a bunch of text
books but I pulled it from there because
I think you know it's a good way to
think about it it can be showing the
programs that correctly use mutexes and
memory ordered sequential
sorry I'm drawing a blank it's a memory
very a sequentially consistent memory
barrier and a memory order sequentially
consistent memory barrier operations to
prevent all data races and use no other
synchronization operations behave as if
the operations executed by their
constituent threads were simply
interleaved with each value computation
of an object being taken from the last
side-effect of that object in that
interleaving this is normally referred
to as sequential consistency so why is
this an important sentence well it means
that we can always think about mutexes
as if we had some set of operations and
those operations are interleaved right
that we have op and I messed up the
numbers here so I can have operation 3
repeated multiple times right right and
and what an operation is is that's the
code that's executing onto the lock and
its operating on some value what's that
value it's the set of all objects that
are modified by that operation ok so by
all operations okay so so X is
everything that gets modified within my
lock ok by all the pieces of code that
acquire that lock on the mutex ok and
each of those can yield out some values
some result right those operations can
say it's a read-modify-write operation
they can read modify write the data
under that yielding out some results
from the read
okay so the same is true of all atomic
operations right so there's really not a
lot of difference between an STD atomic
in fact there is a call on STD atomic
that says is this lock free so what that
means is is their processor support to
do that as an atomic item within the
processor or is there not processor
support and the compiler has to generate
a mutex pair to to lock make the change
on the atomic operation and do the the
unlock right so all that mutexes and
locks are our way to construct atomic
operations right so if you look you know
fetch subtract follows from my previous
example for the bad cow you're doing a
section subtract that's a read modify
write operation on X to the atomic so
what we have here then is that we can
take any piece of code that has mutexes
and we can transform it into a queued
model okay so if we assume that I've got
a serial q and my serial q has just one
operation on it which is go a sync
execute this thing and it's the same
same calling conventions as STD async
here except it guarantees that that the
next item being processed in that queue
doesn't start until the previous one
completed okay then I can rewrite my set
string to go do QA sync okay and I can
rewrite my get string but I've got a
little bit of a problem there in that I
need the result back out and I need it
paired with that particular gift okay so
I'm going to use a future there and
we'll talk more about futures okay and
I've got the Q async here
why is this important to understand
because any place I have a mutex in my
code I can always make this
transformation I can always trance
form it into a serialize queue model and
that means that within the serialized
queue model now anytime somebody comes
along and says set here regardless of
the amount of work that set takes the
time it takes for set to return back to
the caller is constant okay
so that means that I can add something
like set an arbitrary set of value the
whole vector of key value pairs okay and
to the caller that set will take just as
much time as the previous set it's an on
block okay so so this puts an upper
bound now there's overhead in this right
I've got to queue an item I've got to DQ
the item I've got to deal with futures
if I've got results coming in if I'm
calling this set as opposed to calling
just set string set sync set string
maybe I have to construct my vector
pairs
I can't just pass iterators in there
because my whatever I'm passing
iterators in to might go away okay so
there's overhead in this but it gives me
an upper bound so if I'm looking at a
piece of code or I'm like I've got this
big hoo Creek and piece of code with a
bunch of mutexes around it I'm like
you're totally in the weeds right the
place is enough abound on both
synchronization overhead and time to
issue operations now couple more
definitions threads a thread is an
execution environment consisting of a
stack and processor state running in
parallel or concurrent plugged in my
slides to other threads okay a task is a
unit of work off often a function that
you want to issue on a thread so tasks
are scheduled they can be scheduled onto
a thread pool top to Mai's machine
utilization I mean threads tend to be
relatively heavy weights because they're
carrying around processor state
and and stacks and if you saw cat
Kevin's talk previous to this you had a
great graph showing just how expensive
it is to do a task switch from thread to
thread right you can do an awful lot of
processing on the 10s which now
unfortunately C++ 14 doesn't really have
a task system we have threads okay and
we have futures okay but we don't have a
task system now worse it's in its
implementation to find whether or not
STD async spins up a thread or execute
it on a thread pool in C++ 11 it was
actually defined that it's spun up a
thread but Microsoft's complained
because under Windows 10 Microsoft
doesn't want to expose developers to
threads they want everything to go
through the windows thread pool so they
didn't want STD async to spin up threads
so in C++ 14
they changed the standard wording to say
well it's undefined maybe it's a thread
maybe it's executed on a thread pool
we're not going to tell you I'm going to
show you in a few slides why that was a
very problematic decision okay now I'm
going to walk you guys through building
a tasking system we're going to build
kind of a portable reference
implementation and C++ 14 but don't do
it on your own right not unless you have
to I actually had to do this because in
the previous slides getting Lightroom
web to run one of the places where I had
to make it run was inside the Chrome
browser using pinnacle which is Google's
portable Native Client and that's
basically in an environment where you
can run threaded C++ code inside of the
browser so long as it's a Chrome browser
or or on a Chromebook but they didn't
have a stock tasking system for it and I
couldn't find a reasonable one that was
open sourced apples Grand Central
Dispatch is open sourced but it has a
lot of dependency on Lib kernel and I
don't have a kernel inside of Chrome
unfortunately so I couldn't make that
work
but there are lots of good ones here
right there's windows has thread pool
and ppl Apple has Grand Central Dispatch
which can run on Linux and Android
there's until TBB there's HP X I'm sure
that you can find others but those would
be the ones that I would I would
recommend okay this is one that I would
not recommend so there's a little link
in the URL there Oracle has this paper
that they published years ago which is
how to write high-performance
multi-threaded server code and and as an
appendix they have how to build a
tasking system in there for a thread
pool this is their model you've got a qu
su tasks into the queue the tasks get
DQ's by multiple threads you have one
queue per piece of hardware that you
have and and their implementation is
about 20 pages but we can build with c++
better one pretty quickly so we're going
to build a notification queue here we're
just going to build it out of a deck of
STD functions with the mutex and a
conditioner variable around it we're
going to have a pop operation which is
just going to pull one item off of the
queue okay in our function we're going
to have a push operation to just push
one item in the queue and notify anybody
who might be waiting on the queue okay
and then we can build a tasking system
where we have count which is the number
of course we have and our vector of
threads okay and our notification queue
a run function for our our threads which
is just going to to create a function
pop the function from our queue and
execute it
okay and then our constructor here for
our test system will spin up one thread
binding it with a lambda against our run
when we destruct we need to join all of
our threads
and then we're going to have an async
operation where we just take a function
and we push it okay so pretty primitive
the one thing we would hang on
destruction right there because we
didn't have a way to signal done so
we'll add that real quick we're going to
have a bool that's done when you call
done we're going to set the bool to true
and notify everybody okay in our pop
function when we're waiting if we get
woken up from our condition of variable
then we're also going to be checking
done right returning false and that's it
that's the complete thing so how do you
guys think this performs I already told
you right right badly kind of barely get
off the mark horrible why is this
horrible makes a good example right it's
easy to write nothing fundamentally
wrong about it it's exactly this model
right we've got a single Q and we've got
a bunch of threads and they're banging
on that Q okay and we've got locks
around it now unfortunately here
understanding that I can transform those
locks into a queue model doesn't help me
because all I have is a Q it locks
around it okay so I need to come up with
a better way
okay so that's X so here is the common
answer I put a Q on each thread that
minimizes my contention okay so let's
write that we can do this pretty quickly
we're just going to have a vector of
notification Q's we get to reuse or same
notification q okay one for each thread
so now our run is going to take an index
into into which q it's actually bound
against when we join now we need to tell
all the Q's they're done
next one and when we're going to do our
push we're going to keep a little atomic
index so we can be pushing from multiple
threads and we're just going to keep
incrementing it but then we're going to
modulo it by our count and so we're just
going to round-robin going through
pushing into into it there okay and if
you notice that's unsigned so wrapping
around on that index is not a problem
well-defined behavior on our overflow
it's a modulo arithmetic okay so how do
you guys think we did on our speedometer
twice as fast is it going to go half
halfway up we're just going to go nobody
so so I actually did profile all this
code I wrote this all profiled it it's
about ten times better okay
barely get this movie okay so and we
have two problems here one is is I can
get a long-running task in one of my Q's
and it can cause a bubble and I can have
my other course sitting there idle okay
so the other is is even in a fully
loaded system I can end up with a fair
amount of contention right where one of
those mutexes blocks and that's you know
upwards of a million cycles to do a full
task switch right in overhead so having
keeping one of those mutexes and
actually having it locked kills
performance okay so here's the next
solution right called task stealing okay
now there are lots of sophisticated
algorithms that are heavily tuned for
doing task stealing but we're just going
to code it very simply here and the idea
with task stealing is is if your queue
you being a thread is busy because
somebody's pushing in the other end okay
ah or it's empty
either you don't really care see if you
can go pull something out to somebody
else's queue
okay so let's do that so and we're going
to take our pop and we're going to have
tri pop here okay so our tri pop is
going to take a tri lock on our mutex
okay so it's going to try to lock will
either get the lock or not if we don't
get the lock we just return false okay
or if the queue is empty we return false
by the way
so we'll do that we're going to do the
same thing on our push just for good
measure okay
so if I'm trying to push something into
a queue and that queue is busy I
shouldn't wait on it okay cause hold a
mutex cause a whole process or cast
square to even do a spin lock okay what
I'm going to do is if it's busy I'm just
going to return false
okay couldn't push it okay so now our
caste system our constructor and
destructor stay the same okay but when
we do the run now what we're going to do
is we're going to do try pop and we'll
just run once round all the queues okay
and then if we don't get anything once
around then we'll just wait where we
were
okay now
on the push side we'll spin around and
it turns out on the push if all the cues
are busy right right you have to push
okay so so what are you going to do well
you could just sit and wait on one cue
but there's no guarantee that that's the
first cue that frees up okay so it's
actually better on the push to spin a
little more okay so I've got a que there
right right and there's a trade-off
there you're you're improving overall
throughput of the system versus
sacrificing a single core to burn it in
the spin loop so whether or not it's a
win in your system depends on what the
load is on your system okay whether or
not you have that core to burn so but
you can tune it quite a bit so now how
do you think we did
okay so way better okay so we got way up
into there now at this point we're about
a hundred times better than when we
started from Oracle's example code so
this is a pretty huge improvement okay
now what I was benchmarking these at was
Apple's Grand Central Dispatch
that's my one point oh right that's my
mindful thing apples Grand Central
Dispatch is a very nice piece of work if
you actually go and read the code and it
even has hooked into Lib colonel and the
reason why it has looks into Lib colonel
is because it understands when one task
is blocked and how to go just ignore
that task and go take care of other
things so you cannot match it with the
primitives that you have in standard C++
okay but we actually came within about
15%
okay of it with some straightforward
code so that's pretty good okay now if
you happen to have live dispatch this is
a standard STD async compatible function
I'm not going to walk through the whole
thing here that sits on top of dispatch
async F which is a call into Lib
dispatch so this is is is fully STD
async compliant including allowing you
to move arguments through the whole
thing and stuff written in terms of
dispatch async so if you're on a machine
where you can use Lib dispatch by all
means steal this code and do
okay now the first time I gave this talk
somebody went and bent my ear for about
three hours and said well if you're
doing anything you should be writing and
writing it in terms of a CEO and a do
tasks because that's been proposed to
the standard and a CEO being a
synchronous i/o and it has been as an
experimental library and I think a zero
is a beautiful piece of work so don't
get me wrong here
but a CEO is not a tasking system so in
fact a CEO doesn't even have tasking
primitives so is this person spending my
ears and why didn't I use a CEO I'm like
well what are you even talking about
okay but you can build a tasking system
out of a CEO and and this is what it
looks like it's fairly straightforward
and that's how it performs almost
identically to my first piece of code
for exactly the same reasons in here we
have a single single service queue that
we are posting work to and it's under
heavy contention and it's getting
pounded upon it is actually slightly
faster by about 10% then my
implementation with a single queue okay
but that doesn't show up as a pixel even
on my speedometer so so that's where a
CEO is so don't let people tell you as
you as a tasking system it's not okay so
now we have a system where we can issue
tasks
so what happens we issue a task which is
some function some work bound to it it
goes and executes we've got another task
that task yielded some object and we
want to communicate that result to
something else that's executing right
how do we do that what's our what's our
form of communication between tasks
all right well this is where futures
come into play okay so we're going to
calculate Fibonacci of a million CPP
inte is a boost a infinite precision
math type so that's an infinite
precision integer okay anybody you have
any idea of how big fibonacci of a
million is so it's big
subin achi is also is often used as an
example in in parallel algorithms please
stop
almost every talk I go to on parallelism
uses Fibonacci numbers uh here's why so
there's a slow way to calculate
Fibonacci which is what most people
demonstrate and there's a proper way to
calculate Fibonacci to calculate
Fibonacci what you need is a power
function so this power function will
take X to the power of n for some
operation so if my operation here is
addition then this does multiplication
okay if my operation is multiplication
this does exponentiation okay so this is
a generic form of it unfortunately power
isn't in the standard library but it
should be if you want more information
about how this works there's a great
write-up on it in Alec Stepanov and Dan
Rose's book for mathematics to generic
programming kind of the full history of
this algorithm it's also known as
Egyptian multiplication or the Russian
peasant algorithm okay so what I want to
do then is I want to take a particular
matrix which oh there it is I want to
take that matrix which is 1 1 1 0 which
is known as the Fibonacci matrix and I
want to raise it to this let me back up
didn't mean to go that far that's
Fibonacci of a million
slow okay
so I want to take a Fibonacci matrix
raised to the nth power and that will
give me the Fibonacci number of n plus 1
in the top left Fibonacci n on the two
diagonals and n minus on the bottom
right ok so I can actually take
Fibonacci here of n minus 1 and pull it
from impulsive iÃ±aki n from the top
left okay so with that code and now you
see the delay here it takes a key note a
minute to prep this this is the result
of Fibonacci of a million okay it took
point seven two seconds to calculate on
my old on this laptop on my old 2013 or
so MacBook Pro it's two hundred and
eight almost two hundred nine thousand
digits and and it calculates and and
that's just running single threaded okay
so I can calculate way bigger Fibonacci
numbers then then people do in the
parallel examples the reason why bugs me
is because I've actually seen people use
the bad algorithm that people use to
demonstrate with parallel algorithms in
actual code okay stop so try to only
give real examples so we'll calculate
seven oxy of a million with our future
so what's our mental model of futures
okay what we're doing is we're taking a
function and we're slicing off the
results okay
so what I end up with is a function okay
still take some arguments but has no
result and a future which is a token for
the result and that future will appear
sometime later okay so that's the model
the futures it's separating the function
from the results of the function and
having a communication channel before
them now the nice thing about futures is
they allow minimum code transformations
to express dependencies
okay so if I have a future and oh the
other nice thing about futures and C++
is they can marshal exceptions right so
if I have a future and I fail this will
end up printing failure right so across
threads my exception can be thrown in
one thread and caught in the other
go there and then I call future get and
let's just stop right we just block that
thread while we wait for the results STD
futures and then the result comes over I
hate stops right unfortunately C++ even
14 lacks continuations with the
continuation instead of saying dot get
and stop you say when the results for
this is available then do the next thing
if you've done JavaScript programming
with with promises it's the same idea
okay
so along with that C++ 14 lacks joins
which are win all and win any when all
these features are available to
something it'll act splits which we'll
talk about more cancellation and
progress monitoring in C++ 14 features
don't compose particularly easily to add
these things okay so what's the model
for continuations well we've got is a
function yielding a result which we want
change to the next function yielding
some results and so on
okay now STD future has two additional
problems
so first is that it consumes one
resource right which we talked about
increasing contention but it possibly
deadlocks if you have a tasking system
and any non subsequent non-dependent
calculations on the task girls are
blocked
now even though C++ 14 doesn't have the
primitives boost futures has been with
TS concurrency which is following to the
standard has been TVB has flow graphs
ppl has chain tasks CCD has serialized
queues and groups these are all things
to solve the same problem as
continuation
okay so how can get caused a deadlock
let's say I have a task and it issued
another task and then it waits on a gift
that other task will never complete
because it's not coming out of the queue
so if I'm on a single threaded system
okay then a future get is an immediate
deadlock this is why promises in
JavaScript do not have git or dot wait
they only have continuations in a single
threaded system it's an immediate
deadlock okay well what if I have two
threads that guy issues two tasks one
task goes into the queue he waits for it
he's deadlocked well the other task a
task steal unless he issues a task and
waits for it and then we're deadlocked
okay and so on right
it's very difficult to use future like
it is very difficult to use safely in
the presence of a thread pool an STD a
sink it's undefined whether or not it's
a thread pool or not okay that's a
problem it's not just yet it's any
conditional blocking right such as
condition variables weight or
problematic in a task system they're
exactly the same thing
somehow I dropped a negative here this
should say do not call s to be future
get or STD future wait when the
originating task or any subordinate task
is on the same queue even if it's a
concurrent queue or a thread pool that's
the rule
for using them safely okay which means
that I can issue a task within C++ with
async from my main thread and call yet
on the future okay but I can't call get
on a future from anything that was
queued through STD async
okay so you see this problem in every
tasking system it will have some kind of
warning like this so I could have put a
whole list of them up there but this is
from lib dispatch I've kind of been
using live dispatch as an example you
should never call dispatch sinc what's
just batch sync it's the equivalent of
queueing something into the queue and
then waiting for it immediately right
never call dispatch synch from a task
that is executing in the same queue that
you are planning to pass to the function
this is particularly important for
serial queues which are guaranteed
deadlock but should also be avoided for
concurrent queues okay
why because depending on how the stars
align it will be a deadlock now how did
I discover this well taking a heavily
threaded piece of code and taking it
down to one thread it was very apparent
where all the dead locks were right
there but then we also realized that hey
if we took it to mobile where I also
took Lightroom engine to mobile with two
cores we were seeing a lot more dead
locks
okay when we actually finally figured
out what in the world the problem was we
realized that even on eight cores and
sixteen core machines the system under
heavy load would occasionally deadlock
it was just rare enough that you
wouldn't see it okay so so but in our
bug backlog when we went back through it
was like oh yeah this this is that
problem this deadlock is that problem
okay so a huge thing okay so this is
what a dot then continuation looks like
right we can calculate Fibonacci and
then once the Fibonacci gets we can
serialize it we're going to go ahead and
put our weight in here because we're
executing this on the main thread not
over over on our async it's useful at
least for debugging so so now we can do
it and then print it
okay so bins are are the equivalent of
completion handlers okay and in the
standard committee a lot of times
there's debate over whether or not the
futures are the right model or whether
or not completion handlers are the right
model okay so so conceptually the two
are interchangeable okay so the
advantage with the completion handler is
that there's no need to synchronize
between invocation and and and setting
it because it was set in advance the
completion handler gets passed into the
function okay whereas a future there's a
little bit of synchronization that
happens because you don't know well your
asynchronous task is setting when you're
setting the value is somebody attaching
a continuation at that exact moment in
time okay so there's a small amount of
synchronization that has to happen but
the advantage with the future is that
you don't have to know where the results
are going in advance right so so those
are the trade-offs so you can actually
express one in terms of the other and
vice versa if if you if you build build
a system correctly so my answer is
always when people say do we want
futures or completion routines I'm like
yes that is my answer I want both okay
now the other thing you find when you
start to construct these things is we
need joins okay so what I want to do is
I have two futures and when both of
those futures are available then I want
to issue the nest nest next task now a
lot of people have the wrong mental
model about this what they think is is
then I will have my continuation I get
get one result in and then I will block
and wait for the other result but that's
not how joins work at all
what happens if you keep a count of how
many things that you're waiting for that
are pending okay and when I get one
result in I issue the task or when I get
one result in I'm okay when I might when
my count goes to zero then it's the it's
the setting the result decrement
the count to zero that causes the next
task in the chain to fire so you're
never blocking and waiting you're just
counting down
okay so this is what it looks like with
this is using boost when all I really do
not like the syntax here and that a win
all right I'm taking a win all of two
futures and then a continuation and my
argument F coming in there is a tuple is
a future of a tuple two futures which is
why you get the in order to print my end
I've got get get x get get right right F
is a future tuple futures and the result
of this is a bunch of digits we're not
going to print it so the other thing you
want to be able to do is split
so here now this is still the boost
slash proposed C++ 17 model so it seems
like the way you would do a split is
just attach to continuations to your
future so I've got X and then I want to
say then go multiply it by two and also
when I have X divide it by five that
looks like a split
yeah just abort okay the reason is the
first then actually consumes X so trying
to add hacks a second X a second then is
a cig abort that's not a particularly
friendly thing now a shared future lets
me copy them around okay and do multiple
get operations but it still doesn't let
me do multiple event operations okay
well that's horrible so now if I've got
a shared future in my hand I don't know
whether or not I can attach a
continuation to it okay so my opinion
the future should behave as a regular
type it's a token to the value so you
should be able to to copy them to get a
split but in absence of that we could
write a split so let's see how we would
do that okay so this is what it would
look like to call what we're going to do
is we're going to take X we're going to
split it okay which is going to leave
our X as a future and give us a new one
to which we can attach our dot then okay
so we can split it and then we can
attach it and if we wanted to keep going
we keep splitting okay we could just
keep splitting X
so and then we can write this and that
will print that right the first one beam
times two and the second one divided by
fifteen okay so let's see how we would
build that well first we've been talking
about futures but we haven't been
talking about the other in the futures
we can c++ is known as a promise okay so
a promise is the sending side of a
future so if we create a promise X then
we can get a future out of it and if we
set the value 42 on our promise then we
will get the value out of our future
okay so so that's the pairing there so
internally async STD async is is
creating a promise that we'll turn our
42 okay so here's how we're going to
write our split we'll walk through it
here okay when we initially come in
there is a promise somewhere that's in
gray a gray circle okay which is going
to set our future which is X okay so
we've got a promise somewhere and we're
hanging on to X so we're passing in a
reference to X now the first thing we're
going to do is we're going to move that
future out of X to get it out of the way
okay so the promise still points to the
move future now which is in temp then
we're going to construct a new future
and get a new promise for it and put the
new promise into X okay so now we've
rewritten X to be our new future okay
then what we're going to do is we're
going to take our temp
okay which is our future we're going to
put it in to our lambda here so now it's
underscore temp we're going to attach a
continuation to it which is going to
return our results we're binding P right
we are moving the promise we created in
the dotted line there eight so we we
took P we moved it over into underscore
P okay
that's the result we have the original
promise which points into our underscore
temp future okay which has a
continuation which is going to set the
value on T my underscore P set value
okay and it's going to return the value
right so there's the copy right there so
that's how you implement the splits okay
then we do the same thing to split the
exception right so we want the exception
to also go down both paths okay so if we
have an exception then we're going to
send an exception down both paths you're
not sure we write a split
okay now here's a tough thing this is
something I want with futures but
they're not in C++ 17 and I don't know
of a good way to compose them into the
sequence plus 17 futures basically what
I want is now that I can split split my
futures right right and I can join them
so I can build these complicated graphs
is when the last piece of code see I
think I have a graph here when the last
piece of code lets go of a future like
the top right one there okay then if the
task associated with that piece of code
hasn't started to execute then I want it
to not execute and to free the resources
okay which then leaves two futures which
are no longer needed and so those
futures go away and I'm left with this
right so I eliminated one task from my
system that's executing now when you're
developing interactive software this
comes up a lot the user does something
and then the user does something else
and you're like okay well forget about
that old thing you don't want to keep
doing it so a cancellation model is nice
because that wasn't there and because
even the boost futures with
continuations will deadlock on a single
threaded system which I consider a bug
they shouldn't but they do it's a part
of their their their implementation I
ended up writing this experimental
library which is available on that
website and I've got futures in there
that support multiple continuations and
they can be copied they're optimized for
our values so you can have move only
types with them it supports joins for
winning and win all it has cancellation
on destruction as well as an explicit
recess reset and it has a detach so you
can detach a future meaning that but now
I can destruct this but I still want
whatever it's associated with to
continue processing right just like you
would detach a thread and say keep going
even though though I'm destructing my my
toe
and hold on to it so I put this library
out there and I have to thank Felix
petrosini who's in Germany who's been
just a massive contributor and so when I
wrote it I wrote it to the extent that I
needed it and going to show it to the
standard folks and say here's some
issues with the direction you're going
and a Felix is really kind of flushed it
out and and our goal now is to release
kind of a 1.0 version sometime earlier
this year that's there but futures
aren't enough right so the next thing is
to talk about channels and I am probably
way over time aren't I at this point I'm
not seen any hands
since this took me a little longer to go
through then I wanted to I will stop
here Kevin's already talked some about
the basic concepts of channels channels
are also in my library so if you want me
to to discuss them just see me outside
of this and the last thing I will just
flip through slides very quickly but I
wanted to finish on that slide won't let
me go too quick all these great
animations
channels it is this what we're building
here is dependency graphs within the
code and it turns out there's a lot of
information in the graph right when
you're building channels and you're
building futures you're building
dependency graphs and there's a lot of
information that's in the graph but
these are are what I call incidental
data structures right there isn't is no
place within your code where it's like
here's my dependency graph okay which
makes it makes it nearly impossible to
mine that graph for values especially if
you want information about all the
possible states of that graph
so maybe constructing these dependency
graphs is is not the right approach
instead maybe we should be actually
writing the graph to script descriptions
and and compiling those and statically
validate them and I've got a whole body
of work and a bunch of published papers
on something called property models
which is exactly that which is how do
you describe dependency graphs and flows
between states and reason about them
outside so if anybody wants to talk on
that way outside the scope of this talk
but I'd be happy to and there are some
slides there for my papers and
presentations and the library and what
we're talking about here largely is
communicating sequential processes which
is which is from Tony Hoare and his book
is now out of publication but freely
available online and so that's the link
to CSP so thank you and sorry for not
paying attention to the time and running
over</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>