<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Performance Investigator's Field Guide - Sasha Goldshtein | Coder Coacher - Coaching Coders</title><meta content="The Performance Investigator's Field Guide - Sasha Goldshtein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Performance Investigator's Field Guide - Sasha Goldshtein</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LgAtQF_XvTw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right mm welcome everyone thank you
for coming how many of you are from
London all right well thank you and
thank your city for the hospitality and
the beautiful weather I was a little
worried like 7 degrees Celsius is the
lowest we get all year in Israel where I
live so this is a this was a concern for
me but it's lovely outside and thank you
and I'm very happy to be here for NDC
again so my name is Sasha
I work for an Israeli training and
consulting company called Sela and for
the last 10 11 years or so I've been
doing mostly performance and debugging
work across a variety of application
types but dotnet has always been my main
focus and Windows and Linux are the two
places where I've been doing dotnet
performance work and in this talk which
is slightly less technical than I'm used
to that I'm used to giving will talk
about the whole performance
investigation process I want to share
with you some of the things I've been
doing a couple of examples of what I
call performance investigations and
hopefully you'll be able to come out of
the start with some ideas for how to
actually do performance work in your own
organization and maybe some tools as
well that could be useful but this is
all a bit higher level and so if you're
interested in more specific references
and examples I'd love to chat with you
during the event so again the general
plan is the strategy and tactics for
performance investigations mostly in the
field by which I mean as a consultant I
get called to different customers and
magically expected to find or sometimes
fix performance problems as well so it's
a it's interesting and very challenging
work and there's really I think a few
highlights that I can share from the
whole process so I want to talk about
the performance investigation process in
general methods and anti methods a
little bit of tools and how to avoid
getting lied to by some of the tools
that you might be using so let's start
with the structure of a performance
investigation by the way everything I'm
talking about is kind of you know like
written in blood but it's just my
experience and the experience of other
people on my team so diff
practices might work for different
people and I'll be again delighted to if
you could share your own experiences
afterwards as well so here's the
structure of a performance investigation
how I usually prefer to conduct them so
first obtain the problem description and
that sounds a lot easier than it is if
you have been doing performance work in
your own organization if you've been
talking to customers and users at all
not just for performance issues but in
general you know it's all it's not
always easy to extract the real problem
description out of humans or systems
also so we'll talk a little about
examples of what a good problem
description looks like and what a
totally non actionable problem
description looks like then what I like
to do especially when coming as an as an
external consultant is try and build a
system diagram try and understand all
the different interacting pieces now if
you're working in a single team or a
single product you might think it's a
waste of time to build a system diagram
when you start a performance
investigation because I mean you already
have one you have the design documents
you have the architecture blueprints for
whatever system you have a lot of times
though especially when you're doing a
new investigation you discover that you
know the system implementation has
slightly drifted away from the original
design or from whatever you have
documented on your internal wiki and
this is where building a diagram of the
actual system as it is currently in
production and that's the thing you're
currently investigating is quite useful
then I like to run a quick performance
checklist and what exactly to put on
that checklist we'll talk about this
little during the talk but this just
gives you a high-level overview of
resource utilization across the system
how the different components are
behaving sort of lets you know which
parts are ok and probably don't warrant
additional attention and which parts do
really need a closer look and might be
causing the problem you're looking at
and so this is where you hopefully are
able to focus a little more closely on
the specific area of the system that's
worth further investigating and then you
investigate thoroughly and this is like
the vaguest step of the whole process
you investigate and you hopefully find
the performance problem
you find the root cause and I'll just
spend a couple of seconds on that by
root cause I mean the thing that
actually fixes the problem in a way that
will never have the same problem
manifest again so not just you know
fixing the symptom by maybe scaling up
the system or adding a little more
memory but actually finding the root
cause that we can now eliminate and fix
that problem for good now some things
are just naturally fixed by scaling up
or adding memory but often we tend to
fix the symptom and not the actual
problem and then the problem really
doesn't go away and we end up spending
more time on it in a week and that's
just a waste of for everybody's time
then we fix the issue we resolve the
problem and we verify that we have
actually resolved it it's now gone and
that's also something that people often
forget you know we're all excited about
just having found the problem and sort
of found the three lines of
configuration that have to be changed to
fix it and then we forget to make sure
it's really helped and we go away and
then we have to come back in an hour and
just go through the whole thing again
and finally and this is super critical
and important and again often forgotten
is to do a post-mortem document what you
did document the whole process and I'll
talk briefly about the post-mortem at
the end of the talk and what should be
on it
so this is the general structure and
let's start with problem descriptions
and an example of a performance
investigation that went quite terribly
wrong and what was the cause of that so
a couple of examples of what I think are
not so good problem descriptions in this
one the customer complains that the
application feels slow users can't
really put their finger on it but it's
bad my taking a look again in in your
organization if you are sort of the
performance expert or the go-to person
for performance issues you might get
this sort of thing often as a consultant
I get this sort of thing often as well
it's highly unacceptable I don't know
anything about your application or your
system what do you really expect me to
do with this another example which is
also quite common I think again in
organisations as well as with external
consultants is we have a budget for
fixing performance issues
so let's now use that budget because the
year is coming to an
we have that extra budget even if we
don't use the budget for this year we're
not going to get the same budget for
next year so we just got to use the two
days we have for finding and fixing
performance problems this is better than
not investing at all in your performance
problems but it's not ideal this is not
the way it should work right you should
factor in the performance work into the
whole project lifecycle and not just you
know when you have two days of budget
left at the end of everything just sort
of try and see what you can do with
these two days but that also happens a
lot and of course again there is no
guarantee that we will actually find and
fix anything if we only have that little
budget at the end a couple of slightly
better examples I think so this could be
an email from like an automatic system a
template and this says so since for 25
a.m. the 95th percentile latency to the
front end asp.net application increased
to 1,400 milliseconds the previous value
was 60 milliseconds and this is
consistent across geographies
auto-scaling has kicked in and didn't
help now if you have an automatic it
automated system that can generate this
sort of email that's like really awesome
or it could have really been a human
operator but anyway this tells you
what's wrong or at least it tells you
the symptoms it tells you what person on
the other side is expecting you to do so
let's say if we bring the latency back
down to 60 milliseconds you know they're
going to be happy at least with this
particular alert and we know what
mitigation steps were already attempted
so that also saves us a bit of time like
yeah we tried scaling and it didn't help
so maybe the problem is somewhere else
another example which is less detailed
but I think is also quite good and
that's just a real example just slightly
rewarded in the latest version of the
software clicking from the patient
details screen in some kind of medical
software to the CT screen takes 11
seconds in certain hospitals and this is
unacceptable it has to be under two
seconds because our technicians they
don't have that sort of time to waste
so they can't wait 11 seconds for the CT
screen to open so this is something
you've gotta fix right now so again we
know what the
person on the other side of this email
is expecting they want this whole thing
to take two seconds it's now taking 11
seconds could be for a variety of
reasons but at least we know what the
problem is
so having said all that about problem
descriptions let's take a look at an
actual derivation of these values of
these problem descriptions of of the
metrics that are getting violated in in
all of these cases so this is like
imagine this conversation with a
customer it's rephrased from a really
great blog post called how to not
measure latency which i've linked at the
end of the slides so let's say we have
this conversation with a customer or
some other person on your team trying to
figure out what the performance metric
violations would look like for for a
system so I asked what are your latency
requirements do your customer the
customer says we need an average
response time of 20 milliseconds now
let's assume for a moment this is like a
web service web application let's say a
web application that serves actual web
pages to the clients so they're saying
like 20 milliseconds response time it
sounds a little fast to me but maybe
like there was a reason for that so I
asked what's the worst-case requirement
and and where it's what is like what is
the reason that's your worst case
requirement the customer says we don't
have a worst case requirement we just
want it to be 20 milliseconds okay so
I'll go like so some requests can be
more than five minutes you can still
have an average of 20 milliseconds but
you can have some requests take more
than five minutes
well of course not like the worst case
is really 100 milliseconds that's
something we can't in no conditions go
or go over even if it's just two
requests per day right so this is like
bargaining for it all right fine nothing
worse than two seconds two seconds is
really the the farthest we can go how
often is a 500 millisecond request
response time acceptable so for example
if you have 96 percent of your responses
take one millisecond and four percent
take 500 milliseconds that works out
trust me
221 milliseconds average so it sort of
works out and nothing is greater than
two seconds so is that an acceptable
distribution of respond
times and and I mean you can see where
I'm going right I'm asking not just for
a number that you you just sort of pull
out of the of thin air I'm asking for a
distribution which is actually based on
what the customers are going to
experience with the system but it
doesn't always end well especially if
you're not prepared to to go through
this whole routine but what I'm saying
is what we need for monitoring for
alerting for generating these problem
descriptions is really more than just
one made-up number like twenty
milliseconds latency or twenty
milliseconds average or twenty percent
CPU usage across the the system so
here's an example like for a full text
for a full text query engine we could
say 90% of the queries should complete
under 200 milliseconds 99% under 600
milliseconds and all the queries should
complete under two seconds and anything
higher than two seconds that that's a
timeout it's an error and I don't want
to see that ever so this is already
better and hopefully these numbers that
that this distribution is describing are
not just made up in a vacuum but they're
based on actual users and business
business cases so for some systems it
might be really hard to actually extract
this information out of your users maybe
you're gonna need usability tests maybe
you're gonna need to talk to business
people what I'm saying is we as
developers just can't make up these
numbers and operations people they also
can't make up these numbers we have to
get our alerting we get we have to get
our problem descriptions we have to get
the definition of what is a problem in
general from an authoritative source
which is hopefully again tied to our
business requirements and once you have
those metrics described as some kind of
distribution which we'll talk about a
little later then you want to monitor
these things with some kind of dashboard
APM solution cron job that runs in the
background and gives you alerts we'll
talk about that little
in the rest of the talk but again for
these problem descriptions to be
actionable for the numbers to make sense
for us to know what to fix and how often
it's happening we need to have thought
about these metrics ahead of time so
let's take a look at a specific example
of a performance investigation gone
wrong
and to do that I want to just stop
briefly about five different
anti-patterns for doing performance
investigations for things people do
along the way for common mistakes and
you'll see some of them illustrated in
my example so the number one thing that
usually goes wrong and a lot of in a lot
of my work is making assumptions we are
all very vulnerable to making
assumptions about software about the
world in general and so often we already
know where the performance problem is
before we even look at the before we
even look at the system because it looks
like the problem from last week because
it's John reporting the problem and it's
always the same thing when John is
reporting the problem we have some sort
of guesses and these assumptions can
lead to a colossal waste of time and
again I'll talk about a specific example
coming up trusting instincts and
irrational beliefs is sometimes even
worse than making assumptions
assumptions at least you can sort of
contradict but irrational beliefs they
are really hard to fight and by rational
beliefs I mean things like the storage
system is never at fault it's never the
problem with the storage system or it's
just that the cellular connection is so
slow in India that you know we should
all just ignore performance issues
coming from from cell users in India so
these sort of rational beliefs or
instincts that we might have about
software are again considered extremely
harmful and a couple of additional
examples looking under the streetlight
is something I'll illustrate as well and
what I mean by that is looking where
it's convenient for you to look so where
you have observability into the system
if you only have observability into a
specific area it's completely natural to
jump to that area first and start
investigating that because you simply
don't have the tools or the knowledge
right now to investigate other things so
that's also a natural thing to do but it
could be again it could lead to a huge
waste of time
and there's some additional examples
which will illustrate later so let's see
a specific a specific case how I failed
to diagnose a performance issue I'm not
super proud of this but it is a good
case study I think of things that can go
wrong on a performance investigation so
here I am sharing it sharing this
personal failure with you so what we had
is a document management system with
several thousand internal users inside a
specific organization and it was
basically a document management system
right so you could load documents from
some sort of network storage you could
modify them save them conflict
management all all the sorts of things
that come with document storage and the
network storage itself was a net up
system accessed through SMB through file
shares and this is like the general very
very simplified architecture of the
system so you have clients connecting
through WCF to an application server and
the application server is talking
directly through a file share to a net
up storage appliance which is hosted on
the same network so it's all a fairly
tight and closed internal network not
over the internet or anything
now some parts of this system I could
investigate fairly easily and I had the
knowledge and the tools to do so but
some parts like specifically the net up
storage appliance I simply couldn't
physically access at all and that's a
pretty common theme I think with storage
appliances they tend to be those black
boxes that you're not really allowed to
access and investigate directly and sort
of it leather investigation a certain
way at least in the beginning suspecting
the application server suspecting the
clients were causing the performance
problem now what was the performance
problem anyway it was actually a little
hard to describe but we managed to
narrow it down to certain operations on
certain documents leading to timeouts so
it wasn't immediately visible with like
an error message or anything but we
could figure out that the subsequent
problems were caused by a timeout in in
some load and save operations all right
so here's how it starts we processed a
lot of network traces trying to figure
where the timeouts might be coming from
and I was able to see that the mean
latency to storage from the application
server going back to the previous slide
from the application server to the net
up storage appliance the mean latency
the average latency was 11 milliseconds
but we observed peak values of up to
1,200 milliseconds so over a second and
that could potentially lead to the
timeouts people were describing so we
had to look at some information from the
storage appliance itself in the storage
latency counters so we did have to look
at that black box on the right all right
so a couple of days later we got some
information from network engineers
looking at that net up storage appliance
and the network engineer says his name
was network Mac network so he says if
I'm not mistaken the network latency
maximum was 10 milliseconds with an app
with an average of less than one
millisecond now this was computed by
looking at the network ingress point on
the storage appliance itself but
discounting the actual storage disk
accesses so now we have the full latency
of 1200 milliseconds to storage and we
have the specific part of that which is
network related and that seems to be 10
milliseconds only so that leaves eleven
hundred and ninety milliseconds
unaccounted for and we suspect they are
in that storage subsystem all right so
the net up consultant who is the only
person in the world equipped to monitor
and diagnose issues with net up storage
appliances he will be in next week but
the system administrator we have who is
in charge of that storage appliance says
there's no way the net up latency is
greater than five microseconds now five
microseconds for all disk accesses
sounds really optimistic to me but I
don't know much about net appliances so
I just had to trust them until the net
up consultant came in then he came in
and he said it's actually five
milliseconds and not five microseconds
but that's almost the same thing and in
any case it's not the storage system
in any way because it's it's it's
nowhere near the latency that you're
describing that might have caused the
timeouts we're talking about five
milliseconds only and then I asked is
that the average or the peak value the
five milliseconds because I already know
that the average time for loading and
storing documents is pretty fast anyway
it's like 1011 milliseconds across the
board but we do have those rare outliers
which are over a second so when you tell
me that the storage appliance is five
microseconds five milliseconds whatever
is that the average latency for saving
or storing documents or is that the
worst case and then we got the following
reply it's the maximum average value
over sixty second intervals I love this
I I gave this presentation couple times
and I still don't understand what this
means and to this day we don't know what
exactly it means because we got a lot of
conflicting answers when we try to press
it so that's what we got again maximum
average value over 60 seconds intervals
now what could it be right so maybe we
take the average value over 60 seconds
so we aggregate for 60 seconds all the
latencies of storage accesses we get the
average value of that and then we take
lots of these 60 seconds intervals and
we gather the maximum of all these
averages that's one option
maybe we get the maximum over each
60-second interval and then we do an
average of the maximum values that's
another option maybe it's something else
entirely we just don't really know and
it's not because it was all done over
email we actually had a few phone
conversations about this as well and we
still don't know exactly what was meant
here by the maximum average value of vs.
yeah it is probably a communication
failure of course it is thank you but
yeah that's where we were and then I
again not being a NetApp expert really I
try to look up online if there is a way
to maybe monitor the storage system a
bit more closely and I found that net up
actually unsurprisingly has performance
counters that you can enable
over SSH and just read them whenever you
want right and these include the average
read and write latency and that's in
milliseconds and that's gathered every
second so you can get every second the
average read and write latency for net
appliance it's not exactly what I wanted
but at least it was updated every second
unfortunately we ran out of budget for
the net up consultant at this point it
turns out they're really really
expensive he had a bag that said net up
or all over it it's a it's a very
expensive bag as well so at this point
the investigation just ended and a few
months later I actually heard back from
that customer and they were able to work
around the problem not by replacing the
storage appliance or by figuring out why
the storage maybe was slower than
expected but by retrying automatically
in the client like under the standard
covers by hooking some api's and just
injecting retries until it worked
and it meant it meant deploying
additional code to every single
workstation and doing that work on the
client but hey it did solve the issue
they never ran out they never ran into
these timeouts ever again so I consider
this a serious failure from a lot of
aspects right so there were assumptions
made a lot of irrational beliefs along
the way communication failures during
this entire investigation the whole
budget problem was a problem so these
are some of the things that could go
wrong and I think hearing this story
sort of gives you an idea of what to
expect I have another story coming up
which is a little more optimistic but
that that sort of thing definitely
happens and will happen to me again and
after all that we talked to the net up
guy again and he said again you know I
understand that you did this workaround
and it's all working fine now but I mean
I'm telling you there is no way it could
have been more than five milliseconds
because that's how net up works there is
no way for net up to actually work
slower than five milliseconds this is
the sad ending of this story all right
so we talked a bit about some of the
things that can go wrong in a
performance investigation I want to
touch really
on the other three things look under the
streetlight random tools and blame the
tools so here's an example of looking
under the streetlight if you have a
bunch of expensive tools that give you
observability into into a certain part
of your system you might be just using
these tools automatically for every
problem you now have and a classic
example of that is CPU profilers CPU
profilers are really abundant and
they're not really hard to use and this
is an example of output from the visual
studio CPU profiler it's really easy to
run and get results from an application
and in this case we run it on a web
crawler which is like downloading pages
of the web as fast as it can and profile
the results point to a particular
function called console dot write line
which is taking 66 percent of the
applications CPU time well does that
mean we have to fix console.writeline
or maybe replace our usage of
console.writeline with some other more
efficient method not necessarily because
maybe our application is not really
taking lots of CPU anyway and if we run
a CPU profiler we're looking into only
that particular component only that
particular resource which is the CPU and
we are ignoring time spent doing i/o
network requests and all other kind of
activity so this again is an example of
using a tool that gives you
observability into a specific area of
the system because that's easy or
because you've paid a lot of money for
license for that specific tool and it
gives you some results but they're not
necessarily the results that can help
resolve the issue you have another
example of this is sort of not trusting
your tools or maybe misunderstanding
your tools this is a really simple
example of a console application I wrote
which runs for two seconds and then
throws an out of memory exception but
then you look in task manager and in
task manager you look at the memory
column and it says the application is
only using like ten megabytes of memory
so could it be that the dotnet runtime
is so
bad that you can't allocate more than
ten megabytes of memory before it runs
out of memory and frozen out of memory
exception or maybe task manager is just
lying to you and like misrepresenting
the memory usage of that application
there's a lot of theories that you could
have at this point in unless you know
already what's going on and what's going
on is that the memory column and task
manager only represents part of your
applications memory usage specifically
by default task manager shows the
private working set which is memory not
shared with other processes and memory
which is resident in physical memory
which is sitting in RAM and not possibly
paged out so this is just part of the
applications memory usage and that could
be why we're still run into an out of
memory exception even though the private
working set is so tiny so it's just
about misunderstanding what the tool is
saying or maybe not exactly aligning
what the tool is saying with our model
of how the application or the system
works and another example of a pretty
common problem is just sort of throwing
away the tool completely and here's a
classic example of that
so I ran notepad and then I attached a
really popular sysinternals tool called
VM map VM map if you haven't used it
it's really it's really super cool you
attach it to a process and it tells you
exactly a breakdown of that processes
memory usage so how much managed memory
is there how much memory is used by the
heap by the stack by loaded dll's it
just breaks down the whole thing into
components and then like vm map says
about notepad that notepads total memory
usage is two point something terabytes
and I mean it's it's notepad and I
haven't even opened the file it's just
plain blank notepad so that couldn't
possibly be right
it means VM map is lying to us maybe it
wasn't updated for the latest version of
Windows
maybe there's a bug in notepad maybe
there's a hacker attacking your system
so in that sort of case you might just
discard the tool completely throw it
away and never use it again
but actually VM app is telling the truth
on newer versions of Windows there is a
security feature called control flow
guard which leads to two terabytes of
memory being reserved in each process
even if it opts into that security
feature and this has just reserved
virtual memory it's not actual memory
that you're paying for it's not disk
space it's not physical memory it's
basically nothing it's just a reserved
memory space which is used by that
particular security feature so every
process will have two terabytes of
memory if you look at reserved memory as
part of your memory so it's not lying to
you via map it's reporting the truth but
you have to understand what it's saying
and maybe just maybe you have to trust
the tool first and try to understand
what the tool is reported but I will
have in just a couple of slides an
example where you shouldn't trust the
tools so it really goes both ways
sometimes so these are again some of the
common mistakes people make when
conducting a performance investigation I
say people but I do that as well I mean
these are all very natural things to do
trusting instincts irrational beliefs
making assumptions using tools because
you've used these tools before or
because they're handy and give you
observability into a certain part of the
system so let's talk about the method
which is slightly more rigorous it's not
a lot harder but it works well in a lot
of cases I haven't invented it it was
popularized by Brendan Gregg who's a
performance architect working at Netflix
today and it's called the u.s.a method
and US a-- stands for utilization
saturation and errors I'll talk about
each of these in a moment and then I'll
tell you what the method is so
utilization I guess is the easiest
metric to define it's a metric of load
which proportion of time a particular
resource in your system is working as
opposed to the time it is idle and not
doing anything for different resources
you could define the utilization
slightly differently but that's the
obvious metric of load or usage
utilization so for a CPU it's the
proportion of time the CPU is busy
executing instructions for a web
application could be the proportion of
time the application is
servicing requests and not just waiting
for additional requests and so on
saturation is a metric of overload how
overloaded or queued up that resource is
so if you have work waiting for that
resource to become available then you
have saturation for example for a web
application if you have web requests
HTTP requests sitting in a queue and not
getting processed immediately then you
have saturation in your web server if
for a disk drive for a disk drive if you
have requests writer reads waiting for
the disk to start servicing them because
the disk is busy doing other things then
you have saturation for your disk and so
on and finally errors are just that
errors exceptions failures faults
whatever you call them so for each of
the resources in your system how it will
resources as well as software resources
you want to look at utilization
saturation and errors model these things
measure them look at them for each of
the resources in your system to
understand what's going on and this is
the initial checklist stage of your
investigation you haven't done any
thermal analysis yet you're just getting
a reading of how the system is behaving
and this is it's it might sound
completely trivial and you're already
doing it and you have wonderful tools
and dashboards that give you all this
information but if not then you
absolutely have to start there if you
don't have that visibility into each of
the things you have in your system then
you have a problem whenever you perform
a performance investigation you will not
know what to look at you will not know
which potential black box could be
causing problems for the rest of the
system so just a couple of examples for
how to restore TSA's this is like a
typical workstation you have a CPU you
have a disk drive you have a network
card you have physical memory you have a
graphics card attached as well for each
of these things and for the buses for
the interconnects between these things
you can look at utilization saturation
and errors I've given a couple of
examples but for Hardware they should be
relatively easy to derive
and they should be also relatively easy
to monitor because I mean most hardware
is is just generally the same so CPUs
are CPUs and we have pretty standard
tools for monitoring CPU utilization or
CPU saturation but software you know
software is a whole zoo of different
frameworks and components but we can do
it for software as well we can identify
the software resources in our system web
servers load balancers backends
databases caches and so on and for each
of these things we can look at
utilization saturation and errors as
well and for software if it's well
designed software and monitoring is part
of the whole thing the whole framework
you're buying then you'd be able to
easily monitor these metrics for that
particular software if it's not that
well designed it might be really hard to
extract us information out of a
component so this is again something
that obviously depends on the specific
pieces that you're using to build your
system but you should have that sort of
visibility into them as well now I've
given here in the slides just for
reference aus a checklist for some of
the hardware resources on Windows
systems is just an example and you can
find this sort of thing on the web these
are just various performance counters
that give you for network cards for
disks for CPU for memory the u.s. a
matrix which you can easily monitor I'll
give you the hood the whole set of
slides so you don't have to take photos
of the checklist now and you can
automate the process of collecting this
checklist and getting alerts based on
that checklist just a couple of very
basic rudimentary tools if you don't
have anything super fancy to monitor
your system type perf is a command line
tool built into Windows which can
generate CSVs of these different metrics
on the console and perfmon is the
windows performance monitor which can
collect performance information for you
in the background and it can even issue
alerts it can for example run some sort
of application when one of the Metro
is breached and of course there's plenty
of third-party agents that will collect
this information for you and many of
them are already tuned to collect the
USC metrics already so once we have that
kind of high-level set of things about
our system we can perform a performance
investigation more easily and I want to
show you one example where we really
didn't need a lot of information to
diagnose the problem but we did have
access to metrics we did have access to
the system and that helped solve the
problem at the end so this is a story of
how I found a bug in the CLR 2.0 garbage
collector and that sounds like really
unlikely a bug in the garbage collector
when they do happen once in a while
before we blame the garbage collector we
do have to you know make sure exonerate
every other piece of the system but in
this case it really was the garbage
collector this is from nine years ago
eight and a half so it's not a super
brand new story and eight and a half
years ago we didn't have access to the
CLR source code so that was a little
harder to diagnose than it might have
been today anyway the symptom was we had
a packet processing system getting
requests off the wire and doing some
kind of processing before just throwing
the packet away or recording it to a
file and sometimes when processing a
very specific kind of packets we had
hiccups in the application so it would
drop thousands of packets before
proceeding to operate normally sometimes
for hours at the end so it would just
work for a few hours and then it would
have a hiccup of a couple of seconds and
just keep working normally for more for
a few more hours in the logs it looked
like this so it would just work normally
and then boom we have a 500 millisecond
pause where nothing is happening even
with the highest with with all the logs
turned on nothing would be printed just
500 milliseconds of doing nothing now if
you're if you've been doing some
investigations that involve the garbage
collector you might immediately suspect
has to do with GC pauses so the GC is
stopping your application in order to
clean up memory and this is why you're
having these pauses it's not a bug in
the garbage collector Sasha you're just
not understanding how the garbage
collector works so that could be the
case but let's keep talking so this is
what it looked like when we collected a
bit of performance information you can
see two metrics here the blue one is the
number of packets processed per
millisecond so we were processing tens
of thousands of packets per second or
tens of packets per millisecond and the
red line is CPU utilization across the
system and then you can see it's
time-stamped 3,000 something weird
happens we have a CPU usage drop but not
completely drop to about 20% and we have
packet rates drop again not completely
to zero but very close to that and then
after a little time we have CPU spike up
to 100% and packet rates drop to zero so
this is complete zero no packets are
being processed over here and then it
all go back goes back to normal
slightly more jittery because you know
there's some packets to make up for but
generally it goes back to normal and it
could go for hours on end like this okay
so again to make a long story short
another funny thing that we were able to
observe after a while is that these
delays during which no packets are being
processed were always multiples of 250
milliseconds so it was like 252
milliseconds 503 milliseconds 751
milliseconds it's almost as if someone
had some kind of timer or a sleep
statement for 200 milliseconds which was
getting hit somewhere inside the system
so we kept investigating and eventually
and again it's 9 years ago it was a bit
harder back then eventually we were able
to get a thread dump of what the system
was actually doing during one of these
hiccups
during one of these delays and then what
we discovered is we had four garbage
collector threads and they were just
sitting there
waiting they were not doing a garbage
collection yet they were sitting there
waiting for a garbage collection to
start and then we had to application
threads one thread was parsing packets
just as fast as it could and another
thread that was starting a garbage
collection so another thread was trying
to allocate memory and it couldn't and
it hit a GC and it tries what it's
trying to do right now it's trying to
suspend this other thread it hasn't
succeeded yet it didn't suspend the
thread because the GC threads are not
working yet is just trying to stop this
other thread on the left and it's trying
really hard and it's trying really hard
for 500 milliseconds and 750
milliseconds and so on sometimes all the
way up to one and a half seconds just
trying to suspend that other thread and
that looked really suspicious to us so
we started reverse engineering that
suspend function and relevant code
around it today we would just look up on
github at the CLR sources but back then
we reversed it a bit and found the
following loop in there and again
cutting a long story short there was
actually a 250 millisecond timeout in
there so basically it goes something
like as long as we haven't successfully
suspended the thread and we haven't
given up yet wait for 250 milliseconds
to see if that thread was successfully
suspended and if not then sometimes it
would retry and sometimes it would jump
out of the loop and just do a an
aggressive suspension which doesn't
require cooperation from that other
thread so this whole thing eventually
looks like a bug because we're not
supposed to be stuck in this whole
suspension process for so long but I
didn't know it back then I couldn't you
know create a pull request so I think I
submitted an item on connect and I wrote
a blog post about it to see if you know
someone else has ever run into this sort
of thing and then a couple of weeks
later I got a comment on my blog post
from my oneÃ­s Maroney Stevens who's the
lead developer on the garbage collector
at this at Microsoft she's still at that
role today and she wrote I ran a retro
and I do see the suspension problem
we'll look at that and see what happens
and they found a bug in
garbage collector which was responsible
for that suspension problem and they
fixed it eventually and so if you're
interested in the whole story you can
read it on my blog but this is an
example of where you know we didn't make
anything we didn't use any super fancy
tools or techniques we just sort of
methodically narrowed it down a little
more and a little more a little more
until we were able to figure out it's
not really our fault at the end we did
manage to find a workaround as well it's
all on my blog if you're interested so
this is an example that I'm a little
more proud of than the previous story
with the network appliance so now that
we talked about the performance
investigation process itself I don't
want to leave you with some practical
things as well so I want to touch
briefly on good performance tools what
good performance tools look like and
again it's not a very hardcore session
so I'm not going to be showing you demos
of performance tools but I do want to
sort of touch on what kind of tools are
out there and which tools work well so I
think there are three kinds of
performance tools all in all the first
is counting tools which only tell you
how often something is happening and
these are the cheapest tools usually in
terms of overhead they don't have a lot
of overhead they're just counting
something so they're telling you the
number of requests your application is
able to process or the number of packets
sent over a particular network interface
that sort of thing then we have latency
tools which tell you how long something
is taking and that's usually a bit more
expensive to collect because you have to
you know record a timestamp at the start
and the end of a certain operation and
have to aggregate this information the
output of this sort of tool of a latency
tool could be not just the average value
which is not really great but could be
something a little better like a
distribution a histogram a range of
values or a range of latencies that you
can then investigate and finally the
deepest tools will tell you what is
causing something to happen across your
system I call those the stack collection
tools they will give you call stacks of
interesting events like disk accesses
garbage collections memory allocations
and it would be able to look at these
look at these stacked races and
aggregate them and this is usually the
most expensive things because because
collecting stacked race is aggregating
them and so on that's a rather expensive
operation all right so these are the
types of tools and I'll mention a couple
of examples but here are some of the
qualities I want these tools to have and
again they it might sound a little
trivial but not all tools are really
great at these things
so first low-overhead I'll touch on that
in a moment but if you're running a
performance tool especially on a
production system you don't want it to
slow things down too badly you want the
tool to be accurate again it sounds kind
of obvious but not all performance tools
really are that accurate or they could
be skewed by what your system is doing
or what your application is doing you
want your tools to have a quick
turnaround so you don't have to wait for
eight hours to get results out of a
performance tool when you're under
pressure to do a performance
investigation you want the tools to be
production ready by which I mean you can
actually put them on a production system
and run them over there without any
additional extra preparation or any
invasive steps like restarting the
system and finally I want the output
from the tool to be easily focusable so
that I can find interesting things and
the output right away without having to
read through say 50 pages of reports to
find what's causing my problem so let's
talk about some of these aspects for a
bit
let's talk about accuracy first a lot of
the Java profilers and I know it's a
surprising twist but I've ever said it's
going to be a talk only about dotnet
performance tools so a lot of Java
profilers actually produce blatantly
irrelevant results completely irrelevant
or false results when you use them for
CPU profiling and it's a result of using
a documented API which collects stack
traces of threads when you want to see
what your threads are doing which sounds
like a totally legit things to do you
want to stop threads look what thread is
doing collect the stack trace let the
tread keep running it's useful to do a
performance investigation or to profile
the trip unfortunately the way that API
works the way it captures the stack
trace of a thread is completely biased
because it doesn't capture the
stacktrace right away it waits for the
thread to reach a safe point and I'm not
going to explain exactly what a safe
point is but the tread could be running
for a while until it reaches the next
safe point so if you ask what is this
thread doing you're not getting a right
the right response right away you you
will know what the thread will be doing
when it hits the same the safe point
that's the result you're going to
eventually get and sometimes it's it
completely skews the results and there's
actually a research paper on it called
evaluating the accuracy of Java
profilers I recommend it it's really
funny and one thing it shows on the left
here is that they ran for different
profilers on the same workload exactly
and each profiler thought some other
method was running longest and they also
disagreed on what longest means so some
profilers thought the slowest method was
taking 5% of the time other tools
thought it was up to 20% of the time so
they not only disagreed about which
method was the most expensive one they
also disagreed what most expensive is
for this workload again it's exactly the
same application just for different
tools and that means like someone is
lying maybe all of them are lying maybe
just one of them is is is giving the
right results but this is highly
suspicious so what I'm saying is that
unfortunately in some cases yes the
tools do lie and if you're using a new
performance tool and you don't
understand how the performance tool
works then you sort of have to sanity
test your profiler you have to see if it
makes sense the results you're getting
if they make sense on a known workload
where you know what results should be
should be generated and see if the
profiler actually generates that result
now when Windows for dotnet applications
and for other kinds of applications on
Windows we have a really great system
for doing profiling and performance
collection which is called etw event
tracing for Windows and we could do a
whole different talk just focusing on
atw itself and it's really it's a really
great facility because you can enable it
without touching the target process you
can collect a lot of information it's
all really wonderful and useful so again
a completely different talk on the
matter
it's uh VZW but I just want to focus on
one aspect of it when talking about
tools and that's quick turnaround
getting quick results out of the tool so
this is an example of a performance
session where I collected information
from etw
using a performance tool called perf
view and this is what the log looks like
so first it says collecting information
for 10 seconds and that generates 15
megabytes of data so that's already a
bit suspicious it means if I collect for
hours or days on end I'm gonna get
really huge log files but ok let's leave
that for a moment so collect it for 10
seconds and then I stopped collecting
information and it took 12 point 7
seconds to do something called the seal
or run down I don't know what it is
well I do know but you don't have to
know you don't care what it is and that
took twelve point seven seconds to
complete like this is not part of what I
was measuring it's just extra time spent
at the end of the profiling session and
then it took four seconds to do some
kind of conversion again I don't care
what sort of conversion it was it took
another four seconds of my life and then
another four seconds opening some file
and another 0.2 seconds opening the same
file interesting so all in all it took
like 20 20 seconds to open the file from
a 10 second performance investigation
this is what I mean by quick turnaround
this is not quick turnaround if I have
to spend twice as much waiting for the
results as collecting the data in the
first place that's not quick turnaround
so I'm not saying I have the perfect
solution but I don't want to tell you
about a couple of tools I built which
have sort of which are helping me anyway
address this problem at least a little
bit and these are eat race which is a
command-line tool for monitoring etw
events which works in real time so you
can run it and get real-time printouts
when etw events occur and it looks
something like this so you can run it
and then you get in real time you get
printouts on your terminal with the
event you care about for example on the
top i'm collect
garbage collection events so whenever a
GC occurs I want to know about it on the
bottom I'm collecting allocation events
so when a certain threshold is breached
I want to know that the process is
allocating lots of memory again just
examples but by having that printed in
real time I can do a long term
performance collection and not wait for
the whole thing to complete before I can
view the results and that's really
valuable when you're not just measuring
a 10 a 10 second workload another tool
is called live stacks and this is again
a real-time tool which collects stack
traces of interesting events and prints
them out to the terminal as well so you
can use this to the real-time profiling
so you can profile what the process is
doing and just get a dump every 5
seconds with the hottest methods what
the process is doing right now you can
use it to profile disk accesses garbage
collections lots of other events it's
all based on a DW again and the results
are kind of ugly it's just a bunch of
stock traces printed to the terminal but
they can be visualized you can take the
output from live stacks here's another
example you can take this text output
and you can visualize it
for example using flame graphs which
again we could spend the whole session
talking just about the benefits of flame
graphs but this is one way of
visualizing stack traces visualizing
lots and lots of stack traces in a
format that allows easy drill down and
focusing on whatever you care about so
if you haven't seen flame graphs before
it's perfectly fine but you might want
to consider looking into them so just
wanted to talk to you briefly about
again some benefits of tools with a
quick turnaround with a low overhead
with of course accuracy hopefully is a
is a feature that these tools have as
well
and one final thing I have 7 minutes and
22 seconds for is is a quick rant on
statistics and I have nothing against
that this statistics is a beautiful
thing and a beautiful field but the way
we sometimes use statistics the way we
use output from performant performance
tools is often again leading us up
completely wrong Valley and that's what
I want to talk about
briefly so averages like we already saw
are often quite meaningless for
understanding what's really happening in
your application for understanding if
your system is really experiencing a
performance problem so some people use
medians instead they're not a lot better
for understanding the whole range of
values because you can't really reliably
take lots of values and accurately
represent them with just one number it's
it's impossible and so any attempt to do
that sort of thing without really
looking at the whole range is it sort of
doomed to fail percentiles or one common
way is one common way of addressing this
problem trying to visualize a range of
numbers a distribution of numbers and
we'll talk about them in a moment
and finally visualizations for all this
stuff are really important for trying to
understand the actual values so let's
see a couple of examples the average
response time of my application is 29
milliseconds is it good is it bad does
it mean anything suppose we do have the
context and we understand what kind of
application it is and what we're talking
about is it good is it bad so here's a
distribution of values which has an
average of 29 milliseconds I know you
can't see all the tiny labels but this
distribution looks like it's really
close to the average the average is this
blue line over here so most of the
values are clustered around the average
and it's perfectly nice and pretty this
distribution also has an average of 29
milliseconds and then the average is
over here this is 29 milliseconds we
have a lot of values clustered close to
that average but they're really smaller
than the average but we have this
additional cluster of values which are
almost 10 times bigger than the average
and they can't be discarded because this
could be the set of users having a
problem this could be the set of
problematic requests this could be the
timeouts in my document storage system
we can't just say the average is 29
milliseconds so these things do not
exist and it's very common in
performance monitoring that you would
have this sort of second mode of values
for latencies and for other metrics as
well it's pretty common that you would
have the
normal values and you would have some
kind of cluster of problems over there
this is a really cool paper from
autodesk written last year which focuses
on visualization and all these different
dots you see on the screen they have the
same average across the x-axis the same
average across the y-axis and the same
standard deviation as well on the x-axis
and the y-axis and even the same
correlation between the X's and Y's even
the dinosaur which showed up at some
point even the dinosaur has the same the
same average across the XS and the Y's
as all the other sets of dots so what
I'm saying is and what these researchers
are saying as well is that without
seeing the actual shape you can't say a
lot about the values just by looking at
the average or by looking at the
standard deviation show so
visualizations are really important in
general and for us humans specifically
for understanding information at a
glance here's another example of what I
think is not a great way to present
performance information this is output
from benchmark dotnet hopefully some of
you have seen it or used it it's an
amazing benchmarking library for.net an
open source project and benchmark dotnet
reports results at the end somewhat like
this it prints out the average and the
standard deviation as well as a bunch of
really interesting statistical
information like skewness and also
kurtosis yep I don't know what it is but
it does report this and also confidence
intervals and a lot of other beautiful
things but one thing that's missing in
my opinion is just the visualization of
what the numbers look like what the
numbers for that benchmark actually
looked like and there is a way of
getting this information out of
benchmark net but it's not on by default
so if you run this thing by default you
only get the numbers you don't see the
results you don't see the distribution
and what's even slightly worse is that
benchmark dotnet because it's a
benchmarking library so it's maybe a bit
more acceptable it actually throws away
the outliers it throws away the unlike
results so in this example I know it's a
little hard to see here because of the
light bleeding into the projector but we
had 52 results from the benchmark and 11
of them were removed because they were
considered outliers so you know not even
just one or two but 11 out of 52 results
were removed from the statistics because
they would screw up the values so we
just removed them we got rid of them
it's not a great thing to do if you're
looking at actual performance numbers
from a production system because the
outliers the hiccups those weird lines
breaking out of all the charts these are
you know these are the problems these
are the things you are investigating
these are the things possibly causing
problems for your users now just one
final thought if you look at this sort
of diagram so this sort of visualization
you can obviously see the outliers you
can also see the 99th percentile which
is over here and the 99.9 percent tile
which is over here and you might ask
should I really even care about these
super super outliers
about these super hiccups which are
completely off the charts like is it
even likely that a user would ever hit
this sort of behavior in their whole
lifetime so yeah that's how you can
visualize this stuff but I'll just jump
straight up so who cares about the 99th
percentile so I did a little experiment
and I opened up my browser developer
tools and I went to amazon.com and I saw
that the amazon.com webpage made 328
requests and so let's assume for a
moment these are all completely
independent and they all have the same
latency behavior what is the likelihood
that you would see the 99th percentile
it is actually 96 percent so if you make
a lot of requests you will hit the 99th
percentile pretty often and so if you
have operations in your system that the
users are performing again and again if
you have certain behaviors that cause
lots of requests then you should care
about the 99th and 99.9% I'll because
they're not as unlikely as we think just
looking at the dashboards behind
things so there's just a parting thought
about statistic so wrapping up this is
the final thing I want you to think
about at the end of a performance
investigation conducting a post-mortem
is something I've mentioned at the very
beginning is quite important a few
things I like to put on the post mortem
is what we did what the steps we took
were what was effective and what was not
which tools would be useful for
diagnosing this problem in the future
which tools that we didn't have would be
useful and how we can maybe build them
and one thing to think about finally can
we somehow manage to maybe in the future
just diagnose this sort of problem
automatically can we remove the humans
from the equation can we diagnose
problems and just resolve them without
any human intervention so automating
ourselves out of a job so we've talked
about what a performance investigation
looks like some methods and
anti-patterns on 30 seconds behind so
I'll just leave you with link to the
slides if you have any questions I'll
step off the stage real quick and I
would love to chat with you I'm here
today I'm here tomorrow as well thank
you very much for coming thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>