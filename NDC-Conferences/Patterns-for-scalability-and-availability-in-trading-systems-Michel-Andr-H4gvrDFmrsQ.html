<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Patterns for scalability and availability in (trading) systems - Michel André | Coder Coacher - Coaching Coders</title><meta content="Patterns for scalability and availability in (trading) systems - Michel André - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Patterns for scalability and availability in (trading) systems - Michel André</b></h2><h5 class="post__date">2017-05-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/H4gvrDFmrsQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm standing between you and lunch and I
respect that but I think we have a
fairly interesting topic here I'm going
to talk about design patterns for
scalability and availability especially
in the context or in the context of
trading systems but these patterns
design patterns I guess are generally
usable but there are some constraints
when it comes to trading systems which
makes them applicable and and and maybe
they are used in more extreme versions
in a trading systems than they would be
used in in other software systems but
you will see some of them in in other
systems as well so I'm going to start to
give you a very crash course of what
kind of system what we're solving for so
saxy back is a global online investment
bank which is a broker we have offices
in 25 countries we have clients in 188
countries trading both over UIs and
api's we offer access to the global
financial markets so we're essentially
connected to two broadly each and any
exchange any market you can imagine we
are on the third generation of a
technical platform and involving its
Microsoft based mostly developed
in-house and most of the components and
whatnot I will show you on the screen
are actually in-house we're serving tens
of thousands of concurrent users and
more than actually I will get into more
of detail on the numbers but essentially
very high transaction Peaks both in
terms of online users that's also the
time where we need to be responding even
more timely than when it's silent so
there isn't really a trade off so the
business model is essentially we're
taking liquidity or things from the
market and we're providing that to end
client globally a lot of products and
there is also back-office risk
management and other stuff so that's
broadly the business model and then we
essentially make money on on on
commissions and and other fees related
to your to your trading so this is just
to show you that it's a we're covering a
very broad range of asset types and
instruments I'm not going to go into
detail but its its global and very wide
variety of products so in the workflow
and service requirements for being able
to deliver this globally is obviously
based on a some some fundamentals king
out indications authorization
subscriptions setting profiles and
configuration but then you have the
different legs of trading and execute
trading execution where you have
tradable quotes requests for quotes DMA
different kinds of way to interact with
with the markets you have the order
management where you receive in process
orders you route the orders to the
market you might bundle orders and send
them in batches and you get execution
specs on orders then you have the whole
risk management aspect of it and it's
actually more than just when you want to
trade checking that you have money on
your account because your current
positions might be in deficit you
actually might hold more positions or
more assets than you have money on your
accounts
that's called leveraged trading and the
actual value of your portfolio is not
yes the linear function of you know the
current profit losses on each and every
of your assets but it's a it's it's a
nonlinear function so you essentially
need to look to the whole portfolio of
the client every time he trades there
because of currency exposures and
whatnot then you have a pricing and
and reference orders price formation
price distribution and the reference
date around the instrument you have on
the platform and then you have the
content which is short news analysis and
research
I will largely today talk mostly about
the trading and execution and the
pricing side because that's where those
more extreme requirements emerge and and
to maybe to some extent risk that's
where the extreme requirements for
throughput and latency occurs that that
have given rise to some of these may be
non-traditional patterns so we are
seeing in excess of 20,000 concurrent
online users we operational and open
five and a half days our twenty five
hours a day which means there is no
downtime there is no time to apart from
the weekend to upgrade the system we
process in excess of 500 thousand price
updates per second that's essentially
price coming in and we might need to
react to that price in a very short
amount of time route and order do
something on behalf of a client and so
forth reevaluate the clients holdings
based on on the play price the creation
we are doing intraday execution of two
thousand trades per second in two
thousand trades per second and that's
business trades but each and every trade
is also at least you know coming with
with an order request and some state may
change azan the actual order that that
trade generated so this is actually real
trades but a trade generates a lot more
messages in the system actually say
roughly maybe three to five messages per
trade just for that trade to come into
life so to say we booked over 1 million
trades per day 160 unique users and
we're trying to maintain latencies in
the scenes in single-digit millisecond
range throughout from we receive
something until we react to it
essentially so that was he have to give
you a small crash course in in the kind
of technical challenges we have so here
you can see one of our clients sitting
in the platform and doing and this is
obviously a end-user sitting on a tablet
but there might be an API which in turn
has a lot of other users or he might sit
on a web or a desktop application so the
business challenges also is to provide a
unified and compelling client experience
across devices platforms and across the
world do that in a cost-efficient way by
using as low as much technology sharing
as possible reuse trying to avoid
duplication and proliferation of
technologies so essentially we serve all
our system from it from a single system
so we don't have multiple systems across
geographic locations but it's a single
instance systems where we host all the
tenants in the same infrastructure
faster time-to-market and flexibility
adding new products features and
sophisticated or of 16 products and
entering new geographies is typically
also what comes up compliance and
regulation Financial Services is a
heavily regulated market and we also
face the challenges of multiple
regulators as we operate in 25 countries
and we will white label this
platform for other financial services
providers to use it which means we're
indirectly regulated in even more
markets and that's that that is the
challenge when it comes to having audit
ability and traceability and your
process is in order and stuff like that
so what are the critical business flows
where milliseconds matter it doesn't
really if it's creating a client it's
okay if it takes more that can take some
longer time but essentially you have the
markets here we have the liquidity and
therefore two flows we get some price in
from the market and we need to get that
out to the client in in in either clean
form or in some kind of manipulated form
and then the client makes an order and
and wants a trade and we might or might
not want to go out in the market here
and get that back to the end client if
he's sitting on a UI
but these these are essentially the
flows we're optimizing for so price
comes in through some feed handler where
you normalize it might be different
protocols it goes to some level of
aggregation then we do some calculation
and then send a price out to the client
so they're multiple steps here and then
yes we get five hundred thousand in here
this price might also hit downstream
services for them to react to so it's
not just going out to the end client
here but we have in internal services
that need to react to the price route an
order will do whatever and then you have
the actual client he wants to trade you
he sends an order we do some calculation
you do some handling you decide if you
want to go out in the market you get the
order back you get the trade back and
you need to get it back to the client
the B's and before we accept your order
we do a margin or risk SEC essentially
saying and if you're allowed to take
this position or not so crash course in
in bangbang or going trading and you can
see there is also a challenge here of
exponential business growth this is a
generic measure of number of business
events per day over a period of time so
the new system needs to scale it needs
to be resilient and and and and so forth
so trading systems I don't know if you
have read about the reactive manifesto
but trading systems are actually I would
say the definition of a reactive system
so if we look at the core tenets core
design principle map to the tenets of
reactive you would see reactive systems
should be responsive I they should
respond to the user in a in any
condition so we use casual and mental
consistency to allow to be responsive we
use horizontal scaling massively or
partitioning sharding segmenting of
workloads work sets to be able to scale
and be responsive asynchronous kind of
fire forget action where it can be
supported and we are very high
throughput and tight latency
requirements elastic we strive to be
able to scale out on commodity hardware
using horizontal scaling of work set and
and be able to grow capacity swiftly by
by horizontal scaling resilient
redundant no single points of failure
running fully active active
so there is no passive passive service
it's actually active and maintaining the
state that the active one has with
automatic seamless failover in some
instances when we do risk chef sex for
example we actually send the risk checks
to two servers and we take the one that
responds first back because we know
they're eventually in casually
consistent and we're good enough with
one of them responding because the other
one might be garbage collecting for
example or whatever at that point in
time it's very unlikely that the two of
them dude at the same time we need to be
able to survive with a reduced service
if we have a massive break out of a full
data center or a communication line or
whatever message driven it's
event-driven and message driven at heart
loosely coupled systems using a topic
based publish/subscribe messaging
essentially so a little bit about the
technical environment I'm not going to
go into deep we are we running purely on
Microsoft its dotnet c-sharp which was
to do TFS so for most of the
applications you see in here actually
Windows services or is hosted apps we
have a lot of shared components for
business logic and and and caching
that's used throughout the system we use
last Ashok's a cabana for feature
tracking and and some monitoring scum
and so forth we're running large parts
of the critical infrastructure on bare
metal because we cannot get the
performance when processing 500,000
price updates per second on virtualized
environment let allow let alone in the
cloud it starts breaking down and even
doing multicast is a challenge in in in
those environments actually so the
architecture very high level we have the
end clients here we have an open API we
have another API we have some shared
components we have a message pass high
performance through some infrastructure
services I will go into some of those
there are some core services around
order management and risk management
there is some connectivity to the market
and then in the end there is some
databases here as well and as I said
we're going to focus on core and trading
and execution there is also back office
system and money going in out of the
bank as well I won't cover that in in
this talk and so essentially the front
ends are html5 rich very rich single
page applications and they're hosted in
native shelves so we can distribute them
in in in app stores
but essentially it's the same codebase
running whether you do whether you log
in on Android or on iOS or on your
tablet it's just essentially different
layouts and we are in in developing a
this html5 to be in the desktop
application as well essentially the same
components will be repackaged in in an
electron shell in that instance and all
those applications are served by a open
rest-based API
and here you also use sticky sessions
because there is a lot of session state
you need to maintain which instruments
are declined watching and so forth a lot
of business logic is actually front
loaded and executed already in this
layer in this process so an API is rest
based with streaming over WebSockets out
to the client we don't use that
internally that we use multicast there
is a high performance messaging
middleware with with horizontally
scalable persistence so we don't need to
write every transaction to the database
but we can SM essentially just publish
it and and and be rest assured that it
soon enough will hit the database or the
downstream services interested there are
some back-back in infrastructure
services they are a horizontally
partition fault tolerant with fully
redundant running in multiple data
centers and also traditional databases
but running in mirroring high available
high availability mode those are single
instance as we do a lot of the scaling
actually up in in this layer and I will
come back to that because we actually
want to avoid the database as much as
possible because that's usually a
performance bottleneck in any in any
system when when you hit a specific
scale but definitely it adds some
complexity or a lot of complexity so
there is active active messages of
segmented partitioned workload and these
might be partitioned on different axis
so for orders it might be around the
actual instrument for the risk it might
be in
dividual client residing in one segment
and then some large grade shared
components they encapsulate some core
business logic around trade capture
order capture and some of them are just
essentially caching data in a way that's
optimized for the actual use so it's not
generic caching but actually data read
up and kept consistent and structured in
a way that's optimized for the specific
use whether it be finding Commission
rules or deciding where to route an
order which is all semi static
configuration that you can change
intraday but you don't need millisecond
it can take some time before that
actually gets through in the system so
what are some of the interesting pattern
if you look at all this well we use
asynchronous transaction logging so we
actually accept the business transaction
whether it being an order what not
before we have written it to database
we're pretty sure that it will at some
point in time hit the database and and
be secure it's it's it's actually
tracked at multiple levels levels and
there is a replay restore mechanism on
on this message pass as well we use
publish subscribe message messaging
heavily using multicast over using an
optimized binary protocol so there will
be no angle brackets there will be no
YES on there will be no it's it's it's
bikes paired down to the metal so we
take everything coming in normalize it
and and in an optimized manner for this
use case that goes for both of those
flows we saw
on the screen both getting prizes in and
handling transactions we use meshes of
partitioned serving service they execute
real-time alternate automated business
processes an action that might be
routing in order to the market or
sending in push notification to client
that you're a little bit too close to to
being stopped out now or even in the end
actually protecting the end customer and
and and and and closing his position if
if it goes that far and that needs to be
fairly quickly done so we cut the losses
for and protect the client and ourselves
from from any losses we use a witness
server to achieve the seamless failover
here between two essentially active
active instances of a component they are
essentially receiving the same stream of
information they maintain the same state
and one of them is deemed active using
the witness server or if they are con
contact with each other and it's only
the active one so they go through all
the hoops and loops and and process
everything they even make the decision
but it's only one of them that then
actually executes the decision and sends
out this is the way it's going to be and
that can flip very quickly if if one of
them goes down and both of them can
actually respond to requests you can
even have more than two actually in a if
you want to scale out but only one of
them will actually generate the business
the automated business transaction based
on some input also a lot of monitoring
and tracking going on
it becomes crucial when you're all
message based when things goes fast so
we and and that use both just bespoke
logging of data and but also using log
stash in Cabana to do tracking and and
and so forth on the back the open API
actually also has scalable in-memory
session management and is now it looks
like a large monolith but but it's
actually divided in in in relevant micro
services or service groups hosting
specific functionality one that can be
deployed and independently one can be
order and trade management one can be
portfolio one can be charts for example
and and there is a range of those but
these all share a common sessions or the
client logs in once using the single
sign-on system and he can use that token
to access any endpoints and all these
sessions also share a common streaming
channel so any of these services can
actually given knowing your session ID
can stream a message to you over a
single shallow channel because we don't
want to end client here to need to
understand the underlying architecture
and have like 25 streaming connections
for each and every service that or
endpoint that needs to do streaming so
there is a common framework around that
but then it's it's fairly common
compartmentalized so in the humble
beginnings
some of people in the room here have
been been with it essentially you write
it a system fairly traditionally the
client comes in he makes a request you
register it in a database you send a
message back to to the end user and now
we need to get this out to the system
whether it be a risk system or some sort
and then it needs to get out end users
so back office there might be multiple
so now you have a guy here essentially
pulling the database for new things and
pushing it out and this polling
obviously introduces a latency and wait
time and then if the more layers you
have here the more polling you have so
here you can say there might be another
round of polling and if you compound
that up from the time something is done
here until someone can react to it a
system or human a long time can have a
post and actually the market might have
moved against you or the information is
too old the database here becomes a
bottleneck you can obviously short a
database that's also complex but this
little spindle here again it's not a
spindle anymore but the disk where you
keep the transaction log on will will
become your scaling point and this
polling there is only so much you can
get that down to and and and the more
you squeeze it the less throughput you
get in this end so you need to give key
you need to give the writer some chance
to get in for the readers so so you
throughput here and latencies are
essentially capped by this lump here in
in in in the middle yes I've been
through this
so instead what we do is we use
something called as a a synchronous
transaction registrations and
essentially try to abolish any say no I
oh but any database IO in the in the
transaction part at least and we try to
avoid even even Network IO to do to the
extent possible I mean that's always a
balance but essentially in this case the
client makes an order or trade request
it hits some front end open API or
whatever that generates a new trade
message that goes that is essentially a
single write on the network in an
optimized binary format if someone is
having a birthday congratulations
Darren and so it's a single write a
single network write on a socket and
then all downstream services here
needing that reads that message and
consumes it and does whatever it wants
with it so this is fairly close to what
you would see events or seeing or
whatever you call it and and they take
this message put it into their a
internal in their internal structure and
start reacting to it or process it or do
whatever and in this case so essentially
the transaction is fully verified and
accepted very early on here and
essentially when this message goes out
it's a fact it has happened now it's
just for the downstream system to
consume it and we have confirmed the
client actually
so the transaction message is actually
persisted in memory in two in two of at
least two of three machines if so we can
replay and and and and apply some
resilience this is essentially a
transaction log or buffer of whatever
has transpired during a last week or so
so any service here can be down and do a
replay off of what's happening and then
come up to speed so this means that that
these guys that were in in the previous
pictures they were kind of two layers
down with two rounds of polling now
essentially get the message immediately
and can reactivate me immediately and
more often than they're not this person
here actually reacting to it is not a
person but a computer or an algorithm
you could argue that if it was a human
it it didn't matter that much I think
humans start seeing delays of maybe
hundred to 200 milliseconds if they're
consistent but yes
no the client sends in a request to
request is processed in the front-end
and and and and and accept it
so you verify basically everything using
in process caches and and whatnot all
the state is there to be able to fully
you know validate and verify the
transaction with really with the only
acceptance of the actual pre-trade check
which which actually or the risk check
which which is a which is a single
single round-trip that's still there you
could remove that as well but that is a
bit more complex and then the message is
now now the transaction has happened but
we try to minimize process yawns IO I
mean that even goes for simple logging
actually which is you've done
asynchronously in in in here as well in
to the extent it's done and then the
transaction is picked up processed and
aggregate is is sent out and then you
have the critical piece here of actually
you know getting the transaction to rest
is essentially you have a set of trade
ready station services which
asynchronously writes the transaction to
the database yes
the entire state is actually it's or the
entire state or state changes is kept in
this message so this message is it's a
message so actually when this message
hits the database it will update
multiple tables from you know your
orders tables to your audit logs to
whatever so so that's also key these
these messages they are you know real
business level events they carry a fair
amount of context and data with them
yeah yeah the messages here from it from
a single source they come in order so if
you if you followed from the beginning
we have sticky sessions so each request
will end up in the same server so the
message is from a single se single
session or single user will actually be
processed in in order broadly so then
which means that this guy actually knows
that even if this message hasn't hit the
database it knows that I've sent this
message and incorporated it in whatever
state he has up there so which means
when the next message comes that is
taken into account and the same goes for
these guys down here they will see the
messages from from a single session in
in order lo and behold there are raised
conditions here so because that
guarantee essentially only holds from
for a single source so so there are some
raised conditions you need to solve in
this
a scenario as well which is usually if
you look at the problem something you
can do because the messages or the
entities usually either partially owned
by parts of the system or there is one
owner right now of that entity actually
the one that sends out the messages all
you need to serialize some some stuff
and good questions and I like that you
speak up because inter interaction is
good yes the actual transaction happens
when we have accept accepted here the
rest of the system is essentially
eventually or casually consistent I know
once I publish this message that it will
hit the dancer it will hit the database
it will hit their modern server my risk
system in in in the correct order and
and these guys they're essentially as
keeping an in-memory state of of your of
what's in the database in in in in
essence which means that that these guys
down here they have any memory state so
they don't go to the database either
normally because they essentially yes
that's why I have this snapshot service
here so they start up they read the
snapshot and then they join this message
box and say they look at this map shot
and see to some sequence numbers there
and then they joined this message bus
and say the snapshot I've Gordon has the
sequence number please replay from that
that gets replayed you incorporate that
into your state now you're online and
new messages will come in and so they
don't touch the database either in a
in a running State they essentially just
use the database used through the
snapshot service to prime prime their
initial state and then get a replay and
be online with the current state of the
system yes you can that's a good Anna
analogy and actually and yes could you
could you repeat the question the state
in the message was consistent I'm not
sure I fully follow the message process
it's essentially just a transport layer
something built on top of multicast so
not fully because they did the message
bus essentially it's just you know
transporting bytes the state is kept
there is a buffer here of
the recent state changes these guys have
their state in in RAM and these this
essentially has to have the persistent
state of the system yes
yes yes that's why I say the whole
system is built around casual and
eventual consistency but casual and
eventual here is is is sub-millisecond
which means it's it's it's very unlikely
that you and and and usually also one
guy it will hit the same note typically
but one service speaking to this guy
will typically hit the same node so yeah
they are their position most of them are
actually in copenhagen there is also
service in in in in closer proximity to
the marketing in london for example so
yeah the transport ladies is obviously
something you need to take into account
as well when when you do all this but I
don't think that's the largest latency
SP I mean it's going the lot the last
mile out to the end-user you know if I
have a client that that is a sheep
farmer in New Zealand wanting to hedge
for his sheeps or whatever he's far away
from from from a lot of it but this is
mostly also to protect to also also
protection for for us not to take too
much risk so we're actually good with
measuring the internal latency here but
the clients it's also about the clients
because he might get reactions or he
might not get the prices so on the
screen and so forth so if that last mile
also becomes too long then it's it's a
problem yeah yeah in the end did the
kind of things we do it's not this is
not high frequency it's not high
frequency algo training we do some
algorithmic trading but I wouldn't label
it as or and we let some of our clients
do as well but I wouldn't label it as
high frequency that's why consciously
here talked about milliseconds and not
microseconds or nanoseconds there are
some parts of the systems where we talk
microseconds but that's usually within a
box in London somewhere
but that's very small Micro Micro
workflows in this large scheme of things
so scalable resilient we can have a very
high throughput low latency and and be
purely event-driven using this approach
we have talked about some of the flip
sides like yes you have some race
conditions you need to handle in some
cases that can be a mind-boggling and
but that's also inherent in some in some
way with with the market that's like on
the micro level if I order something you
know from from Amazon I cannot cancel
that order because it might actually be
on the way to me I might be able to
cancel it if they haven't sent start
sending it to me and this happens here
at the microsecond level so you have
some inherent inherent race conditions
yes in there in the business domain but
it's on a different scale than when you
order something from Amazon but there
might be a trade actually on the way on
the wire to you when you want to cancel
a transaction that will obviously get
reacted that canceled transaction and we
might even say we have accepted your
cancel request we're we're forwarding it
on and then someone says sorry we
couldn't cancel so there is a bit of a
final twist here I want to go through
with the and such a asynchronous
transaction registration and and that
it's it's it's fully active which means
essentially you have two failures owns
or server rooms or data centers where
you have front ends receiving X
setting transactions sending them out on
this message path which is spread over
both of them then you have the
transaction logger which essentially
receives the transaction and one of the
guys actually write the transaction to
the database in then it gets shipped to
the mirror database and get successful
and the other guy essentially so these
messages also here needs to be Edom
potent or so writing two of them
shouldn't should still leave the
database in the same state which is very
easy in this case because you
essentially does have a unique ID on
each message and if it's been processed
you say this means they're very easily
there is no there's no failover there is
no everything is running all the time I
can add multiple of these I can restart
these guys without having zero impact
which is a very nice property because
always when you have this failover of of
any kind there is always a chance that
it might not work when you need it to
work so this is actually exercising the
same code path all the time
all the day every day obviously we're
hitting this guy quite a lot but in the
end we have also said that we're not so
dependent on the throughput here that
that you tend to be so we are happy to
take that hit for the simplicity so to
say
yes yes Terry I mean fail theory it's a
it's a bold word but in in in essence if
this guy has won the race you
essentially get a message back saying
message was already there but if if the
connectivity to the database is down and
none of these guys can get to the
database those kinds of failures stay at
sense they retry it's a good point right
now we're using that's a commercial
product there are several products out
there we're using a informatica lbm
format 29 West TIBCO has something iBM
has something there is also open source
one called Aaron right now out there
that you can use to get these very high
low latency predictable latency speeds
and they're also I mean more 0 mq and
other stuff that you can use to
implement this pattern but you might not
be able to scale it to distribute
500-thousand price updates per second
but that might not be your use case yes
that's 500,000 price updates per second
that's what we essentially pump out
through the system and and and reactive
so and they are essentially multicast it
out over on the network yes I think
we've gone through this so here you have
the transaction fails gracefully there
are two levels of failure one is
graceful and one is an graceful and on
an ungrateful failure you retry
so and then on the on the on the
database layer we use traditional
mirroring or high-availability
that's available in you know a lot of
sequel or oracle or my sequel or in a no
sequel database there are different ways
to to achieve this but the same
principles can be applied
that's why I've also chosen to keep this
fairly technology agnostic and free from
code because this is actually generally
it just depends on what what are your
business requirements you can design a
system like this and get some of the
benefits by applying different
technologies on the different layers
depending on on on we might have some
very extreme requirements but there are
the use cases where you actually can do
similar things and and achieve some of
the benefits and you might not be as
hardcore with your serialization
deserialization you might actually also
do some more i/o in there in a in a path
but in general it's applicable as a
design eye level design pattern so it's
not it's not it's not all about
architecture and technology and and and
people or architecture and technology
and patterns and so forth
it's also the people try to hire best
cultivate the performance and winning
culture cultivate engineering mindset
it's very I mean because yes we're
solving a business problem here but
maybe not in the in the most
straightforward way because we have some
extreme requirements which means you
need to have engineering
culture not yes I'm going to take the
shortest path to solve this problem to
the business kind of way so supporting
management through some process tights
changed in relief management processes
testing quality processes automated
testing and so forth and when she hits
production you need to have a incident
management process in place that allows
you to respond very quickly when there
are issues and we get worried I mean you
can say yeah we're fully resilient and
redundant but if one of the nodes is
down that's still cause for an incident
because we don't want to run with a
single point of failure that goes for
any layer in the stack so it's not just
relaxing when yeah we still have a
failover so and because you're when when
that guy is down you're you're
essentially running or not as you
intended it with reduced resilience that
goes all the way through the stack from
from Network lines to hardware to
applications monitoring it goes without
saying real-time and reactive cooperate
closely with the gate business agile
mythology hamper complexity I mean this
is complex but there is still some
simplicity in in the design and and and
if you try to stick to some few core
principles then you essentially yes it's
complex but you have some very
high-level constructs that you apply in
and and reuse to solve some of the
challenges you have throughout
so some learning points well if you are
designing an architecture that needs
facility and scalability monitoring that
needs to built-in and taking care from
the start because it's fairly complex
and it's huge estate there is also a I
don't know if you have listened to
Martin Thomson but there is a and and he
speaks about mechanical sympathy that's
that's really optimization and and
understanding things how things hang
hang together from from the CPU down
when you optimized code but that also
goes on a macro level so which means you
actually need synchronize things so it's
optimal all the way from the way you
have your network firewalls the way you
divide multicast groups down to your
application how your applications is
structured and all those strings
actually need to tie together because if
you at one layer introduce something
that breaks that everything need go to
the single firewall or you design your
topic space in such a way that all
messages even if they are on an
application-level nicely divided into
topics they will actually hit your hit
your CPU on a single core and need to be
processed and then discarded so they
actually they go into the tadhana
machine they go into your network or
they go up to the operating system they
get delivered to your application and
someone here says I'm not going to use
it and if you do that and and and and a
logical design might be very beautiful
because you I'm just subscribing to
updates for Apple here but in the
background you're getting hit by five
hundred thousand messages per second so
it needs to hang together all the way
not yet the logical design actually
because this goes down to physical
design as well and so forth so do
gradual rollout provide rollback and
backwards compatibility to decrease
operational risk when you do large
architectural changes comes as a cost
test and verify very thoroughly and then
last but not least this might happen to
you while you're trying to do this so be
careful and I think that's it for me if
you have any further questions
please in yes yes it's it's holding up I
can tell you two to two best days we've
had from a from a from from both I I
would say a business perspective and and
the technology perspective was actually
the Trump election and brexit days where
the system actually hold up very good
both technically and and and for our
clients and these were record days by
all by all standards actually I think we
have seen in in terms of number of
transactions and throughput of the top
of the top 10 days eight of them has
been the last years the last 365 days
and and we have maintained response
times and up times and and and kept our
availability at a very high level during
those so today continually we're
continually optimizing and redoing
things in the in in the small the
fundamental architecture is holding up
what we continually tweak and optimize
and find things and from a you know in
efficiency standpoint that that we need
to do but usually the problems are not
you know macro structural they are local
more more local than then when you have
and not a kind of design where where
where some of the problems you face are
more very structural on the macro level
I mean there might be structural issues
within the component as well obviously
we it's it's a mix actually the most
parts of the state is released weekly on
Saturdays which is very good for being a
regulated bank and we do a lot of
releases and a lot of changes some of
the components you see there can
actually be released where we have the
best you know resiliency and so forth
they can be released intraday as well we
don't we typically don't do it unless we
don't really have to but if we have to
we do that and we can do that do rolling
upgrades without is significantly
interrupting the business on on on on
most of the layers
yes yeah but but that's why I say
feature flags continuous delivery be
backwards compatible handle missing
fields gracefully all that needs to be
in place if you're going to run those
amount of components on on that scale so
you can roll back roll forward with with
some certainty so it's not we very
seldom do Big Bang forcing changes where
we need to you know push something out
in the weekend over everything that's
guess not or it's feasible but it's not
it's too risky
and also rolling it back is very risky
so that's more than on a on a component
service level
that in this weekly release cycle thing
there is actually a release test
environment where we do both automated
and semi-automated and manual tests on
what is going to be deployed in the
weekend where you so a lot of their
testing is done you know both using unit
testing and some integration testing in
your continuous deployment pipeline but
that's obviously for your component or
for you module so a lot of things happen
there before you decide but then there
is a a track for for doing a full more
full-scale release test that's less than
a week so actually the cutoff time for
getting someone something in to the next
weekend's release
I think it's Thursdays so then there is
some position and then there's
baselining on off of that environment
and then there's tests going on and then
there is essentially a change board on
the next Thursday which decides that
what goes in so it's less than a week
and that's also something that we
continuously try to compress with
automation and and so forth yes we
verify I mean every we do we say cannery
releases or being able to enable
features for a specific user or release
something for just one broker out of our
whatever number of brokers and then so
so we're yeah that's that happens a lot
throughout
good thank you now it's lunch I guess</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>