<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Enhanced AML fraud detection solutions with Azure Machine Learning - Ravi Kanth | Coder Coacher - Coaching Coders</title><meta content="Enhanced AML fraud detection solutions with Azure Machine Learning - Ravi Kanth - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Enhanced AML fraud detection solutions with Azure Machine Learning - Ravi Kanth</b></h2><h5 class="post__date">2017-04-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KdVwmph7HDk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everybody can everybody
hear me okay perfect
my name is Ravi Khanna and I'm a senior
with the clarity solution group I work
out of their New York office I focus on
the financial services domain before we
start just a show of hands how many of
you work with AML Oh perfect how many of
you are familiar with agile machine
learning perfect in today's session we
take a quick peek into what am l is we
will look at how email solutions can be
enhanced using machine learning and how
specifically as UML helps us achieve
this we also have some time for Q&amp;amp;A in
the end
AM L stands for anti money laundering
before we get into what anti money
laundering is let's look at what money
laundering is so money laundering is the
process of making illegally gained
assets appear legal so this has three
main steps first is to introduce the
illegitimate money into legitimate
financial system without getting noticed
second is to move the money around to
create confusion this is done typically
by transferring the money through
various accounts and finally additional
actions will help with integrating the
money and making it appear clean money
laundering really takes various forms
like for example structuring where
you're taking a big amount and then
dividing into smaller amounts and trying
to get it into the accounts so that you
avoid the reporting limits that are set
by the regulators you also have other
forms like using cash in terms of
businesses like there are some
businesses which by their very nature
generate a lot of cash so criminals or
bad actors typically use those avenues
to launder their money
you also have shell companies which kind
of hide who the true actors behind other
companies are finally also have casinos
or which are yet another way among the
many other ways so a banker actor could
walk in with illegal cash get some chips
spend a little time go back to the
counter
get the money get his money back return
the chips and claim those to be winnings
from the casino so all of these are
various ways in which you could have
money laundering so the purpose of the
money AML rules is to help detect and
report money laundering activity so for
the purpose of today's conversation
we'll be looking at AML from the purpose
of banks and financial institutions a
typical AML flow looks somewhat like
this there are various feeds of data
like accounts transactions customer
information other historical information
you have information from the
regulator's like specially designated
Nationals or persons of interest whose
activity the regulator's want to track
and you also have the bank's own
internal customer identification program
data you have I mean any other data that
the bank or financial institutions want
to use so using all these feeds
surveillance is conducted based on a
predefined set of risk factors and
scenarios most often than not this is
performed using a rule engine based
approach so from this surveillance
activity potentially fraudulent
transactions are flagged and given the
nature of these transactions you don't
mark them as fraud and report them
immediately you have a AML case of a
desk and when they investigate the
transaction and if they confirm that
there is indeed a fraud then that's
reported to be a regulator so the report
is called a suspicious activity report
so some of the common pitfalls of the
AML solutions are particularly around
the rule engine based approach that it
doesn't keep up with the dynamic nature
of fraud detection I mean of the
laundering activities of the criminals
the criminals are very clever and they
keep constantly evolving so you need the
systems to be able to keep up with what
the criminals do or are the bad actors
do an example of this is let's say there
is a threshold that you've established
of a
over 10,000 euros or pounds let's report
every transaction once the bad actors
get a hang of it they're going to
structure and bring their transactions
within that gradual so how do you adapt
to this right if other I mean this is a
simple scenario but it's a real scenario
there are other pitfalls like there's a
lot of manual data collection going on
and risk scoring there's also a lack of
social graph analysis what we mean by
that is let's say there is a bad actor
and you've identified a fraudulent
activity transaction by this actor
having a social graph of that
interaction graph of that actor within
even if it's within the confines of your
own organization helps you identify
potentially any other actors that this
actor has interacted with so there's
also a very little customer segmentation
at most places or modeling of customer
information and finally being able to
apply this in real time in a scalable
way all of these are common issues that
we have organizations facing so to
enhance AML fraud detection some of the
approach and one of the approaches is to
be able to use machine learning
obviously we are not saying throw away
your rule-based
rule engine based approach so what we
are saying is use a mix of rule engine
as well as machine learning based
solutions so the picture will help us
understand this a little better as to
what we're looking to achieve so on the
left side you have most typical current
state and the right side of the picture
is what we are looking to go to get to
so if you see the universe of
transactions on the left the fraudulent
activities are marked within a rectangle
and then the oval marks the transactions
that are flagged and the circle marks
the transactions that are actually
investigated by the MLK's management
team so as you can see there are a lot
of false positives like those that don't
intersect between the fraud and the fact
items so there are a lot of false
positives and if the case management
team again it's a manual effort so this
and a lot of time trying to investigate
all of these activities and in all of
the flag transactions and what we're
looking to achieve with the AML Plus
rule engine based approaches to kind of
reduce the number of false positives and
have better utilization of the AML case
management beams on time as well as also
to get a better priority to the
identified items so typically in a rule
engine based approach you have first
come first serve kind of an approach to
resolving the issues whereas with using
multiple scoring mechanisms you could
score the same transaction against
multiple systems and then get an
aggregate risk score that will help
better prioritize your AML transaction
in your transactions so how do we use
machine learning for this right this is
a typical scenario that we just dropped
off where you have incoming transactions
and other data that is badge
surveillance happening and you do the
case management with AML I mean with
machine learning you're kind of feeding
machine learning data from data incoming
data as well as data from the case
management system and then you're
supporting enhancing your batch with not
just the rule engine base but also the
machine learning based solutions this
also has the additional benefit of
providing you the ability to wrap your
machine learning based algorithms into
services and then these can be consumed
in real time as well we'll get to how
you can deploy create and use them in
real time when we see the demo so in
machine learning with Azure you have
four major steps one is identify
prepared
model and deploy so with identify what
looking to addresses hey do we have a
clear understanding of the business
domain and are we able to clearly
identify the data sources so once we
have done that the next step is to
prepare the data so with preparing the
data the key is selecting the data then
once you've selected the data we'll also
have to make sure that the data that
we've identified is clean like there are
no missing values be able to apply any
transformations that are required to get
this data to a state that you can
actually use and then some of the
typical things that you'd be looking for
here are a what are the attributes are
the attributes consistently present do I
have to convert any of the types from
one to another to facilitate my feature
engineering and so on so once we have
identified the data but once we
understood the domain identified the
data cleaned and transformed it the next
step is to build our model so the build
step is really around identifying what
machine learning algorithm best suits
your purpose so for this you have
multiple machine learning algorithms
that are available like they're
supervised learning the unsupervised
learning and so on so with supervised
learning the goal is to be able to use
example or sample data have the Metall
algorithm identify how the label is
being arrived at and then apply that on
a data set of data so that you can
evaluate hays it performing to what the
way I wanted to perform and then you'll
be applying it in the real world whereas
with the unsupervised learning there's
really no label data or there is no
classified data I mean there is no
example data really you just give it a
set of data and then you have it explore
the organization of the data and it
tries
and what the structure of the data is
and it tries to give you the simplest
and best organization of the data that
it can think of within again building
the model one of the keys is to be able
to do feature engineering here what
you're looking at is are there after all
variables what makes most sense how are
these related to each other and how do
we machine learning algorithms use these
features so as part of this you're also
probably going to create additional
features that are relevant to your task
at hand you'll also be potentially
changing your algorithms based on the
features that you've identified so other
than there's also one other lab solely
within the supervised learning there are
a few different classes of algorithms
the first one is classification where
you're using data to predict a category
then there is regression here you're
looking at predicting a value sometimes
it could be a continuous value based on
certain inputs that you've received and
then there is anomaly detection where
the goal is to identify data that is
different than what you have so
something that's unusual for the data
set so all these three types come under
supervised learning and then within
unsupervised learning as we said as we
discussed before it's primarily trying
to organize data that doesn't really
have any labels in it but again as your
machine learning supports all of these
with various algorithms there is one
algorithm that's unsupervised learning
this cake k-means clustering other than
that most of the others are supervised
learning algorithms so after you choose
the model the next step is to train the
model are you randomly split the
training data set I mean the data set
that you have in your training data set
as well as a test data set and
you run the model on the planing data
set once you finish training the data
set you would be evaluating it with the
test data that you have and finally you
identify the best model that addresses
you'll need after you're done with
building the model training and
validating it you would be deploying it
and they're trying to operationalize
your model one of the key things as you
go through this steps are is to make
sure that you're building a pipeline
because this is not a once and done
thing once you've built a model and
deployed it you want to be able to get
new data constantly and you want to
maintain that model by braiding it from
time to time with the new data and you
want to keep it updated so now let's
just look at the Azure machine learning
studio before I go there this is a cheat
sheet that you have most of you have
probably seen this this is available on
the agile site it gives you all the
various from algorithms that are there
available for you to use and under what
category they belong to
just bear with me really bring up the
down
okay so this is Azure machine learning
studio again the key thing with Azure
machine learning studio is it provides
visual workflow capabilities for data
science like you've used Visual Studio
for your regular programming and this is
it provides you similar capabilities for
the data science workflow that you need
to create so you have you can create
projects and then your primary time
would be spent creating experiments let
me just open an experiment
so overnight okay so the experiment is
really a sequence of tasks that you're
performing that are all wired together
we will see how to create new steps and
wire them together on the left here this
is really your toolkit where you have
various elements that you can use to
perform various tasks so you can all
your data sets will appear here if you
have any train models they'll appear
here you have the ability to convert
from one format to another let's say
your export you've built a data set and
you want to export it somewhere else for
use you'll be able to do all of that
here the key things around I mean the
key capabilities of this are around
really being able to transform the data
as you need whether it's filtering the
data or joining data being able to add
columns add rows clean missing data all
of these are elements that you don't
have to really build these are readily
available for you to be able to drag and
drop on you can really focus on the
experiment that you're looking to
perform as opposed to the scaffolding
and building the underlying code for the
experiments you also have a lot of
support for being able to split data or
there are all the machine learning
algorithms and models that are available
for you to use classification clustering
regression everything that we saw on the
chart earlier so let's before we look
into the code I mean before looking to
the experiments you can also use your
Python or our modules if you have any
you can plug and play and use them
within these experiments okay let's take
a 10,000 foot view of how an experiment
looks like so this is an experiment that
is already performed by
in this experiment the top portion
that's highlighted now is really where
the data has been I mean data is being
aggregated so we will look at this we'll
run this we'll change this but I just
want to give a 10,000 foot view of for
this experiment before we get into
hands-on so this is transaction data
there are multiple transaction fly files
that are coming through and then there
we have the ability to add them and then
there is selection of specific columns
that's going on on the other side there
is client data so we're bringing in the
client data similarly selecting both
sets of data selecting specific columns
that we want to perform the experiments
on and then there is the ability to
split the data this is training of the
module this is the actual model itself
this is the evaluation and this is sorry
this is the scoring and this is the
evaluation so all of these capabilities
are available within the same
environment so you don't have to really
go across various environments to be
able to support your experiments all of
them can be done in exactly the same ID
let's look at how this works it
so when this opens up we'll see that
there are there is some wiring that's
already done I just rebuilt the data set
so that we can build the machine
learning model on this again as I said
there's client data on each of the
elements you can actually go and
visualize it and this will give you
information about what is the up sorry
that was a zip file once you unpacked
and combined the data sets okay I
haven't built this yet so let me build
it it will go through each of the
elements and it will build the
experiment and it will collect and
generate it'll generate metrics and then
it will provide us the ability to be
able to visualize so what you see by
these icons next to it is its building
and it builds it sequentially top to
bottom as the workflow specifies okay
so this is just the data part of the
experiment we haven't done any machine
learning on this yet so we'll first look
at the data so I have this raw data we
have the ability to look at are there
any missing values or what is the
feature type of this data data element
what are the number of unique values and
so on and it also gives you a fairly
good idea of how the distribution is of
the data points within this data element
it also have a good picture of how many
rows there are what is the number of
columns and so on
so you can use this visualization
feature at every point that you have
you're touching the data so whether it's
when you're adding rows video selecting
columns or when you finished finalizing
the actual data set that you will be
running the machine learning experiments
on so we took data set of the client
data we look took the data set of the
transaction data and finally we have
done some transformations and we arrived
at a data set that we will be running
the machine learning experiments on here
so in here I'm curious about what the
distribution of the transaction types is
okay 89 percent of my transactions were
credit-card this four percent cash and
so on okay the keys with AML companies
and financial institutions event
regulators they're looking beyond just
cash as a means for AML they're looking
now at the universe of transactions
within that your organization supports
to be able to attract money laundering
because there's also the possibility
that somebody repays credit car and then
claims the additional amount that
they've paid back from the credit card
company and use that as a
money-laundering technique so all
transactions could technically run
through such an experiment right
okay now in here let's add some machine
learning capabilities and make sure that
we create a model that we can
operationalize web service so the very
first thing is let's make sure that we
have a data array so again it's very
simple to drag-and-drop and need to make
sure that you're wiring your elements
right next you need a model I'm going to
be using a multi class decision forest
and let's create the training model
train model and we need to also score it
and finally we need to evaluate this
model okay as you see if there is any
required data then you will be prompted
for it so within the train model its
first wire it and then we will see what
is missing there so within the train
model particularly given the type of
model that we are using we need to
select which column is the label column
so in here I have a column that's report
flag so that's my label column and for
this I need to make sure that I am
wiring the data so in the split data I
need to make sure that I've assigned the
right kind of percentage of transactions
that I want to train the model on so I
want to send 80 percent of my
transactions for training and then I'm
going to use 20 percent of that for
testing and then once we are done with
this we want to evaluate the model
so let's save this quickly run the
experiment so what is happening now is
it's running through the data that we
have taken created the data set and then
it's splitting the data using the multi
class decision forests explaining the
model and subsequently it will score the
model and finally evaluate and it'll
generate metrics and it'll give us the
ability to visualize the results so
let's see how the split data worked so
there are two data sets let's visualize
data set 1 this is the 80% so I have
eight hundred and thirty-four thousand
that's 80% and this should be the 20%
tornade cousin so we're good there
the Train model so this is taken that
80% and it's now identified some
patterns so let's see what patterns have
been identified if you see on the left
it has given you the identified patterns
based on the trees that have been
constructed so it used here it's just
used one variable with industry but then
the next one let's see maybe it used
more okay so it's used different
variables so it's identified patterns
based on the data that we have given as
example data to identify hey what is the
optimal way to generate the label that
let's see what the scoring how it
happened okay so here it takes the test
data set and then using the Train model
its course each of the labels so here we
said a report flag this column is the
label so it's coded using the training
data
and it's providing us the label as well
as the probabilities for each of the
labels that it has developed and it is
again just for our convenience to be
able to go through and see hey how did
it arrive at this particular label where
was there any other label that was
considered and what are the probability
for those other labeled and then there
is the evaluate model this tells us how
the model performed right again this
particular data set is handled so it's
100% accurate but again in real world
this is probably not going to be a
common occurrence you'll find some
deviation and you should expect some
deviation from a picture like this but
ideally this gives you this element
gives you the ability to look at what is
the class that is predicted and what is
the actual class that was that came in
as a label so you'd be using this to
identify hey what is the accuracy of my
train model so having built this
experiment and generating this generator
this train model now the next step is to
set it up as a web service so let's set
it up as a predictive web service what
it does now is it picks up the
appropriate elements and then it creates
the web service input and output so our
experiment has been collapsed into
something that could be used from a web
service which is excusable
so our experiment has been converted to
something that could be used from a web
service this is the output we need to
provide an input for our web service
okay let's save this quickly
then the next step once we've created
this is to deploy the web service when
we deploy the web service it
automatically generates the URIs for us
and it also gives you sample code in
c-sharp our Python but to be able to
consume it okay it's still building this
model one good thing about these expert
the Azure machine learning studio is
your experiment can have multiple
machine learning models built in and
crank so you don't really have to have
one experiment for each model you can
have multiple models within the same
experiment
so let's deploy this
so it's going to take a little bit
deploys I'm going to start with
something else that I built this morning
so let's loop it
select a plan that deploys let's look at
something that we already have it gives
you clear usage statistics and what
response times it had how many compute
hours of were consumed whether there
were any batch requests or over there
all real-time requests you have all the
statistics provided in the dashboard and
then it also provides you the ability to
test the web service let's see okay so
this is the web service that we created
you can also have the swagger API and
generate I mean for you to be able to
use that currently is unavailable so it
will be available after a little while
while it prepares it so this is the
experiment that we created puts just
wait for it to show for the first time
okay
while we wait
well that comes up I can show you
something else so this is how the
configure screen is like it gives you
what your keys are it gives you the
ability to specify your name enable
logging if there's any sample data that
you want to enable all of that you can
go to the configuration for the web
service and then there's also for the
consumptions and it provides you the
request response URI is the batch
request you arise it also gives you
sample code I was mentioning earlier it
is also the Excel of consumers like
you'll be able to download these files
that are embedded with the URIs and also
they allow you to be able to test it if
you're going to be using Excel as the
interface for your machine learning
activity okay let's see if the stable
processing
okay so I'll take a pretty basic
scenario
the report flag is the label so that
doesn't really matter for us so this is
a pretty straightforward transaction
based on the features that I developed
that there's no suspect I mean this is a
very clean transaction I would expect to
see a label of zero for this which is
what we receive again based on the
feature set that you develop you'll have
use cases where you would expect to see
some other labels so this is a set where
I expect to see a label of one and again
this really is coming from the web
service that we develop and deploy so
you'll be able to use this in Europe in
any of your applications for real-time
or batch use so where does this really
come into play and how does this help
with case management and how does it
help enhance the AML activities so if
you look at the typical case management
desktop this is how a typical case
management desktop would look like you
have the incoming queue of all the
flagged items and then you would look at
these are the details within the item
and I'm going to work on it once the
reviewer has identified whether it's a
fraudulent transaction or not they would
add their notes in here and move on to
the next one all right with the Azure
machine learning or any other machine
learning approach you could potentially
provide your case management team a much
better and much more informed and
prioritized case management cue so here
you've scored each of the transactions
against multiple models you've scored it
with the legacy model you're scoring
with with machine learning approach a
machine learning approach B or however
many approaches you have when you're
generating an aggregate score based on
all of the scores that you received and
also you can provide them much more
detail on
what has caused a particular transaction
to be flagged or not so we also talked
about being able to do social graph and
network and social graph analysis so
with all the data that you've processed
you'll also be able to pretty easily
generate social network graph of your
actors like if you have reflect the
transaction you'll be able to identify
okay let us assume that this particular
transaction belonged to this user right
here in the middle and this user is
interacting with wherever the cards are
taking and based on the strength of the
number of transactions the thickness of
the card is increasing or decreasing and
this will help you identify hey within
my universe of events of universal form
customers what are the other potential
suspects that I should be following up
on our investigating right so this is
for case management again think of other
use cases where the web service since it
is now available you'll be able to
deploy it for let's say real-time use
and again one of the key things with AML
is not to alert the suspect but rather
to alert somebody on the inside so that
they can investigate and alert the
regulator so you will be able to use
machine learning for use cases that you
potentially are not able to support
today and also reduce the number of
false positives the same approach could
potentially also be used in other areas
like insurance fraud for example or for
detecting insurance fraud particularly
around claims fraud and also for
underwriting analysis where you will be
able to use these machine learning
techniques to be able to provide better
underwriting support I'll just take a
pause here any questions
so the question was how do you provide
this information to the UI right yeah
you should be able to do it with
definitely of the keys again this
particular environment is for the visual
workflow development for our machine I
mean for data science activities so when
you're using Microsoft Azure machine
learning studio primarily most of your
main activities will be drag and drop so
if you had let's say pre-built code for
Python or R to be able to execute a task
then you will be able to bring in that
code and use it within the workflow but
if you want to execute all of this
within code then this is probably not
the best environment for you
this is really for being able to use the
features that are in here and bring code
that you want to embed within this
workflow okay any other questions
excellent so let's look at the data set
that we generated here
so in the data jet in the data set that
we used for this experiment there is a
column called the report flag so this is
historical data so in the historical
data this call this column is the report
flag so these are all transactions that
in the past have been flagged and
reported and confirmed to have installed
so we've through the processing of the
data so we had as I was showing you in
the customer database we had 19 columns
here let's just sorry we had 16 columns
ok and then in the transaction data we
had probably 19 columns so we merged all
of them and we created the features we
extracted the features that we wanted to
use right so that report column when we
went to create the machine learning
model right fielitz let me go back to
when we're creating the experiment and
when we were creating the model we had a
step where we chose which column was the
label column so let's let me find one
experiment where I haven't build it to
the predictive service okay so in this
there is one element here in the
training so you'll see that there is a
label column here so this is the report
flag so we are telling it based on
historical data this column is giving
you whether this is a fraudulent
transaction or not so it's using that to
understand from the data a what patterns
of data have this flag set to true or
false or whatever code if it's a multi
classification what is there some secret
code with it and then when you bring new
data in it looks for any pattern so I
was showing you the strain'd model with
the various trees that it built right so
all of these are the various approaches
that could be used to say hey this is
for this pattern of data this is the
score that I've assigned so when you get
new data it's going to try and match up
against this existing patterns that it
identified and based on the best match
it's going to give you whether this is a
fraudulent activity not fraudulent or
some other special kind of code okay any
other questions okay one of the other
things that you can consider is like
when you're really building machine
learning based tools for your users when
you're focused on data science this is
really a significantly small portion now
given the capabilities that are built
into the tool
most probably time is really going to be
spent on trying to understand your data
and I hey what are the features that I
want the engineer and what is the best
way that I can get the additional
features out of my data set some of the
times you'll notice that this is not a
one-and-done kind of the thing it's not
even a step a step B step C it's very
iterative in nature so you'll be going
through multiple iterations and then
you'll probably identify hey I don't
have some data that I need to get
probably I don't have it available or I
need to augment it with something that's
available outside my ecosystems you'll
have to iterate through the steps
identify what data really supports these
algorithms and what gives you the best
results to be able to identify the front
activities so some of the additional use
cases that you'll find is you'll be
augmenting this with social data social
media data there's probably Twitter
feeds or Facebook feeds that you will be
bringing in and augmenting your own
transactional data to identify if there
are any fraud patterns that you could
look at right I guess there's a
insurance use case this gentleman here
you were talking of insurance and claims
or fraud so if there is a user I mean
there's a insured who's claiming that
they were an accident on a certain day
and you find that their Facebook is
showing pictures of the holiday in some
other place then that's potentially
fraudulent activity that you want to
find right so you could definitely use
so it's not a once and then it's not
just the data you have you have the
ability to bring in all different types
of data and use this to be able to merge
and create the data set that will help
you best identify the front and again
it's not just one let me show you that
example
so most of the times you find that this
is not I just pick my one model and I'm
done with the model and it's making me
happy even if you're happy with the
model if it's your first one you want to
make sure that you're trying out
alternate approaches and you're
executing you're evaluating multiple
models so in this you'll find that on
the left here we are using a regression
based approach and then on the right
here I'm using a clustering based
approach so within the same experiment
I'm using multiple models and then you
evaluate each models performance with
the other again it's not just - this is
a play area which allows you to have as
many models as you can accommodate again
there is a hard limit of 10 GB you
cannot have data denser than 10 GB but
other than that you should be able to
within those limits have as many
experiment I mean have as many models as
you want evaluate and compare and once
you're happy with one then take that
converter to the web service deploy use
it for other batch or real-time purposes
and be able to operationalize it and the
other advantage with this is as you
might have seen I made a change in here
in the split I think earlier it was
point three I made it a point eight if I
brought in new data my pipeline still
remains I don't have to if I am having
new data I don't have to rebuild this
whole thing each time so this pipeline
exists for you to be able to retrain
your model from time to time with your
data again if you're selecting new
columns are you bringing in new columns
and obviously there is some work to be
done but the scaffolding and majority of
the pipeline exists and you'll be able
to reuse it so that's a pretty powerful
feature to have
is please so when you say a be testing
that's really around scenario for like
let's and marketing where you have
scenario a scenario B which is giving me
most this is not connected with that
this is within the machine learning data
science field where the left side is
really giving you the training data set
so you when you have a data set you want
to make sure that your model is
performing accurately so the way you do
it is you have a historical data set or
some data set where you have labels that
are already marked data that is already
marked as fraudulent or not fraudulent
you will randomly sampler take a portion
of it and you will first train the model
so you'll ask it to they use this data
to teach yourself what is the fraudulent
and what is not a fraudulent transaction
and then after it is done the training
you want to use the rest of the sample
to test it so here it ignores that flag
field so it uses the rest of the data to
generate its flag based on the learning
that it has done and then it compares
the flag that it generated with the flag
that's already present so this is not
that a be testing that a be testing is
in more of what the users have like like
approach approach B this is not that any
other questions okay I guess I'll give
you back ten more minutes if anybody has
any questions I will be around here so
feel free to ask</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>