<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>TensorFlow in Three Sentences - Barbara Fusinska | Coder Coacher - Coaching Coders</title><meta content="TensorFlow in Three Sentences - Barbara Fusinska - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>TensorFlow in Three Sentences - Barbara Fusinska</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DBW284ZDSJ8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello good morning everybody I'm really
impressed with your presence I had to be
here you didn't so thank you for this
and today I will talk to you about
tensorflow and how I compacted it in
three sentences as you can imagine if
you want to really really know well
tensorflow you probably need more than
an hour and probably more than three
sentences but those resentence is I
chose for you I think they are a good
start to to play with it and to
understand what it is what it is not and
hopefully give you a head start
when you want to dig deeper just out of
the curiosity who here have ever worked
with tensorflow or touch tensorflow
great and who has ever work with deep
learning AI machine learning okay cool
who has come here because this big hype
of machine learning AI and all those
robots yes yes so we have quite a
diverse audience here and as I said I
will show you some stuff
that our introduction re to tensorflow
but I will go deeper and I will show you
how I go deeper in a second so Who am I
to talk to you about tensorflow
and I used to be a programmer for years
but I did my master's degree in machine
learning although back then we didn't
call it machine learning we'll call it
data mining we called it AI and I was
dealing with fuzzy rules with genetic
algorithms and there was this niche
thing that my mentor said don't go there
and those were neural networks and it
should be I should be mad at my mentors
now because as you know it's a big thing
at the moment but I will explain to you
in a second why I am not so what am I
going to talk to you today about first
we will talk a little bit what was deep
learning
and as you may know everyone has their
own definition and there are a lot of
articles headlines blogs courses and etc
universities even and that talk about
deep learning and the reality and then I
will introduce you with the nice dataset
and then I will use this data set in my
six demos if the time and Australian
Network will allow us to do but I have a
good news for you
all those demos are available for you on
the platform Kolkata Koda it's not just
that you will have the code you'll have
the whole environment to run it just
basically do whatever I will be showing
you today and go into more details you
can spend more time on those demos then
I have within this hour so what's deep
learning who can tell me what's deep
learning subsets of machine learning
sure anyone else it involves neural
networks yes why deep multiple layers so
the official definition is if we have
more than two hidden layers and for
those of you that don't know all explain
what hidden layers is in a second then
we call it deep learning and my favorite
definition is matrix multiplication
because everything that is
mathematically down deep in in those
neural networks basically you take one
matrix and other matrix you multiply
them you add them together and get
miracles and that's my point
actually we are now at the moment living
in the future when technology is almost
indistinguishable from magic
I have read this guy's t-shirts also
very funny so when I'm thinking about
deep learning when I'm thinking of AI
this is the technology that actually
made it possible that when I'm was what
watching the movie off I don't know
Harry Potter and I had this feeling
because I'm a programmer so I was biased
that Hermione is this brilliant hacker
knew all the spells / comments right and
she could just achieve everything she
wanted because she was a very good
student and she and she knew all did all
of this and whenever I think of stuff
like this I cannot I cannot not think of
technology and how how it is actually
magical so what are those neural
networks so natural neural networks the
ones that we have in our brains of
course have neurons neurons are those
structures that look kind of like this
they there are connections between them
there are some signals passing from one
neuron to the other and basically this
is the moment when the analogy stops
when we talk about about artificial
neural networks because artificial
neural networks are just numbers are
just wait so if you look at this very
simple neural network we have three
layers here input layer it's basically
what we want to classify find out what
we feed our network with there is an
output layer that has different sizes
depending on our problem if we're
thinking about classification problem we
have three classes to predict we'll have
3 output neurons and then we have this
hidden layer which is not really hidden
you can access it quite easily but some
PR people think of it as a hidden layer
maybe back in the days it was actually
hidden so what is happening inside all
those neurons as you can see this is an
example of so-called dense layer so
every neuron has the connection to every
neuron from the layer before and what is
happening it takes the input it
multiplies it with weights then adds it
and then put it through some
activational function and if you know a
little bit about linear algebra algebra
you know this is basically what matrix
multiplication is if you treat those
inputs at matrix if you treat the
weights as matrix this is what you get
this is how how you multiply
mattresses activation function plays
different role and depending where we
are which layer are we talking about but
it's basically some function that you
take your values put it through
nothing magical with it and this
aggregation equation is all over the
places and it can actually do miracles
so why am I not angry at my mentor when
I was doing my PhD because neural
networks were there forever soon there
will be like 100 years when first neural
networks were fought so they were
basically in since there were actual
computers weren't invented and the real
hype was around 2008 9 and well
seriously no one actually predicted it
and it has happened to be a big thing at
the moment so what happened what changed
why why now neural networks are such a
big thing
machines are stronger we actually have
big data right at the moment anything
else
GPUs yes all those things we have cloud
everything is quite cheap so everyone
can access everything and we have
libraries like tensor flow tensor flow
is not the only one it just happens to
be the most popular at the moment but
all those efforts of democratizing AI of
democratizing neural networks are
actually paying off and there is another
reason neural networks happen to solve
the biggest AI problem there is who can
tell me the biggest thing I problem
there was because they solved it its
distinguishing cuts from dogs if you
didn't know for years we didn't know how
to explain to the computer how to
distinguish and actually recognize it on
a picture and now now we can all because
of those multiplications and adding
stuff
let's start with my first sentence my
first sentence build a computational
graph so for those of you who know
tensor flow this is this is your first
when you first encounter tensor flow
they they told you tell you to build the
graph what what is it
it's basically something very simple
like you can see at the top you have
some some operations you have some
inputs and those operations are taking
those inputs and well perform those
calculations and have some outputs and
those inputs if they are not the output
of another operation they are called
tenser and tenser flow what are tensors
tensors are basically multi-dimensional
arrays and thank God we have computers
because our human beings we cannot
really imagine more than three
dimensions if you can any of you you
want to offend you but most of us we
cannot and machines are great because
they don't have to imagine anything they
just take multi-dimensional arrays and
perform calculations and linear algebra
works very well on any dimension you
basically can imagine simple simple
examples of course will play with your
imagination so start with two and three
dimensional stuff and so how to build
this computational graph first of all
you're just saying which calculations
which inputs which output are you
expecting and then there's and there is
this thing that is called the third
execution so the fact that you've built
there this recipe doesn't mean it's been
run so first you're building it then
you're starting a session and then you
can run basically any bit of it you
don't have to run the whole thing you
can just run this adding thing and there
is a tool called sensor board that is
also available for users of
and you can see can build very
complicated stuff you can drill down and
see how does it work and it has very
nice visualization because you know code
is great and we all love to code but if
you're building complex stuff you
sometimes I do this very often used to
do this very often before this tool I
had to draw my calculation graph to you
know not get crazy
so let's see how you can build
computational graph in tensor flow so if
we go to all this course I have six
demos for you and I will just start the
first one so as you can see this is all
in a browser you can access it for free
and you yeah you can just use it
basically what is on those guided lines
here is a little bit what I'm saying at
the moment and a little bit more because
I added more content this is the Bosch
has nothing to do with Python or
tensorflow first I just want to go in a
batch of Python and then I can start
using tensor flow so to use transfer
flow I need to import it presentation
and to start working with pencil so just
build very simple computational graph
will take two Constance's and will add
them and to build Constance's constants
sorry you just use constant function in
tensor flow and you need to remember
that because you just defined it it's
still part of the computational graph
doesn't mean they are actually holding
any values yet because we didn't run
this computational graph so if I print
it I'm not getting values two and five
right I'm getting information that I
have two tensors of those types
and of this kind of a shape etc so let's
start a session and this is how you run
your computational graph and then use
session run as you can see I've used
session run put my inputs there and now
because now I run it now I actually put
all of this row calculations I have my
values to one and five and that's
basically the whole idea of
computational graph that it is a very
simple example so let's let's try
something more complicated well let's
build this adding example so I'm using
add function here for my add node and
let's print it let's print first node
and then running this node so first we
are expecting there will be a tensor
displayed and second the output of
adding two and five
now maybe I'll go out of presentation
mode okay sorry I think it's the network
problems so you can see there is a
tensor print printed and then there is
seven printed at the end let's start
with that okay
Constance's are not very they're not
very interesting but they're useful
because in many of our calculations we
need them what is more interesting are
placeholders and I don't have a better
definition but placeholder is a
placeholder so whenever you're building
your graph you're saying well this is
when I expect some input to my further
calculation so you're building a graph
and saying I will be expecting a float
number there and then I will be
expecting a float number there and at
the moment of performing session run you
need to feed your graph with the actual
values and I will show you how to do it
so let's create those placeholders and
then create the node that has that is
adding them and you can see that I used
plus instead of tensorflow
add because that's nicer and what we are
used to and it works also so now we want
to run our session with a specific
values so you can see I'm saying run my
session with this adding node and feed
it with two and a five for placeholder
one and placeholder to then feed it with
one point two three point five and then
feed it with two arrays one two and five
eight and only the recipe is the same
because I'm feeding it with different
values and there you go as expected we
have value seven four point seven and
six and ten because our eyes can also be
added together so that was easy the part
that is not that easy our variables the
variables are
kind of like consonants but they can
change their values so you have to
initialize it we have some values as you
can see I'll show you that here so we
have to initialize them with some values
so I'm initializing my a with value 1
and might be with value minus 2 but then
when you're performing your calculations
you can change its volume and if you
think about machine learning process
this is what machine learning process is
doing there are always some variables
there are always some parameters
coefficients that you want to adjust for
your lost function to be minimized and
I'll show you a very simple example not
really connected to machine learning at
the moment but maybe a little bit later
how to do it so let's think of a very
simple linear model a times X plus B so
we will have our two placeholders x and
y and we'll fit our algorithm with the
proper values and we'll try to adjust a
and B which will be our variables to fit
the perfect line for our points so I'm
defining my two variables and I'm
defining my linear model and because we
are using variables you need to do some
trick there is this one trick you need
to initialize all those variables before
your you're performing any of your
calculations so I'm initializing all the
variables this is just the step you need
to always do so what I am now doing is
nothing different than what we did with
placeholders I want to run my linear
model with X values from 0 to 5 so we
have our preliminary values for a and B
and we're fitting it with X so we have
we know the perfect point of Y would be
DOS
so let's start let's try to make it a
little bit big big grass we started with
the Green Line and we want to adjust it
to fit the blue line so that's basically
the goal if you want to fit the linear
model and of course normally you're
using some kind of our optimizers I just
know the values so I will just go
straight to the point and just change
those values but first I'm defining a
loss function so basically I'm taking a
differences from my points to the line
and I'm putting a square on it and then
I sum it up that's a very very popular
way of having a loss function then I'm
feeding my algorithm with x and y values
and I'm printing the loss function for
this as you can see green line so what I
did was I'm taking this point I'm taking
the difference taking this point taking
the difference and adding all those
squares of those differences and why my
loss is pretty high I would like it to
be zero in this case I know it can be
zero because I know the actual values
normally it won't ever be zero because
real data are not allowing us to be zero
and I know the exact values so I will
just change my variables and this is
where TFSI n-- comes in you can change
your a and B then you run it and then
I'm printing my loss again and my loss
is now zero but this is the only time
when you get the loss zero in the world
word real word it will never happen so
let's go back to my slides and let's go
through more interest in actual machine
learning examples so we'll go through
classification tasks
that is hello work tasks for deep
learning which is classification of
handwritten digits so this is a very
well-known dataset and you can go there
download it play it play with it
yourself what is funny this dataset
works very well with a very simple key
nearest neighbors are gourd we don't
really need deep learning for it but it
is those are images and it's it's very
good exercise to do it on this data set
and neural networks are trained very
quickly on it so I encourage you to go
through this exercise so how how do I
approach it or how tensorflow in default
as default is approaching it those
images are 28 by 28 pixels so thanks for
fall is flattening them so our input
layer will have 784 input neurons then
the training process is a little bit
different than a classical training
process because you can probably never
get an example online about deep
learning neural networks that is trained
on the whole training set it always
works in batches first of all it it's
quicker this way because you don't fit
in every step of your algorithm you
don't feel it with the whole thing if
you think that in this dataset has
20,000 of images it's quite a lot every
step would take ages and second of all
it's kind of it's written over if you
think overfitting is the thing when the
machine learning algorithm is learning
by examples by heart so almost never you
will get an example online about
tensorflow or deep learning that is not
taking batches into account and once you
have your trained model what you can do
this is what actually we will build in
in a second in the second demo we have
our input layer we have our output layer
which has ten neurons because we have 10
labels we have 10 digits and once this
whole thing is trained which means all
of the weights and
of the biases are adjusted we can feed
it with the new example and hopefully
we're getting the label 9 for this
specific example so let's do it
let's build our computational graph for
the machine learning process
I will just reset it to default
so remember what we want to do we just
want to build one output layer that has
ten neurons and all I just said is here
so we can then go through this first
what I want to do is I want to import
all the tensor flow stuff and I want to
read my data set and tensor flow makes
it very easy because it's a hello world
data set so it has capabilities of just
reading it downloading and reading it so
you don't have to do it yourself and
second thing we want to do I want to set
up some variables some numbers so I'm
not using numbers in my code don't like
to do it our image size is 28 as I said
label size we have 10 labels learning
rate is 0.05 we will train our neural
networks in a thousand steps and batch
size will be 100 which is pretty small
but if you think of 20,000 examples in
your training data set so let's let's
train our staff so first I need some
placeholders and this is where you will
very often get stuff in examples or in
real life you'll get your training data
as a placeholder and you say when you're
training it or then when you're
evaluating it use this data set use
these images and then you think but
those are the labels so for example when
we are thinking of our training training
process those are the true values adjust
the coefficients so they fit those real
values the best way but you can use
those labels also when you're evaluating
later your algorithm I will show you
that in a second and then we need our
variables this is the tricky moment so
we have variables for the weights and we
have variables for the biases and then
we are creating the output layer that is
multiplying our input data with our
weights and adding the bias so I have my
variables
just initialize them and in a different
way it didn't actually put numbers there
I just said use normal distribution for
weights and use constant value 0.14
biases and my output layer is just
multiplication and you cannot actually
do stuff like X time W because this will
multiply your mattresses like literally
take piece by piece and multiply it so
it's not matrix multiplication that's
why you need to use this function what
else I want to do is define my loss
function so my loss function is reduced
mean we have softmax cross-entropy with
logits and i'm not going to go into
details it's all explained in the
scenario you can go through this late
later and it basically means adjust it
to the true values of the labels I will
provide you with and then I'm saying
training step every one one of the
thousand of the steps use gradient
descent optimizer with this learning
rate and minimize my loss so I really
like this level of abstraction but if
you don't know much about what's
underneath you can get pretty lost and
this this is kind of a point I'm trying
to make here tensor fee is one of the
tools that you can reach use for free
it's open source etc but if you don't
know what's underneath you will just get
some values and you you won't be able to
well basically judge if this is an okay
value if this is a meaningful value will
this bring anything to your business but
basically what we are doing in this line
is this is my training step use this
optimizer to minimize my loss function
and then I want to build another
note that I call a courtesy not just I
called basically I want to build a note
that is checking how many times I was
right what what's the rate of me being
right or what's the rate of my neural
network being right so it's it's taking
the number of times the network was
right when classifying images and
dividing by the number of examples and
there is this part when we need to start
the session and initials initialize all
the variables and then start our
training process so what I'm saying is
you know mine is data set
get me a batch in every step and then
feed my algorithm with input part and
input labels so this is where
placeholders are being filled in you're
getting your input data you are getting
your labels and your fitting it with the
training algorithm and what I also want
to do I want to see how my algorithm is
performing every 100 steps so sorry
so I'm evaluating this is another way
how you can run the session and actual
calculation instead of doing session run
you can do eval and remember this
accuracy note we defined before I'm
saying take take my badge and see what's
the accuracy at the moment of my neural
network on this specific batch and then
just display it and the last thing I
want to do I want to evaluate my model
once it's trained so you can see here
we're training our neural network and
after those thousand steps we have ready
to use model then we can classify new
examples and I'm taking those new
examples from minused test data set and
I'm just running all this accuracy thing
on on the examples that weren't
available during the training phase
okay we are now ready to run it so first
thing sometimes takes time but you can
see it's a very simple network so it
worked very quickly and I will show you
more at this examples in a second
but you can see that first it let all
the data set I can even see the folder
here we have this data set then it went
every set every hundred steps and
calculated accuracy on this on the
current batch and at the end we just
checked how does it perform on the new
examples and you can see that you can
see kind of a progress but it's not very
consistent because it's only being
trained on few batches and it's only
calculating the accuracy of the specific
batch
so that was building a calculation
computational graph very powerful tool
not always needed and I will show you
that in my second statement tensor fall
is giving you different levels of
abstractions so you need to choose your
level you need to choose how are you
comfortable with all those details with
all those variables with all those
biases do you actually want to adjust
those variables maybe you just want to
use something that is out of the box and
algorithm and say fit it train it and on
the high level when you think very very
high level of machine learning deep
learning it's no different process is
you get the day time to prepare them and
clean them don't be fooled this part is
taking up to 90% of your time when
you're working as a data scientist when
you're splitting the data or reading the
data from different sources so you have
training data and test data which we did
for the previous example and then then
there is this magic thing fitting the
coefficients once you have a trained
model you can evaluate it so maybe
tensorflow will give me this high level
API that I just have my data I can just
fit it a value
you
so but you know some stuff about machine
learning maybe this is the this is the
moment when you can start approaching
tensorflow
and I will show you in a second that
there is no computational graph at this
level of abstraction it just this API is
just doing it under the cover so you
don't have to actually build it yourself
so let's build a little bit more complex
neural network
so remember input layer output layer and
we have a hidden layer now so this
hidden layer will have weights and
biases in the same computational model
as our output layer had before they will
be just different and they'll have
different sizes because now this input
layer is connected hidden layer is
connected to an input layer and output
layer now needs to be connected to the
hidden layer so what we want to do is to
use something so a model that will be
this neural network with the hidden
layer fit it evaluated maybe predict new
things
so estimators I will use and they're
different estimators you can use like
linear classifier I will use the nm
classifier which goes for deep neural
network classifier and it also is
building dense neural network and this
is what what we will use for the same
example so let's open the code I have my
image size 28 label size 10 and another
variable I put here is hidden size so
the hidden layer will have 1024 neurons
that's basically it I'm reading the data
the same way I was reading it before
with one difference there won't be one
hot vector there will be actual integers
that are representing our labels this is
some function that is helpful
help-help function and yeah this is
basically how you're defining your
network so let's go through it DNN
classifier deep neural network
classifier because we'll be classifying
stuff we have feature columns which I
basically set up the dimension image
size times the image size because we
have 28 by 28 hidden units we only have
one hidden layer but if you want more
hidden layers just put anything you want
here it will just create another layer
another layer we just want one
we have label size we have ten labels
and I'm using atom optimizer not
gradient descent this time so it's very
simple I'm just getting my input and
saying you know classifier fit it those
features with those labels using batch
size of one hundred and thousand steps
basically repeating what I had to do in
several lines of code before now in one
line with more complex neural network
and then I'm saying get me the test data
and you know classifier once you're
fitted evaluated
evaluated on the test data and I have my
chorus II and then once I have my Icarus
II I'm saying well take another like an
example from validation set and try to
predict it so yeah um I'll just run it
now it will take some time because it's
now takes time to fit it one output
layer was very quick when we added
another layer it needs to adjust
coefficients but yeah you can see it's
still pretty quick and you see Octus
accuracy now 97 it was 18 9 or something
before just because we added another
layer and those are predicted labels and
if you compare them with the actual
values at least for those 10 examples it
looks like it's performing pretty well
so this is a very high-level example and
yeah if you just want to enter
tensorflow
start from it maybe not even from neural
network start from linear model start
from k-means if you're doing clustering
etc so if we want to do something more
complex we need to go into details we
need to build our networks shape our
networks ourselves so convolutional
neural networks are the type of network
at the type of the layer in our neural
network that work very well happens to
work very well on images because it's
taking into account and only performing
calculations once in a once at the time
on this 2d or 3d pieces of our inputs so
imagine we have the image we have this
square that is going through through all
of this image and performing the
calculations it's still matrix
multiplication don't get me wrong it's
just we are so saving some space because
we have much smaller kernel much much
less of our weights and biases and
another thing is
if we're working with images and we're
flattening our files
we're losing what's like what are the
stuff near each other once we have
kernel like this one we have once we
have convolution and we are capturing
stuff that are near to each other
it's proves to prove to be very very
effective another thing another layer
although I don't call it layer it's a
polling layer so after you're doing
convolution usually after you doing
convolution you're doing something that
is called polling and again there is
this square that is going through your
layer and trying to get either an
average value or the maximum value or
minimum value average and maximum are
the most popular so let's build a
network that is very complex now we have
two convolutional layers we have our
hidden layer and you can then put some
drops out at at the end we'll see so one
problem with maybe not a problem one
different approach we need to do is we
cannot work on the flattened file we
need to reshape it to be an image again
because that's how convolutional layer
works so let's see how we can use what
we know about variables about weights
and biases to to build such a
complicated Network
and we'll be using tensorflow and n
which is a short for a neural network
namespace I'll go straight to the
convolutional bit and let's see how does
it work
I've put all the training into separate
files so don't be confused we'll just I
will just focus on how to build the
graph with all those layers so we have
convolutional layer max pauline
convolutional layer max polling then we
need to flatten more polling layer and
then we have hidden layer and then we
have output layer a lot of layers
placeholders are basically the same what
we need to do is reshaping our image
from the flat one to 2d now and let's
build our convolutional layer what we
first need is weights and biases nothing
different then from hidden dense layer
then we're using a function that is
called convolution or 2d and not to go
to many details
you really should understand what's
underneath first I'm feeling if with
those images then I'm saying use those
weights and use those biases and then
I'm using strides and then I'm using
padding so and this is the bias here
then I'm using poll polling layer so I'm
using Mac's polling layer and this is
moment when you're thinking like K site
strides etc and I really encourage you
it's it's the thing that could take me
the whole hour to actually go how does
it work and how to adjust those those
parameters and trust me on that that
this will work but experiment and this
is why I cut the code is right there for
you to see and try and fail and succeed
so this is our convolutional layer and
we have to do everything again for the
second one
so we have weights biases convolutional
polling layer and at the end we have to
flatten it so again
flattening it and then we have our
hidden layer weights biases hidden layer
and then I even added another layer so
called drop out ask me later if you're
interested would drop out this and then
I have weights and biases for our output
layer and I'm thinking this drop out
layer with output there so we have five
to seven layers depending how do you
feel about naming them layers I'm very
conservative I just think with a layer
is the one that is holding the weight so
for me we have four layers only but even
even tensorflow is not agreeing with me
because it's calling them a layer so
let's try to run it and this will take
time so I'm not going to go through the
end because we'll see what we'll see
this at the end what I wanted to tell
you is we went through very high level
machine learning fitting evaluating
predicting to building our neural
network from scratch maybe there is this
intermediate ground and there is maybe I
don't have to think about all those
variables all those biases maybe I don't
have to maybe I can do less work
I just want convolutional layer that's
it I just want dense layer and that's it
and there is a middle ground so we can
see this six to seven layers it's the
training is getting much longer because
we have so many coefficients to adjust
so let me go to another demo that is
taking instead of tensorflow and n it's
using tensorflow layers and this is the
level of abstraction some kind of
between there are a lot of different
level levels of abstraction I'm just
showing you a few of them let's see
the two convolutional layer I think this
will be next one yeah so basically the
same network will building just be much
easier placeholders reshaping same old
same old but I only have use
convolutional layer and use it with
those images
I need still some parameters of course
and then instead of wrapping it with my
relu activation function like I did here
you could see it right here I was
wrapping convolutional with reloj I'm
just saying you're a layer you know what
to do use this activation function when
you're done with your work so in this
one line in this one function not just
one line one function I just defined my
convolutional layer then the polling
layer and the same for the second part
instead of having a lot of deep
calculations and at least two of the
defining two of the variables being
defined I only have one line not just
one line just one function that I'm
using and then there is the same thing
for the dense layer I don't actually
have to define my variables I have a
dense layer and I'm just saying take
this as inputs use this size and use
this activation function and again this
dense layer doesn't have to be wrapped
in the activation function output is as
simple as this but I'm not using
activation function so if you want to go
in and try to tweak all those stuff this
scenario actually walks you through from
one layer to to all of them so you can
see how does it work and you can compare
how long the training is taking because
it is taking longer as you can see you
can go back here yeah finally it ended
training
and now it's evaluating so we have 90
7.15 so let me go back we went through
different levels of abstraction and as I
said I didn't exhaust the whole list
there are a lot of stuff there is Karis
level where you can basically choose
your own deep learning library so you're
using very high level API and saying use
tensorflow underneath use down or
underneath your coffee underneath and
different level abstractions when you're
working in tensorflow and building your
networks going through the third and the
last sentence is work with the
communities and by communities I I
understand everything from open source
from the support from all of the people
that are working in contributing to make
this product better and better the
service open source as we all know and
this is an actual open source project
one of the few that got popular and that
are actually working that people from
Google are working on and it comes with
with all the flavors all the great
things and also a lot of stuff that are
not that great which in my opinion it's
not just answer flow it's just every
project not just open source project
every project is the documentation
examples tutorials that are there and
this is great we have so many tutorials
even getting started has over the 10 you
have programmers guides you'll have
api's descriptions you have there is
this thing deploy and there are extend
they're also having tutorials and the
examples and if you go to the github
repository you have all the code that is
there
the problem is tensorflow is releasing
new versions and documentation is not
being updated
and it it is going to be updated it's
just it's more important to build new
stuff then document them and that's why
we are working cattle Koda with
tensorflow a team at the moment to
tackle this problem it's not maybe like
a problem that we cannot live with it
because we've been living which is from
every project of course but if you're
thinking of getting an example from one
of those tutorials and they wrote it in
the previous version you very often
don't have even information which
version was it and you're trying to run
it you get some mornings sometimes
errors and there's nothing in between
what the tools like kata-kata give you
is the actual version you're working
with the code that is worth working with
that version the version changes the
environment stays the same if you want
to change it if you want to upgrade it
you know which version was it but there
are a lot of great things and I would
say even another level of abstraction
and we've built a nest neural network
that was doing the classification
problem for for those specific images
but if you want for example translation
this is the example of the translation
there is a model somewhere there in the
github repository for those people who
built the whole network trained it and
sterilized it and it's somewhere there
you can just download it build your
model use this Python script and just
translate stuff this is very very
powerful thing but for me it's still
like it needs one others one another
step or even another step and what do I
mean by that is I'll show the example
have you heard of image net no image net
is this database of images that people
went through the exercise to label them
and if you're building a neural network
deep learning machine learning
algorithms for image recognition this
the database you're probably will tackle
and try how your algorithm is performing
and there is this architecture called
inception be free I will show you this
architect in a second so you'll
understand why inception and this code
and this model is in github repository
so you can just take it use this model
and classify your examples
just put some image there and classify
it it's just we have docker no days
right it don't I don't want to take this
whole repository build the thing I want
to just have a process that will run
take it and serve me and there are some
attempts in the internet but I think
they are working with the previous
version of tensorflow
so if you follow the instructions and
you can kind of figure out how to build
this docker image but now you don't have
to because I will show you that image on
cattle coder this is by the way the
image v3 kind of an architecture pretty
complex right so good job tensor for
people that you actually build it
trained it and just gave us to do to
work on so the last demo I'm going to
show you is using image net and it's
using inception and Tucker
inception if you can see this if you can
see this architecture now you cannot see
it very clearly it has like stuff inside
inside inside
that's why inception so cata coda built
an image to serve inception tensorflow
inception server so we'll run the server
and we'll query it against few images
and check how does it perform which
labels it will will give us this image
if you want to use it at home not on
category is also available publicly so
if you have docker if you just use what
I'm using it will work you just download
this image so what I'm going to do I'm
just running the server and I'm using
this image and then I'm running the
comment to setup the server inside and
to serve me
with capabilities of querying it docker
PS yes yeah it's running so what I want
to do now I want to go inside this
server and I'm in a bash at the moment
and then I want to classify those three
pretty pictures it was really really
curious what it will come up with it was
quite disappointing you'll see
so first I'm just downloading it into
this server because they are not there I
just want to them to be there and then
there is this very nice query just using
ception client on the server and use
this cut image and let's see what it
comes up with
so if you go like up here we can see
this Network clearly needs some training
because it's obviously a grumpy cut it's
not that great with the puppy I mean
with the puppy itself it's okay but it
recognized a tennis ball it's a red
balls not a tennis ball right I was the
most disappointed with the dog didn't go
to our come on okay that's it that's the
community work I was going to show you
and I think things are flow is going
there there are a lot of there is a lot
of support in different ways like when
you think of how many clouds are now
supporting GPUs and even specifically
configured machine to run tensorflow
you can see how big it is if you didn't
know that before of course and it's
tensorflow
the best deep learning library I would I
would argue but I would also say it's so
massively supported by community that
there is a big chance it will it will
become the best one it is the best one
for us regular people that that is not
hidden in some big corporations inside
because there are also libraries that
companies are using internally so to
finish this I'll just summarize it we
started with the deep learning and
hopefully I convinced you about my
definition of matrix multiplication
because it's just all over the places
and you really really mean I really
couldn't believe at first but back in
the days years back how powerful it is
just simple adding and multiplying of
course there is some linear algebra
there and of course we have a lot of
calculus inside train it but it's still
very powerful and it still goes back
to this multiplying and adding then we
went through the three sentences and I
think this is a very good starting point
if you want to enter the tensorflow
build a computational graph use your
level of abstraction and work with the
communities and the point I'm trying to
make is especially with the second
sentence level of abstraction can be a
bless because you don't have to deal
with so many details but it also comes
with the price you need to know about
those details to use this proper level
of abstraction so with that I would like
to thank you and go to cata coda and
have a fun</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>