<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Diving into Elasticsearch with  .NET  - Russ Cam | Coder Coacher - Coaching Coders</title><meta content="Diving into Elasticsearch with  .NET  - Russ Cam - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Diving into Elasticsearch with  .NET  - Russ Cam</b></h2><h5 class="post__date">2017-01-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lwMzO8kxQ18" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're ready to get going yep okay well
thank you very much for everyone for
coming along to the closing keynote as
you that will be next year that's yeah
thank you for coming along there's still
time to go and see John Sookie if you
want to if you want to duck out now I
won't be offended that's totally fine so
i'd say twitter poll a few months before
this and said hey you know what do
people want to hear about elasticsearch
and it was a unanimous decision they
want to hear about using elasticsearch
word net so that's what this talk is
going to be about so just to kind of run
through a few other things that we're
going to we're going to look at today
this is going to be a very very example
heavy talk so I'm going to be switching
between slides and examples are like I
like to see code myself so there's going
to be plenty of code here for you to see
so we're going to start off basically
with what elasticsearch is how that fits
into the great scheme of the things in
the elastic stack and then we're going
to get up and running with the dotnet
clients so there are two clients and
we'll go into that in a little bit it
will take a slight detour to just kind
of go over mappings and analysis we need
to kind of do that because in order to
talk about queries and aggregations we
kind of need to just kind of dive into
that a little bit and then we're going
to finish off with completion and
recommendations so those queries and
aggregations are going to we're going to
look at a few different things their
buckets pipeline aggregations so for
this particular talk here I'm using a
few different data sets one of them is
the stack overflow data set I've taken
the entire history of slack overflow
questions answers uses their badges and
I've indexed that into lastik search
running on my old trusty laptop here so
if it holds up hopefully we'll see some
some interesting insights so they say
that every every talk every good talk
should start off with a pissy quote so I
scoured the internet I find this guy
saying something about elasticsearch
enabling use finding answers to
questions you don't yet know you have
and
think that kind of thumbs-up
elasticsearch quite well you have all
these dreams of data coming in you have
no idea what what you want to do with it
right now but but for the moment you're
just gathering all of that data up
shoving it into elasticsearch and then
at some point someone asks you hey this
server goes down or some attacking the
server over here and you're aggregating
all of those logs into elasticsearch you
can then start going and exploring those
particular pieces of information and try
and piece together a puzzle of exactly
what is happening there so what is
elasticsearch this is a very high-level
view so it's a distributed and scalable
search and analytics engine it's
schema-less
although we'll see that that's actually
an implied schema from what you send it
but we'll talk a little bit about that
in mappings you interact it with it with
the json rest api and there are client
libraries for pretty much every
programming language you can think of
and so elasticsearch itself is open
source that's under the apache license
bill it's built on top of leucine which
itself is also under apache license so
it allows you to perform real-time near
real-time search on large amounts of
data full-text search is well search
relevancy scoring aggregations
geospatial also handles multiple
different languages and there are
plugins for handling non-english
languages as well it's highly available
so you can run elastic search on a
single node you can run it on a bunch of
nodes and those can be separate machines
they can be the same machine and
elasticsearch will largely handle the
the scaling out of that for you so if
you find that you're reaching the limits
of your particular cluster you can add
another node elastic search will balance
the shards of an urban index across
those other nodes so elastic search
that's probably all I'm going to say
about elastic search because we could
talk for hours and hours about that but
elastic search itself is part of
something known as the elk beasts pack
so those of you who are familiar with
elastic search it's very the Elks pack
is something that's very popular within
the logging world so using elastic
search log stashing Caban
so using logstash to bring logs within
to elasticsearch than using Cabana as a
visualization and back year and a half
ago we also built on on top beats as
well so then we were kind of thinking
well how does the be fit into ELQ you
know is it ELQ B is it black is it club
so we came up with out B but we didn't
really we it's actually the elastic
stack so elastic stack comprises of
Cabana which is the user interface so if
you've come and visited the booth you've
probably seen the nice pretty visuals
there are various different data sets
that we've had elastic search there is
the the actual search analytics tension
where all of the data is stored and is
searched then logs fashion beats are the
ingest to actually get data into elastic
search on the side there x-pac are a
bunch of additional commercial plugins
elastic offers that provide additional
functionality to elastic search and then
finally elastic cloud is our elastic
search of the service offering so we
take care of managing the cluster for
you if you want to scale it up we take
care of that getting it down etc etc
multiple use cases people using it for
log analysis so Verizon and after using
it for analytics of the Mars rover
telemetry data github uses it for search
so if you've ever searched for anything
on github that's going to be hitting an
elastic search cluster so enough
preamble let's get started with dotnet
so the dotnet side of things so there
are essentially two clients
there's elastic search net which is the
low-level client so it's a very
unappealing dependency free client there
it's has the entire lastik search api
mapped it works basically with strings
objects lists of strings lists of
objects and byte arrays very unappealing
ated that's the low level and then on
top of elastic search net you have nest
which is the higher level clients of all
the all the request types all the
response types are mapped to strongly
typed c-sharp pipes for you to be you to
work with
uses JSON net under the covers for JSON
serialization and it exposes and
actually can uses elasticsearch net
under the covers so you can still use
the low-level client if you're using
nest simply by calling client low-level
you have access to the low-level client
some of the nice things about Neff
though that it has on top of
elasticsearch dotnet and why I think
it's a better choice to get started with
is it provides you with additional
functionality things like covariant
results set so for example an elastic
search when you index data you index a
JSON document that document goes into an
index in elastic search and that
document is going to be of a particular
type as well you're going to expect file
types of that document and you may have
more than one type in one index so for
example in the case of Stack Overflow
you might have a question like overflow
question then you have a collection of
answers and those are a collection of
answers for a particular question so you
may have questions and answers both
indexed into the same index and you want
to return questions and answers that's
what covariant results sets will be able
to give you a nest nest handles that and
we will be serialize into the correct
type also has some functionality for
providing auto mappings so given a a
poco given a c-sharp type it can infer
the correct mapping to send so lastik
search there and provide some nice other
functionality there with operator
overloading for example if you need to
combine to query form a compound query
from two different queries that there's
some overloading there that allow you to
do that we'll look at examples of all of
these so let's get started so this is
how we would end up connecting to
elasticsearch with the low-level client
simply new offer client and this client
here is going to be trying to talk to
elastic search on our local host on port
9 200 so that's simple now sometimes we
want to provide additional settings as
well to the client so
specify a different URI for a start
maybe we want to use a different type of
connection pool so for example
elasticsearch may be running on a bunch
of nodes they all have a different
address to them we may want to use all
of those addresses and pass them to the
settings there so that when a request
comes in the client is able to send it
to any one of those nodes so same as
before
we can run that and we have a client so
the client here and CIMMYT so similarly
we've messed we can you up a new client
and we can see here that we have we have
the low-level client here available as a
property on the nest client so a little
bit more of a complicated example here
so this is using some of the some of the
additional next features that I was
talking about so we have a bunch of your
eyes here so we have a three node
cluster here running on line 209 201
mine 202 we pass those to a sniffing
connection pool so significant we have a
bunch of different connection pool types
sniffing connection pool is quite good
for clusters that automatically scale
because that sniffing connection pool is
able to send a sniff request to
elasticsearch understand if new nodes
have been added to the cluster or if new
earth nodes have dropped out of the
cluster and then use those in order to
send requests to the to the correct
place there in addition to in addition
to the connection pool we have a set of
connection settings here where we can
specify a default index for
elasticsearch for the client to use if
an index hasn't been specified on the
request we can also specify an index
here for a particular type as well so
when at any time that we're going to
perform a search on users posts
questions answers use this particular
index and we can pass in a collection of
indices there as well if for example we
have our data across multiple indices we
could pass in a comma-separated string
of indices to use there obviously this
is all over rideable on an individual
request basis these are just defaults
that we can run with
so let's let's hit our first make our
first request with the low-level client
so we're just going to perform a search
here simple search on the posts index
the question document type and we're
just going to perform a query here so we
specified our query here with an
anomalous type this is going to be
serialized into the JSON query DSL but
you probably seen on this site we're
simply doing a match phrase query here
this should never happen and we're just
going to take the response of that so
that the the generic string type here on
this on the search cool is the the
response type that we expect to get back
so if we run this lump we have a massive
blob of JSON come back which is a very
easy to read like this so what we're
going to do is we're going to run this
example again but now what we're going
to do instead so we we have the same
query as before but what we're going to
do instead is we're going to take
advantage of some of the connection
settings so what we're going to do is
we're going to disable direct streaming
so we're going to capture we're going to
buffer the request bytes and we're going
to buffer the response bytes and then
when that request is that she completed
we're going to logout the request and
the response to the console just so we
can see what's going on so if all of
these examples up stuffs what I'm going
to do so we can get an idea and
understanding of what's the incentive
elasticsearch and what's coming back so
if we run that same query again we can
see it we can see it's a little bit
easier to read now so this is the query
that we sent in much phrase this should
never happen so we have a 200 response
back it took 61 milliseconds and inside
appear inside of source we should have a
collection of sorry insider hits here we
have a collection of the documents
within elasticsearch that were a match
for our particular query so in each one
of these documents this source property
here contains the original JSON document
that I sent in so we can see there's a
few there's a
two different properties here I said it
they atop the title of the question so
what's your should never happen
exception the creation date the body
which we can see as HTML here so we were
searching in the body so this should
never happen so we should see that
somewhere yep
should should never happen inside of the
body and we can just see that we've
returned back the the first 10 records
there and in total that's 521 questions
in Stack Overflow that have this should
never happen in the body
so I'm just going to submit that with
all with the low-level client we're now
going to start looking at the high level
client and the rest of the the examples
here are going to be working with that
high level client
so I've done a similar thing as before
I've disabled their experience we
capture the request and response bytes
and again I'm just going to be logging
those out to the console so down here we
can see we're going to perform a search
request specifying our question type
performing a match phrase query same
couriers before we're just using the the
nest query DSL here instead and this
query DSL is based around lambda
expressions so we can see here we can
specify a query match phrase query we
want to run it on the body field and
this is our particular query text and
you can what you'll be able to see here
when I run when I run this query is that
the layer of this lambda API is very
very similar to the layer of the JSON
query DSL but actually ends up getting
sent elasticsearch so if you're ever
looking at examples with the the JSON
query DSL and you're trying to translate
that into what it looks like with nest
you can you can almost you can almost
picture what they're going to they're
going to end up looking like and we can
see again that we have we have results
back here interestingly for these
connection settings here I post I've
performed a search on questions but the
actual the actual URL that has ended up
being hit here is the local host just to
search so what I've ended up doing here
is actually searching across all indices
and all types in all indices and that's
because I didn't specify a default index
to use I didn't I specified a type here
which was the question type I didn't
specify an index so if I was to add in
an index now say to use the posts index
and run that one again we can see now
we're just we're just looking at the
posts index elasticsearch and we're only
looking at question types
so those of you that are less
comfortable with using the lambda API
there's also an object initializer API
as well so you can initialize a search
request object here same as before same
as using the lambda API except you can
work with objects here and pass your
search request object in the results of
this would be the same as as we saw
before
okay so we've kind of looked at the
first kind of searches obviously
elasticsearch can do more than just
search we actually need to get the data
into there first of all
so running through this particular
example we're going to create an index
we're going to specify one charge about
index so an index can be split into n
number of shards where a charred itself
is a leucine index so we're just going
to specify one shard for our
elasticsearch index here we don't need
any replicas we're only running on my
machine I don't need to replicate the
shard to other machines so we're simply
going to run with one shard here I'm
going to map my constant content type
here that I've specified and just going
to new up a new conference so for here
and I'm just going to index that into
elasticsearch so if I run that one we
see this put request here so I've
created the documents index with a bunch
of settings for which to use to create
that specified my mapping which we'll
look at in a sec and then I've just
posted a document to there and I've
received the two are 201 back there to
failure document has been created so
your typical HTTP response codes you'll
get back four created okay
bad requests etc etc
now of course if we're indexing at many
many documents we don't want to be
inserting those documents one at a time
so we can also create a couple of
documents and index those both at the
same time now what what index many
actually ends up doing it uses a the
bulk API behind the scenes there's a
bulk API for performing a bulk of
operations a bunch of creation
operations deletions update updates etc
so index many it's just a really nice
shorthand for using the bulk API so we
can see here that this has ended up
calling the bulk endpoint there are two
documents there that we have indexed and
we can see that we have a 201 response
back for each one of these documents the
order of these items that come back are
the same ordering as you saw it the
order in which we sent the bulk
operations to them so we should see this
what this one would be the Oslo Spektrum
Norway location and this one here would
be the Hilton Sydney
and as I mentioned index many is really
just the shorthand for the bulk API so
the bulk API here what we just saw is
exactly the same as doing this here so
calling the bulk API calling create many
passing it our collection of conferences
and indexing those into elasticsearch so
we can we can create we can index
documents we can also update them
obviously so if I create a conference
object here index it pull that document
back and then go and update it so here
we go but I put my conference object in
are then pulled out I then change the
end date so then change the end date of
the conference because we all want it to
finish tomorrow and then I've just
updated that conference document there
what I've also done here
elasticsearch of a concept of optimistic
concurrency control so when I pulled
that document back from back from
elasticsearch here I also got a version
number back as well so the version of
that document as it exists in
elasticsearch so that when I've gone and
done my update now so I have deserialize
that I have a conference object now in
my application when I go to make the
update it's possible that someone else
has modified that document in between so
I can also send the version number back
that I received when I originally
searched the document I can send that
back and update the document so if the
document has changed and the version
numbers changed I should get thought I
would get a four a nine conflict back to
say hey this document has actually
changed in between you fetching it and
you updating it so we don't actually
have to we don't actually have to grab
the document back each time we can just
perform an update instead by sending
just a partial document to elasticsearch
so again create a conference but now I
can just use an anonymous type that has
the same property name as the end date
here on my conference I can send that to
elasticsearch specifying the idea of the
document that I want to update and let
elasticsearch do the perform the update
on elasticsearch itself so we just have
a quick look at what the JSON ends up
looking like for that create my
conference get a 201 response back and
then I just post my document just purely
with the end date that I want to change
so receive a 200 back there and then I
finally fetch the document here just to
prove that that that end date has
actually been changed as you can see
here so partial updates good for when
you just want to modify things about
going faxing fetching documents and
similarly we can also delete individual
documents as well we can also delete
many documents in older versions of
elasticsearch there was a delete many
API that allows you to specify a query
for a bunch
documents that you wanted to delete and
in newer versions of elasticsearch
there's a task-based api for performing
that deletion as well so that was kind
of a brief rundown of all of the crud
operations and a brief look at some
searches so you have a JSON blob that
you send into elasticsearch you you've
indexed that document now what actually
happens so you have represented
elasticsearch cluster there as a gray
thundercloud there to represent the
speed which it operates lightning-fast
so we now need to look at mappings so
we've indexed a document what what is
actually going to happen so by default
elasticsearch is as I said before it's
schema-less but when you send a JSON
document to elasticsearch it's going to
by default first of all if you send it
to an index that doesn't yet exist it
will create the index for you and then
what it will also do is it will infer
the mapping for the document that you
sent to it based on the properties of
that document so for example if it sees
ISO 8601 strings it will expect most to
be date times if it sees integers it
will map those as integers so on and so
forth for all the primitive types you
can imagine where it sees a an object
property it will map that as an object
type so you can have nested objects
within your JSON document so in this
particular example here this is this is
my stackoverflow user with a bunch of
different properties ID reputation
profile age and they have a collection
of badges so these are the badges for
questions that they've answered they've
received a question received a badge for
the date which they've received that
whether it's a gold silver or bronze
badge
and a name for the badge as well so the
name for the badge is that usually the
language which they advanced that
they've received this badge for us a C
sharp F sharp etc so within nest we have
when you want to map a particular strong
type so those those default mappings are
usually good for when you want to get up
and running but for most scenarios you
actually want to control the mapping for
your documents the reason you want to do
that is there are various you know there
are domains in which you operate you
know more about your data than
elasticsearch does so for example in the
Stack Overflow questions the body of
those the body of each question is HTML
there's a bunch of HTML tags in there do
we want to be when we search do we want
to be searching with HTML tags in there
no we don't but by default what
elasticsearch will do would just map
that using a standard analyzer and we'll
end up indexing a bunch of HTML tags as
well which is what we don't want to do
so we look at mapping I use a type here
we're going to specify that it's going
to go into the users index and we're
just going to say to the client look at
this user type here look at all of the
properties on on it and tell
elasticsearch which properties to use
for the mapping for this user type so if
we run this one we can see here so the
properties here are yet the eyes the ID
of the user is an integer reputation it
comes through as an integer state some
offset a date so on so on and so forth
it's mapped badges here as an object
type and it's also recursed down there
and seen that each badge type also has a
string and integer and a date as well
for for the name class and date so
although muffinz great that gets us
going but we might want a bit more
control than that and that's where
properties come in so we can also map to
say to say to the client in further
types that you need to send a lot
such a user type but now I actually want
to override your in your the clients in
third types so I want to map age here as
a bite in elasticsearch because I don't
want it taking up in integers worth of
space no one no one's going to be older
than 255 so I'm just going to map age as
a type I still want to work with it as
an integer or as a nullable integers it
is in in my application but I want to
map it as a byte in elasticsearch and I
also want to map badges as a nested type
as well I don't want them to be object
types I want them to be nested typed so
I can also say auto map badges as well
and then I want to override the
properties there and whenever you see
the class for gold silver bronze badge I
want you to not only analyze the the
actual text that comes in for that class
badge so silver gold bronze I also want
you to just store it verbatim as well I
want you to store also what comes in and
I want you to store that in a field
that's going to end up being called
class rule so I'm matching mapping the
class property of a badge here in two
different ways I'm having it analyzed
because I may want to perform some
searches on it which we'll have a look
at in a second and I also want to store
it for Batum as well so that I can
aggregate on it and see the original
values so if we have if we have a look
at this this one here and we look at I
use a type here and we have a look at a
badge type so badges is being mapped as
a nested type our class type here has
the type of string so it's going to be
analyzed but then it also has this
fields object here as well with a rule
with a rule field a rule multi field as
they're called a multi field as they're
called underneath as well which is in
not analyzed field
another way that we can map as well so
we can we can auto map we can override
the mappings using properties we can
also pass a visitor in as well to say
for example we wanted to map all of our
string properties on our on our user and
we also wanted to set up a rule mapping
there we wanted a store verbatim the
original text all of the string
properties of a user we can use a we can
use the visitor pattern to do that as
well so we can use our not analyze
visitor here we can specify a rule field
specified as being not and not analyzed
and we can pass our visitor then into
the auto map function so that's going to
as it's going through and inferring all
the string all the strings field of our
user it's also going to map each one as
a rule field as well so if we just run
that one we can see here that display
name yet comes through all of all of the
string fields have also have a rule not
analyzed field as well
sort of a brief detour and mapping so
we've indexed we looked at how we want
to map our JSON documents now we need to
have a look at exactly how individual
fields there on our documents are going
to be analyzed and this is really kind
of if if there's nothing else you take
away from this talk this is probably the
one thing that's worth knowing when you
index a string field in silastic search
you undergoes analysis by default and
the analysis that undergoes follows this
kind of analysis change it uses an
analyzer and an analyzer is made up of
zero or more character filters a
tokenizer and zero more token filters so
what character filters do first of all
they will pre process that string maybe
we want to strip HTML characters maybe
we want to replace C++ with symbols with
C++ words then it's going to go to the
tokenizer the tokenizer is going to take
that entire text and split it into
tokens and then the token filters are
going to do additional things on top of
those tokens so they may remove tokens
that we don't want there so stop words
for example the R etc maybe they provide
synonyms for words there as well maybe
maybe they do a bunch of different
things as well so the standard analyzers
is the one as I said that normally gets
applied to string fields so it has no
character filters but the tokenizer that
it uses uses Unicode text segmentation
to break down that strip that string of
characters into words into tokens and
then it runs it through a bunch of token
filters here so it lower cases it
removes stop words so if we were to look
at basically what how that happens if
someone makes the claim f-sharp is
clearly the superior language then
running that through the standard
tokenizer we're going to get split up
into these
Brookins by the standard tokenizer so we
can see that the the sharp symbol is
being removed serves the stars and as
that's passed through the standard token
filter which is a placeholder doesn't do
anything right now so that gets passed
through exactly as it came from the
standard tokenizer then gets lowercase
and then stop words get removed in this
particular case in the normal fund
analyzer configuration no stop words
that you get removed there and then what
happens for those tokens they go into
the inverted index so the original field
from which these tokens came from title
and then we have LexA lexicographically
sorted the individual tokens from that
analysis stored in the inverted index
and then the ID from which that from
which that token originally came from
the document IDs from which token as you
can for can see someone else has made
the claim here for JavaScript being the
superior language as well so let's have
a brief look at how analyzers work so
I've got a very very contrived example
here first of all to have a look at I
have my type here a semantic version
type has an ID has a version string for
semantic version string so we all
familiar with semantic versions yep so
what we're going to do is we're going to
specify a bunch of token filters and
what these token filters are basically
going to do because semantic version
numbers they're effectively strings we
have maybe three or four elements their
main a major minor patch maybe a build
maybe then a pre-release suffix as well
we what we want to do is we want to take
that but we want to pad out each of a
major minor build such that for example
semantic version 11 comes with comes
after semantic version two so if we're
just looking at these lexicographically
we would find that 11 would end up
preparing before 2 so with this analyzer
here we're specifying a bunch of token
filters they're going to pad out each
each number there and then
so preserve that fruit that pre-release
suffix as well so we have a bunch of
token filters to do that that are going
to pad out the various sections and then
we have a token filter at the end that's
going to then d-pad all of those filters
and we put all of those together under
an analyzer a custom analyzer that we
have here called semantic version
tokenizer we're going to use is a
keyword one which is basically just
going to take the entire semantic
version string as it is and just pass it
to the token I - the token filters and
then we've just provided some mapping
for a semantic version here so if we run
this one we've created our index with
our with our custom analyzer then we
want to we want to check to make sure
that our analyzer is actually doing what
we think it should be doing so we can
use the analyze API here to do it so we
have we have a one point two zero here
we have one dot 149 zero alpha three so
yeah they still haven't released their
version so we're just going to run those
through specifying our semantic version
analyzer we just set up and see what we
get back there so we can see this is the
token that's going to get generated for
the first one so it's nicely padded out
1 2 etc etc it's also pended these herbs
on the end because this isn't a
pre-release and we want the the actual
releases to appear after any pre
releases and we can see that similarly
that we also have this token is padded
out here as well so our analyzer seems
to be doing what we think it should be
doing but we can we can run a little
test on that as well so we can index all
of the semantic versions from 0 0 0
alpha 1 all the way up to 19 19 19 so
let's index all of those
so we've indexed those and now let's run
a term range query on our semantic
version index so we want to be looking
for numbers between 1 to 0 alpha or 1 to
0 a and 1 to 1 so what we see we get
back here is 1 to 0 alpha 1 alpha 2
alpha 3 etc etc and we're just sorting
on the the key here that the actual
taupe which would end up being the
tokenized the analyzed key from that
interesting to point out here term range
query itself so some queries undergo
analysis some queries don't analyze the
input to the query the term range query
here doesn't perform any analysis on the
input to the query so what we're doing
here is we're cheating a little bit
we're actually expanding these tokens
out as well in a similar similar way to
which our analyzer works so that we can
see we can search on those ranges in a
sensible way and then we can have a look
also then it's worth to see well I
expected this document to match and I
didn't expect this document to match you
know what's going on there so we can we
can ask elasticsearch to say hey why did
this one match why did this one not
match so given the idea of semantic
version 4000 one of 8,000 and this
particular query tell me whether it
matches or not so we can see for the
first one yes within the explanation we
can see a description here that it did
match because it was in this version
range and if we look at the second one
we can see there's a failure to meet
condition so 8,000 didn't the somatic
version with ID 8,000 wasn't within this
range we can have a little look and see
why that might be the case so yep the
semantic version is at 2 to 0 alpha 1 so
it wasn't within the range that we were
specifying in our query
so let's have a little brief look at
queries okay so we're not going to be
looking for line or this time but we're
going to be looking at some Stack
Overflow data so touch briefly on
queries and filters queries essentially
fall into two kind of buckets you have
structured queries so does this
particular value fall within this range
that does it match this particular value
does it match this reg X does it match
this particular prefix and then you have
unstructured full-text queries these are
more kind of a blended more of a more of
a fuzzy kind of query so tell me stuff
give it give it a score to tell me how
relevant this thing is to my particular
query and that uses things such as
tf-idf BM 25 they're two different types
of similarity algorithms to determine a
score for a particular document and so
let's have a look at list of a look at a
few different queries so again we're
just indexing our conferences the ones
that we saw before one four zero one
four one four here and then we're just
going to perform a query on those and
we're just going to look for those that
contain spectrum or Sydney match for
spectrum of Sydney so we can see we've
got a back we got a match for both of
them but we can see here those at the
the hilton sydney document was actually
scored higher than our oslo spektrum
norway document whilst looking at the
location field does it does anyone know
why that might be the case okay so yeah
so it comes down to something called
norms which is based essentially on the
length of that individual field so
although a spectrum Norway's having
three words and being a longer being a
longer field than Hilton Sydney Hilton
Sydney has ended up being scored higher
than then also spectrum Norway so what
we can do is we could look at just
disabling norms for that particular
field when we index it so we can go and
have a look at using a non disabled
so when it visits when it visits any
particular string query let's disable
norms for that field when we actually
construct the the mapping for that for
that type and let's perform the same
query again and we can see now that we
get the same score out now if the hilton
Sydney and for also spectrum Norway so
we can also perform a term query here so
let's index those same two documents
again have a look at Sydney so we don't
get a match what's going on forth so we
have a look back to the analysis is
because this location field has been
analyzed and it's been analyzed with the
standard analyzer so it's being
lowercased so as it's gone through
analyzer it's been tokenized and then
it's in lower case so a term query here
doesn't perform any analysis on the
inputs of the query so we specified
upper case Sydney here we don't find the
match and we specify lower case Sydney
you should expect to find a match which
we do so we get our Hilton Sydney
document back we can also use the
verbatim rule field that we also set up
here as well so using location suffix
rules so we can still strong type
against the location field here but then
specified to use the rule field that we
set up earlier and then we can use the
original value that we actually provided
here so we would expect to see a match
here for Hilton Sydney as well which we
do
so then we can also combine queries
together here so - too much queries here
one for Sydney once the spectrum and we
we've put those inside of a boolean
query and assured query here so
either-or and then we've also applied a
filter here as well so filter is not
going to influence the scoring but it's
just going to filter on those that have
a name that contains NDC and also that
now going to boost anything that
contains spectrum we're going to give it
a boost of two so we would expect to see
those appear higher up then those from
Sydney yep so we see our also spectrum
Norway appear first and as I mentioned
earlier we actually have a shorthand for
this so boolean queries compound queries
of a very common case so within nest we
have a way of actually shortcutting that
so with operator overloading so here we
can specify the same query again we can
or these two queries together and then
we can and this filter query here so
using the unary plus there we can
specify a billion filter there and we
can run this one again we will get the
same out but you can see that this ends
up being translated to a boolean should
query with our too much queries and a
boolean filter there as well so yep I
briefly touched on covariant types I
have questions and answers within the
same particular index in aiesec search
here so we can run a query here looking
for a sink c-sharp a sink awake with
weight within the body and then all I'm
doing is dumping out just that the type
that that particular document has been
deserialized to so we have a bunch of
answer types here and a bunch of
question types so that was kind of
looking at text and looking at term
queries and a couple of range queries
now let's have a look at a geo query so
as well as all of the stack overflow
data I've also taken all of the
Australian suburbs and I've index those
into lastic search so we we have the Geo
JSON
Gong's for every single Australian
suburb here so let's have a look and see
which suburb this this conference has
taken place in apparently we're in
Willem Alou which I don't think is true
but this is based on 2006 suburb data so
I've just done a geo shape point query
here specifying the geometry field which
contains the actual polygon or multi
polygon for a particular suburb and I've
just passed it the coordinates for this
so the Hilton Sydney so we can see
Willem Alou there so taking that a
little bit further why don't we try and
find the surrounding suburbs to Willem
Alou so again we can use a geo query
here and we can do an intersection so
let's grab the polygon for Willem Alou
and tell me which other suburbs
intersect with this given with this
given suburb so we got you go and grab
the suburb let's get the coordinates
from it and then perform a Jewish shoot
polygon and exclude this particular
suburb as well we don't want to get the
original suburb that we're passing in
back in the results so we can run that
one we can see there's a massive a bunch
of Geo JSON here representing the
polygon and this is our this is the
start of our query and then we can see
here that Potts point Sydney
Darlinghurst are the intersecting
suburbs with with Willem Alou now we
don't need to go and grab the polygon we
can actually just say to elasticsearch
hey I want to find the intersecting
polygons to a polygon that already
exists in elasticsearch so use this use
this ID and go and actually perform that
intersection on elasticsearch without me
fetching without me fetching it so this
will give you the same result here as
well just using the Geo index shape and
here we're only returning the names of
the suburbs here we don't want the
polygons for the suburbs coming back as
well we only want the names so that's
using fields there just to specify the
name
that was a kind of a brief tour of
queries geo queries term queries seller
saw a brief look at aggregations they
kind of generally fall into three
categories metrics they can give you
single new numeric values or multi
numeric values so stats for example give
you the mean median average and a
deviation variance etc buckets is pick a
bunch of documents and put them into
individual buckets based on some
particular query or based on some aspect
of each document so there are ways of
doing that to say specify put questions
into particular weeks in which they were
asked based on the their creation date
or we may specify to put questions into
buckets based on the the tags that
they've been tagged with so this
question about c-sharp question about
PHP etc the nice thing with bucket
aggregations we can aggregate on those
aggregations so we can specify buckets
and then specify buckets for those
buckets and buckets for those buckets
and n buckets deep and pipeline
aggregations there as well those work on
the output of other aggregations so we
will see an example of these ones here
so let's start off simple let's just do
a search using Mexico aggregation so
find me the the highest-scoring question
on Stack Overflow so the highest score
there is fourteen thousand seven hundred
seventy-two and it's a question here
about wise process processing a sorted
array faster than an unsorted array
there's a question about Java so that's
great so we've seen the max score
excellent let's go just go a little bit
further let's take questions and let's
look at the top five tag questions on
Stack Overflow and then for each of
those top five tags let's calculate some
statistics for them so meme media and
etc and just simply dump those out in a
table now so we can easily see those
so most questions about JavaScript
however the highest-scoring question as
we saw before was that you Java but if
we look here a c-sharp guys with our
questions are generally scored a little
bit higher than both Java and JavaScript
so maybe we're asking slightly better
questions so that was an example of a
bucket aggregation with another
aggregation statistics on top of each
one of those tags let's look at an even
more complicated example here so we're
going to have a look at questions tagged
vnx dotnet core from 29th of June 2015
and then we're going to perform an
aggregation on on those questions that
match our query but we only want to look
at the NX and dotnet core because
questions can be packed with more than
one things but we only want to look at
those two tags in this particular case
and then we're going to bucket those
into weekly questions and then on top of
each one of those weekly buckets we're
going to perform a count of the number
of questions there and then on top of
that cang we're going to do a moving
average so we're going to look at we're
going to look at the moving average of
those questions there using halt winters
and we're going to look at moving
average window of 12 weeks and we're
going to have a looking to see what that
gives us so we have it back in a table
here we can see the count we can see
date moving average etc but that's no
good for us we want to see it and we
want to see in a graph so let's have a
look in a graph so what we can see here
the DNX questions this peak here in this
moving average is when beta 6 came out
you can see here this is when RT 1 was
released and then we can see these
gradually kind of kind of taper off a
little bit but we can see boom there's
suddenly these dotnet core questions are
just coming in and this was when there
was a move over from vnx to dotnet core
tooling one of nice things with moving
averages we can also use it too
predict where these things are going to
go as well so we can make we can use
that data to make a prediction so let's
see where things are going to go in the
next six weeks okay so yes on that
course idea next questions are going
downhill but we can see hopefully people
getting more familiar with dotnet core
questions are gradually slipping off
nice thing with this moving average is
where I can take it takes into account
seasonality as well so as a brief look
at aggregation so let's have a look at
all auto completion so more kind of
familiar with Google of course we are
and if we want to do the same thing in
elasticsearch we just simply do the
sequel equivalent of we're filled like
this thing and as the data center burns
we you know we walk away into the sunset
and we can do that as well if we want to
in elastic search by using a prefix of
wild card or a regex query but that's
kind of not the way to do it really you
can do these these are crews that exist
but they're very the more ad hoc rather
than particular queries you want to use
to solve a problem there are usually
better ways so what better ways are
there there's something called a
completion suggester they are super
super fast they are particularly
designed for solving that search as you
type scenario so they use something
called an FFT which is finite state
transducer essentially a graph that maps
out the relative path from each letter
within a particular string of characters
you can also apply weighting here as
well so use a weight from so say for
example if you're searching Stack
Overflow for questions you might want to
weight those questions that have a
higher score as being more you know
being more important as you're searching
terms adjusters a slightly different
type of suggester they allow you to
provide things such as did you mean so
they're useful for spelling Corrections
so given given a set of terms show me
one show me terms based on what exists
within the corpus of documents show me
terms that may you know may indicate
someone has slightly misspelled
something and phrase adjusters are
really similar two terms adjusters
except they take into account the entire
phrase rather than individual individual
terms so we can have a quick look at
those so this is a regex query we can
see here the top top example here is
doesn't start with WH as we expected and
that's because this field here has been
analyzed and therefore we are seeing a
match here on when here on this
particular token here so we can see that
quickly that we're falling down with
this with this prefix query here if we
have a look at using a completion
suggester for that instead we can see
that we we start getting some answers
back and we have a slight bit of
fuzziness here as well so it can take
into account some spelling errors there
so you know so yet what the difference
so it's been able to understand that
I've spelt things slightly incorrectly
but that's kind of completions adjusters
terms suggestive Ruby on reins active
recorder let's suggest some other terms
for that so for the Ruby term for Ruby
miss spelt here we're getting back the
Ruby term here this is the highest
frequency so we probably miss spelt this
for range we can see here that we're
getting high frequency here for rails so
probably miss spelt as well active
recorder yeah okay active records maybe
we might want to
start to the user instead we might also
use the result of this term suggested to
feed into a query as well - maybe cast a
wider net so it's kind of a brief look
at terms suggestive so let's um look at
recommendations now so looking at things
given this title share me questions that
may already answer the question I'm
about to ask - what shards and
elasticsearch and we can take more like
this so given a piece of text or given
an actual document show me two other
documents that are like this document
and we can take that a step further with
function scoring so with more like this
we might also want to factor in other
signals about the document so our
friends at domain here for example
they're able to when you're looking at a
given property you might want to know
about other properties that have similar
attributes such as beds baths parking
etc but you may also want to wait those
properties that are near the property
that you're looking at higher than other
ones but then gradually grade degrade
the weighting that you apply to those
and maybe properties that are newer you
might want to wait those hot slightly
higher as well that's what function
scoring allows you to do and significant
terms is another type of approach that
you can take to get recommendations for
a given term so for a given field of a
document so we'll of I'll have a very
brief look at those things so performing
a more like this query reduce shards and
elasticsearch I'm typing out my question
that's my that's my question title we
can go and have a look in Stack Overflow
reducing number of shards how to improve
we can see a bunch of questions here
around shards and elasticsearch which is
what what we would expect so given a
particular document so the document that
we were looking at last time let's pass
that in use that to find docket other
documents similar to this particular one
so we can see here again questions about
elasticsearch kind of generally scored a
little bit low
so let's see if we can improve this by
using function scoring so maybe we want
to maybe we want to wait questions that
have a score a high score or higher than
other questions so we still want to find
questions like the one that we're
looking at but then we also want to we
also want to favor those that have a
higher score and we also want to favor
those that are newer creation of newer
questions because you know maybe
elasticsearch is changing quite quickly
so the older older questions may not be
Sorella Ventoux things now so we can use
that tool to weight things and if we
have a look at the answers now we can
see we've got some much higher scoring
questions coming out near the top here
and we've also weighted those as well as
a slightly newer so looking at
significant terms let's have a look at
those users that have the c-sharp badge
and let's look at other badges that
users that have the c-sharp badge also
significantly tend to have so some
familiar some familiar kind of tags
there those are answer c-sharp questions
generally dotnet asp.net link there's no
kind of surprises there that's just
running significant terms on c-sharp why
don't we just have a look and see
running it on those questions that are
tagged C sharp F sharp and vb.net and
now let's have a look at the significant
terms for each one of those different
badges so if we run that we can you find
something kind of interesting here so
those have c-sharp badges they're very
much on the asp.net MVC sequel server
javascript kind of train our vb.net guys
they seem to be stuck in Visual Studio
2010 WinForms yeah maybe a little bit C
sharp for cross-pollination there and
our F sharp guys well we're pattern
matching functional programming
oh camel house going away so it's
embarrassing very different some very
different badges there that we see
and we can use them significant terms as
well to power a graph exploration I
probably don't have time to go into that
but if you'd like to grab me afterwards
we can we can have a look through that
stuff but it's a it's another API with
an elastic search that since you can use
those significant terms to understand
some implied relationships between the
values within particular fields in
documents so for example we can look at
the F sharp tag here and we we can see
as we saw before in significant terms we
have these are the badges that are most
significantly related to the F sharp
badge so users having a sharp badges
generally have functional programming oh
camel Haskell etc and what we also
received back here are the relevant
weightings for those particular badges
so how how significantly housed
statistically significant is dispatch
related to this other badge and we get
the source badge here in the target
badge and we can actually use those then
to form to form a
to form a natural graph so with in
Cabana so let's open up Cabana we talked
briefly about Cabana we have a grass
plug-in so we can kind of do the same
thing that we saw in the API there let's
have a look at users have a look at
badges F sharp so this is using that
graph API in the background but it's
just linking those badges up together
there so we can see there's a very
strong relationship here between X sharp
and functional programming based on the
thickness of the connection there we can
see that there's a quite a large overlap
there between between those those those
badges
one second I've lost my slides that was
a brief look at recommendations and that
that was a very very brief overview of a
bunch of different functionality with
elasticsearch hope it was helpful to
everyone do you have any questions no
obviously did a good job yes
yep we can talk about it afterwards
because it's a very long answer but it
can complement it yeah but it's a yeah
world war chat afterwards thank you very
much and if you'd like to know a bit
more about elastic it's a slight and the
github repo for the client thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>