<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Goodbye history tables, hello full audit- exploring message streams &amp; event sourcing- M. Bustamante | Coder Coacher - Coaching Coders</title><meta content="Goodbye history tables, hello full audit- exploring message streams &amp; event sourcing- M. Bustamante - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Goodbye history tables, hello full audit- exploring message streams &amp; event sourcing- M. Bustamante</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QcHJl7DNaHI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone
so I'm Michelle darou Bustamante I am
founder of a company called science we
do consulting and I run a practice for
micro services and security we assemble
teams around projects I'll tell you this
because we tend to work with many
different micro services platforms over
the past few years try to you know work
with customers preferences platforms
Azure Amazon different orchestration
platforms and in these implementations I
get to work with some of the brightest
people that work with event sourcing
products for event sourcing
orchestration platform development and
so on
and I tell you this because in every
single implementation that we've done
for micro services for a larger
enterprise let me preface it with that I
have not seen a situation where adding
messaging for full audit log history and
so on hasn't been a valuable asset so
the emphasis for this talk is to share a
little bit about that with you in
goodbye history tables hello full audit
so I'm going to talk a little bit about
how we get to that understanding how we
try to make that manageable and what
that means to the concepts of
potentially message streaming and or
event sourcing so and then some demos of
course so with that let's start with my
micro services talk saying most people
don't need microservices how abouts hot
but my point really there is that when
you do need it when it does look like a
good fit then every effort involved in
messaging and in audits and in full
visibility into your logs of course is
absolutely imperative and invaluable ok
just like DevOps is absolutely necessary
for any successful micro services
deployment so with that in mind let's
talk about how we got here
and I like to start with hello world
1992 not because I'm that old or maybe I
am but you know we used to build one big
thing and try to compile and walk
and five hours later hope that was done
and today we have a different picture of
hello world which looks something like
this we have a little bit of everything
right there isn't an app you build today
that doesn't have you know back into API
so it doesn't have probably some form of
web front-end backends probably spa you
know back in for front-end api's or BFS
as they call them which is always funny
to me
shared ap is that might be called from
the same from JavaScript and then you've
got your mobile apps and back-end api's
and your third-party application partner
applications and ap is and API gateways
that help to manage those things and
hopefully give you some throttling
sometimes security although that is
questionable and then we've got of
course behind all of that back-end API
is that serve you know probably the
thrust of business logic behind all of
it
probably in a microservices here if
you're going that route and then async
processing of course and this is just
the tip of the iceberg because of course
there's machine learning and data
science and a bunch of other things we
could talk about too so it's already
complicated that's hello world what
about if we sprinkle a little
microservice dust on top of that how bad
can it be
right I mean we got here for a reason we
started with things like you know again
your monolith your simple one big thing
application where everything was
co-located and coupled the new word for
coupling is cohesion but used in a
different way then we went to
client-server applications to decouple
data right so another layer of
decoupling then we went to three-tier
for business layer decoupling with data
then we went to distributed components
so that we could be couple and reuse
actual binaries but that was also to
help with scale which of course had a
limit because the greedy clients with
things like corpora and you know decom
and then we finally got to what we
thought was the panacea with SOA because
that gave us distributed stateless
application components over HTTP and
soap of course had its heyday but really
we could do it with rest as well right
our HTTP services and that's typically
what we see now but this idea
also introduced the concept of data
ownership in a vertical so the idea that
through only the service boundary you
were allowed to access data and there
was controls there for around how you
get to the data what you can do with it
and so on and any other kind of
aggregation had to happen at the service
tier in theory and so with this we
gained some new insights into how we
could build distributed systems but you
know the vision of SOA and isolation of
data behind the service tier wasn't
perfect right because it works really
well at the Enterprise Architect here
because there you had things like your
ERP in your CRM and your CMS and
possibly a workflow orchestration
platform like a business talk server or
something or you could try to avoid
those two and the idea would be this
could really own its own data right
because they were whole systems there
were solutions in themselves that you
know really controlled that that
back-end and if you really needed access
to the backend data of course you would
just go to sequel or Oracle or whatever
was back there against the
recommendation of the vendor of course
because they didn't have a feature you
needed but the idea was you never did
that so this was you know your purest
view of SOA right I own my data behind a
set of contracts and that's the only way
in but when we started building our own
solutions for SOA it kind of got
complicated because now if you think
about something like what I might build
for a security system users you know
profiles permissions that starts to look
a little complicated because of course
my data is not as easily isolated behind
a boundary like that even though those
might truly be separate services one for
profiles one for permissions and one for
users and login and things like that
because the idea would be you know I
might need to aggregate two of those
things right I might need to relate my
users to profiles and permissions and so
on
so in an SOA world what I called little
SOA was really difficult to follow the
purest pattern unless of course you did
something like this which was you know
the Oh data viewpoint or data services
and of course that had no problems at
all
for example performance and aggregation
you know issues with the ability to
aggregate effectively we didn't run into
any issues at all with that so that was
solving all the problems hell no so how
about we move on and say the layers of
decoupling that we reached with SOA
we're pretty good right we got to the
point where we were decoupling
technology platform where we were able
to distribute actual services and
boundaries and data but we didn't quite
have the full pass and that's where we
still are sort of looking for that
vision of the best possible balance of
decoupling reuse scale distribution and
microservices promises to pick up where
SOA left off and make some of those
things that were difficult they're a
little bit more attainable perhaps
through principles we follow and
introducing other concepts right so
that's kind of where we've landed in my
viewpoint and you know a few of the
micro services architecture gold send
that feed this would be things like you
know producing these services that are
of course indeed a well-defined set of
functionality but the goal here being
business domain and I think that with
SOA we didn't really think about it as a
business domain right a bunch of
developers or architects got in a room
and said yeah that looks like a service
boundary and we just had a bunch of
technology focused isolation patterns
around data and schemas and it kind of
worked again up to that point we reached
and now we've got this alignment with
business that gives us maybe a new
viewpoint right on how we design and the
design step is so absolutely imperative
to get to the point that we want to be
at whereby we can actually have services
that own their own data and can Co
evolve alongside others without
interrupting each other which means we
have to be very flexible about data
ownership and how we eventually perhaps
project additional views into the data
and so aligning the functionality with
the business domain is step one and
again that is a finessing process around
design we'll talk a little bit about
that to lead into
the discussion of messages and again the
idea is to co-evolved we want business
features to Co evolve so that the
business people that actually need the
features can say I need this and the
development team and the DevOps team can
say yeah I can do that and it's not
going to break the whole system how many
people have ever deployed an update to a
system and said hmm I wonder if that
will have regressions it happens I don't
think I've ever encountered that at all
but yes of course it happens so the idea
is to try to get away from that problem
and again the isolation gives us that so
we want to remove barriers also to the
development process around this meaning
if there's a team that does golang and
that's a good fit for this
implementation and a different back-end
store you know we've got Mongo over here
and Cassandra over there and relational
data over here the idea would be it
doesn't matter right because that
ownership of the implementation of the
service and its data is purely isolated
from one another and the future calls on
its own and is distributed on its own
and has its own DevOps like life cycle
so we can now develop with whatever
tools make sense or we could have
third-party stuff come in and sit
alongside our other services and that
also helps updating features then
without impact to the other parts of the
system is the end goal DevOps and
automation being the thing that makes
that possible so for example you know we
don't want to be finding out after we're
live in production that we don't have a
disaster recovery plan that we don't
actually know if updating the service
will impact another and one of the best
ways I have found to ensure this
although this is not part of this
particular talk it's still an important
data point is by making sure that your
services the consumers of your services
are the ones that should define the
tests because the acceptance of the
service and what it's supposed to do
that integration intent is really really
important it's fine for me to say even
let's talk about a permission library I
want to associate permissions with users
I want to you know associate permissions
with users per tenant
I want to make sure that this permission
is always associated with that role or
something along those lines those are
business rules to get somewhere
integrated into the backend of that API
but no matter how will I document it
somebody's going to try to use that in a
way I didn't expect so if they tell me
what the integration test is well what's
going to happen is we're going to log in
we're going to go to this app we're
going to try to access but because of
the tenancy and oh by the way this other
entitlement oh I didn't think of that
you know that's how we're going to
validate access and that's how we're
going to you know lock users out if
they're you know there's frequent access
that's you know invalid so just throwing
at you that the actual integration step
requires the people that take the
services and integrate telling you how
they're going to use it so that the
validation of those tests then is is
provided by them and then you run that
as you build your service to know that
you didn't break anybody that's a lot of
work but if you do it it's going to work
otherwise guess what happens you get on
the phone and you're talking to the
people are integrating and they're
calling you saying the docs say this but
it's not working and you're like well I
didn't know you're going to do that and
then you add it to the docs and you
quote you evolve that over time so you
can either do it ahead or you can do it
later it's going to happen so continuing
then when we think about our view of
hello world what we're really looking at
doing with you know this vision of now
moving to micro services is a lot of
smaller things being deployed a lot of
smaller services that fit in your head
that are manageable amount of conceptual
functionality and that fit again that
business domain that use case and that
can then Co evolve and integrate with
other pieces of the system in whatever
way makes sense so each of those
services then ideally would own their
own data as we've talked about with us
away but now potentially making it real
and it becomes real because of things
like eventual consistency which we need
to get to so we want the business to be
happy they're going to say I want new
features and then what they're going to
get is this beautiful picture of I can
update
these two things and not affect anybody
else everyone can you agree this is a
vision we want to get to the question is
again how do we get there
DevOps Automation really really well
thought out design upfront and with that
design the ability to you know really
align with what the business intentions
are so the concepts that we use to get
here then are things like domain driven
design again the concept of services
being smaller fit in your head is a good
way to think about it right a small team
can manage this and what we'll see as I
start talking about messages is it's
also really important that it fits in
your head because when we start thinking
about all the messages that could apply
to one service and I go through my
example that has to be manageable and if
it is manageable then it starts to feel
really doable to actually build in
messaging to all the services once you
have some baseline patterns for usable
components and things so smaller
services services owning their data
potentially CQRS patterns because of the
fact that we want read optimized views
and also because we're doing eventual
consistency which implies that I'm going
to have some views projected from
messages perhaps eventual consistency is
a big part of that then right so
eventual consistency provides me with my
read projections provides me with my
aggregate projections and so on and then
of course backing all of this is message
streaming is the ability to have you
know message streaming events and full
audit of everything that happens in the
system that's the dream
and in a large system that has a lot of
workflows jobs that run at different
times things that maybe the current
system does that are hard to follow in
terms of what's the state of the system
now in this area of the business in that
area of the business it's completely
unmanageable for most people today
because they don't actually know because
there's no one view of what happens the
messages the logs and so on it's the
number one thing you can do to surface
to the top that data
even if you don't know how to query it
yet even if you don't have dashboards
yet even if you haven't made it look
good and made it easy to get to yet as
long as you're collecting and putting
that data that asset somewhere then you
will be able to build those views and
you will be able to get there and even
build perhaps you know process managers
on top that can say these messages have
happened therefore I know where we're at
on this particular workflow so it gets
really interesting
of course again part of the concepts we
care about and you have to continue to
emphasize is the DevOps culture because
there's nothing worse than not being
able to automate recovery or rolling
forward when something goes wrong and
and having the ability to with all these
little things running around have alerts
and knowledge of health of the system
which again is that side of the scope of
this talk but still very very important
to the whole concept of deploying a
micro services architecture that perhaps
lives on an orchestration platform that
has self-healing and and other types of
recovery mechanisms you want to gather
all that data to and make sure that
those logs are also surfaced right every
bit of data you can get to know we're
healthy and if we're not how do we
recover so let's talk about design a
little bit then so to get to where I
have a manageable size service that I
can work with that I can build messaging
around I need to first go to the design
board and when I work with customers on
this type of stuff it's interesting
because you know some solutions
literally have hundreds of applications
right like and and they've got some
off-the-shelf products they've got
back-end you know mainframe system still
they've got jobs cron jobs running all
over the place dropping files picking up
files
you know again all kinds of legacy
potential pieces of the puzzle and then
new code that they've written even new
monoliths right in the new style that
includes api's and web and so on so when
we go to the drawing board and try to
think what do we do I usually take use
cases across the various parts of the
so I mean this would be an example say
of some sort of fulfillments you know
business that has things like ordering
online and products and fulfillments and
the backend and of course you have
customers and maybe recommendations
engines so now we've got things like
machine learning in there right so the
idea is you need to go to the whiteboard
you need to pick out pieces of the
system and go through activity diagrams
that's what I usually do and I'll say
you know let's get the different
business people in the room for how this
part works how to shop in cart work what
kind of features do you care about how
does product ordering work how do you
manage that do you do product
development you have a warehouse it does
stuff and how do they all communicate
together and you start to go through say
some use cases of each of these things
and maybe at the end of say two to three
days locked in a room with all these
different discussions you end up with
some patterns that evolve like okay the
legacy apps look like this and the cron
jobs look like this and the COTS
products have these issues like I need
to replicate data and then I've got you
know some newer
applications that are sort of monolithic
but I could start teasing those apart
and you start to see where the problems
lie with the business now I didn't say
problems with the technology even though
those exists because the biggest problem
you want to solve if you go to micro
services is the biggest business problem
they have today because if you invest a
bunch of time in designing and
implementing your first go live with
this that eight months is so painful and
so expensive for the business that if
they don't get value out of it
immediately they're going to not back
the next phase and all that fun stuff
that the technology people got to work
on and get their hands in is going to be
gone so the business motivation behind
moving to micro services has to come
with I know it's expensive I know it's
going to be hard work here's where we're
going to get to I'm going to have full
visibility over these crazy workflows
that right now cause us to call
developers on the phone to look at data
stores and
five different places and file drops to
find out what's wrong that's a problem
to solve that's a problem that needs
visibility that's a problem that you
know if you can surface messages and
information and start building
visibility into that you've now made it
worth the many hundreds of thousands of
dollars that it might cause the company
to do the first eight month launch with
five services from there to a perfected
again disaster recovery full audit log
automated deployment process that whole
goal I've perfected to the point where
now and go from five services to 20 and
to then one hundred with less pain the
investments upfront 100% okay so when we
look at this design right now this would
be sort of a you know hand wave over
okay here's the basic moving parts that
this company has but I'm probably not
done because what I haven't thought
about is single responsibility principle
right like what are the one thing a
service should do to do well and every
service in your solution should do one
thing really well that doesn't mean it
doesn't coexist with other services to
do one thing really well that's not done
really well but you know the example is
there if you have one job it's
manageable that doesn't mean it's one
service right a micro service can have
more than one service in it that's
another misnomer it depends what if you
have CQRS
I still might have a shared scheme on
the back and I still might co-evolved
the read and write API but those would
go together or I could have user
identity related things that just always
go together in certain systems not
permissions and identity but to say
identity and profile it could go
together or it could be taken apart
depending how robust working with
profile is with this company with what
they do do you see so you have to think
about it doesn't make sense to
co-evolved it does it make sense that it
belongs together and that's where you
start
so these are finessing things I wish I
could tell you there's one guide that
says here you know here's a roadmap
here's how you design you know your
solution as a micro service architecture
there's principles to follow but you can
get stuck on the principles to the point
you don't make decisions so you have to
be prepared to iterate you have to be
prepared to take the time upfront to
look at the whole system start taking
apart where the big business problems
are then take some of those examples and
say okay I think we get the idea that if
we used a message based architecture our
pattern could be api's in the front
api's in the back command query and also
messages and then we can have a topic
that's the general topic that always
audits you know we get a full audit from
and then we're going to have funnels
into services so you get some patterns
out of this and at the end of that you
can start building out a POC that is
going to solve the biggest business
problem you have and that's where you
invest your first dollars so let's take
a simple example identity management
most people understand identity I need
to login I need to log out I might get
locked out if I have an invalid login
several times in a row retries I might
have self registration which requires me
to confirm my email
so there's processes around creating the
user object around creating a profile
and so on now identity management
involves other things it involves
permissions sometimes entitlements so
now I've taken one box on that and said
well this is really potentially for
different services for different micro
service you know verticals potentially
for sure permissions would be separate
from identity that's just something I
would say out of experience and also
because they don't belong together
entitlements could be in itself a huge
implementation I mean sometimes those
are done with exact mole and other
protocols sometimes those are just
custom code against a database that
looks at ownership you know user
management and single sign-on single
sign-on
the login logout lockout you know
process around identity usually in an
identity server these are the things
that you know I have to now look at and
say do they belong together do they
belong separately but you see what I'm
saying is identity management is not one
box it could be broken out once you
start thinking about the business cases
I'm going to evolve permissions
completely separately from you know user
management user management I don't know
yet I need to think about it so it could
be that each of those owns their own
data
I have users I have profiles or
registrations I have permissions and I
have entitlements loosely put as a
collection of data behind some services
so those could be for micro services and
they're going to be related so now we
have a problem and the problem is
services own data I can't get to the
data without the service or can i what's
the way that we solve this problem in
the post SOA days eventual consistency
so I have users when I create a user I
might create a profile at the same time
I need to relate them but they're in
different stores behind different
services so how do I decouple the
process of getting those things
eventually consistent and usually the
way to get that done would be with
messages because then I can somehow have
some form of guarantees and again that
that's a whole nother discussion right
because there are different ways to
achieve you know the not acid but at
least you know guarantee of eventual
consistency I don't want to take you
there right now but there's lots of
great reading on that with things like
eventuate i/o and events store as event
sourcing models because those become
pub/sub those become event stores but
also they're able to publish messages so
their data store essentially events and
also can publish messages as part of you
know a heuristic when you write and make
sure that's a guarantee so there's
things like products like that to do
those things so users profile
permissions entitlements all related so
at some point when I create a user and a
profile eventually those both need to be
written let's just say now when we look
at domain driven design we think in
aggregates so usually a micro service
would own its own aggregate and it would
only write to its aggregate so user
would be an aggregate with login details
in user state am I locked out am i
active inactive is my email confirmed my
profile first last name date of birth
all those other things unrelated right
not related to login nothing to do with
signing in signing out
locking has nothing to do with that that
has to do with information about me that
maybe shows in the app information about
me for contacting me sending me you know
calling you know CRM style that kind of
thing so my aggregates are clearly
separate but yet they'll be related so
the way that things get related in
aggregates is an identifier right the
idea is that the user ID of course will
then relate to an ID in the profile
right so profile will have a
relationship back to user ID but I can
still have services that own writing to
those for example so I could have a user
management service I could have a single
sign-on or login service let's call it
and now I would have ownership of
profiles and users now it's not perfect
because you can imagine when I create a
user or I create a profile I probably
need to create a user too so now I need
an eventually consistent model for both
of those things getting created so I
still need the services to own their
data but something to tickle the second
service hey by the way you now have a
user and this is where the messages come
in so I'm leading up to a story where I
can actually give you a demo and
hopefully this is painting a good
picture when we think about CQRS we're
thinking about the separation of reads
and writes and you know optimizing for
reads
so like my login server that would be a
great example if I have scale of
millions of users then my user
management stuff shouldn't interfere
with those optimized reads for getting a
user logged in because people are going
to log in a heck of a lot more and
request tokens a heck of a lot more then
they're going to update their profile
and/or be managed by the user management
you know area or UI so I could have
commands but then write to profile and
user notable that I could have a command
and this is you know sort of true of the
principles of command query a command
could potentially affect more than one
aggregate how should go through a
service boundary but it can logically
affect more than one aggregate where is
an event if you're thinking about an
event stream should only ever talked to
one aggregate and that's where we can
get the decoupling when you combine
command query with events because you
can have command query trigger messages
that then become events that lead to
each aggregate meaning there's something
in the middle here that we'll get to so
it will end up updating users and
profile as a result of a command so when
we think about command query we're
thinking about optimizing for reads so
my you know user management will read
from the eventually consistent
projection which is my read profile
store my read user store for optimized
logins would be over here and so the
idea could be that again this could be a
service or multiple services that own
the collection of these and this
aggregate is eventually consistent or
this read store is eventually consistent
this guy can trigger a message that gets
to users as well make sense excuse me
one second
so the point here again is sort of just
to tell you the story that we're leading
up to which is users even our complex
with users and profile we think about a
command like create user and it could
lead to updating a user and a profile so
there could be a crossover of the
services in the data and the way did you
couple that is messages okay so with
that let's do some work I'm going to get
in here and so here's what I've got
going here I've got on a local kafka
running and a local event store running
which I'll get to in a second part of
the demo so my my Kopke container is
basically going to be receiving messages
for all the users and profile requests
now in this example I'm just going to
use one topic but I want to talk a
little bit about aggregate topics of
global order as we go so this is already
get up and running and I'm going to go
over to my solution in the solution what
I have here is a few things so let me
kind of close this up so I have a web
app the web app is my harness for
testing messages so every time I do
micro services related message based
solutions there's a couple of things
that have to happen one I'm going to
design my API so that are responsible
for the business logic processing
messages I'm going to have obviously the
definition of what are the messages that
are triggered and I'm going to build
those out based on workflows create user
has to have a workflow or your register
user that I have to decide what should
that event be so I'm going to go through
that in the form of demo and explain how
that sprays into addition
messages later so my web app is going to
start with registering a user and we'll
take it from there so in my web app I
have a registration controller and we'll
hit register when I do so I have an API
on the other side that API is
responsible for the logic post
projection so my web app is going to
send messages publish my projector is
going to pull messages consume and it's
going to be responsible for creating the
eventually consistent projection of the
current user state and the current
profile state and it's going to be
responsible for generating other
messages that indicate something's been
done or something failed so what I'll do
is give it a run my store is an azure
dot DB so what we'll see is a no sequel
solution which is the projection so let
that load
yeah this is local so that's not
happening
I think I know what it is
there you go it's because the app URL
isn't set so when you do multiple
project start it's not set to the right
kestrels not set to the right port so
okay so what I have now is just the web
app running and it's going to generate
messages
so I'll click register and I'll create a
new user so just for the sake of bumping
this up I'll go to user 10 and I'll
create an email user 10 we'll call it
user 10 boo stuff so my user is when I
create it we're going to hit this so
create user async is the this is the
controller what I'm going to do is step
in and you'll see that it's creating a
user registration requested past tense
message so notice it's not saying user
created I don't know if the emails
available I don't know if the user name
is available I could go query the
current state and see if it's available
but I could silver raise condition where
it still fails the only way to guarantee
that I can access and use that email in
this request would be is if I move over
to the event storm model where I'm
actually hitting the stream and the
stream is the user and the stream is
named for the user emailed and I can
guarantee that if I got the stream I own
it so in other words you have to think
about your messages as what is actually
happening what is the past-tense
events that i can garantee just happen
because every single event means a thing
happens what I did here is I am
requesting a user registration and so if
I take a look at the user registration
requested message it's
defining first last etc so this is my
message and you'll notice that every
message has a base the base message has
an activity identifier because what I'm
going to do is activity tracing all the
way through from my logs through to my
messages to the other side that way if I
see a message that I wasn't expecting
and I want to go to my logs in Cabana I
could actually look up the causality all
the way through from the web app to the
sending of the message to the API that
processed to the writing of the data any
component that I'm logging from should
have the same I activate ID for that
entire request thread the other thing I
have is a causation which would be if a
message triggers sending other messages
then I will link those so I can say this
message actually triggered a failure
message after or a success and another
couple of messages so I can actually
have that causation and I can find it
and then of course correlation is more
like request reply so if we were doing a
sync to two directions and I can collect
that so the idea is my messages all have
you know some common denominator there's
usually other stuff that we're adding in
for you know production things this is
simple enough my registration is first
last email password hash notice it's
hash I can't write the password to a
message so my business logic should be
in an API upfront that knows what our
password hashing rules are and that same
component should be the one that's used
on login so there's things we can do to
make sure that's shared like have it be
in an API endpoint that's part of this
process but the idea is sometimes
there's processing before the message
and sometimes there's processing after
and you have to think about that when
should I generate my email confirmation
token not before I write the message but
after I process it when I'm sure I want
to send it so there's rules for you know
before and after processing so in this
case all I'm doing is I'm creating a
message saying user registration
requested and if I continue forward send
message async
is really just going to publish
shikaka now of course this doesn't have
to be Kafka
it could be Kinesis and Amazon it could
be event hub in Azure you can use
rabbitmq and others I tend to prefer the
models that have you know the topic and
you know storage 30 plus days potential
replay of messages from multiple
consumers and tracking those cursors and
this is just the pattern that I've been
using that doesn't mean it's wrong to go
another route so send message async is
going to call producer send and again I
can have logging throughout within those
components this is just an error because
I took too long so that should have
triggered sending the message the way
we're going to know for sure is if we
try to process the message though now
right now my consumer is not running
because I didn't actually run all the
things yet and so what we'll notice is
if I come over here I'm not going to
have anything in my data collection so
for example if I just click over here to
the data Explorer I'm not going to have
a collection I don't have any data just
yet
because it hasn't been processed right
so we're waiting for that let's go over
to this code and what I'm going to do is
I'm going to run the management API
because the consumer is going to expect
that to be there and that's the API that
didn't run when I was trying to run
multiple projects
and then I'm going to run the consumer
so this consumer is going to process
messages and it's looking at this micro
service topic so again thinking about
micro services I have a solution
it is my micro service it has an API
that is part of the micro service it
projects to a state store that is part
of the Microsoft ID to the micro Service
it has a projector that reads topics the
topic is tied to the micro service and
if the micro services fits in your head
small enough to manage you start to see
patterns where by this vertical is
actually easy to manage all the way
through and through topic messages
commands ap is on both sides of the
message processing the messages
themselves commands himself I think I
said the projector right this is my
micro service and there will be base
things that I can reuse across all my
micro services when we think about how
do I register events how do I use what
base type show use for all my events and
all of those things become reusable
stuff that help me with activity tracing
audit and so on so this one solution
then again becomes sort of a way to look
at the micro service so when I look at
processing the message what you'll see
is my projector or my consumer is
looking at every message it cares about
it may not care about all of them
because sometimes we're looking at an
aggregate topic versus a micro service
topic so I could say I don't care about
these messages but somebody else might
and so we'll get to that in a minute
because I have a history consumer that's
going to project all the history of
everything that happened so when I look
at this every single message once
processed will will will end up calling
the API so what I'm going to do is start
this consumer so that will kick off a
service and that will call the API so
here I am now in my user registration
requested API endpoint and it's going to
to create a new password user and save
that to my datastore and if that were to
fail then I'm going to get a failure but
if it succeeds I'm going to generate a
new message for user registration
success and I'm going to generate an
email confirmation token and send that
over to the user so now I've got a
correlation or a causality sorry from
the first message too to others so let's
go ahead and run it what I should end up
with here and those errors are okay
those are just messages we didn't
process and I'm not swallowing it so I'm
going to hit refresh
let me do this
it is preview afterall somewhere in here
there we go so I got my user and you'll
notice is confirmed is false so it's at
the moment what I have is an eventually
consistent user record I can also
generate a profile record from a
different message so right now I'm not
breaking that up but I gave you sort of
the visual on where that could go
and what I'm going to do is go over to
our UI where we were and we'll go ahead
and instead of stopping on breakpoint
I'm going to create another user 11 I'm
going to set another password and we'll
call that 11 and this time I'm going to
continue or quickly it's hitting the API
doing the same thing you'll notice that
it advanced me to confirm the email of
user 11 so I'm going to go ahead and I'm
going to put an invalid token because
I've sort of set it up so that I know
which one is invalid so that says
confirmation failed now I'm going to put
a valid token and we'll go ahead and
verify and it says successful which
means if the projector is running I
should get an update let's click this
again and you'll see an update from is
confirmed should be let's do this oh I
need a new user excuse me let me refresh
second user coming up
I think I had to click away and click
back so I'll do that again okay here we
go
and the second user user 11 confirmed
its true yeah
so I eventually got that message to say
update confirm the user so let's go
ahead and try something else
I'm going to go and login user 11 and
when we get to login we're going to hit
validate password so what this is going
to illustrate if we step in is that I'm
actually going against the eventually
consistent store to check if the user
can log in now this is a very important
statement because this is security and
we do this a lot with identity server
deployments under event store with Kafka
messaging and then some that are just no
messaging at all when you're using
messaging you have to make a decision
what is your tolerance for eventual
consistency and so if your tolerance is
we're going to monitor for your from a
DevOps perspective messages are
processing and if they don't we'll take
that risk then we can go against the
projection when we use event store we
actually go against the stream so we
always know for sure the current state
of the user whether they're locked out
or not whether they're active or not and
so these are decisions you make right
around eventual consistency and
tolerance when you're looking up certain
pieces of information in this case I'm
going against the projection but I don't
have to do that I could also go against
quote-unquote the message stream and get
the absolute truth because the message
stream if I'm doing pure event sourcing
becomes the source of truth becomes
what's true about the state of all the
things so you need products that help
you with that like event store or event
eventuate i/o
to make that easier or you can do fancy
stuff with Kafka that replays up to that
point right so that's a lot of work
otherwise you take the eventual
consistency story so that's my point
right okay so I'm validating the
password I'm going against that store
you can see I'm hitting store reads so
I'm actually hitting to see if I can
log-in the user and if I gave a good
password then I'm authorized right so
now I'm going to log out log in again
put a bad password and this time it
should fail right and if I try that
again
with a bad password a couple of times
I'll end up locked out so I'm going to
actually get rid of these breakpoints so
we can hit it faster let's try again get
rid of the breakpoint lockout okay
and at some point I should reach locked
out so let's go into our data store
click the user again unsuccessful
attempts five and I'm probably in locked
out state which means that I won't get
unlocked until the expiry passes so
after five minutes usually is what
that's set to and then I would be able
to get in okay so I can do with some
more things like reset my password you
know user eleven and what this will do
is request a password reset send a link
or an email to the user I'm going to
show you all these messages in a moment
because I'm going to go to the history
log so right now this is just updating
my current state right if I change my
password then resetting that should get
me logged in update the password login
login successful no not yet because I'm
still locked out so the point is that
updated my current state I still have
you know whatever my state is my
password has now been updated and things
like that but I don't have a sense of
audit yet right I don't have in a sense
of what has been going on while I've
been using the system so the projection
to eventually consistent state where I
have my user store or my profile or even
some aggregation of users and other
things somewhere else that I might need
at some point with a different
projection with listening to the same
messages or an aggregate of messages
that's still only projecting a state it
doesn't tell me the history so what I'm
going to do is I'm going to turn on my
history log so one of the things that's
interesting about working with messages
again thinking microservice manageable
number of messages thinking that I've
now got data that I've emitted somewhere
to Kafka that is there waiting to be
processed again by somebody else so I've
processed it to do my current projection
of state I may at some point change how
I process the generation of state and I
could go reprocess all of them again and
generate a new current state I can do
snapshots to make it faster to get there
but I can also later decide I need to
use that data in other ways like we
didn't use all the information in the
message to create the user record for
login or lock out or active inactive but
I've got this profile data that's now
interesting and maybe some other
messages that are related and I can go
process those and project something new
or in this case what I'm going to do is
turn on the history consumer so I'm
going to go ahead and start my history
consumer which is another projector it's
going to look at the same topic but this
time grab all the messages and produce a
history log of the things that happen to
the users so you can see this is
processing processing processing a bunch
of messages registration success token
sends registration requested etc yeah
and once it gets through all the
messages I should have produced a
history state so let's go ahead and
refresh over here and now I have a
history document or set of documents and
each of these is actually a message so I
now have for example for user 10 I could
say hey let's take a look at select star
where seedot user name equals user 10
and apply the filter
and oh that's first name haha yeah so
pair programming thanks username sorry
what what am I doing wrong first name no
I want user name user ten yeah so it
gave me just that now what if I say user
11 and what does that give me
just that so now I have a bunch of
things but this is a bit hard to read so
what I'm going to do is go over to my
query floor and let's say instead select
star from C where C dot user name equals
user 10 or 11 and I think I can do here
say C dot message types and let's run
that query so now you can see for user
11 I had user registration I confirmed
confirmation failed confirmation
succeeded login success user unlocked
log-in failed attempts incremented
attempts incremented attempts
incremented somewhere down here actually
it might have been over 5 would have put
me over the top for 4 locked so I guess
I didn't reach that but you see the
point I now have a history which has
more information than just the message
type and if I ever needed a history
table for the user I now have it right I
have everything that could possibly
happen to be Caesar I could aggregate
more than just this I could go to their
profile and pull that in I could go to
other topics and pull those in and now
I've got built-in data waiting for me to
do something useful with it when I'm
ready so you know it's very powerful the
idea that you have this full audit and
furthermore I can also have full audit
of things like unauthorized access to
assets you know so you're supposed to
track things like when you try to access
a
a pass and you don't have the permission
or you're not logged in and those are
different heuristics if it's the same ip
address doing machine learning on those
things as interesting but even just
having the history who granted that
permission to this user there's all
kinds of things we can go down the road
of but the point is if it's all done
with messages and then you tie that in
addition to the messages to your actual
you know logs so now we get into what if
I wanted to look at my logs and tie
those together with the messages so
right now you know if I take a look at
my command logs I'm actually also
shipping the messages up here in
addition to shipping up you know
failures successes and other pieces of
useful information and I could come in
and find messages that have a type for
example like let's say scroll down a bit
processing message password reset token
sent so for example if I open this guy
up or let's look for register actually
I'm going to go through here and find
register register is somewhere yeah
logging failed either locked out
lots of messages the reason I was
looking for you no registration created
actually is because what I wanted to do
is just take you to for example a
activity ID or a message identifier so
like for example this ID somewhere
message template oh I see what I have
here this is my fault I'm going to run
my elastic projector I have another
projector that actually ships up the
activity I did correctly because my logs
aren't currently doing that so I guess
the the main thing I was trying to get
to there is that you know I can come in
and I can find out data of one of these
messages like some identifier for
example add that up and then that
becomes my filter so if there's multiple
messages that have the same identifier
or causality then I get to see the trend
here but also tie that to the error logs
and things like that so how we usually
do that is with a component like a
filter that's attached to every single
call into API is out of API is to call
and write data every component that that
matters essentially every service
component so one last thing I wanted to
show you here is how this might apply if
I'm using something like events tour so
I have a simple demo here that just when
I run it it actually does sort of a
contrived workflow again test harness
for writing messages I always test the
messages first by looking to see what's
the output that I'm going to get in my
logs what's the output when I'm looking
and inspecting at the order of the
messages in the content of the messages
before integrating it to a wider
application that actually is where the
messages will be sent from for example
because that's a lot more work for test
so my harness would do things like
simulate I'm signed in I'm going to hit
enter I'm going to lock out the user by
signing in three times badly
I'm going to hit enter and then advance
the time five minutes so that I can sign
in true and then the end result of that
will be that I can come over here and I
can look in my events store and let's
come over here and see my stream browser
probably have a user here and a list of
message sign-in succeeded password
locked user locked sign-in failed failed
failed and locked so it's actually in
reverse order registered succeeded etc
and then each of these expands into what
is the data that was sent in the message
so the idea here is that when I want to
know the state of the user for login I'm
actually I'm actually looking at the
whole stream of messages and aggregating
that to a state on the fly which is what
events tour can do really really fast
and eventuate I do io does similar
things so the idea is my data state my
current state is actually here and not
in a projection at all and so that's
another very powerful model again I have
customers are doing this with multi
region Amazon deployments so it's quite
robust but it's a commitment so you know
a lot of folks go with just the
messaging route so that they can deal
with you know eventual consistency and
projections that they know and
understand rather than dealing with a
completely new technology because this
is another piece of work right but both
are excellent ways to achieve the same
goal okay
so on that note there's a lot of good
information here I mean when I do a
larger you know sort of implementation
usually we have everything from UI has
again for the Micra service vertical
models that turn into commands the
commands call ap is those commands turn
into multiple messages the messages tie
to topics or streams those streams are
then projected to call additional api is
over here to finish the job and all the
things that are not grey are really sort
of part of the single micro service
vertical but i may have other things on
the outside that project additional
messages that lead to other aggregate
projections that sir
other you know you eyes or services that
need to perform additional functionality
across multiple you know service
verticals that own their data
so anything that owns its data can still
be projected as long as the messages
exist somewhere in either a global topic
that could be listened to or a topic
that the project another projector can
listen to and pull and build other
aggregation so it's really incredibly
powerful and I think the main thing that
I'm trying to get across here is that it
gives you that full audit trail if you
just start thinking my service is
manageable I can do messages now you've
got your starting point once you have
the data you've got history built in but
you can deal with producing those
projections later when you're ready then
the potential for all these new views
can come up and you're not tied to the
data model in the schema because you can
project a new one from the messages in a
different way
that's useful to the next service
without having got yourself stuck I'm
not saying you can't make mistakes and
need to refactor because that can happen
but it certainly gives you a certain
liberation but that I have found
possible with with this approach you can
add on to this with full visibility of
all of your error messages and logs and
and successes and failures through the
system tie it to activities tie it to
activities and messages so the messages
actually correlates all the things that
happen from API to UI and so on and I
think at the end of the day the barriers
to this are always going to be cost
complexity resources and training so it
can't emphasize enough but if you choose
something that's valuable to business
then the business can get behind it and
you get your DevOps story your DR story
your implementation story down perfect
it in a way that that that you can be
resilient and recover from any failure
but also start showing business value
early because that's what's going to
matter it's all about the business
benefits and of course no pain no gain
so with that I'll let you go and I'll be
around this afternoon so if anyone wants
to talk about things we've done in
production I'm happy to have come
stations today thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>