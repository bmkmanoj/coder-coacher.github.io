<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance as a First Class Feature with NBench - Aaron Stannard | Coder Coacher - Coaching Coders</title><meta content="Performance as a First Class Feature with NBench - Aaron Stannard - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Performance as a First Class Feature with NBench - Aaron Stannard</b></h2><h5 class="post__date">2016-08-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LBUcDEzKxBo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">not everyone will go ahead and get
started even as people are son
in here so today we will be talking
about performance as a first class
feature in your Donna applications using
n bench so my name is Aaron standard and
i'm the founder of n bench and i'm going
to give beginning our talk today the
little story about how added bench came
about and some of the problems we were
dealing with annika dotnet which is the
other open source project i spend most
of my time working on so philosophically
speaking performance is a feature of
your application just like anything else
so imagine this you're all developers
who have employers who care enough about
your career progression development to
you know send you to a conference like
this you are proud of your work would
you ever go ahead and ship a feature
that you thought was really important
useful to your customers without testing
it would that be something that you
would deem acceptable and the answer is
probably not well it about the
performance of our applications so for
instance if we're shipping a web
application one of the things that can
make a really big impact on our end
users is how quickly we're able to
fulfill a response to users request what
would happen if Google every time you
want to go and load it in your browser
took but say four seconds to load the
homepage would you keep using it
probably not right you want to switch to
something faster or how about a cell
phone application so your iOS or Android
app that ended up using all of your
prepaid bandwidth every month would you
use that are switched to an alternative
that was better designed probably switch
or how about a desktop backup
application that took 10 hours to backup
and to end and use a hundred percent of
your CPU while it was running would you
keep using that where would you find an
alternative these are all extreme
examples but nonetheless realistic ones
of where the performance of an
application could make such a negative
impact on the end user as to drive them
to find an alternative so those so the
underlying theory here is that
performance is a feature or application
just like any other piece of
functionality but what separates it from
everything else is how
classically difficult it is to assess
and test the performance of our code
versus the functionality of a feature
when you write a traditional unit test
for a feature you basically have some
input and you expect some output to
occur right and you can basically
objectively determine that pretty easily
well how do you measure performance and
how does that vary by the type of
hardware someone runs or what else is
happening on the computer at the time
you wanted to gather that benchmark data
these are all problems that are actually
pretty nasty and so what I'm going to
present you with today is n bench which
is a tool that is designed to help
simplify all those problems and make it
so programmers can productively manage
the performance of their applications
without having to write large pieces of
infrastructure themselves so I'm going
to begin by sharing a performance story
from Akkad Annette so this is the other
big open source project that I work on I
just gave a talk about it the last
session here and it's designed to be a
high-performance distributed actor
framework and so that means that we have
to have some performance goals around
how quickly actors process messages how
quickly they were able to send them over
the network how much memory they use how
quickly they can start and shutdown so
forth these are all things that actually
really do matter to our end users
because all of them are building
applications that have their own
real-time response requirements we have
people who are using a cadet for
safety-critical systems do you think if
we had a four second GC pause between
when someone pressed and all system stop
button and when the machine turned off
that could be a problem yeah so here was
the problem that we had in one patch
last year we rolled this out to our
users and production and we got a bunch
of reports back right away saying hey
the my aqua dotnet cluster slowed to a
crawl where I'm getting something like
one fifth of the throughput I used to
get and that's the difference to my
system staying up and serving my
customers and going down and so we went
ahead and we loosely measured it and
realize that we were doing something
like 2400 message
per second which is terrible it's not
good and so the problem was when our
team decided to get on it and tackle
this problem the conversation looked a
lot like this one person asking maybe
it's the serializer that's the problem
then another chimes in my benchmark
shows it's rtcp transport that's the
issue this is probably me in this
conversation why benchmark shows the
performance is fine this users doing it
wrong then there's someone else's as I
profile because it's the thread pool I
found it then we have the one jerk on
our team who basically mentioned that
hermione dies and the deathly hallows
yeah harry potter joke and so the gist
of it was this became the sort of
endless conversation where it was like
watching dogs chase their tails except
in github comments where no one was able
to have a coherent conversation about
the performance of our code because
we're all using totally different tools
totally different environments totally
different hardware totally different
benchmarks totally different
instrumentations and you get it there
was nothing in common at all it's like a
bunch of people shouting in different
for languages to each other trying to
have a conversation so we're n bench
came into the picture here as I invented
it to put an end to this conversation
before it drove me insane so n bench was
designed literally to settle an argument
is where this originally came from about
what is the real performance number of
this feature of akkad on net and more
importantly how is each change in our
software going forward affect this
measure so the idea was to go ahead and
standardize our benchmark on one piece
of code measured with one
instrumentation measured on one machine
our build server was going to be the
single source of truth for this that way
when people want to have a conversation
about the performance there was one a
thaw rotative place to get that data
going forward and so here's what the
results of that looks like for us on the
academic project we started really
recording our end bench performance data
at the beginning of december and you
see this is sort of the messages per
second is what this graph is showing you
would see here on the left we're sort of
written site 2400 range right around
there as measured by n bench and we see
a little increase right here in the
performance of really tiny minut one
that's because we found some inefficient
thing we were doing with I think a
string format call somewhere in there so
he fixed that tiny nudge and performance
well then one of our developers spend a
week or two profiling some code and came
to the conclusion that we had a
contention issue and a custom thread
pool implementation we were using so he
went ahead and rewrote that and we had
this big jump in performance up to about
eight or nine thousand messages per
second again big improvement great then
we had another incremental improvement
here as a result of fixing something in
our serialization system I mean this
giant jump here was me screwing up a
benchmark so specifically I designed it
so the benchmark I started counting its
results before most of it finished
running so I fixed that and on top of
that because of the windows azure
marketplace doing something weird our
visual studio license on our build
server expired had to reimage it so we
should have lost our performance base
line here and it kind of jumped around a
little bit let me sort of stabilized
again this is the most important part of
the graph what never happens there once
we start measuring it the performance
never goes back down again there's a
saying so I'm a patre preneur addition
to a software engineer and this is
saying sort of amongst people who are
working in marketing at a startup
company that if you can't measure it you
can't control it if you aren't aware of
sort of what your metrics look like as
far as people using your product you
can't have a real conversation around
what you need to do differently the same
is true for performance by having this
sort of history of our performance
available for anyone to look at we can
have a real conversation on whether a
given piece of change to our code was an
improvement or not it stopped being the
sort of mysticism everyone knows a
stringbuilder is faster than your
pending strings together right well we
decided to go ahead and basically users
and bench system as a way of talking
about data
rather than talking about theories and
then the performance continue to improve
gradually over time to that's the other
thing to notice once you sort of set a
performance baseline and you start
testing against that you'll find the
performance tends to always get better
around the clock so really n bench was a
tool that we wanted to use to sort of
create a culture of measuring and
testing against performance so I look at
performance testing is actually a type
of defensive programming to some extent
we want to try to avoid our sudden
negative changes in the system memory
consumption being one example if someone
changed the default q implementation
that we use in our mailbox in acha
dotnet to be something that was much
more memory expensive that might mean
that our customers who run acha dotnet
on xamarin android or on embedded
devices running yo linux out in the
field wouldn't work properly anymore so
we have to go ahead and basically
protect against that throughput the
whole story it has told you was how we
didn't do that and it really pissed off
some of our users never want to let that
happen again garbage collection overhead
the number one enemy sort of any high
throughput application is going to be
your garbage collector for the most part
and the reason is it that one you need
to go and collect those older generation
objects gentoo particular unless you
have sort of asynchronous garbage
collection turned on that used to mean
you'd have a pause across all your
threads every 20 or 30 milliseconds and
that can actually make a real difference
in some of your in your total system
throughput and also your total system
reliability too so this is something
that we tend to keep an eye on so these
are the three metrics that and the
academic project we really care about
now other things the other folks using n
bench use a lot of them care a lot about
yo if you're developing a xamarin
application you want to know how much
bandwidth you're using right that makes
a real impact on this experience your
end users have so you might want to keep
an eye on how big your message sizes are
going over the network that'd be one
example number of network calls how many
round trips does a request a Web API
make the sequel server every time you
have to process it
if your entity framework the answer is
something really large sorry shared
resource contention is another one so
one thing that some folks will try to
track is forgiven scenario how many
resources how many different sort of you
know threads or tasks all trying to
concurrently access the same thing
contention overhead can be your real
cost in a system quick show of hands
who's ever had the debug a sequel server
lock contention issue in production
before Amen brothers and sisters so it's
a real thing real problem that we have
to deal with so this is where the
concept of performance lifecycle
management steps in so the way I look at
this is that for any given application
you have a set of performance
constraints that you have to live in
this is determined by the environment in
which your application runs so quick
show of hands is a application running
on someone's phone going to have
different performance constraints than a
big server-side application in Windows
Azure who thinks so yeah exactly right
the environment is sort of what dictates
a lot of those constraints the other
part of it might be the expectations you
set with your users how many of you run
SAS software as part of your as part of
your work quick show of hands a handful
of you do you have service level
agreements with your customers around
turn around time availability that sort
of thing those might determine some of
your constraints as well then the next
part of the performance lifecycle is the
notion of your performance critical path
now we'll cautionary note about this
it's not always obvious if you're
building a Twitter application on you
know I xamarin your performance critical
path might usually be the home screen
where you're loading all the content
that's what people spend you know ninety
percent of their time doing but that
doesn't mean that other parts your
application aren't on the performance
critical path to and i'll give you
another embarrassing example for market
net so unfortunately before we started
using and bench and even a little bit
afterwards the way we found out about
performance problems was typically from
our users and one example was we had a
situation occur
where we made a change to our software
we published it and we got a bug report
back about 48 hours later saying it
takes 10 minutes to shut down my actor
system now and so we started taking a
look through it and what turned out to
be the case well as we had a link
statement of someone modify between
patches that used to be lazily evaluated
someone added a to list call at the very
end of that so it was fully evaluated on
every request so we're sufficiently
large actor system that became immensely
expensive so you had to go and change
that back and come up with a better
construct for then we added an ad bench
test and that's never happened again
since but the performance critical path
can surprise you so the way I'd sort of
describe what's on the performance
critical path it's if a customer has
ever yelled at you about it it's on the
path then you set targets against the
past ten minutes for shooting down an
actor system with children actors is not
acceptable we said we set a target a lot
lower than that two seconds was the
number we picked so we want to go ahead
and make sure that we have a target that
is sort of aligned with the expectations
of our end users so if you're working on
a server side application a user might
expect to get a response back from a Web
API you design in 500 milliseconds that
means that the components that are
running inside that system have execute
faster than that right considering all
the other types of overhead you're
supporting there so you might go ahead
and bound all the other subunits of your
application underneath that one global
target then here's the tricky part
measuring against the target how do you
determine if you've actually hit or miss
the target or not and then my opinion
the most important part of the
performance lifecycle is maintaining
what's called the performance history
this is a record of how your application
has performed against its goals for
every single patch every minor change
every pull request puts a new entry into
the performance history what this allows
your team to do is produce a graph like
the one I showed earlier we can go ahead
and see how well your applications
performed over time
and more importantly it tells you who to
yell at when your performance drops so
blame distribution is sort of solved by
the performance history too so for every
regular update of your software this
would be a minor patch pull request
would be a good example you want to go
ahead and run this process over again go
ahead and measure against target save it
in the performance history for any major
version change your software you want to
go ahead and run the entire process
again reason for that well reason why
you'd presumably do a major version
update is because there's significantly
new pieces of functionality or little
pieces of functionality been
significantly rewritten or maybe you
went ahead and added support for a new
runtime you didn't have before great
example that we're all dealing with and
neck right now is dawnette core our
benchmarks that we had for dotnet 4.5
4.6 might be totally different than what
they are for dotnet core we might have
to set totally different targets that
are aligned to what that platform can
deliver and bench belongs in this part
of the sort of region here is designed
to help you measure and also assert
against your target as well as update
your performance history so n mention is
really designed to plug into your
continuous integration system allow you
to continuously measure against your
target and create a record of what your
what your performance was for each
change and ideally you want to do this
before that change gets merged into your
code base now after the fact
retrospective performance debugging
isn't fun so why is this hard why do
people have such a heart you know when
we have the sort of argument within the
academia bout what the cause of this
problem was what were the what was the
real challenge there why was it so
difficult well the first thing you have
to think about when you start measuring
performance is the fact that we're using
a pre-emptive operating system that
means that you're going to have things
like schedule at different schedules
that happen while you're running a
benchmark so for instance windows might
go ahead and schedule what run one of my
benchmark differently the run to and I
might see a divergent
result there as a result of that virtual
memory is another fun problem you think
you're allocating an object in the
memory when it's going to disk that's
going to show up in your total
throughput measurements other processes
are a bane of our existence too I gave a
version of this talk at dotnet fringe
last month and one of things I did was a
had two different and bench benchmarks
one was a stress test for Helios the
socket server that power socket net
where I had something like 700 open
connections on one machine here I had
another benchmark trying to assess the
performance of some simple function acha
net well the academic function ended up
timing out and failing because the other
process was using so much of the CPU
that's just something that can happen
next is hardware variability so cash
pinning is a fun little exercise where
let's say for instance this is sort of a
nuanced issue you decide ok I want to go
ahead and try to get rid of operating
system unpredictability as a concern so
I'm going to do the smart thing and run
multiple iterations of my benchmark and
then average them all at the end that
way you can sort of average out some of
the noise let's say you pick an even
number for the number of iterations
you're going to run well all of a sudden
the compiler in the CPU will get the
smart idea to go ahead and do some loop
unrolling and other stuff under the hood
and you'll end up in a scenario where
your result is actually pretty skewed
from what it would have been if you had
picked a prime number instead so that's
sort of a little fun hardware variation
you can deal with but another issue is
actually the generation of the chip
itself what if one machine your bench
marking on has a newer generation Intel
than another it might have more l1 l2
cache available and it might also have
potentially different optimizations
built into it who here does work with
floating-point math in some part of
their system quick show of hands all
right well you might end up in a
scenario where you benchmark on one
machine where the dotnet sort of just
some time compilers able to go ahead and
take advantage brand
new x64 instructions for a new
generation chipset that will produce a
totally different result than what you
get in an older machine that doesn't
have those another fun one runtime and
dependencies so the garbage collector
the JIT or even as third-party modules
you can depend on can actually impact
your performance one of my personal
favorites was we had an issue where
someone noticed that when they updated
the version of think it was want to say
it was one of the serializers we support
I think it might have been JSON net
where they went ahead and upgraded that
version of JSON net which took into
account some basic neways or
representing objects and so forth and
our wire format actually got a lot
bigger as a result the bath so just
using sort of a new version of it we
ended up being able to sort it out and
change some settings to smooth that out
but that's an example of the type of
downstream change you may not expect
right that's another reason why you want
to measure those things keep an eye on
how your third-party components are
working to don't treat them like black
boxes next is this is actually kind of a
subtle problem how you gather the
metrics so these are platform dependent
quick show of hands who knows of a good
cross-platform way of gathering
performance metrics and net okay the
number of hands is zero that's the
correct number the answer is not even
the it's very difficult to do today if
you're working on Windows you have great
options like performance counters I've
been tracing for Windows Linux you kind
of have stuff like being able to go
ahead and collect the total amount of
available memory from the garbage
collector that's sort of about it
actually instrumenting your specs can be
challenging the last bit here is that
it's difficult to design isolated tests
so in particular one thing you might
have to do is be aware of when you need
to go ahead and force some garbage
collection to occur during different
stages of your test run and there's also
potential background noise that can show
up to such as things the garbage
collector is probably the worst source
of this but there might also be other
resources that are being released in the
background etc these are all things that
make
clean performance data tough and bench
is able to solve most of these problems
not all of them I can't stop you guys
from running your benchmarks on totally
different pieces of hardware but I can
try to make it easier to get clean data
what end been truly offers is an API
that feels a lot like riding a unit test
the a real idea behind em bench is the
concept of a performance specification
I'm going to go ahead and basically
dictate that this piece of code is going
to be able to execute n number of
operations per second and if that code
ever falls below the expectation we're
going to scream loudly and fail the bill
during a pull request that's exactly
what I want as an open-source maintainer
I want to go ahead and let my CI system
tell me when someone's done something
that adversely affects the performance
because you know what even with a team
of really good dotnet engineers trying
to review every pull request how many of
you think you can reliably spot a guy
change is going to have an impact on
performance just by eyeballing it I know
I can I see the guys in particular
shaking their heads you know it's it's
definitely a it's an issue where you
really want to let your automation do
that for you nice and the end bench does
is it basically creates a record of your
performance we write the output of your
performance specification to a file that
way you can go and review it later and
we also have some customizable outputs
to you can go ahead and basically write
your output to a sequel server database
if you wanted to next is the data itself
is collected reliably and it's
believable when you take a look at it so
we have instrumentation built into n
bench designed to go ahead and isolate
the N bench process some other processes
and windows we go ahead and adjust
things like the thread priority and the
actual process of the priority of the
process tube sort of ensure that the end
bench process gets scheduled ahead of
other things these are just sort of
common practices when you try to collect
performance data and mint is also fast
how you sore a benchmarking library be
if it was slower than the thing it was
benchmarking not very reliable right
that much is designed to be fast it's
meant to be configurable
it's also meant to be extensible and
benches systems can be extended to
support alternative ways of gathering
metrics you can go ahead and extend it
to base your rights output the different
targets we even had someone who just
came up with a way of having n bench get
hooked into the X unit test runner and
resharper people have come up with some
pretty clever stuff through with it and
it's also open source it's all licensed
under Apache to this is what n bench
spec looks like so this is one I think I
pulled this from yeah this is from Akkad
on net actually so I have this object up
here called a counter one thing that end
bench was designed to do so what some
folks might have asked me before about n
bench vs something like benchmark net
who's heard of benchmark done that quick
show of hands so they've been using that
on the.net core project to go ahead and
measure the speeds of kestrel and some
of the other tooling that's coming out
there I use benchmark net my day to day
work and it's a really effective tool
for micro benchmarking if you really
want to precisely measure how many
allocations or how much or how long it
takes to execute you know a million
iterations of a particular function
benchmark done that will give you really
precise results one of the things that n
bench does is it gives us the ability to
go ahead and perform that sort of
measurement for concurrent code because
it was designed for acha dotnet and it's
meant to be concurrent itself so these
counters are thread safe and then here I
go ahead and have a counter name and
then I specify how many what's my
boundary going to be and I want to test
in this case a million operations per
second I want to guarantee this parsing
function can perform that and I think
right now it goes much faster than that
but still that's like the minimum
threshold we agreed on then I have a
little setup method here but I basically
go and allocate the counter in advance
one thing you never want to include in
your benchmark is any of your set up
overhead for obvious reasons right the
amount of time it takes you to
instrument the test shouldn't impact the
test itself that's why we treat those
separately so we get our counter save
its value up here and then we have the
actual benchmark specification itself so
i have this / French mark attribute this
space it tells the M bench runner that
we have a performance specification here
and I'll go ahead and walk
some of these settings mean a little bit
more detail but here's the really
interesting part right here we have our
throughput assertion and our GC
measurement what a throughput assertion
is basically saying is that for this
counters value we expect at the end of
this benchmark that the actual number of
operations per second will be greater
than whatever discounted this value was
I think I specify that at a million so I
run this benchmark we'll go ahead and
see that the actual output of it will
will exceed this and so this assertion
will pass then i also have a GC
measurement here i'm just here to go
ahead and record how many garbage
collection events there are well the
benchmark runs and i'm also specify and
this benchmark will run 13 times we'll
go ahead and allow it to run for about a
second and we're doing what's called a
throughput benchmark this means that
what what n bench will do is it will
estimate how long how many iterations of
this function it will take to last one
second and it'll use that as its base
line for this spec and then the last bit
here this test mode equals measurement
this means that we're actually not
triggering the assertions right now even
though I have this counter throughput
assertion attribute here and that I
should get to run the assertion so
there's going to be no pass/fail logic
here sort of a soft switch for turning
that off if I set this test mode back to
assertion it would run this assertion
again that's what's going on there so n
bench is really about testing code
against performance specifications I
want to go ahead and set a target for
what I want to hit and I want to get a
pass/fail desertion back depending on
whether we achieve that or not now i'm
going to go ahead and do here is I'm
going to pull up a piece of code go
ahead and switch here alright so I'm
going to go ahead and open up my purpose
alts folder let's see don't think i have
a result yet okay that's all right i'm
going to go ahead and actually run this
acha dotnet benchmark here let me go
ahead and copy this real quick
there we go let me copy that go to here
so we're going to run the benchmark that
was showing on screen all right I'm
going to go ahead and let that run for a
moment so I basically specified with the
command line arguments I wanted to skip
a whole bunch of benchmarks that weren't
really relevant here so here where you
can see on the screen is each iteration
of the benchmark is going in counting
you know basically counting how long it
took to run how many different
iterations of the counter were there and
how many completed in under a second all
right so this should finish in a couple
of seconds here all right cool and let's
say it has another speck that I caught
in my reg ex that's okay well that's
running I'm going to go ahead and pull
up my little NDC folder here all right
which will shift be all right okay so we
can go ahead and see the output the end
bench produced right here so if i scroll
through it you can see the fully
qualified name of the performance method
you can see the human readable
description i applied here as well as
some system information so what sort of
what's our processor count what version
of windows are using version and bench
how many a threads other sort of thing
let me get down to the data so we can
see here that based on our totals we
basically estimated that we could run
about 500 97,000 iterations of this code
in a second that's using the very first
warm-up that we generated so one of the
things that occurs during the warm-up is
remember if I mentioned earlier that we
want to try to smooth out as much of the
noise of the system as possible what are
a couple of things that can occur the
very first time you run a benchmark that
might throw off its numbers any guesses
how about the JIT compiler you have to
go and get the code the very
first time you run it right so that's
one piece of overhead that we try to
smooth out by running a warm-up before
we start doing the totals what are a
couple of other factors that might
affect the benchmark disc it's actually
going and running the loading the
assembly running of the first time yeah
that's one well good cash warming up
this is our the cash pinning inside the
CPU we want to pin all the instructions
for this function in there too so a
benchmark done let's see what the N
bench will do is it will go ahead and
actually run a number of warm up sets so
try to basically ensure the cash is
pinned and to go ahead and basically
start getting a cleaner signal for a
read here so you see up top that we
picked 597 thousand operations during
sort of the first part of our warm-up
but at the very bottom you can see that
my sort of per second totals down here
well actually higher than that that
we're doing closer to 700,000 operations
per second reason why that is is because
we actually did get the instructions
cashed the jitter didn't need to do any
additional work and at this rate the
only thing really affecting the quality
of our benchmark was the actual overhead
of the function and anything else the
operating system might have been doing
in the background now if I scroll down
here and take a look at the raw
collections you can see here the total
number of garbage collection attempts
for generation 0 so we're averaging
looks like 332 collections / run the
fact that number is pretty consistent
across each of these runs probably means
that it's we sort of collected the right
number this is a pretty precise figure
but looks of it for gen 1 we could see
we're not allocating any objects neither
are we for gen 2 so this is good means
this functions not allocating long-lived
objects that's what we want then down
here we can see the post results so I
can go ahead and see that we run the
same number of times but take a look at
these values differing a little bit you
can see there's little deviations here
this is as a result of the pre-emptive
operating system at work where there
might be other things that are getting
scheduled on are the same cores in the
background you know I
running power points and a copy of
chrome and other stuff in the background
so there's lots of other processes that
might be competing for some of those
cycles and where you can really see that
show up a little bit is this value on
the right number of nano seconds per
operation you can sort of see how that
varies a little bit we're all sort of in
this range of about fourteen hundred and
fifty nanos per operation but there is
some variation that occurs there so what
you want to do with any benchmark is
don't worry about a single data point
you want to plot a line instead so you
want to care about the line on the graph
not an individual data point and so
we're not running any assertions here so
this is there's no sort of real pass or
fail going on now one thing I want to
show a sort of an example of things that
can affect the benchmark pull that back
up I'm going to go ahead and unplug my
laptop so it's a low-power mode now I'm
going to run that benchmark again and
we'll see if the results are different
this time now that my laptop is running
on battery power okay operations per
second still looks pretty good we'll go
ahead and have to see what the final
results look like looks to me like it's
running a little bit more slowly but not
by much because this isn't really a
super io intensive piece of code that's
using a lot of different devices of
peripherals in the computer it's a
fairly simple string parsing function
basically the code that we're testing
here is what we use to parse an academic
actor address from a string that's all
that it's really doing okay looks like
the operation sort of finished let me go
ahead and pull up my code here again I
can see that these files have different
time stamps on it so I think this is the
1 534 yeah this is it alright cool so
I'll go ahead and do ctrl shift begin
let me pull this up so this is the
second benchmark that we ran now we take
a look at that the average value here is
six hundred and sixty-two thousand
operations per second whereas here was
six hundred eighty-four thousand
operations per second so we're seeing a
deviation
about 20,000 operations per second again
not very much but what you'd expect
given that the laptop is going to be
running on the lower power mode when
it's disconnected right that's an
example of being able to produce a sort
of reliable result under the
circumstances there so this is what n
benches instrumentation is designed to
help you measure reliably is the ability
to go ahead and determine when your
environment changes when your
dependencies change when your code
changes what was the impact on the
performance of this unit of code so go
back to my powerpoint here all right
sorry about this ok so in bench is all
about testing code against this
performance specifications so some of
the different types of benchmarks we
offer from a test mode point of view we
have measurement versus assertion the
difference here is whether you get a
pass fail at the end when you go ahead
and use some of those assertion
attributes may also have the different
run modes there's the concept of an
iteration mode which I'm not using the
idea behind iteration benchmark is let's
say you want to test a fairly large
block of code like an entire feature you
want to go ahead and benchmark how an
entire feature works end-to-end you dip
ibly set that up as an iteration
benchmark what you want to say ok I want
to go ahead and run this benchmark that
uses a fairly large piece of code a
hundred times and see what the final
output is no pulp an example one of
those in a moment so they have one end
times the other idea is this throughput
benchmark which is what we just did with
that parsing function these are designed
for basically run a piece of code
continuously for a fixed walk of time
and what you want to do there is
basically generate some load and
measured by how well a very small unit
of code worked so this throughput
benchmark is really for these like micro
benchmark components iteration is really
for these more macro larger components
now in terms of the things that we
measure throughput is one that you saw
us use how many things happened in a
given second memory consumption is
another thing that we can test we use a
fair
coarse grain way of measuring that we
try to actually design and manage to be
somewhat cross-platform so it'll be
easier to port to net core we use the
garbage collector for getting that
number which isn't as precise is what
you can get using like event racing for
windows or dot memory but you can
actually plug in different sort of
memory consumption measures if you want
so to remember bytes allocated a sort of
what we measure today garbage collection
you saw us going and measuring
collections / generation so you can go
ahead and basically write a benchmark
where you can assert that gen 2 is
always equal to 0 if you wanted to
that's the type of assertion we support
elapsed time this is end-to-end how long
did it take to do something we have a
benchmark in alkanet for testing against
that long shutdown situation where we go
ahead and say the elapsed time for a
actor system of this size the shutdown
must be under this number if we have
that setup then we also have a support
for some custom measures so we have a
performance counter plug in for n bench
that allow you to go ahead and
instrument any arbitrary performance
counter it's an end bench spec so we've
seen people use that for measuring
things like total number of bytes
transferred to sequel server before
that's one example it's already gave you
a little demo now the next so a big set
of features that we're going to try to
add to end bench down the road is the
notion of an automated performance
history where n bench will output its
history to some service some web service
somewhere that will be able to go ahead
and ingest that data and give you a nice
graph but not whether than the one I had
to construct from Excel before this will
go ahead and do that for you
automatically I know by the web your
wife last thing i want to show real
briefly is an iteration benchmark so
this is one of my favorites i don't
think i'm going to run it because it
will take like 10 minutes that's not
what i want to do all right this is what
we use and we have a version of this and
end bench in another version in.net e
which are both sort of socket libraries
and net we call this a horizontal
scaling specification well we want to
try to determine here and actually let
me scroll back up but we won't try to
determine with this benchmark is on a
single machine how many connections can
we open before the throughput
on a single server before the throughput
begins to drop this benchmark isn't
exactly fair because the clients
workload and the server are both
happening on the same machine but it's
to help give us an approximation for
what the sort of resource overhead looks
like so we have sort of a client in a
server bootstrap we have a set of event
loop groups for the client and the
server these are the different thread
pools that are going to be running those
workloads and then we set up a few
counters we have a counter for the
number of clients that be able to
connect the inbound throughput and the
outbound throughput and what we do here
we have a couple of setup functions set
up our server and our client yep yep yep
let me scroll down here we go well this
benchmark basically does is we go ahead
and pick a random block of time they
want to choose like three or four
minutes I think in the last version of
this i did for dotnet II or channeling
has been merged in yeah I think I picked
a static amount of time for that and
what we do you just go ahead and iterate
over this loop and they continue to
basically just keep spawning clients
that go and generate a continuous load
they'll go ahead and generate I think 10
messages per second each in the
background and this test on this little
surface pro 4 here can generate about
850 open connections before we basically
hit our maximum sort of time out which
is that it took longer than 10 seconds
to open 10 connections we go ahead and
sort of end the stress spec then this is
an example of something that's a little
bit more like macro scale you can do it
end bench 2 and the the merits of this
are really there to determine sort of
how much overhead goes into a client
server connection and add so that's the
idea behind n bench and what our goal
with this project really is is to allow
you to go and plug in performance
testing into your continuous integration
system and have it as something your
team continuously monitors and each pull
request going forward so that's it for
sort of my prepared content and I'll be
happy to take some questions if you have
any now thank you very much
okay do we have a microphone I can pass
around either getting it okay yeah back
what microphones coming there we go so
you mentioning before bad always
variability would that be mitigated
somewhat if you were to run your
application in the container if you're
going to run your application in
container those cores are still being
watching the movie phrase as long as
you're able to pin one quarter one
container meaning that nothing else
could really schedule on to that then
that would mitigate that to some extent
however if that cooler so live it like
logically belong as the container but
it's still really time shared across
them then it wouldn't make a difference
at all so that's also kind of a problem
one thing we do with the polka dots
build farm right now which is a bit of a
problem is we use auto scaling the team
city to go and dynamically allocate our
build agents well we've noticed that
even though we're all using the same
generation of azure VMs there's still
significant variations and the values
that we record in our benchmark there's
two reasons for that even though they're
all in the same generation they can
still have different hardware but
there's also the fact that we're running
on a virtual machine who is to say we
don't have a noisy neighbor who's
sharing the same physical hardware that
VM is on any given time my one of my
board members of my last company was a
major AWS customer and noticed that they
were having these horrible CPU
starvation problems on their own system
even though their application was under
a huge amount of load turns out they
were sharing the same boxes as
Foursquare was that's why they're having
problems there is time was being stolen
by their neighbor basically so that's
one example of where hypervisors and
containers still don't necessarily give
you the resource isolation they promised
sometimes yeah good question you have
one
so on at what stage in your alm cycle do
normally before these tests what stage
in your alm cycle do you want to start
introducing performance tests that's a
good question you know I would say look
the quote that people throw out there
about premature optimization is the root
of all evil there's actually like a
second part of that I don't fully
remember but it's basically like you
should still measure performance of your
code anyway and try to improve it where
you can just that don't over optimize
things right away I think you should
introduce performance tests around the
same time you start finishing features
when you have something that can be
performance tested you should start
doing that that way the very least let's
say the performance sucks for your very
first generation that's okay no one's
using it yet right well when you start
setting that baseline you can see
improvement over time what it gives you
is that sort of defensive programming
means of never backsliding into bad
performance again so when you're at a
stage where you're able to measure the
performance is something that's when i'd
start trying to do that as long as it's
reasonably easy to do it okay cuz 11 kv
to make perhaps IAM you know setting a
schedule time on these for incest is
that i can see really quickly that you
know you might have a branch and after
unit tests you might run a performance
tests but if you said a bunch of these
to run say for a minute or two minutes
these aggregator do you d please may I
kroger to you know we're just running
five minutes until it actually um we're
just back into master what develop
whatever you're talking about the length
of the build process yeah so here's
here's the scary fact algodon Nets end
and time to basically run a full bill to
the systems we have 80 projects in our
visual studio solution and about a third
of those or tests of some sort so we
have I think three to four thousand unit
tests about 200 performance tests and
about 70 what are called distributed
unit tests we're actually going to
simulate a network if I ran the end
and build chain on my laptop here it
would take 90 minutes to execute what we
do on the build server is we actually go
this is a little bit more about CI than
n bench we actually go ahead and have
different machines run each of those
steps in parallel is what we do so that
way I'm running the same 30 minute
execution block in three places at once
right so that's one way you're doing it
yeah one thing you absolutely cannot do
the performance test is parallel why's
it that will that's for obvious reasons
that won't work so you do end up having
some extra time added your build process
for that so what I would encourage your
developers to do and that environment is
maybe don't run all the performers test
locally for each step they do have a
build server process that's designed to
paralyze that work and just farm it out
to that that's what we do in the on the
dot Nettie and akka dot net projects to
as we let the build server do a lot of
that work because it's just not
productive for the developers to run it
all the time locally anyway so yeah
hi there hey doing hello good how you
really yeah okay I've got two questions
first one is around ad answering
dependencies who talked to me at this
though the worst avoid them yeah I get
that I'm just kidding um but they don't
stream the pattern see real world in the
real world now and dependencies here's
whatever they mean particularly with
major versions of downstream
dependencies you got to go ahead and
maybe say okay is this down share
dependency and a performance critical
path great example of that naka dinette
is our serializers that we depend on
with the remoting pipeline also the
persistent system that we use for saving
events the serialization system sits in
the middle of that so the impacts our
cereal of the speed of our serializer
will affect the speed of our remoting
system so we're in the process of
switching from json net to wire right
now I knock it on net why is about 25
times faster than JSON net and that
shows up in the benchmark big time so
what you want to do is just have a
measurement setup so you can witness the
impact of that it just gives you
visibility into it as a team so that's
that's the sort of high-level part of
that that's how to answer your question
yes that's one pop okay of my first
question what about anything has it
across the network do you ever use this
to hit network like external systems and
kind of I test them yes I've used this a
hit likes equalizer databases before yep
so I had some performance test a
entity-framework application at one
point and one of the things I was paying
attention to was my lapse time and and
how long did it take for me to complete
a full cycle of one request and so I use
n bench there to go ahead and basically
just get a measurement of that the other
thing I measured was I used a
performance counter to see how much
traffic was going on to the network
during this part two so I wanted to get
a sense for okay if I have a pipe
between here and signal added I can
handle n number of bytes and window in
Windows Azure is well designed it can
handle a lot or so I wanted to know
what's the upper limit of this system I
want to go and see okay how much is when
we go ahead and
this one webmethod of this Web API that
was developed what's the total amount of
bandwidth it uses so I'd go ahead and
measure that those benchmarks will take
a while because they depend on external
network calls but they're still worth
doing so yeah not all benchmarks need to
be running a function hundreds of
thousands of times to see how quickly it
turns right that's the idea of those
iteration benchmarks that basically are
bigger blocks of code to take longer to
run okay yep second question is can you
catch can I catch ya not very well yes
well enough commonwealth bank all right
thank you next question Sergei first of
all great work and I forgot who said
that if you don't measure what you do
you're not an engineer yak you fooled
that I'm curious if you ever measure
impact of hyper threading measure the
impact of hyper threading now that's an
interesting one we when we actually go
ahead and take a look at that core count
in this sort of report up there it's
number of logical CPUs right so for
every sort of intel based processor
that's going to show what whether it has
hyperthreading in it let's go show up as
to we don't have a I don't have a
measurement right now for breaking that
down where it's like what's the what's
the impact of having things scheduled
under the same like same hyper threaded
cpu right so no I haven't done that
that'd be interesting though actually
don't know it a while ago and it was
very negative so we turning it off and
never measured up you turned
hyper-threading off yes oh that's good
to know i should i should take a look at
that one thing will be I have a pull
request that's open for this on end
bench right now but we have a young well
these were to make more extensible you
saw that list of information about the
system is pretty basic right now what
we're going to do is we have a system
that queries wmi and pulls a much more
detailed fingerprint of your system
being able to see what generation of
processor using is hyper threading
turned on and maybe some other important
settings to like what's the size of the
virtual page file just throwing that out
there as an example these are the
interesting things to know potentially
in sort of getting a more complete
benchmark another thing I've seen
particularly the benchmark dinette
project do is experiment with what's the
performance of your application with
different jit configurations comparing
classic vs ryu jet for instance it'll
also go ahead and test thing is like
x86x64 now if you've got a piece of code
that doesn't align to a 32-bit cash
you're going to see a noticeable
performance difference there so those
are all things that I think you know in
terms of what end bench scratches the
surface of today there's a lot more work
to be done there in terms of getting
that much more detailed metrics about
different system configurations in the
impact they can have cool yes so I was
wondering if you thought this might be
good contacts friend bench that my
company Global come they've got a lot of
jobs that uses run and the jobs can take
anywhere from three seconds to ten
minutes and some of them are very time
sensitive obviously I'm talking outside
the build pipeline but you think and
bench would be good perhaps nightly or
weekly kind of running through hours and
hours of these like running each job
once just to kind of get that historical
graph of time because that seems like
it'd be really useful yeah absolutely
being able to use so basically heavenly
a cron job that runs n bench and like
using the elapsed time measurement might
be a good one so when we run this job
what was the end-to-end processing time
for that being able to go ahead and see
a trend line where let's say you're able
to produce consistent results over time
for the same job that's good but what
happens if you see a big a big dip in
processing time will that be something
though to indicate that maybe there's
something up with that customers account
if it's a data-driven thing or maybe the
piece of code that that job is running
has changed in some way so yeah that'd
be useful thing to do absolutely any
other questions wire was the name of it
so it's if you go to Akkad on net / on
github / wire you can see that they're
they're currently working on dotnet core
support for it yep WI re yep we like
picking names for open source projects
that aren't easily googleable that's how
you know
all right any other questions right
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>