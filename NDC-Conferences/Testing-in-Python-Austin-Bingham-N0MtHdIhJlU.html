<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Testing in Python - Austin Bingham | Coder Coacher - Coaching Coders</title><meta content="Testing in Python - Austin Bingham - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Testing in Python - Austin Bingham</b></h2><h5 class="post__date">2017-03-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N0MtHdIhJlU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right on a huge crowd but not
the flavor of this conference so
hopefully we'll grow the the pint and
presence at NDC over the years so this
is a tip of the spear I guess so today
I'm Austin Bingham I work for end and
half own with this fellow here 60 North
a small software development and
consulting and training company based
out of Norway and we do a lot of work
with Python and has to have done so
historically not just with 60 North but
with other companies we've worked for
and today I wanted to get kind of a an
introduction to what I call the
landscape of testing tools in Python you
may have seen some of these if you any
work with clients on already and
hopefully you haven't seen them all or
at least teach you some new tricks will
cover everything from well let's look at
the agenda I have modules for unit test
which is built into the standard library
I have a module unlock on PI test which
is a in some sense a replacement for
unit tests and more exotically a tool
called hypothesis which does property
based testing and a tool called cosmic
ray which does mutation testing for
Python I've on the main author on cosmic
ray so you know hard questions about
that I can help answer those in the 60
minute format we have here say we can't
really cover all of this meaningfully so
I think we'll go with unit test
hypothesis at cosmic ray in that order
and if we have time left I'll do a quick
intro to PI test but given that we've
really only got four people is there a
preference is anybody really want to
hear about anything else in there okay
then we'll go with that order and just
real quick are you using Python now okay
so it's not like not not paying the
bills and how about how about you you
started pricing last week your company
did okay well that's good then I think
you really will get quite a bit out of
this will give you some good ideas about
how you can effectively test you're
practically really start to do so right
so let's let's dig right in this the
slide deck by the way is available on
github i'll have a link later on it's a
nice and note
if you've never heard of that it's
essentially a way to mix text notes and
actual code and I'm hoping it's an
effective way to present this material
so unit test unit test is testing with
the standard library to built into the
standard library so that means that any
Python installation that hasn't been
mangled will have this and that's
perhaps its greatest strength the fact
that it's always there the reason I
think it's important to learn unit tests
because you will see it if you start
using any third-party libraries and any
code somebody else has written you're
very likely thrown into it because it is
sort of pervasive and it's a powerful
tool to know and it's good to have you
can write to you know spot tests very
quickly with it and it's um it's a
little bit weird it doesn't follow Pepe
entirely different strange things but by
and large it's a very effective tool the
basic features that unit tests and n PI
tests and other tools in that area
provide you our test automation
obviously that's it's a raison d'etre
that's why it exists to help you
automate and have repeatable tests so
that when you make a change you can run
your tests and go get some coffee and
come back and see the results it has
facilities for doing set up and shut
down and tear down around your your gear
code around your tests what they call
fixtures typically so if you need to
create temporary files or clear a
database or something it has tools for
doing that it allows you to aggregate
and sort of organize your tests so if
you just want to run a subset of your
tests say the database stuff or the you
know the communication stuff or
something like that you can organize
your tests in such a way that it's easy
to select which once you run and it has
a nice independence between the test
execution engine and detest reporting
engine so if you want to make fancy
reports of your own and inject them into
some report that you presented somebody
or put it up on the screen so everybody
can see it makes it pretty
straightforward to do that the basic way
you organize tests and unit tests is
with something called test case and this
is a class and use it as a base class
you inherit from it and then you can use
some of its methods and you create your
own methods that actually run the test
or implement the tests so it provides
basic assertion methods by which we'll
look at these in detail the assertion
methods or how you express the truth you
expect to
to hold at some point in time we'll talk
about fixtures which are the way that
you do setup and teardown for your tests
and some related test methods will kind
of see various miscellaneous utility
functions the basic approach when you
write tests and unit test is you create
a subclass of test case and then you
write some methods that start with the
word test and these methods get
discovered by the unit test framework
and then executed when you run things so
what is the simplest possible test case
look like it looks like this and I
should just point out this % % unit test
run you'll see these things those are
magic code words to the ipython engine
to help to execute this you can ignore
those that's not actually Python code
I'm looking for a way to make those
disappear in the slides but it can't be
done right now but the basic structure
is what you see here that this is a test
case that does nothing and inherit
through test case and he just says pass
so it's doing absolutely nothing but I
can I can run this so let's do that
right so what you expected you know zero
tests run zero failures your errors so
in some sense that's really good in
another sense that's just not terribly
exciting so but that that is the that's
the fundamental that's the foundation of
how you do things in unit tests the way
you write actual tests tests it start to
do assertions and make claims about how
the universe ought to look is that you
create methods on your your sub class
let's start with the word test you can
you can actually control a that's again
you can configure unit tests to look for
different patterns with my default it
looks for methods that start with test
so in this case let's try the laser here
we have our test case subclass and we
have a method here that's empty I mean
this will be detected because it starts
with test this also starts with test
although not test underscores it's
irrelevant test your test underscore
this also gets detected as a test but
this does not by default get detected as
a test so this is invisible in some
sense to unit test so I can run this as
well and what we should expect to see is
that two tests ran no failures no errors
great so still not you know the most
exciting thing in the world but you can
kind of see now what unit test is
detecting when it scans for tests
so now we want to look at assertions we
really want to start making claims about
you know what we expect to be true when
we reach a certain point in execution we
expect to be not true or how we expect
data structures to look and things like
that so the whole bunch of methods on
the test case subclass called a search
something assert that this is true or
asserted that this is equal to this and
that's how you tell unit tests to run
tests a failure of an assertion is a
failure of a test so how does that look
so finally we have tests to do something
this test always passes because it
trivial e asserts true that's true okay
so this is saying that whatever
expression is passed in in this case
true we expect it to be true if it's not
then if it's false or false e and Python
terms then this will be a failed test
and this will always fail this we're
asserting false we expected this
expression to be false and it's actually
true so of course what we'll see is that
we get to test run one failed and test
always fails it's the one has failed so
this is my secret code f for fail this
by the way this what you're seeing
printed out here I can do that because
of the separation of the reporting
engine from the execution engine I was
able to ask in a test to run my tests
grab the results object and then inspect
it to figure out what to print down here
so this is an example of the separation
of execution and reporting which is not
so easy to do in for example pie tests
which is kind of why I parked on it a
little bit but this this is ultimately
this is this is finally an actual
functioning test this is the flavor of
what a lot of your tests are going to
look like a bunch of things that say
test something run some code check some
assertions and that's it and they should
typically be quite small if they get to
be big you want to start looking at
using six years which will examine in a
bit there are a lot of assertions built
in to the test case class I won't go
over them all in detail you can get
these slides and see but you can check
the two things are equal or not equal or
true or false or is or is not for
reference reference checking is not
instance if you need to check us some
things of a certain type and you know in
not and there's a whole bunch of these
and you can see all of them in the test
case documentation if you want to but
remember these are all methods on the
test case
class so if you look back here wait
sorry not there that's why we say self a
search false this self of course is this
class instance of this class and a
search false is defined on test case
that's the basic mechanism of how these
assertions are made available to another
kind of thing also want to check for
especially in Python which is a very
sort of exception centric language uses
exceptions for all sorts of stuff and
you're encouraged in your own code in
your own api's to use exceptions you
want to check that a certain kind of
exception is thrown in a certain context
and so there's assert raises also a
method on the on the test case class
which you use as a context manager so in
a in a wisp lock if that's if that's an
exotic term for you and let's see how
that looks so here we want to test that
an index error is thrown in a certain
situation and so it's very synthetic of
course we say with self assert raises
indexer so we're saying in the context
of this block down here we expect an
index error to be thrown and of course
we just raise an index error just to
simulate it happening which you could
imagine more complex code that did
something that should raise an index
error similarly here we're gonna have a
failing test because we assert raises
that a value error will be thrown
somewhere in here and of course pass
does not throw a value error so we can
see that in action great two tests were
run and test value error failed so this
this is the canonical way and the
correct way I'm going to say to test for
exceptions you could maybe in some
really weird situations need to use try
catch and then assert inside the catch
block or something like that to check
for some bizarre situation but by and
large testing for exceptions should be
done using this approach and this is
identical to the approach used in PI
tests for example so it's an accepted
idiomatic way to do this kind of work
and it's kind of elegant I like this
this use of context managers here feels
right so six years we've mentioned these
a few times fixtures are essentially the
way that you do test setup and cleanup
so before test is run I need to do some
work to set the you know to kind of
clear the floor baby or create a temp
file or something along those lines and
then after the test execute I may need
to clean up remove a database table or
delete the file that I created or any
number of things so there are
essentially six hooks that you can use
there's test case setup so this is a
method it's an instance method on test
case and setup if you define it well you
it's always define means you create your
own your code will be executed before
every single test and then tear down
similarly is executed after every single
test you run in that in that class setup
class is a bit different setup class is
executed once before any tests in a
classes run and then tear down class is
executed after all of the tests in the
class events only once it's like a
bigger sandwich than the setup and
teardown and then finally setup module
and teardown module work like this but
at the module scope so setup module is
executed once at before any test in a
modular executed and then tear down
module analogously at the end and we'll
see i'll try to demonstrate that here by
showing you how well you'll see in a
second but so what are they used for you
could probably imagine a few things but
very common in my experience or things
like you know creating and clearing
database tables you're putting a
database into a known state so that your
tests know what they're getting into
when they start to execute you might
call hygienic testing configuring
entities numbers so you can use setup
and teardown to set values on the test
case class itself and then use those
values in the test or maybe create
temporary files this is very very common
in a lot of testing environments but
basically think of it as you know setup
and cleanup that's that's what fixtures
reform use me so how do they look in
practice so here we've created you know
yet another test case sub class setup
class and teardown class turbos class
both class methods if you're coming from
C++ you would think of the master static
methods or something like that and these
are the ones that are executed before
any function in a for any test in a
glass and this is executed after all the
tests in class and all we're going to do
is print
some some string so that we can kind of
see the flow of the logic I'm not going
to actually try to give any real work in
these similarly setup and teardown
executed before every test and after
every test and they're going to print
something out as well we can identify
the order of execution I need to execute
this cell real quick all right this next
slide is a continuation of that class so
I now I've added to test methods that
don't do anything except themselves
prints so what we're going to see when I
execute this is the ambient relating the
order of all these things that were
printing out and hopefully that will
make it very clear what's happening so I
run the test we get to total test run
these guys here the very first thing
that happens of course is set up class
execute and then set up rim which is our
our function level fixture execute we
run the first test and then tear down
execute set up runs again test number to
execute and teardown happens and finally
we do tear down class so that's in a
nutshell that's how these things
interact just various levels of scoping
effectively for your fixtures so you can
you know imagine how this might apply to
the test code or bbcode you're trying to
test there any questions about that so
I'm entirely clear that I feel like
maybe that's a bit of a rough section so
as long as it's clear one thing you can
do and setup and very often need to do
is configure member attributes and this
is a bit tricky for some people to
initially grasp maybe they're there when
they're there conceptualization of the
kind of a context of test execution is
not clear yet so I want to kind of drive
this point home you can use it to assign
attributes on the class itself and it
just makes perfect sense this isn't
rocket science but it gets a bit of a
confusing concept sometimes because so
in this setup here we set self data so
we're setting some data member on the
class that it's going to be self in all
of these tests we put it to some value
and both of these both of these tests we
assert that self data contains the value
that's set in the set up and then we
change it in both cases we change it to
something else these don't step on each
other because of course setup is
executed before every test so this gets
reset essentially before every single
test and like I said this isn't this
isn't some mind-blowing news that's
going
you know make you guys better human
beings or anything but it's good to
remember that I think if you can if you
get that and you understand the basic
flow of fixtures and how they interact
with tests so this is executed literally
before every single test is run unit
tests main this is um in some sense it's
a convenience function what it does is
scan the module in which it's executed
for tests to run so it looks for test
cases and it looks for test methods and
then it executes them in prints of
liberal reports it's a really convenient
little function and what it's very often
used for is to create little executable
test modules so if I've got mine you
know unit tests may not py here and it's
created a test case that does some
questionable tests I can then go down
here and okay I apologize there's some
kerning problems here so I'm going to
just go out of that mode if you've never
seen this before dunder name equals
dunder main this is the idiomatic main
function for a python module it's saying
if this value which is set by the pipes
of runtime is equal to main that means
that this model is being executed as the
top level module and if so do something
special all right so be it this is kind
of the main function so all we're saying
and in a situation like this is if I'm
being executed at the top level module
run unit test amane which means go find
these tests and execute them so it's
really convenient to take your test
classes and bundle them up into modules
and have them executable in a standalone
way you'll see this pattern repeated
over and over in a lot of unit tests
based testing code I apologize again
about the the layout issue we had there
so if I then execute this we'll see
actually I can make that look a bit
better so if I have unit tests 9py that
we looked at just a second ago and I
execute it on the command line that's
what this magic here is saying it prints
this report that the assertion failed
because one does not equal to accept in
bizarro world and we get some stuff
there and that's all because we use that
idiomatic dunder main equals Thunder
name block so
if you have the niceties that I'm going
to kind of go through quickly if you
need to skip test you can do that using
certain decorators and it's just an
exception you can use as well and it
looks like this if I need to skip test
skipper tests for some reason I can say
you know I can decorate it with skip and
give a reason and then you can execute
the execute your tests and it will print
out the reason things are skipped and
that can be really useful in cases where
you have a test that you know doesn't
work right now because something is
broken that you intend to fix it so you
just want to kind of market as you know
not working you can skip conditionally
maybe a test only works on you know AIX
or something like that and so you only
run to run it on AIX you can check your
platforms if AIX is a strange term for
you if you're lucky and you can skip
unless as well and they just in verses
of the same logic so if we execute this
we should see right three tests run to
skipped and some skips here so you can
analyze the test report to determine
which things were skipped which is
really handy there's also the skip test
exception which you can throw from set
up or from your test functions and and
what what this will do is if if this is
thrown and bubbles out of your of your
tests or your setup unit test code will
see that recognize and say okay I'm
going to count that as skipped so you
know if you try to make the uplink to a
satellite and it's not available at that
time you should just skip the test and
the testing framework will say it's not
really a failure but I want you to know
that we skipped it and why we skipped it
you could also throw this from inside
your tests like I said and get the same
effect but these have the exact
ultimately exact same effect as using
the decorators that we looked at just a
second ago you can also in this very
similarly you can say I expect a certain
test to fail this is put a question out
on stack overflow or reddit and ask
books like what situations have you
actually used this in and this is
another place where people have said
well I've got a test that is broken I
know it's broken and I know why it's
broken and I expect to fix it so I can
mark it as expected to fail and the
reason this is this is useful is because
the test runner will report to you that
the test failed and I expected it to
fail there will also tell you I expected
it to fail in and succeeded meaning you
somehow fix the test that's a really
useful bit of information to get back so
if you're in some heavy development
phase and things are in flux in your
looking things was broken and fix this
stuff you get a nice report back unit
test telling you what you fixed maybe
accidentally you might scratch your head
and go did I fix that smarter than I
thought and it looks looks like this we
expect this test to fail and if I run
this I can see ya and if I somehow you
know fixed my logic and executed again
it's going to go uh-oh I got an
unexpected success and that's really
useful information somehow I've changed
something that sticks the universe let's
put that back alright that's cool a
raincoat and slides it feels feels neat
so now it's become an expected failure
again oak because I misspelled false for
one thing there we go this is relatively
new parameterizing your test this showed
up in 34 and what the discussions about
this into going on for years on the
Python dead list the basic idea is that
you have a test that you want to
logically you want to run this test say
three or four or five times with
different sort of input parameters the
normal way to do that initiative you
know a long time ago would you have a
list of these parameters you loop over
them and if any one of them fails or
cause the desertions fill the whole test
would fail and this was not great
because image you didn't actually run
all the tests you expected to run and
the reporting was in this fine grained
as you would like so this notion of
subtests allow you to mark a portion of
a test using a with with statement to
say this is a sub test its parametrized
on a couple of different inputs run each
of these independently treat them as
separate tests the failure one will not
mean that everything halt and the others
fail as well so as with all these
examples it's a bit contrived but okay
we got a normal test test case and we're
going to test fit all the words and some
lists are a ski and obviously they're
not all a ski this is Norwegian for
thread and the region for islands so to
da da da so ultimately but clearly not
asking right so here we say with subtest
binding words to this word here it's a
bit it's a bit weird but if we're saying
that in this context we r binding the
name words to whatever word is here and
in which looping over this here we run a
test fundamentally what's happening is
that this block here this assertion is
going to happen
four of these every time and the failure
of one assertion will not cause the
others to not run and won't result in
failure of all the other ones so the the
output is a little bit what you might
not expect we it says we ran one total
test with two failures and the Savior's
were when the word was thrilled when
wordlessly so the other ones passed just
fine so if you find yourself you know
writing five or six tests that are
basically the same test just have some
different parameter think about using
subtests that it'll clean things up and
it's much more expressive and get your
point across much better I want to kind
of close this section up with some
advice like that's not how to structure
things i've i've personally gone through
you know iterations of how I think
projects ought to be laid out and and
I've had various reasons for believing
that but I settled I think Rob is kind
of on the same page with me that this is
a pretty good structure here at the top
level you've got your project name so
boom underneath that you would have
another directory called boom your
project name that contains your done
during it this is actually your package
in pipe in terminology this is your code
here you'd have a setup dy which I
highly recommend it's not required but
very very useful critically for the
purpose of purposes of this discussion
you would have a separate test directory
that is you would not have your tests
inside your package the main motivating
reason for that for me most of the time
is that I do a lot of mutation testing
and mutation testing work and I don't
want these getting mutated while this
code is getting you ticket that would
throw things off in a really big way
maybe more fundamentally though these
aren't part of the package they don't
need to be part of the package so
putting that into package is just
confusing it just it just conflates
things that don't need to be conflated
so keeping your tests out here makes
them a bit more independent from your
package it makes your package
installable without the tests for
example there's a number of reasons for
doing this this isn't required of course
you can use whatever setup you like that
you'll see this in the wild quite a bit
and it's a very effective pragmatic way
to structure your projects and okay so
this is an example of a sort of a real
test just to kind of bring everything
together and I again I apologize because
the underscores
are invisible but this is what a full
test might look like and this is our
test sorting that we looked at a second
ago just to kind of bring it full circle
with your import unit tests create the
test case have a test do an assertion
and then have your idiomatic main down
here to run unit tests made this is a
complete thing you could ship this and
you know everybody'd be happy the only
that brings the unit test discussion
sort of full circle for you that's no
that's the last line so it has to bring
it full circle for you are there any
questions about unit tests or anything
that I've covered yes Oh unit test goes
back to probably pipe in one I don't
know how old it is but it's been there
forever do you know Rob it's ancient
yeah it it three days if you if you if
you remember we looked at the that the
fixture function is called set up right
and it does everybody know what eight is
Bev eight for those who don't know is is
well a pet is a pice and enhancement
proposal and it's how Python makes
changes is how we discuss changes to the
language pet eight is a description of
how code should be laid out naming
conventions and things like that this is
very not pet eight this precedes pet
baits we mean it's really really old
they this naming convention using camel
case rather than snake case would not be
allowed more or less today so it's quite
well you can use it in Python to code
very easily not all the features I
talked about or enticing to of subtests
for example or fairly new very new most
everything else i think is in there and
a lot of things has probably been
backported if you need it may be that
subtest septums somehow magically
backported to I'm not sure okay mark I
said because because of the time
constraints here I'm going to skip unit
test mock and pie test I just really
quickly say unit s mock is a way for you
to test how an object is used you
essentially create these mock objects
and then call functions on them
pretending that they were some other
thing and then the mock objects keep
track of how they're called and you can
inspect them and say or was the function
food called
on you and was it called six times if so
dime going to say this test passed it's
a very sophisticated kind of subtle
library but it's very useful in a lot of
situations pi test is you could think of
it as a more sophisticated modern
replacement for unit tests the basic
difference between PI testing unit tests
if I had to summarize it is that pie
test is lower ceremony to create a test
function you don't need a test classroom
and you have tests understand a desk
test underscore something and that's
your test and instead of having these
assertion functions like a cert false
assert true you just use the standard
assertion keyword and then PI test is
really clever about catching assertion
errors because point of interest defy
some people and what happens when an
assertion when you say a surgeon asse or
key space some expression pricing on
failure throws an exception which you
can then catch and deal with and that's
what PI test does so it's not like in
see where assert causes you can just you
know segfault immediately so if we have
time I'll get back into PI tests but i
want to show you hypothesis because this
is one of my favorite tools most
favorite tools in the past few years
written by a friend of ours called David
McKeever very very very smart guy so
hypothesis is property based testing for
Python and you want if you want to go
read more about it you can go to
hypothesis that works that's the URL
property based testing in a nutshell
we'll get to that I like I like David's
quote though the larger purpose of
hypothesis is to drag the world kicking
and screaming into a new and terrifying
age of high quality software so if
that's where you want to be and he's a
bit hyperbolic that's how he is but he's
smart enough to get away with it show
the hypothesis manifesto is worth a read
the basic idea of hypothesis testing is
that you define invariants or you state
hypotheses about how your code ought to
be how the state of the universe ought
not to change or should change no matter
what given that your code works properly
so you make these statements using this
lesson assertions you then tell
hypothesis how the test could be rand
and what parameters your tests need to
be fully tested so you
you define using variants and say that
no matter what no matter what values you
give me for these parameters this test
will always pass and hypothesis does a
really hard job of searching
systematically and very methodically for
parameters that cause your test to fail
all right that's it sits keep our is
that it's really smart about searching
potentially infinite spaces for
parameters that can cause your test to
fail and then when it finds a failure it
has really clever logic to reduce those
input premiers down to the simplest
possible versions so if it finds some
input list that causes your assertions
to fail it will work to shrink that list
down to a minimal example it's really
really clever and I find I have trouble
expressing how much it's changed the way
I approach testing in Python it's really
it's a mental oh did you have a
question oh I sorry I saw a hand up and
there's a bright light right in my face
so any motion is what's going on but
hopefully I can get across you at least
the idea that get not my pure unbridled
enthusiasm for the project so okay we
need a motivating function something we
want to test so this is an
implementation of greatest common
divisor not the best implementation in
the world but I can fit it on a single
slide so you may or may not believe that
it works but i'm pretty sure that it
works so it just taking m and n returns
for you the greatest common divisor the
biggest number that divides into each of
them is a 0 remainder oh i need to sorry
i need to execute that so it's in the in
the scope in the in the session so how
would i normally test this without
hypothesis i would probably do something
like this i would say well one thing i
know is true is that whatever d is
whatever the divisor is it needs to
divide evenly into m and n so i just
check this in a mod d gamma 0 and does n
mod megami 0 and i would test that for
some set of input numbers right in this
case I'm saying you know 4m from 1 to
1000 + n 1 to 100 give me the product of
those and passed it into this function
here binding to imminent and run them so
I'm doing some work of guessing what I
think are reasonable input ranges and
similarly I'm going to test that
whatever D I get back from GCD is
actually the biggest number that fills
the bill so this test is basically
saying there's nothing bigger than D
which
he divides into m and n so these are my
these in the sense or my assertions
about what should always be true and
then this is me doing some heavy lifting
to figure out the inputs to pass to my
functions and we execute those cranks or
a bit these are actually reasonably big
state spaces and then we see okay two
tests pass so it looks like our GCD is
in pretty good shape what hypothesis is
going to do is this work for us it's
going to do it smarter and better than
us it's the bionic man of input
parameter of calculation this looks a
bit more verbose because it is more
verbose but it's actually doing a lot
more this is the hypothesis version of
what's going on and so let me kind of
step you through the parts i apologize i
secretly imported given earlier given is
from hypothesis and it's the decorator
you use to explain two hypotheses how to
bind the input values to your test
function we have these things called
strategies that we'll look at in some
detail but strategies basically are the
things that generate input to your test
functions so putting it all in English
what's happening here is we're saying
given that I have one strategy that
produces integers between one and two
thousand and another strategy that
generates numbers between one and two
thousand bind those two M&amp;amp;N and execute
my test function try to find values for
M and n it causes failure in here I
should also point out that i put a limit
on these purely so that i could run the
tests here if I didn't put limits on
those hypothesis would feel free to run
for not unbounded amount of time but for
longer than I had patience for so you
can see it ran pretty quickly but that's
the fundamental idea of hypothesis you
give a simple excellent hopefully
relatively simple explanation of what
you expect to always be true you tell
hypotheses have to kind of punch your
function and it will do that until it
finds a failure or it gives up so of
course it can't it can't search all
integers because in Python all integers
is infinite right so we'd have infinity
times infinity which is you know mega
infinity we can't search that much space
this is doing effectively the same thing
so I will look in detail then what
whatever you know this
while we're moving parts inside
hypothesis with this in a nutshell is
what a hypothesis based test looks like
you have very very much smaller test
functions in general and your test
functions are less baroque they're less
complicated they're they're kind of
planar statements of truth alright we
doing on time here good so I mentioned
strategies and these are these things
that you use to generate and put into
your function and this is where the real
cleverness of hypothesis comes in that
they does these things in what doesn't
quickly it does them in statistically
valid way so you get nice distributions
across whatever range it is you're
trying to fill up and it can do it for
you know simple things like integers and
text but also more complex things like
fractions or going to lift complex data
structures with billions in this case
what we're going to see here is when I
run this a list of one example from each
of these just to kind of prove the point
that it can be done so we see we got an
integer 190 we got some random text we
got a fraction and we got a list true so
this great with very little effort we
were able to convince hypothesis to give
us some test values and that's the kind
of data just plugs into your test there
are primitive strategies for pretty much
everything you can think of Sloat
centers Williams more complex things
like text and binary and in vastly more
complex things like you you IDs and
fractions and you can write your own
strategies if you want but very often
you can get by just using these this is
a sort of full-fledged example we're
going to show that you know given a UID
a fraction and a binary of some certain
size run this test and really the point
here is just to print out what
hypothesis generates so all we do is
print out the things that come in and
basically a search false and let's look
at what's getting printed out here it
runs once and then it runs again so I
process this was doing some scanning and
then sent it said okay I'll catch you
some input and you failed it could be
that you have a kind of non item potent
test function so I'm going to try it
again with the same Memphis no we look
it's failed again so now I'm going to
say that everything is is dead and I've
come up with the falsification of your
hypothesis and here's what it did the
work so one failure and that's
failed okay not to be exciting to look
at this outfit but to give you some some
sense of what hypothesis is sort of
doing in the background it's very
sophisticated about the kind of work
that it does there are more complex
strategies for constructing lists
dictionaries streams tuples all the
kinds of things you would need to
construct in a in a Python world you
know containing elements drawn from
other strategies so you kind of compose
strategies and we can see that here we
want to create dictionaries of integers
to text your map of keys of integers and
text values and we want to create tuples
of floats and we just run this we'll see
some examples that it generates and
you'll see that it starts off with some
very very simple examples what's the
simplest possible dictionary of integers
to text it's the empty dictionary what's
the simplest possible to offload its
this you know one tuple you could argue
that actually an empty tuple would be
even simpler but it wouldn't have any
floats in it so it'd be hard to argue
that that's a tuple of floats static
typing and dynamic typing there you go
but again nothing nothing earth-shaking
here except we can kind of see what what
them hypothesis is doing in the
background what is passing into us kind
of quickly go through the rest of this
we can get onto cosmic ray and hopefully
some pie test every strategy has a
function called map and it also has a
function called filter so map takes the
output you know whatever examples
generated by the strategy and runs a
function on it and then whatever comes
out of that function is what's pumped
into your function into your test
function filter similarly takes the
output of a strategy runs a predicate on
it and if it's true then passes the
damage to your function otherwise it's
filters it out so here we use an integer
strategy and we map this function over
it so instead of just integers this is
going to produce only even integers
because it's going to take whatever
comes out of here x x 2 and that becomes
the value that could pumped into x
similarly we're doing exactly the same
thing here except we're saying anything
that comes out that's not evenly
divisible by 2 that is odd numbers are
filtered out so only even numbers come
out of here as well so math and filter I
mean if you ever use math and filtering
any other language if you've used link
or anything like that these are very
familiar hopefully
to you these are not not difficult
concepts but they can be they can be a
nice way to generate specific streams of
data so to speak flat map is a bit more
sophisticated a bit harder to explain
but in this case and this is drawn I
think directly from the hypothesis
documentation we have a strategy here
and we tell you know we say just
generate integers between one and two
really I guess meant that the thing is
next value is one more than what you get
met my experience at least pass that
value into the slap into this lambda
here whatever function is inside past
the flat map and then use the result of
that as the actual strategy so it's a
way to bind the output of one strategy
and use it at to parameterize another
strategy so what this overall strategy
is going to give us is a list of lists
of integers and these lists inside here
are going to be of a specific size that
fixed size in so whatever number comes
out of here it's bound to in and works
there that's a bit convoluted if you've
never dealt with anything like flat map
so come back to it later you want spend
more time on it but it's good to know
about this can help you construct some
very sophisticated data structures if
you need to we can we can run this just
for just for fun right so we see if we
run it we get you know three lists all
the size one if I execute it again I
should get well okay that's that's a
valid as is that these are you know
lists of size three and on and on and on
those are all coming from the same
strategy right here and guess we never
talked about this but you've seen it
several times if you have a strategy and
you call an example on it it gives you
an example of that it takes one element
from the from that strategy so you can
kind of probe your strategies
interesting i was talking with David
about hypothesis and what I should
stress in this in distress entation he
said he's finding more and more he uses
strategies completely outside of the
testing context they're a great way to
generate data for all sorts of purposes
he does a lot of what I would classify
his research into voting algorithms and
things for example and he uses his
strategies to generate input into his
voting simulations it's not testing it's
not hypothesis testing or anything like
that it's some something totally else
but it's a useful way to apply these of
these very powerful tools
other goodies you can use this thing
called recursive to generate things for
example in case on data structures piano
or any big complex recursive thing you
can have a thing called composite for
combining strategies and arbitrary ways
and data for interactively kind of
getting information out of strategies
all these things we're not going to talk
about in detail but you can look at them
on the hypothesis website we've seen
this a lot already the given decorator
and given we say at given and then we
describe the parameters we want pumped
into our test it's just the way of
binding strategies to input values so
you woke over these in detail but you
can you can use keywords if you want to
define the mapping between strategies
and arguments by default it's all
positional and yeah it kind of kind of
all works very intuitively and works
well with the testing tools that
hypothesis embedded inside of for
example if you think about how the these
might interact with well we haven't
looked at PI tests yet so it's a failure
on my part but PI tests also wants to
pass parameters into your functions and
so how does PI test play with hypothesis
if you start running into situations
where that matters you need to read the
documentation for both very carefully to
understand which of these are being
dealt with why apophysis and which are
being dealt with by for instance for
instance hi test but you can see here I
say given that x is integers and why is
floats then obviously you know the
mapping is pretty straightforward in
this case we're using curity positional
information to describe how to map this
enta jerz to X and this floats to Y and
here we're saying well that X is
integers and it goes down there and
floats us going over here and so kind of
see there's an interplay of different
orderings and ways of binding your
strategies to input parameters that's a
bit of a ramble and I apologize for that
but it all is actually very simple in
practice but there are some subtleties
to how it all works and so sit down
again on your own and kind of think
about what needs to happen we'll skip
over example because I it's it's not
terribly critical when you the basic
idea is if I've come up with some input
parameters but I know I need to test but
I know are critical I can use example to
say always guarantee that you're going
to pass these as input and always
guarantee you're going to
theses input you generate some of your
own but always use these so if I found
somebody give me a crash reporting said
it always crashes when i pass into well
i want to make sure that foo is one of
my input parameters and you can use the
example decorator to do that assume is
an interesting one um assume lets you
tell hypothesis that I know something's
about to fail but don't count it as a
failure because I want you to assume
that that value is never passed in so
it's a it's a word trick almost and it
makes much more sense if I show you an
example so I want to test that a
floating floating point number negated
and negated again is equal to itself
right that should hold for floating
point numbers except for nan right man
it doesn't follow these rules if it's
not a number wonderful so we use assume
here to tell to tell um hypothesis look
we know that man might come into this
function you might generate it to float
the totally legitimate thing to pass in
is X in fact I'd be surprised if it
wasn't passing in man we want you to
assume that X is not man and what this
is telling hypothesis is that ok this is
going to fail we know that but since we
told you to assume that X is not an when
this fails and X is man just don't count
it as a failure it's that simple it's
really why call it in a linguistic trick
because you have to kind of read it as
English committed for it to make sense
but you're still thing and just assume
that X is not man secretly we know it
could be nin but we're going to say that
if it is nan and we failed it's no big
deal and if I run this well I mean is
just going to tell me everything worked
fine if I take this out I imagine
actually yeah it will fail because X was
man and that doesn't work this this
logic fail so let's put that assume back
in so that we can ship this right
there's a thing called stateful testing
that is kind of new it says new and
semi-public although david assures me
that it's generally ready for use the
idea is that you can work at a higher
level than just single test functions
you can define your API and the
interaction of your API functions with
the state machine pass that state
machine into hypothesis and pointed at
your test what
passing some strategies to parameterize
the execution of your API and it will
try to run your API in the legal ways
defined by that state machine and find
failure modes look for assertions that
trigger I've never used it and it sounds
really cool I'd love a chance to I just
haven't had a reason to yet but David
assures me that it's super cool and I
trust I trust him further topic there's
all sorts of stuff you can read about
there's special support for instance for
Django numpy and things like that if you
are using those technologies I i really
want to end this section though by
saying that I highly encourage you to
check out hypothesis even if you have a
test this and you already like and are
happy with it is really powerful and
it's helped it's helped us for example
to find release obscure weird bugs that
we never would have found on our own
just bought into the fact that it's very
very smart it's smarter than us at
deciding what's a good input parameter
so on that note you have to try it out
it's very very cool we have about 15
minutes left that's plenty of time for
cosmic ray and maybe a bit of pine test
so cosmic ray is the cutesy name for a
tool that we've written for doing
mutation testing in Python if you want
to get it it's all open sorts at them on
github and we'll talk a bit about what
mutation testing isn't in some specifics
about the tool itself so that you're not
totally ungrounded does anybody know
what mutation testing is ok good this
will be the mildly education will have a
full talk on that as well as if you want
to talk about it afterwards them I'm
happy to talk your ear off about it so
what does mutation testing this is taken
from P I test or P I test this of Java
mutation testing tool into the gold
standard it's really really good at what
it does but it doesn't work for pricing
codes we had to write our own it's
conceptually quite simple you have your
test um you inject automatically feed
faults into your code and then you run
your test suite if your test suite
failed that's good it means your test
suite detected that change but if your
test suite passes it means that the
mutant survived and that's what you
don't want and you can use mutation
testing to gauge the quality of your
tests that's really what mutation
testing is about not testing your code
per se but verifying their tests are
actually doing their job you're ten
our high fidelity enough to determine
when you've broken things so what
happened to mutation testing we have our
code under test so you package your
library or your application and you have
your test suite they they really need to
be separate here which is one of the
reasons I advocated earlier keeping your
test separate from your production code
so to speak it makes this kind of
testing simpler you then introduce a
single change a very small change and
we'll look at some examples of those and
you run your test suite and hopefully
all your tests something in your test
suite fails that's the ideal that means
that your test suite detected the change
kill the mutant and and you have a good
test suite the basic algorithm is
something like this without going into
much detail for every up there's some
list of mutation operators and mutation
operators are deep the knowledge about
how to make a small change to your code
you replace an operator or replace a
constant or something like that so we
have some list of operators so any
particular operator there's any places
in your code that it might be able to
change right there might and many you
know greater than so they could turn
into left spend it you take that site
and then it runs your test so I call
this a triply nested loop for operators
sites and tests there and many tests you
want to run so anybody with any
experience in software knows this is
probably to take a long time you know if
if I have a million lines of code or
getting mutated then my test Scalia run
that can add up to a large amount of
time and that is one of the Achilles
heels mutation testing but it takes a
long time and a lot of the effort and
research in it right now is aimed at
making these things faster what does
mutation testing tell us well it'll tell
us if it killed the mutant that is at
the test suite failed there's also a
class of meatless called incompetent
mutants which are ones that can't run
for some reason maybe they immediately
throw an exception or seg fault or they
don't compile or something along those
lines it's the class of mutants that you
can't test so you can't really say that
you killed them but they don't pass
these I mean they don't pass your test
suite indices flag them as incompetent
and count them as counseling is killed
and then there's those which survive
those which that for which your test
suite passes and these are the ones that
should raise a flag for you to force you
to look at your code in decide what's
actually going on
what are the goals why would you go
through all this trouble because it is
it's quite a bit of machinery and work
one is coverage analysis of some of you
are actually all of you probably dealt
with the traditional notion of coverage
analysis we say when I run my test suite
do a hundred percent of the lines in my
production code get executed you know
are they passed through the processor at
some point that doesn't really tell you
all that much because it doesn't tell
you if your test me is actually
verifying the functionality so the goal
of mutation testing is to tell you is
your test suite actually testing
functionality and not just exercising
lines of code as a big big difference
and we can there's not time to really
cover that but intuitively they are
fundamentally different things and this
is what most people get coverage
analysis and this is the goal of
mutation testing actually verifying your
functionality is being tested properly
tested the other is to detect
unnecessary code so one of two things
might be the case if mutant survives it
might be that your test your tests are
not sufficient to detect and fully test
your functionality the other is that
you've got a bit of code in your package
that you don't need anymore because it
actually doesn't provide any
functionality so it's not that you're
missing a test if you've got too much
code and you can delete that code so
when you get a survivor it's up to you
the engineer to sit down and think about
you know which of these cases holds or
both could be so cosmic ray is the tool
we've written to do this kind of testing
in Python at the center of cosmic ray is
this notion of mutation operators what
do they do they first can identify
potential mutation sites so there are
passed an abstract syntax tree and they
report yes this note I know how to
mutate in this note I know how to mutate
and then the machinery of cosmic ray
sort of iteration da st and at the right
time says okay you found this spot now i
want you to make your mutation in hand
to AST back to me so ASCO abstract
syntax tree for those who do not tap on
the the terms of art but it's a it's a
data structure representing your code
fundamentally and then piping can take
an ast and compile it into code that can
then be executed you can replace the
module at runtime so these operators
really are the core of what mutation
testing is doing examples of mutation
operators we talked about this one
replacing relational
greater so maybe I find a greater than I
want to make it into a less than or
equal or something like that these ought
to be testable things if I make such a
gross change to my code it ought to be
conceivable that I have a test at the
Texas it's not always the case and there
are some pretty pathological examples
where you actually can't test or certain
changes and that's another research area
mutation testing another example
constant replacements maybe I replace a
zero you know literally rode with a
literal six and if that's an index into
a list that really ought to cause a
detectable change in my behavior more
sophisticated things include exception
swallowing it might say oh I second
that's a nice catch clause you've got
there what if I just caught all the
exceptions and never let them bubble out
the fact that should cause behavioral
changes you can detect so these are the
flavor of what mutation operators do and
there's a whole list that i would like
to eventually implement but we've only
got a handful of them so far there's an
ocean of sessions and a session
essentially wraps around the notion of
what the database fundamentally of all
the mutations and all the tests you want
to run and then as the results arrive
the results for those test runs and so
you in a cosmic ray run you will
initialize this database and scan the
code to figure out what work needs to be
done and you'll execute that session and
we'll start as results arrive it will
put them in a database and this is this
is nice because it means you can kill a
session halfway through and results that
are in there are in there and results it
or not you can then pick up and get it
later by reacts acute attest so it's um
it's a nice way of keeping results
around then for reporting as well so
this notion of session is sort of
central to how cosmic-ray execute we
have to execution modes currently it's
actually there it's a pluggable
architecture so you could have more not
sure what they would be but one is you
can run everything locally just on your
local machine the other which will be
suitable for real-world use i think is
using celery to distribute tests through
a task to you to other machines or your
or your local machine but to as many
machines as you want celery is a very
powerful task queue and it's been a real
win for this for this project the
mutation mechanism exists because it's
interesting i like to talk about this we
use the AST module two so we parse code
into an ASC we manipulate the ASC and
then we compile it into with
code object python has this technology
called finders and loaders which are
used and you say import something
finders and loaders are the machinery
behind the scenes that figure out what
to inject into your current namespace
and we kind of hijackthis to inject
mutated code into the proper name space
we use pipes of one price one Python
process permutation as a sandbox
technique because without that you have
the potential of the the mutant actually
breaking the runtime and breaking the
test execution framework and that that
would be really bad and really hard to
detect and fun and this is this is
really important to me as aesthetically
and and because I feel that it's a good
goal for the project that you can use
cosmic-ray without having to modify your
tests or your production code this is
this is an ideal we haven't been forced
to deviate from this yet although we do
need to think about how to handle
certain kinds of exceptions and that may
require that we inject special comments
into the code we're not going to force
you though did you make make radical
changes to your code to run cosmyk rain
so what's a full example look like well
I can show you the code here but
actually that's this let's just run it
so here I've got a this is a celery task
you or there's a worker sitting at the
end of this casket it's going to take
take work to do so the very first thing
we need to do is initialize I can make
that a bit bigger that's a bit too big
okay is initialize a session so we say
cosmic-ray initialize baselining is how
we tell cosmic very how long to run
before it consider something to be in an
infinite loop we tell what we're using
pi test the session name is NBC that
test module is called Adam and this is
arguments to the PI to surrender so all
this is going to do when I push return
is scan Adam figure out all the
mutations it needs to make and make a
little database for us so there's
there's this NDC JSON is is that
database now we want to execute in
distributed mode the session we just
created so I press return here if I come
over to my celery worker we should
eventually see it start to spit out
information about work that it's doing
here we go you'll also notice that all
four of my processors were pegged now
because celery is really smart about
knowing how many processes it can run
reasonably on my machine in that case
that's four by four course I assume so
it's running for parallel pipes and
processes each during a single mutation
and fully running the tests and now my
mutation are now my processor back to
zero so we come here and see the
execution is ended and finally you know
that drumroll the report says great were
56 total things mutations we made we've
run all of them and the survival rate to
zero that's solid that means that our
test suite was high fidelity now okay
just this test suite and encode is all
taken from our be cosmic ray test suites
so of course it's really really good I'm
going to go in and what really good in
the sense that it catches all the
mutants what I'm going to do here is
this is one of the tests that we just
ran I'm going to comment that out
conceptually well what's going to happen
now is without this test there's some
mutation that's going to happen in the
code that no longer has a test designed
to detect it so what we're going to have
now is a mutant that survives and I know
it sounds really scary but don't worry
it's okay if we reinitialize the session
weary exact the session we let it run
for a few seconds here if we really want
to go and watch the watch the fireworks
should fall over there it's kind of
comforting to watch this I the open
until I can see progress being made
because my tests are being executed and
then it runs and runs and runs it's
almost done right now when I do a report
I ought to see that I had at least one
survivor and indeed at the bottom here
the survival rate was 3.57 percent 3.57
percent of the mutants we created and I
don't know if I would trust that that
precision but that's the number we've
got more interesting Lee is that it
gives us a little dis a little patch
here to show us what it changed that
didn't didn't trigger a failure in here
we can see what happens we started off
with this line returned object and none
and we change it to return object or
none
pretty substantial change in some logic
that we had a test for but without that
test cosmic ray said I'm sorry that
mutant survived you're fired so that in
a very small nutshell is causing great
and mutation testing we really don't
have time to go into PI tests but what I
will do is give you some links if you
want this full presentation you can just
go to this this address here or just
talk to me i'll point you in the right
direction it's got instructions on how
to set it up and how to run the slide
that you want to do that if you want low
and want to learn more about python
we've got a book you can get we've got a
special discount code for everybody
seven dollars i think we do training and
consulting on this kind of stuff if
you're interested in that and on that
note let's see if this works right any
questions we got to keep in it yeah the
I'm gonna say no I've heard the term but
it doesn't oh I would like the kind of
stuff you see in from the java test
frameworks like jasmine and stuff yeah
I've seen those I didn't know that was
called suunto okay probably
yeah I would imagine that's to drop in
kind of thing you can just drop that
into any testing framework its
orthogonal to them okay well I guess
that's about it I love to talk about
this stuff so if you want to just grab
me in any point in the conference and be
happy to chat with you about it and
hopefully next year there'll be more
pricing stuff at this conference so talk
to the organizers tell them you want
more Python it's good for everybody
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>