<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>C++ Performance and Optimisation - Hubert Matthews | Coder Coacher - Coaching Coders</title><meta content="C++ Performance and Optimisation - Hubert Matthews - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>C++ Performance and Optimisation - Hubert Matthews</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G6IYBY-ZyLI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone to last talk of the day
on this one CBS 11 optimization and
performance I usually start off with
this question why why this talk I like
to say well what made me think I was
going to talk about those stuff well I
go around doing quite a bit of C++
training consulting talking to other
people and people use C++ because they
want control they want optimization they
want performance they want to get
everything out of their machine and I've
seen a lot of people talking about
things I think should I do this should I
do that which is more efficient so what
they mean by efficiency what I mean by
optimation optimization those are some
interesting questions all the time but
I've seen a lot of advice and the times
are going well it also appears to be a
little bit of sort of kind of well a bit
of this and a bit of that so you'll get
something so people asking should I do
pre pre incremental post increment and
I'm thinking well actually most of the
time that's gonna make absolutely no
difference whatsoever on on integers if
it's your own iterative type possibly
but apart from that really thinking my
great deal different oh yeah well
when we when I was in college I was told
I should be using shifts rather than
divides and I'm going well yeah a
compile it usually does that for you if
it's constant anyway and well yeah
divides are a bit more expensive than
than this ones out there probably quite
an expensive one altogether but really
is that the limit of the advice that
we're given Shirley's got a better way
so there has to be some way of doing
this in a more structured way and then
you'll get some people who might say
things like avoid array right this is
one that and rounds and rescue says well
yes ray writes do have a problem because
you have to do a read and a write and
then you haven't got and you can't put
them into registers yes that's fine if
you are if you're finding that's a
problem then probably then there's
something else that you're dealing with
what's the bigger picture here
ah well move semantics they're gonna be
great big thing on C for us 11 yes there
are performance advantage when why when
when the how they fit into this picture
oh we should be doing cash for ng
programming data oriented programming
you get my cat my customer is no
with that I forgot his name doing games
programming and data oriented
programming and pulling his hair out
telling us we all don't rock what he's
probably right
but for a games programmer when you've
got 16 millisecond batches essentially
there's a very different thing than if
you're doing data lookup you're doing a
web service or you're doing some bulk
calculations it's very different
ah well multiple threads will save us
all for this one well the answer that
one is definitely no it will probably
create more problems than it will solve
so I've been trying to teach people
about this one or getting to think about
this and they went well but well there's
more of a kind of a story and it's
there's a story that sort of kind of
goes like this so now this is what I try
to get people think about so you start
off and he says well your application is
too slow well yes fine if it's not too
slow then that's the end of this one
here and we're just all go to a bar
right now
so start profiling you start timing it
you go well what's going on and what I
usually find is that the first thing and
that next level is you're doing
something silly you're going like oh
dear it's the full the full facepalm at
this point where you start thinking why
are we calling the database every time
in the loop why are we doing this one
why are we doing that why are we doing
that so you find that there's usually
some silly little thing that you've done
that's consuming a lot of time it may
actually be a surprise to you that the
thing that you're looking at takes a lot
of time but you can sort of kind of get
quite a long way with that first part of
there and that's not really about sort
of Superbus optimization or whatever
that's just good programming right okay
so we get that one out of the way then
the next step is right okay what we
going to end up and then usually what I
found is that then you're profiling is
going to tell you that malach or mem
come or mem copy at the top that you're
spending a lot of time doing memory
management this is really quite common
and I've seen a number of people get to
this one and go oh you're at that stage
of optimization are you this is a this
is a process I've seen happen in number
of places so it's not about do I just do
this or that the answer is no there's a
story going on here we can get to
something faster why what do we do about
that one okay well actually at that
point we then need to start dealing with
things like temporaries
we need to start thinking about our
memory allocation pre allocating memory
rather than bits and pieces maybe you
want to choose we using vector we're not
using reserve those sort of kind of
things like that maybe we should be
using move maybe you should be using in
place rather than just creating stuff
how do we avoid the overhead of dynamic
memory allocation we'll come back to
some of those ones right so you somehow
get rid of that one and then you go oK
we've stopped throwing cycles away we've
stopped generating all these cache
misses and other things with thousands
of cycles going on Melek
and then you go right okay now at this
point it starts we can start looking at
our algorithm is it order n order N
squared in order log n order 1 what does
that mean and then we can start getting
those benefits because we've stopped
throwing away stuff so stop doing silly
things stop throwing away my time on
malloc and copying and then you get down
to that one this then really starts you
start asking about operational profile
and data access patterns have you got a
read heavy or write heavy application it
makes a big difference as to where the
caching makes a difference to your
caching both at the application level
and at the hardware level and then right
down at the bottom you're starting to
view with then you're really getting
down to it which is like memory
bandwidth and you're then starting say
well actually how do I lay this one out
how do I use the cache in a better way
but to start playing around with sort of
trying to cache friendly stuff when
you're throwing things where you're on
malloc is is you're starting the wrong
point so my view is that you should be
thinking this in a more structured
engineering approach top-down measuring
it that way
we're going to go through some of these
things I'm going to show you bits and
pieces about this one but if there's one
thing you take away from this one it's
that and you'll see that again at the
end it's not a set of tweaks it's an
engineering process and we're going to
have a look at a few things here it's a
multi-level problem it's everything from
architecture and design one of the
things I would like to stress is domain
knowledge domain knowledge is really
useful you can't expect the compiler to
do stuff that you can't communicate to
it there's a lot of things that we need
to understand about what you
doing how you use it as to whether
something is faster or not your access
patterns the SQL is great I like the STL
yes it is good but we better use it in
the right way there are alternatives to
it it's not always the right thing but
where if well used in lot of cases it is
strings so fill mash has just been
talking about string handling I'm going
to mention string handling so I will
have a few things to say on that area
about alternatives memory allocation
mentioned mu semantics and modern
machines mod machines are quite
different and they're surprisingly
different and compilers also surprising
as what they can and can't do profiling
tools very useful they tell you where
the problem is they don't tell you how
to solve the problem if you see mem copy
at the top which I saw recently on one
well you could try optimizing that mem
copy I mean going well if you notice
that says mem copy SSE for one it's a
compiler intrinsic the answer is you're
unlikely to improve that one stop
calling them copy why are you calling
them copy and it's getting you to think
about that one it's not just what's
there but how do I get rid of it how do
I reduce the number of things I'm doing
that so it's actually an engineering
process rather than a series of tweaks
no doubt you're familiar with the
Donalds North paper from 1974 that
everybody quotes and it says blah blah
blah premature optimization blah blah
blah we should forget about it 97% of
the time there's another section later
on but I want to refer you to which is
this one so that's 1974 that's quite a
long time goes 43 years and he's then
saying even a few years before that so
we're into 45 years plus so a few years
ago Stebbins looked at these Fortran
programs blah blah blah a bit that's
important here is how do you produce
hand up to something that's as good as
the hand optimized ones we found
ourselves running to the same problem
the compiler needs to mean the dialogue
with the compiler with the programmer it
needs to know properties of the data and
the certain cases can arise etc we
couldn't think of a good language in
which to have such a dialogue so back in
1974 he recognized that the domain
knowledge knowing properties of the data
and how you use it is absolutely creaky
to this one and you can't put it into
the compiled code in a way that the
compiler can do this for you it's your
job so two main knowledge very important
modern CPUs are fantastic amazing pieces
of kit they've got five billion
transistors in them and Counting tiny
little things you know 14 nanometers and
I'm just amazed at what they can do they
have long pipe lines ten stages or so
because they're splitting things up into
lots of sections because each one then
can go as fast as possible it means that
pipeline stalls are expensive so a lot
of efforts gone into reducing those ones
with branch prediction etc what's
interesting is you can get four or even
six instructions executing at once so
that's different to how you might think
I'm actually really a modern CPU if you
look at the assembler code that's
actually a high-level language so if you
think
EA X is a register it's not it's
actually a variable because it could be
any one of a whole pile of pieces of
silicon so a common to that one so what
you see in the assembler code is not
what's happening inside the CPU and if
you think oh well that's got more
instructions well it may be so but
actually it depends they may happen in
parallel you don't know and it's very
hard to to say so really you have to
measure out of order execution it may
not do things in the order you expect it
goes while I'm making for memory on that
one I'm waiting I'm taught that one here
I'll start that one because it doesn't
depend on that one
that'll waiting the memory and I'll come
back to that one okay so it waits for
that it's doing whatever it can is
building a dependency graph in Hardware
at runtime
okay that's pretty pretty neat
branch prediction this is the the
problem that you have a pipeline stalls
its guessing where a job is going to go
if you say well I have all this code
here and have a branch and you know
executing these bits in the middle in
parallel you've reduced that so yeah
we're getting lots of instructions here
but we saw well that branch now that's
basically just a jump inside the
hardware it's an if statement inside a
heart in the hardware and if you ever
put an if statement inside the loop
you'll know that that slows it down but
you've now got one but it's in hardware
the branch predictors are really very
good the AMD Rison chip now uses a
neural network for doing branch
prediction I've tried to find out more
details about that but they claimed some
fancy marketing speak on that so that's
how far they're doing this one you know
kind of machine learning inside the CPU
to the branch prediction it's that much
of an issue speculative execution you
can execute multiple things at once and
it picks the right answer hyper
threading you can have multiple cores
because you can say well actually I'm
waiting for memory but I'm going to just
do more some more compute here whilst
are waiting for that thing to arrive so
it switches between the two you also
have things like vectorization that can
go on and compilers can do vectorization
for you so you can load things faster
and compute faster 120 800 256 or even
up to 512 bits now here's the inside of
a has well nice intel chip just to give
you an idea of sort of kind of things
that are going on inside here 56 entry
instruction decode queue so the
instructions that come in are decoded
into simpler instructions and it's got
56 of those that's followed by 192 entry
reorder buffer so it takes those simple
instructions those my crops and reorders
them so if you thought you know what was
going on the answer is you don't and
it's doing whatever it feels like it's
then got this eight port 60 entry
unified reservation station thing okay a
reservation station is basically like
saying a piece of memory that can hold a
register at some point it's not register
one register do read two three so EAX or
your register can be any one of these
things if you do Zor EAX with
vx all it does is go and pick one that's
already 0 so it's basically got up to 60
things it can do at once
these can be storing and we've got
things here with shifts we've got
brought to branch detectors we got
divide we got vector logicals we've got
fuse multiply accumulates and two things
for doing load and store addresses
there's loads of things inside there the
answer is don't try and guess do measure
because modern machines are really
really rather important and clever
things much cleverer than I am one of
the consequences is you can keep jamming
that silicon in there but there's a
problem and the problem is that the
memory is out there and the CPU is here
now CPU when it's 40 nanometers yeah you
can do lots of stuff like this but
trouble is if the memory is out there
you've kind of got to go off down this
piece of wire up the other side so the
one like nanosecond is about this in a
vacuum it's about that on a circuit
board the trouble is you've got to sort
of kind of push electrons into this
thing here to try and get this piece of
copper to move up and then go down again
then when you want to go the other way
up to pull them out so there's always
pushing and pulling of electrons which
is why it's slow and what this graph
shows you it's one of many doesn't
particularly matter is that the red line
is you can add CPU great what happens to
RAM speed it hardly has gone up at all
as a jump at the end here but this is
gigaflops that's gigabytes per second
which we're pretty much limited by
memory these days not by CPU so trying
to save CPU instructions is not the way
to get optimization make sure that you
use the memory correctly is the way to
get optimization in most modern things
the consequence of this is 80 or 90
percent of the time the CPU is actually
waiting for memory it says on top or
your thing as measures yeah I'm hundred
percent busy what it's actually doing is
just waiting for the memory and if
you've only got 10 or 20 percent of the
time is actually being spent with
instructions if you apply all the
compiler flags in the world you can't
only really you can only add a save
little bit on that so your 10% might
down to 9% you save 1% of your run time
by adding all those extra instructions
in because you're dealing with a memory
the compiler can only do with a bit that
10 to 20
the 89% of memory that's your job
in order to make these things go faster
we have these wonderful cache things and
we have lots of those so modern CPUs
have got lots of caches in here they're
nice and fast at doing this vertical
kind of stuff here they're very bad
actually at doing the horizontal thing
so I'm not going to touch on
multi-threading which is really the
horizontal cache thing because that's
really a disaster from performance point
of view but one two three cycles 100
three cycles you've got a factor of
about a hundred two orders of magnitude
between accessing l1 cache and accessing
main memory could be even worse than
that if it's off port so this is why
using the memory effectively is really
important here's a lovely picture from
Sergei in that Japan the trimco this is
time across here and this is in orders
of magnitude there are six orders of
magnitude between 10 to the 0 and 10 to
the 6 here just one thing to point out
to you is that concurrency can offer you
about one order of magnitude you've got
a factor of 100 between here and here in
order to try and make that up with extra
CPUs you need 100 CPUs minimally and
that's before Amdahl's law comes and
gets you so don't try and throw multiple
threads out this to try and solve your
problem try and solve it by actually
using the memory that you've got
correctly so make a single thread go
fast before you start having extra
threads
we've got everything here from less than
one cycle for simple register dredges
because you're running multiple ones I'm
not going to go through all this one
because there's lots of staff we got
branch detection here one or two if you
get it right 10 to 20 if you don't the
divide that we talked about here
division it's in bits
it's kind of slow yeah you don't really
want to do too many divisions if you
want to go first if that's what you need
to do that's what you need to do there
of the order is the same as a function
call an indirect function call for a
pointer or virtual like this with extra
parameters in it 30 to 60 that's about
the same as a reading from the l3 cache
so function calls do hurt you from a
performance point of view so inlining is
your friend at this point more important
main ram read here 100 hundred fifty
three hundred whatever you feel like
that
notice here allocation and de-allocation
two hundred to five hundred cycles
that's malik that's the direct cost of
malik that's the i'm going to go over
there and going to go and look for some
memory oh I found some actually Malek is
really a way of saying please go and
find me something that's not in the
cache so nobody's using it's not in the
cache as soon as you try using it bang
what do you get oh you're going to get
one of these you're going to start doing
main Ram reads that you've not had
before because it's nice cold memory so
malloc is nice and slow so let's see if
we can get rid of that kernel calls
300-500 context switch so this would be
on the concurrent one uh that's a
thousand or so but that's just going
into the kernel and doing the scheduling
coming out again but the problem with
the with the switch is that as soon as
the thread comes around it goes right
okay now what was I doing bang
completely cold cash because it's all
somebody else's stuff so you're then
going miss miss miss miss miss so you
can end up with thousands of cycles if
not tens of thousands of cycles for
context switches so there's just an idea
to to give you an idea of the range of
things that you can have measurement I
mentioned it being important so let's
see if we can have measurement on our
big o-notation here's some wonderfully
artistically drawn things like this you
may have order ends straight-line order
N squared which is a really not nice
order log n which is sort of kind of
flatter an order one remember these are
asymptotic as n gets very large and
there's a multiplicative factor that's
involved in here as well what you may
find is that for small numbers like this
it doesn't matter linear search on ten
items is probably faster than binary
search or as fast you're not going to
notice the difference so dunno how big
your n is and also check see how big
your K is you could say I
do really really good hashing but by the
way my hash function takes me half an
hour to calculate that's not a good use
of your time also if you are going to
measure one of the things that I found
and I have to keep in mind myself is
that the optimizers oh three for
instance are really good at getting rid
of your code so you end up benchmarking
something you not think it's doing
because you calculate a result in it's
throwing it away
so so do do that so do make sure that
you don't throw that away use the result
it'd be nice if you could have
everything faster that we can't and what
you end up doing is making trade-offs
and you make some things faster than
others what's important and this means
knowing your operational profile this is
part of that domain knowledge what do
you want to be first and what are you
prepared to make slower to make that one
faster usually 8020 kind of stuff comes
into this one and classic example making
reads fast and write slow in a read
dominated system tools plenty of them
around I've mentioned mostly these are
Linux ones there are some equivalent
ones around on Windows stuff and it
depends you can say I have an unmodified
application so I can take the the binary
as it is and run that one so I can use
perf all the things like old profile top
yes that's useful just to see roughly
what's going on io type I've used that
quite often to see what's happening with
input and output the usual vmstat io
stat force are all those kind of things
or even just bash and just say just
timeless and it's surprising surprising
useful just if I make a change does it
go fast or not your Vantage it's full
speed it's often based on sampling you
can usually use large loads with it or
something of reasonable size commercial
ones Intel have some you can modify your
application you can say I'm going to put
coverage or profiling in there I can
write my own stuff I can put in chrono
high resolution clock for sections and
if code that I'm trying to do that there
boost has one you can write your own
benchmarking stuff Google benchmark
allows you to do those once a micro
benchmarks
you can also use simulators so the
valgrind set of tools
valgrind cashman's etc they slow it down
by a factor of 10 but you get very very
good insight so an example with cash
grins is that you can say all right
okay I've simulated this one I can see
where the cache misses are come because
it actually shows you which line they're
on and whether it's an l1 cache miss or
l3 cache miss so it's well worth looking
those to understand data layout and
performance so this is something you'll
find the games industry they refer to
this one as array of structs instruct to
the race and it actually comes up in
lots of places do you say I basically
have my objects like this and I have a
vector array of those ones like that do
I store in rows or do I store them in
columns well it depends and it may be
that you end up actually having to store
both for performance you may have may
have to duplicate your data and so there
are people doing memory analytics who
have a row and a column store if you
store it by row like this array of
struts and you go through and pull these
ones out then actually you're just
jumping around in the memory you're not
using very very well if on the other
hand I just store in columns now I do
this you'll find that is now we're
iterating through this in a dense linear
fashion the first one has a thing called
read amplification because it says well
I'm going to read this entire this
entire row like this the memory is read
is 64 bytes at a time but actually if
I'm only using four bytes out of 64
bytes I'm using 1/16 of the memory
bandwidth actually for what I'm really
using this is called read amplification
there's a similar thing for write
amplification when I have to read all
things put one byte in to write it all
back again that's you're just throwing
away memory bandwidth that's fine if you
only do it occasionally but if that's
what you're doing all the time you're
wasting it if you go to this one here
now we're going to dense linear pattern
like this the compiler goes through look
I can vectorize this so we can use the
sse or AVX instructions so instead of
picking them up add add add like this
four bytes at a time
so these two bits it can say well
actually look I can pick up four maybe
even eight bytes at a time
so GCC will
like this visual studio is similar it's
a dense linear access pattern so if you
want performance a dense linear access
pattern is a really good way to go nice
and fast the generated code for that
well if I do owe to on this one you'll
see ad so just pull that one thing here
moves four bytes on and does the carries
on going here with the vectorization
it's doing it in the XML and doing
sixteen bytes at a time two and a half
times faster online online little laptop
here the hardware may even be
prefetching that for you so it may it
may go faster than you think not that
one and that's now definitely memory
bandwidth limited rows at a time columns
at a time hmm which do you choose
well actually you may find that there is
a compromise between this one so one of
the things you can do is say well
actually look here I had all these
fields like this I had a and F and lots
of other things like that if I'm
actually only interested in one or two
of these fields the really common fields
why don't I do store those one
separately so if I do this I say well
actually look these are the most common
fields like that I'll store those
separately from these ones here have the
hot data and the cold data and now my
hot data is much denser in memory and
not I have got my read amplification
problems so the columns bit Hey well
that's me I get to do that I'm now the
developer that I get to do that one
because that's my domain knowledge the
rows on the other hand are done for you
by the hardware because it says look I
can do that in a cache line so it's
keeping things in memory that way so the
hottest of the hot is now being you're
now concentrating on that because your
cache is full of the hottest things and
nothing else and this is halfway between
rows and columns which is sort of kind
of like groups arose and groups of
columns so you may not have to choose
one or the other but it can be worth
splitting this one up to do that
one of the classic techniques strength
reduction and I mentioned you've you've
got memory cut I've got mem copy in
there well how do we eliminate it first
thing is do I actually need to make that
call how could I get rid of that call if
I can't get rid of that call maybe I can
reduce the number of times I can do it
maybe I can cache things maybe I can
just do bigger things I can roll them up
together as a batch and say well
actually why am i allocating one by two
that a 1 integer at a time sector for
instance puts them all together and says
oh I'm going to allocate a chunk of
these now I'm going to Kate twice as
many next time so the overhead of memory
allocation and vector is amortized
across that one loop unrolling is
another example of the overhead of the
test on the loop how do you reduce these
things they're classic ones the old
divider shifting and Ray likes this one
so strength reduction is one of these
big ones and so I will mention that
there move semantics
this is super Russell Evans stuff
cheaper bus zero three couldn't tell the
difference in temporaries and non
temporaries so what you got there is if
you do string 1 plus string 2 plus
string 3 you're creating temporaries
you're allocating memory potentially
depending on the sso things like this
you may be allocating memory and then
just changing his temporary changing his
temporary you've got a lot of stuff
going on there
and you couldn't say well actually I
want to take this temporary and modify
it you also then couldn't movie you had
to copy the temporary in the end so that
was nice and slow C++ 11 introduced move
Auto pointer with something in zero 3
which was an attempt to do move
semantics and language it didn't allow
you to do move semantics unique pointer
is taken over from that and that does
actually do move semantics yay
and we've added r-value references move
constructors move operations just for a
basic kind of introduction to this one
on the left here what we have to do
before with this one where this is a
buffer here with a part so this
an X with a pointer to an allocated
buffer if I wanted to move this one I
couldn't I have to make a copy which
means allocate copy all the bytes across
move the pointer and then delete the
original so you end up with an allocate
in the copy and add er Kate this is a
real waste whereas in the move world you
say take that one just move the pointer
yay no allocations
you just got essentially a couple of
assignments to do it so where does it
happen well in in all of these ones here
if I'm returning by value all of these
ones here are then going to be copies in
C plus 11 this one here is a returning a
temporary therefore that's a move it's
called the move constructor this is an
l-value
so LHS here has got a name and therefore
Streeters non-value won't move from this
one but you can be explicit and say
please move from it here be dragons if
you're not careful because you've moved
from it you now have an object which is
in a state that you're not quite sure
what it is it's a move from State move
semantics very useful how you implement
them well notice that it's so these are
in this case what I've got here is just
pointer assignments so just move the
point copy the point across and all that
one out the move assignments has to
clear the left-hand side first
notice I've also done no except no
except here is important for libraries
for things like making sure that your
vector can move things not actually in
to copy them which leads us also onto
the subject of resource management so
very briefly you may well have come
across the rule of three which says okay
if I have any of the copy constructor
the copy assignment or the destructor in
my class I need all three in order to
make it work correctly C plus 11 we add
a then move constructor move Simon this
became the rule of five this became
really boring and it's meant that you
had did a lot of stuff and actually now
the rule of zero is the way to go and
also from the performance point of view
which is do not do any resource
management in your classes use of
standard library use things that copy
and move correctly and then you don't
have to
it saves you a lot of time and it saves
you a lot of heartache it also can mean
that you end up having faster code
interesting thing a discussion was just
having earlier is that if you have a
class with the unique pointer inside it
just a unique pointer it then becomes
move only which means that you can't go
around and just accidentally copying it
and copying data using got pointers of
megabyte of data you wanna copy that
accidentally so now move only you can
now have to make an explicit copy so
you've stopped the accidental copying a
quick example here of taking 10 million
entry 10 million integers and putting
into a container essentially doing push
back or insert into them in sorted order
vector without any form of optimization
does 25 allocations because that's now
that's a 64 megabytes plus than the 32
that was before plus the 4 but all the
way down to here so that's the 25
allocations Dec 78,000 because there's
512 byte pages but if you notice these
other ones list an order set and set it
at 10 million or so the reason that
unordered set is higher than that one
because it's got buckets and other
things to allocate if there are
thousands of cycles guess what the
performance of those is going to be
you're not going to do very well with
these ones if you do a lot of allocation
like this so be careful with the STL and
how you might use that we can optimize
it vector instead of the 25 allocations
I can reserve it and that now says right
ok I can reserve 10 million and I get
one allocation and notice I get that
improvement here and it just has exactly
the right amount of memory rather than
64 Meg's it's 40 Meg's so reserve one
well well-known way to use vector it's
also safe safer than just saying it's an
array of this size because if you now
push an extra element it will still work
it just has to go and reallocate it just
saves you the time getting there a set
if you're going to put things in here
the domain knowledge I've got here is
I mean putting these in a 30 - order in
sorted order so if I just insert them
it'll start the top it's a balanced
binary tree so we'll have to go and find
this one here there's ten million of
those then we've got log to base - which
is what - twenty-three twenty-four
something around that things of how many
comparisons I've gotta do to get there
for the last ones number of allocations
etc well I could say well why don't I
start somewhere else you can provide a
hint I can give it exactly the wrong
hint and say why don't you start the
beginning and put essentially I'm saying
you know that ten million why do you
start at zero down here and see where
you go and then how's it going go all
the way through the tree which is why
it's lower if on the other hand you say
insert at the end well here we go much
improved because it's now just at that
one end you don't have you cut down all
that search the 24 to go so watch out
for things like hints just a few things
about standard containers that's for the
rights remember that this was also about
trying to reduce the number of
allocations because we know that
allocations are will often dominate
finding items lots of stuff in in data
structures
here's a random look up into some of
these ones here what happens we use
standard find whatever use the STL
incorrectly and what happens if I use
the containers correctly so I've got a
bunch of things like this I'm pulling
out some random values and number out of
that so I've got ten million I gotta do
100 finds generate 100 random values I'm
gonna push back here's my timing loop
standard find this is the kelie
optimizer not to eliminate this one here
because otherwise it just throws away
the results of that that's what that's
for the linear find works well on vector
and Dec list is relatively poor
performer I would and for exactly the
same reasons that Phil Nash mentioned
this is like pointer chasing it also
doesn't use the memory bandwidth very
well because we've got 24 bytes per
elements rather than 4 bytes per element
so it's eliminates memory bandwidth
limited unordered set is actually quite
good on this one because it's actually
in terminator
singly-linked list set is not
particularly good at all because it's
got my out more pointers to each of us
so that's a linear search but we know
really that an ordered set and set you
shouldn't be doing that
really please don't do that so let's
have a lot more of these ones here
four million lookups and let's actually
use the Container one and now we're in
the right sort of kind of place ten
thousand times faster excellent here so
that's what we want in the first place
domain knowledge comes up in the moment
which one will do with vector in a
moment hashing hashing can be a lot of
fun there's some optimization about the
hash function and hashing functions you
need to know about how well old is
distributed how well that they they
split things up
well the GCC library says if you give me
an int it knows ah why do i I don't need
to ash against I'll just use the int so
actually does no hashing at all on int
and if I try to Prime in Tashia
myself that basically does the same
thing it gives that result if I say a
float instead it goes on r2 I don't know
about that so if I use float you can see
it's know to be slower but if I go back
to a float with a hash function that's
just the conversion of that one it we're
back in the same thing so watch out for
your hash function for doing a lot of
these things you can end up with a
slower lookup table than you wanted that
hash by the way is an SMD one a hash
that just takes the bytes one other time
and does a an exclusive or and multiply
I mentioned how do we get faster on
these things well vector and deck we
know domain knowledge tells us we put
these in orders no now got a sorted
vector ah maybe we can do binary lookup
so binary search or lower bound my
research just gives you back true or
false lower bound just gives you back an
iterator and notice that vector is
actually faster than set for that
particular one it's dense in memory deck
is not quite as fast because they're
pages a memory you have to go through a
vector of pointers to find it
so again this is just the use of domain
knowledge which leads me on to this
there's a lot you probably know that you
can't put directly into your code and
this is what real make the biggest
difference so what are the relative
probability of these operations how many
reads how many writes what do you do
that thing you need to understand in
order to say well these things need to
go quickly
don't try make everything the same I had
one one client that was trying to make
everything look the same they were
tripling this was in telephony and they
were trying to save everything so we're
trying to do a complete do a failover
and duplication or everything else and
and I went well guys there's a reason
that you can't go this fast is because
you're trying to say to do the same
level of safety on something here that
just says you've got an extra you've
used an extra kilobyte of data on your
mobile plan as to that 911 emergency
call over there that's really not the
way to go fast what's the worst thing
that can happen if you lose this well
somebody gets an extra kilobyte actually
it's not worth trying to save that too
much 911 you stopped the entire world to
to get that emergency call through so
don't treat everything the same what's
good enough that's also something that
people seem to forget that faster is
better well actually saving one
millisecond off the startup time your
program by optimizing your configuration
reading to do things in the right way
with string views because you've saved a
few allocations well probably not
necessary what do you know about the
data the size how long are these how
long are your keys so you're going to be
using strings that fit into SSO are they
all variable length or a little all the
same size if you're using product codes
at all six or eight long or your ISP
ends you know what size they are why you
using dynamic data structures when you
know how big they are what's the
distribution in your data what are you
doing you do batch so LTP these are the
things that will give you more games
than just trying to use the STL
or compiler flags I've got six questions
that I'm going to or go through just to
say well okay
these are things which are very
important to understand from a
performance point of view about the data
the way you use it primary key lookup
non-primary Kugler key look up range
scans rewrite working sets and
consistency so primary key access most
common form of access database primary
key that's a classic example
so inaudible map or map classic example
I'm just looking that thing up I'm
looking up a financial instrument by a
number I'm looking at the customer lots
of things like that you can use hashing
so that's order one you can use binary
search or linear search you really only
need an operator equals equals to do
this
there's no ordering you don't need
anything else you can just say it's that
one or it's not that one it's very
straightforward it allows you to
partition things if you come to a
conference like this you may say well
actually okay
people with surnames a2l going to this
queue people with m20 here
perfect partitioning fantastic don't
need the two don't need to talk you get
almost perfect scalability so primary
key lookup absolutely wonderful great
product catalog your typical even up
products by individual numbers their
customer records and want that customer
web sessions you don't want to go and
see anybody else's web sessions you're
not iterating across them all no SQL
kind of key value stores memcache for
instance doesn't even have a way of
iterating across keys you can only look
something up you can't find out all the
keys or voice rate through them that's
the most common one non primary key
access so what do you need in terms of
looking things up not by the primary key
show me all accounts with a balance
greater than 100 so often looking at by
value not by the key in the databases is
secondary index if you're building these
in memory you may need some additional
maps that point into other places like
this maybe you're looking at part of a
record do you need to search on metadata
so are you looking up oh when did this
happen who did this in certain cases you
may actually need full-text search
inside
it's substantially more work so on the
web for instance primary key axis is a
URL this URL don't care about anything
else that's really fast non primary key
axis is called Google it's a very very
different kind of thing so you need to
understand whether you have this whether
you need some not so it's almost always
slower range scans sequential access
phil was talking about looking things up
and you had his hash maps and ordering
so a binary map binary tree you're
paying for the cost of knowing they come
in a particular order with a hash table
you go I don't care what over there and
you actually gain a significant amount
this is why an unordered map may well be
a lot faster than an ordered map don't
use a map if you don't need the ordering
because even paying a price for it this
requires operator less than you're
probably going to need iterators you may
need some sort of kind of traversal
state there's a lot of things going on
in this area you may need to do a seek
followed by a scan Hadoop is really good
at this one you go oh go find that thing
here's five milliseconds to find the
blocks and I've got 64 megabytes of data
coming out that's a very nice gain dense
linear fashion watch out for things like
the n plus 1 database problem if you're
doing oo based programming or using an
ORM it's very easy to say Oh on that one
I want this one I want that one I want
this one you go backwards and forwards
to your database and you can make call
thousand records each is maybe three
milliseconds that's three seconds to do
those ones where it's actually one SQL
bulk call might have done the whole
thing in maybe 20 30 milliseconds so
watch out for bulk and see where you
want to use that particularly in things
like distributed systems it makes quite
a difference on your read to write
ratios if you've got a lot of read
caching is your friend but you then have
say well okay this is application caches
as well as some common memory caches in
the hardware what do you do about write
back what do you about eviction how do
you keep those up-to-date hmm what do I
need about here you may well find that
actually the metadata
there is worth cashing but the data
itself is not worth cashing the index
structures are very useful in certain
cases you're doing data logging and
caching really doesn't help you much
other than the metadata because you're
just writing and writing writing on the
end so knowing this read/write ratio is
a very important thing for performance
point of view working set how much data
have you got how big is it so these are
pictures the bottom here if you have
something like a news website that's
people are coming in here Moe starting
to look at this today's articles or
yesterday's articles is very strongly
biased towards a a small set if on the
other hand you go to the airport like
this most people travel once or twice a
year you will probably find that
actually that you've got a very flat
kind of approach here there are a few
frequent fliers and crew at the top but
the rest of it is very flat and you're
dominated by i/o here on in the passport
case and the news case you're you can
fit it all in RAM you get a very
different answer I was speaking to
somebody yesterday he said well actually
he was doing with publishing but it's
it's scientific publishing and it's all
longtail the documents are all 40 or 50
megabytes of XML and there is there is
no peak out here it's all like that he's
basically all that even though it's the
same publishing thing because just
because of the access ones so this is
then how much will fit - I need a bigger
machine can I actually fit this one here
is it worth trying to do compression on
my data to try and fit it in because
that saves me the overhead going to the
disk all the things are very important
to understand from an optimization point
of view the last one is consistency acid
versus base transactions if you're doing
airline booking it's like one seat there
on that aircraft you need to know
whether you've got it or not and that's
a very different matter from saying yeah
I've got a hotel here and I'll just say
has the hotel got Wi-Fi or not doesn't
really matter if that happens now or a
little bit later you need to understand
that one because if it's if it's acid
this transactional you're probably gonna
have it centralized you have lock
two-face commits all those kind of
things which is a very different matter
because you won't be able to speed that
up anywhere near as much as you'd be
able to speed up the the base kind of
stuff with copies and other things so
understanding that is very different and
again it may be data versus metadata so
your transactional data may well end up
being acid whereas or reference takers
not so that's an idea of kind of that
middle section there once you've got rid
of your man copy and some ideas about
memory structures and other things down
there we may be also be looking at
strings strings as mentioned just before
reference counted in in the 98 standard
or specifically allowed to be the C
purpose 11 standard specifically said it
was not allowed because you don't want
hidden shared State inside the STL
because the memory model and concurrency
issues GCC moved to that version 5 the
visual studio has been using the SSO
non-shared version since about Visual
Studio 2005 we've got the lovely string
view coming up boost string ref
similarly you could use arrays of chars
or even C style child pointers and we
end up with some choices and again I
can't really tell you which is the right
one domain knowledge is the thing it's
going to do this one if you have char
star well its variable size and you
don't know about memory allocation maybe
it's all Stephanie allocated I don't
know you have all functionality as
external things like strikes and struck
copy all those other things and there
will be a null at the end which is good
but you have all this ownership problem
you don't know if you're pointing at
something whether it sound or not is it
going to go away or not and obviously
there have been all the problems over
the years with buffer overflow because
it forgets its length it's one of the
reasons I really really don't like using
Chester's with a raise so standard array
of char n 6 sighs there's no malloc you
know what you're getting there you get
most of the stuff you from STL container
including an operator less than by the
way but it's a byte at a time so that is
where you
get this will translate to mem Kemp up
to you whether you have annul on the end
or not if it's fixed sighs you may not
need it
you have ownership nice and cash
friendly because it's I want one of
those it's right there it's not
dynamically allocated is right on your
stack and the stack is almost always hot
because you're not suddenly look off
into the heap but there's no move
operations because it's fully owned
because there's no pointer to do so
movie the same as copy version for
string and version five string have
slightly different characteristics in
terms of the size notice with the GCC
one for instance is 32 bytes the
Microsoft one is 24 bytes from a
brightly because it's three-pointers
they haven't got this fourth pointer in
there lots of functionality
lots of nulls etc you don't have to
worry about ownership and you get the
SSO kind of stuff string view 16 bytes
on 64-bit machine because it's a pointer
and length no malloc it's great it's
basically constant string you don't have
a null at the end because it's pointer
and length but that's again part of it
so a big advantage you have got this
ownership now you've transferred you
said right okay I have to know what I'm
pointing you have to know where it's
still there and so that is a proof that
you now have to take on it's basically a
fat pointer and the advice these day
seem to be passing by value so just to
go through some of those ones again we
start off at the top it's too slow you
start profiling you get the silly kind
of oh why am i doing that one
don't moments often this is about a
strength reduction how do I get rid of
some of those ones here then I usually
find that I find that you're into sort
of kind of malloc and memory allocation
issues and memory copy and mem comp
memory memory comparison for instance if
you're doing lots of stuff in maps
because it's going to have to go through
a big map and do all that memory
comparison all the time
this is where having temporaries can
hurts you so see we're just to go back
over this is where move can help you or
pre allocation of your memory or saying
reserves on my factor
because milwaukee's is not cash friendly
at all and also our pointers in there
which is again taking away off your
stack this point I were going to start
looking at my the choice of MySQL
containers or other ways of doing it
with say open interesting hash tables
that point and I can start doing the
thing of saying well actually hot and
cold coms and separating those ones out
so that leads us into sort of kind of
the last few things about this one the
strength reduction things like static if
you're creating an object in a function
how often do you create it you're
creating every time we call that
function is that oh is that constructor
causing you too much trouble for
instance if it's a vector zero I
allocate it in the dik-dik allocate DRK
maybe you should use a static allocate
it once and call clear at the end or
clear at the beginning so you to hold on
to the memory but basically get rid of
the contents silly things I've seen
people doing log log all these arguments
are well it's all right sister we
haven't got logging turned on that
moment in any on that's a debug and
we've got a set T to error level said
yeah but you're actually calculating all
those arguments transferring and sewing
through to the log thing that has an if
statement inside that throws them all
the way so basically is throwing all
that performance away creating these
objects and doing nothing with them and
so the advice there was fear or log
macros out if log level equals is
greater than the debug or what I'm sorry
is less than debug do this so don't
calculate all the arguments just a few
things I've seen you can do things like
string interning if you've got lots of
strings back to the immutable strings
you go I know what the strings are on
this one I got an example which is
looking up 5744 airports we know what
the IATA code are I got all those why am
i storing these all do I need to what do
I need multiple copies of these ones do
I need multiple copies of the name
London has several airports do I need
London several times maybe I can
actually put all those into one to
reduce the size of my memory and then
use a string reference and so use a an
India
into a series of strings and if I have
that well if I've got my statistics I
can now put all the common strings at
the beginning and the less common
strings at the end that's making me more
cash friendly instead of having an eight
bike pointer I can now have a two or
maybe even three byte index into this
one and I've then made my cash smaller
don't use acid when bass will do because
that limits a load of stuff about me
when you're getting it the locking and
all the other things that they do that
one so this is overheads to doing am to
do acid stuff I've seen people doing
stuff and like they calculating the time
one more time and changing time into a
string and a friend of mine who does
tuning on on IBM mainframe stuff he said
he found that actually in one particular
case the top lying there was converting
times and strings so be careful that one
if you might as well just do it once per
second if that granularity is good
enough do it once and there you go I'll
give you that for all the hundred things
I'm logging after this do virtualization
you can do virtual eyes your calls if
you I use the keyword final for instance
if you have in c plus eleven you have
virtual calls you know finally of a
point of that one if you say final then
the compiler knows that there cannot be
anything below that so it doesn't have
to it doesn't have to do the virtual so
therefore can inline the call and that
then says in line here in line in there
so final on the edges it's safe because
if you try to inherit from it it won't
let you and you'll find out the compile
time so use a final you can that can
help you there fast path and slow path
if you know what the fast path is detect
that very quickly and cheaply and say
right this is one of my 80% ones and do
that one here if it's going to be and I
want to have something that doesn't have
any false negatives it pushes me into
the slow path occasionally when it
shouldn't do that's that's fine but want
to be able to say is right in the fast
path there's no memory allocation
there's no locks as there's nothing it
likes straight in there straight back
out again
so again that's knowing you're right
8020 you can save yourself ten two
thousand cycles with this one avoiding
Malick's and copies so strings how do
you use those avoid temporary string
string 1 plus King 2 per string 3 you
may be used plus equals maybe you can
use string view instead watch out for
conversions it's actually surprisingly
slow to convert it into just two
floating points on the other way around
at a low level use move a number of
people you'll now see using sink
arguments on constructors you'll pass by
value and then do a moving sight that
that is counter intuitive Lee faster on
number of compilers these days
pre-allocate your memory use return
value optimization so returning by value
you'll find that actually the compiler
doesn't even do that one in C over 17
it's mandated that that will that that
optimization happens so use the rule of
zero preferably few use inheritance
avoid deep inheritance hierarchies
because you've got a lot of constructors
to call that's much less of a problem
that used to be string to USS said avoid
allocations watch out for our own
algorithms you may find that you can
that order n log N 1 going back to map
unordered a nap or your own hash tables
very useful you'll need to know about
your data access patterns and whether
you require ordering or not and how you
want to handle those strings in terms of
cache effects array of structs start to
raise maybe you can do hot cold
separation to say here is these are the
pieces of data I want most and then
crunch them up that way and then that
the hardware do that one you have a
choice of hole by point or a hole by
value holding by pointers this form of
hot cold separation it allows you move
semantics but then you get this
trade-off with it's being through a
pointer so you have to do some
measurements to understand that ones a
lot of these ones here this cash effect
also depends upon the size of your data
because you've got even 32 kilobytes of
data cache and there's other things like
that watch out for read and write
amplification don't if you're going to
read something make sure that you're
using everything that's inside that
don't read a - you're not going to use
playing around with the compiler
optimizations is fun the vectorization
is a particularly fun and inlining
inlining is definitely your friend on
this one so multi-level problem it's not
just set of tweaks so I'm trying to
provide a way of the order which you
might would look at those ones here and
stressing that domain knowledge is
really a key to this one here
Donald nothing new that long time ago
and we seem to forgotten it or actually
if we do we just don't make it explicit
that we know that we're doing we're
using it and that will beat your clever
algorithms all the time go and find some
tools learn how to use them learn how to
interpret them that is it any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>