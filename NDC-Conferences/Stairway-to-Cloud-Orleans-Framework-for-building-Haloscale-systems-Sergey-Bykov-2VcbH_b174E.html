<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stairway to Cloud: Orleans Framework for building Halo-scale systems - Sergey Bykov | Coder Coacher - Coaching Coders</title><meta content="Stairway to Cloud: Orleans Framework for building Halo-scale systems - Sergey Bykov - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stairway to Cloud: Orleans Framework for building Halo-scale systems - Sergey Bykov</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2VcbH_b174E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome everyone so I'm gonna be
talking about Project Orleans is Orleans
framework is I've been dev lead since
the beginning so it's very dear to my
heart who in the audience ever played
the Halo game oh it's good so the
experience you're dealing with is very
different from what's behind the scenes
too so instead of explaining what I mean
by saying halo scale I have this video
that I'd like to show you the beginning
also to wake you up a little bit let's
see if it plays halo is a rich immersive
story with millions of loyal and
dedicated fans we deliver an exciting
and engaging experience to these fans
they need to know what the hot playlist
is today they need to know what the
challenges are they need to know where
their friends have been what their
friends have been playing have their
friends gotten more medals than them
they need to know all of this and they
need to react to it and interact with
their friends in real-time we need to
deliver hundreds of thousands of updates
per second to millions of players across
the Halo universe we need to get them
the right information to the right
device at the right time
there was nothing off-the-shelf that
solved the problems we needed to solve
but the scale we needed to solve them so
we turned to Microsoft's extreme
computing group
hundreds of thousands of requests per
second across thousands of servers in
real time these guys are crazy but the
extreme computing those are the kind of
challenges would like to tackle you can
probably tell that video was a couple
years they made a couple years before so
I was younger but I think the coop gives
a very good idea of what actually we're
talking about we're talking not scale
tailor and those kind of services so
we're going to be talking about the
cloud obviously and people give these
definitions of the cloud by the way
we're also going to be playing the game
I'm gonna be playing the game name the
tune who knows name the tune No
so when you see when this is in top
right corner sentence in quotes if you
can if you know what song it's from or
at least what the band that played it
just yell it
and somebody who gets the most answers
right will get beer at the party so
there is a price so just yell this one
the hardest one I promise you anybody
knows anybody got the power no that's
actually from David Bowie actually it's
also my test for the age of the audience
just to get a sense
well no no Justin Bieber no Taylor Swift
that Noah boy either I'm you don't want
me to sing on stage next time so when we
talk about the cloud really like the
essence of the cloud is that you get
this enormous resources available to you
to rent so that's why everyone's got the
power to get almost infinite amount of
resources so you have a credit card that
you can charge to pay for the services
this power has been available to major
corporations or governments for a decade
but now anybody can do it a small
start-up can suddenly grow from nothing
to call unicorn I hate the term but they
call them unicorn right but with great
power as they say comes great
responsibility
so you build systems at that scale you
you face new challenges or new all
challenges in a new forum like for
example concurrency like who in the
audience enjoys debugging multi-threaded
code and data races and dead logs I
don't I'm just kidding but now who likes
to do that on distributed setting when
you have a logs from say 20 machines and
they try to figure out what happened
that's you would have magnitude more
difficult than just attaching debugger
in finding that dead log or the dead
arrays so you have these issues of
distributing your computations
concurrency scale failures are the norm
right and in the cloud what used to be
happening maybe every few years in a few
months now those failures happen every
day depend on your scale because
machines get rebooted they get patched
you see it as a failure often times see
there is a set of new challenges that we
haven't seen before and then when
businesses look and try to figure out
what to do with this all that glitters
is gold name did you
thank you that's great one point so you
hear this cacophony of this analysts and
consultants and Talking Heads saying
here's the solution like for example a
few years ago people who are saying you
see Facebook was built with PHP and
MySQL so if you use this technologies
you can build anything right the build
Facebook so even if you use even before
that web services and soap they were
supposed to solve all the problems in
the world all good technologies and
don't get me wrong
these technologies are fine but when
somebody says that this technology will
help you build a cloud scale solution I
look at it as they trying to sell you
this elevator or if you watched Willy
Wonka and the Chocolate Factory like one
conveyor whether it is button and you go
up and out that solves all the problem
um like for example go right is the new
the hipster language programming
language no wait
because docker is written in go so again
if you write in go you need to learn and
go inaudible solve your problem of
course not it's not the case and then
you see other comments like oh you have
to be stateless or observation that
micro source is a term a good term a
good architectural term got abused too
fast and this is my favorites a manager
Foley thanks Mary George she said that
the release would solve all the cloud
problems back in 2010 that's my favorite
one but then you see this picture who
has heard Kyle Kingsbury talking about
Jepson called me baby great so if you
had never watched go to youtube search
kyle Kingsbury Jepsen's call me maybe
you will not regret
everyone who deals with the color has to
watch this talk it's a brilliant guy he
just single-handedly showed that all
this pretty much all open source
distributed bases that are available
they all don't maintain their guarantees
in case of network partitions he got his
beefy machine in his apartment and run
all this commercially available open
source software in South EMS and he
recorded
reads and writes to this day
while he was partitioning connections
between those VMs simulating actual
network partitions and node fillers and
he shows that every single one that got
a MongoDB
read is elastic search all these
technologies break down and violate
loose data
violate their guarantees so he showed
this picture of tire fire and he
explains that at the top of it the API
level of the database you have this
rainbows and unicorns everything is fine
from the API perspective but if you look
underneath under the cover there's this
tire fire of code that doesn't really
maintain its guarantees so you look at
that and it's very hard to decide what
to do but that's the reality of our
industry in my view if you stuck back
there is this triangle of really
concerns you have compute you have state
and you have connectivity and there are
many choices like you have to make these
trade-offs who are you what have you
sacrifice in the tune Jesus Christ
Superstar because you need to sacrifice
something to get something for example a
batch processing is very efficient if
you can afford high latency if you can
process within minutes hours you can be
extremely efficient by putting a lot of
data and processing in the MapReduce way
with Hadoop but if you need sub-second
latency that doesn't work you have to
sacrifice this efficiency for low
latency and and this challenges and then
trade-offs going on and on the database
the sequel is very good at transactions
and guarantees but doesn't scale well
the key value stores are very good at
partitioning scaling but they don't
provide usually you secondary indexes
that sequel does it for you for free so
again you need to get something you need
to sacrifice something to get something
I just highlighted what we sort of were
concerned with in the project Arleen's
but then if you've heard of a cap
theorem I hope everyone heard of the cap
theorem that says that you cannot get
consistency and availability at the same
time in the distributed system that's
pretty much the axiom so this is the the
real challenge we deal with when we talk
about the cloud and the solutions are
different right so we have we can hire
hero developers
in years ago at Microsoft and developer
division had a different term with
Einstein developers the category these
are people they can build very complex
systems so somebody built Google
somebody built Facebook somebody built
MSN and hotmail and those kind of
systems so it is possible to tackle this
challenges and to build stuff but those
developers are rare and they're
expensive and they're all happily
employed so if you try to build business
by hiring a bunch of hero developers
that can kind of solve all these
problems you can run out of budget very
fast but most likely you won't be able
to hire them because what's in for them
to leave their job they like and join
your company so in reality when you try
to hire people you need to look at the
available pool the cool here a program
in Erlang ok there's a couple of people
yeah I know young did Scala yeah one
person two people f-sharp ok more but
still a minority so I have really
sincere deep respect for people that
master these technologies really like
the joy I'm strong is good and giving a
talk I think about Erlang but if you
look at reality you can hire people you
cannot find people that have these
technologies to try to hire young to
your company you'll fail but but also
even at the hero level these developers
not Indian to make mistakes and and the
pattern of successful higher scale
services if you look at Twitter if you
look at LinkedIn if you look at Facebook
they have the same pattern where the
rear key checked it and rewrote their
system three or four times as their
usage grow so they had to throw away the
whole solution essential not just
incrementally improve not just refactor
but throw away the architecture in put a
new solution in place at the most
critical time where the business was
growing and some people some people
argue that that was the failure of
MySpace that the reason myspace lost
competition to Facebook is because they
weren't moving fast enough they couldn't
scale with their users and their
experience suffered there are too slow
so I would argue it's not a scalable
solution to try to hire more than a
handful of hero developers for a company
but then we look at the problems as
engineers but if you talk to business
people they look at it from very clear
business lines like they see time to
market the return on investment those
are the terminologies that they use
which means I need to build systems fast
I need to build them cheap and they need
to be reliable so that cheaper to
operate it's a capital expenditure
versus operation expenditure that's fine
mental picture is those people trying to
sell you this elevator wonkavator way
push button and you up in the cloud
which is not realistic it's just up
oftentimes a bunch of people that don't
know what they're selling or the
Charlatans that trying to sell you this
bridge to nowhere in reality it you need
like a stairway where you can walk you
can run because you're in a competition
if you if you work in your competition
is is running then you'll lose any
competition so you have to run to stay
in the competition that interesting
quote from Alice in Wonderland but the
Queen says it takes all the rain you can
do to keep in the same place if you want
to get somewhere else you must run twist
twice as fast as that which is I think
it was about other business it's not
about Alice in Wonderland it's not for
kids so that's my mental picture is we
need a stairway something realistic not
climbing like with the ropes not magical
elevator but something real if we step
back and see how we've been building
services we didn't call them cloud
services a decade ago but for 15 to 20
years we've been building them as and
tier 3 tier architecture this picture
must be familiar to everyone assume so
you have a stateless layer of front-end
so the web server is a servers that
terminate client connections the
authentication DDoS protection admission
control and then forward requests to
middle tier or or several tiers but
still it's the middle tier stateless
again middle tier that goes and talked
to storage to pull data in perform an
operation or not and potentially write
data back to storage so if the request
comes for a user profile
Milty or cold storage give me profile
for that user and then I do some update
and right back to storage or maybe I
don't even write update I return back to
the front and to render a webpage or
respond to the mobile client so this
this is wonderful model it's beautiful
it's very simple you can scale easily by
adding more servers in the middle tier
more service in the front end the
problem is storage is much more
difficult to scale so you especially if
you have a database like a sequel Oracle
database at some point you exceed its
capacity and the turns out they cannot
scale and as the industry we realized
that a long time ago so we put the
solution a cache layer in front of it
memcache D read is all those solutions
they reduced the load on the storage
because now first time you really put
data in cache and then after that you
read it from memory which is much faster
you move data between memory and then
you only go to storage to update but in
reality that complicated the solution so
much now you talk to two storages you
have your cache storage and you have
still your cold storage you need to
coordinate that and you need to write
updates to both and as you probably know
a cache invalidation problem is one of
the hardest problem in distributed
systems fundamentally so programming is
it is not really nice really I think
this is what we want we have we want to
have a stateful middle tier where data
would be cached but also the computer
would execute so this is what I call the
stateful middle tier have benefits on a
bow instead of putting data in cache and
Ryan compute somewhere else can we have
them together then the tune anybody
knows these are good together no llores
so I would argue that's what we want and
that's how we approached or least when
we when we started working in our lives
we really check out this kind of two
challenges we wanted to have a
probability model which is easy and
attainable for a wide range of
developers so you don't have to be here
developer to understand and write
successful software with it but also we
didn't want to turn away expert
developers those heroes they should like
the model as well and the model needs to
be flexible enough and powerful enough
to empower
those developers as well so that's the
trade-off between simplicity and power
but also we didn't want to make
developers 20% or 30% more productive
wanted have qualitatively better
productivity and which means 3 times 5
times ideally 10 times more productive
and the main way we know how to make
developers more productive it to have
them write less code because the best
code you write is the code you don't
write because you don't have bugs there
that's paradoxical but it's true right
if we can eliminate code from our code
base we eliminate box that we didn't
introduce there so that's what we target
our goal was to reduce the amount of
code you write but also make it a code
you write much simpler so you much less
error-prone and more productive in
writing and debugging it in testing it
the second pillar of the project was to
make this code scalable by default which
means if you write code following some
simple guidelines there is a good chance
you'll build it in such a way that it
will scale so if you have suddenly ten
times more business ten times more
customers or 100 times more customers
your code will work you may need to
tweak if you optimize a few places but
you wouldn't have to go and really check
and throw away the whole thing like in
those cases like with LinkedIn Twitter
and Facebook so there was a kind of
conflicting goals in a way who has heard
about the actor model excellent so hope
people attended yesterday's talk by
jorgensen for those who don't know you
can think of actor model at just the
distributed object model so you have
these isolated entities that do not have
access direct access to each other's
memory they have to send messages to say
hey do this from you know give me this
value and of course they can create
other factors so the model was invented
in 1973 by Carl Hewitt a long time ago
and you can imagine there was no cloud
and was built for a very different
purpose so cute invented it for is a
concurrency model for single machine
single process systems for AI artificial
intelligence application
but this often happens in our industry
nothing is new Under the Sun so that the
pros get rediscovered like in late 80s
and 90s by Joe Armstrong Carrick's on
the bill
Erlang in a new implementation of the
actor model that they built some control
plane systems for up there telco
equipment that later some distribution
features were added through OTP in the
cloud space people rediscovered again
this model because if you think of it
because you have these independent
entities and they exchange messages they
don't have any assumption of locality so
if I'm sending a message from actor a
tractor B I don't assume that they're on
the same machine the implementation on
the runtime could have been that it
that's how its implemented but
fundamentally the model allows me to run
this actors anywhere they want as long
as they can so as I can deliver messages
between them so it's easy to distribute
these models and these modules these
actors so that's what we took as kind of
the base approach for project carlene's
namely tuned no also doors so we we
didn't want to just go blindly and look
at the models we with sort of to key in
independent approach and we came up with
this what we later called virtual
accuracy as we work on the system as we
try it and different approaches and
threw away some of earlier versions and
work with early customers were utilizer
these challenges fundamental challenges
in existing approaches in Erlang
approach and akka is a sort of GBM clone
of her Lange the fundamental difficulty
was that in the distributed highly
concurrent system it's very expensive to
write code to coordinate this actors we
need to create an actor for user use in
the front end receive requests for but
what if your three protons receive
requests for the same user first they
need to go check do we have an actor for
this user in some registry so you do it
concurrently indicator is response no we
don't all of them three independently
realize I need to create a new actor for
this user
and of course they in parallel try to
create an actor and then they need to
register in the registry and all of them
but one should fail and should handle
this gracefully of course it's a lot of
coordination to get right and of course
that kind of code works fine in the
simple unit test but when you aren't
scale suddenly they have this
concurrency in race conditions that's
what we heard from Erlang developers
later that that's actually indeed is one
of the biggest challenges to build
distributed systems of Erlang so the
idea behind a virtual actor is very
different so the analogy is like virtual
memory when you write code to say touch
or update the value in the array but
index X you never checked with the
operating system is this memory page in
memory or is it in page file you don't
write code you say load this page file
from me and then I'll set the value you
just set the value and its operating
system job do you realize oh this page
was in the page file I'll bring it I let
you update the value and then once it
gets cold I write it back to page file
so it's the same basic idea so all
actors in Arlene's we call them grains
instead of actors to differentiate that
actors in Arlene's that very different
from what people used to think about
actors that's we call them grains
so there's grains they kind of always
exist virtually so you can always make a
call to any actor in the system so long
as you know identity of the actor and a
call generally will always succeed
regardless of whether the actor is in
memory or in storage or in the process
of being activated all this complexity
of coordination is done by the runtime
so earliest runtime really performs
behavior lifting it's interesting that
what we discover people equated erlangs
approach with actor model save so when
we started talking about orderliness the
first reaction was what you build is not
an actor model because you don't have
super vision trees so they thought I was
an axiom in in the actor model that you
have to have a super vision tree to be
an actor model which is actually not
true which Carl Hewitt no less that no
that's not the case he's complaining
about Erlang was kind of similar we
didn't know that - what release did
eventually so you remove all all this
complexity of managing life cycles of
of this actors give it to the runtime as
a result you write less code and you
write simpler code so that's how we
achieving the goal of developer
productivity
so let's look how the code looks in
reality when I'm asked to explain what
arline's is in one sentence I say
distributed C sharp which is any kind of
to two words or one sentence a 30-second
description is not accurate
it's not about C sharp it's just
distributed net but that that kind of
works for people because you program it
with the same paradigm you have inner
spaces and you have classes we start
with the unit phase so you define what
we call a grain interface and you define
it by extending one of the marker
interface in this case I use I grain
with gooood key which says actors of
this type grains they'll have good as
their identity and then within this
interface you can have one or more
methods one requirement for those
methods that the asynchronous the return
task promised for a value who is
familiar with TPL task is sink await
great so the majority that was that are
not I think that's the best way in
c-sharp 500 that the best innovation we
talk to JVM people when I first started
talking to them they didn't believe me
that was true when I will see the code
it's just brilliant how it works so
that's the requirement that all calls
are asynchronous whenever we make a call
to in this case like hello world say
hello the result you get you get right
away before anything happened you get
his promise task of a string means the
promise for a string that will arrive
later maybe milliseconds later it may be
seconds later but later so you're not
you're not blocking on this line that's
why one requirement is that everything
is asynchronous so when we invoke the
grain this is an example I just need
three lines I use static class a grain
factory and say give me a grain that
implements this interface that we
defined above for a user with this
identity so I pass through it and what I
get back is it under the covers a proxy
object the variable user which
implements the interface that I asked
for its return immediately it's
constructed locally if there is no
messages involved and then I can make a
call and in this case you say hello
right away so the first two lines they
will take probably in nanoseconds to
execute because they do nothing you just
say okay here's the promise for a future
result and then through the magic of a
wait keyword and in c-sharp 500 can say
execute the rest of the method when that
the result comes back without blocking
the thread so this is very simple the
code looks straightforward in sequential
but in reality it executes very
efficiently because we're not blocking a
threat we're giving up under the covers
compiler reaps of briefs out there
remain remainder of the method as the
continuation and executed asynchronously
later so that's all I need to try to
make a call to your grain and once I get
response back once the way its returns I
can kind of do something with this value
when I implement the grain class it's
also very simple I extend the base class
grain that's in the library and then I
implement one or more interfaces grain
interfaces that I defined again it looks
just like your normal object oriented
programming unlike one way message
passing state machine and things like
that you just implement interfaces and
classes but notice also that this method
say hello has a counter it increments
the counter does on the last line and
the reason I can do this without any
logs any synchronization is because
every method in in the grain executes in
a single with a single thread guarantee
so the release runtime guarantees that
your code never runs in parallel on more
than one thread within a single grain so
you always have full control of your
private State you can always assume that
nothing else is touching it so you don't
need to put any in the locks semaphores
any other synchronization mechanisms
which simplifies the code and removes
lots of again the box that's the way in
and that sort of the reflection of the
original idea of the concurrency model
of actor mode that you can write safe
code nobody else will go into variables
as you execute even your own methods
will not touch
because they only around one at a time
so what happens behind the scenes the
grain is is kind of a logical contract
it always exists but if the physical
incarnation of it goes through this
lifecycle there can be in persistent
storage and probably most of the time
it's there not not in memory and only
when the call arrives for a particular
grain the runtime gets and instantiates
a physical incarnation of that logical
contract we called activation goes
through initialization through
activation process where if needed blows
it state and calls the method that is
kind of like a constructor say hey I'm
activating you do your initialization
and then delivers the request that
triggered activation so for a while done
grain stays in memory and and the
runtime checks what was the last time
that a grain guy touched its activation
of McGregor touching a message to
process if it hasn't been called for a
while and by default is two hours but
it's configurable you can set one minute
five minutes for different types so
what's called nobody send a message with
grain there's no need to keep it in
memory so release runtime garbage
collects again we go to the activation
process a a I'm about to deactivate you
if you want to do something gives you a
chance and then removes it from memory
so that's the model behind the scenes on
the programming on the caller side it
you program as if it's always in memory
but in reality runtime manages resources
and does this distributed asynchronous
garbage collection of your resources and
I will stress again with no code from
the application maybe configuration how
how fast how aggressive you want this
garbled garbage collection to happen so
if we go back to this picture with this
actor based middle tier because of this
lifecycle what we're really getting is
what in memory is just a sliding window
of all possible grains actors only those
have reasonably used now or within that
period before they get garbage collected
an example would be like a major game my
Hale or Call of Duty they solve probably
3040 50 million copies that doesn't mean
that all those users are in memory are
active in fact you can find very few
days in the year when there is more than
1 million of them playing at the same
time so the reason
reason to keep state of 50 million of
the players in memory you can just have
those automatically that they actually
turn on their console and then started
the game and as they stopped playing the
game or shut down their console there
their grain will be become cold and will
get deactivated so the runtime does this
resource management for you for free
again without right you write an
application code for that so the early
neuron time it runs like in overlay our
physical resources or virtual machines
so it on every virtual machine that you
run in the cloud or in the physical
machine a few random premises
there is one usually process of released
runtime we called silo and those silos
they form a cluster automatically and
they start pinning each other to see
that who is up who is down if this silo
didn't respond to me three times I
suspect it's probably dead so it it has
all this magic of tracking Hardware
status essentially so if one of the
machines balls-up the runtime
automatically detects it and understands
what grains were running on that machine
so they're gone they're lost because the
Machine disappeared maybe the physical
hardware failure or network cable got
cut there are many reasons why a machine
disappears what's important is that once
the runtime this distributed logic
realizes we lost this machine it knows
that grains Orion there are not running
anywhere anymore so you can place them
when the new request arrives for a grain
that used to be there to a different
machine so you can operate with the
cluster without that machine for a while
and then if that machine gets gets
repaired or restarts and comes back he
joins the cluster again it becomes
another resource to place these grains
and executes Moore's all of that is done
by the runtime again so you don't need
to write any code your individual
request may fail so you make a call to
your grain and you may get an error back
and you may get an error back for many
different reasons that storage is
unavailable or or something else or the
machine just died in this window before
the runtime realized it's dead you may
get a failure but fundamentally you can
keep repeating this request and
eventually you'll succeed once all these
conditions
recovered so you don't need to write
code to understand where things run or
what stay there on you just write your
code in this simple manner as if they
always exist in always in memory so
besides Hillel world let's look at
something more complicated than that
let's see see this is the a made up of
social network example so have notion of
a friend they have this user interface
but I have a method at the friend myself
so notice that in the method signature I
can use I user as an argument type so
the runtime knows how to serialize these
references on how to pass them around
without you writing any code in fact the
compile time we generate serializers to
efficiently pass data data types and
preserve them as if nothing happened if
they're on the same machine so we define
the interface and then let's see how we
implement executing this method so first
two lines get two references four four
two grains for me and for my friend like
in halo world example just say give me a
reference for for this grain with of
this type for this identity and then
what I do I just call mine grain say at
the friend and I'm passed directly this
reference what's important to understand
here is that the reference is logical
it's always valid it doesn't point to a
physical machine physical IP address URL
nothing like that it just encapsulate
the type and identity of the grain so
it's always valid I can save it in a
database I can shut down my system I can
restart it a week later I can read this
record and make a call to this grade and
call will succeed because the runtime
will activate grain with identity
deliver my request and execute it and
deliver response to me
so unlike physical references this a
logical reference they're always valid
one thing here which is not so obvious
is we're making horizontal calls so
these grains they live in the same level
in this middle tier if you go back to
the picture of three-tier architecture
if I had the logic for one user to make
a call to another user
to go all the way out to the web service
and make a call to user service and pass
target user ID as one of the arguments
go all the way through front ends to
another middle tier server to execute
this request here the whole call happens
in the middle tier so just direct
communication between the doing those
grains on the same layer the other thing
that is interesting here we put
try-catch but is you can imagine the
caller migraine and my friends grain can
be on three different machines so how
come we can catch an exception here so
here's the picture to demonstrate it a
little bit better so say we have a
front-end receive request may recall to
grain a to process this request it's
part of this a logic of processing that
grain called another grain of a
different type maybe on another machine
which in each certain may call to the
grain see to do it's part of logic and
imagine green see through an exception
for example the friend you pass to me is
already in your friend list so you're
not allowed to hit him twice or this
friend this person cannot be your friend
forever reason so traditionally what you
have to do you have to analyze the
return result and then propagate it
turns out back and then propagate it
back again what happens in Orleans if
you write no code zero for error
handling here that exception from C will
be delivered to color B and if B has no
try catch it'll automatically propagate
to a and if a doesn't have try cache
will propagate to the original color we
call clients of code that runs on on a
front end not within the grain space and
call them Orleans clients so this
exception will be automatically
propagated all the way up with no code I
can put try-catch anywhere I want for
example I may decide to put it in C or
in B but by default it will be
propagated if I put no code and as I
mentioned before the error handling code
is the code that usually is the buggiest
because that's the codec hardest to test
so we get here is essentially
distributed synchronous try catch
semantics
with the very powerful contract that I
can only boot code were actually needed
in most of these cases I can do nothing
I cannot retry or or do anything to fix
an error I just need to report you end
user requests failing here is the error
code or description there you see from
an exception so I can do that the front
end player and just render a webpage or
a responsive mobile client so that's
actually a very powerful feature up the
runtime
so look another example still staying
within the social network sort of theme
but when you say social network don't
think just Facebook or Twitter or those
kind of things like gaming like a
multiplayer game with a social network
just a much more fluid where these
relations are forum for a multiplayer
session and they dissolve and then user
during different session this is
essentially social graph if you're
talking about IOT devices it's kind of
the same but much more static you have
sensors they have rooms you have
buildings so you have this relations
social graph kind of relations so it's
not limited to just a traditional notion
of social network so imagine I need a
method to return status of all my
friends like for example my stupid UI
wants to render a table with friend
status friend status friend status
and let's say I'm very popular have a
thousand friends so if I were to do it
neibling call one friend at a time get
response back then call another friend
and get response back even if the
latency of a single call is very short
if it's take 10 millisecond if I call a
thousand friends see really the minimum
latency of the whole series of calls
would be 10 seconds 10 millisecond the
times thousand so of course I don't want
to do that do that I want to call them
in parallel and that's what's very easy
to do to fan our calls in our links and
this two lines of for each what we call
friend get status and remember get
status returns a promise a task for for
a result which we put in in list right
away so this whole for each again we'll
execute it within nanoseconds or
microsecond so it doesn't do anything
just prepares those messages to be sent
and then through the match
GPL and async/await we can join this in
my example thousand promises into one
the chassis will result when all of them
get responded to and then await it so
with this one line we wait all the
responses and then once all the
responses arrive we can process the
results and render my web page my stupid
table with friends statuses so in the
few lines of code we find out requests
and and the process results very easily
it's very easy to do these kind of
patterns so in ideal case or ideal
latency is the latest every single call
but also notice that again we didn't put
any multi-threaded code no blocking we
do nothing here that would be out of the
ordinary so we write is if it's a single
process code in the single application
the writing on a single machine but we
get a lot of parallelism so you have
enough course all this calls will be
executed in parallel so it feels like a
desktop app but actually runs on the
cluster who is familiar with MPI it's a
few people so that's a library for very
efficient distributed computations so
there's this famous professor Dennis
Gannon he told me a couple years ago
said we don't want to teach your
students same ti anymore because it's
very hard to get it right with the
release is so much easier to do it you
can implement the same patterns but with
much fewer lines of code with much
simpler code and I was so happy when
Carl Hewitt the inventor of the
actor/model wrote this thing last year
in his paper so he said in his orleans a
couple paragraphs said that it's an
important step went further than the
goal of the actor model that application
program in need not be so concerned with
low-level system details so that's
exactly we try to achieve to raise level
of distraction to make developers more
productive and code the right simple and
and I think I tweeted at the time that
I'm ready to retire and they check my
savings account they decided to stay
work not ready yet but interestingly
enough in another survey
Kyle he pointed to Erlang's deficiencies
lack of error propagation which I showed
you exactly what Putin only it's not
knowing that that was his concern and
also lack of resource management these
two complaints about Erlang you would
you see without knowing that we
implemented exactly those things in or
liens there are many features like I
just highlight a couple of them so it is
declarative persistence you can you can
declare a state for your grade in class
as a property back class just very
simple poco class and then you pass it
as a type argument to the base class
grade and when you declare your user in
this example use the grade in class and
then you get this method the state
property of the type that we declared as
the poco class and you have this usually
use a single method rights that is think
this is where I say persist my state I
said my property is persisted to storage
and how it works there is a plug-in
model their persistence providers so you
don't have to write code against
specific storage like at your blog or
sequel or the s3 and a double yes you
just write a single line and then
provider will know how to deliver this
state update to specific storage how you
link them is through this this attribute
they say I want to use provider with
this name and then in the config you can
declare that this name is for as your
table storage so you can change your
storage and that you target that code
without changing application code you
may need to migrate your data if you
decided to move but you don't have to
change your code at all you just change
your config but this is an opt-in
feature you don't have to use it can
just write code where you talk to
storage directly yourself so it's up to
you just a convenience feature so if we
included a few providers with the code
base but there are others that are built
in by the community for storages we
wouldn't even consider building
ourselves another feature that we added
maybe a year ago slightly more came from
this knee
that when people used orleans and see
this RPC pattern when you call and you
get a response to request request
response remote procedure call a pattern
they say well i want to return a series
of values or i want to subscribe to
dallas that somebody will produce i want
to produce a series of values so you're
talking about streams and we have a
stream API which is a single API over
different delivery mechanisms so there
are three categories there is direct TCP
messaging where you just won't deliver
this is synchronous updates directly
over these connections between silos
just by sending messages no persistence
or you may do the same over durable
queue is like a jerky or SQS event hub
like actually when happens if in the
third category Kafka in the bed hub
they're in a different category by
themselves because they're not really
queues they're distributed the partition
logs where you can say I want to go back
to this person this offset in the log
and redeliver messages from that point
it's a very different very powerful
model but we have a single API that
works across all the three of them I
would say it's a controversial decision
to have one API over a three because
they have different enough semantics so
we questioned that decision but that's
what we did we put a single API and if
you look at how it works I call provider
again by name because it's driven config
driven like with the persistent provider
and I say give me a string upstream of
integers with this ID ID is a good so
again like we took the virtual actor
model and made virtual streams so it's
almost you know identity of the stream
you can always produce a consume from it
you don't need to create it you don't
need to find it just say I want to
produce a stream with this ID or I want
to consume from a stream or this idea so
you have GU it on the namespace it's
easy to model things like user and the
user ID X or device and device ID Y and
then produce or consume
messages and you produce by just calling
on next they sync we model API on our X
or async version was supposed to be
coming that's not a controversial
decision because
we took Namie from murex which may not
be obvious or the best choice for naming
we just try to be consistent when I Rex
regardless you put the call on Nexus Inc
and produce a valley or you can produce
a batch of values and on the consumer
side you define your handler which will
be invoked and you subscribe say for
that stream I want to subscribe my
handler so will be invoked on every
value of every event on this stream
arrives and that's all and that's it
it's very few lines of code and again
those streams they're virtually exist
the whole time you don't have to do
anything about managing them but also
the streams work not just between grains
the work between the client like the
front end and end grains in both
directions a very symmetrical model so
if you have front-end that terminates
WebSocket connections or nqp connections
it's easy for that client to subscribe
to it event streams from grains and
deliver updates and this low latency
interactive scenarios so that's what it
was built for there is a lot of
complexity on the hood to make this work
so this sort of fooling agent so you
need to distribute work if you run a
cluster of a hundred nodes each no it
needs to pull from queues if you're
using a k'vin hub or Azure queues and if
machines go up and down you see but it's
work you need to cache this to be
efficient so there's a lot of complexity
there that you deal with that and again
that complexity is done primarily by the
runtime so the application code can stay
simple but the performance will still be
powerful and robust and dealing with
failures and redistribution
automatically for that will leverage a
bunch of other orleans capabilities it's
running smoothly namely tuned everybody
knows when tomorrow comes
no.you written x i'm gonna drink beer
with myself so we when we built years
ago we built a first couple of
applications on earliness and we started
talking internally to proto groups and
saying look this thing seems to work but
there's usually like people in our users
just skeptical they'll look at it and
say it's
simple is too good to be true if it's
that simple there's probably like a lot
of things that cannot do there was like
a lot of disbelief until this guy came
what happened the hoops Omar and those
guys they came to us and say look we
build we designed this architecture for
a future services but then we discover
it or leans new paper and looks like you
implemented 80% of it and much better
and deeper than we thought we would so
why don't we join forces and work on the
remaining piece oh and by the way we
need to be in production in three months
and this is where like in the cordon
video is real true story I turned to my
team and said once they left the room
these guys are crazy if you want to take
technology from Microsoft Research input
in production three months I don't know
what they're thinking but let's drop
everything and help them be successful
and we put this service in production
the first service for thinking was Halo
Reach University Edition in three months
and worked fine we work out a couple of
bugs after launch but nothing broke
experience and they decided that this
far exceeded our expectations we're
gonna standardize in Orleans for the
next major release which was Halo 4 and
so Gila for all of these services for it
were billing with their liens
within six seven months with very small
team so it was very productive
successful launch high scale all of that
to prove that work so that removed very
much all concerns the release is a toy
it's too simple people were saying if it
works with Halo it must work for me
and because I'm smaller scale and then
we had this other gamers came anybody
played Age of Empires halos Castle siege
so he the backend runs on Orleans and of
course in the fall we had Halo 5 release
which was very smooth we were asked to
be on call for the weekend and on Friday
we said we're told
nobody needs to come it runs smoothly I
think it's good
and then came kind of non-gamers so we
have a couple of services reveal for
Skype have several services in in Azure
monitoring and security does fancy IOT
project were launching this
device into stratospheric 40 kilometres
and very high this application that you
have on windows or in Windows Phone if
anybody has a Windows Phone still they
may not look as sexy but they all have
like hundreds of millions of users
behind them so it's still a lot of scale
of data to deliver another game user war
it's going to be released this fall it's
also using the same back-end
we never designed our links for gamers
which is like paradoxical people keep
asking oh you build for Halo no we
didn't tell it we didn't even have in
mind they came to us when we already had
the system but I think why why gamers
come first it's like typical in our
industry because they have a very
different environment they're always on
the bleeding edge they're always under a
lot of pressure they always rewrite a
lot of code for the next release and
they're low is unforgiving so they have
this spike on the first few hours few
days of launch which is very different
from any other service whatever is you
hear about snapchats and whatnot they
have the user base growing over time so
they have time to fix things up things
don't scale if performance rubs down
they have time months and years to
improve and even react I checked if a
major game is released and it has a
problem if you first few hours or a few
days you lost the business and this this
users will just trash and it's just
unforgivable
so it's a very risky business and also
the economics is shifting to these
business of selling DVDs were Best Buy
and other retailers it's slightly going
away so they moving more towards virtual
goods virtual currency content deliver
through cloud a lot of logic goes to the
cloud so they need to be in the cloud to
stay in business they compatible they're
competitive they're good customers to
work with because they're very fast very
ambitious named attune
great so when you talk to on read
analysts they talk about quadrants magic
quadrants oh I thought why can't they
have Serge's and magic quadrants and
define my own yes it's also a queen so
the interactive entertainment goes
beyond gaming you have interactive TV
other similar types of applications when
you have sub-second ladies and
requirements and you have high skill you
need to deliver things tailor to
specific user and analyze things on the
fly
which kind of bleeds into near real-time
analytics has a different angle but
similar requirements of getting data and
quickly making decisions and then find
enough to look at fraud detection for
detection for credit cards is not the
different actually from Qi detection
games very similar approaches iut is the
hardest area so that's why I put us I
think subconsciously as read and we have
this project I'm most proud of that
people build already these thermostats
for Honeywell RINO no liens or the
project were literally build a system to
control up to 2 million of mouse traps
because the company services are the
businesses with mousetraps and they need
to know when they need to go and come
when the mouse is there so it's funny IT
project and the other one is this green
power storage facility in Hawaii on Oahu
which soars up to half a gigawatt of
power which some some people wrote it's
like a small nuclear power plant but
it's just storage for wind turbines and
solar panels but there are many more
things are possible the showed with this
patterns and I show you just the glimpse
of it but it's much more that it's
possible you can build all kinds of
scale out computer applications with
Buddhist primitives we open source
release in January 2015 the experience
they exceeded far exceeded all our
expectations so it's very different
experiences
thank you it's yes it is from sting full
of somebody set them free it's just a
great experience of dealing with all
these people out there that collectively
are much smarter than you are so you
have to be very humble once you go
through the experience because you can
never be as smart as all of them and and
they are all passionate they come there
because they want you contribute not
because somebody asked them to and that
helped hiring I had no problem hiring
five people just last couple of months
because they say look you'll be paid to
work an open source project and you'll
be building your github profile for your
future employees the best deal I think
in town that worked that also will help
move our liens the Corps sealer and make
it cross-platform because there are
people that want to do this work with us
so we don't have to do all the work we
have to coordinated with the community
below work can be done by the community
itself one important thing about the
release is that it runs everywhere like
we don't want to it's not locked into
agile is this misconception that the
release is for Azure no it's not you
know in anywhere you can run it in your
closet and your garage on some hardware
that you purchased off eBay you're gonna
need a little yes some people do that
you can run anywhere and it's not tied
to anything so this can flexible
configuration and provider models you're
not constrained by words wrong and
usually Microsoft is viewed as fast
follower and a lot of technology I'm
proud to say that in this case JVM
people were fast followers so there's
this orbit JVM clone of Arlen's and they
told us very explicitly they wrote about
Arlene's the hardwood and they got blown
away by the model but because they were
GBM shop there by where is one of the
Electronic Arts companies just
implemented the same model in JVM and
they like it and Roger Johansson is
somewhere here he's trying to do
something similar in go we moved out of
research great
thank you people know it we moved out of
research what year-and-a-half ago but we
continued to product group but we
continued working with the research it's
just a couple of projects that we been
recently one is do distribution so all
pictures I was showing there are about a
single cluster of the Rhine Orleans
service cluster of machines so we went
further from a single node single
cluster multi clusters instead of one
cluster Iran is kind of a constellation
of clusters and you can do distribute
them you can put them in different
geographies for locality but also for
availability so one of them goes down
the model stays the same so you program
again against this grades are always
available the fact that one datacenter
went down that shouldn't be your concern
application logic the code should work
and the grain will be reactivated
somewhere else in a different geography
if needed but he also can serve your
your local customers from the nearest
datacenter automatically the famous
field Burstyn who co-invented acid
transactions he's working on adding acid
cross grain transactions to your
aleene's and they're very far enough
project and have some very promising
numbers we have other optimization paper
with neurosis this year in London a few
months ago published in optimizations
but looking why I think why this model
works I would say that there are just a
couple things that need to consider so
one is this contextual orientation
because you have at least model works
when they have lots and lots of
independent context that's like users
sessions devices if you have this kind
of application requirements this is
where the model works if you want to
build distribute database I would
advocate against user and release for it
when you have lots of rows and you need
to write operation it goes across and
that will not be efficient in Orleans
but when you have this independent
context that's easy to scale them out
it's easy to express the logic in this
isolated manner of actors but also I
would argue that this approach brings
object-oriented view back and I am
arguing that it's more natural the world
is not service-oriented I use it example
when saying in African savanna and when
the lion is talking to you gazelle
through his clothes and teeth he's not
talking to you Cazale service idx these
two actors interact in the
penalty from other lions and other
gazelles in the Savannah so that's the
reality of natural world where things
are not service-oriented they're object
oriented distance oriented in this model
fees as well in the people we have a
graph that shows that we scale linearly
and actually numbers now are 50 percent
higher but that's a graph from the paper
so if we get back to this business
requirements picture of time to market
return on investment I would argue that
more or less we we hit first three
requirements so I hope I demonstrated
develop productivity linear scalability
you can find details in the paper but I
also didn't touch a lot on high
efficiency our links code is very
efficient that's why we build our own
serialization layer one of the reasons
so and people measured against some
competition and found that's forgot
twenty three or twenty six times faster
than something down there so we don't we
didn't sacrifice efficiency for
simplicity that's why I think we begins
all the world problem but I think we
give enough tools to address a class of
applications in a very easy and very
powerful way that's my my claim to you
and I would encourage you as a takeaway
to take a look at our lanes take a look
at open source if you've never done that
if you're a JVM look at orbit even if
you cannot apply it is kind of people
technologies maybe the approach will
resonate later in in your work and when
you're build your system kind of learn
from our experience from our mistakes
but also learn that questioning
established wisdom sometimes pays off so
you don't have to have supervision chase
I would argue so that's all I have for
you thank you and if you have any
questions I can answer now or later
the question is what the relations with
service fabric actors and other related
differences yeah so first of all the
service fabric the whole name conveys
that service fabric is about service
model about service about distributing
running services managing services
that's the primary reason for a service
fabric so it's release is about
implementing services so yes service
fabric includes some libraries that got
a common probably models but more like
libraries and one of them is act
oriented but it even though the EPI is a
very similar act like the simple API is
in reality and implementation is very
different because it's built to
highlight are the features of service
fabric for example replicated local
attached storage well in Orleans it's
all remote partitioning story is
different the placement is different
there is a lot of differences there so
that that's why I would suggest you look
at those differences and see which case
works for you
insight in why the service fabric team
chose to
the question is my insights into why
service fabric team decides to build
reliable actors like like I said so the
reliable actors highlight features in
his service fabric that are specific to
service fabric work for example is
replicated storage and in general in
memory replication so you have these
features you have to to leverage these
features you need to write service code
and they're different like they have
stateless services and stateful services
they just added this third model that
actually can leverage these features in
different way so I think that that's the
biggest reason to kinda showcase the
features of the underlying
infrastructure any other questions
run this on premise
so the question about hosting your liens
that can run it on premises on a single
machine in the cloud the answer is yes
to all of this so yes you can run it on
a single machine especially like a
developer experience with f5 debugging
is very easy because you run two nodes
within app domains of the same process
where your client runs so that makes it
very easy to debug and develop so you
can deploy a single machine because it's
just the process you start at and the
configuration you give it you can run on
premises in fact our nightly tests there
are performance measures and reliability
as they run on the private cluster have
some hardware that we inherited for some
reason it's no problem because it's
really about storing membership
information which we recommend to use
Azure table anyway because it's very
cheap we will write just a few lines
there
you'll pay pennies a month even if you
run you can do it even if you run
on-premises that's that's how we run our
tests so we run on private cluster but
we store membership information in Azure
table so then moving this code to say a
worker role or to scale that VM scale
set is very very easy because the whole
mechanism stays the same any other
questions
No
about the messaging between the actress
so instead of questions about thank you
so instead of questions about the
messaging and delivery guarantees the
messaging between actors is over TCP
between two nodes where those grains run
or single note if they're together the
guarantee is at least once but but we
don't we have to retry logic and it's
there you can enable it but we turn it
off by default because in case of
failures when they keep retrying and
deliver messages that can get there you
just exacerbate the problem so we
usually don't recommend to imply a
retrial logic but there is also the one
thing I didn't mention there is a
built-in timeout so when you make a call
to a grain internally it timer starts
and when there is no response within the
set period of time then you get a
timeout exception so either your message
get delivered or you get a timeout
exception that's the typical case so
when there is no failure you get a
response or maybe an exception just fine
but to retry we recommend living into
the application logic case and in many
cases you don't want to retry you want
to do it once and if it failed it's too
late to retry for example so it's in
memory it's not Q does not persisted
unless you use streams so streams can go
work with system storage but messages
general messages then method calls go
over TCP does the answer question
comparison with persistent queues I
think it's the throughput question so
all persistent queues they they have
limits on throughput and latency or both
right so this this model is the most
performant because you don't write to
any storage you just send it directly
but if you need to if you need to
guarantees then you can use streams and
go or persistent Q's as easily so that's
sort of one of the trade-offs you need
to decide on early on but you can change
it
excellent question about handlers that
are attached to extreme only subscribe
to see what happens with his handler if
Green goes out of memories persistent or
not it is not persisted primarily
because we couldn't serialize delegates
and we couldn't do the magic work with
one at first but it was not possible to
do so the typical pattern is there's
this method on activated sync which is
like a constructor of a grain what gets
called when grain is activated so this
is where you put the logic to resub
reattach your handler so when you grain
when message arrives and the grain is
not a memory and get activated the
method gets called you attach a handler
and then the manner the event gets
delivered
that's yeah you need to persist that you
subscribe to the stream and then retest
your handler you have to do it
unfortunately any other questions
well thank you then</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>