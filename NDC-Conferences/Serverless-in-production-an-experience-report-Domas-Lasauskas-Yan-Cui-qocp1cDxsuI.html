<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Serverless in production, an experience report - Domas Lasauskas &amp; Yan Cui | Coder Coacher - Coaching Coders</title><meta content="Serverless in production, an experience report - Domas Lasauskas &amp; Yan Cui - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Serverless in production, an experience report - Domas Lasauskas &amp; Yan Cui</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qocp1cDxsuI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">test test okay Mikey
all for coming here surprised how many
year I'm missing how it does get stolen
each of seven so thank you very much
very much appreciated I guess before we
start I just wanna quickly ask at the
room how many of you guys are actually
working with some form of functions and
service aren't your functions or a dress
okay
quite a few hands great and how many of
you are working with a TAS in that case
okay
quite a few more people coming in so for
those of you who are working with a
dress good news is that they actually
just announced yesterday support for
Darnell code 2.0 I see some fist bump to
the front and also support for golang as
well so if anyone wants to use try out a
double cylinder with calling and don't
know quota per node now it's a good
chance to do that it's gonna be for the
guys to set it down alright so my name
is yen and I've been using Amazon Web
Services since about 2009 and in that
time I've worked for quite a few
different companies and took on a few
different type of roads right now I'm
working for a company called space 8
games I can see from a t-shirt we make
mobile games for living and
unfortunately I can't talk about
anything I'm working on because it's so
heavily nd8 if I tell you I'm gonna kill
you afterwards but what we can talk
about is some of the things that I did
in my previous company which is a social
networking startup called yopo we
actually worked my friend and former
colleague Thomas here so my name is
Doris and I started using it'll be us in
2012 when I joined game sis to work with
this guy and then he poached me to come
over to the startup double the company
that we're gonna be presenting our
experience report so you can see our
romance goes a long way so right tell
you about yep oh so at a time we are
trying to build a new social network for
your twenty-something years old and it
was a mixture of Instagram Twitter
Facebook and it was when we were working
at yeah both that we learned a lot about
working with it was lambda and how to
operate it in production yes she became
a very
part of architecture which when I took
over at the yabo in April 2016 we had a
very simple on paper at least
architecture at a time it was all
monolithic there's a few services
running on ec2 servers and as the early
days early saw social network we had a
pretty low average traffic that's not
many people using it but we did have a
couple of power users with I think the
most probably user we had had about
50,000 followers so whenever one of
those guys does something one campaign
like I run a I'm gonna give away some
designer handbags so anyone who votes in
my post by 10 and Croft and I so
whenever they do something like that we
see a massive uptake on traffic
sometimes from one minute or next you
have a 70s Society so 70 to 100 the
times jump in traffic and let me face
one on ec2 so scaling was pretty slow
and so we have to run at a much lower
utilization that we like to leave some
room for some of those massive spikes we
get and also we have to scale up a lot
earlier again because all these sudden
spike in traffic at the time we all sold
well didn't do a very good well we
didn't have a very good infrastructure
in place for doing deployments and the
department was taking up to 30 minutes
and required downtime so you put all
these things together it means we're
both a paying a lot of money for
resources that we are not using and B we
are taken down we are we are losing
availability whenever we need to do an
update so that's not great especially by
today's standard and what my favorite
speakers that don't know if once said
that the lead time to someone saying
thank you is the only reputation of
metric that matters to us as engineers
but together thank you
you have the first deliver some value to
someone and to do that we have the ship
our software until our Co is ship them
running in production as far as users
are concerned we haven't done anything
at all
so with Joran team we see that
architecture in place is not great we
have to do better but before we can hope
to do that we have we have to first
understand and figure out an agreed on
what good actually looks like forwards
in this case and we sat down the team
and we came up with a list of criterias
that we think that we're gonna need for
my architecture firstly we want the
problem to be small
to be fast and we obviously do not want
to have any downtime at all and also we
don't want to be in a situation where we
have to lock step deployment with the
client team which just the sheer amount
of coordination it's just involved it's
just no worthwhile and we also want
features to be independently deployable
and to be loosely coupled through
messages so that different components of
a system can move at their own pace
and from the architecture side of things
we want to cut out all the fat we had in
our a the best bill and we also want to
minimize the amount of time that we
spend just babysitting the
infrastructure as far as I'm concerned
the architecture should work for us not
the other way around it shouldn't be a
constant tax on the team to make sure
that it's working and making sure that
oh yeah is skating up right on time and
fast enough or that should be taken care
of by the platform itself
and the time we also had a lot of
technical mess to deal with
and that's a very conscious choice of
wording because if we talk about
technical debt in prices someone has at
some point have to had to made a design
decision to take out some debts with a
plan to pay back later what we found a
couple was not that in fact it was a
crime and see if I was a go I can spend
whole hours talking about how bad it was
but that's not what you hit here and
once we fix all this technical mess we
also don't want to just take six months
out and clean out the co-pays we want to
keep shipping features keep delivering
values to our customers in fact we
wanted to be faster than we've ever done
and if you fast forward just a few
months this is the architecture that we
arrived at which is both event-driven as
well as the service oriented where
lambda is a centerpiece that glues
everything together so at this point we
had about a hundred seventy lambda
running in production with many more
diverse student development and we were
paying for lambda invocations about 5%
for what we pay for
ec2 for comparable amount of compute
resources that were using but perhaps
more importantly from the cost saving is
how quickly we're not able to ship
features and ship code to our users and
going from doing production release
maybe four to six times a month
in April by October we were easily
shipping code to production somewhere
close to about hundred times a month so
that's a big increase in terms of the
output of a team without actually
changing the massive increase in the
size of the team and from the moment we
decide that lambda is a good fit for the
direction that we want to move towards
and to having the first function run in
production we had to as professionals
and we have to be responsible we can't
just say hey this type of skill let's
just put into production without you
knowing how we can operate it so to do
that we had answered a bunch of
questions around how you're going to
test your functions when they're all
host in the cloud and what about your CR
CD pipeline and how do you not know to
log in monitoring and alerting and so on
the thing to remember is that a spray
attack as lambda is or any form of
service technologies they are just new
tools and typically when you change the
tools that you build software with often
the practices and patterns that emerge
with the previous generation tools
they'll have to change or at least
evolve and be adapted but the principles
that underlines the emergence of those
to those practices they still very much
apply you still need observer
observability another hot key word in
2018 on your architecture to understand
what has going on to be able to operated
with responsibly in production another
there's also add a bunch of other
principles as you still follow
principles around a high cohesion and
low coupling and single responsibility
and store and so forth and Palatine
architecture expanded to include more
and more services often one service will
depend on another one you have to answer
a bunch of other questions around
teachability tracing how you gonna
manage configurations for hundreds and
thousands of functions running
independently in production and how I'm
going to keep all of the information
that you have secure as well and for now
I'm gonna
awkwardly pass these to my so to start
off I think it's good to give some
concrete examples what we actually built
what we achieved and the first thing we
tackled was search because when you have
a social app you want to find somebody
to follow so
to get that you want to search by first
name by last name by username and what
we actually found in the legacy monolith
it was search implemented using regex
that was hitting MongoDB and you can
imagine that performance was not really
great it was actually struggling at the
hundred thousand user mark what
currently like in this day for a social
app it's nothing you want to have at
least maybe handling millions of users
easily and at that point for us it was
difficult to do any kind of random entry
ranking system by popularity or maybe
drip trending user should be on top so
we started off to build something from
scratch simple search so what we did we
looked at the monolith without many
modifications we just allowed it to
emits state change events so that means
whenever user new users credits or
somebody changes a name or updates the
profile we want to raise an event then
we had all of those events are being
published to can use a stream and we had
a lambda that would process them and
updates index in amazon cloudsearch then
we could take api gateway and lambda put
it on top and easily make a new search
api that words curry club search but at
that point we did not want to impact
existing applications and we did not
want to wait until our client devs are
ready to start using the new search and
actually even release the app and then
users needs to update it so instead of
that what we did we put we just proxied
legacy endpoint to actually hit start
hitting our new search api so in this
way we delivered value early on without
waiting for app release also we had no
analytics at that point so to do that we
also had to build everything from
scratch and we did that over many
iterations but for the first iterations
we followed similar pattern where we
updated our legacy system to start
emitting detailed events about all the
actions that
users are doing in the app all of those
events are going through Kinesis streams
and being handled by a by lambda and
then lambda is live streaming all of
those events to Google bigquery Vickery
is amazing for large amounts of data and
this is something we use previously in
the Games's where we work before with
the answer we knew that we can it's a
really good fit and actually to get this
first iteration out one developer it was
his first um first um he had to deal
with lambdas this is what's his first
piece he had to implements using lambda
and actually from design to production
it took only on two days to get that and
our bi guy came to us when we said that
it's done and he tried it out and he was
amazed he said nothing ever got this
done this fast in Skype and where he
just came from and of course the North
fence to Skype but it just shows that
that that leads on to someone saying
thank you that's the only metric you
should you really need to care about so
we got a bit more comfortable with using
lambdas shipping them to production and
so on so the next thing we tackled a was
it feet as you can imagine like an app
you open and you see all the posts from
people you follow so similar like what
you haven't Facebook or in Twitter so
the legacy implementation was really
complicated it was just a mess to
untangle and actually they when they
were implementing legacy they've written
a huge spec so there was a document
listing every single use case that he
should support and then when QA team
picked it up and tried testing it every
single one of them failed so they just
declared it untestable and actually
refused to test feeds at all and to top
it off at that point
new CTO came in so this all happens in
the legacy still world so when new CTO
came in he looked at the mess that's
current system is he actually fired the
whole team that was involved in creating
such a mess and so when we came in we
had nobody's actually asked how it
should behave or
only why the hell they they built it
that way so we had to build it from
scratch we sat down with a product team
to figure out what our actual
requirements are and we did in a similar
fashion where everything is based off of
events so legacy system is publishing
events such as unfollowing somebody or
somebody unfollowing posts on you it
creates a new post so I wanted that post
to appear in my feet so all of those
events were being handled by lambda
where lambda sees that okay new post
just was published and needs to get the
list of everybody where so everybody's
feed where it needs to go so this
operation is quite costly so what we did
we got all of them the feeds where it
needs to go all the destinations we bash
them up by a hundred or a thousand it
doesn't really matter and then published
via SNS so then we had another lambda
listening to that SMS SNS feeds so
updates are sorted set in Redis and at
this point we're getting retries between
SNS and lambda so we are we trying at
this point but so we don't have to retry
from the start the costly operation of
actually getting the list of the
followers because somebody you might
have thousands of followers and that
operation retrieving that list might be
quite costly right so we are using Redis
so have a soil set and then we're just
building an API on in front of it where
we just have API gateway and lambda the
same person as you seen before and then
we updating legacy system to proxy the
old endpoint to actually hit the new API
so at that point we also wanted to
tackle a bit more complex situations
which is a recommendation system so it's
what we found in the legacy system was
anything unlike Facebook or Twitter it
was actually returning you the first 30
people from the database by account
great
date so that means it was all gamble
employees that started off and created
accounts first so it might be great for
them that everybody is following them
but actually if as a customer like if
you open an app and you want to find
somebody to follow it's useless
so we started off simple we already had
old analytics in bigquery then we had
just a simple cron job that kicks off
every hour or so and so it's a clutch
event the scheduled events that just
works as a cron job with triggers a
lambda lambda hits Google bigquery
Vickery is a good four day for this case
because it can process huge amounts of
data in a couple seconds or tens of
seconds so then we by using a simple
time decay function we can figure out
who is trending at this point and then
update the trend trending users list in
DynamoDB and then we built API on top of
it by using API gateway and lambda to
just query to get the list from the
track of the trending users from
dynamodb at that point we built the
other way because it's good to know
who's trending but also you wanna know
who your friends are following because
if I'm following somebody I'm probably
gonna be interested to follow the same
people that that person is following so
we listen to the events being published
by legacy the system here for any
relationship changes so if you unfollow
somebody or follow somebody knew we had
a lambda to process those events and
abate updates graph in DB updates your
social graph who following graph in DB
is just a hosted version of in ear for
Jane then we build an API on top with
API to n-lambda and to top it off we
proxied that legacy system to hit both
these new api's for the recommendations
from the your relationship graph and
also to get the trending users and now
the old apps can still work as they were
with no
Changez and to achieve this the first
bit bigquery I think yan took like one
night or so to implement the lower part
of this and maybe another couple days to
do the the whole social graph thing so
it just showed how quickly we could get
a feature from design to production
really quickly and these are just a
couple examples what we actually
achieved there is plenty more there is a
lot of post on ganz blog that you can to
the fun part what did you when you're
actually getting to production Thank You
Thomas so I guess the number one thing I
would say about in terms of getting
ready for production
is don't try to reinvent the deployment
framework yourself I've done it enough
up in the past that when none of the
frameworks are quite ready yet and one
of things you really quickly learn is
that there's so many there's so many
nothing bows you need to understand of
the underlying infrastructure and the
getting right is really hard and all the
frameworks that you see available on the
market today
they've all gone through that learning
process making mistakes fixing it and
the one that we use is called a service
framework and when a reason why we chose
it is that is this is backed by a
financial company there's a even at a
time when we adopted it in the twenty
sixteen that was a lot of momentum in
the community but also at that at this
point in time they are they've gone
through the first version where they
made a lot of mistakes and they've heard
feedback from the community and now that
version 1 and version 2 is not far from
the in the horizon but since then amazon
also you know got himself into the game
with the same framework as well and
between server listed Sam these are the
two biggest deployment frameworks for
the is for in EPS lambda the one thing
also one thing to keep in mind the
service framework supports other
providers as well so he supports after
the parks run plug it well after the box
your functions google cloud functions
for insta
open bisque and the more available and
also added constantly by the community
by is very extensive and also very
flexible framework a plugin structure
but then also there's a patters up
klaudia zappers sparta
so symptomatic
of any new and hype technologies this
new framework just about every single
week and the finger saw in the one last
last week called architecting code or
something
I think the point I'm here to make is
that there's going to be a lot of new
frameworks and what you rather than
getting distracted we have all these new
frameworks to try the mouse trying to
try a few of them pick one that works
best for you and your team and just mend
data on the team you don't want to waste
so much time just no jumping between
different frameworks and also you want
to minimize you want to maximize the
amount of knowledge knowledge sharing
within your team at the same time
minimize the friction that you're gonna
cause when people move from one project
to another when I go to a different
project that maybe Thomas is working on
I want to focus and spend my time
learning the new domain the new copays
rather than working out ok so I was
using servers to do my diploma in the
previous frame were in the previous
project now they are using something
else how do I deploy my code those
friendship become institution knowledge
in your team you don't want to be some
instant or wedding wasting people's time
learning how to deploy your code every
time they move to new project but once
you start working with lambda you could
have figured out how do you test your
code and as far as testing goes this is
my favorite book by net price and Steve
Freeman and you talked about different
levels to testing no you first you've
got your unit test where your test your
code at the object or module level and
nothing's changed here ultimately
because some business logic encapsulated
into a class or module you can still
write code to test them the same way I
say you have always done and then you've
got the education test where you test
your code against code that you can't
change and since lambda ultimately is
just some functions some business logic
there are machines calling on your
behalf when some event happens there's
nothing that stops you from exercising
the same code by calling your function
locally we have a stopped event and
context object that way you can still
exercise the same logic locally as you
would do when it's executed in the cloud
the thing to keep in mind is that the
purpose of integration tests is exercise
your code against Cole you can't change
so when you run in your integration test
it's important to set them out even when
running them locally
to hit the real downshift systems that
you're talking to
it's nice don't believe they shouldn't
be using marks and stuffs for this
contest otherwise you can't be learning
my code works fine against my stops and
the moment I deployed production
BAM because the thing that I'm stopping
doesn't behave exactly the way that my
stops do
and once you coach me deployed and
running in the inside address
environment you can then run acceptance
tests where you test whether or not the
whole system works and to end and as you
go up this this is a triangle of testing
the unit tests give you much faster
feedback loop but you get a far stronger
sense of confidence with your certain
tests that what you've got here is
actually couldn't do what you're
supposed to what is deploy into
production and one of the things that
one of the key things I learned from
this book and I'm gonna read it because
this column is that we find that tests
that mark is Turner libraries often need
to be complex to get a car into the
right state for the functionality that
we need to exercise the mass of this
test is telling us that the design is
not right but instead of fixing the
problem by improving the code we have to
carry the extra complexity in both the
code and the tests and the second risk
is that we have to be sure the behavior
that we stopped or mark matches what the
external library will actually take and
even we can do right once we have to
make sure that the tests remain valid as
we upgrade the libraries now the same
principles of things to apply in this
increasingly service-oriented world that
we live in that you really shouldn't be
mocking services that you don't control
think about how you the second risk that
the nan and Steve talked about that you
can't you have make sure that your marks
is continued to work when you upgrade
the library now when Amazon updates one
of the services you are no you can solve
when they do that but you still gotta be
make sure that your stubs and your mark
still matches the exact of what the news
of the new version of a service is gonna
do but I think fundamentally we have to
rethink how we test software when it
comes to service technologies are lambda
because I think I found before because I
think that the risk profile of what can
go wrong has fundamentally changed if
you focus on testing was inside your
function
with your unit test and just mocking and
stubbing your dependencies you're
missing out most of the things that can
actually go wrong when your curve runs
in production for me personally the
first thing that often goes wrong is I
am permissions I created a new function
is using a new table and you always find
when I'm using mocks and stubs the
moment are running and deploy it and run
it BAM
okay might my function can't talk to the
time TV table because it is mixed is
missing the permissions or this whole
bunch of orchestration involved in terms
of the creation the maintenance of those
resources my functions using the
configuration of upstream event sources
and so on and so forth and this risk of
shipping broken software due to miss
configuration is is vastly magnified
when you have a tune I manage the
configuration for each every single one
of those hundreds and thousands of
functions that you can have running in
production and smart observation that
most of your lambda functions themselves
are actually now very very simple if you
apply the learnings we have done with a
single responsibility the individual
functions themselves are also are
normally very very simple the risk of
you actually shipping some broken code
has now largely shifted to how your
function interacts with external
services and its downstream dependencies
and this whole purpose of us of writing
and writing tests or sorry the whole
purpose of us of writing code in the
first place is so that we can ship
working software at the end of the whole
cycle and that's the goal that we should
ultimately optimized towards or in terms
of how we work so as far as I'm
concerned it's okay for me to looser my
feedback loop speed even though are
lovely as a developer but ultimately if
I'm learning the wrong things faster
with the fastest feedback loop that's
not going to help me ship working
software faster to my users and recently
in the last six months you also start to
see some of these the local I guess
environment we fade abrasive cost in the
same local
there's also some too from jetbrains as
as well I think those tools are great
and they've definitely very helpful in
certain stages of your development cycle
but you shouldn't use them in place or
instead
of your engine test you should still be
writing end-to-end tests and making sure
that your code is running it's going to
work the way you expect it to you and to
end and that means you have to rely on a
lot of external services during your
testing and I mean that's okay
as Paul points out the so many services
for you to choose if one doesn't work
quite well for you just pick another one
just hundreds of them now there nowadays
as for engine testing Steve and that
also talks about how we ever possible
and acceptance says should exercise the
system and to end without calling
directly his internal code and entrant
tests interact with the system only from
the outside through his interfaces and
coming back to the exam the search api
example the Domus gave earlier to test
the system we will interact with the
legacy system by its HTTP interface the
same way that our mobile client would do
to create new users and the test case
would be to validate that within a
reasonable amount of time those new
users are then can be searched by our
first name last name and username by
talking to the Search API directly
another thing we notice is that when
you're running your integration test
you're testing your code locally by
hitting the same view down streams
involve databases and message queues and
so on but when you test your system with
acceptance test you're talking to set
two into the same code but instead
you're talking to them via HTTP
endpoints or SNS messages and so on so
the only difference between these two is
how your code is being invoked right so
when we write in the test we can
actually use the test cases by defining
the one step so when we invoke some
important the get end point so inside
the definition of that one step we can
have a taco which can be passing as
environment variables or any other means
that you can you want to use instead and
where in this case if it says is handler
then we will call the handler function
directly with a stop event and context
but if it's a something else then we
call the HTTP endpoint as be configured
as part of our test a test setup as well
and you can see how this works you can
try out yourself as well and I've got a
kidnap repo where you can actually see
how this can be configured for your own
use case and this is what we get to see
I NCD because this is something that
there was always temptation to leave it
at the very end to automate your
deployments automate your tests and so
on because it takes time but actually
the earlier you consider CIN CD the more
time you're going to save in the long
term especially because that's going to
avoid all the human errors there you can
have otherwise and especially because
there are tools like service you have no
excuse not to use it or service makes it
so simple to deploy just single command
that you run on command line done other
things you need to think about how
you're testing so coming back to the
same book that we really like is how
Steve says that we prefer to have
end-to-end tests exercising both system
and the processes by which is being
built and deployed this sounds like a
lot of efforts because of course it is
but has to be done anyway because during
the deployed development or your system
so no excuse not to do it and another
pet peeve of yarn is that he's always
annoyed when configuration in your team
city or Jenkins or so and it's not
versions so when something breaks you're
not gonna know who changed it you don't
know why it changed it and more
importantly you don't know what it was
before so you can't revert it back it
just wastes time and moreover it should
be repeatable both on the CI box and
your local machine so what we had in
yellow we were using Jenkins so the main
principle was we wanted to have it might
be difficult to read but the command so
unit tests to run unit tests and
integration test was just one single
command with bash script with a couple
parameters the same way to do a
deployment the same built bash script
with a deploy parameter to run
acceptance and so on so now the whole
configuration can be run both on your
local machine when you looking while
your deployments are failing you don't
need to have a really slow cycle
you change something and then you need
to wait for Jenkins box to actually kick
off the job and build it you can just do
that all locally so what we had we had
as you can see like what a small the
bash script but importantly if
especially if you're using serverless a
thing to keep in mind is if you look
there where the note modules is listed
we're using local not modules cause in
not Jes you can install packages either
globally or locally as part of your
project so all we suggest do that
locally so now the build process is
independent of what its installed on
your builds your machine because now if
a service does a breaking change you're
not gonna break all your bills because
the bills are going to be using the
version that they depend on and that you
configure also I would actually
recommend to instead use Jenkins file
that could be versions and controlled
together with your project but if you do
that still follow the same principle the
build command should be just single line
that you can easily run it should not
have some a special logic about how you
run it and what you do you still want to
be able to do that just really simple
small command another thing we have to
consider is whether to do continuous
delivery or continues deployments the
only difference between those two is
basically how your code gets to
production whether it's automated or
it's manual in yellow we had that as
soon as you push to master we're gonna
build it we're gonna run all the
integration tests unit tests
deploy it run acceptance tests in the
dev environment in test environment and
staging but it's not gonna go to
production just yet because we wanted to
have that control about when it goes
live and what goes live just to be sure
and especially we had a rule to never
deploy on the fridays evening because
i'm guessing in many London offices you
might have a beer fridge and so on and
you don't want to be after couple bears
deploying to prod and then just going
home all happy so next we get into
logging because it's all good and fine
we deployed our systems to production
but now when the bug come it comes along
how do you diagnose where it got so with
lambda it's quite simple anything that
you print out to this
and outputs are gonna be shipped to the
watch logs and additional information is
being appended as well like the
timestamp when it happens and the ID for
example if you're using API gateway is
gonna be the request ID so having all
this if you're not familiar with how the
clock watch looks in the it'll yes a
console you have the cloud log group
that actually the name matches what your
lambda name is so there is one Law Group
for lambda and then each specific
version and execution of the LEI lambda
will create a new lock stream so but the
problem is that looks cloud which is not
really easily searchable or using
especially when you have hundreds of
lambdas running and you easily gonna get
or overloaded with all the logs and this
is a really common problem with micro
services so the solution for that is
centralized logging so for the
centralized logging is going to make
them easily searchable so one of the
industry standards is alec stack this is
what we chose to use it could be
anything it could be Splunk or so on all
of these principles are going to apply
so any of those systems so the good
thing about cloud which logs is that for
the cloud which log you can actually
stream all of those log entries to
either a Amazon Elastic search or you
can actually stream that so LDS lambda
so what we had is you have a lambda
they're gonna be subscribing to the
clock which logs and gonna stream all of
them so elke stack and you don't want to
have a manual process because you can do
that by logging in and clicking or you
hitting against the a api AWS AP ice but
it's gonna be painful what you can do
instead is enable clap trail and clout
trail what it gives you is any event
that happens on the system in AWS for
example creating a new resource updating
deleting somebody logs in and so on is
going to be published as an event so now
you can subscribe to it as any cloud
which event and you can have another
lambda that's
responsible for doing subscribing for
you automatically so whenever a new log
group is created it will subscribe that
log group with the lambda that will ship
all your logs to L stack so now you just
have implemented once small lambda and
it just works you can also filter it so
it only ships for example lambda log
groups you can have a filter about what
should be shipped but the thing keep in
mind is that when you're creating a new
law group the expiration is actually set
by default to never expire so it could
get costly in the long run so as part of
that when the log new log group is
created you can also update this you can
do many automation bits off of those
events and if you want to have a bit
more read and play with it yourself
there is a great blog post by Yann about
the centralized logging and and if you
just follow that link below and now
we're getting to distributed tracing so
so very good having already lost for all
of your functions but for many of your
features you have to rely on multiple
functions all working in perfect tandem
for you to work from the moments that
I'll create a post for example you have
to go through multiple functions and
multiple different event sources before
the data eventually gets to where it
needs to be and for someone to then
build a cordon API to fetch their
timeline so now what happens if
somewhere along the way is something
running wrong and one of your Kiwi guys
or one of their users raised a complaint
that hey my wife created post last night
I've really wanted to say my timeline
but it doesn't show up why is that -
trying to debug it is quite difficult
even though you've got all the different
logs for all the your functions but it's
not easy for you to string together
everything there happen for that
particular post so that you can see a
chronological view of everything that
has happened across the entire
architecture for that particular post
again this is common problem people find
in the micro services and a solution
there is to use correlation IDs which
can include any number of aliens could
be a user ID the person creating a post
it could be the ID for the posts that
have just been created and
so on the interesting thing to remember
here is that those those that the data
that's been created is moving around
your systems through HTTP core so you
can easy streams through SNS messages
that means all the collation IDs that
have the goal that those that data that
they the flow have to flow through all
of these different event sources as well
and for HTTP endpoints we know how to do
that we just pass them along as HTTP
headers but what about some of this
other event source is just where you
don't have obvious way to tag all these
context data around with the to quote
the data yourself and the way we
approach this was to create our own
libraries for HTTP by wrapping around
our favorite HTTP client as well as
wrapping around a domestique a crime for
Canisius for SNS and so on so that when
the purpose inside we can shove for
example for SNS we can shove those
contexts the collation IDs into message
attributes when we publish a message
into SNS for Canisius there's no obvious
places to put them so we include them as
part of the message to payload with
especially especially convention
underscore contacts or something like
that and for HTTP endpoints person
belong as HTTP headers and on the
receiving side we also have the
middleware tier in our code base where
we will extract those and then we set
them as a global context so that every
time from our code from our function we
write a lock message those to be
included as part of that lock message
which in this case would be for a JSON
JSON string so that when the log
shipping function picks it up you can
actually create at the lower context to
the log messages that you're sending to
your lastic search so that way someone
comes along with question hey so-and-so
is the user ID created post yesterday
didn't go to where his husband's or her
husband's field what's going on then you
can start looking for that user you can
search for our users user ID and maybe
in the post ID as well to see everything
to happen your system for that
particular post again in writing a lot
of stuff over on lambda so there's a
blog post that shows you how you can
actually do that including the demo code
for both the live with the client
libraries as well as the middleware that
you can do
and then ask better for this to be
chasing he's understand where the
execution time happens across your
function invocations and for that again
is a common problem people have in the
microservices and where's in the soft
server for Microsoft this world two
stars if Kim is kind of the standard
there with Amazon they also introduce
the x-ray a thing at two years ago we
factor eight you can see for one
function invocation with instrumentation
on your own your code you can see all
the different segments of how long time
how much time it took to say talk to
SNL's to talk to tournament DB to talk
to some internal API and so on and from
that data you can also see a service map
so that you can see all the different
services that your lambda function is
talking to as well as the average and
max execution time for those services
the actually it's alright it does it
works quite well for a certain set of
things it does but one of the big
problem I find right now is that it
API key witness a natively supported so
I'm gonna step out of the microphone
range have a char little bit but when I
call the function what happens that's it
API care support is coming I've been
told I don't know when that's typically
do so just wait for it
another thing API actually the support
is say dissing work with event sources
that causes async receive engine
vocations
so if you're sending something to SNS
that gets picked up by and the lambda
function you don't see that second
invocation the hack that happens as part
of your trace there's a number of tools
that are being worked on by people that
run off the bridge the gap that we have
in the tools that we have in this server
a server observability space mode I will
talk more about this some other time
and for now if you want to read a bit
more about APA
x-ray what know what's cool about it way
doesn't work wrote a blog post recently
as well
how are we doing on time or few okay so
moving on to more
one of the things that were used to
doing and again on the server for space
is to install monitoring agents and
demons to collect logs and select
collector metrics
unfortunately with lambda you can't do
that anymore you can't access the
underlying host operating system instead
you are more reliant on the information
that you get from cloud watch which
gives you the basic set of metrics that
you probably want anyway and then other
providers also study support lambda but
most of them just use the same set of
data you get from cloud watch but give
you a better looking dashboard in front
of it and in this space the guys IO pipe
they do something slightly different
it's quite interesting in that they give
us DK where you can have a factory
function essentially to grab your
function code around so that they can
integer and intercept your function
invocations and then send metrics to
their own back-end systems when your
function finishes either successfully or
with some exception and you can also get
additional data points around the memory
usage as well CPU usage which may or may
not be of interest to you but the
problem I found with this approach is
that because we've lambda everything has
to happen inside your function
invocation there's no background
processing anymore so where our pipe is
sending metrics to their own back-end
system you are paying for that
invocation time in fact not only you are
paying for it because API K we doesn't
respond to the caller until your
functions invocation finishes and once
that's happening some poor guys using an
app is actually waiting for you to
finish sending metrics to our pipe so
that's why main concern with the
approach that got our pipe research Pro
has has gone with because again we don't
have background posting anymore we can't
do all this ship function well shipping
logs and sending metrics in the
background there is however workaround
which I find quite quite interesting one
of the things you do get the background
processing for is the other things that
the platform gives you including
collecting logs from standard out and
shipping them to cloud wash logs so what
you could do and this is what data talks
does for custom metrics is record your
metrics as specially formatted log
messages so that you can then process
them asynchronously in the background
outside of your phone
location so that the same function that
shipping logs to elasticsearch or
wherever can also then send those
metrics to your metrics the monitoring
system instead it's not the right
approach for every scenario for four
user facing API so I think this is
something that is worth considering
because ultimately you don't want you
don't you you don't want your users to
be waiting for you to do this background
processing but four phases are involved
asynchronously from second easiest
events or SNS messages you can just put
it on care about actually ten
milliseconds of execution time just fire
all your metrics as you want again you
can try out you try yourself yourself oh
yeah we have custom demo code and so as
the blog post explains how this might
work they can try out yourself and once
you got all your metrics you do the same
things that you've probably known to do
already setting up dashboard setting up
alarms and also any sort of application
level metrics like the number of posts
people have downloads of view has
created also track those as well however
as much as no car was being a decent
fairly cheaper service use the monitor
or your lambda functions and also in
most cases it's the only way for you to
get metrics about Amazon's web services
so your talam deepest metrics your s3
and so on they're only available through
cloud watch however cloud which is
lacking on a number of different fronts
the UI is not great for anyone who's
ever used it and there's not as much
customization options as you find in
other services and also doesn't have
some more advanced features like
protections that you find with wafer on
and stack driver but a big reason for
not using car wash for me is that it's
good a tendency to go down the same time
as the thing that using it's a monitor
this happened to me a few times where
okay Amazon's having some networking
hiccup but it's - it's down or not
working her 2% so I know my services are
being interrupted but I have no idea how
bad it was because also cloud watch was
also down and sometimes when your
service comes back up the cloud watch is
not you have no idea okay is it working
is it now what do we do so that's not a
great place to be and
that's I'm gonna pass you back to Thomas
so now we're coming back to the
mentioned configuration management's
because now you have many lambdas
running in production and so on
so it's all good and fine but how do you
easily and quickly propagate
configuration changes to all of them
so initially you can just when you're
playing around you can just use a Lewis
console too for example deploying lambda
you can set environment variables we
chose this approach because a server
list does allow you to set in moment
rables in your service configuration for
your project when you're deploying it
but it makes it difficult to share
across all the lambdas because if you
have 50 lambdas using some config and
you want to change it now you have to go
in and update all of them and because
they are specified during deployment
time and function also it gives you no
way to have like what's the fine-grained
access so not everybody should have
access to it but if you're using an
armed rebels that are required during
deployment time now all of your
developers needs to have access to that
or else they have no ability to deploy
but you don't want to have somebody
leaving the company taking their codes
with them on the laptop and then having
all the secrets there so you want to
have some kind of a centralized
configuration service and it could be as
simple as some key value store so we had
a look at what's available out there we
had a look at console we had a look at
ET CD but the thing is you still need to
run instances for these it's gonna cost
you even though if you're not using it
and a to learn how to use to configure
and how to maintain it's quite steep so
in our case because we already knew how
to build lambdas and api's so we just
did our own small so there was no
infrastructure to run it was really poke
it quick and small but this was before
AWS announced that they have parameter
store now this is a great thing and you
should be using it it gives you a way to
encrypt the sensitive data in flight and
at rest and it also is
role-based access by I am rose so this
is a cheap wine so you have your admin
who is the only one for example in
organization who can update your secrets
so he can get access because of his I am
roles and then developers don't need to
access to change them at all because
they are all your secrets are being
sought in parameter store and then they
are encrypted at rest using kms and when
you are deployed your lambda when it
starts up now lambda now has read access
and you can specify that specific lambda
has only for the secrets that it needs
to access and not for everything in this
case so it's all good and fine but it's
quite a you still need for example how
do you cache how do you poll and so on
because you don't want to be hitting on
every request you don't want to be
hitting your configuration service so in
this case we build our own small library
for that that would be responsible for
fetching the information caching it so
it only being initialized during the
call start of your lambda so the next
time it's being called it already looks
up it has in its memory you can use it
and then it can it was also responsible
for invalidating and refreshing them at
some period it could be like couple
minutes or so and another thing
listening to signals what I mean by that
is that for example your lambda is
hitting some HTTP API so it has the URL
for it it has some secrets for it and
then if suddenly you start getting back
404 you could raise a signal and then
your clients for configuration clients
will invalidate that URL so the next
time it's being called it will try and
get the new value because if you got 404
probably it moved somewhere also if you
wanna see how it could be easily
implemented there are code examples and
everything how to use it on yawns block
as you can see I've been writing a lot
so I guess we've got 10 minutes if I'm
conscious of times I want to give you
guys some time to ask questions as well
so I'm gonna skip a few of these but one
of them is worth remembering and
mentioning is that Amazon lambdas got a
75 gig regional limit on all of your
package for your age with lambda that
you upload and so - and you may find 75
kids quite a lot but we found that
because we've lambda and Services
Department so easy and we were easily
deploy hundreds of functions or hundreds
of times a day each so even before to
make up package sizes very quickly we
will run into I think at one point after
about three months six people we were we
had about 20 gig of deployment packages
on lambda so pretty soon we could hit
there signify kick limit and so in that
at that point I took a leaf out of
necklaces their playbook they got the
janitor monkey that goes round and Q
resources that you don't use so I wrote
the janitor lambda function that would
run on a schedule and then clean up all
versions of your of the different
package for your functions that are no
longer reference and one of the things
you can do nowadays with version one of
the service framework is also to say
that when you do a deployment don't
always create new versions therefore you
don't create redundant packages for
every single version which in deployment
or insert in developing environments are
just not used for all you won't turn you
on for production
sure so that your core version they can
quickly go back to but not for the not
to native development another thing that
you can do nowadays since one point 16.0
version of a service framework is if you
install your service framework as a
dependency or anything as that
dependency they get excluded as part of
the package step where if you call if
using it was DK and you don't want to
use your own Val you don't want to
package your own version you can store
as that as a dependency and that'll be
excluded so your package size becomes
smaller and before the service framework
you can also invoke functions locally
using a invoke command and that means
you can also if you're using Python or
nodejs
it also means that you can hook it up
with
viciously do Co so they can actually
debug your code locally you can do the
same thing with the same framework as
well with Sam Evoque that's also a way
for you to just hook up your video code
so that you can debug your functions
locally
which is that okay and with co-star this
is probably that the pain of everyone
was using a nice form of service
technologies today and so co-star is
basically what happens when you function
what runs for the first time
Amazon's gonna have to load your code
have the starter container under the
hood yeah we saw a container under the
hood and so that time is what we can
ormally call Co start Amazon doesn't
talk specifically about what happens in
those in that co start but the open
which guys have talked quite openly
about the different stages of co-star
where is going to load up a container to
run your code and then it's going to
download your code and then it's going
to initialize the runtime so when you're
using different languages there's a
massive impact on how much co-star you
experience and I did a bunch of
experiment earlier to find out how the
language they use as well as a memory
setting you use affects your co-star
time and that one is C sharp with
running tonic or 1.0 at a point and this
is Java by comparison look how small no
js' and python is and that's one of the
things that really maybe me being a
tonal developer by trader and you
probably heard me know going on here
flapping about a few how great is I
still finger is my favorite language but
when you well human lambda
there's a massive penalty you have to
pay well not you but your customer have
to pay in sense of the experience you
get from your application which is why
when it comes to lambda I have really
embraced the use of dynamic languages
are no js' a Python and again
what about type safety well nowadays you
can use go so that give you some type
safety not the same ones you get with
c-sharp and the and I sure and you
probably heard me go around and DC
conferences telling people you should
use a shelter system to make invalid
state and represent Poe in your domain
welcome to you yeah well I think the
thing I found is that with a traditional
nodejs application you can make things
really really count
really easily the complexity ceiling is
enormous but when you have a really
constrained execution environment and
you have to write your lambda functions
and deployed an invoice more units the
tools that I really enjoy from our
sharpness of note I transfer influence
the different types I get I they are
there for me to help me manage
complexity but when you can really
constraint the complexity of the things
that your developers can develop we all
can build I find that as you didn't miss
all the things that love about F sharp
which to me is kind of painful to admit
at the same time and with that finger
with them you have for about five
minutes so I'm gonna skip to the end
where I I thank you for your time and
also give you a shameless plug about
cause that I've been working on
remaining we have talked about basically
all the things I've mentioned so far
including a bunch of other things around
canary deployment and other things are
instant role to be production already
running I ate up slam that introduction
and whilst in the early access we for
many you can also get 40% off as well so
shameless plug over now so time to ask
questions fingers of my crowing life no
question just raise your hand
oh is it too late five o'clock time to
go home one question just a quick
question
that's fine so are you doing any warm-up
for yourself to get around the cold
start issue or I just accepting the hit
on the first good question so the
question is do we do any warm-up for our
functions to get around the coastal
issue the question the answer is
sometimes
so for functions there are certain API
endpoints that are seldom me exercised
that means chances are every time
someone hit the endpoint they're gonna
hit a co-star for those we do have like
a ping function that paints them with a
special message so that a function
itself knows what committed here for the
function knows that okay this is a pain
I shouldn't try to execute the normal
user flow which most likely gonna fail
but for most other functions we even
though we still do that for all the API
sorry API functions nor for Canisius or
as an S and well now for those other
ones we find is less effective because
when you have concurrent execution for
the same function you have no easy way
to find out how many concurrent
execution czar happening so when you
trying to ping them you only get one of
them at one time so the other four can
still gradually be going into co-starred
and I'm with the lambda also got this
thing where a garbage collects the
function type in either for a while and
that hinder be away for five minutes or
whatever so for PC functions that are
constantly spawning new concurrent
executions you have really challenged a
very good chance of actually hitting the
right one to keep them warm or to maybe
don't need it the few cases where I
think is reused for these for example if
you are doing a food ordering service
like just eat where you know the peaks
of the traffic where that happens
lunchtime dinner massive Peaks so if you
let them happen as I said only would
that means all the concurrent execution
that gets born doing that those peaks
everyone the histones your API is gonna
hit the co-star but if you know when
that's gonna happen you can easily
sketch your crunchy of that happen so I
say five minutes or two means before the
speak and just spam your your API so
that you cause the lambda to call it a
Pik with the creoles the concurrency
fusions so the part of the time the
actual user requests a load comes in
those concurrency fusions out there so
that that's how you can mitigate post
out for those specific cases but if your
spot your spot and traffic are not
predictable then there's very little you
can intercept it to besides using using
node.js or Python or go where the cosine
sub is actually very small and the
really just optimize your function so
that they don't a have really they don't
they do very little outside of the if
you have the handle the function code
like no shipping logs and what no all of
that wealth metrics and also just
optimizing the dependencies so that you
don't use have to have so much
dependencies especially for JVM and
c-sharp projects where the more
dependencies you have even though even
if you're not using on every single
execution path they still takes all the
time to warm to warm up the your
application with the virtual machine
very long answer to a simple question
but it's not a simple
as a question to be fair any more
questions okay
follow-up the the correlation ID
probably described that's out looking a
very tricky problem to solve how long
did it take you to actually figure all
that stuff out and are Amazon doing
anything to make that easier okay so the
question is correlation idea seems like
a tricky problem to solve and how long
did it take you guys to fix it and it's
Amazon going to do it for you as far as
I know Amazon is not going to do
anything for you to fix that particular
problem because a lot of ways to do it
for how do you collect what's
correlation IDs right should be moving
them around one problem but also
capturing them and making sure that it
going to all the other logs is something
that you have the T as for shipping them
around Amazon as far as I know is not
going to do anything there for you but I
didn't take us very long to implement a
system that we had in place for starters
actually implemented a similar system at
the previous company companies so I kind
of know what to expect because is
something that you even if you're not
using lambda and using services in
running your swing in ec2 but you use it
you're consuming SNS or consuming
Canisius events you've got a same
problem we have lambda it just has to
work out actually if lambda was easier
in the sense because you don't have to
worry about concurrency one thing comes
into your function one invocation
there's no concurrency happening so you
don't get multiple contexts coming in
apart form Kinesis when you got patching
but that's something that you have to
figure out how to do yourself we did it
a certain way with some trade-offs but I
can't talk to you about the afterwards
but for everything else it means for
nodejs
I can capture the incoming context
object context IDs relationality story
and shove them into a global variable
since I know there's Gordon gonna be one
execution I can just put in a stop into
that global variable to my heart's
content without having worry about
concurrency kick now you know getting
wrong correlation ID between different
executions but I didn't think us very
long a fingers like a week less than
week for me to to put something in place
if there's no more questions then I
guess the thank you very very much again
for coming to this talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>