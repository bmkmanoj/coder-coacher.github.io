<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>So many Docker platforms, so little time... - Michele Bustamante | Coder Coacher - Coaching Coders</title><meta content="So many Docker platforms, so little time... - Michele Bustamante - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>So many Docker platforms, so little time... - Michele Bustamante</b></h2><h5 class="post__date">2017-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_CWpwTymxDI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi how are you all doing I'm gonna just
take a sip of this water if you don't
mind
my throat's a bit dry it's actually
vodka don't just kidding is that on film
so um anyway hi how are you doing today
who likes docker oh yeah some people do
some people don't well so the impetus
for this talk is to fit what normally
use a three day workshop or lots of
different experiences in an hour which
should be fun so the whole so many
docker platforms so little time thing
the so little time thing is maybe not
enough time for everybody to know all
platforms but also maybe only a little
time to talk so I might have to go a
little bit fast I have a lot of things
I'd like to share with you maybe I
should introduce myself I don't know if
I've met everybody but Michelle Drew
Bustamante I'm a founder of science and
we have an architecture consulting
company that has of late I built a micro
services practice with a lot of great
people in the ecosystem and so we've had
the good luck of I guess getting some
you know real experiences on Amazon on
Azure with DCOs with without ec2
container services with docker
datacenter even which is an interesting
option or at least will be probably at
some point so I'm going to talk just a
little bit about those experiences but
also talk a little bit about just the
things that we think about and at the at
the same time show a few demos just sort
of bring you through the story if that
makes sense so I'm gonna start with just
sort of the flow of you know we could be
developers in a room and have a startup
and just build images and run containers
and put it on a box and have no load
bouncing and that would be my simple
docker experience but how many people do
that actually and are not afraid to
admit that right because it's fast and
it's effective and the containers
restart really fast so if you want to
replace one and if high availability is
not necessarily mission-critical for
that particular applet maybe suggest
your blog or website and you just don't
really need to pay for all that
infrastructure it works right so there's
some people that actually go that route
and just automate that with ansible and
so on so that's one story and then
there's of course the come
other end which is the one that we
normally deal with when we think about
people trying to move to micro-services
right so I want to take you through that
flow talk about docker networking
composition scheduling constraints
orchestration features and as we do that
show some demos in the different
platforms and talk a little bit about
how we make decisions so it all starts
with this right people get excited that
they can build once and have it run on
this machine and ship the same image
somewhere else and have it actually run
there and actually work which is great
and that's sort of the promise in
general of docker right as we know it so
to start off I'm just going to go ahead
and jump into a command line here let's
get into a good spot let's see which one
I'm in okay perfect so I'm going to just
tunnel into my build agent over here and
we're gonna go into my directory and
what I have on here is two applications
I have an API service and I have a web
application a client so the application
client will obviously call the service
and so we need to bring those two
together right what I'm gonna do first
is just kind of bring this up here so
you can see and I'm going to jump into
content
- API and we'll take a look at the
docker file so this is just a simple
node application right it's going to
copy the directory over there's no
compile step so I don't have to do any
sort of build of the application itself
and we'll copy the files over will
expose poor at 3001 and then start at
the entry point so that's it nothing
fancy going on there what I'm going to
do first is do a docker images so I do
have a couple of images on here actually
so I'm going to go ahead and just do a
there we go
oops
okay sorry got another thing to do here
I thought I cleaned that so just just
second I'll show you my fancy remove all
thing okay cuz that's pretty fancy let's
do this then and we'll try and get
through that image perfect and we'll do
another one for web perfect and good to
go
okay so docker images and it looks like
we're just about good okay so I'm gonna
ignore the other two they don't matter
so I have argon already downloaded
already and so now I'm going to go ahead
and do a build so we'll do a build of
the API so I'm going to build the
content API I'm using a prefix dust
blonde that's my docker hub account so
the idea would be eventually going to
push that to a image registry right so
on my machine as a developer this is
something I work with some of my
companies around in terms of getting
started we need a environment on our
local we need to be able to run and test
things just as regular api's and then we
need to flip the bit and be able to run
it as a container hopefully with
environment variables just allowing us
to witch between the two right so it's
one of the sort of new procedures that
happen if you're in Visual Studio you
would do it one way if you're using
sublime you would do another but it's a
thing to think about right so when I go
ahead and build this I should be able to
oops something else went wrong here oh
I'm not in the right directory excuse me
for that content API I think I moved out
and we'll try that again
yeah built
okay oops April's I'm completely yeah I
messed up my command line and I hit
something didn't I I missed the dot
hello I think something's happening
while I'm typing that's mischievous so
anyway that's gonna go ahead and build
and that was only you know five minutes
really it shouldn't have taken that long
at all it would have been like one
minute if everything went well so um
okay so I have an image now right docker
images docker images yeah my typing is
like a little latency here I'm gonna go
slower that's the problem so content API
is now there on the top right so das
blonde content API not sure if you're
actually seeing the full word there
what's up with that yeah better okay so
I have this running if I do the docker
PS we're gonna see that there shouldn't
be any containers running right now so
what I'm gonna do is also run that API
so we'll go ahead and do a run I'm gonna
just do a quick run command and a couple
things I want to point out here one is
when I run locally I could just run the
API by itself but I also want to connect
the web app to the API and I also would
like to do that with something simpler
than a hard-coded API URL with port so
what I'm going to do is use DNS and so
I'm going to create a network so I have
this network called fat medical that is
already created and so I'm attaching
that when I run it I'm going to show you
how to do that with a composed so the
point of this is really just to show
that one of the things I might consider
is how I want to connect containers and
package those because when I go to a
real orchestration engine I'm obviously
going to have a way to stitch those up
to a load balancer I'm going to have a
way to have requests to the load
balancer find the right container and
actually have multiple instances of that
container with different ports randomly
stitched and distributed across the
nodes right so I want to still have that
same configuration or potential behavior
when I'm running locally so that I can
have that you know consistent
environment variable progression through
my development through my tests through
my orchestration engine if that makes
sense so I'll go ahead and run this and
all that does is you know basically give
me
a running API so if I do a docker PS
we'll see that it's running if I do a
curl on the endpoint I should be able to
do a localhost 3001 speakers is the
endpoint and that should give me a bunch
of stuff which it did right so we know
it's working great so I'm going to go
back and do a Content web and the same
thing goes here but this time I want to
do a docker file and just show that I
have an environment variable there right
so right now the environment variable is
actually pointing at localhost 3001
which would mean that when I run this I
should be able to hit the outside port
the web container should be able to find
you know the the the web container
should be able to find the API container
but there's a there's a catch right if
I'm coming in from outside there needs
to be some form of DNS and that's where
the network would come in so if I'm
actually hitting at localhost without a
proper DNS then that's actually not
going to work so I just want to
illustrate that first so that you knew
that there are some of our variables in
there and also that the app is actually
respecting that internally it's actually
calling the API based on the environment
variable so I'm gonna go ahead and exit
and let's go to our build
and we'll build the web this time and
yeah oops
forward one here I'll just get that and
yeah that just did the wrong thing
we'll let it finish cuz well I have no
choice probably there's that okay so
let's see finish one of these days okay
let's see what I actually did colleges
run that API one again which is wrong
because that will replace the API with
the web content so we're gonna give that
another shot it's fun watching the
command-line run by though isn't it cool
content API is probably a mess right now
so I'm just going to go ahead and fix
that up yeah build content API and we'll
let that run and I will now have another
image okay docker PS I still me with one
container running docker images I should
have the new image and I do so I have
the content web image and now we can go
ahead and try to run that so let's go
ahead and do a run only this time I'm
going to do it with web so you'll notice
that on the command line I'm passing a
content API URL so I'm overriding the
API URL with the DNS name of API which
is the name of the actual API when it's
running so basically under that medical
when I registered the API I'm using the
DNS name API and I should be able to
then hit that when I'm coming in from
outside because there will be a network
to it on this single Linux box in the
cloud
okay so this is equivalent to passing
parameters that would be like a service
description right or a task definition
and so this is a very beginning of the
things we think about when we're running
containers when we go to an
orchestration engine we need a way to
supply those environment variables
network definitions dependencies may be
between different containers in a single
definition which would be potentially
done with a compose file which I'll get
to or potentially be done with other
capabilities that are provided by the
platform so for example when I upload a
container to Google
container engine it gives me a way to
use a compose file or a Yama file but
when I go to Amazon continue services I
have two or three ec2 container services
then I have to provide a different
format right it's a task definition so I
convert the gamal to a task definition
when I go to DCOs I'm using a JSON file
is a completely different format but
they all sort of contain the same thing
and the beginning of that is how do you
want to run the container so the point
that I'm just bringing up here is right
now I'm just passing a couple things on
the command line which happened to be to
the run command like I need to know the
network I need to know maybe some
environment variables and those are the
things that would switch when I go
between environments right and so
eventually I would do that a different
way right now this is pretty crude it's
just docker you know native right so
when I run this it will have that
override and if I do a docker PS then I
have the two containers running and if I
do a curl against it then I should be
able to hit a local host oops
keep doing that and 3,000 for the web
and it should just return you know that
the page is off right now I'm not going
to see success from the web page to the
API unless I actually go ahead and run
it and hit something that's going to
call the API right so I'm gonna head
over to here and this is the website and
so if I go to the main part of the
website if I just kind of launch the
first page let's do this then I'm not
going to see anything interesting but
when I click on the speakers or on the
sessions that's when it's actually
calling the API which it turns all that
JSON you saw the first time so we know
that then that's working and had I gone
through the step of showing you without
the network or miss configuring the
environment variable then we wouldn't
see that work makes sense okay all right
but that is a quick start now the point
of that was just to sort of talk a
little bit about task definitions talk a
little bit about the fact that it
actually applies even with docker native
run
and that you know there are many ways to
do that or achieve that same goal when
we get to the container platforms docker
compose then is you know the sort of
docker native mechanism or descriptor
for creating service definitions which
would be a Yama file and so in here I
can follow the definition language to
put things like you know how many may be
multiple container images may be link
them together or make them dependent on
each other there used to be a way to do
direct linking but that's sort of
obsolete because we really want to use
DNS I can specify the network it will
create the network if it's not there and
then attach or join that container to
the network it will allow me to specify
volumes of environment variables and
other configuration values that I might
want to have access to okay so that
definition file then becomes a thing
that theoretically if all platforms were
created equal they would all share the
same language they just don't so it's a
bit of a modification but we can take a
look at how I would run that from the
same machine if I just wanted to so
let's go ahead and do a quick removal of
all those containers so docker images or
docker PS should show me nothing running
right and then what I can do instead is
go back a level and do a docker compose
oops sorry I forgot the dash
and then that should address starting
both of those and then I should be able
to then you know see them running so
let's go ahead and do that right so now
I have at three thousand one and at
three thousand those API and web
application running but it was done with
the single command so it sort of grouped
them together right so it's a single
definition for actually multiple
containers doesn't have to be multiple
containers could be one with environment
variables and so on
right so this would be a way for
application grouping for example if you
want them to be a single sort of
deployment artifact and not all of the
orchestration engines let you do that
the same way so each of them might have
their own thoughts around how do you
combine so when we think about composing
and we think about application grouping
with kubernetes you have pods which
allow you to have multiple containers
treated as a single unit in terms of
deployment upgrades and so on when we
think about DCOs
they have application concept which I
think they just renamed to services
because there's been an update so now I
have to remember when we go in there and
then in AWS it's a little bit less
obvious right you have to upload a task
definition and it's it's not exactly
done in the same way in terms of
grouping so we will see AWS when I go in
there too okay so again compose grouping
network environment variables having it
all run at once it looks like I can
still you know curl the URL and that
should work and I should be able to hit
that and that should work and I should
be able to still hit the site and that
should work okay so far so good
that's good so you know because you just
never know with these connections and
things okay so we've done a bit of a
compose we've done a couple of quick
tour items let's move on for our quick
hour so the other thing we need to think
about is where do I push the images
right um I'm going to be a developer
developing locally like I just did I can
fully function that way I can have a
build agent I'm using
cloud I don't have to actually even work
off my local but I could you know I
could use docker for Windows and work
locally the same way I'm working off a
Mac here but this actually does happen
to be a little out ok when we look at
actually deploying to another
orchestration engine or to deploy to an
environment in the cloud I'm not likely
to be tunneling in like I'm showing now
and manually doing that deployment right
so I need to be able to pull the images
from somewhere first of all and then
probably that's on purpose because of my
AWS demo but I'll turn it off for a sec
I might get an alert later
so we remind me to turn it back on so
right so I need a place that I can put
the images so that they can be
distributed so that the you know
orchestration aja cluster can actually
pull the images down and run the
containers right as part of a scheduling
act so we have to pick a container
registry for that so in the demos I'm
using docker hub right so docker hub you
can do free open public containers and
have your own account likely for a
company that's not going to suffice you
can have a paid account and docker hub
to do the same but likely if you're
choosing a platform like AWS
then you're going to use the you know
the Amazon container registry here right
so you're gonna use that instead easy to
container registry if you're working in
in Google container engine then you're
going to be using its internalized
registry which it provides for you as
part of it if you're using Azure
container service then you're now able
only of late to use the Azure container
registry so up until then I was kind of
stuck in Azure right like what do I use
for demonstrations I could use you know
the docker hub but otherwise I'd have to
have a paid account or go through the
heavy burden of implementing my own
registry and hosting that cluster and
managing that cluster which is yet one
more thing to manage out of all the
things I'm now taking on when I take on
a micro services architecture and
containers make sense so we probably
want something built in we want to try
not to have to deploy it I've actually
worked with teams and we've managed to
finally get one up and running
but it's just not really trivial
honestly it's better to have a hosted
option if you can something baked then
there's the new docker data center
implementation which is a docker native
but paid enterprise product that's just
on top of docker and docker swarm and so
on
and that comes also with a DTR or a
docker trusted registry so they try to
take the pain away for you by giving you
a product you can pay for that makes it
easier to set that up on your own VM so
you're still managing it it's still your
cluster so it's not a hosted service but
it's at least something accessible to
you maybe a little easier than manually
trying to deploy it makes sense ok so we
need to care about registries we're
doing that because ultimately we want it
probably at least to the CI part of CI
CD not everybody wants to automate all
the way through to deployment because of
the you know the risks associated you
have to be very ready for that you have
to be ready to respond to rollback you
have to be doing drills around those
things so that you're able to manage all
these hundreds of containers that are
floating around they probably don't
float that was just too badly chosen
word so the idea would be at least I
want to be able to check in and have it
automate building the image at least and
this is an interesting topic too because
what I need to care about in this whole
world of containers and orchestration
and deployment is the process for not
only just building an image and having
it deployed into the image registry but
how do I tag these things how do I
promote these things through
environments how do I get it from dev to
test to QA to production a lot of folks
maybe just rebuild right from your
source today when you're not doing
container work so you're probably just
you know expecting that a particular
label on the source if I build it once
and build it twice and build it three
times it'll just be the same right
except for environment variables and
that should be the case but maybe
there's just a slightly more reliable
aspect to this process which would be
you literally built it once you
literally don't touch it again and
same image literally went from test to
test - sorry from dev to test to staging
and to prod with the only change being
those environment variables that come
out of your service description and so
on make sense so that's the idea is we
want to get to a point where we're not
we have no problem with automating the
build of the image let that happen all
the time when developers are checking in
but we need a labeling process for after
the fact
the default labeling will be latest the
you know thing we need to think about
our repository tags if I'm pushing it to
Asher Amazon DT our Google container
engine docker hub actually docker hub
doesn't require anything but that you
put your account in the name which is
why I chose dose blind slash something
but the others require some very long
interesting tags to get at that registry
and then you always have to security
around that with Azure ad sorry as your
registry we have Azure ad and it's
actually surprisingly because I'm not
always a fan of of the the process of
setting things up in Azure with a ad but
it actually works very very easily in
this case it's streamlined really nice
so I can set it up so that developers
can each have their own account and do
their own push of images to that Azure
registry that raises another question
though right should developers ever be
doing that maybe to a dev area for
testing right between each other but
when it comes to the CI process really
legitimately the stuff you check in
should just be automatically built right
there should be zero reason that you
have to do anything but check in and an
image appears over on the other side
which you can now pull by saying get
latest which is the default and so
that's a pattern that you start to
establish again I'm giving you you know
some discussion around this because it's
a thing on that list of things that we
have to think about now where we're
moving into the orchestration flow and
it does take a bit of strategy but not
too much right so we got repository tags
we're going to have version info maybe
which service it is some sort of
labeling and at some point we might
include the the branch label or
something so if we move over to master
and do a merge of updates that are going
to go to production and you have a
process for that within your git
repository then maybe Jenkins can pull
that and intelligently apply the right
tanks so we've done that before with
Jenkins and had it set up you know the
right tagging mechanism based on some
you know branches for example in the
repository the other process would be
you know to have another entity do the
REIT agging for example run deck would
be another example that we've used in
the past on our team to apply images
with tags as they go through and
traverse the environments and so run
decks responsible for the actual
orchestration and delivering of the
scheduling instruction in that case for
example with Amazon you need that
because you don't really have a full
Orchestrator in Amazon okay so getting
things into the registry is not too hard
getting things out of the registry again
not too hard if you know what tag you're
pulling securing the registry would be
obviously a process that should happen
within your ecosystem probably beyond
this discussion but you know what I've
done already in here if you if I take a
look at the images right I've got this
content web and content API and if I do
something like let's see let's do a tag
and what I'm gonna do is rename that to
my daus blonde repo just for
illustration and content API latest is
going to be renamed to let's say v2
because I've done an update now right so
oops I know I spelt it wrong thank you
I'm typing at my normal speed but
apparently that's too fast for this
connection I'm really that fast yeah
something to be impressed by I'm sure
everybody is really okay so what I'm
gonna do is just quickly go over here to
the browser I'm gonna make you dizzy for
a sec and find it and we've got our repo
over here let's see
so dass blonde I've got this content API
and it's already got a latest tag so I
just actually cleaned that up and
recreated it and what I'll do is do a
docker push oops sorry
read my mind why don't you content API
and so that will do is you know push any
updates to the image layer and also
retag sorry apply the tag also in in the
get in the sorry docker hub I wanted to
say get repo so we'll come up here and
refresh and I should have a v2 if I'm
not mistaken well then that wouldn't
work with it no pair programming I'll
catch you later okay so did I do it or
not
oh I pushed the wrong one and haha okay
well that's I meant to do that because I
want to show you how easy this to get
mixed up because it you just really
never know honestly it's important that
we think about these things upfront okay
alright so let's see docker images web
is v2 indeed so we want to do a darker
bush
just blonde okay try that again Redux it
might even show in progress over here
sometimes well one of these moments
it'll come through in the meantime the
other thing I could show you is that you
can create repositories that have
automated builds so if you're just
testing and wanting to try that out then
this would be an easy thing to do just
connect it to your git repo and have
your check-in find the dockerfile build
the image and push it here you can
actually VSO with Visual Studio online
now has a really nice set up for that
too so we just finished setting that up
another project it was kind of new
feature going against that your registry
and setting up the security with aad and
it's working very nicely with VSO so
that's another if you're doing dot net
development with asp net core most
likely if you're doing docker containers
today then that would be the thing so
that's a really nice set up and you can
do the full CI CD even have it push to
your you know orchestration environment
and have it scheduled so that would be
an interesting way to look at it too if
if you're ready for that in your
production environment what you might do
is for some environments automate that
and for some environments like say for
production maybe you want more control
till you're sure about the whole CI CD
process all the way through okay so I
was trying to get back to my profile
here and I'm just not there let's try
that okay good
so I wanted to get to just blonde the
registry here we go and wet the fridge
and it should have finished there we go
so okay so just a quick look at that
let's keep going
okay so at that point what I've done
I've built images I've run them locally
I've got environment variables to flip
between I can run and play with my
docker images locally and I can push
them to a repo and I can even go up to
another server and pull them down and if
I'm just a startup I've maybe I've you
know finished what I really need to get
to a certain point right because I can
stop and start and replace those
containers without high availability and
it'll just be really super fast because
it's fast right so I know some startups
that do simple things like that or even
just simple projects that you have on
the side that aren't mission-critical
it's always really bad to say not
mission-critical because everyone thinks
everything they do is mission-critical
so you know there's layers of
mission-critical let's call it but the
fun begins now when we start thinking
about all the other stuff we have to do
right so what do I do in terms of you
know deploying to a cluster right and
having multiple instances of that
service running for high availability
distributing it across nodes having it
scale when it needs to scale having it
automatically stitch up to load balancer
so that it can be discovered what kind
of discoverability are we going to be
working with right because there are
layers of that too
so first thing we usually look at is the
platform choices right are you with an
affinity to Amazon already or Azure
already or Google already although I
don't run into Google container engine
very much I know
kubernetes definitely has a lot of good
winds behind it sails but that it seems
like most large enterprise aren't
looking at Google for their deployments
in my worlds that's my world that's not
everybody's world
so definitely Amazon or if you've got an
affinity there then you've already kind
of chosen what kind of platform am I
going to think about right with Amazon
you're going to use the ec2 container
service with Google you're going to use
Google container engine and it's
kubernetes you know basically and then
some and it handles all of these things
for you it's built in and then Azure
container service is actually kind of
different
right what it does is it it provisions
with a template your I as environment
for the orchestration cluster of your
choice so you choose if you want to use
docker native swarm if you want to use
DCOs which is the mesosphere stack with
marathon and missiles or if you want to
use kubernetes now which is a new thing
they just released right support for so
I just spun up a cluster with that but I
haven't played with it yet in in Azure
specifically but it's there it's
available and then there's one sort of
last choice I have on the list here
again there's others right like this is
just the mainstream that I run into so
you know there's core OS there's rancher
there's there's all kinds of hosts that
try to sort of you know abstract you
away from needing to manage your
container clusters but internal
enterprises tend to go with the big
names so this is what I see more more
often okay now docker data center is an
offer by docker so that is their paid
Enterprise
you know orchestration stack which is
really based on the docker swarm and
then the jaakor trusted registry and it
has this thing called the universal
control plane underneath that is you
know the orchestration engine and so
what docker data center really is is two
kinds of offers one is a hosted offer
and then the other is you deploy it on
your iOS which I have done actually and
again it doesn't really have a lot of
feature so more than anything I run into
DCOs with Azure DCOs
with Amazon and then kubernetes really
just new with Azure right so that would
be a good comparison with DCOs I think
in terms of like capabilities but if I'm
in Amazon I'm probably going to go with
ECS because that is you know built in
that's a bit more done for me I still I
as under the hood I'm still managing VMs
I still have to provision VMs for my
container cluster but it handles the
clustering part for me right so that
gives me some services around that that
feel native to Amazon and that were
if you're an Amazon make sense so we
need to care about the management
cluster the agent nodes we need to care
about discovery of nodes discovery of
services that are hosted across the
nodes we need to care about the docker
registry which I've already talked about
right in terms of which one is where I
have a couple of charts at the end that
will give you a cheat sheet so I'm just
kind of talking through it right now the
core features we care about again
discovery load balancing routing auto
scaling self-healing
right we want to know when something if
you tear down one of your containers
something's wrong with it it's not
responding that it will be replaced
right and new one spun up so things like
that and so that's where we get into the
fun part we can talk just briefly then
first about scheduling so scheduling and
orchestration refers to the deployment
right of your services and running them
on the cluster so running them on the
agent nodes that are available and so
typically there will be a master node
cluster that keeps track of all the
other available nodes and their health
and all of the containers that are
running and their health hopefully if
the orchestration engine supports that
so of health endpoints are available
then it will track that for you so when
you for example on swarm go against the
master node you know the the leader and
you don't know which one is the leader
you're just going against the load
balancer right against the cluster and
you issue a docker run with commands
against the cluster then it will issue
to the available nodes for the number of
instances that need to deploy the
container instructions to go get the
image and run the container and with the
right parameters and environment
variables right so all that's done at
the master node level basically so
that's the idea is that you ultimately
want to send an instruction to the
master to say can you please run this
container with these parameters I need a
minimum to healthy maximum grow to ten
you know these are my environment
variables if there's any networking or
dependencies on other containers any
restrictions like how much memory you
need and that's the thing that is kind
of interesting to get used to is
the constraints concept so let's take a
look at that so I have another VM here
let me find my mouse okay and okay so
I'm gonna get out of there okay so I'm
going to connect to a swarm cluster that
I have in Asscher right now I deployed
that with Azure container service so
it's just thinking right now I'm getting
mean hopefully okay let's go okay so
what I'm gonna do is let's say issue a
command for just information let's say
so what I've done is I'm if you look at
the bottom there I'm hitting this
because I'm not coming in from outside
which would have meant I'd have to come
in from outside with security enabled
and so on so instead of opening ports
and enabling that I'm actually tunneled
in to right now one of the masters and
so I'm issuing the commands as if I'm
coming in from outside but I'm not and
the idea would be I'd have to hit a
specific port I'd have to issue the
instruction and this is what tells it to
distribute that request so that it's
going to issue the command against the
master and apply it to the available
agent notes so for example if I say I
want to have info it will tell me the
number of nodes I have available and
healthy so it's kind of giving me some
insight into any containers that are
deployed how much CPU I'm memory I have
available and so on right and I have
three nodes so you can see I have
different agents right so each of these
agents start with you know different
value here 7 9 and then 0 0 0 0 0 1 0 0
2 so those are the ending yeah so I
don't have anything running right now so
if I do the same docker PS command that
I would normally do it'll tell me I
don't have anything running across
cluster right now if I say I want to see
what images I have I might have one
image that I already have down here
right and so I have a HelloWorld docker
just a simple image right so what I'm
going to do is run the container right
and we'll run this and let's see so I'm
going to do this and ask for a random
port so the - P just says go ahead and
assign a dynamic port the idea would be
ideally that there would be a load
balancer that it would stitch
automatically the dynamic port that the
container runs on right that's not going
to happen here by default right now I
would need to do something more
interesting with swarm but most of the
other orchestration engines that's
built-in like they have an internal load
balancer I'm going to show that in a
minute - so I'm gonna run this now and
just show what happens after the
containers run so if I do another look
at docker PS it will show me that it's
got a dynamic port of 32 771 right so
that's that's the end point it's mapping
from the internal port 8080 of this
particular container and it gave a
dynamic name I don't really care about
that in this particular demo so what I
was going to do next is show you what
happens if I run info and we kind of
scroll up a bit I should be able to see
that one of the containers is now
running on one of these nodes right so
there you go alright ok so if I run this
a couple of times and actually let's
just do that let me just get out and run
the same one it'll let me keep running
them because the ports dynamic but if I
were to ask for a specific port what
would happen
I think 80 80 or something like that it
worked
I've worked again it worked three times
and now it can't write so that means it
distributed one to each node and it used
up the port but that's a resource that
can't be shared so now it can't
distribute another right so that's a
resource constraint if you will but I
can have other types of resource
constraints so let's go ahead and do
another info pass and what we should be
able to see here is I've got you know
three containers here with not very much
used on the CPU or memory right because
I haven't actually reserved any I
haven't said my container needs X to run
most of the other environments actually
require you to specify something for
your container because the whole concept
is that you should get to know through
drills ahead of time and load tests like
what kind of a capacity does the
container need to operate in a healthy
manner and the idea would also be that
if something doesn't work right if it
sort of exceeds memory that that
container can can fail or go down but
you know another one should come up in
this place because you have these
minimum said I need to healthy always
running so the orchestration engine
should know enough to say ok I'm going
to spin up another one that's the idea
so right now I'm just distributing
freely but I ran into a resource
constraint for the ports let's try
another thing and what I'm going to do
is delete everything on this in terms of
the running containers and so that means
I should be able to see docker PS
nothing running and what I'm going to do
is do a run but this time I'm going to
require two gigs so you can see that I'm
asking for two gig over here and so it
will try to run one and there's only
three gig per box right and it will try
to run another and it will try to run
another and then at some point no
resources again right
so that's again part of the the whole
idea is that I should be able to you
know constrain resources and dynamically
assign you know to each not dynamically
but specify for each container what is
the resources I require so that the
scheduler can do its job which is find
the right node to put it on there's
other kinds of constraints like
proximity constraints and things like
that but hopefully that just gives an
idea of one of the considerations right
of the container orchestration engine
that we need to care about so you know
this is just a screenshot of what we
showed before which is the containers
how many are there and how much is used
in terms of CPUs in memory there are
other constraints we've talked about
just briefly and then here showing that
I've already reached my you know my
constraint right I've filled with two
gig or three Greg three gig is used over
the 3.5 so I can't issue another command
for a three gig right so okay and so I
would get an error like that I already
talked about the fact that this whole
process of then scheduling which I just
showed up from a high level just with
swarm which is you know simplistic and
command-line right I'm obviously not
really going to do my scheduling that
way I'm gonna have some automation
around it I'm gonna have automation
around deploying a container and I'm
gonna have tasks definition or a service
definition or an application definition
all the same right that describes what
are the requirements of this container
of this service what memory requirements
what affinity to other containers so
then the scheduler can do its job so
what I just showed you was the
lightweight version of that that is
masked a little bit when you go into a
platform so the challenges we face are
things like load balancing Service
registration discovery and server
density when we think about I want to
get as many containers running as are
needed to serve requests and if we start
to you know exceed load on those
container instances then I want to be
able to scale them out so I need to be
able to set up some sort of alerts or
alarms around that that would be one
example that the orchestration engine
can provide for me is auto scale right
of the container
then the next step is when I start to
run out of some resources on the actual
node or VM I want to auto scale you know
the actual nodes in Amazon in Azure and
so on and so we get varied results with
that depending on the platform we're
using right so Amazon's got some ways to
do it
asher has you know Auto scale
capabilities but it's not quite tied in
to DCOs yet so that's all still coming
together but the point is these are our
things we're thinking about right so
that's another topic and so what I could
do is just to take a look at ICI at ec2
container services for a second one
deployment you know that I might have is
something around having as your load
balancer so when we think about
discovery you know one aspect to
discovery is having services deploy and
stitch themselves up to whatever the
load balancer is that is going to know
how to route to that service right so I
could either have an e lb attached to
each service or I can have a lb which is
now more intelligent which will allow me
to have dynamic port assignments which
will allow me to you know just spin up
additional services on an individual
node instead of one per node so in the
past we were stuck with one service per
node because of port restrictions
because you couldn't get the dynamic
aspect and again alb shipped last
September or something so that was when
we could start playing with that and now
we could build a better starting point
around discovery so the you know one of
the parts again around every is just let
me distribute more instances with
dynamic ports and have the load balancer
somehow know about them this is not
dynamic discovery though right there's
client side and server side dynamic
discovery as well for example the client
could call in and ask a console for
example or zookeeper or SCD for
information about where services are
deployed in order to find the URL that
it should call so there's overhead to
that it's chatty right there's another
request in the mix in order to find out
which service should I call but you're
pushing it to the client in that case I
don't prefer that model but there are
probably isolated cases where that
useful on the other hand what you could
do is set up you know a cluster state
with consul or something and have that
be what is communicated with say you
know from either an engine X tier or
from even something you call from al B
that goes and finds out which service
should I forward this to but you have to
set something up that can go and request
where should I send the call if you want
full dynamic discovery so there's
overhead to that it's another cluster to
manage it's oftentimes not the first
thing on the list for people if they can
find another way to achieve something
acceptable and what's acceptable is
typically starting with this with Al B
so it's just a big undertaking to go
down the road of console for example for
that for dynamic discovery so when we
think about requests coming in through
the public load balancer those might go
for example to ec2 nodes and then I
would have another private alb behind
another subnet so that it's not
available to the public and that would
be maybe where the websites on the ec2
nodes call my services tier so one of
the things you can look at with
container
you know orchestration and deployments
is whether or not you want to have a
services here that's on the
orchestration platform and have your
websites just be traditional and have
your backends continue to be traditional
as a migration plan right so I've got
these legacy databases I don't have
everything isolated into domain
boundaries yet I need to take time to
get that done but we can start by
scaling the middle tier and making that
a more agile
you know deployment and development
story so I can you know enhance features
create new product listings create new
services for the business that live in
the middle and consume the same data
models we already have and then over
time migrate to that pure sort of micro
service isolation of those data stores
and maybe even move the websites into
the containerization story so that I can
optimize my deployments and not have to
have ec2 notes as well right
both of those are acceptable models is
just
it's a decision that you make right so
having this dynamic having this you know
happe services I was kind of pointing on
the wrong side here before dynamically
stitch up to the alb would handle the
sort of discovery of services that are
healthy you have to have health
endpoints that are available so that alb
knows when to remove something from the
alb when a service is not responding
correctly that means your service when
you build them have to write upfront
think about what is healthy is it just a
two hundred at a ping or should I be
checking that a few other things are
available or should I have two different
health checks one for sort of the
ongoing ping and then every once in a
while let's do a sanity check that the
database is actually available on the
other side so you can think about those
things and those can be built into some
of the rules around health checks and
and how alb response for example right
okay so one of the things I wanted to do
here is go to Amazon and right now I
have some alerts set up I'm going to
turn my phone back on and those alerts
are are I'm going to go through them in
a second but first I'm going to go and
start a low test so that it can kick off
because it takes a few minutes okay so
let's do this I think I'm in the right
one okay so I exit out I'm typing way
too fast over this guy seriously okay
okay so I'll just show you what I'm
doing here oops
yeah
so all I'm doing is running a command
that's going to send a million requests
for over to the services that are
running in Amazon right now and it's
actually putting in some instructions
for how many calls to make and how many
in parallel right 125 in parallel and
the idea is I'm gonna load up those
services that are just just deployed
right now so that we will get will
trigger some alerts we're going to first
trigger an alert of memory usage on the
individual nodes that will trigger
deploying additional services and
distributing those and then we're going
to have a CPU overload that's going to
actually expand increase the number of
instances so let me just run this first
and there we go
okay so that's going to take some time
and while we're doing that I'm gonna
head over to the display and so these
alerts this one will like I said right
now this is read because it's saying if
you're using less than 25% of CPU scale
down but scale down means go down to one
and I'm requiring two minimum so it
can't scale down so it just shows us red
and again I might not do that in
production but it's just a way to show
that that when this trigger is a green
then we know the first alert has
happened right because what will happen
is I will you know start using more CPU
so that will first turn off that alert
and then I'm going to hit the CPU
utilization for greater than 75 and what
that will do is start triggering
deploying additional instances to the
Amazon cluster so in my ECS cluster I
have two services and three tasks
running here's the task definitions that
I have available and I think the one I'm
caring about right now is
no that's not it
demo task it's this one
API
okay sorry I'm just like trying to
figure out which one I last activated
because I had it all sitting there and
then I went away so
that looks better okay and um the other
thing I was gonna do is go over here I'm
just waiting for this to kick off so
that we can kind of get a look what I
was gonna do is show you what happens so
basically this is saying add two hundred
percent capacity so it's going to go and
try and add double the number of
containers that I have right now
basically that are running so this alarm
will change and then it will follow with
the rule if we take a look at the one
for memory greater then it's going to
talk about instances so just a second
I'll go in here navigation is not
clicking the way I like so right so CPU
is first and then memory second okay and
so what you're gonna see is when the
alarm triggers it will obviously show us
some history so you can see that maybe
some of the things that have already
happened but basically once the alarm
happens so this is my alarm thank you
it sends me a text message and so if I
actually head back to the dashboard
we'll be able to see that the it's
probably got a refresh but it's almost
there hopefully yeah so now the one
that's triggered is the CPU greater than
75 right so this is load is still going
it's still pushing out you know requests
right to the containers and so based on
that because this one is currently
triggered it's it's going to start
adding capacity to the cluster so if I
go back over to my container let's come
back to the clusters and I think we'll
just go in and take a look actually at
the number so right now it says desire
task 2 and running tasks so it might
have already started doing it let me go
to the tasks and we can see with our
running let's see if it's added any yet
there's probably going to be adding some
more it takes a little bit of time so
unfortunately I'm just sort of waiting
now so let's see the other thing I can
do is I can really cause chaos and I can
delete a couple of api's so if I stop
these it'll also continue to try to
provision enough and it will do that
automatically right so these are set up
to have a minimum number of healthy and
that minimum healthy includes it too
right now and so that's what the task
definition indicates and so this will
automatically start spending up
additional while we're in here because
of course I want to just give you a
quick taste also of the D
cos let me come into D cos so in d cos
while amazon is figuring out its scale
out and the alarms are going on right
now I have oh let's go over here there's
my other alarm okay it's gonna add
instances now let's go to my terminal
which is this one
it won't jump until I get there okay I
need to tunnel in so that I can see my D
cos panel so this will stop being like
zero so the other thing I wanted to
illustrate was that when you come into D
cos is a totally different view of
similar things right you're going to
know how many nose you're going to see
how many tasks are running but more
importantly if I come in here I have a
public marathon load balancer and I also
actually have that listed here probably
right and then I have an internal load
balancer so the internal load balancer
is responsible for the API and if I look
at the logs you'll see that when I
actually add a new API it will
dynamically configure it at the front
end of the load balancer at ten thousand
one so it's going to stitch up dynamic
ports as I add and scale so I'm going to
just go out over here and actually show
you the services that are running I have
an API have a website and I have my
internal load balancer attached to the
API so if I select API and I do a scale
event and I add say three nodes so I'll
go to five it's going to start adding
those run them stitch them up distribute
them to random you know nodes it'll show
that they're healthy at the moment and
then if I go over to my tasks here I got
a bunch of events here so it just added
several new services basically to the
internal load balancer so that it will
dynamically find that was basically my
requests come in so I guess a couple
things just to sort of tie it together
because obviously we have to finish up
soon you can see right now that my cloud
watch is actually all healthy because
what it's done is it's finished its
scale activity right it's added
instances and it actually added nodes so
it'll start scaling down if I actually
kill this guy over here and then that
will start to go away and I'll get
alerts in the other direction when it's
done so for example here I can go to
here and kill it and that'll start kind
of cleaning up but let's kind of just
sum up what I've got here so one of the
things we talked about was self-healing
and I showed that just really fast for
time which is I killed one of the
containers another example would be the
containers not healthy anymore and a new
one tries to spin up it can only do that
if there's enough resources to do so so
you always need a little spare in order
to have the ability to spin up the new
before tearing down the old that counts
for upgrades rolling upgrades so that
you never have any downtime always high
availability always two nodes healthy
the two new ones come up those two
others are drained and that's what the
orchestration engine will do Amazon will
handle that for you automatically DCOs
handles that for you automatically
kubernetes handles that for you
automatically so those are you know nice
features of all three really I don't
think there's really a question that all
three can't do pretty much all the
things we care about that we've talked
about so you know when we look at the
the summary basically each of them has
their own native registry you know
whether or not you can use them on pram
or cloud so one of the things that I
find I run into on premise is the
decision around should I use a cloud
native mixed with an on-prem you know
deployed item and and what I've kind of
landed on is I think you want to go with
something like D cos or kubernetes both
in the cloud and on-premise or hybrids
when you need to have because you want
to learn to manage one thing it's too
much to learn to in my opinion so that's
that's a decision that's cultural as
well but if you're in Amazon or Google
or Azure these are your choices and I
kind of already went through that so I
just put a chart up here just to follow
so in summary in only an hour we talked
about simplicity of containers developer
workflows that you have to care about
you
the service definitions you have the
immutability of the images tagging them
all the way through to production you
need the platforms to handle all the
rest of the health concerns so we need
scheduling constraints we have to think
about the constraints we have to do
drills before we go live it's a lot of
work right so you have to see the light
at the other side in order to want to do
it but if you do it right we're forced
to do it right or you won't be
successful in it essentially you could
scale the containers you need to know
how to auto scale the nodes as well you
need to think about the load balancing
and discovery aspects so I just showed
really load balancing not and and and
dynamic discovery underneath a load
balancer but not console which would be
the full dynamic discovery methodology
health checks and self-healing so
killing a node having it regroup and
then you know just a peek at some of
these you know so I think in terms of
your decisions it really comes down to
which cloud are you in you know what do
you have an affinity to its cultural a
little bit
most of these actually deal with all the
features the only ones I would say maybe
aren't as fully functional yet but it's
gonna be interesting in the next year I
think and that is swarm darker native
and docker data center because those are
coming up fast
whereas kubernetes is already pretty
seasoned and DCOs with mesosphere and
and and marathon are also very seasoned
right and and of course Amazon is quite
seasoned I think also it's going to get
interesting in Azure because a sure is
just a template right now but it's going
to start providing services around all
of that which is going to be really
helping you to you know manage the
cluster or aspects of it that are part
of your Azure assets right and so I
think that's sort of the difference
between is it really is or is it more
it's going to be more right now it's a
bit more like I as right and you know
remember the goal of this which is
deploying a bunch of little things
instead of big things and having a lot
to manage which means you have to do it
right the first time you're going to
start with - you're going to get to ten
services and then finally be able to go
up to 100 or 200 because you've mastered
that whole cycle that we talked about a
little bit and
then some so you know business can have
new features everywhere and that helps
them to you know get what they want
because you can just replace and have
those rolling upgrades and have it be
seamless because you've done the drills
and you've done the right things to
master the orchestration platform
because microservice is and it's about
the business so I guess in summary
forget everything you did before and
enjoy you know looking at whichever
platforms makes sense for you and if you
ever have questions feel free to send
them because our team's been working on
a lot of them so it'd be interesting to
find out if what kinds of questions
people have in terms of digging in and
and getting started so happy to try to
help if you need it all right take care
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>