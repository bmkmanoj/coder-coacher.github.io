<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Resilient Applications In Microsoft Azure - Scott Allen | Coder Coacher - Coaching Coders</title><meta content="Building Resilient Applications In Microsoft Azure - Scott Allen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Resilient Applications In Microsoft Azure - Scott Allen</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q4kNu8Oclew" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Scott Allen I have a lot of
materials to cover so we will just
commence the talk
first I want you to think about the
lighthouse here in this picture it's
obviously facing some adverse weather
conditions and someone had to make a
choice about how to build that
lighthouse if I was building that
lighthouse I could have gone out in the
water hammered a pole into the ocean
floor put a light on top ran an
extension cord back to my house or you
know wherever and that would be a
lighthouse maybe but it wouldn't be very
resilient
but it was very cheap and inexpensive to
build if you want resiliency in a
lighthouse then you have to start
thinking about drilling into the ocean
bed and using concrete and steel and
battery backups and things like that
it's obviously a lot more expensive so
this may be obvious but I just want to
point out that the amount of resiliency
that you want in a system is more of a
business decision than a technical
decision you have to figure out how much
down time can your business in curve how
much availability do your customers
expect and then you need to apply some
engineering constraints and do some
technical research to figure out what
can we provide for the right amount of
value so that techniques we're going to
talk about today are really for
resilient services which is about
maintaining high availability not
perfect availability high availability
and a lot of the techniques that I'll
talk about also applied to disaster
recovery so if an entire data center
goes offline and takes your data with it
how do you recover from that problem
because in both of these cases to
maintain high availability to have a
good disaster recovery plan you
typically need things like redundancy
you need to replicate data to different
data centers but I'm gonna focus on the
high available high availability part of
this disaster recovery would be a
different topic so I'm going to focus on
talking about specific asier features
that give you resiliency and high
availability and one of the things you
have to do if you're gonna decide to
move to a cloud is you have to go to
that cloud provider you have to figure
out what they offer what services they
provide how how much resiliency is
inherent in that service and I just want
to give an overview of some of the
services that I've been using an azure
over the last year so there's compute
services these would be virtual machines
or containers running on a virtual
machine everyone uses storage and azure
everyone uses networking to summit
there's platform services integration
services app services so if you're not
familiar with Azure and app service is
something that you can use to deploy a
web application but it's a platform as a
service feature so instead of running a
VM you're just running on a platform and
yes behind the scenes it's Linux or
Windows and it's running an IAS server
but you don't really care about
operating system updates and so forth
but that brings up one of the first
considerations for building resilient
services is let's say I build an asp.net
application or asp.net core application
there are several different ways that I
could deploy that into Azure and I went
to evaluate my choices if I'm really
focused on high availability for example
I could take an asp.net application and
easily provision a set of VMs inside of
azure and deploy onto VMs that have is
installed I could also take something
like an asp.net core application and
deploy it in a service fabric cluster
which we'll talk about later in this
presentation and I can also take an asp
net application and deploy it as an app
service so three different services all
of them can host web applications for my
customers all of them have different
perspectives that you need to take for
availability different techniques that
you need to use to make sure that that
web service stays up and running those
are the kind of topics that we'll talk
about today each of those resources
talking about availability will also
have a service level agreement and in
Azure it's very easy to do a google
search and say tell me the service level
agreement for app services or for
virtual machines and you should land on
a page that where you can click on a
link and without very much legalese mmm
Microsoft on the very opening paragraph
of that page will make it very apparent
a couple different things first of all
for the resource that you're using like
a virtual machine or an app service what
is the expected uptime what does the
guaranteed uptime that Microsoft gives
you that's one piece of information
second piece of information will be
under what conditions will they
guarantee that uptime so for example for
storage
depending on what you select you might
get a guaranteed uptime of 99.9 percent
which is about 10 minutes of downtime a
week but depending on your configuration
you can also go for four nines or 99.99%
uptime for your storage and that would
be four minutes of downtime a month
compared to 40 minutes of downtime per
month the difference is that the four
nines 99.99% require configuration that
is about twice as expensive as three
nines so right there as a decision you
have to make do you want do I want to
pay to reduce my downtime by an order of
magnitude the other thing that the
service level agreements will tell you
is the amount of credit that Microsoft
will provide if they do not meet those
guaranteed uptime statistics
another thing to understand about these
SLA is when you look at them is you also
have to understand when you're designing
their system how the SLA is combined
together so if I'm building a web
application on App Services the let's
say has a 99.95%
SLA so that's 21 minutes of downtime
which is the SLA currently for app
services
well specific app services is not the
free ones and I'm using a sure sequel
database that has four nines in the SLA
so that's 20 minutes 21 minutes and 4
minutes my system could be down for 25
minutes in a month when the how services
are down that's a different time than
when the Azure sequel database is down I
could be down for 25 minutes a month
Microsoft would still be meeting their
commitment and I have to understand if
that's good enough for my application
how many customers could I lose how much
revenue could I lose if it's down for 25
minutes and the interesting technology
choices and design patterns come into
play when you're trying to do better
than that
SLA so you're trying to do something to
mitigate when as your sequel might be
offline for a few minutes those are some
of the things we'll look at
we also have to understand so when
you're selecting which services and
Azure you want to understand what what
is the SLA for that particular service
and also what are some of the inherent
resiliency features that are built into
that service because most things in
Azure they'll automatically give you
some resilience ease for example with
blob storage when I went to store files
somewhere in Azure I can set up a
specific type of redundancy in my blob
storage so literally when you're
creating a storage account there's a
little drop-down that says do you want
locally redundant storage that's the
storage that gives you three nines of
availability or 40 minutes of downtime
per month and every bit of data that you
write into that type of Azure blob
storage gets replicated three times in
the same data center and it's guaranteed
that your data will have copies on
different racks of server so something
catastrophic happens to a specific rack
like the both power supplies go out or
whatever will still be another copy of
your data and said inside that same data
center and then the interesting one the
one that's twice as expensive is read
access GA overdone at storage where not
only as your data still copied three
times inside of the same data center
it's also asynchronously replicated to
another data center but somewhere else
on a different continent or perhaps on
the same continent if it's a big
continent and if the primary data center
goes if a whole data center goes offline
you still have access to your data from
somewhere else and we'll talk about how
to take advantage of that that's
actually really easy to use read
accessed year but none in storage mm-hmm
if you use some of the azure SDK
libraries to access blob storage for
virtual machines
what is something inherent about virtual
machines that will give you resiliency
that would be what Azra calls an
availability set so if I go into Azure
and I provision a single virtual machine
to run
I just really know there's no sl8 for
that if I want guaranteed uptime from
services that I put onto a virtual
machine I have to create at least two
virtual machines put a load balancer in
front and put those virtual machines
into it an availability set and what an
availability set will allow me to do
focus your attention down here is I can
configure how many fault domains and how
many update domains I went in thus
available DS set so a fault domain is
basically if something goes wrong inside
a pager we'll try to limit the effect so
if I say I want to different fault
domains and I want to put create two
virtual machines I want them in two
different fault domains a fault domains
basically a server rack so if an entire
rack goes offline the other computer the
other virtual machine I have provisions
should still be up somewhere in that
same data center and there's also update
domains so let's say I have a hundred
virtual machines provision across a few
different three different fault domains
there's also update domains the update
domain is Asher's way of guaranteeing
you that even if they have to supply
some sort of security patch to the
hypervisor the actual physical system
that's running all the route VMs and
they have to take everything offline
that your virtual machines will be
distributed across these multiple update
domains they won't all go down at once
to apply this update and there's a very
simple mechanism in the UI or with
powershell script or however you want to
do it to take your VMs and put them in
these different update domains if I'm
building something with as your sequel
there's a couple different ways to have
resiliency if I'm using Azure sequel as
your sequel which already has a four
nines of guaranteed uptime one its
replication so it's easy for me to go in
and say this as your sequel instance
that I have running in an Australian
data center
I wanted to asynchronously replicate all
the writes and inserts and updates and
deletes that I do into another data
center perhaps on another continent and
you can have multiple secondaries with
Azure sequel and it also provides point
in time restore features which there's a
significant difference between
replication and point and time restore
so if I have a bug in my application and
it goes in and truncates a table or
updates a customer table and sets
everyone's name the Scott accidentally
it's a bug as you seek will happily
replicate that error around the world
and that's when I would need a point in
time restore also so you can configure
those easily
this is a little interface where you can
literally hear I have a as your sequel
instance running in the east us and I
can just click one another data center
the one in purple is the preferred data
center for me to replicate my sequel
server into and when you setup
replicated sequel server you'll get to a
little dialog like this where you can
make the secondary type readable which
has a number of interesting
ramifications on your software design if
you have say a reporting system that
hangs off the main application it's very
easy to point your reporting system to
this replicated read-only version of
sequel server that kind of offloads
things from your production system of
course that does mean that all the write
still have to go to that primary sequel
server but there's automatic failover so
my primary sequel server goes down one
of the secondaries will get promoted my
application will just fail over and
start using that secondary for app
services when we want to host web
applications one of the things that I
can do with an app service to make it
more resilient is geo distributed that
is literally take my application and
install it in two different data centers
around the world then the question is
which ones do my customers connect to
how do I route traffic to Australia
versus one in the United States that's
something that we'll talk about here in
just a bit too so let's get into some
specifics let's say I'm building a
relatively simple application I don't
need a lot of resiliency I'm not willing
to pay for dedicating 10,000 virtual
machines to maintain you know 99.99%
availability I just have my customers
coming to an app service my app service
has an app service plan which I'll talk
about in just a second maybe I used just
blob storage and Azure sequel so I'd
like to maintain some amount
availability in the system and it turns
out app service plans which are what
describe the virtual hardware that
you're running on if you're not familiar
with Azure so an app service is here's
my application app service plan is what
hardware do I want to run it on I might
want to run it on a to poor machine with
7.5 gigabytes of memory or I might want
to run it on a 4 core machine but once I
define my app service plan for my
servus I can also describe how many
instances of that app service plan that
I want so how many copies of that
hardware do I want do I want to run two
instances so if one goes down I still
have one available or four instances ten
instances you can go up to 20 I believe
without service plans depending on the
price in tier you select an azure will
automatically have a load balancer in
front of you to distribute things across
the app service plan so that's a little
bit of resiliency there but I also need
to think about the relationship between
my app service and Azure sequel and
using blob storage one thing you can do
in that situation is to take advantage
of connection resiliency so if I have an
application that tries to reach blob
storage or Azure sequel and it doesn't
connect on the first try and it just
gives up
that's not very resilient but it's also
simple to program if my application does
let's say three retries with a little
bit of back off time in between that can
give me a more resilient application
because sometimes you just have this
transient network conditions that don't
let you through to a specific server but
if you wait let's say fifteen hundred
milliseconds all of a sudden the
connection will work it turns out that
if you're using some of the official
Azure SDKs for these services like blob
storage and for Azure sequel these SDKs
that are available for different
languages for JavaScript for c-sharp
they have built-in retry logic you just
have to take advantage of it and perhaps
configure it differently if you want it
to behave differently so for example
Azure storage the official NuGet package
from Asher if I use that from c-sharp
has a retry logic and here's a little
bit of code where you can see when I
connect to blob storage with a blob
client one of the properties that is
hanging off that object is the default
request options and I can customize the
request options to say my location mode
is primary than secondary in other words
I want you to try to go to the primary
storage account location first and if
that fails then try the secondary one
and actually you have to do that if you
want to have the 99.99% uptime guarantee
like yourself says you have to try the
secondary I can also specify different
retry policies that is just a simple
interface if I want to say specify a
custom retry policy but this linear
retry which the default is exponential
by the way we'll just try it fails okay
let's wait 500 milliseconds we'll try
again
we'll try that for a maximum of three
attempts before we give up for the
entity framework entity framework and
any framework or both provide an
extension method enable retry on failure
which is highly suggested when you're
running inside of a sure just again
because of the transient network
failures that occur so the entity
framework knows about the specific types
of errors that it might get back from an
azure sequel instance where it makes
sense to retry that so if you try to
insert something that violates a
constraint that's inside of a database
table and the error comes back about
that entity framework smart enough to
know well if I try that again I'm just
gonna get the same error message but if
it tries something that gets some sort
of network related error then it knows
okay we will wait for a period of time
500 milliseconds we'll try that again so
those are some of the things that you
can do that are really easy to have a
more resilient service now let's talk
about graceful degradation this is where
things get a little more interesting I
want to use a document database Cosmos
DB whatever it's called this week as a
sequel blob storage I'm not making that
connection the connection has completely
failed now what I do now what do I do
well if it's a read operation perhaps
you have a cache with some older data
inside of it that you could read from
instead and this is where but you know
there's a cost for provisioning
resources in Azure if I want the the
better uptime guarantees I pay a premium
for the better hardware the better
services from Azure this is where you
start talking about the cost of the
software that you build so now inside of
my code instead of having something that
simply always just tries to go to the
database now I have to think about
situations where okay if the database
failed
let me try
other situations so I started having
some branching logic and that now only
increases the amount of effort that you
spend in building your software it
obviously increases the amount of effort
that you spend debugging things when
something goes wrong and testing things
to make sure you have all these
different conditions covered and testing
that this testing that this actually
works so what happens when I turn off my
address equal instance you know I want
to understand how the application is
gonna behave in that instance it also
depends on the type of system you're
building obviously - and that's where
you know I can't give you specific
advice about exactly what to do but I
can tell you that if you're building
something that involves financial
transactions or stock quotes it might be
a bad idea to serve up stale data
obviously but if you're building some
sort of social media site and someone
updates the user profile and someone
else can't read it right away so we just
show the old user profile that was in a
cache that's perfectly fine
what about writes to that database well
maybe if the database down maybe if the
database is down I start using a cueing
system instead or maybe I'm always using
that cueing system and I have a backup
for the queuing system but one of the
things that I want to point out here is
that one of the best design patterns
that you can have for resiliency has
some other benefits to a cqr app so
separating the model that you use for
reads from the model that you use for
writes because you might want those
things to take two different paths
through your code use two different data
access techniques for example any CQRS
fans good let's talk about a slightly
more sophisticated scenario so now I
have customers I have an app service I'm
using a storage account using CQRS on
fire firing things into queues firing
things into blob storage I'm using that
Redis cache I'm using Azure sequel
search services app insights key bolt
all these wonderful services what I want
to talk about is a couple different
situations in that one whoops now let's
do this one instead
interesting let's talk about geo
distribution actually said I have an app
service I have a web application I want
to deploy that into multiple data
centers and that there could be various
reasons for that
I might went to deploy a web application
into an Australian data center and a US
data center just so that my customers
from Australia have a server that's
really close to them and the latency is
low and the application performance is
really good and the US customers can use
the US server that might be one reason
another reason is I might need a
specific instance of my application
running in a region for compliance
reasons so inside of the EU I might want
a specific installation of Azure sequel
and my app service to keep all the
information about Eun side the borders
there what I can do with a is I can take
the code that I deploy into Azure and I
can create multiple app services and
deploy the exact same application into
each of those app services and then
figure out how which app service I will
send my customers to using a service
known as Traffic Manager Traffic Manager
is one of the load balancers in Azure
and I do just want to talk about load
balancing for a second because there's
there's three different load balancers
in Azure and sometimes that gets a
little bit confusing first if you go
into the portal one you search for load
balancer one of the hits that will come
back is the azure load balancer the as
your load balancer is a level three load
balancer so it operates at the tcp/ip
level it's typically used in front of an
availability set for virtual machines
because what you're doing is you're
distributing TCP or UDP sessions across
a number of machines another load
balancer that is in Azure is the azure
application gateway this is an
application layer it's layer 7 so it
understands protocols like HTTP HTTPS
and WebSocket so it's a slightly more
sophisticated you could say load
balancer this is the type of load
balancer that is used in front of an app
service because I want I need a load
balancer that is actually looking at
incoming HTTP requests and looking at
headers perhaps for cookies to implement
features like
sessions you know once the customer has
established a session on one of my
servers I always wanted to come back
there which is not typically something
you want to do for resiliency and
actually if you create an app service
with multiple instances of the plan so
you have multiple servers running and
Azure currently there's a feature known
as application request routing that is
owned by default application request
routing is this load balancer injecting
a cookie into the HTTP messages to make
sure the client has a sticky session I
suggest you go in and turn that off and
not try to use in memory sessions
there's much better options that are
easy to use these days and then there's
Traffic Manager which I want to take
spend a little bit of time explaining
Traffic Manager actually works at the
DNS level so with traffic manager what I
will do is set up traffic manager with a
set of rules that tells traffic manager
how to connect my customers to one of
these app services that I have deployed
could be you know 10 different app
services and 10 different countries so
when my web browser or my HTTP client or
my operating system goes out and says
where do I what what IP address do I go
to for NDC Sydney com how these rules
configured in traffic manager to say oh
because that's coming from an Australian
IP address we're going to give you back
an IP address that goes to the
Australian data center that instance of
the app service and there's different
rules that you can configure but this is
DNS resolution basically it's load
balancing or sending my customers to
specific locations by giving them back
an IP address the way this works is
let's say inside of azure i have an
asp.net application running in the east
us and the west EU so east US and west
europe those would have two different
domain names so it might be music store
dot music store or music store - us calm
and music store - EU dot-com what I can
do is go in and create a traffic manager
profile I can say I want the domain name
that my customers use to always be music
Dork manager net and yes of course you
can purchase a custom domain and put it
in there so it could just be music store
calm you can purchase SSL Certificates
that get bound to that and then with
traffic manager I pick a routing method
so when users come to looking for this
site looking for that IP address what
method do I use to determine we turn the
IP address that they're going to use
what routing method you can use as
performance which is simply given the
current network situation traffic
manager will try to determine given
where that user is coming from where can
we send them that has the least amount
of latency there's also a weighted
routing method this is useful useful
useful for a couple different situations
I could just wait things evylyn evenly I
could say 7:30 my 50 excuse me I'll slow
down for a second send fifty percent of
my traffic to this one send the other
fifty percent to this one you can also
do interesting things like if you want
to roll out a new version of your
software and just sort of test it out in
production for a little bit to make sure
things are seen you can use that
weighted traffic manager profile to say
send 95 95 percent of my traffic here
but send just 5 percent of it over to
this other app service so I can see how
the haze before I update the other app
service with the latest features and by
the way you can have nested traffic
manager profiles so I can have a traffic
traffic manager profile that points to
another traffic manager profile that has
another set of rules to do the same sort
sort of things you can also do priority
which is a good routing method if you
want to have some sort of failover so I
can say this app service priority 1 this
app service priority 2 request won't go
to this other app service until traffic
manager has determined that the priority
1 app service is in trouble for some
reason and by the way this doesn't have
to be just app services I should point
that out with traffic manager you can
basically go to any endpoint so I can
have things going to app services and
azure or website on a virtual machine
and azure or website that I host in my
own data center and then there's
Geographic that can be very useful for
geofencing so again the situation where
I have European customers I'm going to
keep their data in Europe
I want to make sure that all my
customers that are in Europe go to a
European Data Center you can set up
geographic rules to do that once you've
created the traffic manager profile then
you can go in and in the configuration
blade for traffic manager profile where
you can change the routing method if you
want you can configure the DNS
time-to-live that's important that's 300
seconds right there
so that means if I if traffic manager
hands out an IP address for an app
service that suddenly goes down my user
could be waiting up to four minutes to
get a new IP address to go to an app
service that's working and this is one
of those settings that's just a balance
between you know how quickly do I want
things to failover versus how many DNS
requests do I want my users to be making
I think you can specify a minimum here
of 30 seconds but obviously then it's
aggressively always asking for for IP
addresses this end point monitor
settings so what we'll see is that I
need to tell traffic manager about the
different end points where I wanted to
send traffic and then each end point
that I provide I have to provide
basically a URL the traffic manager can
go to to figure out if that particular
resource is healthy so the path could be
something like slash status a good
recommendation would be to create a page
or a route somewhere in your application
that is dedicated to just returning the
the health of the system and if you've
returned anything anything other than a
200 response traffic manager assumes
there's problems so you might have a
dedicated status route that just checks
do I have database connectivity do I
have blobstore just everything online if
so return to 200 otherwise return to 500
and traffic manager will realize that
there's a problem there and that path
has to be the same for all of your
endpoints which are simply for a traffic
manager profile I go in I add endpoints
I say that this is one app service this
is my maybe something deployed locally
but these are the two endpoints that I
want you to balance the traffic between
so it's easy to use and effective let's
also talk about load leveling so another
thing that you have to worry about to
maintain high availability is when the
crushing weight of a spike of requests
comes into your system
how do you make sure that that doesn't
overwhelm back-end systems you know
there's a lot of resources in Azure like
Azure sequel and cosmos DB when you
provision them you have to provision the
request units or the data transfer units
basically how hard are you going to
allow those things to work and it's easy
to end up in a situation where you have
a spike of traffic where suddenly you're
hammering sequel server as your sequel
so hard that you are exceeding the
number of dtu's that you've allocated
for that particular sequel server
basically how much performance you're
allowing for that sequel server and of
course you pay for better performance so
how do you level things out I've been a
big fan of service bus and azure it's
worked really well there's really two
forms of queuing and azure there's
queues associated with Azure storage
which are very simple and
straightforward and easy to use
there's also queues as part of service
boss which are also simple to use but
service service bus provides some
additional features like topics which is
basically a pub sub mechanism so I can
drop a message into a topic and have
multiple listeners if need be or I can
use queues and drop a message into a
queue where one other service or one
other thing will pull that message out
and read from it and service bus by the
way easy to use I can create Hugh's once
I've provisioned a service bus namespace
configure a maximum amount of size
configure how long a message should live
in that queue do things like enable
duplicate detection and evil sessions
and able partitioning partitioning is
useful when you are creating an enormous
number of messages more than a single
node would be able to handle so you want
to partition those things across
multiple nodes we'll come back and talk
a little more about partitioning what
service bus also basically easy to use
this is the official service bus bus SDK
where I have a cue client that I'll
create I create a new instance of a
brokered message where I can put some
content inside maybe I can Jason
serialize a complex object put it inside
the brokered message and then just send
things off to service bus in auditioning
in addition to load leveling there's
also load throttling one of the things I
had experiment with is if this company
is going to provide a public API that
people
newse how do I prevent people from
abusing that API how do I prevent people
from making too many requests or writing
some sort of have a bug in their script
that just is sending millions of
requests to my API service every minute
there is a service inside of azure News
API manager what API manager can do is
it basically acts as an API gateway it
acts as a proxy what I can do with API
manager is created I can point it to the
different services that I have which
might be in different app services or
scattered across different virtual
machines and I can present a unified API
to my clients I can also present
something that they have to register
before they can use and I can also
easily implement policies like I'm only
going to allow you five calls a minute
if you're on the standard plan but and I
can have a premium tier for my API that
allows them a hundred calls per minute
but they have to pay more for it so if
you're writing an API that exposes
metadata either through swagger or
wisdom it's easy to go into this API
manager and say and point it to your
metadata end point and it will figure
out what operations you have available
and this would be creating that API
management service so what's interesting
and interesting about this sorry if
that's blurry it doesn't matter how many
api's I have how they're implemented
where they exist in the world my
customers can always just come to let's
say Scott Allen API dogger API net or
whatever custom domain I have and I can
give them access to everything
everything that I have in matter how its
deployed or where it lives and you can
see the different pricing plans and the
different limitations here but once I
set that up I now have two things I have
a management portal available to me and
I have a developer portal so when
someone from another company says hey I
want to sign up to use your API how do
we get authorization tokens how do I
register for it as your already provides
an API they can come in they can
register with a username and password
they can then see my the documentation
for my API they can then manage their
own access keys so it's just like when
you go to
use Google Maps or Bing Maps as a
developer and you have to register there
to obtain an access token an API key
that's all built for you
this is documentation that they would
see it just based on your swagger or
wisdom metadata that API manager gets
this is an example of how I can define
products for my api's and I can place my
API into different products so I might
have a starter product that is free you
have to subscribe to use it but you'll
only be able to run five calls a minute
and a maximum of 100 calls per week or
you can pay more money to get the
unlimited product have unlimited access
to my API so this is just a good way to
control your API to be able to monitor
it to make sure people aren't using it I
won't go into too much detail here but
inside of API manager you can set up
policies policies are where you define
things like the rate limits it's all
done through XML and there's some simple
policy statements here that you can drag
and drop into the policy and you can
define policies have a product scope at
a specific API scope or on a specific
operation and again this is generally
useful not just for load throttling but
also building API is where the
developers of the API might not
anticipate what the customer wants for
example things like cross domain calls
or client might want Jason instead of
XML
since API manager is basically a proxy
it can do some of those things for me
can add pores headers to the responses
automatically for me even if the
original service doesn't provide it now
let's talk about something completely
different
I mentioned app services I mentioned
virtual machines let's talk about
service fabric for a Mon there's a
couple different ways that I could build
a movie store application in asp.net I
could just create everything in one
project or I could take this micro
service approach where I want the web UI
to be a service and the payment API to
be a service and the identity management
for my application to be a service and
of all these little different pieces and
that I went to build separately and I
want them to be able to be deployed
separately and updated separately but I
still want them to function together
some sort of complete application what
technology can I use for that that's one
of the scenarios where service fabric
can be interesting so I want to give you
two different perspectives on service
fabric and first of all tell you that
service fabric is actually not tied to a
shirt in any way yes you'll see as your
service fabric but there's just some
features that are built into Azure that
allow you to provision service fabric a
little bit easier so two different
perspectives first let's take the
runtime perspective the runtime
perspective to service fabric is that
service fabric is a technology that is
designed to run services in a cluster of
machines so it understands that if you
give it five machines to run on and you
describe the services that you want to
run they're all those little services
that I had on the previous slide you can
describe where you want those things to
run how many instances how they should
be distributed across the cluster and
service fabric we'll just make sure that
for example service a perhaps we want
that to run on all five of these virtual
machines and they don't have to be
virtual machines again this is a
technology or if I took the five or six
laptops that are sitting around doing
nothing in my basement and networked
them together I could install the
service fabric runtime on those laptops
and set up a cluster but you could also
set up a cluster in AWS and you could
set up a cluster in Azure so there's a
runtime perspective that service fabric
has which is basically I'm gonna manage
this cluster for you and I'm going to
try to make sure that services are
always available so if one of these
nodes fails if there's a catastrophic
storage failure or something goes
offline because a network card fails I'm
going to make sure that I take let's say
these services and if it's not running
another instance somewhere I'll make
sure I move that service and get it
running on another instance in this
cluster to make sure it's always
available for you
that's the runtime perspective to
service fabric and I'm just giving you
the high-level overview we could easily
spend like four hours on service fabric
then there's the the programming model
aspect of service fabric
so how do I write a service to run it
inside of service fabric service fabric
has a couple different concepts first
they have the concept of a guest
executable let's say I write an
application that's an asp.net core or
two node.js application and I want to be
able to run it in a cluster of machines
without my application having any
knowledge of service fabric you can do
that you just have to register a guest
executable with service fabric
tell-tell service fabric basically how
to start your application do I run a
script or run you know dotnet run what
is it and service fabric will copy your
executable to the clusters that where it
needs to run and execute that make
command to make sure that service is up
and running so that's one approach to
use using service fabric where your
application has no dependencies on
service fabric itself the service fabric
runtime you're just a guest in that
runtime then there's what service fabric
calls reliable services so these
services are services that you're going
to reference some runtime components of
service fabric and you're going to
describe the service fabric when you're
starting up and service fabric is going
to tell you when to open a connection so
your application and your service has to
be able to interact with service fabric
at a deeper level and there's different
types of reliable services so there's a
stateless service and I'll show you code
for this in just a moment so it makes a
little more concrete a stateless service
is a service that you register with
service fabric and yes service fabric
controls its lifetime it understands
when you're starting out but your
service doesn't have to store any state
internally you can still use sequel
databases or document databases or
whatever you want to use to store state
but service fabric doesn't have to worry
about managing that state your
application manages that state or a
database manages that state and then
there's staple services so these are
services that where service fabric
provides you with a set of data
structures known as reliable collections
so there's things like the reliable
queue and the reliable dictionary what a
stateful service can do is if it wants
to hold on to a specific piece of
information let's say about a customer
it can ask service fabric for its a
reliable dictionary of a certain name
and insert that data into that
dictionary so what's interesting about
the stateless service is two things
first of all those collections are in
memory
so they're incredibly fast if you don't
want the latency in the overhead of
going to a sequel server database every
time you need a piece of data about a
customer then a stateful service can be
blazingly fast because all of that data
is in memory the second thing is all of
that data also gets saved to disk so
that data is durable so in addition to
being on them in memory if something
catastrophic fails there's still that
state this on disk for when the node
restarts and it's also replicated
throughout the cluster so if a single
node does go down we don't lose anything
there was a third thing I wanted to say
and it completely escaped my mind but
I'm sure it'll come back well it would
be the third thing actually was said it
was replicated so the state is
replicated and then there's also what's
named is a stateful actor which I've
seen a number of people who implement
things like real time things like online
games and online auctions this is the
kind of situation where I want an
instance of my service per each user so
I go into an online auction to start
setting up bids the code that's running
inside a service fabric is going to
create an actor that represents me it's
gonna put it on one of the nodes and
anytime I do something to modify my
state or my bids or the things I'm
selling service fabric we'll be able to
locate the actor object that's in memory
somewhere associated with me and invoke
a method or change the state or whatever
needs to be done and if I'm not present
on the system I stop using it for 20
minutes service fabric and it also knows
how to spin down that actor and bring it
back up an hour later when I come back
in so to make this a little more
concrete about what service fabric is
let's first let's first do this when
you're developing with service fabric
you can install what's known is the
service fabric local cluster manager
that's a little orange thing that
appears down here in my taskbar I can
right-click that and I can say there we
go I went to run a one node cluster or I
actually want to simulate a five node
cluster here on my development machine
and if I go into task manager I would
actually see five copies of the service
fabric executables and services running
on this
because I'm simulating a five node
cluster but if I just want things to be
a little simpler I could do a one node
cluster and there's an option here to
manage the local cluster so this is
telling me that I have in this cluster
which just happens to be on a single
machine I have five nodes but five five
machines are running five nodes I have
one application deployed to this cluster
you can deploy multiple applications
there's two services running there's
just two services in my application and
we'll talk about partitions and replicas
here in just a second but this
application movie store type is just a
web application an asp.net core web
application that's distributed in that
cluster and that people can reach with a
browser just like you would with any
asp.net core application but this
particular asp.net core application was
not implemented as as a guest executable
it was implemented as a reliable service
for service fabric which means it looks
just a little bit different than your
typical asp.net core application so if I
come into program dot CS if you've done
any asp.net core programming typically
program dot CS and the entry point for
your application that's where you create
an instance of the web host builder and
you start firing up cuss true and you
start listening your ports and all that
stuff well service fabric that's not
what you want to do what you want to do
instead when you start up is go out to
service fabric and say hello you've
started this thing up and this is your
web type service so that could be any
arbitrary name there but you're
basically telling service fabric hey
this is the name of the thing that's
starting up and when you need to create
a new entry when you need to get that
service running just fire out this
little bit of code that's in a lambda
expression basically instantiate this
class and that will contain all of the
boot up logic for my service so an
asp.net core and start of instead of
starting to listen for HTTP connections
inside of the main entry point I instead
have to wait for service fabric which is
now coordinating everything
to start things up and it's inside of
this web class where you will find
things like the web host builder let's
use a web listener or use kestrel and
configure our services and use the
startup class and all those things you
would see in core but notice that this
happens inside of a method called create
service instance listeners so service
fabric does want to be in control of
your application it does want to tell
you when you should start listening to
things and actually please return to me
a data structure that I can use to
interact with whatever you're using to
listen for network connections which
could be HTTP or TCP whatever you want
but this basically starts my web
application running and again it's an
instance of a stateless service so I
derived from a base class to use this
with service fabric there's another
excuse-me service that is running inside
of this cluster it's a service that the
web application uses and it's just a
simple counter service so we're going to
spin up also in service fabric a counter
type service that will start running
when we invoke this class and this class
excuse me is a staple service so a
stateful service is one of those
services that service fabric can
distribute throughout the cluster that's
actually going to hold steep in these
reliable collections so for example what
this service is going to do something
very simple just increment a counter
every one second I'll show you that code
in just a second but from the web
application I want to be able to go to
one of my counter services and say hey
what is your current count you know that
would just be like let's look up this
customer and find out their first name
right in the key in this case I'm just
doing something a little bit simpler
which is to get an integer value or a
long value what does that look like well
since I've derived from this stateful
service based class I inherit a property
called state manager and it a state
manager that allows me to interact with
reliable collections so I either want to
get or create a new reliable dictionary
of string and long called whatever name
my dictionary
and once I had that dictionary yes these
reliable collections are durable they
respect transactions so if I write
something into it and it fails without
me committing that'll be backed out
we don't necessarily we'd if I remember
we do need transactions even for read
operations here but this is basically me
saying hey give me the counter value out
of that reliable dictionary and what I
want to do is return that to whoever is
calling this operation what's a little
more interesting is how we increment
that counter so when you write a service
for service fabric you can implement an
implement a method called run async and
in this case what run async is going to
do is just set up an infinite loop so
while true and execute this code every
one second every one second it wants to
fetch the counter value and then
increment it so again go to the state
manager create a transaction try to get
the counter value in an asynchronous
manner we'll log some events here then
we will try to add or update a counter
value that has been incremented and
commit that transaction so that counter
value is stored in memory that counter
values persisted to disk that counter
value is replicated across a couple of
other nodes I can configure how many
nodes it's replicated across and I do
want to point out one of the tricky
aspects of service fabric is that when
you implement stateless and stateful
services you have to think about how
you're going to partition your data if
all of your data can fit on a single VM
because remember these this collection
has to fit in memory so if I provision a
cluster of five machines that have three
and a half gigabytes of memory there's
certain physical limits on how large my
collection can be on that machine if all
my data let's say is 500 megabytes I
don't have to worry about things too
much but if I have 10 gigabytes of data
I need to figure out how to partition
this service so that my data is spread
across multiple nodes in the cluster and
multiple instances of the service and
there's ways to describe the service
fabric how to do that I just want to
show you something which
is a configuration file which you need
to provide the service fabric this
application manifest where I can say
things like the counter service I wanted
to have a partition of 1 or 10 or 20 or
a hundred you know how many instances of
the service do I want running across how
many different machines and I can also
describe how to partition thing so you
can partition things by hash codes you
can partition things by just simple
integers let's say you're building an
address book for the world and you need
to partition the names of everyone on
the planet you might choose a
partitioning scheme that uses the first
two letters or the last name something
like that but you have to spread these
things around right let's see how we can
use that counter service from our web
application just want to show you that
bit also because it's interesting let's
go to the home controller I just inline
this code so when the user comes to the
index action for this particular web
application I want to go out and contact
that counter service actually all the
counter services that might be running
which is actually kind of rare typically
you just want one the partition that
holds the data that you want how do i
how do I communicate with other services
inside a service fabric there's a couple
different ways to do it one way to do it
is using domain name resolution services
that service Patrick provides and just
using HTTP or something like that but
you can also use this technology that
service fabric calls remoting which is a
sounds like remoting from the old days
of dotnet which everybody really liked
that much but in a way it's similar
first of all inside the service fabric
cluster I have pseudonyms for my
services I you know if I want to get
access to this counter service I don't
want to specify an IP address or a
machine name or anything like that so
I'm basically gonna build a URI that
says hey on the fabric in the movie
store application because there might be
multiple applications I'm gonna find the
counter service that's what I want to
connect to and I can use this URI and
call a service fabric API to say give me
that service
and what I will do also is on this
fabric client that I've injected which
gives you information about your
environment there's something called the
query manager where I can basically say
for this service this counter service
give me a list of all the partitions
that are running in this cluster and
what this is basically just gonna do
this little loop here is going to loop
through each partition that's available
it's going to use this class called
service proxy to go out and create or
it's not actually gonna create well it's
gonna create a proxy but it's gonna you
could get a reference to one of those
running instances of the counter service
that's out there given a particular
partition key and I can get the count
value that's inside so now I know if
it's a four or five or 100 or something
like that I'm just going to collect
those together inside of a dictionary
and spit them out on page so I do want
to show you over here in the service
fabric Explorer inside of the newbie
store there's the two services that are
running my counter service which is
stateful and partitioned and the web
service which is stateless because
because it just uses the stateful
services to store everything that it
needs and I can see inside of here
things like where where this is running
now in a real cluster let's say that's
running on a sure this would I would
probably run a stateless server on
everything in the cluster every machine
just because it's easy to do that
counter services that are partitioned
this is showing me there's three
instances of the counter service because
it's been partitioned each one holds a
certain range data for a range of values
partition keys and I can see the primary
for this particular service instance is
running on node 3 in my cluster so
that's where all the requests would go
to when I want to get the count from
that particular instance they would go
to node 3 but just in case there is a
disaster and a machine blows up that is
also replicated all node 2 and node 4
and so I've actually come out here and
look at the nodes that are running it's
not visible in the back try this
let's go out and look at node 3 and
there's a little option here - let's do
a restart on that nude node 3 let's come
back up here what service proud fabric
should detect here in a moment is that
something is wrong in the cluster and
what it should do if we watch here
hopefully it'll happen there we go the
primaries is now node 2 it's a service
fabric switched when it detected that
node 3 was off line went to a new
primary it'll bring node 3 back up on
line here in a bit and that'll still be
an active secondary but you can see the
question worked there right and that
service fabrics whole purpose in life
make sure everything is up and running
and always available no matter what
you're putting out there another
interesting thing about service fabric
that I want to point out down here at
the bottom they have a chaos among chaos
monkey built-in so service fabric itself
sort of a collection of micro services
that run on the cluster there's a
cluster manager service the DNS service
the field / service is the fault
analysis service is actually interesting
it's up and running in there and it
provides an API that allows you to take
notes offline restart them destroy
things which restore things it's one of
those things that you might want to use
when you want to test how does my system
behave when an ood goes offline make
sure that everything works properly and
finally what does this look like in
Azure or what does it what does asher
offer for service fabric some people get
a little overwhelmed by the
configuration but you it's not too bad
once you've gone through it a couple
times and hopefully you'll automate it
right so you can do this all the time
basically with Azure you can say yes I
want a new service fabric cluster here's
the name this is the URL that my users
would use to connect this is the
operating system that I want to use this
is going to go out and provision the
virtual machines for you could be v
could be a hundred it could be a
thousand virtual machines they're all
going to have the same username and
password they're going to be set up
inside of azure in something known as a
virtual machine scale set which just is
something
that makes it easy to manage this pool
of identical virtual machines hundreds
of them if need being once you get
through that simple configuration you do
some of the standard things that you
always do in Azure like choose a size
for the VM oops
you might want to go with a 4 core 14
gigabyte version because you want to
have reliable collections and have a
little more space to breathe inside of
each node for example and you can set up
things like Auto scale all of that's
available once you complete that
everything is provisioned for you and
you can go in and manage your cluster
and deploy to your cluster there before
I leave that slide there's a couple
things about service fabric I evaluated
it quite heavily at the beginning of the
year mm-hmm and I will tell you we
decided not to use it we decided to
stick with app services I think service
fabric is a perfect technology for some
specific types of applications
particularly applications to deal with
real-time data applications that need
really low data latency things like
games and online bidding sites and
things like that but I can also tell you
that trying to get a development team up
to speed on this takes a little bit of
time because it's a different
programming paradigm it's a different
way to think and I think the
partitioning is particularly tricky you
have to kind of figure out up front
using part art and part science how big
is my deed are going to be how am I
going to partition it what sort of
strategies I'm not going to use there
but it is certainly a technology that is
quite useful
I have heard a statistic that's
something like 60% of the cores in Azure
total are running service fabric because
a lot of the azure services themselves
are built on top of service fabric as
your sequel Cosmo DB a lot of the Bing
things and Cortana things run on service
fabric so it's certainly a technology
that Microsoft dog foods and uses itself
a couple obvious things if you want high
availability you have to automate I have
this rule with the development team that
I work with if you're going to make a
change in the development environment or
the QA environment or the production
environment I don't care
what environment it is you know you're
not allowed to use the portable I want
to see a script for that change or an
azure resource manager template as your
resource manager templates are wonderful
that there if you haven't used them they
are JSON files that you can use to
provision anything in Azure you want to
create an app service a virtual machine
a sequel database you want to buy a DNS
name and attach a SSL certificate you
can do all of that with Azure resource
manager templates but they are also one
of those technologies that can be a
little bit frustrating because they
aren't always well documented sometimes
the best way to figure out what you want
inside of this JSON file to instruct a
sure what to do is to go out and look at
a sample or else do it in the portal and
the portal will allow you to export a
template so if you're new to Azure
definitely look at as your resource
manager templates as a way to automate a
sure do all of your change through that
because you can check those things in
the source control and prioritize them
they work great with secrets like
passwords you can pull them out of key
vol it works very well another thing
that should be obvious is you need to do
some testing to make sure that you do
have high availability availability and
things are behaving well under load we
use the load testing tools to come with
Visual Studio but there's a lot of
different ones out there and monitoring
is also one of those things that you
have to have asher provides some pretty
interesting monitoring tools and they
provide services like application
insights if you haven't used that i
would recommend using that for some of
your diagnostic sinks but for complex
systems the monitoring that asha
provides you know it's not customized
for your domain and for your business
and for large systems I would recommend
also taking a look at building a custom
monitoring portal of some sort where you
pull information from application
insights you pull information from some
of your application audit logs to really
be able to drill in quickly on how your
application is actually performing and
what sort of load it's experiencing it
saves a ton of time because when things
go wrong and you find yourself clicking
on things randomly in the portal trying
to find something that tells you exactly
where the problem is it's very
frustrating and when you're doing that
under
crusher it just makes real bad day I've
been there I have more information in a
more structured format than this
rambling presentation in my Pluralsight
courses I think I have 14 hours of
courses covering Azure including some of
the topics here and you know a little
bit better explanation of service fabric
so feel free to go there if you'd like
like more information you can also email
me or contact me on Twitter and I thank
you for attending and hope you have a
good day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>