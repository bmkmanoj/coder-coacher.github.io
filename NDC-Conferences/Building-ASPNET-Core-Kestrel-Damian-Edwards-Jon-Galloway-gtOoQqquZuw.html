<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building ASP.NET Core Kestrel - Damian Edwards &amp; Jon Galloway | Coder Coacher - Coaching Coders</title><meta content="Building ASP.NET Core Kestrel - Damian Edwards &amp; Jon Galloway - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building ASP.NET Core Kestrel - Damian Edwards &amp; Jon Galloway</b></h2><h5 class="post__date">2016-08-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gtOoQqquZuw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome friends we're here to talk about
Kestrel
I am Damien Edwards I'm John Galloway
this is not David Fowler
I'm not what happened I I was hoping you
wouldn't notice I was gonna try and kind
of speak with the gentleman what it we
can don't let him hear I think the beard
gave it away we're going to talk about a
spinet core Kestrel which is the fancy
web server or application web server
that is inside asp.net core or you can
use needs met core and we're
specifically going to look at a bunch of
the things that we have to do to make it
fast
previously a spur net has been there
it's been okay it's been fast enough for
most people but and some things that
happened over the past few years that
have made us care about these things a
little bit further before I jump into
that just go back to the first slide for
a sec I just need to call out that I
didn't put these slides together John
did a little bit of work on these slides
last couple days but most of this has
been put together by a couple of the
developers on the HP net core team
Caesar and Stefan so big shout out to
them and the benefit is they've written
a lot of this code and they understand
it right the but then I don't really I
didn't write anything I don't actually
understand it but I'm gonna I'm gonna
explain it to you anyway because it's
fun trying to explain things that you
don't know so what is Kestrel Kestrel is
a web server as I mentioned for a spinet
core it's currently based on libuv and
we'll talk a little bit about what libuv
is later on some key points it is open
source like virtually all of a spitter
net core is and we can prove that
because we can look at the repo which we
handily have available already in a tab
yeah and you can see that of the top
contributors Stefan halter who was one
of the guys who I just called out on the
slide is the top one but very closely
behind who actually has more deletes if
that means anything is Ben and Ben
doesn't work for Microsoft he works for
Illyria games yeah and so he's one of
the chief contributors and he's actually
just a dude from the community I mean
I'm not just a dude an amazing developer
is done some great work for us I'll look
there you are look
I did write some of this card I even
deleted 28 lines I was just about to all
tab away I'm glad so yeah it is open
source and we've had some really great
contributions to it and we invite many
many more we really want people to to
make this as good as can be it is cross
platform as a spinet core is
cross-platform by default you will
always be using kestrel when you use an
a spinet core app it is the default
server even if you're running on is you
are still using Kestrel Kestrel is the
server that runs in your process and
then you may have is orange NX or H a
proxy or whatever your preferred reverse
proxying server is in front of that okay
we do not host in process in is it is
written in managed code okay so it's a
managed web server but libuv is a native
components written in C but kestrel
itself is all written in C sharp okay
doesn't necessarily mean it's easy to
understand though I'm a web developer
and I've written a lot of c-sharp but I
haven't written a lot of c-sharp that
looks like this C sharp so we're gonna
have some fun looking at this c-sharp
and it is a NuGet package it's not
built-in it's not an ex e you don't go
to the command line type kestrel and
then you pointed in assembly with a
speed on their core the server is part
of your app you pull it in as a library
think it's a little bit like how we did
stuff with owen before it there is a
contract the a spinette core has an app
func and then the server has a has to be
recalled the app func it has to be able
to pass in requests and handle responses
using a bunch of contracts and that's
pretty much it it's really just c-sharp
code it's really nothing special you can
boot up kestrel from any executable
assuming it's dotnet okay you make a
console app and you say bring in the
kestrel package yep and bootstrap it no
way and away it goes absolutely so right
now though you generally only use it
it's factor to use with a spinet core
you can't really use kestrel by itself
we've talked about whether we make
kestrel available as a stand-alone kind
of suckered server thing but we haven't
got there yet
right now you use it in with a snit core
so i talked a little bit about some
things that changed you know a spin it
was ok this is not slow but it wasn't it
runs stack over run stack overflow I'm
with a bunch of other layers in front of
you but you know you can make pretty
much any server be fast but
this was an interesting motivation Kelly
summers who doesn't want a bunch of
wonderful work in open source in the
community she has this service she wrote
called haywire which was written in C
and she uses libuv as well which was so
it's a nice sort of analogue and she
tweeted this what a year ago only a year
ago is only Toth of July so it's not
that long ago really and she said oh you
look I drank Castro on this Z on bla bla
bla bla bla and I got 750 requests per
second which is shaped and haywire which
was her server on a Raspberry Pi berry
pie probably not even version 2 or
vision 3 probably the original one yeah
with a hundred Meg network was getting
72,000 requests per second so I was like
hmm that's probably not good we should
probably do something about that and we
might turn to the team and said you know
why is this so it's a we've never really
cared too much about before now there
was actually a bunch of reasons why she
was that bad while I was getting that
bad a performance when Kelly wrote ran
that test and when she ran some other
tests afterwards it wasn't quite that
bad but this was an inspiring tweak and
so some things are beyond that that have
happened the tech and power benchmarks
anyone heard of the tech and power
benchmarks okay we've talked about a
little bit if you've watched us on the
stand up or seen some of the talks
before this is a set of industry
benchmarks that have been run by the
tech empower group they're up to round
12
now they've done 12 rounds they have a
set of discrete scenarios that you can
see here so this is fortunes which is
the name of one of their scenarios that
does database access and HTML rendering
but they also have very simple ones like
plain text which is effectively HTTP
hello world okay I'm sorry but they have
data updates and JSON serialization a
bunch of other stuff but they're all web
benchmarks their benchmarks designed to
test the overhead involved in your HTTP
server and or HTTP framework like your
web framework so asp net isn't in round
12 but if you go back to round 9
i think is the last one we are in so if
you go to previous rounds to click all
the way to previous rounds and get
around 9 yeah and then you scroll
plain text we're not at the top no we're
not and we're not in the middle which
only leaves one place now we're not that
far down let's call her ballet area in
red a spoon eight seventy one thousand
requests per second which equates to one
point one percent of the one that was
leading this benchmark so we are 100
times slower in this scenario as
whatever the one at the top was which
was I don't know what it was at that
point on and my numbers are good in this
test high numbers are good in this test
yes this is a measure of requests per
second thank you for car jump so we can
do better we should do better
you know benchmarks aren't everything
and there are some people who poopoo
benchmarks and say well it's just a
benchmark it doesn't equate to
real-world perf and while yes that is
directly or partially true this
benchmark obviously just being hello
world does not in any way relate to
real-world path however in order to do
well on benchmarks you generally have to
solve a bunch of the problems that do
affect real would purse so in dotnet you
have to think about allocations and
garbage collections if you think about
latency which is if you're a web
developer you generally care about how
long it takes for your customers to see
stuff painted in the browser right the
time between when they click the link
and when you see that happen is latency
or time to first byte solving those
things even for the purpose of
benchmarks will have direct impacts on
your real-world applications and we have
seen as a result of the work that we're
going to talk about and other work we've
done in a spinet core and some of the
higher-level stacks like MVC we've seen
real performance benefits from people
who move from MVC 5 to asp.net core both
in really big application of that guy
Ben Adams they're running a a real time
like gaming server but then also people
just like tweet us like hey I just
updated my blog yeah and it's like
memories gone like this and it's way
faster and all I did was update yeah I
mean there are benefits even if we're
chasing what is ultimately a synthetic
number ok benchmarks have their purpose
they're not the duo on end-all but
they're they're a pretty good driver to
learn some things and to make some stuff
faster so some very high-level concerns
when it comes to doing dotnet our web
server development anyone who's ever
done dotnet performance optimizations
particularly with servers will run
into the issue of the garbage collection
okay and then a you spit out net
previously there were a lot of different
tricks that the the.net use in order to
make server applications garbage collect
better do it better than say a WPF
application or desktop application
because they did very different app
profiles right in a web application you
have an app that's running there for a
very long time with lots of discrete
requests that don't really share
anything with anything else there's a
WPF application you might have a tree
that's alive for a very very very long
amount of time you're not particularly
churning that memory over and over and
over again
the problem with GC is that when garbage
collection takes place it impacts the
rest of the application a lot of the
time dotnet has to freeze things in
order to do the bookkeeping associated
with memory management so it's good if
we can reduce the pressure on net so
that the garbage the garbage question
happens as little as possible and then
when it does happen it happens as
quickly as possible hopefully without
any of those costly pauses because the
last thing you want is for your
application just to stop and not serve
requests or hang requests because it's
doing a particularly expensive TC so
what do we do we manage a lot of our own
memories so rather than just allocating
willy-nilly whenever we need something
we allocate large blocks of memory
upfront and then we use those over and
over and over again and we pin them in
memory we told net please don't move
this memory around so that the garbage
collected never has to worry about that
move that memory moving around becoming
fragmented we reuse known strings
strings are super special internet
there's a lot of things with the way
that strings work in net that makes them
useful like some easy to use make some
interrupts well with other parts of
Windows but they're expensive because
they are what's the word I'm looking for
immutable so once you allocate a string
you can't change it which means if you
have to change the string you get
another one right and you see this it's
one of the first things that net
developers learn is that or if you come
any strings use a string builder right
because you don't want to do all these
allocations in a non performant way and
so in a webserver
unfortunately the web was built on the
top right HTTP 1.1 is an ASCII based
protocol for the things the headers and
dispatching and the protocol version and
all that type of stuff you may end up
sending bytes down if you're downloading
a file but ultimately a lot of the time
you're just sending a webpage and that's
going to be interpreted as a utf-8 or
something like that ultimately it's a
string alright so if we're gonna give
all that to you as an application
developer we're gonna have to allocate
strings so we need to find smart ways of
avoiding allocating the same string over
and over and over again
so we reuse known strings we consume
incoming data as soon as as possible so
when data is coming in off the wire we
want to make sure that we're gonna read
that off the wire as quickly as we
possibly can maybe even before your
application is ready for it so we will
read it so that we don't stall the pipe
so that the client can keep serving us a
data if they're uploading a file or if
they're sending a request and then we
will do the working kestrel to keep that
that pipe primed and then give you the
memory when you are ready for it and we
have a some interesting data structures
and algorithms for doing them
we're not primarily focused on this word
that I'd never heard of before it's
definitely slides but asymptotic curve
which I believe is something to do with
like peak perform side things or
something like that what we really care
about I didn't do I grew this beard just
for this talk back there they have to
say cuz I fell not going to sign it's
not fully a neckbeard but it's
asymptotic aliyah approaching a
neckbeard the constant per the constant
overhead really matters a lot when we're
doing benchmarking when we're looking at
a very highly performant web server it
sucks if you can do a million requests
and nine hundred thousand of them are
really really fast but the other hundred
thousand take a hundred times longer ok
we're really looking for consistent
performance as much as we can possibly
get when you average is that a
microsecond or a nanosecond or a Pico
second or a femtosecond all right which
one is that one
that's a micros microsecond ok when you
average a microsecond per request every
nanosecond counts ok so some of these
things might look like we're going to an
awful amount of trouble for very small
gains but when you amplify those gains
over millions of requests per second as
it happens in benchmarks they add up and
we'll see some demonstrations of that
and you know an entry and we don't cover
it in here really but you don't know how
they're gonna add up a lot of time tell
you perf test it right I mean and you
guys have a whole / frig yes in the shop
that just does that we have
one actually so we have it's like a dev
perf rig that we have in our team room
that we've automated so that when we do
a pull request we can automatically run
you know per frig's using the app that
you're going to run later and we can see
the see the results of you know a
particular branch or a particular pull
request and then we actually have a
dedicated perf team perform a
reliability team who have what we call
the big iron they have like the 48 core
servers with the bazillion gigabit
network cards and all the rest of it and
we can sort of get them to do more
concentrated runs on particular issues
and find things like that so cool all
right when we look at next memory
optimize you know I was just thinking as
you're saying this but those things
there that list there we manage manage
memory reuse well known strings and
consuming things that really kind of
breaks down to like three different
things that we're talking about that
we've got memory we've got IO and we've
got algorithmic operative optimizations
okay so starting with memory alright
let's look at memory so I talked about
the fact that we allocate and we manage
our own memory okay rather than just
allocating memory when we need it before
we go out which is what you generally
taught to do is a.net developer when you
first start out it's garbage collected
it's memory manager after worried about
this stuff when you're doing this type
of deep perf analysis which you may have
to do on very large applications as well
you need to understand how memory gets
allocated need to understand how garbage
collection works and Hatter the multi
heap generational heap works in dotnet
and so what we do is we pin a very large
amount of memory upfront and the
application starts and we move into the
large object heap so in dotnet there are
multiple heaps so memory is you know we
have a stack which is where like the the
stack frames of the current methods and
the variables and those methods are
stored then we have the heap where
objects and those sort of things live
and then we have more than one heap so
we have a jens ero Heba Gen one heap and
a Gen two he plots with generational
heaps right then we have this other
magic heap called the large object heap
for stuff that's just too big to put in
those other ones you generally don't
want things to end up over there because
I don't think it even gets collected I
think it gets optimized occasionally
someone will yell out if I'm I'm talking
out of my butt
but the large object heap which i think
is like over 83 kilobytes or something
if the object that gets put over in
there it's very special it's put there
so that we don't have large objects
stalling up and fragmenting the
generational heaps that generally turn a
lot more
your application so we allocate a
buttload of memory we stick it over in
the large object and we pin it do you
know technically what that buttload is
is it I think if you actually go to
urban dictionary there's a definition
for buttload and it's smaller than a
crapload but bigger than a shit-ton
so I'm not actually kidding so I
actually don't know I think we allocate
like okay megabytes all right and I
think it might be configurable or if
it's not we're working on making a
configurable now cool and then we reuse
that memory over and over and over again
okay so this allows us by pinning that
memory we allows us to use the normal
net byte array api's rather than just
allocating native memory and trying to
do lots of pinyon Vokoun tore up all the
time we just use standard byte arrays
it's you know normally the idea is you
want like.net to manage things for you
if it's like yeah I don't know it's
probably gonna be this or this but in
this case you know you've got some very
specific information about what sort of
memory you need how long it's gonna last
how you're gonna be pooling it and
that's a good time where you do want to
take them yeah and this isn't this isn't
uncommon for people who are building
servers in dotnet if you're building a
database server like you're actually
building a server not just using someone
else's write people who build these
types of long-running server
applications tend to end up having to do
this type of memory management in order
to manage the GC that happens and we'll
see some impacts of that later on so we
break this stuff up into slabs each slab
containing a memory block so these are
just like chunks all right we break it
up into chunks of memory a bit like this
so we have a big memory pool we we
create a bunch of slabs we usually
create I think if we create the same
number of slabs is the same number of
threads or the same I mean where might
be the number of pools so we have a
number of threads that we use for libuv
right for i/o and then we create a
memory pool for each one of those so
that we don't have threads you know
trying to cop across different parts of
the memory pool and then we further
break that up into slabs and then we
break those slabs up into blocks and we
do all the fancy things like trying to
align the block size to the CPU cache
lines and all the type of stuff that
that we're still you know frankly we're
still investigating and we figure out by
doing real-world testing and then what
happens is while the application is
running these blocks get least something
is done and then we throw them back into
the
pull yeah so yeah we got 128 kilobyte
sides of the slabs we do multiples of
those and then we increase those as we
need more memory when the application is
running and then we do four K blocks to
fit in pages and pages there is meant to
be some alignment with you know memory
pages in the CPU architecture and the OS
and all that type of thing so the first
request causes all that to happen
essentially it's lazily done as soon as
the first request comes in so the
question is there's only this change
between core and desktop no so this is
all kestrel logic and kestrel runs
basically exactly the same with you
running on your framework code on their
core
so we said strings are expensive but
they're very handy and they're what HTTP
is built on and so we have a bunch of
code that we use to kind of read reuse
known string so we'd have to allocate
them over and over again as requests
come in these are really common strings
that you see in HTTP requests who's ever
looked at an HTTP request on the wire
right okay you've seen these bytes come
in over the wire ASCII bytes that
represent either like the protocol
version down here at the end of the
first line or maybe the verb that you're
using which is generally on the first
loop is always on the first line along
with the path and then you have headers
which I'm not showing here these are all
verbs at the moment that you are also
well known right we haven't had a names
that are well-known as well but for
these particular ones which are
generally all involved in passing that
first line of the HTTP request there's
something fairly interesting about these
strings does anyone know what it is
they're all capitals that's not well
that is actually interesting but we're
not going to show the code that makes it
interesting there is actually some
really interesting can do we don't we
show that yeah I mean like there's a
really interesting code that does a very
fast case a case insensitive comparison
but we're not showing that code it's
kind of cool look it up um no these all
are what eight characters or less mm-hmm
right which means that they all fit in
along all right so what do I mean by
that well every one of these characters
is an ASCII byte right which is what
eight bits and if I have eight of them
that would give me 64 bits and how big
is along its
64-bit integer right so I can represent
every one of these strings as a single
number in 64 bits which is nice it means
I don't have to allocate a string to do
a string comparison when I'm looking at
the incoming bytes in the wire and go
well is this H to be 100 H to be 1:1 or
is this again or is this a post I can
just compare to a particular number so
we've got some code I think that shows
this right there it is so this is
actually just getting bytes for known
strings so there's some of the strings I
talked about there some other ones here
which are known headers like connection
keepalive and stuff we allocate these
strings upfront once by calling get
bytes on them and get them as ASCII
bytes we stick them in a static this is
the only time you do really see statics
in a spinet core code we avoid statics
like the plague but for things like this
it's really good because they just don't
change right and then we use these we
even have one for a space look at this
bytes space that's awesome and we have
one for carriage return line for you
code return line feed which in HTTP 1 1
is how you denote the end of headers
before the body starts ok so that's how
we can find we're looking at a bunch of
bytes we need to figure out where the
headers end well we need to find those
bytes ok all right so right so how do we
do that
so here is the first line of an HTTP
request get some URL path protocol
version so we read eight bytes at a time
now it does change depending on the
architecture but for the sake of
simplicity we'll just say it's eight
bytes so then we go and check that eight
bytes against the pre-computed version
of those eight bytes doing that
so there's they get ASCII string as long
that I talked about so that is the word
get followed by a space followed by four
null characters in ASCII so that's my
mask that is what I'm looking for right
and then to do the comparison I have my
mask for characters which is for nulls
at the end plus the the opposite of the
other end and then there's my get and so
we go back to the other slide so I need
to figure out whether the string that
comes in at the beginning begins with
get
that's really all I'm trying to figure
out again as fast as possible way as I
can possibly do and we want to avoid
allocating a string in order to do that
it would really suck if we had to
allocate new memory every single time I
just want to figure out what type of
request it was okay so that's what we do
let me use a new string so this is
effectively the method that does this
get known method out of the memory pool
iterator so this is the handle on the
memory pool remember all the bytes are
coming from the memory pool they've come
in off the socket libuv has put them
into the memory pool we pass it into
this method saying please see if at the
beginning of the memory pool right now
this block of memory if there is a known
string okay so it uses a spree alligator
bites it uses the mask and if indeed it
matches an H should be get method then
we return true so this is obviously
shortened you can see where it says dot
this is basically a massive massive
combination of if statements that does
it for every single known string that we
have and we didn't write a lot of this
code by hand we've generated this code
upfront because there's more than ten of
these strings there are literally dozens
and dozens and dozens of them and we
have some other tricky comparisons that
you'll see in a minute that would be a
pain to write by hand okay so why do we
do this to avoid allocating strings on
every single request which can lead to
GC pressure so let's have a look at a
demo of what happens if we don't do that
all right so I'm going to first of all
what I did here is I went and I grabbed
the I've got a few things I've got the
repo for which is SP net core or SP net
and then kestrel right so I grabbed this
and then I also grabbed the benchmark
code so you mentioned that they have a
perf rig so the benchmark repo is the
tech and power benchmarks and then it's
also got some cool stuff for like
infrastructure to run it and make it
easier to so this is our benchmarks app
that we use to do a benchmarking basic
yeah so what I did is I matched the two
of them together I built my own custom
version of kestrel because I think I can
do better than you guys okay and and I
well we'll see here I'm trying out who's
gonna find out let's see so what I've
done here and my pull request it got
denied the first time and I believe
that's because of my I'm a bad monkey
exception so I've commented that out and
I'm ready to go now so I'm gonna I'm
gonna give this another shot so I'm
gonna run this so I'm gonna say dotnet
run okay so we're running your custom
version of the benchmark app which is
using a custom version of kestrel yes
that you have improved yes okay yes I
have not changed the benchmarks I have
improved catastro okay right so let's
see what happens when you run the
benchmark app it tells you what
scenarios you want to turn on yeah I
mean it boots up the web server or the
application with just those scenarios
enable because we don't want to you know
pay for scenarios that we don't want to
measure so in the way I've been doing
benchmarking is by refreshing by hand
you know you can just hit f5 and hold it
down okay and do like lots of requests
and then but but I also there's this app
that's gonna make it a little bit easier
so net Ling was an app written by
gentlemen in the community who watched a
version of this talk in Oslo and then
was inspired to go and write a load test
generator and he wrote it in WPF and
yeah it's awesome so now we get a really
pretty visual load generator so we're
gonna use this right
John's improved Kestrel yes okay so I've
got this Ronnie I've got that running
and one other thing I want to bring up
is I want to bring up this cool thing
that Matt Warren Road so Matt Warren
wrote a GC visualizer and this thing
anyone use this this is really cool so
you can get that here and again I
improved it I did not improve this at
all I hacked it like crazy to make it
run specifically for this because the
way that the way this is hosted is
you've actually got dotnet running
inside a dotnet running inside I see a
lot of codecs yeah so we're looking at
that no nothing because you typed on
that run yes which they exactly
application you could have you could
have compiled it first and did not Nate
assembly name and then a new one you had
one
yeah it is fine all right so let's do
this so I've got this going this is
going to show Jesus
as I go through and manually refresh I
will see some of them appear so if I go
so you've done like hang on how many
requests did you do 15 and you've
already done - Jane - collection well
let's see if it goes better if I run
this give it some more load
give it some load oh yeah so I'm
assuming red isn't good it's not so good
okay so yeah red is Gentoo so yeah it is
what's the report will beginning yeah
598 requests per second that's not quite
as fast that's actually worse than the
tweet from Kelly a year ago you've
actually regressed kestrel back to
before we did all this work so yeah okay
I better back out my changes what were
your changes exactly well ok so here let
me I'm gonna stop that and I can look
here this is what's allocated yeah what
it tells you then to get a thousand
Ginsu collections ok ok all right well
it's a few then so I am allocating some
strings here I didn't think that would
be it's not a problem let's let's okay
so I comment I may look ridiculous but
if we weren't doing all this that car is
effectively what we would be doing when
we Herbie string building that is every
single time now you know gunky alright
so I did dot in that run because the
it's it's rebuilding all that there it
is
will enable Jason again yep all right so
now let's I got a rerun this GC
visualizer need that one over there will
get this so again here's I go through
and refresh a bit so we hit like a gin -
okay not really seeing much hi pretty
good I don't even see a zero maybe your
code was a little better let's run this
so this is zero I know is that a 1 this
is this a 1 and this is 0 this is 0 ok
does it do ok so that's the impact of
not allocating header strings every
single time a request comes in
in this case is a combination of headers
and like reading that first line doing
the dispatching but you can see why we
care about this stuff okay and and you
know as I was going through I was
playing with different values it really
was like it takes off pretty quick yeah
as soon as you start getting those GCS
through requesters yeah and gin zero is
practically free Jen one is a little bit
slower but doesn't block Jen to is the
one that will really kill you like Jen
to when you're not using works the
server GC pauses the process which isn't
really good and then they're on Dante
framework we have these great things
like multi core gen C and background GC
and all this type of stuff we have a lot
of stuff in dotnet core too but if you
can avoid Gen 2 you want to avoid gin
too so that's good
live and learn alright let's go to Iowa
let's go to i/o alright so we talked
about Lee movie libuv so libuv is this
cross-platform multiplayer form and it
focuses on it's just I oh right it's
really just asynchronous i/o so they let
you talk to the file system that you
talk to sockets and they have an event
loop it's based on like an event loop
model cuz it was originally built for
node which is a single threaded
JavaScript event loop and so they have a
thread that starts up and then you have
to schedule work into that thread okay
and then when if you give it a socket it
will schedule work on that socket on
that thread so you have to be very
careful you don't block the thread which
is why the async is important because if
you do work on that thread you can't
read bytes off the network card all
right and if you're running a server and
you've got lots lots of connections
that's bad because now all of your
network sockets stall which is why they
also say no just to be very careful
about calling blocking api's and why all
their api is a non-blocking yep two
other things I learned about libuv the
UV well there's the unicorn and there's
the Velociraptor that's what it stands
for so that so like a t-rex alright so
in order to make this asynchronous so
you know we did async net shipped tasks
and on there for a TPL then we hadn't
done f45 we introduced a async await
with c-sharp was a 5 back then or 6 I
can't remember one of them which is nice
right you can do a weight Blas something
that returns a task and you get nice
sequential looking code that's actually
non blocking under the covers because
the see char compiler writes all the
horrible
call back code for you you can also
write custom awaiting code so rather
than having to have a code that returns
a task which means the task has to be
allocated you can have a custom a waiter
you can implement a couple of interfaces
and so you can do what we're doing here
effectively we map async/await
in net to the callback model that libuv
users so libera v users an async
callback model just like node does we
want to map that to the nicest c-sharp
idiom of async/await so we do that by
way of a custom or waiter so we call on
read it passes data via the async
callbacks okay so when incoming complete
is called we get a number that tells us
how much was read and if there was an
error we get that number as well
we could have wrapped the entire thing
in a stream which is kind of the other
primitive that people use in dotnet for
doing I oh okay any type of input output
over a a sequential set of bytes but
each await call to stream dot read a
sync would only give you the last bytes
from the last callback so there's kind
of an impedance mismatch between how
stream works and how libuv works libuv
wants to push you data by calling your
method that you gave it where a stream
wants you to call a method on it when
you're ready for data so make sensor you
kind of have this push-pull model so we
kind of we kind of inverted it and did a
push all the way through and mapped it
to async away and then the stream
doesn't appear until you get the code
all the way up and a speed on that core
when you go HTTP requests dot you know
body or response to body and we give you
a stream API there the memory will also
need to be copied if we did a stream so
rather than having libuv push us the
bytes we would have to say read async
and give it a buffer and it would fill
it into there and now we're effectively
having an allocated buffer up front all
right so we can await this sock an input
with our customer waiter here it is
these are the two thing that's really I
critical notify completion is what you
implement to be able to await your thing
and then we await incoming so in
consuming stop we wait that and then
that returns as soon as there is data
available for us so once that'll wake
all that a sinkhole has finished we know
that there's data in the memory pool now
ready for us to read okay so it's
asynchronous all the way down there BV
pushes bytes in the memory pool and then
tells us when it's finished by the awake
call all right are we gonna skip this
this is really in that pool let's just
look at the graph let's look at this
okay so what we've got is here's libuv
and it's gonna be pushing out by its
write it so it's filling up that buffer
and then eventually kestrel says hey I
want some bytes and it starts reading
them right and and the important thing
here is that they're really kind of
decoupled like libuv as it gets bites it
lays them down and kestrel whenever it
is ready to read it goes and get some
bytes and so it's just going to continue
pulling and as far as kestrel knows
there's just a bunch of bytes there and
it reads until they're done
so here Lybia v's done it's hit the end
of the string right kestrel is gonna
keep reading and now it hits the end of
the stream and then it's so we have a
synchronous stream reading from kestrel
reading bytes when it's ready because
we're pausing why we're pausing HTTP
we're not just reading by X we have to
be work and libuv is of solely
responsible for getting bites off the
network card and sticking them into the
buffer for us our code that's parsing
HTTP doesn't happen on the libuv thread
that's why this is asynchronous Lib be
visas this is literally multi-threaded
libuv has a thread that is reading by
it's off the car putting them in the
buffer that we told it to and then we
have a dotnet you know worker thread
that we just cured off through through
the thread Paul that's reading those
bytes out of the buffer when that awake
call returns and then parsing them
through our parsing logic to build up
the HTTP model before we call your code
so we're just moving kind of pointers to
them a string we called my cursors
writes it's like we have a single
continuous bit of memory and remember
these blocks could be anywhere are we
allocated them all upfront we allocate
more as we need to and as you move from
block to block you could be jumping
between areas of memory we just make it
all look like one continuous stream to
the call code so it's very easy to
understand so we start reading an
incoming start we keep reading kestrel
as the green and blue is kind of wet
libuv is that right so we keep reading
we say incoming complete
incoming start again stuff is coming and
then we consume up the top and we can
read all the way through to the end even
though maybe this memory came in off the
network card in two discrete blocks the
calling code doesn't have to know that
it just says I'm gonna start reading it
okay
reading while this bites it could be a
pause but I'm busy pausing and then I
keep reading by the time I get back
there there's more bites and then when
I'm finished I finish it's kind of like
when you're eating at one of those steak
houses and they keep bringing you more
steak yes and you're just and you don't
even care it's like they keep showing up
and bringing you an analogy I'm just
eating steak and of course sometimes
what happens is you finish and there's
nothing on the table brats more a young
child right like you can do either stuff
and then maybe they stop and though
there's no food that's what you have a
pause yeah I have to stop now but
hopefully they keep bringing food before
you're finished eating so it's very much
like that that's a good analogy actually
all right should you say all right suck
it out put right behind poppings that
was incoming what about out coming bites
okay what kind of tricks do we have to
do there okay who's ever seen a diagram
that looks like this most of you have
with your web developers what if you F
4:12 look at the network stack right
yeah it looks a little bit like this
right so this is a diagram that is
showing calls of response dot right top
to bottom and then elapsed time from
left to right okay
the starting on the left hand side here
the yellow is us returning you a task
that has marked as complete remember
writing is asynchronous so we say
response don't write 16 kilobytes please
and we say yeah no problem we give you
back a task that's finished now it
hasn't really finished because what we
did is we wrote it into a part of the
memory pool into a buffer the right
buffer and we just sit there and wait
now it's a little bit like nagging we're
not nagging is this is network concept
where when you push stuff out over a
network card it's not particularly
efficient to push every byte if you're
sending a byte at the time you usually
buffer up a little bit of that stuff
down in the driver and then you flush it
out over the wire okay there's a lot of
overhead with the headers and all that
stuff as far as like yeah in in moving
some bits of data it's harder to move a
bigger bit and like Ethernet headers
there is there is an overhead involved
with sending bytes over copper or over
fibre right you have to wrap it in other
stuff so it's better to kind of chunk it
up a bit so we do that in Kestrel on
your behalf
up to about the 64 kilobytes okay so you
call right async you will wait the task
it finishes so you call right async
again you do that four times which pulls
the buffer
all right we've started sending it out
okay so you can see by like the third
call we've already started sending their
bytes out and by the time you get to the
fifth call the first one the first task
is actually now truly complete we've
actually flush the buffer out and we've
started filling up the next buffer that
we're going to write out so here is that
decoupling again it's kind of this a
parallelism going on you're calling
right we give you back a task that we
tell you is actually finished but it it
hasn't like we've actually put that in a
little bit of memory and I may get
another thread but Lybia V thread not
your thread that it's flushing those
bytes out over the wire now you can
catch up to it you can get to the point
where you're eating young char and you
finish all the food on the table right
before they say that is called 5:00 and
we give you back the orange incomplete
tasks you sent more data than we were
able to flush out before it was actually
finished and we give you back an
incomplete tasks you should be calling a
wait on that because if you don't you
just keep sending data you're just going
to make that use more and more and more
more memory until we have a chance to
drain it out so well-behaved calling
code will always await response dot
write async so that we you know have a
chance to do that back off that
throttling all right and then that does
completes ad infinitum right until the
end so we have that nice decoupling of
the server to the client again cool cool
so how about I run this down I'll do the
demo and you explain what's happening
what further pipelining oh so it's all
about pipeline yeah okay so as part of
the benchmarks that we do with Tekken
power there's this concept of HTTP
pipelining so a should be one one
supports something known as they should
be pipelining which is effectively the
ability for the client to send more than
run one request to the server over the
same socket all right so remember HTTP
is just lay it on top of TCP I open a
fat pipe up to the server and then I use
that fat pipe by sending one request at
a time
that's how ways to b11 works this is
what everyone does right that was the
diagram on the Left pipelining says I
can send you in requests before I have
received responses from the server and
then I'll stop and then the server has
to send me back the responses in order
you can't like overlap that's what hb2
does which is really nice but hp1 one
you can only do it that now no browser
today does this
okay but if you're building a client
like you're doing a Web API and you have
a chatty console app that's talking to
the server you can totally do this right
if you have an HTP client library that
supports platform or like you said HTTP
HTTP 2 is coming and that will yeah and
they should be too if you're lucky
enough to be able to use it right now it
supports an even more advanced version
of this called connection multiplexing
which effectively lets you pipeline out
of order which is just nuts it's
basically Ethernet over HTTP over
Ethernet right it's just crazy
so the benefit of this in terms of
benchmarking is that it allows us to
send more requests to the server with
less latency so we can put more load on
the server to see how really really fast
it is so again it's not real world but
it allows us to get higher synthetic
numbers by stressing the server harder
if I had to wait for the full latency
round-trip on an entire socket before I
could send the next request I would need
more sockets to load up the server for
the same amount of requests as it makes
sense right so if I run if John runs the
same test that he did before so what do
you do before you had 11,000 requests
per second when you were doing no
pipelining yep and then you're going to
bump up the pipelining to ten requests
and then you're gonna run it again so it
sends ten requests and then it waits for
ten responses then it sends ten waits
for ten responses and we'll see that our
throughput should go up I hope right so
now it's nearly four times as high all
right which is good it means that we can
stress the server and really see how
well it speaks HTTP now the way that
this really relates to real-world perf
again doesn't really but it allows us to
really stress the server and see yeah
let it let it really um sort of stretch
its legs there are a bunch of things
that we do incur stall to make this
specific scenario scenario nice and fast
so you want to avoid like calls out to
the kernel in between every request like
Cisco so do this the fact that we have
that buffer that's being filled
asynchronously by the Libby V thread
while we read from a network of thread
means that we can effectively want to
give and socket we can sit in a while
loop and just say is there another
request is there another quest ok call
the app func is there another request
okay call the app func and we can sit in
a tight loop without unwinding the stack
and just process the sixteen record the
ten requests in this case if we're doing
pipelining
all right cool especially good in
benchmark
so because the requests are generally
quite small alright we're not put we're
not doing a lot so we can fit more of
them in a single ethernet frame how do
we coordinate the lib Mui through oh
goodness
the chocolates and roses basically so
this is multi-threaded kestrel as a
multi-threaded server it's even
multi-threaded in the scope of one
connection as we've just talked about
right we have the libera thread and we
have dotnet threads doing the actual
HTTP parsing and actually running your
code your code never runs on the libuv
through unlike node all the code runs on
all the code in the node app runs on
libuv thread right so we used a bunch of
c-sharp primitives to do that you know
locks monitor try enter we try and avoid
locks as much as possible we actually
think the thing that's holding us back
right now from getting even higher perth
is mostly contention which is devil to
really diagnose getting really getting
good data on contention and you know
profiles will give you some but we're
talking about all the way down to kernel
level contention so we're really working
hard to try and avoid as much of those
locking x' as possible it's also the
reason we have four different memory
pool per thread and all that type of
stuff so we can avoid lock so we don't
to do all right algorithms algorithms
yeah so
operations and and so I think here I
just want to dig into the sequence is
there anything else you want to say but
now there are some methods that these
are basically methods exist on those
memory blocks so when we're looking at a
block we can say peek take or seek yeah
right so to look at the memory so look
at what seek does so seek is the method
that we call in order to read bytes off
the incoming wire in order to read HTTP
like the stuff we talked about at the
beginning so we might have to call that
one two four times to consume the
beginning line depending on how long
your URL is right because the URL can be
up to a certain size we have to call it
twice per header because we need to get
the beginning and we need to get the end
and the header can be up to like four K
and it can also be split across lines
you know a should be one one supports
splitting headers across lines with the
continuation character these are the
things you'll only have to write a web
server and we find the first occurrence
of any one two three giving characters
why is it useful well spaces are very
useful in that first line because it
says HTTP 1.1 space URL space verb right
or the other way around a camera might
never get it right and then we do
carriage returns but
the starting line in each header or the
courage returned with the continuation
character and then once all the headers
are finished we have to find carriage
return carriage return so we can then
start reading the body with finished
parsing HTTP now we just need to give
you the rest of the requests appointment
ok so we need really really really fast
ways of reading sets of bytes at a time
to look for patterns sounds like a great
job for vectorization yeah
do you want to bring up the sim D if you
I don't know there's actually a Intel
yeah yeah yeah Intel website that very
briefly anyone used sim D or
vectorization who's heard of ABX no
who's heard of SSE back from the pin
here we go see these these people
showing their age I remember when steam
came out as well I was there too
so I'm looking SSE was there something
streaming extensions or MMX was the
other one multimedia extensions on the
Pentium 4 I think it was there basically
extra processor instructions assumed
internet that I have a good event it
when I want me to look at sim D if you
can look up Intel sim D ok maybe a V X
so streaming sim D extensions go good
these ones over yeah so they're extra
CPU instructions that let you operate on
numbers of varying lengths depending on
what version of the processor you have
how much money you paid and how new it
is that is not the one that I wanted but
never mind there is one with a really
nice like graph like a really nice
diagram that shows you what oh wow let's
just like pick all the random so that's
the one right there actually one on the
left one with the red and the green and
the blue perfect so SSE an AV X 128
instructions let you operate on numbers
of various sizes or blocks of bytes
right so rather than you having to loop
through 64 bits one by one or byte by
byte 8 times to look at a byte you can
say in one CPU instruction please
compare these 64 bits to this 64 bit
mask and give me a result in one clock
like one clock cycle one very very so
this is very commonly used when you're
doing like number crunching and
obviously if you doing video card
development
paralyzation and sing at sim D is very
popular sim D stands for single
instruction multiple data
all right single CPU instruction
multiple data being done okay it's
basically it's for free you've got to do
for comparisons for poor header lookups
or whatever right do them all at once do
them all at once and we can cut it
brilliant if you're trying to do pattern
matching over bytes okay which we are
which we are and so we use the system
numerix namespace which was released a
couple years ago on dotnet we know this
relies on the Riu JIT the new jitter
that came out with don 846 and just the
jitter that's used in don their core to
be able to turn your IL code into
machine code that uses these magic
instructions we don't write native code
we write C sharp and then the jitter
turns it into really nice AMD code ass
SMD code for us so here's a simplified
version I have I'm looking for a hello
world carriage return line feed okay
imagine it's a header and so these are
the bytes that represent that value okay
so what we do is we look one register at
a time one CPU register size so
depending on your CPU that can be
anywhere from 64 bits to 512 bits
because and I think there's even there
even in a 1024 bits on the new zealand's
these operations can even be higher than
the native register on the CPU because
these get their own registers alright
we're getting right down to hardware
stuff here so I can say right let me
take one registers worth of data and for
here we'll just 56 here 56 bits here
just for convenience so it fits on the
slide and then we give it a vector which
is a comparison please compare these
bytes to these bytes it's like a matrix
math you're any idea of your matrix a
bit like that right so you do that and
you get a result and the result here is
equal to 0 because every byte is equal
to 0 so we've done two operations we've
done the aquatic comparison and we did
the Equality check there only cost us
two instruction because we're looking
for that carriage return line feed we're
looking for your time line fail all we
care about so then we go to the next one
which by the way the carriage return
line feed is here you can see the 0d is
the carriage return that's what we're
looking for so we're masking against the
carriage return so we run it again this
time we get a hit ah-ha so this one
doesn't equal zero so what we've done
for instructions and we've figured out
that one of the characters and we know
it's in the second half of this string
is a carriage of ten so now we have to
find out which one it is all right so
now we have to find the first equal byte
in the little block that we know that
it's in so we operated on the register
size now we know a particular register
size eight bytes was to say up bytes has
the character we're interested in how do
we find that character well the naive
thing to do right would be just to loop
through every character
we've gone from a whole bunch down to
eight let's just loop now and that's our
first enough you can do it quicker so we
can do a quasi tree search which is a
little bit like a binary search where we
effectively compare half at a time until
we get down to where it is this is
basically like I'm thinking of a number
between 1 and 100
well is it greater or less than 50
correct right a man is a greater less
than 75 right just you're working your
way and you're working way down so the
worst case here is n log n or whatever
it is I came here you did come sign you
can tell me what this isn't no one say
David Fowler used to answer that
question for me here would be loud so in
this case it's actually a worst case hit
because the character is right down at
the end so we have to do an operation on
the first half that puts us into the
second then we do it there that puts us
in the second half and then we have to
do the last one there but instead of
taking seven cycles
it took three okay and which is the
worst case all right so that's kind of
nice you know someone submitted this
pull request like this insane ternary
operator with a little masculine or to
make it work and I guess you could even
like over touch you're probably gonna
have your carriage return is more than
four bytes in you could even keep
tweaking away you know what I mean and
sometimes I can have first and come in
whatever yeah oh well if you didn't need
my pull request well if you're doing
little-endian versus big endian as well
the bytes of backwards just don't do
that with kestrel it is actually
mentioned somewhere in the slides but we
will make it work yeah so this is
actually showing looking for it and
there we go and then there's something
about some quirk about why that is true
I don't know it's some hex math all
right so at 1 million RP s is this
really really worth this seems like an
awful lot of trouble to go through just
to compare some strings pass them into
variables that you can read in your
application code right and to manage
bytes coming off the wire well
if we compare just a raw benchmark of
that one method the method I just showed
you fine first equal bite well sure it's
faster there's the naive version which
just loops over the bites
there's the slow version which I don't
know what this I think the slow version
was what we had before and then the one
at the top is the one that the dude sent
the pull request for okay so what do we
got the median is I with difference
between 899 nanoseconds vs. 654 under
standard deviation is much tighter which
is nice but you know it's not
particularly massive but when you
multiply that by a bunch of things I
think it's excellent yeah most thing
people would just do that one yeah right
so then you go to the next we just loop
over the bunch right now when you've got
bigger memory blocks now you're
operating on four K of memory because
someone sent a bloody huge cookie or
something right now it starts to make
sense right we've gone from 4,000
nanoseconds down to 800 nanoseconds in
order to find that carriage return in
that block of memory because often a
single block of memory that were
operating on will contain four headers
or maybe three headers and then half of
the next header and we have to know who
you seek through this and pass them out
as we go so we wondered it as fast as
possible so yeah it's worth it it makes
a big difference when you make these
implementation changes and then you
scale it up to larger sets of memory all
right quickly finding known headers from
bytes okay so oh we do have this code I
thought we took this one out okay it's a
case insensitive comparison in his few
so I talked about we have some custom
code that does case insensitive
comparison we want to make sure that we
fail as quick as possible because given
a block say we've done what everything
we've done so far we have a line we know
it's a header and we've found the
beginning the header name right we want
to figure out is this series of bytes
that I know to be a header name a head a
name that I've already seen before it's
one that I know about
is it connection or what's a common
header name user agent or you know one
of the many many common header necessary
except like we don't want to allocate
those strings over and over so how do we
do that really really quickly
and then how do we fail if it's one we
don't know about and then go off and do
the slow thing okay well headers are
just ASCII which means yeah just remind
you strings are just numbers that's all
they really are and so if you want to
write incredibly fast string comparison
code you do some math
yeah like that this is our interns we
get a bunch of interns I am not even
gonna try to exist and I mean the code
is fascinating hopefully one thing that
you'll do after this is go and look
through like it's not a huge repo and
you can look through it online
it's fascinating code even just looking
through the pull request there's usually
a bunch of active pull requests just to
look at deltas even if you don't want to
dive in let's look at some deltas and
see the type of changes being made we
didn't write this code by hand no this
part was generated so we wrote at all
that you feed it a set of known head of
strings and then it generates you the
case-insensitive comparison code using
bit shifting for all the known headers
that we care about yep okay we'll look
at that in a second but this one goes
through and shows like how we're doing
it right so this is actually looking for
cache control oh right sure I could have
read that it says like this is literally
like reading the matrix isn't yeah so if
we massive okay so yeah so that is that
is it and there's there's long sections
that do like I got in Twitter
because I said I don't know how to do I
don't do bitwise operations every day
once in my career congratulations to you
if you do this stuff every day good or
not or not but this is this is cool this
is the stuff about how how it was
generated so this is the card that
generates the code yeah and you can go
in let me see I've got that open
actually so here there's this known
headers thing and so I mean this is like
literally just string template so we're
actually using C shop six string
interpolation as the templating language
for generating c-sharp yeah so like
literally it's just less of known I mean
it's all if this is cheating right I
mean we know that there
well we've every good developer is lazy
right why why why and if I can just do
it we know that there's like we've got a
bunch of strings coming in I've got to
do some something with it and then turn
some strings out yeah but we have this
information we know these strings we
know they're gonna be
to them anyway like we know the common
ones so let's let's optimize the super
common ones and then if we get a custom
strain we'll do the slow thing to
allocate it stick in a dictionary do all
that type of stuff yeah hey speaking of
dictionaries like ultimately when you
get the headers generally you look them
up in a dictionary right that's a
special dictionary it's not the
dictionary in net turns out the
dictionary is really great if you have
like 10,000 entries if you generally
have 7 or 12 it's extremely expensive to
create one of those on every request
when you're doing 5 million requests per
second trust me
so we have a custom dictionary type that
implements our dictionary or string
that's optimized so when you have like 7
headers right and then it just like has
fields inside of it and then it falls
back to a real dictionary behind the
scenes for when you have more than that
all right yeah so here's that generated
code and it's literally just going
through how many lines a thousand yeah
so closing you should send a pull
request
yeah fix the whitespace yep oh we're all
resharper in shop right can you imagine
look and someone please do that and
someone open that and just tell me send
me a tweet with the screenshot of what
resharper does yeah we're using see
chopsticks interpolation to do that so
so this is fast code that figures out
how many ones are in a long this is just
a random one that we found like there
was no story around this other than
here's a crazy method that uses bit
shifting to figure out how many ones are
in along no you're welcome
take the right user take a picture in
your MVC yep yeah yeah and then this I
mean this is the extent over time like
right measuring things and this is
actually looking at the aisle code that
shows that it's faster in cases when
you're comparing this is I guess we're
looking at can you zoom in where it says
we've got a fixed byte we've got a fixed
by the rate so a fixed of byte array
pointer array generates that versus a
fixed of a byte array pointer of an
empty array of a cast I don't know what
those instructions mean it's less il so
it's probably quicker because there's
less instructions right so I think yeah
we have we've been some amazing too
found by some very like senior people on
the dotnet team at all I figured out
that I could just directly take this
part of memory and just cast it - along
with a native pointer and that was three
cycles quicker than just like running it
through in dot whatever or like casting
it naturally like okay great thanks for
that that's awesome
it did show up in a profile we're not
doing it to doing this randomly so after
all this work where do we think we're
gonna land so we have submitted we've
done the pr2 taken power for the first
for the next round of testing and our
hope right now is that we're going to
land somewhere around there in the very
first round that we're going to be on
here a little bit well probably a little
bit lower than nettie when we run Neddie
on our hardware it goes a lot faster
than that so I'm not quite sure what was
happening when this test was run we were
getting my eight million requests per
second we're getting about five and a
half million requests per second right
now but we think Neddy's going to be
faster so I think we're going to be just
under undertow and Eddy and then in
version two we're gonna try and make the
top five we're not gonna stop like this
is really important for us to understand
how is it that all of these other
frameworks that are written in Java
which is good actually because it's a
great analog to dotnet it's managed has
GC right it runs on Windows and on Linux
and everywhere else so it's actually
nice that it's not just a bunch of C++
stuff at the top or something else
it's Java so we have no excuse we really
should be able to make this stuff as
fast and these aren't BS frameworks like
Neddy is used by thousands and thousands
and thousands of servers it's packaged
inside lots of other servers undertow I
think is written by a dude from Red Hat
right so these are real servers and
undertow is actually really beautiful if
you look at the applications written on
the toe it looks like a snake or it's
all functions and lambdas and it's mil
where it's it's lovely they managed to
get that performance even with nice
programming models all right that's all
we'll see if it's worth it and I think
that think that's the end so if you're
interested in learning more about this
hit the repo on kestrel if this was just
a geek out for you for an hour and you
never want to see this code again I
applaud you that's great
that's what we're employed to do is to
figure out how to make this stuff sit
this stuff faster you don't have to but
yeah thanks a lot for coming and yeah
happy coding</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>