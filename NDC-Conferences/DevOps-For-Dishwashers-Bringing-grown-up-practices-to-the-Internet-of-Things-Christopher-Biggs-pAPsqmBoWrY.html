<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DevOps For Dishwashers - Bringing grown up practices to the Internet of Things - Christopher Biggs | Coder Coacher - Coaching Coders</title><meta content="DevOps For Dishwashers - Bringing grown up practices to the Internet of Things - Christopher Biggs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DevOps For Dishwashers - Bringing grown up practices to the Internet of Things - Christopher Biggs</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pAPsqmBoWrY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">afternoon everybody thank you for coming
along so I'm Christopher Biggs and I've
been involved with a better system since
my first professional job about 20 years
ago and nowadays I'm a consultant
specializing in the Internet of Things
which is the new name for embedded
systems that don't work properly so I
work with companies that develop IOT
devices to help them choose the right
technologies and practices to build and
deploy their products now I've spoken in
the past about the state of security in
IOT and while I agree it's pretty awful
I'm not pessimistic I made some
predictions about how bad things could
get and what type of action that I
wanted to see and the bad and good news
is that these are coming true so I'm
seeing the space rapidly mature and the
right things are being done to address
the challenges I've spoken many times
about what I call the Internet of scary
things most recently last month at IOT
Sydney and that presentation is about
how it's like the Wild West right now
and why DevOps is is really a must do
for IOT traditionally hardware has been
developed carefully and conservatively
and had long life cycles the IOT brings
the contradictory requirement of
products that are set in stone sometimes
quite literally but which connect to the
wild Internet where sometimes a day can
be an age so this talk is going to be
about the how how you can apply best
practices from grown up computers to the
challenges of building and managing
Internet connected autonomous devices so
I've called this talk DevOps for
dishwashers
because recently the tale of an
internet-connected dishwasher made the
news a lot of people laughed because why
would a dishwasher need to be on the
Internet well it took us about 10 years
to go from personal computers being
networked only it need to us considering
any computer that is a networked to be
broken so I expect the same for every
other electric appliance in the long
term I think the very word computer is
going to disappear
because everything will be a computer
software as Mark Anderson said will
truly have eaten the world and this will
have a lot of effects but the one I want
to focus on today is that we must really
rethink our concept of quality in the
embedded space the price of an amazing
future where everything can interact
with everything else is that everything
might interact with everything else so
before I go too far I want to define
what I mean by DevOps this is one of
those technical terms that has been
misused so much that it's dangerous to
use it without confirming that the
listener hears what you mean so for me
DevOps is not a thing you do it's a way
you do things
DevOps engineer is not a job title
there's no such thing as a DevOps team
it's an easy mistake to make I in fact
once held the job title of head of
DevOps
but what DevOps is is accepting the
development and operations a part of a
spectrum and the distinguishing between
them is counterproductive so my full
definition of DevOps is evolving a
culture and a toolset that empowers
everyone to effectively maximize value
through radical transparency and extreme
agility so today I'm going to look at
how we pay the price of this connected
future the body of the presentation is
about the practical things that you can
do to build quality products for the
Internet of Things I'll look at the
livestock of an IOT product from
inception to retirement and what you can
be doing to get the best outcomes at
every stage the areas I'm going to look
at firstly choosing platforms that
support rather than hinder quality
outcomes secondly shaping your
development and quality practices to
foster agility while preserving
reliability and third working with your
data in ways that promote
interoperability and reusability so just
before we dive into that I wanted to
look at the universe of discourse in
just threescore years and ten the span
of a single human life time we saw the
ratio of computers to people increased
by around one order of magnitude per
decade if you were a fresh space
undergrad when Conrad Sousa was building
his jet won in 1936 then you could still
be around next year to celebrate your
hundredth birthday fast-forward 30 years
1960's around 10,000 devices per person
instead of one mainframe per large
company
a decade later we've we've seen another
two orders of magnitude increase
computers have become indispensable to
our civilization but this is not really
widely recognized another decade another
two orders of magnitude that's that's
like the inflationary era of the early
universe things are growing faster than
we can really comprehend there's a
there's a school of thought that says
that the desktop computer created the
1980s investment trading boom
because spreadsheets allowed people to
do things in business that they've never
done before
jump to the 1990s computers are starting
to diffuse away from people they're
eating into the fabric of our lives if
you're old enough to remember growing up
without a smartphone then you'd
recognize that young people today simply
wouldn't understand what life was like
before that era today we're starting to
lose count of how many computers there
are very soon I think we won't even
bother to to ask the question and that
trend is not over so in the next decade
I expect another deck toppling and at
some point after that we're going to
completely stop counting everything will
be a computer every thing
hence the Internet of Things a world
where humans are a 0.1 percent impurity
in a sea of machine life if we're wise
we'll make friends with the machines
when they're babies because I don't
think we'll beat them in a fair fight
during the early part of the 20th
century one analyst predicted that an
oncoming apocalypse where civilization
would grind to a halt
because every single available unmarried
woman would need to be employed as a
telephone operator and no further
rollout of the telephone network would
be possible of course now we have robots
running the switchboards and even making
and answering some of the calls
instead of having a sizable fraction of
the workforce operating the
communications network we have millions
of people in less developed countries
trying to sell those people life
insurance over the telephone because the
cost of communication is practically
zero so if you wonder how we're going to
curate it and maintain a thousand
devices per person you're making a
similar kind of category area strategy
is about adapting you behavior to
circumstances when a computer costs
three months wages you shepherded it
carefully when computers cost less than
a cup of coffee they become consumables
they're sheep not sheep dogs the unit of
management becomes the flock now I could
go on for quite some time about the
security landscape in the internet of
things and I have spoken about this in
the past the one slide precis is that
bad people want to break your stuff it
doesn't have to be many bad people
another side effect of that zero cost of
communications is that if you're a
terrible person the 21st century is a
target-rich environment
now I don't really want to go off on
this tangent about security but I find
that the underlying causes of security
problems in the IOT are an instructive
microcosm of the wider challenges last
October the Murai worm spread around the
globe because of well known default
passwords that never get changed this
month we learned that the CIA has been
able to insert spyware wherever they
like because some software developers
found it convenient to put backdoors
into communications Hardware choosing
expedient but fragile programming tools
brings consequences the pattern here is
a lack of professionalism and
forethought moving on in April we got
the endlessly entertaining story of the
compromised internet dishwasher which
demonstrates how many developers aren't
trained well enough to avoid common and
completely avoidable pitfalls the
downside of ubiquitous communications is
that there are no safe spaces June's
wanna cry malware outbreak showed us
that when everything is software
everything has to be maintainable last
month second coming of a much more
damaging successor hammered that point
home if you can't fix it it's no longer
safe to own it firewalls and even air
gaps are not sufficient
I finally anti-pattern I want to touch
on is if you sit in the middle of the
road you'll get run over quality
procedures that worked last year might
be a liability this year it turns out
that the government software quality
guidelines that have been applied to
election security in the United States
delivered almost no benefit and some
potential detriment because they largely
focused on constraints that aren't
relevant anymore these guidelines for
safety critical software prohibited
things like exceptions and recursion and
that's more about preventing your
space-probe from overflowing memory and
crashing into a planet than it is about
software quality on modern processors so
let's summarize what I think a response
to these challenges should look like and
then we'll go into some detail on each
point first choose the right platform
and tools next make your identity and
security part of your framework so that
you're only built at once automate
everything you possibly can design for
testability trainer teams share skills
watch YouTube videos get consultants
come to conferences whatever your budget
there's a way to improve your outcomes
and lastly maintain awareness and use
that awareness to respond rapidly so
let's let's talk directly with our
platforms and Jess humble who literally
wrote the book on continuous delivery
gave a keynote right here in this
building at agile astrologer n-- where
he told a story about HP's printer teams
their cost of engineering was spiraling
but their call it was awful and part of
the reason was that they were spending
over a quarter of their time just
rewriting existing software because
pretty much every printer model used a
different processor so they made the
choice to move to a common architecture
and reduce their re-engineering cost and
that meant that some printers had a more
expensive processor than they needed but
the savings far outweighed that my point
here is that every hardware business has
a person whose job it is to look at
every component and ask can we delete
that and say half a cent per unit and we
need to resist that mindset
this is a person who's empowered to
remove customer value the correct
question when designing your hardware is
will this hardware platform add value in
terms of resilience longevity and
supporting software quality so your
platform is part of your DevOps team
your goal should be to select a platform
that supports your DevOps way of doing
things
so while selecting and building the
hardware is by far the easiest part of
the IOT lifecycle it's the foundation
that facilitates the rest that means
that openness and friendliness are the
first things you should look for select
a processor in platform that's got good
development tools and will be around for
a long time now thanks to the drivers of
Android phones and and TV media boxes
there's a huge variety of arm systems
out there which is similar enough that
Hardware differences aren't really
important so you can develop to the one
architecture and then select an
appropriate board from five bucks up in
my projects we work a lot with the
Raspberry Pi as the development platform
this is a $35 computer which brings the
convenience of a huge selection of
platform tools and a great developer
community and we can use those in the
lab we can use them in our continuous
integration systems and even for the
first hundred or so production units
there's a wide selection of compatible
lower-cost designs to buy a build for
mass market production here's one that
starts at about seven bucks so the same
goes in software think about what
happens if news of a nasty bug breaks
out on the internet the top tier vendors
are probably going to have a patch out
in a couple of days if you pick some
hipster Linux distribution that has 50
users and one maintain is you may never
see a patch you want your tools and your
systems to be a convenience not a source
of pain if you're using a mainstream OS
variant and fire IT that pretty much
means a Red Hat or Debian then there's a
critical mass of users that access a
quality filter there's not likely to be
some hidden problem that you can't solve
if you're the only company in the world
using some niche distribution when you
run into trouble you're on your own I
also think it's important for devices to
be online as much as possible
now power and network constraints may
mean that these devices can't afford to
be online all the time and that's
tolerable but you do need to think about
how to support DevOps processes within
these constraints and I'll come back to
that later I think you should plan to
have two-way communications with your
devices forget about the low cost
one-way communications channels they
might be cheaper but in my experience
you're going to regret it
make sure that you have a way to locate
and identify every device even if you
don't think you're going to need that
I'm talking about putting some kind of
display on a device even if it's only a
single light emitting diode so that and
letting devices determine their own
location either by GPS or more costly
via cellular or Wi-Fi estimation I've
heard more than one story from the world
of Oceanography where probes go missing
because people rescued them and they get
found at the back of a dive shop or in a
junkyard
and use the remote management platform
at the very least you need a secure way
to upgrade and shut down your devices
I'm recommending using an agent oriented
data center automation tool like
saltstack I'll give some examples of
what I've done with this tool later on
the right tool is going to kill two
birds with one stone you can use it to
construct your master configurations in
the lab and then you can use it to
manage the devices in the field so we
chose some hardware and an operating
system and we've installed at least one
blinky light now we have to make it
blink if I could give you one piece of
information today it would be use a
server less architecture in your cloud
services service doesn't mean there
aren't servers it means you don't have
to worry about them I'm putting together
a global building management system that
the so far doesn't rely on a single
fixed server and that's including the
whole CIO pipeline I want you to take a
good hard look at what programming
platforms you use don't use something
just because everybody else uses it
programming has entered an era where the
ability to be rapidly up and running
thanks to an ecosystem of open source
parts is really revolutionising the
crash
one of the most important things to look
for is a healthy ecosystem of actively
maintained components javascript go are
two languages that i find particularly
notable for this in that they've got
components search engines that help you
understand who else is using a component
are you the only person left using this
or is it being used by somebody with a
lot of resources to keep it running
looking deeper at javascript it could be
said that javascript rules the world
it's arguably not the language that some
anyone would have chosen to be number
one but it's there and it's getting
better every year
you can now literally run JavaScript
from chips to cloud on a $2.00
microcontroller all the way through to
your cloud services it's the only
language as far as I can tell that it's
got common support against Azure AWS and
Google Cloud javascript is also a good
way to write user interfaces at the
moment I'm working in helm which is a
strongly typed meta language that
produces highly reliable and quite
pretty management interfaces don't write
your inner your own user interface
components there are so many good
options out there every network router
ever looks terrible and I wish they
would stop reinventing the wheel I'm
using go for one of my larger projects
this is a good fit for container
environments in particular because it
produces executive balls that are self
to tained you don't have to go
and look at a crystal ball and figure
out what shared libraries you need to
ship there's built-in cross-compilation
support so you can work on Windows Mac
or Linux workstations wherever your most
productive my CI pipeline can run on
stock cloud systems and spit out on
native containers I find go is a very
pragmatic language although it looks a
lot like C you're fine working in it is
more like JavaScript you don't need to
worry about memory or pointers and the
type system actually helps you to write
reliable code then I coded in C and
suchlike for a long time and I really
don't think there's a good choice case
for widespread use of these tools
anymore it's simply far too easy to
write brittle or insecure software
modern languages have learned from these
drawbacks improvise
a really effective safety net she'll
scripts in particular are very tempting
but when you are dealing with malicious
users
the attention to detail needed to manage
your input safely takes all the
convenience out of using shell scripts
now frameworks are important for IOT
because there's a lot of foundation
behaviors in common between devices
given the time and budget pressure on
low-cost devices you don't want to waste
the resources on addressing the same
problems over and over or we're still
not address them not addressing them
well at all so I'm using Amazon's
ecosystem in a number of projects
they've really thought about what IOT
means for device management major
challenges for security intermittent
connections data collection are pretty
much sold for you when scale becomes an
issue you have a system called Amazon
Greengrass which lets you move from a
centralized system where everything
talks back to one cloud service to a
more decentralized architecture where
you might have a building level or a
floor level element which is another
control of your AWS if you look at the
awful usability of many home automation
products you'll see that the vendors of
these products have been implemented
basic behaviors badly where they could
have just gotten that work done for them
if they'd adopted the right framework
these are some frameworks that I like
the good news for us is that there's a
lot of vendors going hard at this
problem and most of the big frameworks
very well done but there is a learning
curve you're initially going to look at
this these frameworks and tell yourself
I don't need all this complexity I can
just skip to the end but particularly in
the maintenance part of the lifecycle a
good framework will pay you back now
I've gone on record and saying that as
saying that you shouldn't just screw a
Linux server to the wall nor ship
software that you don't need so here I
am advocating using a common mainstream
hardware and software platform which
breaks both of those rules really and
containment is how I reconcile those
constraints software containment causes
me to limit the interactions between my
parts which helps with security and
complexity concerns and the consequent
loose coupling between the parts lets me
reuse components more effectively now
Doka is the containment engine that
everybody knows there are three or four
others now in the Linux space but to be
honest I haven't done much more than
kick the tires on the others because
docker is the one that has all the major
cloud vendors on board and what I
suggest that you do is you set up your
continuous integration pipeline to
produce docker images as the output of
your software pipeline so that your code
reviewers and your testers can get the
exact artifact that's going to be
deployed and so that your orchestration
tools can push these artifacts down to
your devices across the internet so
testing is probably the single most
critical point for DevOps the goal we're
striving for is more frequent releases
and in order to meet that goal you
really have to address the human cost of
testing the most important point here is
to actually do the testing destruction
your enterprise in a way that provides
fast and easy access to test
environments
ideally anyone who wants to test can
click a button and have a fresh
environment in minutes if it takes days
of work to provision in an environment
nobody's ever going to do it which means
some testing will never get done or your
test system will be out of date so you
want to have fast spin-up and
replacement of services and we're still
if you're not deploying new environments
on a regular basis you might get to that
moment we need to do it in a hurry and
discover that nobody knows how anymore
I've actually seen that happen so in
order to facilitate testing you need
test data so you need to pay a good deal
of attention to ensuring that you
develop test Java generators alongside
your code because making it painful to
test will result in painfully bad
testing now
actual research data shows that the
sooner you find a bug the less it costs
to fix if a developer never makes a
mistake it doesn't cost anything so
that's the first place that good tools
and the right languages pay off if you
find a problem during coding that's
where pair programming and code review
pay off the matted testing tools find a
problem then at least you haven't wasted
anyone else's time now you can't tell
developers always run the test every
time bit before you commit but in my
experience
Pixies will fly out of my ear before
that happens so whatever central CI
system you have needs to fail cheaper
you do the fast test first and the slow
tests later so that what it was most
likely to fail is upfront once a change
merges into your code repository the
costs of fixing problems start to
multiply you've consumed the time of the
person who found it then the developer
who fixed it and then the person who
confirms it's fixed if you make it to
really integration or release testing
then maybe you have to go back and redo
other work and redo testing and possibly
reschedule your release and if a problem
makes it into the hands of your
customers we're talking about literally
hundreds of times more to cost to fix
than if you'd found that problem on day
one it is true that comprehensive
automated testing takes time and money
studies show it takes around costs
around 30% more but brings a 90%
reduction in defects so if your
automated tests catch one bug a week and
that takes to develop an error to fix
and otherwise it would have reached
customers then those tests paid for
themselves twice over now when you're
working in IOT you're probably not
running an x86 CPU so how do you compile
on tests for various embedded CPUs in
your continuous integration pipeline
well good language to us and containers
come to the rescue here if you're
working in NGO or no js' you'll find
that you can run your code on pretty
much any platform and build your
distribution artifacts on any cloud
server
emulators have gotten really good - you
can spin up an army emulator on a PC and
boot it up just like a virtual machine
it is quite remarkably easy compared to
how hard it what it was ten years or so
ago but you do want some real devices of
course so where you need to compile or
test on your target system if you choose
the ARM architecture and linux this
means that your systems can pretty much
integrate right into whatever CI system
you're using now I happen to be using
git lab because it was one of the first
systems to build the automated testing
tools right into the code repository but
now over the last few months github
bitbucket have all that - so really in
terms of CI systems go with whatever you
know and the way I do this is I I tag
the jobs which must run on an arm as
requiring arm and then configure those
arm workers only only to run those jobs
so that everything else happens on an
x86 so and of this complete pipeline
here only that one test on PI job runs
on an arm everything else runs on a
Amazon ec2 instance that is just stock
Linux so first stage of my pipeline is a
pre compilation test that runs code
quality checks and static analysis
second step is compilation now I am
building for two architectures but
that's all happening on the same machine
and that's generating docker images that
contain binaries for arm and x86 that
means that I can do some testing on x86
and not have to use slightly more
cantankerous embedded systems and here's
where that arm co worker gets used in
the third stage we run some testing on
x86 and we run the testing directly on
the arm system and so I have a number of
Raspberry Pi systems which sit in the
office and connect to my CI system
now the final stage is deployment and
because I'm using docker it's possible
to have multiple images in each registry
for a particular product so all the side
branches for feature branches and
developer test branches get pushed up
with a tag of the developers name the
master branch pushes to my main registry
and that gets tagged as staging or
production and any commit to the master
branch will automatically push to the
staging repository and require manual
approval to push to the master
repository so when you want to do a
release you can test it on your staging
systems go yes that's good hit the
button goes out to production and you
can build all that with as little as two
machines but if you need hundreds then
it'll scale perfectly well and this is
pretty much the entirety of all the
configuration I needed to set up that
pipeline I use worker tags to force
certain jobs to run on arm I really
don't like to put too much configuration
in my CI system because it is important
to be able to run the tests manually
from your workstation when you want to
so most of the logic is actually in the
make file and the test scripts and even
with the best of intentions temporary
developer backdoors stick around so the
way I deal with that is I generate a
developer debug version of every
software change and a production version
you can't really out lazy programmers
and I say that as a as a 25-year
programmer so setup your system so that
every CI build gives you that lockdown
version and the developer friendly
version you don't really ever want to
hear a developer say I'll just hack this
into the production build and then take
it out later
Edythe harbor this morning told several
stories where lab code left in
production builds cost hundreds of
millions of dollars per incident one of
them was in fact a a feature flag named
do not enable this flag and guess what
happened
last thing about testing is you may want
to set up daily regression tests
performance comparison tests over over
long time basis and you can do this by
dedicating hardware to those jobs and
basically nailing it to the wall so you
made it to a release
now there's 10,000 instances of your
code out there and they're up a pole or
buried under ash felt or fired into
space so don't stop testing when we run
our test in production we call them
dashboards Bond villains are big on
dashboards you know the funny thing is I
set out to find a Bond moving image for
this slide and they all rubbish compared
to reality it seems that a generation of
nerds have created a future straight out
of fiction that image up there is from
hootsuite operations sensor and that
only shows about a third of the room a
social media company is now more
impressive than a global supervillain
but you don't need all that rubbish
because who's looking at those screens
I count 63 screens in the full-size
image of that and for people that's 16
or more for every person in the room
we need machines to watch our machines
so the simplest approach is gauges that
have a red line and an alarm that goes
off when the needle hits the red line
next level up is what I call artificial
stupidity which is you ignore everything
which is provably safe and then you
refer the things that are maybe not to
people and finally you can actually
apply machine learning in June the the
most recent release of elasticsearch
actually has machine learning anomaly
detection for log analysis so it'll look
at your application outputs and tell you
whether or not things are off the rails
so you've got some tested code it's been
encapsulated in container image and now
you need a device on which to run it
don't turn into the trap of turning
yourself into a robot some people say
that as a programmer you should automate
anything that you ever have to do more
than twice well I'm going to save you
those two times and say automate it from
iteration zero if you are setting up a
system to become the base operating
system for an IOT device you'll find
pretty soon that it's quicker to use an
orchestration tool than it is to do it
by hand even once and anyway you were
wrong about only needing to do it once
and that's just something that I've seen
over and over project starts with
developers putting together a quick
prototype starting from a blank OS card
installing some tools editing the
configuration taking some notes on what
they did probably and then at six months
later and there's a new version of the
OS and nobody really remembers how to
get from a clean OS to an operating
platform and that is how many embedded
devices end up running eight-year-old
kernels that are riddled with bugs so
what's a better way again there's a
hierarchy of technologies that you can
use you could set up a target machine on
your network SSH into it using an
orchestration tool I happen to use
saltstack over SSH you could run your
target machine in a virtual machine or
an emulator so you could use vagrant or
qmu or you can in fact put your target
machine in a container and use docker
files to do the setup basically you pull
yourself up by your boot strap the
system that I've built uses saltstack
serverless mode which means your
installation system will use SSH to
connect into a blank target system it
will install some components on it and
then once the management agent is on the
system it talks to the management agent
and does the rest of the process so the
first part of my process is converting a
vendor image to a product base image
that runs through a bunch of repetitive
stuff that nobody wants to do by hand
setting up time zones and
locales and all that sort of stuff and
at the end of that process we join up to
the output of the CI pipeline so CIA
takes source code and turns it into
tested docker images the orchestration
system takes a tested docker image and
drops them on to the operating system
you mentioned that this process prepared
so we've closed the circle we have
continuous deployment and at that point
you might be thinking this sounds a lot
like a platform as a service offering
like openshift or elastic beanstalk and
you'd be right those systems provide a
pre-prepared operating system which you
don't really have a lot of control over
and you just drop your code onto it and
there are options in the IOT space that
do exactly that so one of those is a
project called rezoned at i/o which
provides a base linux image that
connects back to their cloud systems you
push your code up to their git repo
their system compiles it and pushes it
down as a docker image onto your
registered devices and they support a
number of common embedded systems and
they take requests for which ones to add
next so for example i'm pieter testing a
new device for them at the moment
another system that I quite like the
look of his Amazon Greengrass if you're
familiar with lambda Greengrass is
basically Han premisses lambda so you
use the same mechanisms but instead of
deploying to an anonymous container that
you never see you're deploying to an
embedded system would that you nominate
in the microcontroller space I like
another project called Mongoose OS this
one offers the same sort of facility but
without the central Cloud repository so
you you compile your code to a zip file
you upload that zip file to a device it
verifies and installs the content now
let's zoom in on that verifies part how
do you permit secure remote access to
your devices now the obvious way is with
x.509 certificates as used in the World
Wide Web only the public key
infrastructure for the World Wide Web is
basically a global
ten record so the hard part of public
key cryptography is authenticating the
remote party that's what the
certification authorities are there to
do on the web the alternative is to
secret agents exchanging briefcases in a
park which is exactly what we do we need
a tamper proof channel to get a unique
individual secret key from the factory
to a field device but our device was
made in the factory so we put the key in
in the device on the assembly line and
then shipped the device that way the way
I achieved that in my project is again
using saltstack
during final assembly tests we run the
tests we assign an identity to the
device provide it with a security
certificate and then put it in the box
so we have a trustworthy secure
communications with remote devices which
allows us to send digitally signed
software updates that the device will
trust because they're signed with a
certificate that it is carried around
inside itself since since it left the
factory so we have a secure way to get
our plates out of devices the next thing
we want to do is keep track of which
versions each device has and which are
available well it turns out we get that
for free from the our container system
docker containers have one or more tags
which are intended for version
identification the tag you might be
familiar with is that is the one called
latest which is the default tag that's
used if you don't supply one when you
ask for an image well we can define a
tag for staging an attack for demo and
an attack for debug and one for each
developer and whatever else we need and
in permanent tags for each numbered
version that we have shipped in the past
so now we know what version every device
is running on it and we have a way to
put request a particular version from
the repository so we have this close to
continuous delivery right now the last
thing we need is to control which
devices get which versions well our
orchestration system is also a
configuration figuration management
system and that has a pair of key value
stores one maintained on the server and
one on the devices so we can use the
configuration management tools just as
we would for goddes in the systems to
nominate what software we want on each
device
and then the deployment tools give us
continuous delivery from code through to
devices so we made it
we're deployed but we're still in the
woods I'm working on some devices right
now which are literally going to be set
in concrete and expect to keep working
for a decade when they break down you
will need a jackhammer and some earmuffs
to fix them so let's think about flash
memory each sector can only sustain
about 100,000 right cycles before it
becomes an X sector so you don't want to
be indiscriminately writing to your disk
so some of the things you should think
about it
turning off file system access time
stamps not writing log files to flash
memory the next systems have had an
ability to boot to ram disk for a long
time which is often used on raid systems
you can turn it on and you're embedded
systems which means if the part of the
disk goes bad you've still got a way to
boot up but say your concrete in case
the device goes into a crash loop
well the way I deal with this is my
systems send out a DHCP request at boot
time now typically there's no server
responding to that so they fall back to
their their configured IP but if a
server does respond then they use the
public key certificates that they have
to verify that they trust that response
and if so they drop into a mode where
they run a login server and you can log
in to them with a password that you
supplied to it over a secure Channel and
you then have the ability to repair or
reinstall the system so that's some of
the things that a device can do to look
after itself what else do we have in our
bag of tricks whether orchestration
system includes some beacon facilities
that transmit basic performance data so
our management system can provide a
report of devices that have gone silent
in the event where a device is still
alive but has a component failure we can
be a little more helpful and have it
call attention to itself when every
light bulb in a building is a self-aware
computer or every smoke alarm or every
lamppost in the street a little bit of
help working out which is the one that's
broken
is very handy now in the case where
devices aren't online all the time how
do we manage updates then well Amazon's
RT does a good job of this where it
provides synchronization tools for
offline devices but we can do the same
thing with an orchestration system in
saltstack
there's a component called the reactor
which responds to defined conditions so
we can trigger actions to occur when a
device connects and in particular we can
do things like check for pending updates
so next I want to consider what if it's
a problem that we can't fix with
software maybe the firearm got stuck on
a three o'clock in the morning you want
to be able to shutdown components that
require physical repair rather than have
them become a danger or just a nuisance
so if you can build the ability to
switch off some components into your
systems then I think you should do that
and finally what if the device is out
there and you're done with it you no
longer want to support it there's a
space probe orbiting Saturn right now
that has about twenty kilos of plutonium
on board as its power supply it's
running out of rocket fuel which means
it's going to lose control and someday
smack into a moon now since we think
that there could be life on some of
those moons of Saturn it really wouldn't
be neighborly of us to give them all
tentacle cancer so in a few days it'll
use the last of its fuel to camicazi
dive into Saturn itself now if you're
developing devices that could outlive
your software support I encourage you to
think about that situation the problems
recently with factories and businesses
still using abandoned Windows XP systems
shows us that if you can't patch it you
should unplug it one simple way to do
this could be to stop updating the
security certificates or put out
revocations for them okay so we're
coming to the end the last thing I want
to talk about is the data being emitted
from your things I mentioned that the
orchestration system are used sends a
heartbeat up to the server so that
allows me to visualize the availability
of systems I talked about Flatliners
earlier we can also collect longitudinal
information on availability and that
could be of interest when we have
service level agreements that we need to
meet
I also configure my devices to transmit
their CPU and memory usage stats every
so often 15 minutes an hour it doesn't
have to be too quick as part of that
post release testing regime if you put
out a release that has a memory leak or
a CPU tailspin then hopefully your
longitudinal performance data will tell
you about that before the devices become
dead now right now I'm just dumping all
this data into an elastic search
database and eyeball it from time to
time but there's many other things you
can do about that and I've put some
links to two other presentations on that
subject in these slides that image there
by the way is the control center for the
Brazilian football World Cup from last
year so once again we have reality
outdoing supervillains now one of my
clients puts mesh networks into aged
care facilities so you have maybe 50 or
100 sensors which talk to a local quick
gateway that has a cellular uplink so
the logs from the whole kaboodle
funneled back to the cloud and the
device keeps the last hour or so in
memory in case a technician needs to
look at it so that's the data from the
system software and we can use those
same mechanisms for the application
software or we can do something bespoke
so the first option is to use the
message bus that is maintained by the
orchestration software now for saltstack
that's that's a message transport called
zero and Q which is quite fast quite
lightweight can carry a fair bit of data
so I think that's usually a good idea
I've done it as a proof of concept I
haven't deployed a product using that
yet and one of the good things about
using that is you've got command-line
tools on your device to inject messages
there's quite a comprehensive Python
library to talk to the message systems
if you want to get more fancy you can
write modules to extend the
orchestration system both on the client
and the server to inject custom status
messages and do useful things with them
at the other end
my approach to instrumentation is
basically to overshare
I think it's better to have logs and not
need them
need them and not have them the project
with the subterranean census is fairly
low volume so I send that data up the
orchestration bus and at the master
system there's a rule that pulls it out
and forwards it into my log stash
cluster there's another project that I'm
working on that has significantly higher
data volume and for that I wanted a
separate channel so those field devices
run an mqtt message broker and the cloud
hub connects to each broker at each site
and receives the big firehose of data
back from a lot of them and it feeds
that river of data into our data Lake so
very briefly I want to talk now about
visualization we've already looked at
system performance data so I won't say
too much more about that and certainly
system administrators have known how to
visualize their their performance data
for a long time but what people measure
less often is the bottom line would you
know if whatever makes you money stopped
working do you does anybody remember our
telco called one tell about 15 years ago
their billing system was not very good
about issuing bills oddly enough they
went bankrupt how long would you stay in
business with no income but broken is
not always as simple as on or off so
sometimes you know that normal is within
a certain range and if things are not
normal you probably want to know and
data analysis tools that can do that a
big business there's a ton of them to
choose from and the best way to pick one
is probably to go out to the exhibition
and see who's got the best schwag if
you're interested in rolling your own
have a look at a system called node-red
which is a dataflow processing system
targeted at IOT I've done some
presentations on it there's a link there
in the slide the last two things to
consider are the long-term trends in
your KPIs are you gradually going bust
are you gradually going boom and finally
there could be parts in your devices
which wear out now you know your Flash
storage and your battery are the to that
spring demand so you might want to
consider tracking some metrics about
their long-term performance batteries in
particular on our
an area where there's lies damned lies
and manufacture specifications so let's
summarize or what I've talked about
we've looked at the threat landscape
we've worked through the product
lifecycle from development through to
testing deployment management and
retirement we've covered some things
that you can do with the data that comes
at the end and if you want to hear more
about that next point come along to you
our data conference here in Sydney and
September where myself and also some
very smart people will talk about dahara
so thank you for your time today
I am very happy to take questions in the
10 minutes or so we have remaining and I
will be here all week if you want to
have a longer chat so thanks again and
over to you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>