<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a Serverless, EventSourced Slack clone - Andy Davies | Coder Coacher - Coaching Coders</title><meta content="Building a Serverless, EventSourced Slack clone - Andy Davies - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a Serverless, EventSourced Slack clone - Andy Davies</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8dvzgCcvdGY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Andy Davis I work for a
company called Lynde off providing their
payment solution so if you've ever
bought something online and you've got
the option to pay with an invoice or
with an installment it's probably
provided by us I decided to do a service
events or slack loan partially because
it's got lots of buzzwords in it so I
get people to come and partially because
I wanted to learn more about AWS I use
some parts of it very well other parts
of it I haven't really used so mostly a
learning exercise and how various areas
work usually I've used ec2 and quite a
lot of s3 usage that's primarily what I
use in a de Bresse and a little bit of
lambda usage but I wanted to try and
build something with a constraint of not
having anything that wasn't serverless
despite running on servers then aiming a
nicely but hey as you can get and I've
mentioned a lot this is going to be a
dabbler space but the principles of
usage and some of the tooling will apply
to other cloud providers and my actual
product I decided on was a chat app
because you know everyone like slack and
chat apps are simple right one of those
problems that the more you look at it
the more not simple it can be
which obviously gets into the first
hardest part in Salford arm which is
analysis paralysis sat there for weeks
going now do I do this or this or this
and then later breast comes in and goes
no you can't do that so yeah issues
everywhere so for the first audience
interaction who here has heard of
terraform and hands up like for people a
few so for those of you who haven't
heard of it it's a tool for scripting
your cloud infrastructure it works on a
Tobias obviously and as your and other
cloud providers if there are any others
the only downside to terraform is
company toe support is not there yet
we'll get into what cognitivism why
that's annoying later but that's a it's
only drawback that I have if you want
really thorough in-depth talks under how
to use terraform I will have some
examples but there are a lot of great
talks by Paul stack and James Nugent you
can go and look at online from previous
NDC's that are great that's how I picked
it up yeah not Pablito so this is what a
terraform file can look like this is our
variables file so we put common
information in here in this case we've
got we cater breast region we want to
use by default a name of a bucket that
we're going to use crow bar incidentally
as the name of my app because lambdas
crowbar half-life entered the stealing
of the half-life 2 logo and the final
interesting part in this one is the data
attribute at the bottom this will fetch
your Amazon account ID when it is called
so you don't have to hard code that
number into your into a terraform file
anyway which I have been doing for quite
a long time and every time I've had hard
code the number not liked it because
everything else like your actual
credentials are stored on your machine
so they don't enter your source control
but here's an account ID in your source
control and I don't like that so after I
found out how to use this ADA bus caller
identity that problem goes away to
actually set up a resource in AWS with
terraform is fairly simple this looks
like Jason it's hashey corpse
configuration language which is a
superset of Jason I think I think they
support there's some relation with Jason
anyway you don't have to put commas in
and semicolons so this resource controls
a bucket
it gives it a name and make to public
readable nice and simple
unfortunately not everything is simple
for instance this is how you created an
iron policy an iron policy this
particular policy is what I use for all
my lambdas so that when they're
activated by various when they're
executing they have access to various
other resources such as cloud watch if
you give them access to cloud watch
whenever you do a console log statement
in a lambda it'll appear in cloud watch
this makes the bugging much easier and
once you find out that that's the
permission you've been missing which
took me a long time then everything gets
a lot easier no devouring consoling
everyone go oh you're all your logs
would just appear in in cloud watch and
they don't if you haven't got that
permission but there's no we're saying
the reason they haven't appeared in our
watch is because you're missing your
permission the template file mentioned
at the top of this is because policies
in AWS parlance said written in JSON
files this is the same JSON that you
access through the UI the only
difference here is I'm using a
replacement field so that I can put my
bucket name in without having to hard
code it in multiple places so that's a
brief overview of how you use terraform
we'll have some more examples for the
Darracq various areas of a degrace as we
go through so onto my fantastic UI
skills which are non-existent this is
the UI for my chat app it's written
using react Redux and uses bootstrap and
loop strap material UI because someone
else has worked out how to make pretty
you eyes and I don't have to because I
normally do back-end engineering rather
than front-end stuff as this year line
shows all I'm allowing my users to do
for this is create channels send
messages and that's pretty much it
but despite only having a few functions
we hit a lot of different areas of AWS
which I've never used before
so to host a website in s3 terraform
again so as before we have an s3 bucket
setup the only difference this time
hopefully this will work
okay thanks Simon Brown for this pointer
the only difference here is we've got a
policy of public read and a website
configuration setup this means that your
bucket gets a name which is unique to
the whole of or has to be unique within
an entire AWG so you can't just create a
bucket called admin because someone else
is probably got a static website called
admin already so if I wanted to call my
bucket crowbar the likelihood is well
one I tried applying it once at the time
someone had a public website called
crowbar already so that was annoying and
I didn't think I don't think it was me
in the end I think it was actually
someone else by using s3 for our website
we get a highly available front-end s3
is very scalable and almost never goes
down and we also end up with a slight
limit of routing has to be hash based
you can't use your L routing in an s3
bucket because they have to actually be
documents it becomes a path segment in
the bucket name which means you have to
set up Redux and react Reuter and react
trigger with hash routing and get all
the right versions which is a pain and
well it took me weeks to get it right
and if I upload when I upload the curve
this if I decide not to squash and I get
commit history you will see how long it
took me to get routing working and how
many times I had to go back to it when
it turned out it wasn't working but I
thought it was so yeah out of everything
the hardest two parts are setting up
iron permissions and raging at react but
after everything gets easy mmm
this is our policy for the s3 bucket
pretty much the same as before except
we're allowing anyone to read this
object and they can only ever fetch
doctor
in the bucket we don't want anyone to be
able to post replacement index.html page
to our website that would cause bad
things to happen the actual architecture
of this system comes down to a set of
commands that you issue from your UI
registering a user creating a channel
joining sending messages leaving
channels this is why I thought it was a
simple problem to start with because
although it's just a couple of messages
I mean you can just runs and projections
of that and job done and it doesn't
quite work out that simple the first
point is do you model this as one
aggregate that has all of these messages
coming into it or do you model it as
multiple so say user aggregate and the
channel aggregate and part of that comes
down to the decision of how you want to
treat a channel so I had the option
early on of do you send a message to a
channel or do you send a message and say
this message goes to these channels so
more like a twitter hash tagging you
could send a message and say hash
journal hash project1 and that changes
your design quite fundamentally up front
in the end I decided as I'm building a
slight clone I should probably send
messages to a channel rather than
building Twitter in slack which would be
an interesting problem to start with
anyway so initially I came up with the
idea of these two separate aggregates
one for users one for channels the
downside I realized is that both
aggregates care about some of the same
messages which might be fine if
everything works well but what happens
if you have a user and you say I'm going
to create a channel and the user occurs
right that's fine you've created the
channel I care about this so I've
written it at this store and I've said
that you've done it and that's great but
the channel aggregate receives the
command sign character telling us was
already channel got this he can't do it
now you've got to aggregates in
different different
space I wasn't quite sure how to solve
this but wrote out my architecture
anyway looking like this the first block
which is the the yellow is a an API
gateway so this is Amazon's provider of
a restful or HTTP API and you can hook
it up to lambda and all other kinds of
services and you get again highly
available API that just scales so when a
command comes into this we store it in a
command store dispatch it to all of our
aggregates if they decide to do
something with that particular command
then emit some events events go into
their own event stores and projections
are triggered off which are addressed
lambdas and write us some JSON views so
I'm using s3 for my my views because
then I can just leave them as public and
or authenticate is so valid logged in
users the only people that can see them
and we can just pull them straight into
the EUI itself
this makes views almost free because
JSON objects are fairly small and highly
available again so the the costing of
fetching a JSON object from s3 is
minimal so I don't have to pipe it
through API gateway and pay an API cost
or throw anything else the downside to
this I realized is a fairly complicated
for a first implementation detail or
implementation and I have that problem
with the aggregates not necessarily
being in the same state so if we look at
how you might implement an aggregate in
c-sharp both as anyone done event
sourcing in c-sharp before okay whew and
CQRS like the buzzword done it not done
it right so in the case of this we have
a public join method public method
called join passing a user will validate
that that's a valid thing to happen
create our event and then call the
supply methods apply just stores the
events in memory so when you call save
on the area we write them to the event
store the handle method beneath gets
cooled by the apply method so actually
does your state change and that is also
called when you're reloading your
accurate so if you have multiple events
to reload you to create a blank object
and just say apply all of these events
in order and you get back a fully
hydrated aggregate we also have
projections so aggregates can only have
be loaded by their primary PI D which in
our cases are good but quite often we
want to know summary information about
aggregates like what is a list of all
the channels available who will be users
in my system so we have an asynchronous
projection this sits outside of the the
aggregate and when an event it comes to
that it cares about comes through in
this case user join channel and channel
created it does something and writes it
to of you i've list out the loss of the
error handling type code and actual
writing to views because just are
interested in the structure for the time
being this is also using another view
because our user joined channel event
only crates contains a user ID and the
channel ID we already know the channel
because it's us that the user ID we want
to look up the current name of the user
so we're introducing a coupling between
to view to rather than the name of the
user at the timeless and the message
we're showing the name of the user as it
stands now this is great because you
don't want someone to be as a log into
your chat system under one name say
something really mean and then change
their name and it was now shelf as two
different people saying things in the
background they still be tied to the
same user ID but because it has the name
different you could potentially hurt
people's feelings with by you know
pretending to be someone else
temple covering like this does have
issues but for the time being this is an
acceptable trade-off for me because I
want to show current user so if we take
our aggregate and our projection and I
just crush them up a bit so they fit on
screen just looks better on my monitor
yeah so now they fit on the screen but
we can split this in that this works
fine for a completely server-side
application but we could split it so
that on the client side our JavaScript
will pray to command validate the
command useful and dispatch it
internally the dispatch method is just
calling one of these API go is gateways
in AWS on the server side we only need
to handle that we've received a command
so we grab take the command that
appeared store it somewhere and trigger
our aggregates and then asynchronously
the aggregates run off creating our
views and projections now you might
notice on using JavaScript on the server
side as well as on the client side this
is not in a particular love for node.js
I have a love-hate relationship with
JavaScript it's great and it's terrible
the reason I'm using on the server side
is because response times I've found
that a c-sharp lambda doesn't have a
consistent response time when you invoke
it from an API or maybe when you invoke
it at all so this particular the middle
method for writing the command takes
about 200 milliseconds for the
JavaScript version to run when I
implemented the same thing in C share of
the second run of it took 800
milliseconds as did the third and the
fourth run the first run took 15 seconds
and as he puts a timeout setting on each
lambda and it's normally I have it at
about 3 seconds because that's a
reasonably long time for a web request
if your first lambda doesn't finish
then none of the subsequent ones will
finish because there each one is trying
to I guess do some compilation in the
background to killing the image around
databases infrastructure but for
whatever reason c-sharp is a response
handler for a web guy in native versus
not a good idea at the moment I guess is
its new there'll be improvements to it
if you're doing something is
computationally bound longer running
lambdas where you've got a timeout of or
the maximum 300 seconds I think so if
you have something that's taking a few
minutes and computation bound then sure
a c-sharp version will be fine but for
something you want low latency quick
responses stick with the draft script
you can still write tests in JavaScript
so hopefully it'll work
so our actual aggregate when implemented
in in JavaScript on the server side
looks like this the joint channel method
at the top this all gets called by the
command and I realize that I don't
actually have a need logic as such need
to be done with my commands if I get a
user join channel command comes through
if they're already in the channel that's
fine
it's an idempotent update we can just
keep adding the use of the channel and
if they're already in there we're not
going to do anything if they aren't in
there we'll just add them and then I
looked at the sending of message source
all each message has a unique identifier
on it so if we get the same message
twice we'll just not do anything with it
idempotent again so why bother our
command roughly looks like this we have
a timestamp have an event ID which is
generated for each message that comes
into the system we have a type and in
this case we have a user ID in the
channel ID so this is joining a channel
the event that comes out to the end of
this looks like this which is identical
apart from the fact that the event has
gone from a command join the channel to
a past tense event a user did join the
channel now so I don't have any logic
that would ever say BAM a blocker user
from joining a channel or from sending a
message it's assumed if you're signed up
for my application you're allowed to
send messages why bother with the
aggregates in the tool well in the first
place why why not just use projections
for everything so we can simplify our
architecture down to this in this case
we just dispatched the events from the
client side rather than commands store
them somewhere
triple projections nice and simple good
first product if we need the commands
and the aggregates and we decide later
on that there is logic we want to do for
users joining channels maybe you want to
prevent people from putting swear words
and their messages that kind of thing
then we might need the efforts at which
point we've got
the events stored and as the events were
almost identical commands we can just
replay them into a command store just
changing them back into commands and
then run the whole system again and you
should end up with the same state so
we've got a want to say future-proof
architect architecture but we've at
least got a planned upgrade path now the
actual event storage is one of the first
places that I hit problems with with AWS
this is effectively what our event
eventing looks like we have producers
which is in our case the Web API which
is emitting events and then we have two
consumers or as many as we want which
are aggregates or our projections
producers produce event consumers read
them off as quickly as they can to
create your views which is an ideal use
case for Kafka as anyone here used kefka
before heard of okay the same people use
picking and sticking their hands up for
each question
I'm not giving prizes for saying yes to
things by the way so so categories log
story is a distributed log storage so we
can use that as our event log we have
one node one event is the a log line or
log entry and we can use that for all of
our storage and we can keep all of our
our data there the downside to this is
AWS doesn't offer Kafka as a service it
does however offer Kinesis Kinesis is
very very similar to Kafka at least it's
streaming kanita streams are very
similar to Kafka the problem with it is
is that we want to use Kafka as our
actual log storage for forever and
Kinesis has a limit of seven days as an
entry can live in it so we can't use it
as permanent storage so for a demo I
could use contentious
because I don't foresee a demo going on
for longer than two weeks
I mean I'm timebox to an hour as it is
so I could demo it and be like look it
works fine and as long as you didn't
check it back in two weeks it would look
fine so I don't really want to use
Kinesis I could use Gasca but I'd have
to host it myself in Oedipus
in 82 or I could host it with docker
containers docker is provided as a
service by AWS so hosting a docker
container in AWS is technically service
I guess it's a bit of a cop-out though
so I decided to try not to use that
because I want to break my break my
service constraint on the first issue
Irgun t so instead I felt on Amazon's
got many other ways of storing data s3
would be ideal if it had an append mode
you can only read or write a file in it
s3 you can't append to it you have to
read it add a line write it back and
hope that no one else has written it
back in the meantime so if two users
send messages at the same time I could
lose data or I could use their update
mechanics to prevent that but I'd then
end up with two lambdas firing around
after each other trying not to over
writing then I'd end up with a deadlock
so I can't use s3 I could use the
relational database service store them
in Postgres which is the best database
so you know that was a very appealing
way of storing it but I've used Postgres
before so let's store it in something
different so I decided to store it in
dynamo dynamo is a document store
offered by lab Lewis very easy to use
has some interesting constraints which
I'll show you in a minute the actual
setup for a document database it's very
simple again this is using terraform the
interesting parts of this are the right
capacity and weak capacity so right
capacity is number of one kilobyte
writes per second you want to do and
it's rounded up to the nearest kilobyte
so if you've got a 512 byte event then
that will be rounded up to a kilobyte
encounters one
but as I'm storing two keys suspicion
specifically the actual hashkee the
unique identifier and a range I actually
use two capacities per right one for the
event and one for the index so in this
case I can write two and a half messages
a second which when I'm demoing this app
is fine because there's generally only
me on it so as long as I don't type
really quickly it's fine and if anyone
seen me type you'll know they're typing
quickly means making mistakes and then
deleting things so that's why because
this is a document store we can store
anything in our in our events we can
just chuck them straight into into the
dynamo table and retrieve them as as a
as is or we can query them by event ID
and I've also defined the timestamp so
if I've got a projection that says I
want to know about all the events that
happened between these two dates I can
pull them back by this this is also how
I'm implementing the ability to resume
processing from the last venue store
because if we have the the good of the
previous event we can look up its
timestamp and just say get me everything
after the timestamp now the limit of
dynamodb is that this event is in my
channel created event as before
timestamp event ID type boring user ID
and channel ID then we have the channel
name and an optional channel description
attribute if you try and insert this
event into the inter dynamo it will fail
with a weird error message I can't
remember what the error message is off
the top of my head but this one will
work because
dynamodb doesn't let you store empty
string values null is fine empty strings
not fine which is the opposite way onto
what I was expecting
research on the EDA various forums
indicates this is by design for reasons
and a lot of people who disagree with
these reasons and Amazon don't seem to
want to fix this
so the suggested workaround for them is
to use a value in your string which will
signify that the string is empty so I
guess you generate a good at the
beginning of your app and just say
whenever I see this good it's an empty
string because that's unique and
hopefully there won't be another one in
my case I just check if my text box is
empty and write null to the value
instead that really annoys me because it
really violates the principle of least
surprise in an API why would this fail
after we've moved on past that so as
long as you don't try and store Nell's
it's fine so now we've got a way of
storing our events we now need a way to
actually create events and put them into
this into dimer so here is our setup for
an API gateway in AWS the top block just
says that we want to know the iron gives
it the description and then we have an
API gateway resource which is a path
putt so if you want to add events slash
channel you have two resources one
called events one called channel and
just say that the channel is that it's a
child of events so you can create
multiple hierarchies of the URLs the
downside of this is how much typing you
have to do for it if you've ever used a
lambda library called Claudia jazz they
get around this by just registering the
group URL and then handling routing
internally themselves so all everything
comes into one handler and then they do
routing dispatch it to your hand to your
own functions we've also got post method
sets up on the events because that's all
I care about I don't want people to be
able to get anything from my API you
just need to be able to push event to it
all the reads come from our views and
you'll notice on my post events when my
cursors come there is we have this
lovely authorization equals none this
means anyone can write to my event API
as long as they put their own write
structure and don't put a null key in it
defines so an empty shrinky but that's
only there because terraform doesn't
support coordinator yet so to put
validation on api we have to manually
click around in the UI or use their
duress here like to write a Cognito
authorizer for it yeah and the last
block just says that when this posts
method is run we will take the data from
it and post it to the lambda function
that we want to act as our handler
you'll know that the post in the word
post appears twice on here this is the
api's method so you could have post get
put options that kind of thing the
integration method if you're calling a
lambda has to be post when I first was
doing this I had get in both places and
once again getting strange error
messages because it was trying to do a
get request to a lambda which is not a
valid thing to do so yeah that has to
say post for the type of a degress proxy
most of this code is taken off the
terraform documentation pages there very
complete and quite secure their
developers are very responsive on
twitter if you have questions i harass
them a lot the actual handler we're
going to run when a message or when this
get API is called looks like this the
ADA blue event comes in we use the
object of sign method to force in a
timestamp that we control because I
don't want to rely on the users
computers clock being right and we will
supply an event ID if there's not
already one in the in the event if I was
actually doing this in production I'm
not sure if I'd treat event ID first I
might force them to have my event ID the
reason I do it this way around is
because if I spent specify the event ID
on the client side for debugging
purposes I can trace it all the way
through I can see in my logs
and a client where the event was
generated and then I can see into the a
degress cloud watch logs to see what
went wrong why it wasn't stored where
there was an empty string in it which is
usually in my problem so once we've
hydrated our event with a few more
properties we've right into storage
trigger projections and send okay to the
room user because we're using react we
automatically stick the result of them
typing a message straight on-screen and
I put a little indicating exit to show
the you know it's probably there and
they want it it's confirmed by
adolescence appeared in the view then
the UI changes to reflect you know here
we're not assigning a sequence ID or
anything to our events because I didn't
want any kind of blocking resource so
you know this is the next event if ten
people right at once or a thousand
people use my application which seems
unlikely I don't want to cause any kind
of blocking we could get around this
using Postgres
so again if you decided that you want to
have that many users and you need a
sequential ID and changing the data
store to post press is probably a good
idea in fact if I was doing this for
real in the future I'd probably use
Postgres instead of dynamo because
perhaps Curtis is the best database in
the world I don't get paid to say that
by the way I just think it is actually
invoking another lambda and there is a
lot of conflicting advice about this
online a lot of people say you should
use the notification service too with
the lambda hooked up to the notification
services to send a notification to
notification service and that invokes a
lambda and that does the work when you
can just invoke the lambda and not care
about the callback so that's what I do
I've missed out a lot of error handling
from these by the way when I upload the
code you'll see that there's probably
another 50% worth of error handling in
this stuff rather than actual business
code so in our case here we are just
invoking another lambda called kro-bar
projections and that itself looks like
this
I have a function for each projection
given an event each projection runs it
and then the update view method here is
just a wraparound s3 so I can test the
my projections work without having to
rely on database infrastructure to
downside the way I've written this is
that all my projections run inside one
lambda so if you have one really badly
performing projection all of the others
are going to be slow as well this is
fine for my first product or my first
deployment because I'm only projecting
users and channels there's not that much
information to slow things down again in
the future when you start having more
users you can then upgrade your
architecture to have each projection
running in its own lambda this is one of
the the maintenance of deploying the
simplest thing you need don't over
architect in the beginning because well
the final architecture slide that I
showed I wouldn't want to start with
that because there's too many moving
parts that I will get wrong and tear out
what hair I have left an actual
projection slow this so we just have a
an object of handlers so channel created
user joins channel all they do is they
work them in a mutable manner so they
return a new version of the view updated
with whatever the projection does and
return it in this physical so we just
return an object back this means that I
can test to each handler by just running
the callback on a set known view and in
the the main production code we running
this update view function which is
actually reading from s3 applying the
projection checking the s3 is not being
updated when writing it back
so this means our architecture looks
like this you know we're not actually
using the event storage for anything in
particular yet it's just there for
future usage one we want to make our
architecture more fancy this works we
have a statically hosted website we
called Web API we store events we
trigger projections when we read views
from s3 there are problems with the the
actual projections which we'll go into
later but if you're just one or two
users in a chat room you're not going to
notice them secondly start having more
than a few users they start becoming
more evident but the part I want to talk
about next
so I've alluded to a few times is
kognito kognito is a service by Amazon
for providing authorization and
authentication it's an implementation of
Open ID Connect and it's very easy to
use there are a lot of examples and on
ADA vs. github page they have an entire
repository dedicated to how to use it
from JavaScript a brief summary of how
to use it for JavaScript looks like this
this is signing up of a user the
important part is this function so in
our case we're generating lis good as I
user ID I'm just putting the user the
users email address in as an attribute
the reason we do this is because what we
could use their email addresses yet as
the ID the ID is immutable once you've
written it you can't change it so if the
user wants to change their email address
in the future they wouldn't be able to
log in as that account anymore so what
we do instead is use a good as they use
your ID email address as an attribute
and in the Cognito setup you say that
you want to allow login from that
attribute so i've loud login from
aggress
this is very straightforward preferred
signing up login is marginally more
complicated we take their username and
password email address in this case set
up our user pool and then authenticate
the user assuming things work correctly
i dispatched the user to the rest of the
react system so that it now knows of the
user and resolved the promise the
downside i've had this the thing that
really was not comfortable with
cognitive about was this setup of the
user pool because on my client side I
now have a user pool ID and a client ID
and that's my client is JavaScript
anyone can read these and that doesn't
seem very secure to me I don't want to
give out any kind of editors credentials
anywhere but searching again on the
other breast forms real someone from
Amazon saying that unauthentic these two
IDs can only be used to call an
authenticated API so signing up
authenticating forgotten password so
there's no malicious activity they can
do with just these two IDs that implies
there is malicious activity they could
do with those IDs and something else and
they don't know what the something else
is so apparently this is fines I just
leave it on the client side and this is
only a demo app so you know it's not
going to hit production if you really
wanted to hide them I think you could
probably put your user pool usage inside
a lambda function in API gateway and do
it like that
i I guess I'd need someone who actually
knows a to ref security to come and tell
me if I'm doing this right or wrong but
works on 1g and anyone else is possibly
now that we have authentication and have
manually added it to our API we also
need to know about using sign up now
when a user signs up we have two stages
to that they enter email address they
get a link in their email or
code in there emo which has go back to
the web application type this code in to
prove that it's Yuri Magus basic email
address confirmation which means I don't
consider a user fully registered until
they've approved their email address so
I don't want to dispatch my user
registered event straightaway like when
they hit my mother as of this and my
password is whatever submit I don't want
to send a user registered event at that
time because they might not ever follow
through that might not be their email
address or they've made a typo so the
obvious solution and bad one which you
shouldn't do is just a wait for a time
just say ten minutes after they've
they've signed up check to see if
they've confirmed if they have great
successful user this wouldn't be a good
way of doing it for many obvious reasons
but you know if that was the only way of
doing it luckily kognito supports
triggers lots of AWS services support
triggering so we can put our own lambdas
into the processes and either replace
parts of a process or customize it in
the case of Cognito one of them is the
post confirmation trigger this is after
the users confirm that the email
addresses actually there's run the
lambda there are also ones for pre
authentication and post authentication
which would make for writing interesting
projections of a user attempted login
and then a failed or a successful login
you could then write a projection that
would say look for users who have got a
high number of attempts at logging in
and we might have someone who's trying
to brute force their way and through a
user account so we can put that kind of
event in data into our event stream as
well the actual handler for one of these
looks remarkably similar to the API
Handler the only difference is we're
controlling the whole event ourselves we
grab the user attributes that come
through so in our case we only after the
email and the sub which is the unique
identifier for that account
and we write at our event stream and
trigger all of our projections again so
nice and straightforward that one that
one calls me a lot less hassle than the
API gateway one API gateway is also
fairly picky on the response Jason you
give it if it doesn't have a body
property it fails if it doesn't have a
header property it sails but it only
gives you a 500 error and just something
like invalid Jason and you look at it
going oh what's invalid yes and
everything looks fine to me
it's valid Jason it's just not
semantically valid so as I've alluded to
a few times in this scalability becomes
a problem with this architecture so
while the cognitive side of things and
the API gateway on the website and the
first lambda is scalable the projection
side of things is scalable in that
they'll run and you can have many of
them running but you might lose data
parallel updates to s3 you can use a
more the word for it is like a last
right time so when you write to test
three you can say if the last write time
is different from when I read it fail
the update but if you've now got two or
three or ten or twenty lambdas all doing
the same thing you have 20 language
going oh that failed I'll try it again
of that failed at red and you've got 20
of them and more messages coming in
you're just going to end up with a
massive pile of deadlocks and if you
don't have a timeout on your lambdas or
you have a large time at you will end up
with quite a large a diverse bill
because a 300 millisecond or 300 second
lambda running for forever
or as many of them as you can have is
going to cause problems
luckily a SS supports a lot more
services that we can use to negate a lot
of these problems the simplest way to do
this is just to stick the simple
notification service and simple queuing
service in between the tool and the
notification service literally
distributes notifications to as many
queues as it's got attached in our case
we've only got one queue but in the
future if we want to put our projections
in separates lambdas rather than having
them all running in one we could have a
queue parrot projection now each
projection doesn't impact the running of
any other projections we can also use
this queue for when we want to reload
events so if we want to if we come up
with a new projection we say right we
want a and all users who've posted in
these channels in the last 10 days
production we never thought we wanted
this before but now we do we can just
grab all of the information out of the
event store and push it straight into
either a specific queue for one
projection or we can push it into SNS
and have all of our projections rerun so
if we want to rebuild our data model
that's how we do it and then still sqs
drop down
we can also use this to make our
application more flexible and support
plugins as everyone wants the right
plugins one of the initial plugins I
wanted to write for this like before I'd
even written the application was I
wanted to write an IRC plugin because I
wanted to be able to interact with this
as if it was IRC because slack is IRC
with a really heavy slow client why we
don't still use IRC I don't know and you
can write a chat bot for IRC so why you
slack and it looks pretty little right
plugins I put each one in a separate
queue we really don't want a bad plug-in
with a bad projection to slow down the
running of the rest of our application
we also gained because we've got this
queue in place when we deploy a plug-in
to our our application we can have a tip
box that says yes I want all of the
events for all history to come through
my plugin please hopefully that would
not be kicked very often because I would
imagine that would create quite a large
queue after a long time but if that's
what a blogging author wants they have
the ability to have that using lambdas
for our plugins also it came to us a
couple of interesting use cases we can
either provide a pre-baked Lander that
will run another JavaScript function
which might be like write to a view and
we can handle the fact that it writes to
s3 or does something in the background
we just say hey here's some data do what
you want and we'll deal with the storage
of this but we could also provide a
lambda that's automatically invokes an
HTTP endpoint or you could just rather
than using a lambda at all have a dar m
for a lambda the the full resource name
which as we saw in the terraform setup
is the gigantic string of text and says
where a lambda is where you should post
it but if we let a user specify their
own ADA vs. account lambda we can invoke
that from argue now they can do whatever
they want however they want in their own
a SS account and it not going to cost us
and which is correct because why proof
why should we pay for someone else's
plug into the run so if they get the the
full usage of a duress they could do
machine learning or or some kind of a
ion the people's chap messages and it
wouldn't cost us anything
data retention might become an issue
depending on why your jurisdiction is
and you wouldn't want to just hand
messages out to anyone so there is an
interesting issue there but it does
provide a way of having a nice cutoff
between this is the core systems account
and then we can have a different account
for plugins or a different account per
plugin as I mentioned writing one
wanting to write an IRC plug-in I'm not
convinced I could write it in a service
manner because I think you need a
persistent connection I had a brief look
through the through the the two RFC's
that cover the cover implementing IRC
but there's also a lot of documentation
saying these are the Taurus RFC's that
define it and no server implements these
they do this slightly differently and
quite all the you know this is enough of
a problem as there's - right without
having to try and write IRC as well
maybe later someone wants to send a pull
request for an arson client also a
little wrap-up of my experiences so far
I have learnt loads about the extra a
diverse functionality mostly around
cognitive I want to now use kognito for
other things like possibly
server-to-server authentication because
it can support that - I've learnt loads
more about terraform there's like I said
as I normally only use ec2 and s3 and a
tiny bit of lambda I have now learned
about configuring IP I get raised and
how painful configuring I am is no
matter how you're doing it but if it's
in terraform at least you can version
control it and then once something stops
working you at least now have version
history of why it stopped working
the trade-offs of making a service at
architecture yes it is possible would I
actually implement something completely
service maybe it's an interesting
trade-off to make it definitely makes
you think about problems in the
different way but in the end I'd still
rather of stored everything in Kalka or
had Kinesis with us you know infinite
storage but the other part of the
service trade-off which this hasn't
really touched on at all is cost now
while I think you should always pick the
right tool for the jobs there is a
little asterisk there on the end of that
which says the right tool for the job
within budget just because something is
available as service and you don't have
to manage the servers in the background
because there are servers despite the
name doesn't mean that it's the right
thing to do for your company or for your
budget we have a as an example we have a
service work which when people we have
multiple systems and when someone views
someone's personal information an event
is written to accuse saying who wrote
who read the information what system
what screen they were using there is a
service that reads that off the cube D
normalizes a bit put in a write
structure stores it somewhere and the
separate service which you can query to
get a report so when an end-user goes
hey I want to know who look to my data
we just look at the report now this is
currently sitting on top of the nisi two
instance it's got two services written
in it and then we have an ec2 instance
for dev tests QA and production so there
is contractual reasons we have a
overhead cost for each ec2 machine we
have so add an extra 30 euros 3tc to
machine per month and we now have
suddenly fairly expensive system not
doing a lot we don't care about the
uptime of it because if the system goes
down the queue just fills up when it
comes back up it reads off the queue
externally quickly
so we're paying one 120 ish euros a
month for a system that we don't care
about uptime you get a million free
lambdas per month execution time every
month so forever so it's not just part
of the free tier Amazon that keeps going
so as long as we get less than a million
requests in total over a dev test QA and
production it's completely free to have
it in lambda so why don't we put it in
lambda mostly down to time and it's not
that important moment when we want to
save cost or a developer's spare time
between projects then sure we will
probably switch that to use lambda but
if we were starting from the beginning
and had lambda available at the time we
had have been able to look and go well
on average we're looking at about a
thousand personal item views per
environment a month per production
environment development environment
maybe two events so there is a huge cost
saving to using service there but maybe
for your application you're having 10
billion hits a day to your API maybe
storing having API gateway which costs
you pile request maybe that's not the
right way of storing it so you really do
have to look at your use cases and
finally the code for this will be soon
on github to Pentel awake I am for the
rest of the week and yeah feel free to
check it out simple requests to fix all
of my horrible mistakes if anyone from
write you eyes better than I can which I
suspect is all of you great improve it
but I wouldn't necessarily use it as a
production application just use a C and
finally any questions now there is one
so it is a bright light
right so the question is what's the
latency between receiving a request and
it getting through the queues
projections and such and into s3 it
varies sqs is not the most consistent of
latencies it's usually fairly low within
if there's nothing else in the queue a
message gets through within ten minutes
ten milliseconds ten to fifty if there's
a lot in the queue obviously you end up
with more and more latency so that point
you need to start thinking of a
different way of doing it or in my case
I faked the result on the UI so user
certain type something in the chat box
that's hi how are you hit center I've
put that on screen instantly send the
requester so I am trying to make a nice
responsive UI but there is the latency
cluster the latency between the UI and
us seems to be more of a factor than
with the innate immerse itself so if
you've got a fairly slow internet
connection then that will be more of a
problem but it will vary based on your
message size as well so yeah measure it
and architect round it as a bit of a
washout answer that the best I can do am
afraid any worse or I can like nope okay
well thank you very much for coming and
I hope you know interesting companies</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>