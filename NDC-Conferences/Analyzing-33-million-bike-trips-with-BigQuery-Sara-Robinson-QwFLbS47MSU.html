<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Analyzing 33 million bike trips with BigQuery - Sara Robinson | Coder Coacher - Coaching Coders</title><meta content="Analyzing 33 million bike trips with BigQuery - Sara Robinson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Analyzing 33 million bike trips with BigQuery - Sara Robinson</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QwFLbS47MSU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to analyzing 33 million bike
trips with bigquery how's everyone
enjoying NDC's so far good everyone's
still awake I know it's late in the
afternoon on the second day hopefully
the stock will be fun my name is Sarah
Robinson you can find me on twitter @ s
robb tweets i'm a developer advocate on
the Google cloud platform team so what
that means is my job is to build demos
to show other developers like yourselves
what you can do with different Google
cloud platform products I focus on big
data and machine learning I also get to
give talks at events like this and write
online content like blog posts and
another part of my job is to pass the
feedback I receive from all of you back
to the product teams so they can use
that to improve the product so
definitely come and find me after if you
have any feedback on any Google cloud
products that you have used so far I'm
based in New York it's my first time in
Australia I'm super excited to be here
it's been a lot of fun so far been to
Melbourne Brisbane and Sydney obviously
so let's get started so I moved to New
York just about a year ago so I haven't
been there that long and when I moved to
New York I started to notice all these
city bike stations everywhere city bike
is the local bike share program in New
York and they really love their bike
sharing in New York there's stations all
over the place and obviously at certain
times some bikes are more in demand than
others so like in the summer right now
you can see city bikes everywhere when
it's snowing like in the picture in the
middle
obviously the bikes are gonna be less in
demand so as I notice all these bike
stations everywhere all these questions
came to my mind started wondering like
how many bike stations are there in New
York and surrounding boroughs how many
bikes are in circulation on a given time
are there popular routes that people are
riding or all the different routes
pretty unique and a city bike has a
subscription system where you can pay an
annual fee and you get basically
unlimited access to the bikes for a year
and so I wondered if how many trips were
accounted for by subscribers versus
people that didn't subscribe and maybe
we're just visiting New York wondering
if men and women took the same bike
routes tons of other questions came to
my mind but without any data these are
pretty tough questions to answer right
like might be a tough interview question
on a given day in New York 9 a.m. how
many bikes are in circulation you have
to know you know what
the weather out is it a weekend or a
weekday how many bike stations even are
they're so difficult to answer these
questions so I was really excited to
discover that city bike makes all of
their trip data publicly available the
system started in 2013 and you can
download CSV s on every trip that's been
taken with city bike for about four
years of data which is pretty awesome
the data looks like this which is a lot
of text just a giant CSV file
more specifically one row of data we get
all this information so we get the ID of
the bike and we get the start time and
end time of the trip we also get the
start station end station the latitude
and longitude of both and then if a
person is a subscriber we get data on
their birth year and their gender so
lots of different demographic and bike
data that we have access to here and so
I wondered how we can analyze it and
since I work on Google cloud platform I
chose to use bigquery to do all sorts of
analysis on my bike data has anyone here
use bigquery show of hands looks like a
couple of people bigquery is our big
data as a service platform on Google
cloud it's fully managed you upload your
data it's a big query and you can query
terabytes of data super super fast I'm
gonna do lots of live demos and show you
how it works momentarily all of your
queries are written in familiar sequel
and you can write queries directly in
the web UI and view the data in your
table there and bigquery also has a REST
API and we have a number of client
libraries including net that you can use
to interact with your data upload data
into bigquery if you're not familiar
with sequel bigquery integrates with a
number of third-party visualization
tools so if you if you use tableau if
you're familiar with that you can use
tableau to interact directly with your
data in bigquery and create some nice
visualizations of it a really cool thing
about bigquery is that you can share
your data really easily so bigquery has
a concept of data sets and within a data
set you can have multiple tables so at a
data set level you can share your data
set with a user by email so you can
share it with just one person on your
team you can share it with your entire
team email alias or you can even share
it with any authenticated Google cloud
user which is what we've done with the
citibike data and a lot of other
datasets and you can try it out for free
everybody can run a terabyte of queries
for free every month so definitely
recommend trying it out with our public
datasets and I'll show you how to do
that throughout the presentation and you
also get ten gigabytes of storage for
free with bigquery so a couple steps to
analyzing the bike data first step was
to import the data I had CSV files for
every month of bike data next step was
to do all sorts of analysis on it you
saw all the different pieces of
information we have for every individual
trip so lots of possibilities for how to
analyze it but sometimes we don't want
to just see the data in the table output
we want to see visualizations of it so I
used a couple of different visualization
tools to visualize what this bike data
looks like in bigquery so the first step
was importing it and I chose to do this
using on the node client library for
bigquery you could do it with any number
of of the bigquery client libraries so
what I did is I took all the CSV files
and I put them in Google Cloud storage
which is our object storage product so
to do that all I needed to do in my node
script was just create a reference to
Google Cloud storage and then get the
files from my Google Cloud storage
bucket and then I call this create
bigquery table method passing it what
about my table to be called and then the
file with my CSV data for that table so
next step all I need to do is tell it
the schema of my bike data and then I
can create the table
passing at these options and the final
step is to just import that CSV file so
pretty straightforward then I had all
this data available in bigquery my
personal project we've since moved it to
the bigquery public data project so
anybody can access it there I chose to
do it this way there's a number of ways
you can import data into bigquery you
can do it via the web UI if you just
have one or two files you can import
your files directly in the web web UI
you can use the REST API like I did I
did it with node you can do it with
whichever language you'd like cloud
dataflow is our batch and stream
processing service and so if you want to
do data processing in your pipeline you
can then output the data it's a big
query when you're done
doing any sort of processing on it and
then finally firebase analytics which is
Google's mobile backend platform for
developers if you're building a mobile
app firebase analytics is a product that
will track all sorts of event analytics
in your application and you can create
an automatic export of your firebase
analytics data into bigquery if you want
to write any custom queries on your
mobile applications data so now that we
got it into bigquery the next step is to
analyze it so basically I'm gonna go to
a demo and I'm gonna show you all the
steps I did to analyze the data
visualize it just walk you through my
data analysis process so first things
first is it's a little squished here but
let me show you the table so the table
looks like this and let make that a
little bit smaller ah there we go
so the schema of our table looks like
this so this is all the data we have for
a given trip as you know from the talk
title there's 33 million trips so we
have 33 million rows in this table
it's four and a half gigabytes which is
actually a very small table for bigquery
as I mentioned before it can handle lots
and lots of data and if we want to look
at what an individual row and our table
looks like each row is a trip so we get
the trip duration start time stop time
start station ID a lot of two'd
longitude all sorts of things and then
as I mentioned before if you're a
subscriber citibike provides data on the
birth year so how old you are and gender
so this first query I'm going to run is
just going to count how many bikes were
in the city bike system by year so we
can see that in 2013 we have a little
over 6,000 bikes and then in 2016
nearly doubled we have over 10,000 bikes
in the system and this data is nice to
look at in the table output format but
one thing we might want to do especially
if we had more data points here is you
might want to visualize it so I'm going
to show you how you can do this using
data studio data studio is a
visualization products provided by
Google so if I go over here I'm gonna
risk it all and just create a new report
from scratch
let's see if we're able to visualize
this data so this is a blank report and
I'm going to add a new data source you
can add a number of different data
sources I'm going to use bigquery for
this example I'm gonna write a custom
query and I'm gonna go in here and just
paste my query in and it's using
standard sequel I'm gonna connect it to
my dataset and there we go so these are
the different columns in my data I'm
going to add it to my report and now
that it's in my report I can create any
type of visualization with it that I
would like there we go so super fast now
if we go into View mode we have this
nice interactive visualization of our
data and we can do things like order by
year ascending which would make more
sense for this and there we go so that's
how easy it is to visualize data in
bigquery using data studio throughout
the presentation I'm going to use
different visualization tools so you'll
get an idea of all sorts of different
ways you can visualize your data in
bigquery this was data studio I'm so
moving along to the next query so next
thing I wanted to look at was the number
of trips by station to see like which
stations were the most popular and here
we go so we have a list of station names
with latitude and longitude many of you
probably haven't been to New York so
this probably doesn't mean too much to
you and I I don't know about you but I
can't look at a latitude and longitude
and visualize it on a map although that
would be cool
so what I did was in the bigquery web UI
you can download the data as a CSV just
by clicking this button so I did that
and I used a visualization tool called
Cardo is a mapping visualization and I
was able to just upload the CSV as a
data set and then create this heat map
so the darker red stations have more
trips I can click on a dot and see the
name of the station and the number of
trips so as we can see and
like Midtown lower Manhattan most
popular stations Queens Brooklyn and
uptown have lesser used stations but
still even in these stations we have a
lot of trips they might also be newer
stations as the system has expanded so
that's just one way to visualize
Geographic data in bigquery the next
thing I wanted to look at was breaking
things down by a type of user so I
mentioned before that we have people
that subscribe and then people that just
visit maybe they're visiting New York
and are using the bikes on a one-off
basis so if we run this query we can see
how these people are using the bikes so
we can see here that subscribers account
for a much larger percentage of total
trips than just customers which is what
they call people that are just using the
bikes on a one-off basis so of total
trips subscribers account for about 80
88 % of them and what's interesting here
is that the average trip duration for
subscribers is just around under 14
minutes whereas for customers it's 35
minutes which makes sense because if you
subscribe you're probably using the
bikes to commute to get pretty fast from
point A to point B whereas if you're
just visiting New York you probably are
using the bikes for sightseeing so it
makes sense that you'd be using them for
a longer period of time and then looking
further into breaking things down by
user type I wanted to see if there was
any popular routes so I'm looking at
user type and then I'm using the concat
method to get a list of routes so I'm
just concatenate in the start station a
start station name with the end station
name to see if there's any route trends
in bike trips over the years and what we
find here is that even though the
customers account for such a smaller
percentage they're only 12% of trip of
trips they're taking the same routes
over and over and notice that the start
station and end station here are the
same and this is right around Central
Park so a big touristy area so tons of
people are just picking the bikes up
dropping them off in the same place
riding around Central Park for a fairly
long amount of time they're using the
bikes for over 50 minutes
and the first three are the same people
are just picking the bikes up dropping
them off in the same station using them
to ride around for a while in the fourth
row here we see that this is a
subscriber out but it's only five
minutes so I'm guessing this is a
popular commuter route in New York
moving along to the next query I wanted
to look at routes by gender so my
initial thought was that if I broke it
down by gender to be basically the same
so let's see what so this query is just
taking the top five routes for women
bikers and I'm going to show you a query
next of like how gender breaks down on
total trips but we'll start with this so
these are the top five most most used
routes by female bikers if I then
compare this to the top five routes
white man it's actually totally
different which I thought was really
interesting and I wanted a different way
to visualize that so I used Cardo again
and what I'm able to do here is you can
pass it I'm a start and end latitude and
longitude and then you can have it draw
a line between the two and tell it if
you want walking biking or driving
routes so I just use the biking routes
between the two start and end stops for
all the destinations so and purple we
see the female routes and blue we see
the map top ten routes for males and
what's interesting here is that women
are riding the bikes
looks like longer distances and more
over here in Brooklyn whereas the men
are riding a pretty concentrated area
right here and using shorter routes so I
thought that was pretty interesting and
a nice way to visualize different
demographic trends by bikers and then
finally I wanted to look at how the
demographics of riders broke down by age
and gender because we have this data
available in city bike and here we can
see that 31 is the most common age of
bikers and we we can see that men
account for a much larger percentage of
trips than women and to visualize this I
used a tool called Explorer
taury which is a desktop app and you can
integrate it directly with bigquery or
you can just download a CSV and upload
that to exploratory and create a nice
graph of your data um you can share them
publicly so this is a graph I created
and this indeed reflects the data you
just saw so we saw a 31 was the most
common age of riders but we have riders
ranging from as young as 16 all the way
up to 90 so props to those people
we've got riders you know across the age
spectrum and the last type of bike query
that I wanted to look at was I wanted to
see we so city bike gives us an ID for
each bike and I wanted to see which bike
was had the most number of trips since
city bikes started in 2013 so I ran this
query to just count the trips by bike ID
and we can see that lucky bike number
175 to 6 had five thousand five hundred
ninety-four trips since 2013 which is a
lot of trips it's kind of crazy and what
I wanted to do was visualize how this
bike traveled through Manhattan over
time and what I did was I just ran a
query to get the start time and latitude
of all the trips that that bike has
taken I won't show you the output of
that query here because it's not very
exciting in text format but what I did
was I imported this to cardo and they
let you do animations based on one of
the columns in your data
so I animated based on time and so if I
run this we can see it starts in July of
2013 these are all the stations that the
bike has visited and you can see over
here the month is changing so you'll
notice as it gets into our winter in New
York the bike will take a break for a
minute there it's going dark it's pretty
cold in New York in February and then it
gets a little bit more active I won't
play the whole visualization because it
goes for about two minutes but it's just
interesting to see how it travels
throughout the city and then sometimes
makes its way across the Brooklyn Bridge
and back over time so
I'll stop it for now but yeah it's just
some interesting way to visualize how
one particular bike has traveled
throughout the system I haven't gotten
that bike yet but it's a goal I'll let
you know if I do so now I'm gonna go
back to the slides so just to summarize
I showed you a couple different
visualization tools how we analyze our
data in bigquery and then visualize it
using Cardo exploratory and then data
studio which is a tool that's provided
by Google and some of you may be
wondering I'm guessing we have a lot of
dotnet developers in the house can I
query my bigquery data sets from my
dotnet app and the answer is yes we have
a c-sharp client library for you to do
this and I just want to show you how
easy it is to use it from our c-sharp
library so I'm going to switch on over
here to code and what I've got here is
just a super simple net application
that's going to run three of the queries
from my application so I'm just gonna
run it here and we can see it's running
it's using the bigquery public data set
and it's gonna run trips by member type
they're just finished so it ran trips by
member type and we can see the output of
our query here we're also looking at
routes by member type and top five
routes by women so if you've got a debt
net app and want to interact with your
bigquery data directly from there super
easy to do using the c-sharp client
library and there the queries in c-sharp
so if I go back to the slides so the
bike data is just one public data set
that we have in bigquery I wanted to
also show you a few of the other public
data sets that are available in bigquery
one of them is all of github code
available in bigquery my teammate Felipe
talked about this yesterday so if you
may have seen his presentation on it so
about a year ago we put all the public
code on github into bigquery
it's a giant table it's a little bit
over 2 terabytes and it's constantly
updated and
is just a data studio visualization of
which languages account for most of the
code on github by bytes so by bytes C is
63 percent of the code and if we look at
it by repositories JavaScript has the
most 14% and since a lot of you are
dotnet developers how many down on
developers and in the room looks like
majority I wanted to look at some
c-sharp code trends on github so if I
hop on over here to the browser back
here this is what the table looks like
so we have a couple different tables for
the for the github data we have the
contents table which as I mentioned
before is the contents of every single
file in the head branch and this is what
one row of data looks like these files
are empty but if we scroll over here we
can find some files with data so this
this is a giant table and you can join
this with the files table which has the
ID you can join on the ID field between
files and contents and this has every
single file in the head branch on github
with the file path and this table is a
bit smaller 346 SKU bytes so what we're
gonna find out is how many c-sharp files
are on github and I am going to uncheck
use cache results so you know I'm not
cheating and we'll see that this is
gonna process 129 gigabytes I'm gonna
run it while it's running any guesses
how many c-sharp files on github you can
just shout it out oh it was too fast
sorry but the answer is 19 million
almost 20 million c-sharp files on
github which is a ton it's pretty crazy
the next query I wanted to show you on
the github data is this one this query
is pretty simple
the next query I wanted to look at was
what were the most popular new get
packages looking at all the public code
on github so I'm gonna run this for
running some regular expressions here
this is going to query the entire 2.2
terabyte table oops I use the cache
result
sorry about that but I meant to uncheck
that before I ran it otherwise it would
take about 30 seconds but these are the
most popular packages I don't see the
big query one on here yet but hopefully
soon so I'm sure many of you recognize
these different package names but what
this query did which is pretty cool is
it looked at every single packages
config file on github which there are a
ton as you can imagine and we joined at
a files table with the contents table to
then look for the ID of all those
packages and then just do a running
account with them so just an example of
how you can use bigquery to query lots
and lots of data
since our bike tables we're pretty small
so this is the query for most popular
NuGet packages that I just ran and the
final section of presentation I went to
end with one more thing what is the
presentation without one more thing
what about Australian data so I showed
you a bunch of data from New York
probably not as exciting so this
audience since I'm guessing a lot of you
live in Sydney or somewhere else in
Australia so I wanted to show you some
analysis on data specific to Australia I
couldn't find any bike share data for
Sydney if if you know of a data set
exist please let me know and I'll put it
in bigquery so I couldn't find anything
it looks like the bike share program is
relatively new here but let me know if
I'm wrong so we have a couple of public
data sets where I was able to find data
specific to Australia so one of these is
a weather data set that we have in
bigquery it's called the NOAA data set
and what this has is weather station
data dating all the way back to 1929 for
any station that was providing data and
what I did was I looked at all of the
stations from Australia and then I
wanted to do some mapping to see if I
could visualize any of the weather
trends for Australia and I did I created
these maps again using Cardo to do that
so I'm going to go over here to the
browser this is the output of that query
so let me actually show you
what this data set looks like so it's
got as I mentioned before all the way
from 1929 all the way up to 2017 it's
updated frequently and we have also a
table of all the different stations so
each row in this table is weather data
for one particular day at one station so
we have the station ID year month day
temperature and lots of other types of
weather data like visibility maximum
wind speed whether it's foggy or not
precipitation lots of different stuff so
I took all of the data on Australia
which was what distro provides so I got
the name of the weather station this is
just the output of that query that I
showed you
latitude longitude by joining it with
the station's table and average
temperature rainfall and wind speed and
I took that data and put it into cardo
and I'm just to show you what the editor
part of this looks like is I can create
what's called a chloroplast map which
lets me filter based on a specific
column in my data so here I'm filtering
on average rainfall I could change this
to filter on wind speed and we can see
that actually I don't know if there's
any significant trends in windier
regions but if we look at temperature
this is in Fahrenheit sorry all the data
is in Fahrenheit but this makes sense as
you go further north the average
temperature is gonna be a little bit
warmer so back to our back to our slides
one other type of data they wanted to
look at was data on reddit comments for
subreddits related to Australia and my
teammate Felipe is over there thanks
Felipe has maintains these tables of all
reddit comments in bigquery every month
there's a new table
so not just Australian subreddits but
all reddit comments which is really cool
I definitely recommend checking it out
so I wanted to look at a subset of
reddit comments from June where the
subreddit had Australia and a name
turned out to be majority of the comment
just came from the art / Australia
subreddit so I took put those into a
separate table
I got a subset of them if I score and I
wanted to do some further analysis on it
so I decided I should add some ml to my
Big Data she's a lot of buzzwords in one
slide and what I did was I did some
natural language processing on those
subreddit are on those reddit comments
so I took the table of reddit comments
and extracted the Australian ones and
then I sent them to our cloud natural
language API which is an API that lets
you get more data on your text so you
can extract entities sentiment and
syntax from your text I'm gonna be
talking about it a lot more detail in
the next session also in this room so if
you want to hear more about it stick
around so I took it ran it through the
natural language API which is just a
REST API which gives me a JSON response
of all this data I put that in a
separate bigquery table and ran some
analysis on that and then use data
studio to visualize the data a little
bit about the natural language API so
someone did one of the methods is you
can extract a lot of syntax information
from text that you pass it so this this
graph is called a dependency parse tree
I won't get into the details but
basically what it gives you is a lot of
part of speech data on the text so it
tells you for each token or word in the
text is it a noun a verb an adjective
which words depend on other words in the
sentence and what's the role of each
word in a sentence so all this data this
is just a visualization of all the JSON
we get back from the natural language
API so this is what the table looks like
in bigquery and we can see I have this
one column which is just a giant JSON
string of that response so you might be
wondering how are we gonna parse these
if we're using sequel because we're
writing a sequel to query our big query
data and the answer is big query has a
feature called user defined functions
which lets you write these custom
JavaScript functions that you can do all
sorts of string parsing on rows in your
table so we're gonna run this function
on all of our reddit comment data to
look at things like what adjectives are
people using most in this subreddit so
I'm gonna hop back over here to the
browser and I will show you the schema
for my table so I'm using the sentiment
analysis from the natural language API
I'm getting the magnitude in the score
tokens is that giant JSON string that I
mentioned before we have a subreddit and
then we have the comment text and we've
got about 27,000 comments in here I'm
not going to show you a preview because
what could go wrong with showing reddit
comments on a giant screen so I'll skip
that part of it but I will run this
function and I'm gonna uncheck use cache
results so what we're doing here is kind
of crazy it's running this string
parsing function it's JavaScript
function here on every single reddit
comment in my table and so essentially
it's breaking it down by tokens which is
what the natural language API returns in
just over 16 seconds it ran this on all
of my comments and we can see what the
most common adjectives were here looks
like mostly makes sense nothing too
surprising there so we're looking at the
most common adjectives but in just over
16 seconds it ran this custom function
on all of the comments in my table which
is pretty amazing one other function I
were query I wanted to run on the reddit
data was just looking at the most
commonly used nouns in this data set so
we'll run a similar query just looking
for the nouns instead of the adjectives
here and again the user-defined function
looks similar to the one before and
there we go
so we can see some of the most common
nouns used in the Australia subreddit in
June and I took this data and I put it
into Data studio like I showed you at
the beginning of the presentation and I
was able to create a visualization just
to see how people were talking about
different things on the Australia
subreddit if you want to do more
analysis than
language API also returns sentiment so
you could look at you know break down
the subjects by sentiment so when people
are talking about you know Australia or
the government are they talking about it
positively or negatively another type of
analysis you could do so that's all I've
got for today if you're interested in
checking out any of the different things
I talked about I've got all sorts of bit
ly links here bigquery has great
documentation and getting started guides
cloud.google.com slash bigquery if you
want to use the c-sharp client library I
created a bitly link to that and then
all of the New York Pike data is
publicly available here I also read a
blog post talking about the process I
went through to get the data into
bigquery and to analyze it the weather
data is here again all of these public
datasets are under the same bigquery
project so once you're authenticated
with your cloud account you can access
them there and then the reddit data is
in Felipe's public bigquery projects if
you want to do any more sorts of
in-depth analysis on that so thank you
all for coming if you want to come up
and ask questions after I'm available
happy to answer your questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>