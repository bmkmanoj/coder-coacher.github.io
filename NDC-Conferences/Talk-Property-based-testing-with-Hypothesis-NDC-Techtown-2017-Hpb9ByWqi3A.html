<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talk:  Property based testing with Hypothesis - NDC Techtown 2017 | Coder Coacher - Coaching Coders</title><meta content="Talk:  Property based testing with Hypothesis - NDC Techtown 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talk:  Property based testing with Hypothesis - NDC Techtown 2017</b></h2><h5 class="post__date">2017-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hpb9ByWqi3A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay right hi everyone I'm David
McKeever I wrote
a testing library called hypothesis
which I'm here to tell you about today
and I'm also going to talk to you sort
of in general about some of the theory
of this sort of testing and like why
it's useful and why everything is
terrible and correct software is
impossible but before we get started I'm
just going to on like the bigger picture
I just want to sort of get quite
concrete so this is sort of a classical
test hypothesis is a Python library by
the way most of my examples will be in
Python
other versions coming to languages near
you soon hopefully but this is a sort of
a very classic test using Django which
is a Python web framework and it's very
concrete it's what I would call an
example based test most of you would
probably call it a test and we've got
some sort of collaboration software the
software has projects it has users and
you have a maximum number of users you
can add to your projects as
collaborators so we have this test can
add collaborators up to collaborator
limit which creates two users Alex and
Pat's creates a project with the
collaborator limit of two adds them both
to the project
add a source that are on afterwards it's
a very straightforward boring test and
it's not exactly bad like this sort of
test is very useful because it tells us
a story about our application and people
like thinking and stories and so having
these stories is very concrete examples
to think about the behavior is quite
useful but one problem with this test is
it it's falsely general like it says can
add collaborators up to collaborate or
limit but that's not what it's actually
testing what it's actually testing is we
can create a project with a collaborator
limit of two it has a name of some
projects we can create a user with an
email Alex at example comm etc etc and
there are all these very concrete
specific details that mean that we could
be testing much less than we actually
are
and the test gives no real indication of
like what part of these is this testing
is actually important and what part is
just like accidentally
tell that sort of was there to basically
set the scene of our story and the kind
of test I would like people to move to
writing more of it instead looks like
this this is the same test written with
hypothesis and the big difference
between this and the previous test is
that we have been explicit about the
range of behavior that this test holds
for we've said not I create this
specific concrete project but give me a
give me a project I don't have these two
specific users but I say I want a list
of users we've got some hard-coded
balance here which say the collaborator
limiter between 0 and 20 the number of
user should be between 0 and 20 that's
largely just the practical concern of
that you don't really want to test which
is creating a million users and napping
onto your project and we've got this
assumption that says that this is a
specifically a test about the scenario
when there aren't more users than the
limit and this test it can be harder to
follow particularly for not use it used
to it but instead unlike the concrete
anecdote an example of our Pacific story
we had beforehand it's really describing
an entire range of scenarios and the
advantage of this sort of testing is as
well as not as well as actually
delivering on what the test claims to be
promising that is testing this entire
range of scenarios this can
programmatically be tested we can try a
whole wide range of examples that
satisfy the conditions these tests we
don't have to just try one project with
two users we can try one project with
ten users we can try adding the same
user twice and see where that happens
all we've said is that there are some
users there is a collaboration limit
there are fewer than the forum users and
there's the collaboration limit and this
should all still work so that's where I
want us to get to I think we do I don't
want this to like completely take over
your testing there are use cases for
both but I think if we want our tests to
deliver on the generality we claim then
- this is
wherever want to be doing anyway that
was jumping into the deep end so now I'm
gonna back up to a much higher level and
what I want to talk to you but in
general is about some theory and like
how do we find bugs like if I know that
particularly if you do TDD but finding
bugs isn't the only thing that tests are
for but it's the bit I like so and the
bit I work on so that's where a lot of
my interests are focused and what I'm
going to talk to you about today and the
way I like to think about this is that
you can think of your software rigs
existing in state space there's your
software at any given point in time is
in some state I'll get to what I mean by
my state in a minute and some of these
states are good some of them are bad and
what we're interested in is like I mean
if you in a certain sense most states
are bad because like if you imagine
randomly shuffling all of your
computer's memory and seeing if your
software still works that will never
happen
so like the overwhelming majority of
states are bad but that doesn't that's
not interesting because like we don't
actually have a shop on my computer's
memory button and if we did we couldn't
trigger it in software and what we're
really interested in is instead like the
range of possible states that our
software can actually exhibit we're
actually running it which is this sort
of wobbly yellow region in the middle
and what we want to do with finding bugs
is find areas where the possible region
extends out into the bad region and if
everything we're working correctly this
is why the picture would look like all
possible states would live in the good
region everything would be great the
software would be complete and working
and we could all go home early this
doesn't happen it's one one of the
things I have learned as a result of
working and testing software and also as
the result of giving presentations as we
saw earlier no software ever works
correctly like computers are basically
always broken in some way but this is
the ideal that we are aiming for and the
closer we can approximate is the happier
we and our users will be and what we're
doing when we're testing is we're
basically we were exploring a path in
state space we
start somewhere we poke about her
program a bit we try and sort of get it
to explore a range of states and then we
look for when the program actually
enters a bad state in our tests and
that's a failing test this is a bit of a
lie because you actually take in a test
you do need sort of additional code to
distinguish between whether a state is
actually good or bad I'm going to ignore
that for now
it's that is half of the art of writing
a good test but it remains half of the
art of writing a good test in both like
classical testing and the sort of
testing I'm going to be talking about
and the reason why this is hard is like
a bad state just doesn't mean just like
your software crashes it's very easy to
tell if your software crashed but it
could also mean you showed something
wrong to the user
you got an SQL injection vulnerability
you got a subtle failure to check error
conditions which did something all sorts
of states could be bad and we're not
going to worry too much about the
details of that I said I'd get to what
the state was and now I do want to talk
about that what what is a state in these
sort of them at the grandest most
general possible definition of a state
and in one sense like the only true
correct definition a state is everything
like literally or almost everything
everything that happens in your past
light cone if we want to get a bit
special relativity about it like in
theory a I said there wasn't a randomly
shuffle their bytes in memory but like a
cosmic ray could hit your computer and
that could do interesting things to your
program and in theory that's the thing
you might want to test in practice this
isn't a very although this is this point
of view is true this isn't a very useful
point of view for the purposes of
testing it you may have noticed but it's
quite hard to take the entire universe
and put it into a specific configuration
for testing purposes and so we're going
to need something that is like a little
more specific
the natural more specific thing is like
a so you can restrict yourself to like
the provision and the province of your
computer or a state is a complete
snapshot of where your computer is at a
given point
this is also to general like even when
computers looked like this it was too
general because just the number of
possible representations in memory is
larger than the number of atoms in the
universe so and particularly and this is
true even with embedded software these
days embedded processors and bread
memory is still quite kind of huge from
this point of view so we've got more or
less the same problem we did with the
using entire universe as a state but it
steams down a little bit we're more
specific yet and say state is just the
instruction pointer like there is a the
point the point in memory where which
your code is current where your CPU is
currently executing encode and this is
not too specific this is a snapshot from
a Python program and if you're writing
Python then like this entire idea is a
lie because none of the code that your
act none of the things that actually
determine what code you're running is
represented by the instruction pointer
if the instruction pointer just says
where you are in the interpreter and all
of the things that determine what you're
actually running are in memory not in
assembly code and but I mean this is
true of the programs in general like
fundamentally all programs are
interpreting their data and all on
terrific almost all non-trivial control
flow depends on what the actual data
you're running is so you can't ignore
memory entirely but it would be nice if
you could because like the instruction
pointer you now only have like a few
million a few billion states that's
practically easy so instead we're gonna
go sort of return to my mouth Mission
Ridge we're going to do up two
equivalents we're basically a state is
your computer but we're going to ignore
the bits that we don't actually care
about if you are never going to read a
particular point in memory then we can
ignore that point in memory if you've
done some stuff and that and afterwards
you're never going to read a point in
memory even if you've read it beforehand
we can ignore that and I'm not gonna
make this to your precise like I could
get very very formal here and that would
be a waste of both of our time but you
can think of this as essentially like
the state is some abstract represent
of whatever matters about a snapshot of
our computer unfortunately that's still
quite a lot potentially but the fact
that we don't care we're doing this up
two equivalents thing often lets us boil
it down so for example here's a simple C
function that just takes an integer and
doesn't do anything with it the name is
a lie because it says to do a thing
turns out comments encoded get out of
sync and so there's an integer so that's
2 to the 32 states because like you've
got one where one variable in memory I
mean obviously it's depending on your C
compiler integers might not be 32-bit
but we're going to pretend they are here
well that's about 4 billion states but
the function is not doing anything and
so this is where that up to equivalence
comes in like regardless of what value
is there like it does it always does the
same thing so four billion here is about
one we can essentially throw away all
these differences and pretend they're
not there if we then sort of add this
assertion the sort is not equal to 42 we
still have four billion possible values
in memory like there's still 2 to the 32
different integers but now one of them
is special and so there's this
distinction if I equals 42 that's one
state if I as anything else that's
another state like whether or not we
trigger this assertion puts us in
different state so here four billion is
about two and so this is starting to
look like it's not so bad but the
problem is that states don't add they
multiply so here is a function which
takes twenty six billion arguments
obviously you don't want to write a
function that takes twenty six billion
arguments but most non-trivial programs
will have at least twenty six billion
conditions in them that could be true or
false so this works okay as a toy
version of our problem and Teutons takes
into by the 70 million of states that's
a lot less than four billion we had
earlier but the problem is without
knowing what these functions are every
one of these states could be distinct so
like you could have
these functions set some global flag and
then the final done stuff function
asserts something about about the
combined bit field that that's at and so
it's very easy to make these all
distinct and so in this relatively short
program of only depending on how you
format it like one or two hundred lines
tops we've got 70 million states an
important thing to realize here is that
this program is very easy to get 100%
coverage on if you test it by passing
true for every value and false for every
value BAM you've got 100% coverage so if
you have a decent sense for how hard it
is to get 100% coverage if you're over
your programs when testing this program
is thirty five million times harder than
that
so the idea that you can explore all the
states even in a non-trivial program is
basically a fiction it's not something
that you can ever do so when I said that
your test look like this this was kind
of a lie because what it has actually
looked like as this they're essentially
a scattering of dust over the state set
of possible states that your programs
could be in you are never going to
possibly cover the entire state space
with with all your testing and the tools
that I'm offering you they they improve
on this situation somewhat because you
run the multiple times and they try like
200 examples or a thousand examples or
whatever but unfortunately what that
means is that instead of like this your
tests now look like this and it's still
never really going to cover state space
because like even adding three orders of
magnitude more testing that's even in
the simple example I showed you you need
like another three orders of magnitude
and in realistic programs it's more like
another ten orders of magnitude or
possibly more actually running concrete
test cases will never get you state's
best coverage so that's it abandon hope
testing is impossible everyone can go
home let's go get it ranked
it's not quite as bad as all that
and there's I think I like to call the
fussing assumption this isn't a standard
piece of terminology but I think almost
everyone who works in this area will
know what you mean if you talk about
this and the fuzzing the idea the
fuzzing assumption is basically that
although in theory bugs can lurk
anywhere and like hey you could have
just like one isolated state in the
middle of everything else where there is
a bug hiding that's not what usually
happens in practice and even when I tell
you is what happens in practice we don't
necessarily care that much because
probably no one else can find that state
either
and so the fuzzing assumption is that
most bugs aren't like this they fall
into one of two in categories the first
is bugs that are obvious that doesn't
mean that you've necessarily found them
because like they are these are only
bugs that are obvious when you're
looking at them but there as long as you
look at them they come up fairly easily
so here's an example example of testing
some code with hypothesis we've said
give me lists of integers and the
minimum of this list of integers should
always be less than or equal to the
maximum of this list of integers that
seems straightforward right that's how
many movie maximum work nothing should
go wrong here if you pass an empty list
then you've tried to take the minimum of
an empty list and that doesn't work so
you get an exception instead and this is
what I mean by an obvious bug like there
is a fairly straightforwardly signposted
value that you can just make sure you
follow stride testing that value here
and it will find the bug for you people
forget that the you think that argument
can be empty all the time like if you I
mean sometimes it's because it's not
legitimate to pass an empty argument but
most of the time someone just said oh
yes there could be no this is a getting
this first element of this list and
forgot that there is might not be a
first element of this list or one
function documents that you can pass
null as its argument and it passes it
straightforwardly to another function
that documents that you can't pass nulls
it's argument
and so just like consistently trying a
small number of very obvious values like
empty values finds a really remarkable
number of bugs and most programs the
second sort of bug is the bugs that are
common there's not necessarily like some
big sign saying here here's the bug but
the bug is dense enough in this bassist
states that if you just like do end up
doing 100 or 1000 or thousand times more
testing it's just going to
conventionally crop up and here's an
example of testing in the wild there is
a nice little Python library called
binary or not and it's basically it's a
heuristic library for taking a file and
saying was this meant to be a text file
this is in principle impossible but in
practice you can you can guess the right
answer most of the time and all we've
testing here is the very simple property
like given any binary string this
function shouldn't crash it should just
return a boolean value Sanctuary or
false it crashes obviously of or I
wouldn't be showing you this and this is
a fun bug because what happens is that
binary not is using another heuristic
library called car debt which is trying
to detect the encoding that the file is
in and car dot says okay so this is the
encoding I'm a hundred percent sure of
this is encoding definitely encouraging
and it doesn't actually decode as that
encoding and card is just and this isn't
a bug apparently this is actually just
how card I thinks it should work because
it's faster to do it this way and speed
is good right so binary Ranaut said to
card at like what a coding is this in
and partic David this answer back it
tries decoding it and then it doesn't
handle the exception because I didn't
think that was possible and this is
often where these sorts of common bugs
come from is mismatched assumptions
across module boundaries you both sides
the world hat in the boom
two libraries written by two authors
each of them have a slightly different
set of assumptions about the world and
all of like the really obvious cases are
handled because those are the cases
everyone thinks of but there are enough
weird edge cases that people didn't
necessarily think of the same weird edge
case isn't it necessarily think they
should be handled in the same way and
hypothesis is actually much smarter than
it was when I found this bug but this is
a bug you could find by just basically
chucking a whole bunch of random strings
at it and a lot of what the work in this
field is doing is basically like pushing
the boundaries of how common a bug has
to be before we find it but this one was
pretty common and the consequence of
bugs being either obvious or common is
that they're actually quite easy to find
given this sort of tooling you can't say
they're you found all the bugs but you
can cover a much wider section of the
search base with a reasonably high
degree of confidence so instead your
test sort of start looking like this
they cover a large area of the search
space but they don't really cover it in
the sense that they've proven there are
no bugs there they've just said there
probably aren't any bugs here it's which
is surprisingly good often enough and is
usually at least better at finding bugs
than your users are which is something
and then you can sort of daisy chain
these along where you dare okay this
stay it looks good and all states
reachable immediately trying to look
good so let's test over here and
eventually you daisy chain you outwards
and you often do find a genuine bug and
so although this doesn't sort of
correspond to thorough testing it
corresponds to a lot closer to thorough
testing than the raw numbers would
actually suggest or at least as far as
we can tell the difference it does but
we can go further because I mean this is
good but fundamentally state space is
still huge and what we need are
techniques for using this sort of
testing to get beyond this and to try
and increase the scope of our
thing further because fundamentally we
are still covering a very tiny area of
state space and there are a couple of
principles that we can use with this
sort of testing to get us further
largely by shrinking the size of the
state space we're actually testing the
first is decoupling which you're
normally doing in your testing anyway
like if you're writing unit tests that
is essentially decoupling because one of
the nice things about the up two
equivalence point of view for this is
that if you have two independent parts
of your program then you can basically
treat a state of the combined program as
a state of this part and a state of that
part and you can largely just test them
independently and if you've tested them
independently then you've effectively
tested them together so that part is
pretty much standard testing practice
and effectively shrinks the state the
state space quite a lot because you're
breaking it up into smaller tighter
state spaces where you've tested the
product automatically if I haven't
tested the individual components but
another thing you can do for this is
basically like when parts the system
aren't independent and you are feeding
them a feeding output from one part into
the other then often the way bugs occur
is that you've generated and valid
output somewhere and then it's
propagated throughout your program as it
gets passed further along the system and
so this sort of like very basic bounds
checking on the output of everything
often uncovers quite a lot of
interesting bugs this is a bounds
checking function from for testing
something for calculating the arithmetic
mean of a list of numbers it's this test
is has basically uncovered bugs in every
implementation of the arithmetic mean I
have ever flirted the one in the
standard library is actually not too bad
because it throws an overflow error
rather than giving a wrong answer
everyone else gave a wrong answer and
the reason is basically that
floating-point numbers are hard and the
reason why the standard library mostly
gets it right as they chase and they
turn them into arbitrary precision
rational numbers and do the maths there
and then cover back to floating-point
numbers because the cyanide library in
Python doesn't actually care about
performance but if you tried numpy which
is one of the standard scientific Python
and numerical calculation libraries it
can be but it can generate means which
are both larger than the maximum and
smaller than the minimum even if you
restricts them to them being finite
values I'm reasonably sure like your
favorite numeric library is also going
to get this test that's wrong so this is
not throwing stones at Python there's an
entire 30 40 page paper about how to do
this correctly with two numbers and it's
hard but you wouldn't have detected this
problem if you just were calculating the
mean in your program you would have
calculated the mean you would have seen
yes this is totally a sensible answer
and you will potentially used in other
things and then you might have ended up
with something negative being negative
where we thought it had to be positive
like and these sort of errors end up
propagating and you end up getting lot
into lots of the states that you thought
were impossible because you thought you
had satisfied conditions that you hadn't
and having this sort of checking at the
boundaries means that you can always
trust the output of one function as the
input to another as long as you have
tested both a range of input in a range
and a range of output for that and so
and again this is not a hundred percent
true like particularly because of the
probabilistic is this would this would
be true if it were exhaustive testing
it's only sort of true with
probabilistic testing like the kind that
I'm providing but it's true enough that
it often finds bugs in practice and that
once it has sufficiently explored the
space like the reliability the software
at least goes up even if it doesn't go
perfectly the more interesting thing
that happens and is for testing is state
collapse which is when what you often
want is like the problem here is that
state space is huge wouldn't it be great
if state space were less huge right and
sometimes we can show that state space
is less huge
and that's what I mean by state collapse
so conceptually when your program is
running you can think of every path you
take through your program as leading to
interestingly novel state you do a or B
you do a or B again and at each point
the tree branches and so you end up with
an exponential number of states as time
goes on and what this means is that in
order to cover all this you would need
an exponentially increasing number of
tests which we don't really want to do
but often what actually happens is it
looks like this you have a bunch of
intermediate operations and then you
collapse punch them back down again and
so now the number of states grows
linearly as time goes on and covering
this with tests becomes much more viable
a trivial example where this sort of
thing tends to happen is local variables
like often you will introduce a bunch of
intermediate states and then you will
normalize in some way and or throw away
parts of those local variables and so
States collapse that way just by
throwing away detail here this is a
stupid program again most of my like
actual non test examples are stupid but
here we create a range of a hundred
numbers we shuffle it thus introducing a
hundred factorial intermediate states
which is dramatically larger than the
number of atoms in the universe that
might be more than the number sets of
atoms in the universe and not sure but
when you sort it again we throw all that
away you're actually not introducing
that many states were introducing as
many states as the seed size of your
pseudo-random number generator but we
don't want to make seed random number
traders feel bad by pretending and no by
acknowledging that so we'll pretend it's
a large number but it's still a large
number even if it's a it's only pseudo
but at the end like all you're doing is
you're introducing one state so this is
a rather extreme form of state collapse
but it's also like not that interesting
for the purposes of testing because
that's what it has for already covering
so we want to ask is whether States
collapse in general I do we have this
sort
paths through our programs we're sim
knows where things become briefly more
complicated and then becomes simpler
again we can test that it's we can look
for path through our programs and then
write tests that assert that these all
end up in the same place and there are a
whole bunch of ways this can happen but
there are two that are very common in
most programs and are basically worth
testing all the time because they always
find bugs and both this reveals
interesting bugs but also like once you
have these tests you have more
confidence in your testing overall
because you know that the set of states
that you actually have to test has
decreased because you've got fewer of
these like weird passes through the
system
the first example which comes up all
over the place is serialization like to
a first approximation most programs
exist to transform data from one format
to another particularly if you're
writing a web application then your data
comes in off the web it turns into an
in-memory representation you put it in
the database then you take it back out
to the database and you display it to
the user in a different form and every
one of these is a serialization program
in the problem they're not all
necessarily full serialization but
certainly like the process of putting
things in and out of a database is very
much encoding your in-memory data in a
serialization serializable form and we
can just that we can say I take this
object I serialize it to some text
representation and I pars it back in
again and I should get the same thing
out here we're actually testing that
they have the serial same serialize
representation do you see weirdnesses
with comparing date/time objects in
Python but basically what we're doing is
we have a date/time we convert it into
iso 8601 the one roof a new format for
day x that everyone should be using for
every day time ever we parse it back out
again and we assert that this is the
same name date seems perfectly
reasonable
I love this bug because it's so weird
this bug and it utils drop Prosser
triggers only if you have a single digit
year and only if me let's say which way
around is it yes the year is the same as
the number of seconds past the minute
and under these circumstances and these
circumstances only as far as I can tell
the date you till parser just gets
really confused and it swaps the month
in the year I have no idea why does this
there are there's something of a
long-running tradition that when you
give a hypothesis talk you need to break
a date and time library and find use
that as one of your examples but I
haven't fixed this one yet so I keep
using this for now but I mean I'm being
mean to date you till here but like this
happens all over the place essentially
so many encoding bugs get found here
because you have forgotten that you've
converted a decimal to a float and
you've lost precision or you have a
disagreement about unicode encoding x'
serialization is both ubiquitous and
broken and creates really weirdly
interesting gadgets and programs to
exploit and having these sort of tests
that basically all of these things
round-trip just eliminate so many
classes of weird behavior that many
programs in the wild exhibit the other
big interesting category is
normalization we saw a bit of this with
the shuffle example like we took an
arbitrary list and then we sorted it and
by sorting it we just collapsed a huge
amount of complexity because we took
many different values and we turned them
into one canonical value let's take a
look at an artificial example this is
inspired by real examples in the wild of
username normalization where we we want
people to be able to sign up with
usernames because this is the 21st
century and all barbarians there are
usernames or Unicode so we need to
normalize them into a canonical Unicode
representation and we also want to case
normalize them we don't want someone to
be able to spoof another user by
changing the case with their username
right if you know more about this
problem than me you may have spotted the
bug looking at this code
but this is fundamentally what we're
doing here where we're trying to convert
a user name into a canonical form so we
can go does this user or they exist is
this user name taken and then in our
tests we are saying okay so the user
names are case insensitive so if we take
user name we normalize the uppercase
version of it then that should give us
the same normalized user name right
that's our we don't want the user to be
able to spoon to be able to spoof other
users this property this fails the
interesting character we see here is
lowercase Roman I without dot and it has
the interesting property that if you
uppercase it you get a normal uppercase
I and then when you lowercase it again
you get a normal lowercase I
and because we had this yeah that's a
good reaction to Unicode has lots of
weird characters like this this isn't
even the weirdest one it's other fun
examples included like the number of
code points changes as you upper and
lowercase things a lot of the time but
yeah because we use lowercase saying in
our normalization and upper casing here
we've managed to find this for the Miss
match the correct fix is to use the case
fold method case fold is what you should
be doing when you're trying to do case
instance definitive comparison and even
if I don't convince you about property
based testing some of you are probably
going I need to fix that thing in my
application now so public service
announcement and this sort of thing
causes real bugs Spotify had account
hijack bug related to this a couple of
years ago it wasn't the case
normalization it was something to do
with how they were normalizing Unicode
characters but basically what they had
was that you had some user names you
could create that when normalizing them
twice you got a different answer from
normalizing them once and so you could
create an account hijack by creating a
pre-image where if you normalize it
twice they would go to that users
username but if you normalized it once
it wouldn't so they
fix that in a hurry once someone pointed
it out to them but they could have found
that out with property based testing and
like I've been making sure to a
disclaimer special how long is this
gonna make yourself work correct no
because you're never going to get your
Crosse software correct unless you even
if you work for the sort of people who
will pay to do formal verification of it
then it's probably still not correct
because inevitably there's like some bit
you haven't formally verified correctly
there was a hilarious analysis of how
often formally verified distribution
systems were correct and the answer was
basically never because there were bugs
and the unverified bits there were bugs
and how the verifiers were invoked my
favorite is someone who that just forgot
to check the return code after invoking
the verifier it's essentially like bugs
lurk everywhere
and there's almost no way to gather and
then to get around that but we can still
improve on the current status quo and we
can still get software that is a lot
more correct than a lot of what we're
writing today and one of the big things
about this is like it gives you
confidence in your software it's what
we're aiming for is not software where
we're just like this is correct software
is perfect there can be nothing wrong
it's software where we can release it
and we don't have to be worried about
being woken by a pager in the middle of
the night it's software where we can try
something that we know is hard to get
right because we have a reasonable
degree of confidence that if it will if
our implementation is too badly wrong we
will find it out and having this sort of
tooling at your disposal and having this
ability to just like push back the
envelope a bit more on how we find bugs
and how we write software has really
improved both my life and like the lives
of people who are using hypothesis that
is because it just means that someone
has your back and you have a bit more
knowledge of like the amazingly weird
edge cases and a bit of a guide truth
like the complex data space of your
software and that is pretty much all I
have to say once again I'm David
McKeever the software talk
do you about it is hypothesis and you
should check it up thank you very much
and I think we got a bit of time for
questions if people want to ask anything
doctor yep
so the question is that I didn't
explicitly highlight this I should have
if we look at these when the test is
failing it's providing it with a
specific counter example and the
question was whether you want to turn
those counter examples into concrete
unit tests within your project
hypothesis has a bunch of special
features around them actually one is
that you can just say always try this
example first and just add that to your
tests another is that when you rerun
examples it remembers the previously
failing example so rewriting a failure
is fast even if finding it in the first
place is slow and so in general like for
the common cases you don't need to do
too much to go outside the property
based testing framework for stuff where
the example has revealed something
really interesting is going on I will
often pull it out into a unit test so I
can test more about the behavior in that
case that is sort of specific to the
concrete toy example I certainly don't
advocate writing entirely property with
best tests I would say that I probably
have about two 50/50 mix it's just that
50% that are property based tests are
pulling an awful lot more ways than the
unit tests are in terms of correctness
and the unit tests are then sort of
exploring very specific concrete
scenarios didn't answer your question
cool anyone else
that's correct yes I'm probably not the
best person to do that because my
machine learning knowledge is all like
about five or six years out of date I
think a lot of what it's been testing is
less the machine learning and more of
the data validation pipelines and things
like that search asserting that you can
nothing goes horribly wrong you get
everything within reasonable bounds so
like the mean example I had where you
are testing the output under arbitrary
input other techniques that are quite
useful is testing how your machine
learning algorithm response behaves in
response to changes in data so I'm
moving in this direction should never
change the classification in that
direction there was a good post going
around I've forgotten her name her blog
is composition dot L which was about
formal verification of machine learning
algorithms and most of the sort of
properties that she was talking about in
that are good specific examples
I mean machine learning is to a first
approximation code like any other and so
like you can't answer the question how
do you test machine learning any more
than you can answer the question how do
you test code and so the properties you
tend to want to test or are usually
quite specific to the application but
it's that sort of thing that's the best
I can answer I'm afraid because I
haven't actually sat down with any of
the users who are doing it with machine
learning Rob you ever
so the question is do you start with
example based tests and then generalize
them or do you start to go the property
based test
I recommend that particularly when
people are starting out they start with
an example based test and generalize it
unless it sort of fits a very classic
form property best test so a lot of the
patterns that I've shown you with
serialization or normalization I would
just start out writing the property
based test immediately because it's
familiar
in general I start out with writing
property based tests but I have a little
bit more familiarity with the domain
than most people do and also I tend to
write weird code that is very amenable
to property based testing like I do a
lot of data structure implementations
and stuff like that which is basically
property based testing one easy mode so
the answer is it depends but if you are
all uncomfortable with property rest
testing it's easy to start with examples
and refactor to properties so do that
until you feel comfortable doing
otherwise yep
the question is how does does hypothesis
handle concurrency scenarios and that
this makes two states complicated and
the answer is hypothesis doesn't do
anything for this largely for the moment
because hypothesis is written in Python
Python is so bad at concurrency that no
one was really interested in this use
case in general most language
implementations are quite hard to do
something about this because you need to
be able to take control of the scheduler
and most MV ends don't let you do that
by the way if anyone wants to sneak out
I will know in no way be offended we
have hit the breakpoint but I'm more
than happy to continue answering
questions I think there was another
question over there somewhere so the
question is the frit does the framework
itself have state yes so yep so
hypothesis bite has a database of
examples which by default lives on disk
but you can configure it to other things
so the hypothesis does store a little
bit of external states on a desk you can
turn that off if you don't like it but
it's for development workflows it's been
a huge improvement
yep yep so the basic model for these
things is that you randomly generate
examples initially and then once you've
found a failing example you try and
reduce it to a smaller examples so that
it's nice and human readable and indeed
the state the database the randomness is
part of why the database exists because
it means that the random is always sort
of goes from failing to it never goes
from failing to passing and tusky back
should pass to fix the bug so it is
randomness but without any false
positives like a failing test stays
failing and always indicates a real
problem either in the test or the system
under test doesn't answer your question
you can specify a specific concrete seed
if you want to specify concrete seed but
yeah in general it doesn't seed it
specifically and we'll run different
examples each time and I generally
encourage people to leave it in that
mode because it means that basically
every time you run it you're getting
more testing but I would like to some at
some point work on better workflows for
separating that out from your CI so your
CI becomes more deterministic and you
then have something working in the
background
I don't currently have anything doing
that but in principle it would be it's a
thing that would be great and I'd like
to work on it somewhat okay anything
else so the question is about coverage
feedback and whether and how hypothesis
uses it and the answer is that I have an
excellent plant in the audience
apparently because hypothesis has quite
recently added features about this or
I've quite recently added features two
hypotheses about this coverage feedback
is more useful on long-running tests
because it is one of those things where
if you're running for an hour then you
can get a lot more out of coverage
feedback than if you're running for 20
seconds hypothesis does use it and
basically uses it to slightly improve
the data generation because by focusing
more on rare areas which remote which
are not it is essentially a form of deep
biasing of the random generator and the
other thing it does is it saves examples
when it notices that they cover
something interesting so it uses the
database not just remember failing
examples to go aha that's how I cover
that line and so each time you run
hypotheses the coverage improves but
this is something we're ongoing work is
needed to make it better at that and
also something where like once I've got
this sort of the separate CI and
long running workflows that will get
much better in general most testing of
this sort the answer is no certainly
classic quick check and a lot of the
things based off it don't know don't do
that but I think a bunch of the newer
systems are starting to acquire that I
believe hypothesis was the first but I'm
weeks rather than years yeah so so
hypothesis is currently only available
in Python I have a customer is currently
about start funding a ruby version of it
which will come with a core engine
written in rust which will make it much
easier to extrapolate languages there
are other systems like this available
for a lot of languages depending on
which one you're using they are varying
degrees of quality but but yeah so that
I mean that's the basic road map is Ruby
soon that will result in the Rost engine
that may result in diaspora of many
other languages we will see do you have
a particular language you're interested
in C++ doesn't currently have any good
answers I'm afraid it has rapid check
and CPP quick check I think and based on
friends experiences of trying to use
those I've heard a lot of screaming so I
think currently you could use theft
which is for C I think we might get
being kicked out relatively soon here
but I don't know how natural using theft
from C++ is I'm afraid
um you could absolutely use this
resistant testing basically anything you
can write tests for in Python you can
test with this I would say that for
penetration testing you probably want
more sophisticated tools I would say
this is design no you want the sort of
classic security buzzer tools I think I
need to get off the stage because I
think the next speaker is probably here
so I'm happy to continue this
conversation not on the stage there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>