<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>HTTP: History &amp; Performance - Ana Balica | Coder Coacher - Coaching Coders</title><meta content="HTTP: History &amp; Performance - Ana Balica - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>HTTP: History &amp; Performance - Ana Balica</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/m18QYJ66dSg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello so shall we get started
yeah that's people coming in also these
lasers hope right I can't see anything
right so welcome to their stock we're
going to be talking about HTTP history
and performance very happy to see you
here my name's Anna I'm your speaker you
can find me in all the different places
on the internet including Twitter where
I'll be posting this light if you ever
want to use them as a reference go back
and check something I work at a small
start-up based in London called Verve as
a senior software engineer and I mostly
do Python and Django Django is a popular
Python web framework any Python people
in the audience yeah nice nice I love it
that's very cool just to make it clear
so I am NOT I'm not like a network
engineer or a web standards advocate or
evangelist I decided that I want to
speak about HTTP because this is
something that I found interesting and
I'm just a regular back-end developer no
more than that so when I came to the
well eventually found out about
hypertext Transfer Protocol and I have
an OCD in naming things so I can spend
hours just figuring out what is the
correct variable name for something so
when I saw text that kind of was a
dissonance because I'm pretty sure I can
transfer not only text but I can
transfer audio and images and all sorts
of things so I thought should probably
be called hyper media well in fact it
has been tried to rename HTTP to
something like hyper media but it never
got changed and the initial version
hypertext Transfer Protocol was a
misnomer because the first
implementation of HTTP in fact could
transfer only tax
so in 1991 Tim berners-lee outlined the
basics of the modern web which included
HTTP and HTML at the time there was no
RFC nose back but there was an
implementation of a subset of the
protocol which later on we called HTTP
0.9 so interestingly enough it is
possible to speak to many servers in the
wild HTTP 0.9 as an example you can tell
that to some cellphone the port 80 and
see how it's done and so what we're
doing here we're going to tell that to
miss the Stallman's website the server
will tell us that we are connected to
Stallman org what we do next I type in
get this my request a space and the
index I want to access the index page
and when I hit enter straight away the
server spits out all of the HTML and
closes the connection and this is it the
main features of HTTP 0.9 are mmm the
client request is a single ASCII
characters strength terminated by a
newline it consists of words yet and in
fact at that point the only verb that
you could use was get followed by a
space followed by the document address
the server response is similarly a byte
stream of ASCII text marked up in HTML
or plain text if you wish the response
the response message is terminated by
closing the connection so you need a way
to figure out that the server has sent
out everything all of the data that it
had and this is it like this this is the
end so how we do it we just close the
connection this is the signal that you
got you got all of it there was no
metadata so you can't do any content
negotiation either so yeah
overall a lot of things are not here
stuff that we are fairly used to in HTTP
but this protocol was fairly simple and
in fact it was so simple to implement
and follow along and get started with
that it took off and actually became
quite popular so right after HTTP 0.9
was kind of released into the wild and
developers started implementing the
protocol on the servers and on the
clients works began on establishing a
proper spec that became known as HTTP
1.0 and that happened in 1996 so let's
forget about telnet because this is
obviously a very insecure channel of
communication and let's use C URL if you
want to make a request using C RL in HT
p 1.0
you need to pass in the zero argument so
let's fetch the home page of Twitter
first thing you we connect to the
Twitter server C RL for us will inject
the request headers there is a get slash
the protocol version there are a bunch
of headers like the host user agent we
receive the response which is made up of
the response headers and of the response
payload the response headers will tell
us the response status code in this in
this case 200 ok and other things for
example like content type and then the
response is a plain text HTML and the
connection is closed similarly how it
has been done previously so there are a
lot of lots of things going on here so
the requests suddenly can consist of
walter multi-line new line separated
header fields it also includes the
protocol version can suddenly we started
thinking about the future because after
1.0 we'll probably want to have one
point one who knows the response object
has its own set of new lines separated
Hatter's graphics with a status response
line the connection between the server
and the kind is still closed after every
single request response is not limited
to hypertext anymore so this is the
breakthrough we suddenly can have all
the different content types
exciting and there's generally a lot of
things that have been defined by the
spec for example caching mechanisms and
proxy behaviors and and content type
negotiation and content encoding lots
and lots of things now what are we gonna
do next is we're gonna do some examples
and this is a warning so we will be
timing how much does it costs us to make
a request to to do a certain request
response cycle so essentially like the
whole round-trip and we'll be using
fixed synthetic values say we'll say the
latency is going to be 30 milliseconds
whereas in the real world the networks
don't work like that they continuously
fluctuate so you don't get like a static
latency it's all kind of going up and
down so what do we have here we have our
client and we have our server so as we
said before let's assume the round-trip
is 30 milliseconds that's fairly low
latency it's not too bad before the
client can start fetching data from the
server we need to establish a TCP
connection because most common use case
how we run HTTP is over TCP this is how
the OSI stack kind of works so first we
need to do the freeway handshake the
client sends a syn package to the server
the server responds with a syn
acknowledge the client responds with a
acknowledged package and it packages
together with this also the request so
that it doesn't need to do another round
trip now the server spans some arbitrary
time processing the request say there's
a 35 milliseconds for our server to prep
the response it returns the response
back and then we close the connection so
in total what happens is we
we spent one round-trip for the TCP
handshake 30 milliseconds and for the
HTTP exchange 65 milliseconds
now pages on the web really consists of
a single resource I mean unless you're
using something like links text based
browsers
really cool and suitable for certain use
cases but most of the times it's not the
case so in case of Twitter to see the
Twitter homepage we would need to
download fetch 21 resources and for that
we would need to wait roughly two
seconds if we were to request it over
HTTP 1.0 assuming the time to first byte
is actually faster this is still far
from being instant like generally by our
modern standards this is kind of slow
and according to HTTP archive actually
modern day web pages are roughly consist
of on average 100 resources so that will
make us wait for 9.5 seconds which is
kind of horrendous but actually lied to
you when I showed you the CRL request it
said connected to twitter.com on port
443 which is which means that we've made
our request over HTTPS and what happens
when we do it is on top of the TCP 3-way
handshake we also need to do the TLS
handshake
which is another whole story because we
need to establish a secure channel for
communication and only after that we can
start exchanging application data and do
what we actually wanted to do in the
first place that concludes that - the
one round-trip for TCP we have another
extra two round trips for TLS there's
good news
TLS sessions can be resumed so what
happens in this case we spend one
round-trip for the TCP handshake and we
spend one round-trip instead of two
round-trip for the TLS session
resumption and then again we can
exchange the application data now back
to the Twitter
homepage in our 21 request assuming that
the first connection will perform the
full TLS handshake and for all the
consequent requests will use the TLS
session resumption that will give us 2.7
seconds again going to the hundred
requests per average page on the modern
web there's going to be twelve point
five seconds it is slow it is very slow
like that considering that you have
fairly good latency that's 30
milliseconds but a lot of people don't
don't have these speeds so no surprise
that after releasing HTTP 1.0 RFC works
began on a should be 1.1 mm it was
developed over a period of roughly four
years and the first spec was released in
1997 follow up with the extension to the
spec the RFC 2616 in 1999 and this is
the default this is what we have this is
what every single client and browser
speaks we never question whether it is
HTTP 1.1 compatible it's kind of there
and we have really got used to it so
going back to see URL we again let's
make another request this time to get
hub calm so the same bunch of request
headers we make a get we've upgraded our
protocol version 2 1.1 we get the
response headers and we see some new
headers for example like transfer
encoding and cache control this kind of
sounds exciting and interesting we
receive our payload and it tells us that
the connection to host github.com left
intact we're going to talk more about
later more about that so let's make
another request also to github in this
case let's download the octo cap because
octocat is super duper adorable I've
omitted the requests the only thing you
can see when I'm invoking this URL
command is that I'm passing
another header which is range I'm
requesting that I want the 0 from 1024
bytes this is generally useful if you
want to resume a download or you read
the tale of a growing object or in the
case of browsers you might want to read
for example the just the beginning the
metadata of the image to figure out the
size and so you can start building out
the layout it's an interesting
optimization so let's take a look at the
response of the of from from the server
so we get a new status code that we
haven't seen before 206 partial content
and we get a bunch of headers like cache
control in the age and content range and
content length and connection keep alive
which this is this is something
interesting we get our a bunch of
headers that we've requested and again
the connection to the hosts left intact
so what's up with all of this connection
keep alive
well connection keep alive is a really
cool performance booster that we got and
I'm going to explain in a minute what
exactly that means what else do we have
in HTTP 1.1 well we got compression and
chunked responses and byte ranges we
also got something called pipelining so
that we can break from this straight
request response cycle and we're going
to talk more about that we have much
better flexible caching mechanisms to
improve performance we got 24 new status
codes exciting like the one we've seen
with a partial content and we get
cookies even though the web was supposed
to be stateless we actually wanted to be
stateful so we ended up with cookies
right so remember we talked about the
TCP three-way handshake well what
happens with the keepalive connections
which is which are by the way by default
on in HTTP 1.1 so we do the TCP
handshake once and then we request a
bunch of data and then
the same connection we just request
other data and we just keep on doing
that until the connection is closed due
to a timeout or some other reason so
this is a really cool thing that we got
with it it should be 1.1 because
suddenly we can reuse those TCP
connection we don't have to pay these
extra round-trip for the TCP handshake
and for TLS every time we want to
request another resource so what happens
going back to our synthetic network
calculations and assuming again that
we're making a request to Twitter
homepage now using they should be 1.1
with TLS handshake
we need 1.5 seconds to download 21
resources for a hundred resources the
time went down to 6.5 and previously it
was 12 so that kind of looks impressive
that's roughly 20% increase in speed for
secure request from it should be 1.0 to
1.1 though to be noted again this is all
synthetic calculations so in the real
world example you could get sometimes
better performance sometimes worse it
really depends but if you're kind of in
a vacuum and you just look at these
values you get much better performance
to you to keep alive connections in
reality what happens is that we're not
waiting for 6.5 seconds for a page to
download on average if we got good late
and see it's going to be this is going
to happen much faster reason being
browsers are not simple socket
management apps they're actually quite
smart in doing a lot of things and one
of the smart things that they do is that
they open six simultaneous connections
per domain and this number can vary
based on the browser or a setting but
six is kind of the general magic number
so if you go to the waterfall model in
the network tab you can see how sixty
requests are dispatched in parallel and
the six requests are dispatched in
parallel and if you want to see the
clustering of these what you need to do
is you need to sort by priority if you
don't see priority in your network tab
right click and take the priority this
is the same place where you can find if
you want to see what protocol what
version know what what protocol in
general the client and the server are
using to exchange data so in this case
you can see that the coin downloaded the
page and started passing it from top to
bottom and discovering resources it
fires up the requests where CSS and
JavaScript become the top priority
because they will block the rendering of
your page and everything else is kind of
lower priority things like images now
this is probably a good moment to talk
about out of line blocking hand of line
blocking means that each subsequent
requests request has to wait for the
previous request to finish so it's all
very conceptual it's like a line so it
just goes one of to the other you can't
make the request for the JavaScript
until you've got the response for your
HTML document now if something happens
with the HTML response while it is in
transit here's not only you won't be
able to download to fetch the HTML
document from the server but also all
these subsequent request responses won't
be easily executed so everything else in
the pipeline is kind of waiting there
and is unable to be fetched and you just
looking at your browser and it's
spinning to infinity so I've mentioned
pipelining which pipelining is was kind
of those things that is like oh we know
how to solve had of line blocking let's
try and do it because we've obviously
been
we're of this problem for a while
pipelining is a mechanism that will
allow us to break from the strict
request response cycle so what happens
is after the TLS handshake the client
can request multiple resources at the
same time so in this case a document and
a JavaScript and just fire them at the
same time without waiting for the
response to come for the first response
to come back and hence in this case make
better use of the server time so the
server will will find will will
acknowledge for the pipeline request and
so it will probe the response for the
HTML reads it will probe the HTML
response and the JavaScript response it
sounds great in theory but what why we
haven't been aware of the pipelining as
much and why the question is why is it
disabled in browsers actually by default
is because parallel processing on the
server introduced subtle implications
and because it requires extra
serialization of response and actually
pipelining sometimes introduced worse
performance or didn't didn't bring
anything better to the table it there
are some use cases of good use of
pipelining but it's mainly when you
control both the client and the server
right let's have a chat about HTTP
service a little bit so in HTTP 1.0 1.1
in both 1.0 actually it should be had as
our textual so the average request will
have from 700 to 800 bytes it can
explore it can explode up to to hunt to
kilobytes usually because of the cookies
cookies are can become quite lengthy and
it doesn't seem like it is as much but
it is an
were handed every single request
response incurs every time so even
though we perform optimizations and
compression on the response message
headers generally remain uncompressed so
talking about bundling and minimization
they're like second nature to us we
religiously put everything in one big
file but one big file and we concatenate
it and we try to make it as small as
possible because based on the learnings
of how HTTP works assuming it is on top
of TCP this this bundling and
minimization makes a lot of sense we
minimize because the fastest part is
about not sent and we concatenate
because we can reduce so much of the
protocol overhead all of the handshakes
all of the header bytes all of that so
it's similar to doing pipelining but
it's just we're doing it at the
application level so reducing the number
of total requests is one of the best
performance optimizations spriting is a
very similar hack slash trick this is a
sprite I took from Amazon homepage so
instead of fetching every single tiny
resource separately which would be very
very expensive we put them in one big
file and set it all as one
but we shouldn't fool ourselves that
this comes at no cost this brings
similar this brings some of the issues
so with bundling and minimization we
have a certain complexity in creating
them so we're actually pushing the
complexity from the protocol level to
our application level we have potential
caching problems with bundling similar
with bonding and minimization and
sprites once one of these tiny resources
one of those tiny JavaScript files
changes we actually have to flush the
whole cache and refresh it instead of
just refreshing the one part that has
changed and we're surprised for example
the browser has to keep all the
in memory so the reason why everybody
started paying so much attention to HDPE
as a protocol is because of HTTP - so
the basis of HTTP - is another protocol
called speedy or as PDI it's an
experimental protocol developed at
Google and engineers at Google had
several goals in mind when they were
developing as a speedy they wanted to
reduce the latency of web pages they
wanted to minimize the cost of
deployment complexity and to avoid any
changes in the network infrastructure
because this is something that we
actually cannot change we're not gonna
go all over the world and change every
single router switch and everything just
kind of like erase what we had
previously and start fresh this is not
how the real world works so in 2015 it
has been draft for HTP - and h-back have
been approved but the work is still
ongoing so HTTP - also how the cool kids
call it age - is really really cool and
full of Wonders if I were to do a see
URL request and by the way crl can do
can speak HTTP - then for the outside
observer not much would have changed
you'd see the same request response
cycle same headers status codes that
keep alive connections all of this is
there but for us to understand what is
the difference between all the previous
versions of HTTP and h2 is we need to
look underneath we need to analyze the
packets so what we do we were we fire
Wireshark and once you're able to
decrypt your traffic because HTTP - will
be served over HTTPS only in browsers
and we filter our HT traffic we can see
new types of packets which are actually
we're going to learn they're called
frames so going top to bottom we have a
window update setting
had as data ping on and on so the key
differences between HCP 2 and its
predecessors are that it is a binary
protocol instead of a textual so it
introduces a binary framing lam can
explain more on that later
so because of that it will allow full
multiplexing of requests and responses
we can also do server push in other
words the server can proactively decide
to push some of the resources to the
client similar to TCP that can do flow
control now we can do flow control add
the HTTP at the application level HD and
HTTP so and due to multiplexing we only
need one connection per origin instead
of 6 it has been proven empirically that
one connection is enough so you don't
need to open more you won't get in any
better performance if you're using HP 2
and multiple connections and H 2 also
introduces had a compression to reduce
overhead it's actually not at HTTP -
it's actually H back but we're going to
talk about that soon so binary framing
layer is the key to a lot of features
that are possible in H 2 if we look at
the previous versions like HTTP 1.1 what
we did we had a convention we said the
first line is going to be our request
where we say the verb the document
address we say the protocol version and
then for the each new line we're going
to have a request header and when we
have two new lines this is how we
separate our headers from our payload
and this was just just a convention with
the h2 we get something else so h2
messages are encapsulated in a specific
way and we get these frames and there
are different types of frames there are
10 types of frames we've got had
Hatter's are obviously there to send
headers data is to send payload settings
to negotiate various configuration
parameters between the server and the
client ping to measure the minimal
round-trip and go away to initiate the
shutdown of a connection and basically
because of that because we suddenly have
HTTP 2 which uses which is a binary
protocol it's suddenly not compatible
with the previous version and that's why
we have a whole version increment and we
don't have an HTTP 1.2 but we have an h2
so multiplexing multiplexing in essence
means that we allow multiple requests
and responses to be in flight at the
same time it is and it's something very
different from what we had before where
we said we have a very strict request
response cycle we send the request we'll
wait for the server to do the processing
the client just kind of doesn't do
anything meanwhile then it receives the
response and only after that it can send
another request so how does it work in
more detail so let's imagine the whole
canvas is our one single connection you
could think of it as a TCP connection so
the connection converter can carry
multiple stream streams or virtual they
don't exist physically there's no
separation in the physical world that
will tell you this is stream a and this
is stream B they're all kind of just all
in our mind and the number of streams
can be quite high I think it's up to 2
billion or so so streams are
bi-directional channels of communication
that roughly it will translate like a
request response cycle and each stream
can carry a number of messages messages
identified their stream by the stream
identifier this is kind of where the
whole virtual part comes in and messages
themselves are composed of frames so
when I'm making the request I've got a
header headers frame and when I'm
getting a response I'm having some
headers and some data for example
and that's pretty much it
what happens is that while the client on
the stream one might be sending a
request on the stream to it might be
getting some responses from from the
server and we can have many more streams
initiated to initiate all those
different exchanges and to interleave
requests and responses in parallel
without blocking on any one of them
because we have so many streams as I
said you can have up to a two billion
you need to have something like a stream
prioritization each stream may have a
weight and it can be dependent on
another stream in concept this is
similar to what browsers have been doing
it kind they were assigning some
internal priority to all the different
resources and saying that style shades
and JavaScript are going to be of higher
priority than the images but now you
actually can do it yourself with the
stream prioritization if for some reason
some images are going to be more
important for you to be downloaded
rather than the other images and due to
multiplexing at the application level we
actually don't want to overflow either
the client or the server so we need a
way to tell them to tell either of the
parties to back off so that's why TCP
flow control is not good enough for this
we're introducing HTTP flow control
where both the client and the server are
negotiating and they're setting their
desired window size which can be a
shrinked or expanded right so let's talk
about server push so if you think about
it if you're a client and you're
requesting an HTML document and you're
not a links browser you probably would
want to download all the other resources
for this page to render properly like
style sheets and JavaScript
so kind of makes sense that you probably
need them this is the idea behind a
server push so if the client is
requesting an HTML document what the
server can do in this case the server
will prep the response but it will also
meanwhile initiate a couple of other
streams with some frames called push
promises so your application server can
decide that if you're requesting index
dot HTML you probably want to also
download script or Jess and maybe also
the style dot CSS so what the server
does is it opens up extra two streams
and it sends those messages to the
client and it's important that the push
promise frames reach the kind earlier
then then the response itself because we
don't want the client to grab the
response start passing it and then
sending the requests straight away
because then we'll create duplication
now once the push promise frames reach
the client the client can either accept
them or reject them by closing the
stream I think this is just a conceptual
model of how server push works how
you're going to apply it in the real
world is a totally different problem and
probably would require a talk in itself
because there are multiple problems that
you would like to solve what about what
if the client has the resources cached
and probably doesn't want the push
promise at all to happen what about the
security what about the overflow well we
have the TCP we have the HTTP flow
control there are some proposals where
you can use the link header would like a
preload so that you can negotiate this
better would you like to push all of the
resources at the same time or maybe you
just want to be a more granular so I
think there are a lot of questions that
people are still asking and
experimenting
in terms of serverpush but in general
this is the journey this is the idea of
how it works right so we said that mmm
HTTP headers represent an overhead
especially when we've got fairly lengthy
cookies in h2 this is solved using H
back which is a compression format
specifically designed for to work well
with HTTP headers it is composed of
three main components so we first we've
got a static table
it contains 61 commonly used HTTP
headers we have a dynamic table that is
initially empty but will be populated
with new headers for the duration of the
connection and the rest of the headers
that are not indexed so they can't be
found in a static table or the dynamic
table will be compressed encoded using
static Huffman encoding code which has
been statistically obtained on a large
sample of HTTP headers and so this is
how it's gonna work when I'm making my
first request and if I'm not using
h-back what will happen I'm just gonna
send all of these headers as they are
with no compression now if I'm using
h-back the common headers found in the
static table will be encoded each with
one byte so things like method get' and
scheme HTTP and path slash is a very
very common hat as we use them very
often they can be found in almost in a
ton of requests also things like
sometimes we just we just know that
except or user agent without the the
other part the value of the key value
pair can also be encoded with a single
byte
so everything marked in green is what we
can encode with a single byte all the
leftovers which were marked in which we
may
in black will be encoded with a static
Huffman code in addition to that we're
gonna grab the key value pairs of the
headers that we're just now encoded
using Huffman and they will be appended
to the dynamic table what is happening
now is we're going to save the cookies
which are probably quite lengthy in an
index table and we will save precious
bytes for all these subsequent requests
so what I'm gonna do my second request
using H bar this is what is going to
happen we will encode using the static
table these are all the headers which
are marked in green
so again method get' scheme HTTP we will
encode using our dynamic table how does
that we have recently inserted and these
are pink purplish kind of color things
like user agent and cookie and and host
and the rest of it is going to be
encoded using the Huffman code believe
it or not but this kind of compression
can reduce from 30 to 80 percent of
header bytes and I think this is quite
impressive so let's think about all of
the previous best practices slash hacks
that we've been using in previous
versions of HTTP and how do they
translate in the world of HTTP 2 so we
used to have six connections per origin
this is no longer needed if we're using
h2 so one single connection is enough
hence we also shouldn't be doing domain
sharding domain sharding was something
used for websites that desired to have
more than 6 connections so they would
shot their domains and the brown would
trick browsers into thinking that they
can open not 6 but maybe 12 connections
at the same time so this doesn't make
sense anymore
inlining resources is generally not
necessarily because of multiplexing and
push and also once we are not in lining
our resources we can actually cash them
so I think this is a benefit that we're
getting naive concatenation of assets so
bundled assets assets we said resulting
more expensive cache invalidation and
delayed execution and unnecessary
transfers for some of the pages but
smart concatenation still makes a lot of
sense because a data can be better
compressed if it is bundled and be even
though the requests are cheap there's
still requests and they still carry some
overhead so we should keep that in mind
the similar idea applies to sprites your
app needs to find the right balance
between putting everything in one bin
versus spawning hundreds of requests for
each time of resource so these things
kind of should find the right way to use
them
what does still make sense in the world
of HTTP - well these are called
evergreen optimizations so regardless of
the HTTP version you're using there
should be there and you should be
thinking about them so we need to have a
fine-tuned TCP stack make use of smart
caching use etags caching is or is a
fantastic way to optimize your app use
CDNs because service placing servers
geographically closer to your users will
reduce the latency and this is not only
a problem for some of the more remote
countries but maybe like fold first of
all places it can still be a problem
think about Australia I think they have
horrendous latency and it's very painful
to navigate the web if you're there so
eliminate unnecessary resources there's
if if you don't need something just
don't don't put it there don't
extra latencies for no reason is
generally no fun
always compress your assets regardless
of the HTTP version compresses reduces
the number of bytes on the wire and this
is what we are generally trying to do
and maybe try to reduce your I had a
bytes mostly think about cookies maybe
don't carry so many cookies with you
along but generally hooray for h-back
right let's talk a little bit about
statistics so this is a chart replicated
from a presentation given by Patrick
Hammond on HTTP 2 and even though it has
been quite a long time because this
presentation was given in March 2016 I
think the data from there still applies
so he shared some stats on the impact of
latency and HTTP versions on the
Financial Times comm website so what we
can see from here and as you can see the
blue line that's HTTP 1.1 and the orange
one this is HTTP 2 is that for users
with low latency say under 100 200
milliseconds you can't see much benefit
in switching from HTTP 1.1 to h2 it kind
of seems the same but h2 truly shines
once your latency becomes much higher so
you can see a big impact based based on
that also a bit of the outdated stats
but again couldn't find anything more
fresh than that so last year that is
roughly a year ago Mozilla at FOSDEM
shared some traffic stats anonymously
shared by the Firefox browser so if
we're looking at all of the traffic that
goes for Firefox
it says that roughly 30% of it is served
using h2 I found some other data there
is a website called w3 tasks they have
all the different statistics about the
web and they claim that currently 23.2%
of all the websites using or using HTTP
- and this is this year and last year
they said it was roughly 11 so maybe all
of the Firefox users are all very HTTP
to savvy I don't know because they had
30% last year but this doesn't look too
bad
but if we take into account that HTP -
traffic can be if we take into account
only HTTP traffic based on the idea that
h2 can be deployed only via HTTPS
because otherwise you can't do it is
disabled in the browsers we're seeing
that it's actually 60% of the traffic
the reason reason being is most visited
websites like Google and Twitter and
Facebook a lot of them you are served
partially or entirely via h2 and piracy
if we take a look at the top 10 million
websites then this is going to be it is
12% I think regardless of that it looks
like a very nice trend talking a little
bit more about HTTP - stats so fastly
did a presentation and you can find it
online they've got a whole bunch of
slides on the different experiments that
they ran comparing the performance of
different HTTP versions and this is some
interesting insight that I found out
when testing HTTP 1.1 and HTTP 2 in
different scenarios so in this case what
we're having we're testing serving a
website in Firefox we've got 40
milliseconds later
see and zero packet loss so the website
loads faster over h2 h1 is lagging
behind again this is the blue tie and
this kind of matches our expectations
from everything we've learned HTTP 2
should be this Silver Bullet that is
just gonna be like everything's going to
be so much faster and a lot more amazing
now we are running the same test we also
do Firefox 40 milliseconds latency but
we're introducing 2% packet loss and h2
performs poorly the page will load
faster in the browser over HTTP 1.1 and
you might be having a question why is
that why is that happening
even though HTTP 2 eliminated
head-of-line blocking at the application
level head-of-line blocking still exists
at the TCP layer so TCP as a protocol
it's reliable and ordered and kind of
wants you to make sure that you've
received the packet that it has sent
this makes it prone to the problem that
we have just discussed which is the head
of line blocking another question why
this didn't happen for HTTP 1.1 well it
probably did happen but because in
browsers when we were running h-2b 1.1
we are opening six simultaneous
connections well some of them will
succeed so you're not actually blocked
on one single connection so you're going
to open multiple so yes six connections
in HB 1.1 actually performed better than
h2 with 2% packet loss this is where
quake comes into the spotlight quake is
an experimental transport layer built
over UDP that very much
doesn't resemble UDP apparently it
resembles something more of a tcp plus
TLS plus h2
it is considered to be a step forward
eliminated in eliminating head-of-line
blocking problem at the transport layer
and I think we're yet to see the
finished drafts of it and maybe who
knows the widespread adoption in the
future if you want to learn more I
highly recommend this book
this is high-performance browser
networking by ilya grigorik
an engineer from google it talks not
only about HTTP but all the different
components that make up the web it has a
great chapter on mobile networks and it
explains in a lot more depth all the
different things about h2 and latency
and why latency is more important than
bandwidth so highly highly recommended
and this is it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>