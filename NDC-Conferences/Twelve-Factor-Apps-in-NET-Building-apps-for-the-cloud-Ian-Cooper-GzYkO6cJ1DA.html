<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Twelve Factor Apps in .NET : Building apps for the cloud - Ian Cooper | Coder Coacher - Coaching Coders</title><meta content="Twelve Factor Apps in .NET : Building apps for the cloud - Ian Cooper - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Twelve Factor Apps in .NET : Building apps for the cloud - Ian Cooper</b></h2><h5 class="post__date">2017-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GzYkO6cJ1DA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I know I've got such the remit who what
we can we can have a little prayer
if you like while we're waiting or
everyone else kind of filter in I just
out of curiosity who's who's kind of
heard of 12 factor apps okay quite a few
Liat square you get and how many of you
are actually applying any of those kind
of principles okay Gav and be aware this
talk is pitched as basically being an
introduction 12 that wraps so I'm not
necessarily going to give you huge
insights you might already have but feel
free to take it on board what I will
talk about traffic traps and how I see
maybe the factors and how is he working
in practice but you may if you already
doing a lot of work with there's an idea
that I don't know whether I want to
think about that
well did some demos with Garnett core
and docker in this session I did if you
look at the advertised version of for
twitch what about possibly using service
for as your service fabric or service
header where slander or their functions
we're not going to do that in the
session it just gets a bit too deep into
the weeds of technology stacks so we're
guys can ease docker to manage
containers instead I suggest if you're
really interested in looking at kind of
service to stuff there are a couple of
other talks that the conference around
serve list is Rogers giving one tomorrow
on service so if you want to go and dig
into the weeds and that that's pretty
bad talk to do though I probably won't
do that all right since 20 so I'm going
to start okay so this talk is about 12
factor apps what are they what does that
phrase mean what is it about and the
idea is to run through what are they for
what actually are the 12 factors and how
might you actually apply them when
you're building your own software and we
will show some simple examples in donate
core running with docker my name is Ian
Cooper for those of you that don't know
me anyway haven't you been to a talk
I've given before
great I haven't put you all off yet so
this is the kind of projects much of you
seen before there are only two points I
have a call-out one is that I'm
obviously old and the second is the
point on the bottom which is just
because I am standing here there's a
false notion of a power relationship
which doesn't really exist
I just quite like getting up on stage
and talking about stuff it doesn't mean
my commitment many smarter and you
should all feel encouraged and empowered
to speak at conferences as well right
and we often come across this mistake
that people don't know anything and
that's probably because you know the
things your peer group knows but that
may not the witness or what everybody
else knows so do you get out there and
we want to hear your voices such as the
same guys with time that's where I work
it's of no real interest here whatsoever
were a internet startup say cloud-based
service business apart of the fact that
I'm not a consultant I have nothing to
sell you right and none of the opinions
are given in this we revolve involved in
making any money whatsoever so you can
take them in a certain amount of truth
what's our agenda I will talk about the
origins where is this idea of 12 factor
apps come from why's it become important
the goals what the people why people
choose to follow the 12 fret 2
constraints what are they trying to
achieve by doing that you know as any
kind of practice what as a goal is not
important to you maybe you shouldn't
follow the steps of practice then I
divided up the traffic's from the number
of different groups when I think of his
design factors the stuff you think about
when you're designing and writing your
code one base you certainly to think
about really when you're building
releasing your code on one you think
about really when your code is live in
production you want to actually manage
that for clarity I've got a couple icons
the top one basically says this is about
what 12 factors actually talking about
and one below says that's me talking
about some of the concerns I have in
this space the two analysts say the same
thing I just want to basically get some
colorful disclosure about the to see
them get confused okay
I don't think you can actually download
the presentation Dex I'm not actually
uploaded it so you can't do that
hopefully you could all see everything
at the back you can't see anything or
you can't hear anything please just
shout it's a small enough room we're all
friends just tell me and I will fix your
problem but you can't download the code
it's up there on github
so if any of you are struggling with
reading code or navigating the big
screen please feel free to go to github
and get out of that he wants to take a
photograph either I will put the slides
up at that location so you better get
them after most you're talking
presentation some don't know corn
Dockers being shown will show actual
code words and in live coding but we'll
just run stuff and as I said before but
and change the published programming
we're not going to show so spherical
lambeth running into the weeds on that
okay let's get into the meat of it where
it had to come from so 12 fighter apps
originally came from the guys at Hiroko
Hiroko essentially do a platform as a
service offering for deploying
applications and the originally started
with Ruby but they're now polyglot and
over time they came to write basically a
set of guidelines which came standard
twelve factors as to what made a app
successful when it was deployed into
there at their platform as a service
environment and I don't put Adam Wiggins
use the code found was the co-founder
effectively or Heroku as management
consultant that's in that building I
think was I think one of the primary
people behind driving this set of
requirements that I don't have to learn
either I want to write on the right
we'll run through that but that's
basically where it comes from and a
number people picked up over that over
time you can see there's a you know
people are written up as a pivot we'll
have a kind of PDF ebook you can get
called beyond beyond the 12 factor app
it cetera but not people picked up on
this idea of 12 factor apps as being a
way that you may want to build your
applications I said it would directly
associated with the term cloud native
okay look at the goals about cloud
native
sort of in my cloud made
so crowd native apps are apps that we
believe have a certain set of
characteristics the first ones probably
think about is speed okay some people
talk about cloud native they're often
referring to this agile notion of I want
the shortest path possible between the
desire to create a feature and the
actual point at which I gain feedback on
that feature if it's deployed live into
production on running so native apps are
seen as and ones that basically have a
model which shortens that probably doing
continues deployment certainly doing
potentially continuous delivering as
kind of a goal but at the same time
obviously we want to be safe so crowd
native also says well how do we achieve
that yet maintain safety I don't want to
ship rubbish to my customers even and in
some kind of continuous deployment I
don't want them to basically say well
it's great you've got new features every
day but if only the app actually worked
so you need to be safe at the same time
and need to scale in principle I'm
building this because I'm hopeful it's
going to be very successful potentially
even virally successful and therefore I
want to be able to scale easily I don't
want to have to kind of rewrite the
whole thing when it comes to the point
in time when I decide that effectively
this is things going to take off and
those kind of factors are seen as
important to this idea of being kind of
cloud native so a number of ideas
usually come under the bucket of cloud
thanks people said you want to build
Canada's app sees the ideas you should
you should buck it together so the first
one is micro services this isn't a micro
services talk you'll probably will be
grateful about that fact you can go and
see me online talking about micro
services if you like and you'll be if
you want to pursue that thought line but
obviously this idea that we don't want
to create monoliths anymore we want to
slice our application up into a number
of pieces those pieces give us some
benefits they help us scale beyond tp2
teams we can have more people but also
effectively because there are smaller
units it's easier for us to release them
quickly there's lower risk involved in
those releases it's easier for
even developer to remain aware of their
content so they generally help us with a
faster scale of release and generally
micro-services allows to horizontally
scale because there are large numbers of
these individual parts which kind of
right sized and we can choose to
actually scale those up so API based
collaboration so the other thing cloud
native essentially tends to move towards
is this notion that really what we care
about nowadays are api socials will of
course called api first and we prefer to
ask right so the first thing what to
think about when we're designing any
kind of service is the api and by APL
you already just mean the contract that
this thing is basically exposing to
other other services or devices so it
could be a restful api but it may also
just be the contract we use basically
for a messaging environment for example
there's no real positioning at that time
api it's much more old-school in that
sense but saying these these services
essentially communicate by AP is self
service agile infrastructure
what a great victor neighs so the idea
here is my developers are saying i want
ya I talked to the product guide the
product I dream is a great idea we built
ourselves a flashing you micro service
with api's we want to get it alive as
soon as possible all right and all we
don't want is the ops guy then says what
we could provision you and you server
will get most video onto the phone to
data sent there that would be about six
weeks and then when we've got the actual
kit installed in the rack I'll see if my
team has got any availability to go and
is still basically no less on it and get
it all up to speed right that's not
really going to work for us what we
really want to be able to do is develop
as an idealized environment is I just
simply like you know fill in a form and
say I want basically to have a server of
this characteristic provisioned in our
environment held can I have one of those
please right and your infrastructure
says
certainly here you are here is one I
spun it up for you and you point it and
this is really where a lot of these
ideas like docker pads and server lists
are trending towards this idea that
putting developers back in charge in the
sense of being able to effectively
create the infrastructure they require
to service their code on demand and the
it's a kind of end goal if you like if
essentially the sort of DevOps movement
where effectively teams basically are
capable to pulling out to production
environments and if you do have a kind
of central operations team their role is
much more an enabling the self-service
agile infrastructure and providing that
then it isn't actually providing the
individual machines of form part of the
infrastructure and finally twelve
factors well I've got one more
contractor the twelve factor effectively
comes in because it's seen as the way to
build the individual parts you have the
micro services if you like such that
effectively they work well in those
kinds of environments particularly a
self-service agile infrastructure
environments and beyond that anti
fragile so crab is about sentiment
structure anti franchise this notion
that the more stress we put something
under
actually the less fragile it becomes
which is a paradoxical statement a
general a paradox a statement means you
things like chaos monkey if I
essentially in my production environment
you something like chaos monkey to take
down servers on a regular basis that
effectively my developers will react to
that approach by assuring that that
would they survive machine failure and
therefore my overall apps health becomes
more stable this is like some anti
fragility and your fragile obviously
someone says like Oh show is much easier
in self-service agile structure
environments other temperatures as a
service because we effectively can
provision new machines rapidly to cope
with that kind of world
ok all right today understand what cloud
not've is a little bit let's look at
through what some of the goals of twelve
factor apps are part of that so don't
tend to talk about the other areas okay
such thing twelve fact that's one of
their goal which is basically when
someone joins a team and this you know
it happens I raise our hand to happen
this happens to huddle quite a lot you
join the team and you sit down to start
work and you pull the repo for your for
your team's work
and you then want to try and build it
and run it right and then everyone kind
of is around the rest of the room and
they start saying well you're going to
have to install ax and then the cell is
a wiki page wiki page tells you what
you've got to install to get our app
running and certainly are mired in this
world of complexity and all you wanted
to do was actually contribute to the
team and towards their goals and then
generally probably you've got some
hideous build script brennan rake or
sake or fake or something else and
you're wondering what was wrong with
basically just you know building the
solution file why why does it always
have only got so complicated and why do
i have to understand the build script
which other user the same toolkit other
teams
you've done in your own slightly
idiosyncratic way because Tom or Dave
thinks that's best right what we really
want to do is remove that whole problem
space so we want to create a position
where effectively a developer can sit
down and pull the app and then just run
it we want to use things like vagrant
docker to give us those abilities to
effectively allow us to say okay there
is there a script see we should let you
just run the cart you can do vagrant up
you can do docker compose up and you can
actually get running with your code
immediately clean contract with the
youngling operating system operating
maximum possibility execution
environments fantastic doesn't it um the
key to this one really is this idea of a
software erosion or software rot so when
we design contractor talked about this
quite a lot and of course about this you
know this notion of basically that
software decays over time and obviously
there are two reasons the software decay
the first is to kind of death by a
thousand cards right and in that problem
we're essentially we're continually
making changes to our software which
could be a good thing we're expanding
its capabilities reacting to feedback
from customers but we fail to refactor
and as such we put up technical debt
until the point of activities it becomes
increasingly hard to maintain the look
after our app we've probably got a big
ball of mud with experience software rot
that's not the object is a talk that
taught that the problems solved there
are generally
refactoring and I therefore bring in TDD
and everything else that goes with it
but the second problem we talks about
effectively is the idea of I put some
code on a server I don't change it for a
year and I come back in a year's time to
figure out whether or not this thing is
still running and what problems could
have occurred that means I go there and
suddenly discover the whole thing's a
train wreck and generally speaking what
we find is a lot of the problems are
environmental for example there's a
hotfix that comes out we have to apply
to all the service because of a security
threat that gets those run for servers
and adds a problem interacting with our
library so we had to download new
versions of basically run time so we
were dependent like a GAC and we
essentially install new versions
basically of the runtime and our app
doesn't particularly like them it breaks
them in some way etc the we change
passwords we move a server were
dependent on like a database or another
micro service and we've hard-coded into
the config file where those items live
and without doing and you deploy of our
software it's going to break I mean how
many of you think that if you left
basically a server you installed some
code on it and the code was running and
you basically snapped it for a year but
let ops maintain that server as well
however they need to and never
redeployed that your code would still
run nobody not many volunteers in the
audience right so that's one only two
effects trying to address it also works
towards a little bit you know world in
which we are increasingly possibly
thinking about deploying to different
environments in terms of OSS
particularly for those just in dotnet
land you may well be thinking about the
port targeting Unix now as well as
targeting Windows servers that's quite a
big change and you want to be able to
avoid constraints and given the servers
that you're 2.2 ok yeah this is more of
the same with the last one really what
it's actually saying little bit is that
we we want to avoid basically relying
too much on an Operations team to
provision servers and so for we want to
deploy to the kind of environments to
support
that which could go up to including
platform as a service or a debris
slander or as your functions as models
where effectivity we don't have to worry
about provisioning of the containers
even the actual platform itself
provisions the containers we run it ok
and at certain scales or business
particular at the beginning it now I
think increasingly makes sense through
our initial deployments as a business to
be into things like pads and serverless
and to leave bare metal for approaches
for later in your life cycle when you
need the performance because you can't
cope with basically noisy neighbors but
increasingly it's becoming less
justifiable as two wires companies we
manage our own infrastructure ok
minimized emergency development
production maybe consider máxima dirty
this is about a number of things but I
sent since you chose the course
inventory waste the idea that
effectively I've got code sitting
essentially on my development service
has never been deployed into production
that causes all sorts of problems both
cognitively for the developers you have
a load of well what's living where and
also essentially for the problem
effectively of managing managing live
issues etc so you really want to
basically have them as much as possible
but the other thing finally is that the
developers sometimes choose to use
different tools in their development
environment or different layouts and
development environment topographies
than they do in production so you have a
load balancer for example in production
but you're not using one many testing
locally you decide effectively that even
though using you know PostgreSQL or
sequel server and production you're
getting your sequel life on your machine
or working on your machine and the
problem with that of course is that
while it seems like these themes
probably are hidden behind some useful
abstraction like an ORM or a task
manager eyebright or celery the reality
is that it's not as hot swappable as you
like to think and you do get differences
in behavior and you want to really flush
those out before you get to production
environment because you want to better
to deploy to production without having
another an additional kind of light step
you want good just say what if it works
in my machine is should work in
production
and can see up so this idea essentially
that we would like to really think about
being able to horizontally scale to say
we want to introduce new instances
basically of our apps and all right in
the event that essentially we succeed
right so what are the obstacles to
horizontal scaling okay let's talk about
the fact that then and we'll run through
quite what they say some of them are a
bit strange but we'll try and talk to
what I think they mean rather than they
say okay and one is poor blinding
exposed services while our port blinding
what does that kind of mean at a
fundamental level that says you should
find to a port receive requests by that
port on responses so it's very simple to
understand this in the idea of some kind
of app that's operating in a kind of
micro services environment I spin the
thing up and essentially it says I
receive requests on port 80 or and I
basically respond that reflectively
right and I'm talking HTTP and maybe
effectively I'm being really secureness
in even in those internal it's talking
on port 443 with doing HTTP but the idea
essentially is that the way that I
communicate with the outside world is
well known essentially it's a port okay
and equally the port doesn't necessarily
have to be HTTP we can use other kinds
of poor in terms of what then what are
you trying to say as an approach we
include things like saying I'm going to
talking CPT for example so I'm just
going to listen by like MTP to a rabbit
MQ broker and I've got a subscription
there and I'm just going to receive
events from you over that model but it's
really the cart of this is saying what I
don't want to have is being hosted into
some kind of application container now
dotnet people have had less of this kind
of crack than Java people job people for
you know a heavily it will help a long
time heavily into the crack about
servers but the idea is actually my app
should be self hosted it shouldn't
depend upon a given action
because that tends to create a problem
for us in terms of releasing to a given
environment and we had that in that
environment we want to keep always
maintaining that in that environment
right and it's just simpler to say well
I can self host and spin up and just
receive HTTP requests the big one for
example net Lander BIA SLA SS
effectively an application server and
certainly started thinking about things
like I want to deploy to docker your
question will become or where am i
sticking is right because if I want to
be our is host is if I want to use
containers well which one of those
containers hold holds is all of them so
once you start to think about say for
example container models and I find
contain them on as one of the easy way
to think about 12 factor in a container
water like docker effectively I've got a
host and that host is essentially
running a number of containers and those
containers essentially are lightweight
and sharing current operating system
services class something like IAS and I
want to produce the equivalent of a
webform I'm effectively installing is
effectively across all of those
containers that doesn't really make a
lot of sense and so in this model it's
much easy to think virtually about the
model of me saying well I want to go sit
on port 80 then with what I need to do
is let's get those think about basically
say for example having a reverse proxy
or a load balancer they live outside
essentially my application talks with
over the pour that binds this is very
common in environments like in
programming language like Python or Ruby
but for etc where you tend to basically
spin up something that effectively says
so you can up spin up your Python web
server and we unicorn or whatever or and
you say okay right I mean I said that's
when I serve over port 80 and I'm then
going to use nginx something to actually
act as a web server sitting outside my
app and then dotnet core this is
certainly the same model right but you
can do eyes integration but the
preferred model particulars report to
UNIX as well as towards Windows where
there is no RS on UNIX they're
definitely looking much more a model
where you say I want to basically self
host talk to port 80 and then if you
need to you can load balance me and
everything else basically easy something
like nginx raycho proxy
right this is really the heart of this
one that's saying
please don't use app servers okay
backing services treat backing services
and attach resources okay the first
question is what is a backing service
backing service is something like a
database message oriented middleware or
even potentially another service that we
talk to us another app we talk to to get
our work done right and a tropic trap
says that we treat these basically the
tax resources or what we mean by that
well the idea factorization that should
be hot swappable so in theory what we're
going to say is factually well I may be
talking to database here eventually but
essentially it might move to more and
move here and today it might be hosted
on bare metal and tomorrow it might be
in RVs and I don't really care
all I care about is I have some means to
communicate with like a connection
screen that's all really this says and I
want a better attack attached and deploy
things out will it's a fairly
straightforward one I don't think
nowadays many of us build applications
don't work that way one of the things
that I would recommend in terms of the e
source of this though is I would always
recommend thinking in terms of building
your app supports and apps architecture
but I really like important emphasis
because they make this idea very
explicit of the separation between your
code which essentially you care about
running in your domain model in the
center and the adapters on the outside
which talk to external concerns right
Matt with a bit of Portland between
mitigate managing that process so you
separate your model from all the i/o and
ports mediate so basically you have
primary adapters those are the inputs
and they're essentially the ports that
you are effectively listening on that
you're exposing and you can have
different adapters so basically your
tests can drive your ports but also
essentially you can use something like
you know a Web API live through HTTP
adapter or flask whatever your language
is preferences right an advantage
basically this model is because you had
managing that fire app or you can swap
these so if for example you suddenly
discovered that the dotnet framework
four or five two is no longer the target
of choice and you want to deport your
code I would donate core but you needed
to use the new version Web API that came
with
you find that if you wrote your code
such that most of it lived in domain in
the ports with a thin adapter layer over
the outside that kind of swap model
becomes much easier second receptors are
outputs
those are things I'm talking to that's
really like the database etc now the
output input thing is a little bit off
because obviously I get information back
from my database as well but you want to
think basically of this thing on the
right-hand side of things that I talked
to an organization to fill that original
request and the secretary adapters are
really what I think of the backing
services we talked earlier on about this
notion or bind took pour and I think
basically these other notion of actually
saying treat back in to make sure of
actually treat resources and backing
services leads you naturally towards a
hexagonal architecture where essentially
you're clearly separating yourself from
the means of actually but which you
communicate outside world strongly
consider using the service location ok
we'll talk a little bit why you might be
using environment variable is that any
kind of configuration in an environment
variable or a file they will require a
restart for your application but also
the other problem is is that become
difficult to manage if I have two
machines talking to each other and yes
probably I might better feel comfortable
about going in altering the config file
on one when I moved up onto another
location but obviously they be downtime
during that process when effectively I
had to go in and do the manual editor or
redeploy effectively my config files out
to the location and get things working
again and so even with two machines you
might think whoa be better effectively
but you're just fine with this machine
is by asking it but once you get into
the business of hundreds of machines
it's unrealistic to think you might
actually by hand say I've moved this
basically this app that I'm talking to
this other micro service I've now
deployed out to a new set of servers and
so you need to change the IP addresses
and all these servers that talk to it
originally by hand or you to redeploy
all those service by their work so
you're going to have to use some kind of
service discovery what do you mean by
service discover in this context we mean
some kinds of stupid a key value store
console
all zookeeper something like that which
you can essentially query to ask it for
the locations of services and
essentially some mechanism by way you
automatically register new services when
they come online so you can do sweet
things with things like console
registration engine X for example you
see you can say ok engine X get consort
or straight so and ever basically and
put it in docker and you can say
whenever basically any docker container
comes in with basics it's a web app you
have a template which rewrites a config
file for nginx and will therefore add
that machine into basically a group
which effectively nginx down the load
balancers across and you can do most of
that with stuff that's available on
docker fee to pull down today without
and writing a line of code and that kind
of model which says what I want to be
able to do is deploy very rapidly and
ensure that other services are aware of
my existence and kind of route to me is
kind of a prerequisite to any kind of
real self service in structure processes
and concurrency executes the app is one
or more speculative processes okay
the first key takeaway on this stuff is
statelessness right I don't I mean this
is probably you know one that most of us
now get but essentially your server
process should always be stateless what
you don't want is to force yourself into
the world of sticky sessions you do not
want to get a place where when user a
comes in and talks and gets routed to a
given server they have to keep hitting
that server because you have some state
on that server usually what happens is
someone's written to a cache which is
local to that get the memory in that
given server and they've cached some
information like a shopping cart and
like that and now we have to basically
pull request to exactly that server in
order basically to keep it working so
how do I have this at one point we had a
problem one point whereby you just
follow up loads rather than right into a
shared file system shared storage that
effectively ropes to the local file
system on the server you're uploading to
which meant you had to be sticky for
length of your request that's obviously
a problem because it doesn't allow you
to balance traffic correctly and it
means if that machine goes down then the
users request and workload is lost so we
never do sticky sessions if we a sense
you want to be able to scale okay so you
tend to be what's called Stata where
instead in other words when recognize
that you may have state you should store
in some kind of backing store which
effectively you can reach as a shared
resource so that could be sequence
server or MongoDB or whatever your
database of choices but it can also be
something that Redis write effectively
what you really want to do is a kind of
almost like one in-memory workload for a
cache you can use memory of uh system
process for a brief single transaction
cache I want to mean by that is some
cases you do have a workload that during
the lifetime of the request requires you
to write something away where extremism
at the desk
etc because you intend to complete your
in the operation that process but
otherwise don't do it the other thing to
think about is if you start to think
about basically environments like
service environments right you have no
guarantee in a service environment if
you'd appointed AWS lambda that the
container which is spun up to understand
but it may be that in service
effectively I'll write something
essentially I write a function and that
function essentially I can hook in to
respond to a semantical request a flexp
request amend it to a port and then what
happens is Amazon spins of a container
and that container is use a service or
request
now it may reuse the container between
requests it's it's indeterminate from
your perspective it's Amazon deciding
whether or not effectively optimized
that and that gives you if you have some
any startup time on your container
obviously the advantage of that is you
don't pay that cost of your time
you could be unwise and decide
essentially therefore that effectively
I'm going to use the local filesystem in
my service environment to cache some
information but that's that's going to
be useless to you because you've got no
real guarantee that the next time you
receive the request you won't actually
have a fresh container and you can't
access the con
to the file system so you have to use
shared storage like s3 vocal etc instead
an app or a fashion a particular
instance because it's the only way you
can guarantee that basically your
supplier between requests so you do have
some local resources you can use them
but assume that their lifetime is only
the request to go out by the process
model okay first to go out with the
instances of the process what sort of
factor is definitely not saying is that
in programming runtimes that basically
support concurrency well you shouldn't
use multiple threads okay so if you are
in the JVM or the cell are you should
certainly consider using multiple
threads in your workload and if your
important than you're doing i/o you
should certainly think about potentially
using threads right just don't use it
for your CPU bound but the scale out of
your application should not depend on
vertically scaling the instance that
your code is running on so it's sort of
actually making more CPU power available
to the machine because vertical scaling
is problematic you should have said opt
for horizontal scaling in other words
introduce new instances of your process
so we all know how to do this
essentially because we've done it for
ages in the web why buildings for
example web forms where effectively
we've said well okay I just introduced
new copies of everything and in order to
have more capacity basically to meet
them at the mo on that I'm facing but
generally speaking it's easier to teach
processes to do this model right and
particularly when you get into the kind
of post container world or the post
basically virtualized environment world
even into you're running VMware you're
effectively but you know you're
optimizing the data by running certain
virtual machines on it and trying to
sandbox stuff and what about docker
effectively you're what you're trying to
is sandbox you'll give an application
inside a container run it on a shared
resource like a host but you're
effectively more able to use multiple
CPUs on the actual hardware more
effectively by using processes because
you get process isolation and you get
failure of that individual process
essentially in the sandbox and
impeccable the next to you then you are
effective if you try and scale up by
using multiple threads so you know one
of the early mistakes we made was to
have servers which were running multiple
apps because essentially that the server
we bought Rackspace or whatever had
large numbers of CPUs and to maximize
the use of that server it's running a
single instance of the OS you
effectively have apps that effectively
scaled up heavily by using lots of
threads because that was a strategy
straightforward model once you get into
a more revirginized environment it's
much easier to start thinking about
scaling using sandbox processes
essentially won't impact other items on
the server because if you get around my
process we've had that we had a runaway
process that was dealing with video
conversion it can easily take out the
entire memory space on the server crash
everything else and in a moment of
foolishness at one point we were
actually running video conversion on the
same box as RabbitMQ and someone's video
and being converted took out basically
your message broker you don't do that at
my advice so cleanly separate stuff is
basically one one of voltage basically
moving to a process based scaling model
right you've depth and run the sound
boxes you separate posts of different
workloads what about this a little bit
more make sure to show some code so
generally speaking a good rule of thumb
for some automation if you work load is
that you should respond in about 250
milliseconds to be scalable
if you can't respond within that time
frame and the best thing to do is kind
of handoff that work to basically a work
or task you get to process that so work
or task is just using something like a
message oriented middleware or register
to maintain a cure work and a worker
process then reads off that to your work
and actually then performs the action in
the background right and separate that
and they would you to scale and one of
the reasons is useful to separate those
tens of different processes is because
essentially you may find that the
bottleneck is not your webserver which
is able to factory now to have to meet
the throughput of requests and hand them
back out easily the bottleneck is
actually the processing of the work
basically by your worker process
and in order to avoid becoming to
eventually consistent than what you may
want to do is introduce additional
workers to make sure you're reading
through that queue as fast as possible
so by being having those as two separate
running processes your background worker
process and effectively your web process
you're able to scale more effectively by
contrast if what you did was in your web
front-end use multiple span of a thread
to go and deal with some long-running
piece of work
you cannot scale that separately because
essentially you're now scaling the whole
web server Nords are basically to meet
but to that demand and you're also
basically you know using resources like
we should go towards servicing web
request servicing background processes
and you're defeating your budget or
schedule by moving effectively the work
and the worker processes and web process
to different locations different servers
as all processes are share nothing in
other words there's stateless then it's
fairly straightforward for us to
basically increase capacity under a
model where actually we have additional
processes by simply introducing new
instances of the process so inside our
environment use effectively spin up a
new container the other kind of an issue
here is to avoid doing things like using
demonization the process but just for a
lie on the system's process manager so
in UNIX talking about it do systemd or
up spell that kind of thing in Windows
what this is really trying to ask us to
avoid doing is making everything a
Windows service so you know I've I've
drunk the the top shelf crack or smoke
the top shelf crack I guess which I and
have lots of lovely windows services
that basically run Windows environments
managing basically worker processes
whereas not portable course execution
environments as soon as I do that I get
into a world where that code now can't
run on a UNIX server whereas if I use
task scheduler and I'm just running a
console application and that's actually
portable across execution environments
because I can use an equivalent on UNIX
to then run basically that console
application right so think about
effectively how you are managed in the
operating environment you don't really
to basically plug yourself too deeply
into that operating system by doing
things like writing your own
demonization services it's fairly
obvious you - expensive items we're
saying basically that you can't
effectively use process memory to cash
stuff between requests or the file
system means backing services such as
red especially to do this work instead
okay
and Chad storage again its ability
maximize robust assess that open rates
were shut down right and this is one
pretty much about the fact that if I
want to basically scale by deploying you
in system where application then
obviously I want them to stop up quickly
there's no point in effectively me being
able to say I can deploy in your
instance in my application in an hour
that's really not going to help me and
effectively is no point if my app said
so I'm going to do a whole little work
before actually answer requests and
taking about 10 minutes to actually get
running because before I can answer
requests right you don't want to be in
that position you want to be a position
where effectively I can bring up and you
can pass B to me throughput rapidly that
seems to jumping around I'm not going to
animation right there stop gracefully we
should let me say stop gracefully what
we mean is that when we actually are
terminated we should terminate in a
reasonable time frame we want to
consider how we roasting any running
work okay so say the model engine acts
or bright to the SS library I work on
for TASKE what processes would we say
we'll finish the current running request
for you so we have a question flight
they'll finish them before effectively
they shut down now that can cause you
some issues in some cases where
effectively your preference would not be
to do complete running words because
that running work takes a long time so
for example if you're doing video
conversion think long term space shut
the service down in that case
essentially what you really want to do
is we'll have a model where you would
nap back to a key for example to say
okay I'm not going to complete this work
or give a response to the user
indicating failure and then stop in that
case some of those cases you may need to
consider whether you're you know in a
transaction effectively you can
effectively rollback when you when you
stop or essentially your idempotent and
you don't care for a
Qwest has been partially complete and
you're actually under service again it
is a easier cognitive model to stop
basically by actually finishing work
that's currently in process but if your
shutdown times are too long you may have
to go into other other areas what we're
looking at here trying to try and get is
elastic scaling right so we're looking
to make our apps elastic and that we can
effectively deploy new instances how
those instances automatically register
where some kind of service discovery
tool have our load balancers begin
directing traffic to them or have them
start basically consuming traffic off
keys and have them start up and do that
quickly because I'm making demand or I
have to be able to shut them down
quickly because we no longer want to pay
for them because we're no longer
actually using them or we want to
basically move workload between various
environments to schedule properly okay
all right
which makes me code he's probably more
bored with me talking about design
factors it's only ticular questions at
that point no good okay so we have some
code which we've been writing verse
teams and writing a huddle kind of
example of looking at how we might look
at doing in dotnet core and so we took
as a project for a number of teams so
that others other than like just the
team I work with together project by she
doing to feedback and so if you know
because D back-end essentially is a set
of tests out there which let you
essentially implement an API server and
it's people have basically shown
implementations meeting that particular
requirement in a number of given front
works in case we took a specification to
use we added in so we wanted to do some
asynchronous workloads we added a bulk
add in so that we could basically
demonstrate handing that work off to a
separate worker process to process over
over a key there's a version he was
inspired by a to do project which
basically deals with front-end
frameworks for example angling to react
and shows how they all build basically a
task management or light app right
and there are a set of what you can't
see that side to side nor that one test
that you can run the other working so
first thing I do is I will attempt to
show you this working and then we will
okay then we'll cover cat the code and
hancinema doing for time okay once you
finish at 1120 i guess or so we're
running basically docker and we using
docker compose and we basically have a
number of containers out there there's a
rabbit one so basically we have a rabbit
broker and we basically got a web
essentially container which essentially
is receiving hosting our api and
receiving requests we have a worker
available work that basically is reading
work off the rabbit queues and
processing in the background or we have
nginx acting as a reverse proxy okay and
over here and i will go back so you can
see better i have basically the tests
this is the basically the TD backend
tests I run them locally you can run
them straight off line might want to
make sure it's going to run in a
conference talk and essentially that is
the address effectively of my app which
is basically sitting in port 8080 and
that is the docker machine address
virtually and when I run those tests
you'll see essentially that it's going
to run through and say I actually I meet
all the specific artists specified
requirements for to do app me never you
can this all this code is available on
github so you can go and get it down run
it yourself and the acid test what we're
saying is true should be the you should
better pull it down and run the docker
compose file and actually get the code
working I hope and then you can see
we're proving that point about being
able to get a hold of your codebase and
just run it just to show you solid HTTP
requests going on the backend is doing
some get some post some updates that
kind of thing cetera right and we can
also do offload a back-end workload so
this is going to do a post requested
basically there's only one item into a
but
TenPoint instead we could have hundreds
of items and when we execute that one in
Fiddler this is not part of the original
test we go to our response back saying
accepted accepted the work it's max
packed back to us to say you can move on
with your lives I've got to work and
that gets processed a sensible worker
process so if I look at the share pretty
good history the logs basically the
worker process you can see essentially
at the bottom hopefully that what it's
done is the whole big spray of Jason is
essentially where I'm receiving a
message off the wire I'm basically
debugging what I'm receiving and I'm
building a pipeline found a pipeline
basically for that particular command
and then essentially I'm logging that
I'm actually cool in the pipeline at a
certain point I'd annoys the master spy
driver and key so complete to the world
okay
I think what's the other thing in there
it might be interesting to show you
you'll find a little log history is this
yeah
when this I thought of a little water
before the talks but to avoid timing
issues when this service first started
up it basically got a broker unreachable
exception so one of the things is that
weird or actually compose I was starting
up a whole set of boxes together and by
the time the worker starts asking Rabbit
basically to make a connection rabbits
actually not ready it's not spun up
effectively on on its box so one of the
things you do have to cope with in these
kind of scenarios is you have to make
sure whatever you're using is resilient
to those kind of timing errors right
where you have to say well I'm trying to
watch out of the box and I can't right
now
so potentially my first option is to
retry and then things like circuit
breakers where effectivity you say I
can't talk to it for a while so brighter
which is the dotnet open source library
I work on one of the things here is
effectively it basically you've got an
exception talking to the broker it has a
retry policy and it will effectively
come back at some point and connect and
you can see down here whoops I've
scrolled too far if I can highlight it
this area down here you can see
essentially what saying when Isis has to
be talking to the broker on that signal
line so you have to be aware of these
kind of environments that you can't get
these out timing issues it could have to
go to cope with that you don't really
want to basically be forced to kind of
like start one box wait start on the box
wait all right essentially your script
may create those timing issues and you
want to basically move through that okay
in terms of some codes let's look at
this what may show you actually
somewhere here so this was all kind of
started up with a docker compose file so
that's essentially saying I've got a
number of docker files and I want you to
base run them all for simplicity I'm
actually showing you the app rather than
an image so this the two API which is
basically our first one it's essentially
starting up set some environment
variables just bear that mum come back
to that create the web box because it
depends on rabbit in it and there's
effectively also she's my scrolling
which is a bit my trackpad is a little
bit oversensitive for this font size two
hands okay there's a web and a worker
but to me this is a back-end process and
then this rabbit will come back and look
exactly kind of personalized akka follow
things a bit more or a second but things
to note so if we go and look at
basically to do APL I hear me - let me
try they have a open program okay you
can see here effectively this is
basically the eight web web api you can
see here effectively that we are using
kestrel we're not using our is
integration and we're just listening on
a port okay so the whole notion is
saying bind to ports and don't be inside
an application container that's what
we're doing here folks we're making sure
that works how that works this is pretty
standard in the sense you've got a
controller here listening to the message
and what we do is use a couple of
libraries that we work on internally and
they're open sourced out there so this
get one for example there's essentially
a sequence frame
brighter and darker darker is a top one
basically that's essentially doing the
query effectively on and get and we've
also affected down here for say an ad
here you got brights which essentially
is a command handler so I basically
issue a command and a handle or pick it
up you can always go to by the way types
or control J panel tab is available
online so you can go to the code and
present in your heart's content this is
essentially the command handler that
receives that command added to do I'm
just using entity framework here to
effectively do our work everything in
don't I don't know call and by the way
everything is basically the async right
you want to be async because essentially
it's got the live of engine a library
under needs and effectively your base
should be doing reactor patent basically
inventing for throughput alright what's
the other thing the other thing you may
want to see here is that effectively we
have essentially a nginx proxy config
and we use that in the docker compose
file basically to direct refer the work
the web the only other thing worth
pointing to here if I can scroll this in
a way that doesn't kill me during this
talk right so you'll see in here we
basically got to do API to do happen to
do core so to do core effectively is our
domain model our handlers commands that
kind of thing so these essentially host
those and to do app is our worker
runtime that reads from a queue and I
just use bright so I won't go into the
whole depth of what's going on but
essentially you can see here what we're
doing is simply saying we're going to
basically create a connection to a given
broker and we're going to essentially a
routing table and we still are
dispatcher which will receive messages
effectively from the broker and just a
console application so essentially we
just run a console application and we
use something like scheduler or it's the
container itself to keep it up and
running
I think that's what I wanted to talk
about for this one not just and for
another note here and the thing I didn't
talk about too quickly and that did it
yeah both the work of my person trying
to set their separate processes okay
yeah went to the slide deck and we're
doing the time maintenance right let's
keep moving
code base one code by traction revision
control many deploys essentially what
this is just saying really is that your
app should live in a wrapper in source
control one app is one Rako multiple
repos are distributed system and so what
they mean by this is effectively that
you shouldn't have a rep Oh which
contains your entire system that's kind
of pointless you want an app parappa I
think this has been kind of muted a
little bit by the notion of micro
services where effectively nowadays
we're talking about effectively business
capability and this makes sense
logically obviously Eddy team's business
capability would lean in their lead in
their own repository there is some
dispute when you talk about this one
over situation we have just shown you we
have effectively to do a PR in to do app
and they're both effectively in the same
codebase right and so is that a
violation and the guide pivots will say
would say yes essentially every process
should live in such a zone app I say no
to me effectively the solution boundary
is the CI boundary it said said she I
want to pull that and build it together
and that to me is the basically if it's
a CI boundaries around a context
boundary and therefore essentially it is
the whole of your your bit escape of
your micro service so if essentially you
can have multiple processes inside a
micro service well you did a harm called
autonomous components if you accept that
model and I think it's acceptable to
have essentially multiple processes with
inside the same codebase your app is a
micro service essentially under this
model okay so you're applying twelve
facts on top of micro services I think
in that case it's okay because you are
building it as one it is one CI boundary
as a correlation we'll talk about
basically management processes but
generally they are
instructions for fractured spaceship put
those with your rest of your code so I
think it makes sense in a qualifing
you're sharing a domain model so I would
say choose one rapo per micro surface
stroke app share Comanche rate
dependency manager not share to us and
wrap another words using you girl or use
pepper whatever you want to deploy
running instance app using the same red
boat so basically you can have one red
payment deploys okay look from me I
would say always ship trunk ok I would
have really avoid feature branching the
thing is essentially as soon as you gain
sort of future branches you're no longer
doing continuous integration people are
living now on long live branches and
you're no longer integrating with them
regularly you always cope with the
distributed version control system with
branches because essentially every
developer has a branch and the advice we
usually give people is integrate
multiple times during the day to avoid
issues with your colleagues basically
and having great merge problems so
generally speaking you really do want to
have this model where effectively said
we just have one code line everyone
integrates with it the only exception
would be if you do some kind of big
refactoring but usually the whole team
would then be effectively on that branch
you are simply saying I need to preserve
the integrity of my current code line
which represents watch a master because
what you I might need to do some kind of
set hotfix release this bear in mind is
predicated on the idea that we don't
want to much of a distance between
production and development and therefore
I want to release on a regular basis and
therefore the time window in which
basically my market might like my stream
effectively in development and
production are out is very short and you
can always tag what its current
production in case you need it
unlike client apps where both it was
some of these practices about feature
branches come from a branching really
supports this model and effectively
deploying out to multiple locations
outside your control where you may have
multiple versions your software running
any one time that's real branches for
it's not for saying I'm going to do my
development I said and I cited sandbox
for me and then the web environment of
services we usually say only have one
relational live any one given time and
use feature switches okay right I don't
this one will actually finish in the
timeframe so I mean just this little
thing here says to me we'll just do an
escape but says to me go into the
background and try pulling down the code
I was going to show you essentially
about if I could building it we may run
out of time simply on the basis I don't
know how how good is the network is
going to be for us how kind is rather
but let's try cloning features back
that's not bad
all right we'll try building it and so
we're going to do is a docker file in
there essentially only going to build it
and it's going to go away and build it
so I'm just put on a fresh version of my
app to try and demonstrate that we can
basically put down a fresh version build
it what I should do actually over here
though is I can actually get something I
can read that alone you know methods use
the font on this slightly gentlemen just
so I can read it
where's my cursor stop that one all
right and we will try building that one
when we get okay so we're not going to
what you build because it takes longer
than we probably got it hopefully our
build nicely in the background while
we're doing stuff okay
especially declare and isolate where
dependencies so what we don't want to do
base UV is have a model where
effectively we depend upon a shared
runtime installed on an OS which we
cohabit with other applications so we
need isolation effectively from any kind
of shared runtime we don't want to
basically have multiple apps installed
on the windows running from the gang the
reason is because essentially if we have
that shared dependency if someone else
needs it updated it could break our app
and there's a software erosion we're
talking about
earlier so we want to make sure that we
control within our Apple the dependences
so oh it is basically the language and
the runtime installed and all the
libraries we require to have our app
should be explicitly declared and we
will then pull them in so project Jason
is great for this CS barrage who knows
you can if you're in Python you can kind
of do pip
freeze to requirements tex and pip
install requirements types of sorts and
a patent for doing that and manage of
virtual mms in person as well containers
really help here because essentially you
can run in a container and be the only
person using practically the libraries
installed in that environment so we then
side of container you could get away
with using some like a GAC because
you're the only person actually
deploying insert into that container I
seen that's pretty obvious by the way
only one output container right and I
learn and effectively et cetera our
model should be kind of cloned down from
the repo restore our dependencies build
effectively us off run and then run it
okay yeah
the containerized environment basically
you can use essentially the tooling in
non containerized environments obviously
you may want to something like chef at
around sports basically put your machine
in the correct state right paw restore
and build let's see where we are on that
one actually may go on and come back to
it because it's pretty still building so
a complicated environment okay
configuration is what in this sense is
use particular phrase we mean anything
of varies between environments we do not
mean for example your routing table for
your web framework we do not mean
essentially the message view
subscriptions for your worker process
what we mean essentially is something
that varies between environments so for
example a connection strength to
database timeouts number processes
you're running perhaps the the elements
essentially which which do change for
each environment which you deploy to
sale events between production and
staging and upon passwords for example
and that's what configuration is right a
talkback trap says essentially only code
which is into your project
not configuration okay much as possible
try and put thing money things that you
may think of today as configuration
which don't vary between environments
put them in code but don't put anything
in spatial repository particularly bear
in mind for example that you don't want
to put anything into a public or a pose
that effectively contains things like
your AWS for example passwords and the
rule of some people have here is can i
if i could essentially if i checked in
put my repo live on github rather than
in some sort of private github
enterprise github would there be any
secrets in that config file that i
wouldn't want anyone to see if there are
they shouldn't be in your app oh they
should be somewhere else right and
generally the technology basically
people tend to use his historian
environment variables very easy with
containers you saw actually when I took
the book of docker compose file I was
basically saying up the environment
variable it's that becomes container and
I was running it very easy to vary that
between environments basically when you
when it obscene deployment in certain
environments for example a server less
effective ironmen variables probably
about the only thing you have access to
you you don't really have computers and
disks that you can use to control
basically a processing so you have to
use environment variable model but this
what vector app model says we're trying
to make a self sequel for those alarm so
you should use conflict well you don't
want to have is some hideous build
script that goes away and says I've got
a templated kung fu fight which I merge
with secret hidden lists of values for
given environment so I produced it
because your release artifacts should be
immutable once I've built it
that's what ships and the only thing I
need to vary for environment basically
is environment variables right and you
can locally you can either put things in
your IDE you will support your setting
environment variables or you can
essentially use something like basically
a PowerShell script set to set your
environment or a bash script okay so
you're on your app.net has a quite nice
feature in that if you're using its
built on it cords configure class you
can actually order the wear it looks for
places so you could for local
environments if you wanted to use a
local confit with an environment
variable is basically override on it so
you could say well actually my
developers when they put it down
have to set environment variables so I'm
going to do is put in my local conflict
those values they can put down and get
running with whatever I Asuma my
defaults for stuff installed in a local
machine and then if that works then
essentially whenever choice points
production environments it will always
assume the environment variables and
place those local values you could do
that but generally speaking I think it's
quite a lot nowadays easier quite often
get your developers running by using
like a docker file that has the
environment variables in it forward okay
dobe relation runs strictly separate
buildin release in run stages this is
fairly straightforward essentially the
idea effectively is that I want to
release immutable artifacts I don't want
there to be changes to code at runtime
so realistically what I want to do is
only release publish artifacts that
essentially you can you then effectively
can run but you cannot basically subvert
my process by then checking by changing
the code alive then check you back into
my source code repository right I
understand in the twenty twenty
seventeen this may seem strange but when
the Heroku guys wrote this you know they
were writing it for an audience of PHP
hackers he would quite often Ord modify
their code live on the server right
we're always in a now I'm sure so
releases accommodation in building comfy
runs agencies happen to give an
environment right okay they're prod
parity heat development and production
as soon as possible
so things were worried about time gap
all right so the tightest we don't
always think about some of these other
factors a time we get what we mean by
that we mean that we're concerned is
essentially that effective we have
inventory waste we have things that have
not been deployed production because our
developers haven't got a too far our
line because we have our to longer a
deployment process we build stuff and
then it's like six weeks before it gets
deployed that's not agile because you're
not getting fast feedback so we should
have a low time gap development and
production should look very similar
because the time gap is low personnel
gap we don't want a factory to separate
between operations team members
effectively and development team members
you don't want basically a lack of
knowledge about our production
environments post to our development
environment and a tools gap we don't
want to use different
things in different environments so it
might seem straightforward for example
using something like brighter
effectively to use rabbitmq locally then
deploy to Amazon using sqs and SNS the
problem with that is they have different
characteristics and you're going to be
different problems in those environments
so you want to try as much as possible
deploy to what the environment looks
like and things like docker really help
you with this because they let you
essentially create something that looks
like your production environment managed
logs pretty loaded event streams so this
one is quite straightforward basically
the idea is a lot to stand about okay so
I'll just hand it out then or in AWS
land of example you can use print
statements generally speaking what you
want to do is have a stream that your
app produces and then you use something
like elf to manage basically your logs
particularly in micro service
environments you now have many apps if
you log to the local file system or a
number of problems one is when a how a
local file system number two it could be
ephemeral particularly for example in a
servlet environment and it's going to go
away between invokes a basic your
container and even in the docker
environment effectively your containers
will vanish when you finish running them
so any logs you've logged locally will
be lost
you could try login to a shared
directory but you might as well be
smarter and actually just log to
standard out and get something like L to
pick it up actively and call your logs
for you where you can search them deal
with them and then you think about
correlation IDs to map between write
because you now have logs from many
sources and so you don't to log all
those sources into different shared
folders you want to collect them in one
database okay I can see if we can show
you that other one working and then
we're almost done so I'm probably over
running slightly I am alright let me
just show you this
I'm not in the right directory that
won't work ok we'll skip showing you
that we showed you the code running
earlier I had to basically shown to the
right director I forgot to do that and
do a build and it takes quite a while
basically over this met city network
connection but you can download it
yourself after the talk and go and did
run the docker compose and build it and
you should see it just build straight
out of whack and you can run it which is
kind of the goal right actually and it's
about actually a better goal if you did
at our site and here ok
last pieces I'm sort of progress for
running dependence especially clip we've
done that one let me come back there's
probably another property II managed a
pin ok admin process is essentially the
teachers just saying what's we may have
little processes like migration scripts
or tooling to the operation of support
was done and those processes effectively
when we use them should be located along
with our app in the same repo otherwise
what happens is developers forget about
and they make changes to scheme or and
that kind of thing except when this gets
updated tools that work with it so
always collect your tools gem raids you
want to use come on on infrastructure to
do it and if you work in exact donate if
you think about dotnet CLI effectively
to basically integrated with that ok
summary so top actors one codebase
tracks and revision control main deploys
ok especially declare an isolate your
dependencies still comping in your
environment treat backing services
attach resources shortly separate build
and run stages and execute the app is
one of more fitness processes ok that's
what services via port bindings that's
we talked about patient
avoid application service containers
skeletal approaches model a horizontal
scaling fast startup and graceful
shutdown in other words be prepared for
basically elastic scaling a key
development stage in production as soon
as possible don't forget about tool
that's also about time and personnel
treat logs event streams just having the
Belk and admin management tools as
one-off processes effectively makes sure
that you're supporting code is kept
their main code all right now so much I
apologize for over running please do it
will be interesting exercise
me download the future state code right
and then try and run it locally if it
doesn't work obviously to fix things but
the goal should be should I just run
that run is off your compose file and
get it up and running why do you have
dr. Roberts running your machine thanks
very much please vote especially if it's
green because I know that you love me if
you give me yellow they will wonder what
happened and if you give me red that's
ok thanks very much and I will about the
conference you want to talk to me come
and grab me over a coffee or food I'm
happy to chat about stuff thanks so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>