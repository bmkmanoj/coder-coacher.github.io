<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deploying Applications to Windows Containers and Windows Server 2016 - Ben Hall | Coder Coacher - Coaching Coders</title><meta content="Deploying Applications to Windows Containers and Windows Server 2016 - Ben Hall - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deploying Applications to Windows Containers and Windows Server 2016 - Ben Hall</b></h2><h5 class="post__date">2017-04-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sabcJ1u1e-M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hence the Phoenix and the reason I think
that is really important is because the
limits community for the last couple of
had all the fun that had all of the
great automation I've had all of the
great tooling and in the windows and
when deploying Windows applications
especially legacy Windows and Dutton
applications it's just a little bit more
difficult it's more time-consuming it's
more problematic to automate and it's
just more difficult
that is until Windows Server 2016 has
come along and I've added some new
functionality divided container support
they've got docker integration and
that's made everything a lot simpler a
lot easier to automate even more
traditional old-school Windows
applications instead of your nice shiny
dotnet cause of the world so my name is
Ben hall I am one of the organizers of
docker Linda meet ups I'm also a
Microsoft MVP in cloud and big center
management and more importantly I'm the
founder of Ocelot war we do a product
called cut coder alongside many other
things like teaching people docker and
helping them integrate containers but
cut coder is a free interactive learning
platform for software developers it
gives you a step-by-step tutorial about
how you can learn things like kubernetes
docker and alongside that a interactive
browser-based environment tell you that
you can play with everything and have
everything set up without needing to
download to install anything so what I'm
going to talk about today well I'm going
to talk about how we can start
automating deployment of applications on
Windows Server 2016 what that entails
and what's actually happening in the DES
covers so I'm going to first give you an
introduction into containers and give
you some context into what we're talking
about and then can move on to how we
deploy Windows applications and windows
and applications inside of these
containers discuss briefly the
differences and why window the Linux and
Dockers slightly different and what's
happening in the DES covers something
called hyper-v containers and then look
towards scaling and towards the future
and how Diggle will actually work in
reality and in production so containers
what are containers well if we think of
the real world and we think of how the
real world operates in terms of
containers there's a huge big mess
metal shipping containers about this
wide of their about this high terms of
scaling and they have certain key
properties like for example they have
doors in a certain location and they are
recreated inside they've got certain
label within certain places so they
could they can be scanned and more
importantly they've got four hooks in
each of the corners if these hooks that
allow them to be connected and allowed
them to create this very strong coherent
unit that you can put on big massive
shipping containers and send all the way
around a world without them falling in
the sea most of the time but what's on
the inside of these doesn't really
matter that much and like you don't
really care and it doesn't really matter
from the shipping company's point of
view it could be card it could be
t-shirts it could be part built products
what matters is that consistency from
the outside is that consistently that
they all look the same the auditing size
they can all be connected to in all of
the same way and it is consistency which
makes its scale and makes it work and
that's kind of what we're getting to
with in terms of software deployments
and software containers what's winning
on the inside of a container doesn't
really matter that much it could be a
dotnet core application it could be a
vb6 application it could be sequel
server what that doesn't matter what's
important is a consistency about how we
start these focuses how we build these
containers and how they work at scale
and that's what we're aiming for and
that's what we're heading for if you
think about the two types of
virtualization containers take a type of
virtualization traditionally we're used
to the hyper-v hypervisor based
virtualization for your ESX your then
your hyper-v and while this has been
very successful it also comes with a
huge amounts of overhead and kind of
like a heaviness which comes along with
it and that's because in order to
provide security and sandbox in and
abstractions your host operating system
needs to have a hypervisor which then
boots under the guest operating system
and so in order to where to get your
flexibility
you had butl this guest operating system
with the
in on what you was winning give it
windows it could take ten minutes to
boot up in order to start your
application and a thirst book based and
also to keep your guest operating-system
happy you had to pre-allocate for a 16
gigabytes of ram 40 gig of this space
and it was just a huge amount of
overhead and with slowed everything down
and made it difficult to scale and made
it difficult to be more dynamic and
hence we ended up with long winning
virtual machines which lasted forever
because they're too difficult to move
around and move between different
services and kind of scaled more
dynamically and so this is where the
container mindset has come in I try to
make everything a lot more simpler more
lightweight and a lot more flexible in
terms of scaling and migrating workloads
and so instead of having a hypervisor a
hyper-v we've now got a docker engine or
a container engine and the container
engine contains and offers kernel
virtualization so instead of
virtualizing the entire guest operating
system with virtualizing the kernel and
so this allows us to take advantage of
the host operating system kernel but
still deliver sandboxing still delivered
security still deliver restrictions that
what we used to from the security point
of view with virtual machines in terms
of the container itself is that
everything your application needs to
start so if you're deploying a dotnet
application the container will contain
the dotnet framework it would have
always of your built binaries that have
have your all of your application
configuration and everything in a
container it's separate from everything
else so while one container could be
running no GS and another could be
running no J 13:7 another could be
reading net and all sandbox they're all
isolated at all separate but containers
are not virtual machines they have key
important difference and that is the
overhead and that's the allocation
because you don't have this guest
operating system and which requires
booting up you don't have to wait 10
minutes for your projects to start
anymore it starts instantly because of
this I like to think darker add more
like a process manager more than
anything fundamentally all you're doing
is starting processes you're just
starting you're done application
and darker providing that security that
packaging that consistency that we want
from our viewpoint and duckin is the
tooling duck with something which had
made all of this possible and made all
of this ecosystem available to us
containers have been around in many
different forms for the last 20 years
they originated as a concept in freebsd
where the creator needed to be able to
run freebsd alongside developing future
versions of it they wanted to do that in
a very streamlined very fast very
appropriate way and then they kind of
iterated multiple times they appeared as
jails and zones and Solaris and then
Linux came along with Alexi and then
Google and enhanced that with three
groups and namespaces to provide more
security and more restrictions are more
control over how these processes are
running and while it was all possible
and it all down the Linux kernel you
kind of had to be really really
passionate in order to be able to take
advantage of it but you it would not the
easiest thing in the world in order to
be able to deal with on a daily basis
shamefully Tucker has now made that
simple document provide a set of tooling
and a set of API s so that we can work
with containers on a daily basis and we
can build our entire development
lifecycle around them but doc is also
realistic while it's a huge company and
they're doing many fantastic things they
can't solve every single problem until
they've got the mindset our batteries
included but removable out of the box
they want you to have a great experience
they want you to have all of the tooling
that all of the functionality there to
get you and get you into production and
help you solve real problems but if
you're talking to some like really
expensive storage array or got weird
interesting networking problems that
realistic that maybe that's not the most
appropriate problem for them to solve
instead they rely on the ecosystem and
the ecosystem which is building some
amazing companies we've solved the
really interesting problems of dealing
with networking at scale or dealing with
the interesting storage devices from the
EMC s of the world for example
and the dark I've made it open made
accessible I made their entire platform
pluggable so that these thing can be
easily swapped in and swapped out as you
grow in terms of complexity within your
system and for me in terms of what
containers and darker and what's
important there's three key concepts
firstly we have these docker containers
and these are actually the winning
processes which are FEM boxed and secure
and I've got all the isolations which we
demand from our system these winning
posters
start life as a docker image and
fundamentally you can kind of think of
this as a layered lip file we'll get
more into this in a moment but it has
got all of your configuration it's all
of the files they've got all of your
dependencies and instead of it just
being one huge zip file with everything
required it's been broken down into
layers if these layers allow us to reuse
and reuse lies different files between
different projects and so you don't have
to keep managing and installing the same
dotnet version for every single
application we deploy instead that we
can repurpose Donette dotnet
installation so it gets more
space-saving it's quicker to deploy
quicker to manage and all of these
images live inside the dock of registry
so those familiar with the linux world
then put a public documentary it's a
docker hub it's where all of the images
live is where we can go to find discover
and download applications which have
been pre-built pre-configured and we can
also deploy and host our own docker
registry internally and on our private
network many cloud providers for tillage
or have data the service and this is
where we keep our applications and keep
the images which we're building and this
for that stuck at and in terms of
windows containers and the work which my
crops are doing most of the work in with
docker to make it part of the same
application it's not a fork is not a
different distribution is part of the
main darker release and we can see this
by
looking at the docker source code and
because everything is open source which
is amazing and you can kind of feed on
the interesting aspects like we've now
got windows directory and they're the
particular it's taken advantage of
golang and so we can have targeted
builds for windows for linux for OSX and
also we see some interesting things
coming from Microsoft in terms of how
they've changed and how they're dealing
with the container ecosystem that we've
got a Windows i/o library publicly
available and github whittling go and
we've got Microsoft hates CS shim which
is a host container service and it's an
abstraction of the API to allow that
docker and a Windows kernel to talk to
each other and so it's all open it all
in the public and so we can currently
start to see Microsoft's intention and
workflow in that their internal process
in terms of how they're thinking about
containers internally which i think is
really cool so what is actually coming
in Windows 2016 and what's actually
coming along to make all of these
containers possible and make life easier
for ourselves so there's four new key
components which are coming Windows
server core and Windows Nano these are
operating systems designed to run inside
containers and to design to give us the
tooling which we need in order for our
pulses to run and but also support all
of the traditional api which they would
expect and there's two new ways of
winning that windows containers and
there's Windows hyper-v containers and
I'll get into the differences in a
moment and just like our friends in a
Linux world the way Windows containers
work and feel and operate are very very
similar almost identical in many ways so
you've got your guest
you've got your host operating system
Windows Server 2016
that could be running on bare metal it
could be running in - or it could be
running wherever we desire on top of
that we've got the Windows kernel that
the thing which is now being virtualized
and broken up and got all of the new
hooks in there so that we can bring need
containerized processes we've got the
docker
engine which is the build for Windows so
this is again got all the functionality
which would expect some darker it's a
complete support
release and then on top of that we can
run our processes of individual
containers or separated or Sam boxed or
isolated without them interfering with
each other in terms of Windows hyper-v
containers they offer an additional
level of security an additional level of
protection so instead of this sharing
the same host kernel from our
applications when those hyper-v
containers allow us to wrap that
container inside a very small very
lightweight virtual machine and this
allows us to have a separate kernel and
it allows an additional layer of
security if our particular deployment
requirements desire it and thinking
about when you would use one of the
other ones obviously Windows containers
are everything which you need either
secure the scaleable that are highly
automated there all of the things which
we kind of desire from a deployment
technology Windows hyper-v containers
are useful in two
and potentially three circumstances one
is you're a platform of the service
someone like Heroku or maybe a jaw and
your hosting third-party software which
may be not as twisted and it would be if
it's running internally so this may be
maliciously trying to exploit it in some
interesting and wonderful ways the other
side is multi-tenancy where again you're
winning on you've got multiple vendors
multiple partners which you're hosting
and it could be very awkward if some how
they did manage to break out of a
container and then manage to break into
another competitors container running on
the same machine and that could just be
a little bit embarrassing for everyone
involved and so this is where we know
type of V containers come in is where
you need an additional level of security
additional level of auditing the
additional support just in case a day or
day attack did come out and I contain a
breakout would be possible but for most
of the time and most applications
Windows Server containers will be
perfect and they'll get you at where we
want to be
so now we've got windows containers and
windows hard-to-see containers they
allow us to run processes and they are
able to start our applications so now we
need something to actually run inside of
our container so the containers got
everything which the application needs
in order to be able to start so we've
got your application itself it's got the
dotnet framework it's got the JVM if
you're running Java but alongside that
it also needs an operating system
because it expects your operating system
to be there the only things that docker
and kernel virtualization is offering
and providing the case container is the
kernel it didn't interact with the host
operating system and the files on a host
operating system in any way instead it's
completely sandbox completely abstracted
and completely secure because the worst
thing is you're winning a complaint
container which accidentally deletes all
of it files which happens in an
automated test build scenario sometimes
and then that destroys your host which
has happened many time when you're in in
automated scripts if containers shared
the host operating system in any way
that would kind of destroy your host and
that would then lead to other containers
winning which wouldn't be ideal in order
to live that sandbox in and that
isolation the containers need to be able
to add have the upgrade system running
inside of them or later j'en of the
operating system and that's version to
start with it windows server core it's
nearly Windows 32 compatible which means
it's got all of the legacy API it's got
everything which you your application
would expect apart from a few
interesting things like a fax server
which definitely doesn't need to be
inside of the container doesn't really
need to be inside windows . at the
moment but the idea being if it provides
all the same behavior it provides all
the same tooling it's designed for you
to be able to pick up an existing
application and type in a container and
work and that's the ambition of what
we're driving for and then we got
Windows Nano because yes where we need
to be able to support more traditional
applications and traditional ap is not
all applications need them especially as
we're looking forward
woods done that call and moving towards
a more cooked down lightweight
development process we need an operating
system to match that and that's why
Windows Nano comes in everything has
been really stripped down it's going
down to like the core the key components
of what makes up Windows ie
networking storage drivers the.net the
core CLR framework itself and as we
thought it's a 24 the size it's more -
more lightweight it's much more easy to
be deployed on but as we thought it
didn't support every scenario every
application and so for me the - way to
think about it if you're familiar with
Linux like Windows server call Eden
bootie like it's a traditional operating
system which had been around for last
ten years and you'll get most
applications deployed onto it without
any modifications required where Windows
Nano is like the alpine Linux it's like
small cut down really shiny really new
really fresh but you may have to make
some modifications to your application
in order to be able to take advantage of
it and so this is where I see the two
operating systems coming in and the two
requirements and why they exist
side-by-side so how do you install these
and how do we get started and how do we
actually start playing with them so if
you're willing Server 2016 you can add a
new module you can install the darker
package restarting machine and budget
you have everything which you need in
order to be able to start deploying
Windows containers if you're willing on
a jewel there's Windows Server 2016 with
container support this contains
everything set up everything which you
quiet in order to be able to get started
playing around deploying and building
buildings windows containers and so once
you've got that server in place you've
got duck a client which is your command
line interface and this can talk to you
window server or it can to Linux servers
and so you can work on that from your
guest operating system and talking to as
you're talking to Linux everything being
it's like really lovely cross-platform
ecosystem form your single machine and
then obviously the
client will talk to the doc engine on
one of those two that will in turn talk
to the colonel and that will in turn
know how to start processes as he
containerized ecosystems and alongside
that we've got the layered VIP foil as
I'm going to refer to it today at the
docker image and so this will come from
the registry which has got the all of
the support and all of the images which
we need and we've got that as we extend
and as we build our container image and
add our own applications and add our own
customizations to it that gets added as
an additional layer on top and as such
eventually when that starts from a
containers point of view it looks and
feels like an entire Windows virtual
machine but in reality it's running
inside of a container on a slightly
separate but similar completely
unrelated note in Windows 10 we've got
Windows Linux subsystem the Windows
Linux subsystem allowed this to rim bash
and allow this to win Linux processes on
top of Windows this is completely
separate to the work that windows are
doing in terms of container support but
it's important to identify the
differences on the reasons why the Linux
subsystem is taking Linux processes
emulating them train changing we
implementing the system cause and
recover and translating them to Windows
system cause and so there's this layer
and abstraction in the middle in terms
of what container support are offering
containers are native they allow you to
run Windows applications natively on top
of Windows with proper hooks in eternal
but what that means is that we can't
pick up an existing docker Linux based
image and expect it to work on top of a
Windows Server 2016 machine because the
linux image will expect a linux kernel
to be there and i will expect the linux
hooks in the kernel unordered kind of
expected thing
just in the same way that we can't pick
up a Windows docket image and expect it
to win on Linux because we need a
Windows kernel being there in the first
place and so that's why the
virtualization and the kernel virtual h8
kernel virtualization comes into play
but in the future like this is a really
cool technology and it will be
interesting to see where Microsoft goals
in terms of where that's heading but
it's not it's important not to confuse
the two so keep talking about these
docker images and these windows docker
images but like what are they I let's
start building and let's start playing
so when you have everything set up
you've got your your image or you've
installed Windows Server 2016 everything
fresh and everything's clean and
everything's working so the first thing
you can do is like type in docker images
and this will provide you with the two
container based operating systems the
Windows server core which is that
slightly larger installation slightly
large docker image designed for most
applications and then nano server which
is a cookie on lightweight version of it
and then you will also see the
particular version number as a tag so
this is the Windows version this is the
Windows build that we can use in order
to track what's been updated what
particular version of security fixes
were apps etc in order to take this
these images and turn them into a
container we use docker rim and it
allows us to start processes using that
image as our base as our foundation with
everything set up and everything
configured so if we do govern IT kind of
makes it interactive and attach the
terminal that allows it inputs and input
to it Windows server core which is up
for operating system versions a
traditional one let's say when a command
prompt and in the nice lovely blue we've
got my parish l on my host which
launched a container and in the black
we've got the lovely traditional windows
command prompt which has got everything
you would expect this is running inside
of
Tina it's got different host name to my
host operating system it got similar
looking files i'ii iearn april program
files uses windows directory but it's
completely isolated and completely
separate to the files which are my host
operating system so if I decided to like
format the windows container that would
be fine
obviously the container would be not
very useful at the end of it but it
wouldn't have any devastating problems
to the rest of my application which
written cat coda I'm fully familiar with
how everyone tries to delete the
container if you look at what's running
inside the container and there's
something called SSMS and which kind of
if you are familiar with how Linux
container works in the Linux will do you
say what's winning inside of my
container and then you'll generally
you'll only have bash and you'll have PS
which is listing all the things winning
windows because of the historical nature
of Windows applications expect other
things to be there to support the
winning of the application itself until
you'll expect and see things like SVC
host for example and because it's
Windows there'll be multiple different
things of these working and to support
and to make sure is happy and there'll
only be things like twisting installer I
PowerShell which is what we running but
when you see it these are all positives
running inside of the container again
they're not shared not interacting with
other things happening on the system
they're just there in order to support
what Windows needs in order to be able
to win effectively so we've got
something running we've got a command
prompt which is great but not massively
useful so let's start building something
and like let's be realistic if you're
the 90% of applications I imagine which
will take an advantage of windows
containers will be something which has
been historically deployed onto a system
and wants to be modernized but still
needs to be maintained and it still
needs to be supported in some way and
the tenses of rewriting that inside
dotnet core it's probably not going to
happen this year but you still want to
be able to take advantage
of the container support and all of the
knives tooling that comes with being
able to deploy more effectively
so I tried to find the oldest
application I could and I did it was the
nerd dinner 2.0 application lovely build
and released in May 2010 which I kind of
thought like that's not that long ago
and I was I actually that is quite a
good long time ago we've got the author
still actually works at Microsoft on the
team what to think is even more
impressive not due to a coding habit at
all obviously and obviously this is an
application and it's been like seven
years six years since I last looked at
its application so welcoming number how
any of it works at all let alone how to
get it running but thankfully John kind
of thought about this it's got all of
the binaries which have been built at
part of the package and that no package
managers involves so everything the
application needs is in that VIP file so
this made my life a lot simpler to start
with but for mentally I don't know how
it works so let's turn it into a Windows
container and let's see if we can go
back to those glorious days and have a
nerd dinner again so the first thing
which we need is of the Windows Server
2016 so I'm going to use a jewel because
it's not that expensive and it's a lot
quicker than getting everything set up
on my macbook air especially for
prototyping and seeing how everything
works until once it gets to that talk
you obviously have this great configured
server because I'm running everything or
I will be running as everything as
containers I don't need to do this the
whole benefit what we find with
containers is my host operating system
doesn't need to do anything anymore
everything which I've got some
complexity some configuration required
is doing inside of the container so all
of my hosts does is really bring docker
and win containers and that's it and
because I picked that image that data
center image with container support
already everything is now in the only
thing which I did add was
chocolatey now if you're not familiar
with chocolate please this was like a
revelation to me when I came across it
because instead of having to load up ie
and get around like security barriers to
downloading that executable you don't
need to do that anymore
which I only just really like this hence
why I'm quite excited still about it if
it's all if everyone else knew about it
then I'll get thrown off but you can use
things like install git from the command
line again and you can install docker
from the command line and the Ducker
client and it's awesome and it's ah
good great so I use this to install at
all so I could at least have an IDE on
my box to actually have something
working and it was basically a taco
install at all and I had everything set
up working in a lovely nice way so now
we've got it - now let's start working
with docker so we've got darker version
so the windows build the windows build
of our container engine which is winning
you do docker PS that lists all of the
winning processes all of the winning
containers at the moment knitting is
happening but we list docker images and
we'll see these nice windows server core
latest and Windows Nano latest one
important thing to be number here is
that latest isn't actually latest latest
I consider a little bit of a anti
pattern because latest is a moving
target it's a tag which print to the
latest release at a certain point of
time but if you're building on latest
that means that it also changes as other
things that the packages iterate and so
what could be Windows 10 one day could
be Windows 11 the next if windows 11
ever comes out I don't know how it works
anymore but you get the idea and so
you've got this constant moving target
and so you could accidentally introduced
breaking changes because the latest had
has been moved upstream so instead if
you look on docker hub and you look up
all the images there's actually the
proper build numbers and propositioning
and proper internationalization versions
of each of the containers and so it
emits more accurate to pull down the
latest
build when you get started which was
released eight days ago because this way
you can be confident got order latest
security patches and all the latest
updates if you just start using the
latest image which comes with it you may
be slightly behind the curve and you may
be slightly and building upon an older
version so to download the latest
version you do docker pull windows have
called you give the build number as a
tag I'm using that colon colon syntax
and that will pull down the latest
versions now notice that while my
windows have a call base image is nine
gigabytes which yes it's quite big but
at the same time it's also like the
whole windows ecosystem or whole windows
installation the things which will
download is only the additional layers
which you don't already have so it'll
only download the security patches and
the security fixes which has been added
upon that base build and so in this case
we're only down in thin 900 Meg but it's
still not like insignificant but it's a
much better than downloading ten
gigabytes every single time instead
you're only downloading the changes and
that doubter from what happened
beforehand so now we need to build our
image and this is where our docker file
comes in a docker file is a list of
instructions with define how your
application can be built and an and
deployed and running in an automated way
and these instructions just define steps
that the poster should win to in order
to take your nice clean fresh window
cervical container image and then have
your application on top so this is what
we need in order to be able to deploy
near dinner at a Windows container so to
start with we need to have iOS that
seemed like a sensible point where it's
a full-blown dotnet MVC application and
we see - I believe at that time I think
it would WT - so let's start so we just
define our base image so this defines
all the dependencies which are potential
need in order to be able to
start your basically picking and we
expecting Windows server core or Windows
Nano that's kind of load to bigger
changes to biggest decisions at this
point in time and then we can burn
additional instructions in order to be
able to add and extend the base image in
this case I'm winning PowerShell I'm
telling it to install a window feature
web server which is a is so we've got it
for doc file we can do docker build give
it a nice friendly name as is and tell
it to build and produce our docker image
we're doing this so that it'll
applications would come along can reuse
this iis installation and so we don't
have to keep installing Windows Server
every single time we want to deploy it's
kind of been cached for future versions
but thankfully Microsoft have also kind
of thought about this being a common
scenario in a common use case and so
they've already doing this for us and so
they've already gone ahead and had build
that docker image so that we don't have
to worry about how to take and how to
have an image winning guy is so in the
hope docker hub which is a docker
registry which is where all of the
docker images live you go to the
Microsoft page or the micro profile
you've got the is page and it tells you
the instructions and it tells you the
commands to start and it tells you all
its different versions so that the
version for nano server and added
version for Windows server core
depending on what you're targeting at
the time so instead of building our
image we can target that image instead
and so now our from our application
viewpoint it's basically saying I don't
I need is installed in order for me to
be able to deploy this application
successfully so just make sure that I is
is they're running Windows server core
configured and make sure it's at the
latest version build or make sure it's
against if build version a 14 393 and
then just to make sure that everything's
happy define the shell are you what to
use when you're running feature commands
at PowerShell
this allows us to take advantage of all
of the PowerShell functionality and the
PowerShell scripting approach which we
need so now is the eye is installed we
need to start customizing the face image
depending on what my application
requires or in this case John's
application so the first thing we'll
need is done at framework and asp.net
for five so just like you have with eye
is the thing which you do using
traditional management console is go
through tips boxes in order to pay up to
have that functionally built-in and so
now we're automating it everything is
scriptable everything is repeatable so
we're using a Windows install features
in order to be able to have that and so
when we do a build what we'll do now or
what dr. will do now is right okay I've
got iis and at the end of this we'll say
like right there is with Windows with
asp.net configured and installed and
capable of winning applications which is
cool so now the next stage is we've got
a is set org we need to configure it and
so we do this with this using the
powershell commandlets
why i use the right word with PowerShell
points excellent so the first thing to
do is like remove the default website
because it's just clutter and we just
don't need it hanging around second
thing is we need an up we need a
directory where our application source
code will live so we use mate directory
and no dinner and then we create a new
website new iOS web site give it a nice
friendly name say that it will be
running on port 80 the physical path
will be no dinner and the application
pool will be dotnet 4 5 and so this
should like if you've done any iOS
automation previously with Windows 2013
or something this should start feeling
familiar it's kind of all were doing is
customizing exactly the same way that we
would with a traditional quote we've now
just defined out at the daka file and
then finally to give some syntax and
some heads-up about what ports are
exposed and reports processes winning
with expose 80 and then finally we copy
the directory from our source code
and copy from our host we copy the
source code from our host which is in a
directory called nerd inner into our
container and say play at C : no dinner
and so this is our like structions about
how we need to deploy our application
and what needs to happen in order to be
able to get it from a clean blank is
installation up to have anything running
and everything configured and all the
source code in all of the right places
so now we've got this we do use the
docker build command so this duck a
build command will take that docker file
execute each of the instructions and at
the end of it we get this lovely shiny
docker image which should be capable of
winning our application and so when you
run through it you'll see all of these
instructions being made you'll see all
of the output being created and at the
end we have this bucket image the docker
image just like we did with Windows
Server court
kelan's our command-line application can
be ruined with duh Corinne we say we
want to win it in the background because
it's a web service so like we want to
win in the background we say we're on
port 80 because it's web application so
we need to make port 80 available by
default everything nice and boxed and
everything is closed off and so we need
to be able to configure the right port
and the right network configuration in
order to be able to access those
processes again it's kind of we haven't
if secure by default mentality and
that's what we're going with until we
have to explicitly say what port back
mappings we need to make available to
the public which hopefully will mitigate
if you win in MongoDB or elasticsearch
inside of a container so when this I
give it the name of the docker image
which in this case I gave with the tag
of nude dinner and so that's why we say
I'm winning and then I really felt like
hood back in the dotnet world and
because it didn't win this would because
I had forgot to update my connection
strings and terms like ah okay awesome
duplications like ah fine
so when I need to make changes to my
application what do I do so we've now
got our contain a bill and container
winning and so when we want to make
change
when we want to release new versions we
would we make the changes and rebuild
the image and this is where the ordering
and the approach what we've taken in
terms of how we structured our
dockerfile
is important so instead of it going
through and reinstalling iis and
reinstalling done a framework and
recreate in the website because of how
doug has been created it got some
built-in caching and it got some
built-in mechanisms that know if
nothing's changed in the layers in the
instructions above don't just repurpose
and reuse the existing image which came
before it only create a new image when
you drin
when he's copied in files which have
changed because if something's changed
on the outside into the container then
the likelihood we need to win through
two commands in this case this is why
we're seeing this using cash using cash
so everything went really quickly up
until we do the copy and then that kind
of removes that previous container and
we get that new shiny image this is now
being fixed it now fixed a bug which was
due to connection strings problem and we
have a lovely shiny new dinner
running inside of our container again
now did he winning at a Windows
container because not that I don't trust
John but it is a six year old
application may have some security
problems you never know so if you wanted
to add a little bit more security and
win it as a hyper-v container we just
assigned the isolation mode and so now
when we start that container what doc
will do is when it's talking to the
Windows kernel instead of going like I
will just start a normal Windows
container it will take the Windows
hyper-v virtual machine that really
lightweight small sink that has already
been kind of booted up and frozen in
memory in terms of like really cool lot
to my patients at the hyper-v team - and
so then it will take that fork it will
in free the deployer container onto
there and start it inside of that
virtual machine so we get that separate
kernel we get that added layer of
security and
we can win nerd dinner and I like a
really highly secure highly safe
environment and this all happens in
milliseconds like there is no there's a
hundred millisecond brutal time when
you're at in isolation but this is not
like traditional Bruton a traditional
virtual machine it's very far so
lightweight still with very minimum
overhead the only thing that virtual
machine is doing it providing a
different Windows kernel a different
Windows kernel and if anything did break
out it's not writing it to disk
everything stored in memory when that
container gets closed and container gets
stopped that window VM was wrapping
everything in gets thrown away missing
it persisted to disk again to provide
that Moore's contain contain security
lab and so now we've got our application
built and we fixed it and graduation
working we went through it a few times
and it's immutable right from now on
that images what will get promoted from
our development environment to let's say
our continuous integration environment
to our staging server to production any
configuration tweaks which we need will
confine that environment variables or an
outside-in fillets but the image itself
doesn't change and this makes it
interesting when you talk about Windows
updates because like well traditionally
when I you start a Windows VM we know
Dog Day will happily keep myself secure
and I would have to keep ourselves
updated so how does that work in a
container operating way okay if it's a
Folden instance and so Windows updates
are disabled again we don't what we
build should be for them we should be
able to repeat that build and have
exactly the same response and so if
we've got Windows updates winning and
deploying our own packages that kind of
breaks that mentality in that breaks out
approach so this is where it's important
to know the windows version number and
how that relate to what you're building
at your base image so with our nurdinov
application we use the latest which was
14 393 and so um Microsoft website
they've got a hit history and a list of
all of the different windows version
numbers all of their secure IDs
everything what you need
in order to know what your application
was built upon and what version your
application was using when news isn't
updated and you have like the Patch
Tuesday traditional releases you'll now
have a new Windows server called base
image and it'll have a new version
number with a new tag and so the idea
being is that you would update your
docker file you would increment that
version number to be the latest you'd
rebuild your entire application which
would take into those new fixes take
into account I'd need security features
and then you would push that back
through and redeploy that back for your
pipeline it's all about repeatability is
all about that consistent way of
building applications and making sure
that what we've built is exactly what's
running in production and across servers
without having these no flake instances
happening where everything is slightly
different because things slightly
changed at different points in time and
I think they rolled out and so that's
how we get to that point with Windows
containers in terms some operational
important details right we've got our
containers that we need to be able to
persist some data sometimes and if we
all we did was save data inside the
container when we upgraded it and when
we deploy that new version the data
would have been in the container which
we stopped and removed and so we'd lose
all of that data too which is very
embarrassing twist me from experience
it's very embarrassing when that happens
so instead you can mount volumes and you
can mount drives from the hosts and make
them available inside of the container
and these are fundamentally thin links
what we're doing is creating a symlink
from let's say C colon destination - o
in our container
- C : source inside of our host and so
that's how we can anything wrote to the
destination directory in the container
actually ends up on our source directory
and so now if we redeployed Andry rolled
out our container we wouldn't lose any
data particularly useful for things like
log files so we've got i net port blog
files that can now end up on our host
operating system and so we can we use
those just like
with Linda's containers we can add more
security features if we need to and more
security fence defenses who can limit
how many people you shares and how much
CPU processes etc and we want the
container to utilize so now we can start
delivering a quality of service across
our deployment and to a customers or to
other applications because one container
one we had to take out of the entire
host we can start ringing French on it
to make sure that everyone gets an equal
share we've got PowerShell API so again
we can hook that into an existing build
scripts and our existing pipeline if we
wanted to to start automating we can
contain a creation um and we can create
new containers from PowerShell as well
so we don't have to use those dokgo
build document Eli commands we can I use
the traditional PowerShell mentality
which you wanted so when I tried to
deploy the Nerds in the first time we
had our traditional a speed on that
error message and now it's due to
connection strings and that was because
we had a sequel server dependency which
is pretty common am like sequel server
is kind of like one of those things most
applications are going to need so how
does that look inside of a darker world
so sequel server is stuck alized
and we now have the beautiful case of
having sequel server for Windows
containers and also sequel server for
Linux containers with the recent port of
the sequel server for Linux you can now
win both versions of a container
depending on whether you want to talk
Windows hosts or Linux hosts just in the
same way that we did with our nerd
dinner we can launch this we can say
expose 1433 which is the sequel server
port we can find
define a password we can accept the
end-user license and agreement and that
will download and run everything and if
we wanted to see what was happening in
the covers because it's already in the
background we can do things like darker
logs nerd in a DB and as I realized very
quickly sequel server had got a little
bit more security awareness and what I
do and so it doesn't let you
set the si password the password to kind
of make sense until by using the log
could kind of see and start debugging
what was happening in other covers and
so everything which has been
traditionally were used to with in terms
of debugging had been thought about and
collected in terms of the API so we give
it a bit more of a secure password a
password is young long yo I think I'd
watching Breaking Bad at the background
or had Breaking Bad in my head some
reason anyway so now Secret Service
started we've got the a Hague
credentials and everything is happening
so he's now this is how I got no dinner
working we had the front end I could
talk to sequel server and it could
deploy everything but we've got these
two containers and we've got these two
dependencies and keep winning these
really long commands that need to be
documented that needs to be stored
somewhere or I'm going to keep going
typos and forgetting what password is
and so this is where docker compose
comes in docker compose allows you to
take all of these arguments and define
them at the Yama file again using the
amazing chocolaty we can install it from
the command line and go down and a
download the executable which we need
and to create the docker compose file or
to create the docker compose Gamal is
everything you find which we would
traditionally have entered using the
dock of wintermint
we just assign and we just take it and
use the syntax to define our intentions
so we define our docker image what we're
winning with the particular detergent
number we define our passwords we define
the ports which is required and any
dependencies which it needs and then we
can use docker compose up which I enter
that bucket bottom docker compose up and
that will deploy both applications at
the same time and we'll have our winning
application running happily in our
service but then I saw this I thought
like that's not quite how I remember it
looking from the back in the day like
that looks a bit weird and also not they
successful so again this was winning IE
so again are you chocolaty installed
Firefox and then it decided not to
render the mobile version and then it's
like it's just like the good old days
and nostalgia came flooding back for
the goodness and so this is now an
outpatient and now wanted to make it
available and make it public so it's
written on Azure and because when we
launched it we did that - Pia and
exposed port 80 so now we just need to
expose the windows the firewall in Azure
expose port 80 and then near dinner kena
ruled the world using windows containers
again I then got even a little bit more
confident and I'm running out time but
wanted to deploy the asp.net music store
which has been migrated to dinette core
so I just wanted to see how that looked
inside of a Windows container so the
first thing which is important is
because it focuses in on done at core we
no longer need to have like the full
window server core installation we can
now start targeting the more lightweight
more quick down version we could we
don't need that quite as much of the
windows functionality and the windows
api and again we've got the docker file
which lists the instructions it's a
little bit more complex due to the file
paths but it's kind of still the same
we're adding some files add in our new
get packaged with winning dotnet restore
which will go off and download it have
our bill package we're adding the sample
directory which contains all of the
source code we're building that using
done net build and then at the bottom
and defining how we start and how we're
in that application and so this even
though it's a completely different
application what we're building you'll
start to see these similar patterns and
these similar approaches between what
we're trying to achieve and kind of come
through its or describing what your
application is doing in the steps in
order to be able to build up those
layers to go from a blank installation
of dotnet to having your application or
built and willing and again we could use
a compose to lift everything again just
doing ducks combos up and that's how we
get to our winning application winning
in the cloud so now we've got this happy
but it's only running on one service so
how do we make that scale and how do we
scale
out so the first thing we need to do is
make the image available to elsewhere so
this is way you can push images to a
private registry or private document
whether street or the hosted docker
registry which I did here and you'll see
I and CD build pipeline kind of look
something similar to this like a
developer would do a git push this would
go into something like git lab or ping
foundation server which would start
build your team your build agent no
longer needs to have anything configured
because all of the configuration is
doing inside of the docker file your
build agent and it literally just needs
to be able to run docker build and
that's the only configuration it needs
in order to guide to build and test what
your application is doing so that would
happen
once I've been to the closely built the
output will be your docker build image
which you can then push to like the
private registry and then you can use
docker pool or docker compose up which
will do-- grab the image and then deploy
it onto your target service and deploy
it onto your target machines be it
Windows Server 2016 or something cool
like a jewel container service for
example but what about developers like
what if you don't want to run Windows
Live core as your development
environment which is a reasonable will
do more choice like Windows 10 is a
really good operating system so let's
take advantage of it and now Microsoft's
have announced and officially supported
Windows containers work on Windows 10 to
install it all we need to do is install
the optional features install containers
and then install Microsoft hyper-v
containers download binary and
everything is happy we then have Windows
container support running on Windows 10
and so now we can build the images that
use in exactly the same way using the
same time to compose everything which we
did before we'll build it and the reason
this works is because it's taking
advantage of that Windows hyper-v
container support so everything written
to the hyper-v container
it kind of having a guest Colonel which
had been designed for containers in a
third place and so that's why didn't
interact with your Windows 10 deployment
and that's why it can work and so now we
are yes so now we've got windows contain
container support so we've got docker
for Windows which is the tooling which
you download which is design for winning
docker on your development purposes and
when you download it
it generally downloads the Linux version
with support for learning Linux Tucker
containers and to any if he did dock
version you'd have the server you'd have
an architecture and you'd have like duck
images which are designed for Linux I am
Mbutu and geniux
ionic core in the command line you can
now say wouldn't you've got Windows
containers so to switch to Windows
containers and then this will point the
dark line to point to your Windows
Server installation incident or your
Windows Server support Windows container
support instead and so now when you do
Talcott urchin
we've got Windows architecture we've got
Windows images and everything is happy I
mean like this really nice ecosystem and
support on our development into machines
and this is an effect innocent you can
be needing side by side it didn't stop
containers which are already running and
so now we can have these two great
worlds happening where we can deploy and
build our Linux containers and build and
deploy our windows containers from the
same development machine which i think
is pretty impressive and it just
so what does this look like in terms of
the future and like where do I think
this is going so docker is everywhere
Ducky's the like the hot cool new
deployment technology everywhere until
we've got Linux support which has been
around for last three years very much
Couture and great functionality we've
now got Windows container support and so
we can take the same mindset the same
approach and start packaging or Windows
applications in an automated repeatable
way we can also now deploy ducted
containers unto armed and IOT so it can
ploy them onto raspberry pi devices but
there's also a startup are called
resident dot IO which allows you to use
the same mentality and deploy them unto
really IOT targeted devices we've got
window we've got sequel server as a
container which runs both on Windows and
Linux which is super impressive but what
about digital studio as a container like
we've now got to contain it with work on
Windows 10 and why do we now have to
keep downloading three gig four gig five
gig installation files at Visual Studio
which will then spend an hour two hours
installing to optimize itself and
prepare itself
why can't logically win at the container
Y commit just do a docker pull visual
studio and a download all the files
which we requires and runs inside of a
container which is all isolating sandbox
from each other and so when we have
different versions of our application
running one let's say in visual studio
2000
to modify and maintain our lovely
applications and also keep up to date
with our visual studio 2017 what if
those are containers they wouldn't
interact it wouldn't interfere and it
just happily work and as new service
packs were released you just download
the layer instead of downloading and
reinstalling the entire thing we have
this with Windows now so why can we
start seeing it with Visual Studio and
our development tools at the same time
and then we get to the point where
everything as our container everything
is written of container our build tools
our build scripts our build pipeline is
all containerized we're winning team
foundation server
and then that builds more containers and
then we push it onto a jewel container
service and that scales out and we can
deploy anywhere be in the cloud be it on
premise or a hybrid of both but this is
a new willed and it's lots of moving
parts there's lots of moving pieces and
it can take a few a few attempts to
become familiar with a pipeline and so
this is why we built caps coda
this is why we've got lots of free
content there to make it accessible make
it as easy to get started as possible
and for you to start seeing the value of
containers and with that I've got lovely
red flashing lights as well as someone
shouting at me so we've got four key
points apart from one I didn't have time
to worry about we've got four key
components we've got Windows server core
and Windows Nano we've got the legacy
way which allowed it to an Emily
application even if they're six seven
years old and deployed as a Windows
container and have it automated and
maintainable we've got Windows Nano four
new cool applications that we can deploy
and target a Windows Server host
operating system we've got Windows
containers which allows us to win them
and I still a secure sell an isolated
environment but with Windows hyper-v we
get a little bit more we don't have that
shared kernel support which we did
before
we've got Windows containers winning on
Windows 10 so we can use them as part of
our development environment test and
build locally and have that pushed up
and then hold fingerprinting which is
all about building and cloud and having
workloads disputed and itself detecting
whether it should be and deploying Linux
or should be deploying Windows
containers and which holes to use unless
your container service but that's for
the next talk in the next NBC during
that time in terms of my next set or
what I'd recommend your next steps to be
if you're one in a more obviously cat
coda because that's my baby if you want
to try this and play around Windows
server and assure it's probably the best
way to do it but it's really quick to
get started everything been
pre-configured and everything all of the
images are there for you to jump start
and get started very quickly and then if
you're more motivated at words and when
ten so you can install the tooling you
can install a part and you can start
playing around locally and if you have
any other questions my details are there
I'm always available to provide more
information but thank you very much for
your time and I hope you enjoyed the
rest of your day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>