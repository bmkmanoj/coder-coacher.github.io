<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance is not an Option- Building services with GRPC and Cassandra - Dave Bechberger | Coder Coacher - Coaching Coders</title><meta content="Performance is not an Option- Building services with GRPC and Cassandra - Dave Bechberger - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Performance is not an Option- Building services with GRPC and Cassandra - Dave Bechberger</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4w9twvw0MFg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone how's everyone doing
today so before I get started here I
just kind of was wondering how many
people here have ever heard of grp c12
anybody ever used it one what about
Cassandra anybody ever use that or heard
of it what about other no sequel
solutions ok so what I hope you take
away from this talk is first off a
little bit of understanding about what
grp see is also a little bit of an
understanding about what Sandra is and
then how you can build a simple gr
pc-based microservice and how you might
will be able to persist time series data
into Cassandra with using that vet gr
pc-based microservice and then the last
little bit is why you might want to use
gr pc and cassandra instead of more
traditional rest and relational model
for persisting time series data mainly
around why it's more why it's more
performant and has better throughput so
first a little bit about myself my name
is david beck burger i am a senior
architect at expiry i am from the United
States I live in Houston Texas I've been
developing all sorts of software for
about 16 years now everything from
embedded c systems through
high-performance web applications to
highly distributed systems i am also a
certified architect on Patchi Cassandra
and a data stack certified architect or
work for a company called XP row what we
do at xperia is we solve problems in
complex domains specifically around big
data IOT and high performance computing
we do that across a variety of
industries such as financial services
oil and gas manufacturing anything like
that we basically my company is broken
up into three separate practices we have
an architecture and development practice
where we actually build the stuff we
have a user experience practice which
does an excellent job of figuring out
workflows and how to show all this
complex data in a simple way and we have
a product strategy practice where they
actually
figure out prioritization and how to
actually get this out to the customer
base quickest here are some of the
clients that we have the companies that
we have worked for so first first thing
I want talk about is what is G RPC well
gee RPC is a general purpose RVC
framework it was built by google so i
don't know if you guys know this but
google is highly based on a micro
service style architecture so they have
a micro service style architecture for
many years it ran on their own
internally built a system called study
study was very closely tied to their
internal architecture to internal
infrastructure I should say and as they
moved into more of an open standard IO t
mobile application it was no longer
sufficient because it was so closely
tied to their internal infrastructure
well to solve that they decided they
wanted to move more towards a
standards-based protocol so they that
standards based protocol became what is
now grp see gr pc is based on the HTTP
HTTP to protocol for its transport
protocol so it's available across any
any common network infrastructure which
includes many mobile devices it's free
and open source and it's built this
specifically for the problems of
distributed systems so if you've ever
worked in highly distributed systems
you've probably run into the the posting
on the Internet of the the fallacies of
the distributed systems some of those
fallacies are you know networks are
always available there is zero latency
bandwidth is infinite all these sort of
problems where if you know you actually
start to build an extra bute system you
quickly run into them well gee RPC is
built to handle some of those sort of
architectural challenges well a little
bit about grp sees architecture grp sees
architecture it's it's a it's a remote
procedure call architecture and it's
built to allow clients to call methods
on servers as if they were local so this
is very similar to if any of you very
probably some of you are familiar with
soap services and have used them before
in your applications where you were able
to actually generate a client from a
wisdom
and you will get a claim a client on
your side that you can call without
having to set up you know the network
network traffic you don't have to set up
there you don't to make the request you
know if or what happens if the request
actually fails you know handling packet
loss things like that well gee RPC is
built on the same basic idea where you
can where you're able to call the method
as if it was a local client so it
simplifies the code that you have to
write it's built for writing low latency
highly scalable microservices and it's
built to be payload agnostic but payload
agnostic what I mean is by default the
default message format for transferring
data to and from dr pc is protocol
buffers protocol buffers is another
standard that Google has for a highly
highly performant message it's a highly
performant message format for passing
back and forth cuz its height of
partially because it's highly
compressible well that's the standard
that comes with your PC but it's also
built in such a way that if you have say
a service that needs a binary of
compression algorithm or it had what you
want to transfer XML or JSON or thrift
or atom or any one of the many other
different message formats out there you
can just basically replace the protocol
buffers portion with the portion for XML
or JSON and you can go grp see is also
built with bi-directional streaming and
by bi-directional streaming I mean that
means you can stream data from the
client to the server and from the server
back to the client which coming you know
my background is actually quite a lot of
my time has been spent building rest
style api's which when all of a sudden
you realize you can stream data back and
forth it actually allows you to think
allows for some interesting differences
between how you would build a REST API
and you how you can go out and build a
grp c-style api and erp c is pluggable
and extensible maybe you need a
different authentication method you want
a different method for compression
method for compression something like
that well it's built in such a way that
you can just plug in those different
methods to replace what's currently what
the current defaults are and go from
there
well the first thing when it comes to
grp see is the simple model and service
definition so what you're looking at
here is actually a simple model and
service definition for a service called
hello service well that the first two
things you'll see there are the two
things that in a message hello request
and hello response well what that is
what you're looking at is this is
actually protocol buffers that you use
to define your models and your services
so in protocol buffers a message is
essentially an odd it is will be
translated into an object so you have an
object called hello request and an
object called hello response each of
those has a single property and one of
the unique things you'll notice about
protocol buffers if you have never
actually used it before is that each
property has to have a number associated
with it this is because part of the way
it does data compression is when it
sends it over the wire instead of
sending greeting as part of the message
packet it actually just sends the value
one since both sides need to know what
the format of the the object are is it's
both sides are able to basically
translate that back and forth so that's
why that works that way if you look so
after you two messages you'll see
there's a simple definition of a service
says in this case it's called hello
service and it basically takes it sends
out a request hello request and it gets
back a hello response so pretty
straightforward secondly grp c is
optimized for speed and performance grp
see has several things built into it
natively that act that help optimize
speed performance and actually at all
help optimize bandwidth it has things
like natural message compression flow
control for the message pass as well as
multiplexing of connections so by
multiplexing of connections if you say
are working in a rest I'll architecture
if you need to make multiple requests
you end up opening or you know you end
up opening a request make interquest
closing it opening making a you know
requesting closing things like that with
grp see you can actually have a
long-running persistent connection which
you'll send multiple requests to end
possibly two different endpoints across
that single connection so it helps
optimize the speed because you're not
making multiple
connection and disconnection requests
and optimize the performance because
you're there it also optimizes bandwidth
which is especially important if you're
looking at doing a mobile application
and the last last major feature that I
really enjoy of grp see is the code
generation aspect of it so by code
generation you've seen earlier how we
defined the via the models and the
service well the GRE PC comes with a
tool called proto see well that tool
what it does is it allows you to
generate client clients clients tubs and
server interfaces for a variety of
language I believe currently there's
eight supported languages but being it's
open source you can work with any of
those other languages this is sort of
similar to how soap and with wisdoms
used to work with except except for the
fact that you don't run into the
problems you used to have with Wiz Dalls
in soap services where Java when
wouldn't render the objects quite the
way that dotnet was expecting it that
wasn't quite the way that C++ was
expecting the stuff to work well since
it's all done through the same
executable it's all done in a way that's
compatible this means if you have a
business service team that's writing
services in Java and a web service a web
development team that wants nodejs
services and maybe you're writing the
server in c-sharp and you have an
objective c team that's writing a mobile
app for a windows phone or at sorry an
iphone they were all able to actually
have the code generated and you know
that things will work together so next I
want to talk a little bit about what is
Cassandra well Cassandra is a
distributed no sequel datastore built
for large volumes of data and fast read
write performance it is an open source
Apache project it's built with a no
single point of failure architecture and
it's scalable and by scalable I mean
it's built to be horizontally scalable
on commodity hardware so if you go if
you're going to scale something like a
relational database usually the way you
scale it is vertically you get bigger
servers with
processor is more memory more disk space
with Cassandra being as being as it's
built it to be horizontally scalable
that it what it means if you need to get
a bigger cluster you end up just adding
more service your cluster in those Club
those servers tend to be cheaper because
they're more commodity hardware so a lot
of people may be familiar with the cap
theorem well what the cap theorem states
is there are three basic properties of
data stores consistency availability and
partition tolerance and that you can
choose two of them well what consistency
means is that all nodes will see the
same data at the same time availability
means that every request to the store
will receive a response and partition
tolerance means that the system will
continue you can continue to work even
during network failures well if you look
at something like Oracle or sequel
server they're both consistent and
available all that all the data is the
same across any of the parts of it and
it's always a every request receives a
response the case of something like
Redis or a MongoDB you basically all the
data will be the same and it's resilient
to network failures that you know that's
at the cost of not always being able to
answer every require request well
Cassandra it falls under what's known as
an AP or an available and partition
tolerant data store so it what that
means is every request will receive a
response it's very it responds very well
to network failures you but what you
lose is the ability for some of the node
sometimes the nodes will not all to see
the same data so cassandra is what's
called an AP data store and it has
what's called a vengeance isten see
we'll talk a little bit about what
eventual consistency means here in a
moment so another another comparison
that's often made between relational the
no sequel world is the concept of an
acid transaction so if you're familiar
with what acid compliant databases are
basically an acid compliant database it
has four guarantees it guarantees that
every transaction is atomic meaning that
they're all or nothing and if they're
consistent that once data is that as
soon as the transaction complete scan
all data is the same that transactions
are isolated in as much as transact one
transaction will not interfere with
another transaction and durable that
once the transaction is done the results
are permanent well part of one of the
problems with getting those for
guarantees is it actually test tends to
have a drag on the performance of your
data store because that's some of those
guarantees actually require quite a bit
to actually achieve well cassandra is
not an asset compliant data store in the
no sequel world what you end up giving
up sometimes is for some of those things
in order to gain performance is you know
you end up giving up a couple of those
guarantees and they've come up with this
other corollary or comparison called a
base compliant database what that means
is it has a base availability where the
data store is always working it has a
soft state which means that stores are
not right consistent so data may differ
between between nodes or replicas of the
database by data store and that is
eventually consistent this means that
stores become consistent over time so
now I want to talk a little bit about
what the Cassandra architecture is so
the cassandra is architected as a
cluster it's a group of servers it
there's you don't really ever run just a
single Cassandra server you actually run
them as a large chunk of servers and
it's part of that server there's a
hierarchy in side the cluster as to how
things are built and scaled out in order
to provide that no single point of
failure style architecture so the
smallest the smallest part is a node
well a node is a server you know it is a
server on next step up is a rack a rack
is very similar to like a physical rack
in a data center it can be a physical
rack or a logical rack but basically
it's a set of servers that will fail as
together if they fail usually if the
rack fails usually the in case of like
if you want to think about it as actual
data center you know they have the same
power strip they have the same network
well racks then are grouped together
into what's called a data center a data
center is made of it as i said is made
up of multiple racks which is made up of
multiple nodes a data center is very
much the same as a physical data center
it can be yet again be physical or
logical and in Cassandra
you actually have the ability to
actually have multiple data centers so
you may have one data center that's
located here in Norway and another data
center that's located in the United
States one of the unique features of
cassandra is the fact that data will be
automatically replicated between the two
so you may be so you can actually write
data here in the in your interim read it
out of the Americas this means that if
you have an international style problem
that you're trying to solve you're able
to actually optimize your applications
to write to the to read or write from
the data center that's closest to them
without having your application actually
be aware so the next thing I wanted to
talk a bit about was the architect the
hashing architecture of Cassandra so as
I mentioned cassandra is is built out as
a cluster well in that cluster each node
there's this concept of what's known as
a token ring a token ring is a set of
unique tokens between two to this I
think they run from 2 to the 63rd to 2
to the negative 6 either I don't
remember the number of unique values
that exists but that is a number that
I've never seen before so for simplicity
sake here i've just i've just shown it
at zero to 100 well the way that
Cassandra works in stores data is each
node owns a portion of this token
running so in this case we have four
nodes you know node 0 is 1 to 25 node 25
owns 25 to 50 node 50 owns 50 to 75 and
note 75 own 75 to 100 well each node
owns a certain set of that token ring
and there's a piece of code inside
cassandra called the partitioner well
the partitioner basically generates uses
a consistent hashing algorithm to
generate a token from a piece of the
data that you're reading or writing
called the partition key what we'll talk
a little bit more about that in a few
minutes about what that actually is but
just know it's a piece of the data
you're reading and writing from the from
the data story well it takes uh
partition key it applies a hashing
algorithm
and then it basically from that it gets
back a token and that token is what
determines which node owns that data and
one of the key features as I mentioned
was the no single point of failure
aspect of cassandra and because of that
everything's appear there's no master
there's no slave so everything knows
everything about all the nodes so all
the nodes know which token ranges belong
to which nodes so here's a little
example of how tokens actually we're
going to work so you have a client it's
making a call into node 75 you'll often
see node the node that's being written
read or written to being called the
coordinator node so the first thing that
the coordinator node does is it takes
the primary key out of the data that's
being read well it find in this case
that the primary key is expiry it sends
that prior sorry not primarily the
partition key it sends the partition key
to the partitioner which applies that
consistent hashing algorithm that
consistent hashing algorithm will always
send back the same data for the same
partition key in this case the token
that was returned is 12 so now that we
know that this right is going to
whatever node owns the partition key
range that includes 12 well that
partition he is owned by node 0 so that
is how the data is written where the
data is written to so that after the
data has been written one of the other
key architectural features of cassandra
is that data is replicated there's an
num you can specify the number of times
data is replicated within a data center
and that is called the replication
factor or what you'll see as RF one
important thing to note here is all of
this is done without the application
having to have any concept of how it
actually works so the number of replicas
and you know is basically specifies the
number of times the data is copy within
a data center if you have multiple data
center setups you can actually specify
different copying different number of
times it's copied between data centers
so here in Norway you may say it's
copied three times inside a data center
in the United States it may be copied
that's a one of the other key features
is that ability to actually tune that
based on your needs and the last thing
here is yet again as part of the single
point of failure if now I'm replicating
this data multiple times what happens if
one of my nodes is down for some reason
well there's this functionality in
psycho Sandra called a hinted handoff
what it means is the coordinator node
that the node that you're basically
writing or reading your data to will
basically know okay I need to write to
node 1 well node ones down it will store
that data locally until such time as
node 1 comes back online and it will
then automatically sync that data back
to it so here's a little example of how
data replication it will actually work
in real life so in this case we have a
Ford node cluster we have a replication
factor of three that means the data is
going to be written to three of these
four nodes well first thing that happens
is a right comes in to the coordinator
node in this case the right is a as we
talked about next step is the partition
key is pulled out of the right sent to
the partitioner a partitioner then sends
back the token in this case 12 well note
the coordinator node nodes that part
knows that the token them with the
number of 12 is owned by node 1 and it's
written to node 1 at the same time we
know we need to replicate this data two
more times so while it's being written
to node 1 it's actually written at the
same time to notes too and nodes 3 the
way it the way that cluster determines
what nodes it needs to write to is if
you have a replication factor of 3 and
it's owned by node 1 it just right to
the next two nodes in this cluster so no
it's 2 and 0 3 if it was owned by node 2
it would write it to note 3 and 0 node 4
as the replica data
so what does it mean to be eventually
consistent it means that data will
eventually match on all the replicas
this isn't in terms of seconds or hours
or days it's actually in terms of
milliseconds in almost every case one of
the other key parts of eventual
consistency is what's known as the
consistency level or will you often see
response written down as CL they're 11
different level you can choose for
rights and ten different levels you can
choose for reads but what consistency
level means is it's the number of
replicas that have to respond back that
they've either read or written the data
correctly in order for your request to
succeed so even though it's being
written to three different replicas
maybe you have your consistency level
set to one which just means any one of
those has to respond back to me that
it's written the data successfully from
my my application my client application
to get back a successful ACK well
because you because of this ability to
be able to be able to change how many of
you know how many replicas need to
respond you can actually tune the
consistency level to affect the
performance and availability of your
your application because you know let's
take the example I just talked about
where we had a you know a consistency
level of one well if you have a
consistency level one that means the
fastest one to be able to write the data
is going to be able to respond back to
me first if I have a consistency level
there's another consistency level called
quorum which is very commonly used and
basically it stands it means a majority
means a majority of your replicas have
to respond successfully in order for
that to happen that means so in the case
of a replication factor of three that
means two of them if you had a
replication factor five it would be 37
would be four and so on but that mean
but the more time the more nodes that
you end up requiring to have written the
data in order for your request to
succeed the slower the performance of
your application is going to be well the
other part of this is the availability
there is a consistency level that's
known as all which means all of the
replicas have to respond back so it's
very similar to
a that case it's more like an acid
transaction where everything has been
written and all the data will be
consistent the problem with that is if
any of those nodes are down now every
request is going to fail so if you do if
you choose that it's very rarely
actually used in real life because of
that downside but if you choose that
know that any of your right like any of
your request will fail if one of your
nodes this down and lastly that you know
consistency level it can actually be
tuned for read and write performance on
a per query basis so different queries
can be tuned to have different
consistency levels maybe you have one
that you need all on because you really
want to make sure that if it's written
it's written everywhere in that case you
can actually use the you know you can
use all and maybe you have another one
where you don't have his you don't need
as high a guarantee so you use
consistency level of one the ones you
most commonly see use our one and quorum
well one corpsman there's actually a
another concept in there called local
one and local quorum and what that means
is if you have multiple data centers
it's only waiting for the data in the
local data center you're writing to it's
not going to wait for anything to go
across you know in the case of having
one in Europe and one in America you're
not going to wait for anything to go
across the ocean in order to do that and
this is all to God tuned by basically
they are in Cassandra there's something
called cql which stands for Cassandra
query language which is actually I will
see it here in a little bit but it's
very sql-like so why might you want to
use like Cassandra over a relational
database well the number one reason
probably is performance cassandra is
optimized for high very fast to read and
write performance and when I talk about
very fast read and write performance
we're talking single-digit millisecond
read and write efforts it's pretty much
something you're never going to be able
to actually achieve out of a relational
model it's also linearly scalable and
what is I what do I mean by when I say
linearly scalable I mean if you have
twice as many nodes in your cluster so
if you have if you add twice as many
nodes to you close to you will be able
to process twice as many transactions
that's not really true in any relational
database if you had twice as big a
server you're still not going to get
twice the performance out of that
it's also natively built as a
distributed data store by the and what I
mean by that is the fact that it is it's
built to handle data that's too big to
fit on a single server so if you have a
relational database and you need to
scale it up to data that's too big for a
single server what do you end up doing
you end up having to build clusters you
end up having to shard it you end up
having to do rather relatively complex
exercises if anybody's had to go in
about this and that has a lot of
overhead involved and actually getting
it to work well since cassandra is
natively built as a distributed
technology it can handle pretty much any
size scale of data you know terabytes
tens of terabytes hundreds of terabytes
it doesn't matter and because of the
distributed nature in which it's built
it's actually built for an always-on
style architecture so you can actually
have a cluster that is never not
available so if you start doing a lot of
research on Apache Cassandra what you'll
find is you'll find this concept of
you'll see a lot of references to
datastax and I just wanted to throw a
little bit in there to say tell what the
difference is so don't get confused like
I did when I first started well so as I
mentioned apache cassandra is an open
source free project well datastax is is
datastax provides a commercial version
of that project with additional features
added some of those additional features
are it has a full text it has
integration with a full text index or
based on Apache Solr it also has
integration with a analytics tool based
on apache spark so those two things are
tightly integrated that's available in
Dave sex has two editions there's
Community Edition which is free and
there is an enterprise edition the
Enterprise Edition also adds some
additional features around enterprise
level security integration with the
active directory has some additional
tools around men automating some of the
management tasks you end up having to do
with a Cassandra cluster and there's a
tool it also comes with it called ops
center which is a very nice way to
monitor the the health and performance
functionality of your cluster I'm
probably the last and most
the biggest reason I see a lot of people
do it is it comes with a support ticket
if you know being an open source product
if it breaks on you you end up having to
go to newsgroups you end up having to
you know post things on Stack Overflow
to get help well if you have the
Enterprise version you actually have
someone you can call so next I want to
talk a little bit about a product you
know here's a problem where you may want
to start thinking you may want to
actually a use you know grr PC and
Cassandra instead of a rest and
relational database so to model so you
write your work for a company that does
a SAS product for truck that monitors
truck engine sensors basically you know
you currently have like a thousand truck
thousand trucks in your fleet and you
take a readings every 10 seconds this is
built on a web api you know rest set of
services on top of the sequel server
database so i'm guessing we've all built
something probably very similar to this
before well you recently just landed a
huge new client they love the UI they
love how the product works they just
want some changes to how the data is
actually read out of is how the data is
captured instead of reading it once
every 10 seconds they now want to read
it once every second they also want to
add latitude and longitude information
from their trucks GPS is so they can see
not only where it fail are not only when
it failed but where it was out at the
time they're adhigam they're going to
add 10,000 trucks to the two-year system
and part of the contract was you needed
to minimize their costs and provide them
a zero downtime environment well what
what's the problem well now you've gone
from a transaction load of about 100
measurements a second to 22,000
measurements a second it's a relatively
large increase and you know your data
loads gone from about 35 megabytes a day
to 2.2 gigabytes well your architecture
is going to need to change in some way
to handle this additional increase in
scale I think we can all agree so here's
the solution on proposing which just in
case you are interested all the code is
available on github if you want to take
a look at what we did here well the
solution on proposing is first off we're
going to change out the sequel server
database for Cassandra cluster this will
provide some of the scalability of not
only
the transactional load that's going to
be required but also the data load
that's going to be required because a
two point two gigabytes a day how
quickly or is that going to take to fill
up a single server won't take that long
and we're going to replace our rest cell
base a rest-based set of services with a
gr pc-based set of services
so the first step here is to define the
model in service we've seen this a
little bit already with earlier on where
this is basically using protocol buffers
to define out the models and service and
services and service endpoints that
you're going to have so in this case you
have a service endpoint called chucking
it's got two different you have a
service called trucking it has two
different endpoints one of them is
called record location and the other one
is called read last location what the
Dom so if you look at real s location
what it's going to do is it's going to
take in a trip object and it's going to
return back a point object so it will
read the last location that's been
stored in the system and then you have
one called record location and this is
where it kind of gets a little bit
interesting from an API standpoint this
is one that actually will take a stream
of points and return back a stream of
responses so this is one of this is an
example of how you can set up that
bi-directional streaming functionality
that I talked about with Betty RPC had
so you're able to set that up that way
you'll notice then there's a few objects
of a point one of the things to notice
about the point is the first property is
actually another object so with with
protocol buffers URL able to build out
complex domain objects and transmit them
back and forth across the wire so now
that we've defined our server our client
and server subs sorry the text is
actually pretty wonky here what you'll
see at the top there if you could read
it is that basically this is the code
that you will need to the command line
tool code that you'll need to run to
actually generate out the client and
server stubs what you can see is
basically it's a call to that proto see
file and then it's a bunch of
command-line arguments to specify where
the file you want to write where the
proto file is that you want to actually
compile and where you want the outputs
to go and what you want those outputs to
be unfortunately I know most of us all
here Windows guys and don't really like
command-line stuff but at this point
there is no GUI to toolkit to do this
but like I said it's still relatively
new technology so I expect
we'll probably come along at some point
here in the future so you run that
command that big long command and what
do you get out of it you can end up
getting out two separate CS files one of
those files will be called chucking
service CS and that will contain all
your model definitions the other one is
called trucking service gr pc and that
will contain the the gr pc client and
decline to stub and the server interface
that you will need to actually implement
for your service so now that we've
created the service now that we've
defined the service and we've created
the stubs the next step is to go and
actually create the key space in the
table so in Cassandra you have what's
called a key space a key space is
basically the same as a database is in
sequel server it's a collection of
tables what you can see here is that the
so what you see here is actually this is
all cql that you're seeing on screen
here as you'll notice it's very similar
to the way SQL looks and it's actually
very similar to the way SQL functions so
first thing you'll see here is that
we're going to create a key space called
NDC Oslo which is which with that we'll
just do is create a new key space called
NDC Oslo and then we're going to set the
replication on it this is where you are
able to set the replication factor of
your system we talked about their
application factor earlier is the number
of nodes that data is copied to well
there's a couple of properties you can
set the first being the class there's
there's two separate classes you can set
if you're working on a single data
center you use what's called simple
strategy and then you can set the
replication factor in this case I have
it set to 1 because I was running this
on a single node cluster just not much
of a cluster but the other one is if
you're working in a multi data center
environment you'll get this one called
network topology strategy the
interesting thing about network of
apology strategy is that you're able to
set the replication factor on a per data
center basis so you can set up multiple
data centers they can be of different
sizes and you'll still actually be able
to set
from replication factors on them so once
you run that command it'll basically
create your key space the next thing
we're going to do is run a use command
which will just put you in the context
of NDC Oslo this is exactly the same as
doing a use command in SQL so now that
in the context of the key space we've
created we're going to create a new
table this should look very familiar
pretty much to anybody that's done a lot
of SQL I'm just going to create a table
called location by trip ID i'm going to
set a few properties you know there's a
truck name a manufacturer a trip ID a
time a latitude and longitude I guess
what one key thing to note is if you're
actually using Cassandra the time stamp
is all in unix time epic which can be
kind of interesting to get to from a
dotnet world if you have never had that
fun so the last part you'll see there is
actually the primary key so the primary
key is not the same as a primary key in
a relational database in a relational
database the primary key is used as an
index and it's used to actually provide
relational and data integrity across
tables in Cassandra what it's actually
used for is it's made up of two separate
parts the two parts are the partition
key and the clustering keys what you'll
see in this example is we have truck
name manufacturer and trip ID and those
are what can our inside a set of
parenthesis and that's what makes the
partition key if you remember from
earlier the partition key is the piece
of data that is taken May and given to
the partitioner in order to make the
token the token is determines which node
owns that data so that's what the
partition key does the T the key after
that is the clustering keys right now I
only have one clustering key of time you
can have many clustering keys and then
you can specify the order in which you
actually want them to be written out
ascending or descending order well the
point of the clustering keys is so once
you know that the data is going to be
written to node 1 for example what what
actually happens on the kiss on the
actual Cassandra node when the data is
written it's written each partition is
written to a separate file well once
it's written to a separate file the
clustering keys tell you what order to
write the data into the filing in this
case the data
will physically be written into the file
in time descending order so now that
we've created our database or created
our key space we've created our table
we've created our stubs and we've
created our our objects the next step is
to actually attach to the cluster and
start doing some work well there's an
open source c sharp driver that's
provided by datastax at the address
you'll see there and basically if you
want to connect to the cluster you
basically build it out as you see here
you know you're basically to a cluster
dot builder and then you add contact
points well in this case i'm showing you
want to add your showing adding multiple
contact points well why would you want
to add multiple contact points well the
reason you want add multiple contact
points is one it provides the ability
for no single point of failure because
say in this case node 2 is down if that
was the only contact point they're
listed there your application would not
be able to actually read or write data
from the thing the second part is you
want to be able to spread the load of
those reads those read requests in those
write requests across the different
nodes in your cluster since every node
in the cluster is appear there is no
master any node can handle a read and
write request so if you put all the
nodes in your driver what will actually
happen behind the scenes is the driver
will automatically round-robin across
the different nodes to spread the load
out that way not one load is not that
way one node is not taking a majority of
the read and write requests so it helps
prevent overloading a single note in
your cluster and slowing down the
performance if the cluster is a whole
so now that we were able to now we've
connected to the cluster the first thing
we want to do is write some data to it
well if you're familiar with SQL as I'm
guessing probably everybody here is this
should look very familiar to you you're
just going to insert data into the table
name then you're going to specify the
columns that you want to insert and then
you're going to specify the values that
you want to insert into that column I
mean you could take this and pull it
right into a SQL database and it would
work just fine one interesting fact to
note about Cassandra is when you're
doing inserts or updates in Cassandra it
does what's called an absurd it's
actually this is actually a not uncommon
thing in many snow sequel databases and
what an absurd is is if you're familiar
with a sequel merge statement it's the
same functionality what it means is if
the data exists it will update that data
if the data does not exist it will
insert that data
so now that we've successfully written
data to the data stores let's read some
data back from it well how do I go about
reading data back from it well you again
write a cql command which is very
similar to the SQL command you know I'm
going to select star from my table name
where some filtering criteria in this
case the truck name the manufacturer and
the trip ID and in then there's this
idea of called limit one what i'm doing
here is i'm actually eliminated to only
the top result so i will actually only
get back the last read the last location
ever written by time that's where the
clustering keys come in it's very
similarly by the concept of a top one in
SQL so a couple interesting things to
note here is there are some constraints
around how you can actually filter data
in c ql in c ql you can do a select star
from a table in which case it will go
out it will scan every cluster in the
table are every club every node in the
cluster for that table information and
try to co elite it together if you try
to do that on any on a cluster of any
size it'll probably timeout it doesn't
usually take too long before you get to
the level of it just times out before it
actually is able to process all that
data so if you want to add a filtering
criteria to it there there are some
constraints some of those constraints
are the first one is if you're going to
add a filtering criteria you have to add
at minimum every key in the partition
key as an equality clause this means
because my partition key consisted of
truck name manufacturer and trip ID I
have to have truck name manufacturer and
trip ID as equality clauses in my
filtering in my where clause in my
filtering clause that's because as we
said those are the three items that are
used to determine by used by the
partitioner to determine the token so
what the way that work the way it works
is it takes us when you do a select
query it takes those three keys it finds
out what the token is and then it's able
to very quickly go straight to the node
that owns that data to the file that
owns that day
and pull that data back out that's one
of the ways in which the performance of
cassandra is enhanced by adding that
constraint on the second constraint is
if you want to actually filter or if you
want to filter on anything beyond the
partition key these must be one of the
clustering keys so you have to have so
basically in this case the only
additional item column that I could
actually search on is time because it
was the only clustering key i added into
it cluster if you're searching on that
those can be inequality searches if
they're clustering keys but they have to
be equality keys equality searches if
they are partitioned keys so next I want
to actually show you a little bit of
running code so what you'll see here
this is actually the client side of the
application what we were shown before
was everything that's written on the
server side so this is the client side
so in order to create a client basically
the first thing I have to do is I have
to set up a channel all a channel is it
specifies essentially where is the end
point I'm going to be talking to since
this is over you know since it's over
HTTP heat in this case I'm actually
going to run it against local host on
pork 50,000 51 first thing I do is come
in here and I start I basically start a
new asynchronous task called start truck
recording well let's go look at start
truck recording well start truck
recording first thing it does is it
creates a new trucking client that is
the client stub that was generated by
the generated from the definition so
between these two things I have now been
able to connect those two lines of code
will basically now connect your client
to your server much simpler than a rest
style architecture well first thing next
I'm going to do is I'm going to
basically set up a call to record-low to
record location record location if we
remember was that bi-directional
streaming endpoint so we can actually
this this is an example of how you
actually will write a bi-directional
streaming
extreme data back and forth you know the
theory in this case is a truck you know
when a truck starts up it connects once
and then it's able to stream data back
and forth to get at what it's actually
doing it extreme little stream latitude
and longitude data and it will send back
the actual time it took to write the
data out that becomes important because
the first step here is we're going to
set up an asynchronous task to basically
wait for those response messages to come
in so those response messages come in
the way I have this written is it will
write every fifth it'll basically as it
writes every fifth response message that
comes in it will take the average of
that data it will then call a read last
location so you can see how so it will
actually write would actually read data
out of the data source and then it will
print all that information with those
times out so after you set up the task
to read the responses back what the next
step is to actually basically just start
sending data in so this code all it does
is create a new point with some some
made-up data and write that
asynchronously once a second to the
server
he's really
so what you'll see here is its now these
the servers are connecting together that
that first one is because the server had
not started out by the time it tried to
connect so you'll see it's writing out
data and it's taking about 178 170 or so
milliseconds to read and write data to
and from the cluster well that doesn't
seem super performant to me I don't know
about you guys but 170 milliseconds is
pretty slow however that's because the
cluster this is currently writing to is
located in Oregon in the United States
it's about seventy seven hundred
kilometers away and has about a round
trip ping time of about 164 milliseconds
so if they figure 164 milliseconds out
of it you take you subtract that from
170 milliseconds we were seeing that
means it was taken about five sec 5
milliseconds to write that data out
which seems a lot better to me if you
can see on the left around there right
there I actually ran this locally in
data center in Oregon to get more
realistic numbers and the numbers there
were anywhere from about 125
milliseconds to actually write I think
that's probably a little bit misleading
I think at that point you're down into
the the accuracy of the.net stopwatch
class because I don't think it was
actually reading and writing within a
millisecond it seems a little too fast
for Maya for from my experience but I
have seen many times in in my experience
I have seen where that data will be read
and written in about somewhere between 3
to 10 milliseconds so now I just wanted
to talk a little bit about some of the
trade-offs between grp see I when using
grp seeing Cassandra so some of the
trade-offs when using grp see first off
and probably the biggest trade off at
least from my perspective when using grp
see is that it's not for browsers at
this point unfortunately there is it
relies heavily on HTTP to HTTPS
functionality and so far none of modern
browsers have had a robust enough
implementation of it in order for them
to be able to build a client against it
there is hope that
this will be coming in the future I hope
really hope to see this in the future
because it would make it so much more
useful the other part is if you have
large messages that you're passing back
and forth anything larger than about a
megabyte you end up having to chunk that
data theoretically theoretically you can
write grp see messages or up to about I
think it's 63 megabytes but the
practical limit in what they suggest you
use is no longer than one megabyte this
means if you have a you know five
megabyte file you have to chunk that
diet up you have to send it back and
forth and unfortunately there's no
built-in functionality currently in gr
PC to handle the chunking so you end up
having to write it yourself also in dot
in protocol buffers version 3 which is
what all of this runs which was what all
gr PC runs on there's no support for
nullable data types I find this
relatively annoying because if you get
end up getting having a value that's an
integer and it comes across as a zero
does that mean it's a zero or does that
mean it's a null you end up having to do
some or
it's all its optional but when it's
general yes it's yeah yeah yes yes yeah
and probably one of the other things
especially if you want to use this in
production is there it's still a beta
release so it's a beta release so a lot
of places especially a lot of
enterprises won't allow you to use it in
production yet it's strong beta I think
it's on beta 14 at this point so it's
been around for a while so what are some
of the trade-offs when using Cassandra
well probably the biggest trade off when
using a Sandra and the biggest
limitation that you'll actually run into
is the fact that you are not allowed to
join between tables and Cassandra so of
what that means is it means exactly what
I said as far as every if you want to
get data out of a table in Cassandra all
that data must live in one table this
leads to a lot of denormalize a 2d
normalization and data duplication
basically when you do data modeling in
Cassandra it is far more important than
it ever was in like a relational model
because you have to build your basically
your Cassandra key space is built on
what's called a table per query
methodology so if you're going to build
out your key space the first thing you
have to do is figure out how you're
going to get the data out of your
application because if you don't know
how that you get the data out of your
application you're not going to be able
to actually build out your tables in a
way that you can be able to fetch that
data because the second limit one of the
second limitations as I already alluded
to is that you can't really do ad-hoc
queries if you want to do queries that
have filtering all those filtering
criteria have to live have to be part of
the primary key that's not necessarily
completely true there are several other
methodologies there is there says
concept of secondary indexes and
materialized views and if actually if
you're using the data stacks you can use
the full text indexer inquiry against it
however each of those has an effect on
the performance and generally and in
general they're discouraged from being
used unless there's no other way to do
it cassandra also
minimal support for aggregations it has
support for some min max an average but
those are the only aggregators it
supports and in general because of the
performance hit that it requires to
actually make those work they're not
recommended to be used as you probably
now have guessed that complexity of
cassandra is far more than that of a
normal relational database part of
that's being just based the nature of
being a distributed data store part of
that is just the complexity with which
is required to get the performance that
that people are looking for you know
cassandra is not something you're going
to sit down in a weekend or an evening
and figure out how to use correctly it's
going to take some time in order to be
able to understand what the limitations
are what the advantages are and how you
might best use it in your scenario and
the last one here is sandra's not
relational the biggest mistake I see
with a lot of developers that are coming
in to try and use cassandra is they just
try to use it like it's a relational
database well if you try to do that
you're pretty much guaranteed to fail
you need you know you need to spend some
time you need to understand it you need
to understand its limitations and then
figure out how you're going to apply
apply it you can't just throw your
relational database schema ad in and
expect to actually be able to accomplish
anything from it um so now if you're
interested in learning some more there's
a couple of links i have up here
specifically if you're interested in
learning more about Cassandra i would
highly recommend you go to the Academy
datastax comm link that they actually
provide a lot of very good free online
training you do have to register for it
but it all the training is free they
have several very very in-depth courses
on not only Cassandra but the other
aspects of the data stack system
including solar spark and actually they
recently came out with one there they're
going to be datastax is going to be
releasing a graph engine on top of the
their their version of cassandra and
they recently released a graph training
on that as well so with that I would I
would like to thank NDC Oslo for
inviting me to come speak and I'd like
to thank all of you guys for coming and
listening to me
there are any questions how do you do
security between the replicate the nodes
when replicated you can actually set up
ssl to secure the data between it you
can see in you can also secure the data
and write state as it's being written to
disk
no there's not unfortunate that's one of
the limitations you have is that
partition he has to be equality
comparisons and the reason for that is
because when because those equality
comparisons if it needs to be equality
comparisons so you can get the token out
so it knows exactly where in the cluster
to get it that that's one of the
trade-offs you end up making in order to
get that very fast read performance
nose with a network that is not that
fast could that
absolutely yeah if you have if you have
nodes on it on a network that's very
slow yeah it will dramatically affect
the performance over overall in general
and then the consistency level is very
yeah the consist you can use to probably
tune that to make it slightly better you
know you may want to make it zoom one
instead of instead of quorum but yeah if
you you pretty much have to have a very
fast network between all your nodes
internally in order to make it very
performant
what happens to that date so if you so
let's say for example you have the note
is owned by no data I've been data is
owned by node 1 and node 1 is down well
that's where those two other replicas
come into effect if node ones down in a
queries data that's owned by node 1 then
the replica samode to in node 3 will
service that request because we have a
replication factor of 3 yeah yeah you're
really what owning it means is it's the
first of the nodes to have that data so
Cassandra the actual distributed model
is this is inspired by DynamoDB so that
aspect of it is different so Cassandra
the the it was originally built of
Facebook it was inspired by the date the
distributed design was inspired by
DynamoDB the the data model was inspired
by google big table as far as
recommending it you need to know more
about the problem before I could say one
versus the other but they're very
similar in a lot of ways
as far as I know there is no one that
provides a hosted service however there
you can spin up a mis in AWS and
actually Windows Azure has has built-in
support now for as I think it was like
November they signed an agreement with
Microsoft to have built-in support for
at fora Cassandra I'm sorry actually
it's not Cassandra reads data stack
specific sorry
a classic geo time series database
probably the biggest biggest would be
the scalability aspect of it you know if
your start putting in you know two point
two gigabytes a day it's going to not
take that long in order to overwhelm
many databases so
you're not well I are you talking grp
see are you talking the connecting to
the clot the cluster because the round
robin in the driver was done in the
closet was done in the apat the
Cassandra cluster whereas grp see you
specify the specific address that you're
connecting to know grp see is its run as
its run just like any other web service
there's a one endpoint the round robin
and i think you're talking about was
inside the day you know oh yes that is
not at this isn't grp see this is
actually connecting to the Cassandra
cluster this is the Cassandra driver
so it is actually a binary that runs its
own server inside of it if you wanna the
way I've always done it when I've done
it in Windows is just built a Windows
service around it to run it what I
showed here is actually just a console
app if you want I can show you how it
actually okay you the there is a few
base packages you have to add around
there's like grp see core grp seen a dub
c sharp they're all new get packed new
gettable packages and then what the
what's generated from the model
definition is actually an interface that
you have to implement
and with just one
no you probably wouldn't if you in this
case you probably would actually have to
build a couple of dr PCN points and then
you could put whatever load balancing in
front of it that you wanted to all right
well if there's no more questions I
thank you and if there's any other
questions I'll be around here if anybody
has anything else thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>