<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Writing high performance code in .NET - Bart De Smet | Coder Coacher - Coaching Coders</title><meta content="Writing high performance code in .NET - Bart De Smet - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Writing high performance code in .NET - Bart De Smet</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fI1XGVIQjkA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">working on large-scale events processing
before that I worked on reactive
extensions and one of the original
developers of our X subnet and so what
I'm building right now is a highly
scalable distributed variant of that
platform and so throughout the years in
building this system which we call
reactor it actually powers a whole bunch
of things like Cortana on Windows 10 for
example in building that system of
course we had some challenges with scale
and performance to overcome and so this
is kind of a talk with tales from the
trenches of things that went bad and you
know what we learned in doing all of
that kind of stuff just to give you a
sense of the scale here this thing is
running about up to a billion queries
that are standing in memory all the time
across a couple of hundreds to thousands
of machines so in some sense it's a
relatively small service in terms of the
physical hardware that we have but in
terms of the volume of data processing
we do like thousands of events against
billions of queries it's actually quite
quite big it's all built on top of
service fabric and dotnet as well as
Donetsk or most recently so we are
actually a pretty big consumer of the
common language runtime and c-sharp and
all that kind of stuff especially things
that have to do with system reflection
Ahmet and also its of runtime
compilation of user quays so it's a
pretty it's pretty much a big compiler
in the cloud if a huge optimizer on top
of it you know to do a whole bunch of
interesting things now it's totally
possible that the stuff you guys built
is quite different but what I'll be
talking about is general advice and how
to analyze certain performances use the
kind of pitfalls to actually watch out
for now before getting started I just
want to talk very briefly about this
kind of you know performance engineering
strategy that when we're building a
system like this is actually good to
sort of have in check in your team so
one of the things we do an awful lot of
of course in large-scale services like
being is having a lot of instrumentation
and a lot of counters so we can know you
know where the performance goes in a
production system I just
and more valuable than knowing no way
the liens is going a very long we trust
pipeline that maybe you know stretches
over hundreds of many requests and
hundred components that sort of happen
during the lifetime of a single request
so you only have this this good set of
measurements and there's a lot of ways
of doing that I'm not going to talk
about you know using things like a pin
sides in Azure or some other facilities
you know have something that that your
system isn't a black box and you don't
know what what the heck is going on of
course second thing is the performance
culture and if you know that performance
is going to be critical for your system
then of course it's good that everyone
on the team has solid understanding of
the performance goals and you define
some metrics that you will go by in in
our case it's like the number of queries
that we can host in a single machine for
example as a key metric like a theory
guess that by five percent that means
like purchasing a couple of and some
cases hundreds of extra machines like
you know that's that's a lot of
investment you know to sort of you know
recast performance so you don't want to
be doing that so some general purchase
advice over here all common sounds in
some way now second thing I know your
tools the tools that I tend to use are
the ones I'm gonna list here but there
are many other tools that are useful for
example you know jetbrains has excellent
tools to do memory analysis for example
the ones that I sort of stick with the
most right now at development time the
stuff that we have in visual studio
especially Visual Studio 2017 we have
create memory Diagnostics capabilities
over there so you can actually use
everything that's built in into the
product over there in production systems
we tend to not have visual studio
installed on every single machine in the
cloud so we use some other tools there
one thing that you know we kind of
always tend to fall back to and it's
always good to understand a little bit
of that it's windy bug and SOS which is
a native debugger extension even if it's
just you know a services caching you
have a dump file the dump file is like
our machine some of them have 128 GB of
RAM if you have a dump file of a hundred
GB you're not gonna copy it to your
surface laptop to sort of look at it in
Starbucks
or your favorite coffee shop he will
Remote Desktop to that machine and use
the tool that's available on the machine
so in a lot of cases we just open up
when debug take a look at the dump file
and see like what well what the heck was
going on in this in this particular
system perfu how many of you have used
perfume or heard of perfume yeah the
perfume is a tool built by the CLR team
I'll show you a couple of things later
about it it's a great tool a very
lightweight ex copyable it's just a
couple of megabytes you can just copy it
to the machine and take a look at you
know in real time at you know the
stacked aces and all that kind of stuff
where you know the CPU cycles are spent
where the memory allocations come from a
great indispensable tool at this point
and then finally CLR MD how many have
heard about CLR MD last yeah
CLR MD is actually a managed code
wrapper on top of all the api's deep
inside the common language runtime that
enable you to look at one time as well
as on dump files at the memory
structures of the CLR so you can
actually build your own debugging tool
just like SOS allows you to take a look
at everything on the heap everything on
the stack wrote attach pool is up to
this tool actually builds on top of that
and provides a managed to API experience
and in fact for a lot of analysis things
that I have to do I always use CLR MD
because when you look at SOS well you
can say like what's going on here in
terms of you know why is this object
still alive but then you may have a very
domain-specific problem where you see oh
we have like a chain of a hundred tasks
that are sort of like linked together
and we want to know like who's keeping
that life at that point it makes a lot
of sense to build a custom tool that
says like show me all the continuation
change of chains of tasks for example
and as US doesn't know about tasks of T
it doesn't know the internal structure
you can sort of go one task at a time
and chase pointers but we've you know
CLR MD you can actually write a linked
query that says like from all the
objects on the heap which are of type
tasks find me the continuation to that
task and sort of stitch it together so
you can start start building linked
queries on top of you know
debugging API if you will super-powerful
little thing so I may show you a couple
of examples of that - towards the end of
the talk I have a couple of slides here
which I'm just gonna blast - really
quick and the goal is to make those
slides available I think they're already
available from a previous conference I
spoke at but this is kind of you know
just a tutorial on how to get started
with each of those tools you can find
plenty of stuff online like how to start
with and debug in SOS pretty easy how to
start with perfu that's a UI tool so
even easier and I'll show you an example
of that one later but this is kind of
you know a little picture on how that
sort of looks I'll give you an example
in a bit CLR MD is a new get package so
just create a new console application
import CLR MD the main thing we have
those kind of libraries is if you
haven't used them like where to start
so all this slide is telling you look
for a class called data target and start
partying like you can say data target
attach to process and from there on you
can start inspecting all the memory
structures in that target process if you
have a dump file you can do attach you
know to a cache download those cache
jump over there so just some some little
dead bits here okay
now let's get started so what I have
next is a menu of maybe 20 cases of
things that we run into and so those
will actually always have some grant
lesson on how to make sure that your
service is performant and the kind of
you know mistakes we run into in the bus
so the first case I want to talk about
is what I call the case of the taste
what happens here is the following thing
then we built and reactor about four to
five years ago we started on this effort
the state of the art racing system in
dotnet was a source though shalt use a
source because that's you know the best
thing since sliced bread and hot water
and so we actually subscribe to that
notion besides using trace source and a
source has this mechanism where you can
just talk to the trace source and just
log things and then it can go to
multiple taste listeners and so as we
got more
you know convinced that this was
actually working quite well we added
more taste listeners and one day the
system kind of plummeted you know the
QPS went almost to zero at some point in
time and so the thing we ran into is
that you know this deceptively simple
API that just allows you to log some
strengths has a lot of custom they need
it in case you use space listeners which
are not thread safe and this is actually
the code deep inside trace source that
was responsible for a pretty big service
issue that that particular day and so by
adding a new taste listener you can
influence this flag call to use global
lock and when you see the word global
and lock combined in one sentence you
sort of have to start thinking something
maybe maybe a float here and so we
actually started answering this part and
so all of the tracing was now happening
including going over all the listeners
because there was one listener that said
I'm not thread safe and from that point
on like all the access to all those
listeners which are IO bounds were of
course being synchronized in the process
so like you know if you have a machine
with 64 cores which we like to use for
massive parallel processing and you tow
in a global lock you could equally well
just and Windows 3.1 so you know nothing
nothing much happens there now to make
matters even worse you know the
formatting happens on a poor listener
basis so every listener will do staying
formatting under that lock and staying
formatting is not exactly the most cheap
thing to do so the things to be aware of
here is you know there are sometimes
deceptively simple API swear with a
little bit of conflict you know you can
sort of caused a major avalanche in
performance and I'm always very aware
when I see API is that they can you know
params array of type object array you
see boxing coming around the corner all
those kind of things which cause it can
cause major performance issues so the
short term resolution there was of
course you know to just you know disable
that taste listener which was kind of
throwing an engine in the whole thing
but more long-term and I'll talk about
it in a moment we actually moved
completely away to events source how
many no event source yeah so he found
source isn't much
more interesting system to do tracing
it's based on etw which also all of
Windows and that framework is using so
very robust system and I'll show you a
couple of examples of that in a moment
now to make matters worse we were also
faced with huge exceptions so exceptions
are an interesting thing because this
should be exceptional and if they happen
occasionally you want to put them in log
so you can actually find like what's
happening in my system so you definitely
want a log exceptions but the to string
method on an exception is of course
virtual and if it's an arbitrary
exception you don't know what that
virtual is gonna do for all you know the
to string is overhead and to just you
tattled sleep infinite right yeah most
of them don't do that
but you know the aggregate exception for
example is a notorious source of a lot
of allocations if you take a look at the
two string of aggregate exception it's
actually a loop that's kind of
aggregating all the end of exceptions by
doing single format one at a time so you
can sort of see like you know this kind
of this is more like you know a
punishment room for the GC like you know
like try to print this thing and and you
know a lot of stuff is going to happen
so of course we were logging those
suckers at some point and the fact that
we have millions of those queries
earning per second what happens is that
when a service fabric replica becomes
primary we load a whole engine of
millions of Standing queries that will
happily be evaluated when that replica
becomes secondary we unloaded from
memory because something else has you
know become primary it's in primary
elsewhere so we unload that thing and to
stop those computations we sort of
inject an exception into them to bail
out so we had a million of those
exceptions all getting a bail out
exception and of course they were all
being aggregated higher up in the unload
a sink showing an aggregate exception
saying computation 17 has been unloaded
18 has been unloaded million of those
guys and so we ended up we have a single
log file of 30 MB that's the threshold
for a single log file that we upload
into a log store a single log file
containing about only half of the single
to string output of a single exception
Pence's and so that was also happening
and being passed into that params array
on taste source so all of that stuff was
happening under that lock so at that
point you have like tons of replicas
running one gets unload it wants to log
its single exception and all the other
ones are just plain stalled not doing
anything because they just want to write
it's sunny weather today like something
very small into the log file but they
are all bouncing on that global lock so
you know knowing that there are some
things underneath you is sometimes
sometimes a good thing and so for
example in our system we don't allow you
know calling to string on system
exception anymore
we sort of have a repository of
exceptions how we want to print them
because in a lot of cases like printing
something like you know target
invocation exception well that also you
know has an another exception you want
to print the inner exception so you know
calling those virtual things like to
sting you know it's the return type role
that he says I promise you I'm gonna
allocate a lot of stuff and I'm not
gonna tell you how I'm gonna do it so on
top of that it's even worse so you may
want to like you know sort of look at
those things a little bit as well if you
have a high density system now how did
we switch to event source the essence of
event source is that you basically get
rid of this performance crime of making
everything into a string I kind of
compare event source to PowerShell right
like you know if you take a look at
classic you know scripting languages and
you know bash and CMD and all those kind
of things they're just passing strings
long and you need to parse them at every
single place so I'd like you need to gap
and set and all canned all that kind of
stuff you know what every single stage
instead if you can pass objects and
registers around you end up not
converting of these things you never
need to parse them again and you can
actually do structured queries on top of
them and so we found source is based on
etw if I'm tracing for Windows which is
a very low-level API and so by using
this event source class inside of not
you can actually piggyback on that
infrastructure in fact tools like perf
you are purely listeners to etw events
so as soon as your netw
you can actually start you know
senior events being ordered in time
amongst a whole bunch of other events
that are happening for example the
dotnet framework will raise an e TW
events saying oh I'm acquiring all the
locks in a concurrent dictionary for
some reason which likely can be a
performance issue but if you don't know
where that's happening in the context of
your application it's kind of useless
but then if you also have a lot of bTW
you can sort of see what caused this
thing to happen so we actually use a lot
of etw nowadays and one thing I would
recommend you to take a look at this
link to traces this actually allows you
to use reactive extensions to write
queries in real time or further ztw
events so you can actually do
aggregations in real time on top of what
comes out of etw now how do you do this
it's pretty simple you just inherit from
a class called event source and
somewhere in the system diagnostics and
namespace and then you just start
writing some methods and you call base
class methods called
write event now you can also do it more
sophisticated with a whole bunch of
custom attributes on top of these things
but this is kind of the basic way that
you would declare an event source and
then instead of doing something like a
source or to write line and then some
formats thing and the other yeah the
other you will simply call this method
so you will do my source dot log dot
request start and my source that logged
on to a stop now there is actually a
reason that I have this start and stop
pair here because if you start using
start and stop in combination with event
source you actually get to some new
feature in dotnet 4.6 which is called
correlation and causation so what you
see here this is a screenshot of perfu
this is actually a piece of code that
has been instrument to every cross start
and request stop and you can now see the
duration of all those requests as well
like you no longer have your code letter
- if stopwatch is everywhere that you do
stop or just start stop or stop and see
it elect what's the elapsed time here
you just log the start and stop events
and then you derive the time it took
from that and so that's how we feed it
into our telemetry systems and metric
systems as opposed to like asking people
you know just allocate a bag of
watches and just start timing every
single thing yourself now on top of that
the beauty of this thing is that you
also get nesting that's the new feature
in dotnet 4.6 is that if there are
starts and stops happening as part of
the same logical operation which
actually spans asynchronous calls as
well so if you do an async and a weight
you will not lose your causality tracing
anymore you will actually see this kind
of the crusty so you see that there is
an activity here called loop which can
spawn to fork join which then spawn the
request and so on and so forth so you
can now sort of trace that whole request
and see all the deltas and time between
any consecutive starts between starts
and stop so you can have the whole
timeline of the whole request you know
sitting right over here and so I'll just
give you a very small example of that
just to convince you it's pretty easy to
do here's an event source let me just
make it slightly bigger over here so
people in the back can see it it's you
know the most minimalistic demo I could
actually do here but you see I'm using a
lot of async and a weight over here just
to convince you that you can be jumping
threats all over the place on the
tadpole and what-have-you
this infrastructure will actually keep
track of the causality between all those
you know things that I'm doing here but
so the requests will actually of course
in reality you would take in the URI
over here but I do a request start and
request stop and those will be
correlated together and then deep inside
of this I'm doing an IO which for
example this you know load something
from disk and then I'm just you know
mimicking loading things from disk but
if in here I would be doing file i/o and
the file i/o subsystem in Windows is
also using etw which starts and stops
and Suns and receives you will see all
of that stuff being nested there so you
can actually see you know the windows
etw events for the file system being
sandwiched in between your etw event so
you can get a perfect you know causality
chain across all sorts of parts of the
stack including that framework CLR the
Windows operating system and so forth
so I'll just
gave a quick demo on how to to run this
thing I haven't even even saw his demo
don't bat here let me actually show you
what this thing does this thing just
starts perfu with a couple of parameters
the only interesting parameter it adds
over here is mics or demos activities
which is on my event source that's the
logical identifier of my event source in
reality you will have your own name over
here of course and I'm starting it to
collect traces and so as I start this
thing you'll actually see my application
run and perfu will be tracing will so my
application is running in the background
over here and now it's actually you know
collecting a taste and I'll just do stop
collection over here after a couple of
seconds because it's just a while loop
that's just you know keeping creating
requests and so in what perfu is doing
right now is it's actually taking that
ETL file event tracing for window ctw
file and it's actually analyzing it
right now and will actually start seeing
on the left the CPU traces off you know
everything that happened in my
application during that time and so here
we go it actually finished there's a zip
file here which contains ETL file and so
here you go you have now CPU stags and
all that kind of stuff the one thing I
can actually go to over here is any
stacks with start/stop activities and
this thing will actually show me do
start and stop events that I have now
etw a system-wide so you see all the
other stuff over here as well and I will
have to find my event source demoed
that's this process so I can also use
this to analyze the performance of
Visual Studio or any managed process of
course and you see I'm completely like
surrounded by even kernel events here
like you know a context switching events
and all that kind of stuff but you see
over here my request start and you see
all the individual requests over here
and if I open one of these let me go to
the call tree I can actually just drill
down into all
individual requests over here and I see
I have a request over here which was
doing a prefer perform IO which had to
start and stop event and those kind of
things so you can start drilling down
into like the causality of like what
caused what and you can sort of start
analyzing you know where the where the
time actually went okay and that's
actually pretty interesting if you have
components that sort of come together in
a big service you just ask everyone on
the boundaries of the component to do
start/stop kind of you know tracing and
at that point you can sort of see like
you know which component was responsible
for most of the latency in the system
for example now another example or
another case that we ran into is the
case of the stock finalizer interesting
performance crime as well so everyone
knows of the finalizer does it's you
know the thing in dotnet that you know
when an object is ready to be garbage
collected but it still has native
resources underneath it and the GC will
give it a second run you know to sort of
film the finalizar which can close
things like file system handles and
those kind of things
now what we actually ended up with is at
some point in time we saw the working
set on all the machine steadily growing
and so that's a disaster sort of coming
up so we had to take a look what was
going on over there and also the IO was
becoming incredibly sluggish like you
know Layton sees to open sockets were
sort of like increasing increasing
increasing all sorts of bad things are
starting to happen and so one of the
first things I tend to do when analyzing
such a performance issue is to look at
the finalizer queue in win debug and SOS
so like I had a dump file of one of
those machines being in a deep crisis
look at the finalize queue and seeing
over there like well there are 80 mm
objects ready for finalization and
they're not being cleaned up so we did
the garbage collector is currently in
Hawaii on the beach and not doing
anything but it tends to be pretty hard
worker so the the GC was there so we
actually took a look at the finalized
Abel objects and there you we had our
answer why
io was actually becoming very sluggish
we had like 627 sockets which were
finalized about holding on on to Nate
resources but never got you know that
native resources cleaned up
we had like 4300 Ted objects that were
still in memory even though the tats
were already that no longer than use but
you know they were still taking up one
megabyte of stack space all of them so
like the working set is is going away
there too and we are actually doing a
lot of stuff we have system reflection
Emmitt as you can see and conditional
week tables to sort of like have a week
a Francis to objects that we don't want
to stick around for a long time very
interesting data type if you're not
familiar you should take a look at
conditional week table but you know if
that thing doesn't get you know
finalized that means that there's
nothing weak about the week a Francis
anymore like everything is as strong as
it ever has been so you know massive
memory leak so what we found is then
that looking at tap number two which is
where the fine light sets on this
machine we actually found that there was
actually a lock happening you know just
waiting for some native resource on the
finalizer threatened so the finalizer
test was just simply stuck that locked
or locked in some way for a very long
time in fact trying to do something with
socket API is in a library called zmq
which we happen to use at that point in
time in fact you know we have to be the
beasts for it so we also found you know
the name of the developer that actually
caused this problem to happen was Johnny
we don't know who Johnny is because it's
an open source library from somebody
else but yeah so this didn't work out so
in this case we have to do something you
know to look at you know the bugs and
see rmq and actually try to get around
this but beware of native resources in
this case we were actually able to solve
it by simply putting a using statement
around some resource so that it doesn't
end up on the finalizer to clean it up
over there the result of that was of
course that I'd started locking you know
in the dispose call at some point so
like we have managed that's being
blocked instead but not to finalize a
queue so it was already a little bit
better because the finalize a queue is a
precious thing but you know so
ultimately we moved out this this
particular library but you know anything
to do with to finalize when you want to
be X
be careful about if something happens
over there now
once we solve that a couple of months
later he saw something similar and that
dll was already gone so we were kind of
baffled and yeah it can repeat itself
for a different reason of course we ran
into this a couple of months after the
previous one in this case for some
totally different thing what we have
here is something called autopilot which
is the infrastructure that Bing runs on
it's like you know some something very
similar to Azure but much much older
goes all the way back from you know to
the 90s so that thing has some clients
that can be used to sort of some
telemetry to dashboards you know to warn
people that something is going bad and
so we actually hooked up an event
handler to the unobserved tasks
exception on the task scheduler to
actually send information about
exceptions we didn't handle up into the
clouds now what's happening is that the
star scheduler an observed tasks
exception is actually running on the
finalizar
it's when the task that had seen an
exception which was not observed by the
application gets finalized which we know
that nobody can see it anymore at that
point in time this event handler is
raised on the finalize except to tell
you he is an exception that you haven't
observed so on the finalizer thread now
we are actually talking to an API which
encapsulates some HTTP client in a
synchronous manner so in other words
like the finalizer is now waiting for
those exceptions to go out now the
beautiful thing that happens here is
that because of that the thing that cost
is will some an observed exception on
some socket API this cost pressure on
the finalize of causing sockets to be
depleted causing us to have more
exceptions causing us to deplete even
more sockets so this was just a
spiraling effect for the system going
completely down so the answer to this is
of course well make sure that a you know
where this code is going around so in
this case we're no longer using this DM
client in that particular spot
the second thing is observe your
exceptions and so it's a pretty
important thing to do is to actually
make sure that you
not lazy about like handling exceptions
for a sink api's and there's one single
reason that you know a default default
in the CLR has actually been changed
namely this thing starting with a sink
and a weight being introduced the
default was changed for the behavior on
unobserved tasks exceptions and observed
tasks exceptions will no longer
terminate the process because in that
case people would not be able to use you
know a sink and a weight in combination
with UI applications where you have a
lot of a sink voice and a sink void
means nobody can see the exceptions
coming out of it because you just showed
them in the air
so this default has actually changed but
for server applications it's not the
best change in the world so I have to
say
so we reverted that thing of course you
know in debug builds in a service you
also want to be careful not to just you
know take down your service because some
developer forgot something like you know
the defaults are a little bit different
in some cases but so in the debug builds
we actually have this this behavior sub
to a very pessimistic mode again so like
you know we'll take down the whole
service in our pre-production
environment in case somebody forgets to
handle an exception so that's something
you can do to to deal with that but be
aware that there's actually many
patterns in the BCL that don't exactly
stimulate or like you can make it easy
to actually observe all exceptions
here's a good one which we actually run
into quite a couple of times
quite an idiomatic way of canceling
something if it takes longer than a
second the task that when any pattern
with some operation and then at all some
delay and so then you just look at the
outcome of that to decide whether or not
the first thing was hit or the second
thing but it was the time out of the
operation completing now in that case if
the tons of delay actually wins and that
who async is still going on at some
point and like nobody may be listening
to that task anymore nobody may have
attached the continuation there so you
may want to do something even if the
timer wins to actually observe the
exception potentially log it in a way
that you're not just doing to string
remember the first case
but you know you may want to do
something like this a little bit more
plumbing to make this work and to have
this kind of a delay now on top of that
in a lot of cases is food wasting can
also be canceled so I've seen a lot of
codes where people use this pattern
because MSD answer so as you use this
pattern but you forget that you can
actually cancel an operation that's
timing out so in this case the flu a
sink even two kind of cancellation token
so I can actually cancel that in case my
timeout wins I can cancel the foo a sink
to try to sort of and do it so you know
a little bit more code and this this
kind of cases actually just make a lot
of sense okay good now continuing on the
tasks will delay timers timers are an
other source of beautiful expense that's
everywhere okay like as soon as you have
time we're sitting underneath you and
you don't know how many there are you
look for like some roller coaster right
at some point in time so the timer's
in.net have been improved a lot over the
years actually but you know if you have
a lot of them you will still want to
know that something is going on deep
inside you know the common language
runtime to help you being woken up
because sometime it is expiring so there
are actually three methods that at some
points using perfu showed up as being
the high headers for CPU usage okay
three things the fire of a timer the
change for a timer and the closing of
timer
those three things showed up roughly 20
30 20 percent or something like most of
the CPU was going to those three methods
actually and so what's actually
happening deep inside the timer class is
that there is actually a global lock to
manage the timer queue and the stuff
that's happening under that global lock
is not super expensive it's just
updating a couple of pointers but if
you're actually doing a shitload of
those timers you end up in interesting
situations now where do those timers
come from good example it starts to
delay it's like if you have something
that you're trying to timeout and you
have thousands of requests per second
and for all of these you want to time
them out after 100 milliseconds or
something yeah it doesn't leak as per
second giving birth to a thousand timers
that all need to be started and all need
to be stopped at that point that
starts to be some contention on this
timer queue okay so in fact if you now
go to the reference source or you go to
github and actually take a look at the
source code for timers you see that
there is some interesting stuff over
here one thing and the perfect some
shion's comment in the code is we assume
that time is rarely actually do fire
okay
it also says over here that it's very
fast for inserting the leads but there
may be linear traversal than firing
timers so like in OBS also assume that
is relatively few of those guys so
there's a whole bunch of nodes here
detailing like you know the sweet spot
and the design that was chosen to be you
know the sweet spot at the time of
building the timer code now what's
happening with this timer artists are
still delay is that a lot of people in
this previous pattern that I showed just
a moment ago we're actually doing tasks
or delays and then the delay is actually
winning so the operation is timing out
which means that timers that are just
there for you know timing out something
they're also expiring and you never
cancel them even if the operation itself
wins the race between itself and the
time so like for everything that you're
timing out in the system doesn't mean
class per second all of them a timeout
of 30 seconds 30 seconds later is the
hell for you know the timer queue lock
because all of those timers still expire
and we didn't bother to cancel them all
right so like if you do this idiomatic
pattern where you can cancel an
operation because of a timeout and you
can cancel a timer because of an
operation succeeding it's better that
you double link them that you also
cancel the timer if the operation
succeeds because otherwise 30 seconds
later you're paying the price for all of
those you know timeouts starting to fire
as well so you know this dance doctrine
Annie is a good power but you know don't
make it a shallow pattern where you just
slap that in and then you know sort of
hope that everything will be good
okay so observe the exceptions try to
cancel things which are cancelable and
you of course you can abstract this
whole thing into a little helper method
if if you like to do so okay
another interesting case is acquiring
all locks
that also happened one day you can
actually see the codename of project
that we had in here so just squint and
they then she never saw that codename
but what was happening here was actually
using our X and the reactive extensions
but so what's happening here is that
somebody wrote a query and inside that
query they were using a concurrent
dictionary because concurrent dictionary
as well its concurrent right you know so
it ought to be a good thing I don't have
to worry about locks all that kind of
stuff and that's good but there are
certain properties on even data
structures like concurrent dictionary
that acquire all the locks what's
happening with concurrent dictionary is
that there's a lock per bucket
effectively or like well not exactly
pure bucket but you know this fine gain
locks so if some people are touching you
know the dictionary for key a and some
other people for B they may never see
each other you know acquiring the same
lock but there are certain operations if
you think about it well you know
something like counts it better freezes
all of the buckets in order to get a
consistent County property because if it
doesn't do that well you know you you
may end up with a negative command of
something or like something weird so
something as simple as like for every
event that was flowing just checking
whether the dictionary is not empty by
doing count is larger than zero was
basically making the concurrent
dictionary into a non concurrent
dictionary which is even way more
expensive than a regular dictionary with
a single lock on top of it because now
we need to acquire locks for every
single bucket in the dictionary and of
course this dictionary was more often
than not not empty so this check was not
even worth it so you end up in this kind
of interesting situation where 31
percent of CPU that day was actually
spent in the county property now you'll
actually wonder like it's a lock how can
a lock spend CPU doesn't the lock just
put the Travis leap and you know it will
be woken up and the lock is actually
satisfied well the answer to that is
locks inside dotnet framework have a
little bit of spinning inside of them
like we do a little bit of spinning and
takes until the next field point such
that in case you know we can acquire the
lock before the CPU is gonna be swapped
out anyway or
the trail is gonna be swapped out we can
sort of hope for the Synod a scenario in
this case the day was never sunny at
Seattle after all it's always Haney so
we ended up like you know just spinning
CPU cycles all the time to acquire a
lock that would not be acquired to it in
this quantum anyway another day ldiot
stopped which is not good for a web
service may be good for like you know
something that computes prime numbers or
something but in this case like all the
i/o stopped one day another beautiful
crisis of course another thing that you
may want to look at periodically if
you're sort of tuning your applications
and looking at you know applications in
production finalize queue is one thing
if that thing is unhealthy you can
forget all about it like you know it's
it's it's gonna be the end of your
career maybe not that bad but the tide
pool is another source of like precious
resources if you're tapped pool is empty
not much is gonna happen anymore so
here's a good example
one day the tadpole the completion port
sets were actually one three now what's
the completion portraits for when an
asynchronous i/o is actually finished
you know using overlapped i/o api's and
those kind of things attach from the i/o
completion port pool threat is actually
going to be woken up to serve the
continuation of the i/o so in this case
nothing nothing happens anymore and
nothing could happen we couldn't serve
any iOS been completed anymore that
includes taking an incoming request
sending outgoing i/o requests you know
hitting the file system normal that can
can work anymore okay so what happened
here is the following thing again
beautiful stuff you can already see at
the bottom you know I want to appeal it
anymore another place where we were
chasing exceptions in this case we were
not doing doing that it was WCF doing it
we had a WCF setting in Ableton
production system that says like some
details stack trace information to the
caller when something goes bad for
debugging purposes very expensive thing
because when an exception happens it
needs to do is stack walk try to read
symbols all sorts of things need to
happen to get detailed diagnostic
information
if this thing gets into a production
system and things start to go bad you
can start seeing things going bad really
fast because well one of those guys
being surfed and trying to send the
exception back to the collar may
actually cause you know some cat to be
blocked for a certain amount of time the
second request comes in is also starting
to timeout cause an exception you can
see this kind of snowball effect
happening again so beware of your
settings again this thing is a single to
false boolean flag inside a config file
that cause major major kind of things so
take a good look at things that are
explicitly described in a mass DNS use
this for debugging only like you know
that flag says use this for debugging
only so don't ship that into the
production system like you know it's not
like it's a liability or something it's
not in the small print you know it's
even in the big pants so you know be
careful of that now what's happening
here is you see stack traces that's
something you may want to do
periodically as well we actually have a
system that periodically takes a perfu
trace and stores it and analyze it
offline you can actually do that using
CLR MD just let perf you you know take
some traces and so on and then analyze
them later but one of the things to look
out for is Mon contention which means
monitor as in system doctor adding
monitor contention that's actually the
ztw events around it as well which feeds
this contention counter inside the
windows performance counters but over
here if you see a stack trace containing
this the word contention then well
things are quite contentious obviously
and so what's happening inside this
contention thing is the spin weighting
stuff that I described just a moment ago
so even though all those cases in this
this presentation seem to be separate
they sort of all have a boundary
condition the previous thing was about
spinning here's you know the explanation
on how this spinning actually happens so
you can actually find that source code
on github as well if you want to see how
the spin weight works but here this deep
insight over here you see try spinning
and yielding
before eventually blocking there's an
arbitrary number in here lots of
software has arbitrary numbers so in
this case you know there will be ten
repetitions of spinning before we
actually put it rather sleep and so
that's what's what's going on over here
in in the contention coach but and so
again because we were doing something
expensive namely placing exceptions
which requires acquiring locks to read
resource tables to format the exception
message in the proper language of the
system like all that kind of expensive
stuff that we didn't need in production
system because the whole bunch of lock
contention causing more timeouts in the
system and so on so you see those things
sort of like you know shooting
yourselves and the food really really
quickly in that kind of case so be aware
of like you know debug only facilities
don't have debug only facilities in a
production system yeah that was actually
a msconfig kind of thing using
Singleton's another beautiful source of
things may not apply
well actually this one will apply but I
have another one that may may be a
little bit more specific to our workload
but this one is a pretty easy one just
pointing this one out things like
programs arrays for example you know
those are a locators starting with C
sharp 6 the c-sharp compiler will
actually met more efficient code when
you call something with a parameter a
and you don't specify any arguments that
go into the params array it will
actually met code using a rate of MT of
T which is a singleton instance of an
empty array okay and so if you at some
point you know are inclined to do like
new empty array you know somewhere you
may want to know that there are
singleton types available so for example
there is this thing called type dot
empty types which has been there since
of not 1.0 there is a raid on empty of T
for an empty array singleton and their
stars dot completed tasks as opposed to
do in return tasks from his old Co
everywhere you need to return a complete
a task
so those Singleton's are really useful
because they sort of reduce background
noise now if you're at the point of like
reducing this kind of
background noise likely you've already
graduated in the school of performance
optimization but you know those things
if they're everywhere they can start
showing up like I've had cases of the
core event processing engine where you
know the cpu sample showed like
allocations of a race but this thing
should not be allocating anything it was
just actually calling some member with a
parameter a before c-sharp 6.0 you know
and it was just allocating empty erase
all over the place
and of course you know if you spend five
percent of time in GC may not be a big
deal but again five percent of
processing billions of events means like
not processing stuff while the GC is
happening so if you want to go for
low-hanging fruits there's a lot of
those small little things that you may
just not be aware of existing like tasks
completed tasks for example okay and
Singleton's that's the other one I was
talking about that may not be that
relevant but this is something you
should be aware of
beware of putting like static single
tones inside open generic types because
you will have an instance of that single
tone for every closed instantiation of
the type so in this case there will be a
few instance for a bar of end bar of
double bar of staying bar of date/time
like because it's inside a generic type
and so it's often too easy to sort of
forget the context where you are like
you know let me put it statically the
only field here with shared instance but
then that's sitting in an open generic
and in our service well there we have a
lot of T's because every event schema is
a unique T and so we are doing billions
of events you know that we're processing
or like thousands of events per second
that we're processing with lots of
different you know schemas so if we end
up having an expensive single tone like
let me cash you know the first billion
prime numbers here because I will need
them all the time for something and you
do it in a generic type well then you
will have a billion prime numbers per
type right at some point and you don't
want to to do that as well so be aware
of this little caveat another one
subtle sources of boxing now we have
sort of had you know little things that
sort of show up as background radiation
the previous things were oh my god the
whole system came too
halt like what's going on here this is
you know when you're over that hump or
ideally you're never there and you start
looking at the system and you come to
the conclusion oh there's a lot of
background stuff happening here and you
may want to look for those kind of
things here's a good example calling
something like 'get hashcode and of the
equals on an e new causes boxing you
know beware of those kind of things like
you know you may actually end up causing
allocations there and so the right way
of writing something that's I equatable
you know to check where the two things
are equal is actually to use the
Equality compare of T to default and
call the equals and get hashcode methods
on those kind of things in fact if you
look at the code generated for things
like anonymous types and so on it
actually uses equality compare of T and
if you want to score a little bit extra
points on your team and you're not using
net score yet even consider storing that
default result into a static you know
variable that you sort of use as a
shared variable because they're all to
default by itself also has some expands
in it and that's often done at core you
know was pretty involved change at some
level actually but but you know it's
something to be aware of that when you
build those things that have deep
equality use equality compare of T it's
there to help you like because in fact
you know this kind of code is playing
the angels as well the previous code say
this is not an in but it's not
Willingham or something and you just to
get hash code or dot equals or something
well know the reference exceptions are
just around the corner right and so
doing it right you know also gets you
better performance in this kind of case
okay here's another example this is an
async lock it's actually on the DPL blog
I believed the Stephen Pope actually has
at least this one a while ago as an
example of how to build a sink in a
synchronization primitives this is a
pretty common use case if you like to
use the using statements to have some
kind of a block like you know using some
timer you know
or something that will measure how long
this block takes consider actually
making your disposable object is struct
the c-sharp compiler is smart enough not
to call the dispose method to the
interface which will cause boxing it
will actually cause I call the dispose
method on the struct itself knowing that
that's the one that implements I
disposable so it will avoid the boxing
altogether there so if you have a using
statement you will actually end up you
know calling you know a non-virtual
method here on the release our struct as
opposed to making this thing a class or
you know whatever so this is a small one
read-only but with caution once you
start using more structs and you know be
careful with structs especially if
they're mutable don't do mutable structs
but here's a good example I've seen
cases where people say oh you know I see
a field and a code review right you know
and it's a field that's only touched in
the constructor let me put a remark here
to the developer please mark this thing
as read-only there are certain stocks
which are mutable a good example is the
numerator struct of list of T now this
is just to drive home a point the
enumerator struct on lists of DSS
structs such that for each loop doesn't
cause any additional allocations if you
do a for each over list of T it will see
oh this thing implements the enumerator
pattern it has a Getti numerator it has
a move next it has a current all that
kind of stuff I don't need to go to the
ienumerable interface which will cause
the allocation of an enumerator on the
heap which is an additional object on
the heap that's not needed because we're
just doing the enumeration and in the
context or for each loop so it's a
struct for performance reasons but it's
also a mutable structure it contains the
cursor inside of it so if you end up
storing this thing in a read-only field
then updates will never be persisted to
it like you know we'll just make it
immutable and you know the enumeration
will just be stuck and not not move
forward that's just to say mutable
structs are evil but you know sometimes
when you do real low-level performance
tuning kind
of things immutable struck can be quite
useful in fact all of the value tuples
in c-sharp 7.0 are mutable structs so
like this plain advice of like never use
immutable struck twelve Never Say Never
right are certain cases where it does
make sense but then also be aware that
certain certain things like marking
things he only all over the place may
not actually preserve the correctness of
your code and that brings us to
immutable data structures our system in
bang actually uses a lot of immutable
data structures now how many of you have
actually played with system collections
immutable like the immutable array type
and all that kind of stuff in fact
rosslyn the.net compiler source code
uses an awful lot of immutable design
and the beauty of of immutable objects
is of course that you can share them
across multiple threads and you don't
have to have any price to pay for
synchronization now this of course like
a flipside to the metal because well if
it's immutable and you want to change it
you need to create a copy so you have to
be careful on how you make those copies
the main thing here is actually to just
raise awareness that there are immutable
api's already in existence that you can
start using good example is the
immutable collection stipes if you for
example have an array and it will be
rarely changed and you don't want to
have any costs in acquiring locks on the
array in order to read from it so that's
like it is a concurrent update happening
you make it like one cell update it and
the other one not updated if you want
consistent reads but you don't want to
pay a price for synchronization then you
can actually start using immutable
collections where the mutation will
create a new copy of the collection and
just replace the reference from the old
copy to the new copy
so there's immutable collections i've
beautiful and in fact if you're in
visual studio and you're just jamming on
the keyboard right
Rozlyn is allocating new data structures
for every keystroke like it's just
creating new ASDs all the time and a
memory efficient way by trying to reuse
as much of the existing data structures
that it can and that's the other beauty
immutable collections lead
a lot of reuse if you have say an
immutable array and you just want insert
an element well we can actually split
the array you know and we can sort of
create two segments of it and sort of
create the illusion of you having a new
consecutive array without copying the
whole thing like we can just copy the
head of it all the way to your mutation
and then just point to the remaining
tail of an existing array so we can
actually share all of that stuff in
memory and so immutable collections can
help you to actually get rid of some
some expensive synchronizations
and so a good example of that is in our
service we have a lot of configuration
that barely ever changes okay a
dictionary of staying to value a typical
kind of thing a lot of things can be
optimized with dictionary of staying to
value like if they never need to be
updated well don't make them into a
concurrent collection for example like
you know because then you have a huge
amount of you know lock setting
underneath you that will be there in
case there's a lot of writes happening
all the thing
it's a read-only collection for heaven's
sake nobody's gonna write to it so then
don't use a concurrent collection just
because once on a day there may be an
update to that collection like use
something else like know the frequency
at which reads are happening writes are
happening if like 99% of the time it's
reads then optimize for the reads and
don't slam in like some concurrent data
structure then there's no concurrency to
be expected on the right path for
example so I'll give you an example of
that here's something that happened a
couple of months ago and just take a
quick look at time here yeah I still
have five minutes or will be just in
time so this is an example where we had
a configuration manager at the
configuration manager would reload
changes dynamically from some underlying
source it's an abstraction over a
configuration system in our case it
would listen to an event handler and
service fabric saying somebody has
deployed a config update the code keeps
running we get a notification the config
has changed you can refresh the config
and adapt your service to just while
it's running
pick up the new config like beautiful
system
and so in this case you know this
configuration manager was encapsulating
all of this kind of stuff and there was
a lot of frequent accesses to get you
know the latest configuration settings
now there's already something you have
to think about there if you have config
updates how quick do you want them to be
seen are you gonna say in nanosecond so
you can say milliseconds I gonna say
seconds well in a lot of cases the
limiting factor is the human being able
to hit the button it's maybe because the
system is in a crisis so you want to be
it as soon as possible to be picked up
but nanoseconds are not gonna make the
difference between the human like you
know getting his coffee first and I'm
saying let me let me fix the system over
here like so you can actually allow a
couple of milliseconds to elapse before
the convict gets picked up and so what
what's happening here is that the person
right in the configuration manager had
already done a lot of reading and said
like wow look we can use this
reader/writer lock you know because you
know these are gonna be frequent rights
are gonna be infrequent and so like you
know the cost of reading should be less
than the cost of writing blah blah blah
and so like you know he or she actually
used the reader/writer lock around a
regular dictionary so already kind of
better than using a concurrent
dictionary like you know it's like one
step up from that because concurrent
diction is very expensive in case you
know it's ideal if you have a lot of
concurrent rights but if you don't have
a lot of concurrent accesses to it you
know there's no point so it's it's kind
of better here but then doing some
performance analysis on this thing the
person concluded that doing a regular
lock as opposed to reader/writer lock
was actually faster well he the writer
lock is there to assume that at some
points there will be rights and we sort
of want to amortize the cost for all the
reads and so on but you know there will
still be rights at some point and you
know there's a lot of bookkeeping to
happen by the if the lock has to be
upgraded you know from shake and all
that kind of stuff like so there's
bookkeeping happening in there and so we
looked at that thing and we said well
why do we need this we already have a
field which contains a dictionary and
every single read is gonna read that
field and then go into that dictionary
so then a convict
change happens just prepare a new
dictionary it's a dictionary of like 20
bytes for not chanted like hundred bytes
or something very small dictionary like
why try to protect against mutation
inside a single shared dictionary if you
can just swap out the dictionary for
another one so we just said well let's
just change it like this we have our
addiction it's no longer a read-only
field and when the conflict change lands
we'll just create a new dictionary take
our merry time outside any kind of locks
to prepare the new dictionary reflecting
the configuration and we'll just swap it
into the field over there using the
interlocked operation which will also
cause like flushes and those kind of
things and so you know very simple thing
like you know the most naive kind of
solution okay there's some interlocked
stuff here but whatever but you know as
opposed to putting a locker and all of
the single reads and to a dictionary
that will be basically a publication
pattern like you prepare it and then you
share it with the world and you never
mutate after you share it very common
pattern like instead of doing that let's
just you know substitute the dictionary
okay
that's what we did over there very
simple trick you know God has a long
long way
actually now to wrap up I'll just do a
key takeaways for ya okay actually
that's just the key takeaways you know
on this one and and we'll end over here
one interesting thing this talk is
something I've been giving in a couple
of presentations and different
conference it's just like my C sharp
internal stock yesterday
if you actually look online I think you
know there may be already one conference
that has this stock posted and it's in a
different order I sort of tried to
mingle the order so that like you know
if I don't get to the end here in
another conference the things that were
at the end we're at the front so you can
sort of like get all of the stuff anyway
so the key takeaway out of this thing is
okay we have a configuration manager we
abstracted overheads we created a
beautiful I configuration abstraction of
ilogger abstraction but it sort of
contains a lot of cost inside of it
because what's really happening is that
when service fabric tells us the
configuration has changed it just gives
us a new config object
so if you would just do it naive without
some platform abstractions because at
some point we may move to a different
platform
it would just be storing that config
object in the field and just reading
from it all over the place now this
abstraction actually caused us to
introduce some additional kind of
locking to copy things out of that new
configuration object into our own object
model and then you know require a cost
in doing so so there's some cost to
abstraction you may want to have that
abstraction in place we decided to keep
the abstraction but be a little bit you
know
wary about like all those little
abstractions that you put on top of
things like you know the eye counter the
eye lager the eye configuration if you
build those kind of wrappers make them
efficient because those are gonna be
sitting on the critical path everywhere
you know for every single request you
may be going to those little
abstractions which are there because
maybe in five years when your product is
irrelevant anyway but you know you were
sort of thinking in five years this
tingled have to move to a different
hosting infrastructure and we want to be
ready for that well believe me at the
point you need to move to another
hosting infrastructure and it's likely
not gonna be the worst of your concerns
you'll have a lot of other things to
look at and so this little configuration
you tell these and so on
think twice you know like think think
carefully about them okay so if that you
know I'm at the end of my talk if you
have any any questions you know follow
up and so on please feel free to come to
me I'll be around at the conference for
the rest of today and tomorrow actually
and I'll thank you for coming and please
come to me with any questions okay thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>