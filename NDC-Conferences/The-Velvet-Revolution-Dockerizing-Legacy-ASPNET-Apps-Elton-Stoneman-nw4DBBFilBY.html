<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Velvet Revolution: Dockerizing Legacy ASP.NET Apps - Elton Stoneman | Coder Coacher - Coaching Coders</title><meta content="The Velvet Revolution: Dockerizing Legacy ASP.NET Apps - Elton Stoneman - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Velvet Revolution: Dockerizing Legacy ASP.NET Apps - Elton Stoneman</b></h2><h5 class="post__date">2017-04-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nw4DBBFilBY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so Daka is this fantastic technology
that emerged in the Linux world it's
bent
evolving and maturing in Linux and then
at the end of last year it landed in
Windows Server fully featured fully
fledged ready to go TACA doesn't care
about what sort of applications you run
but today I'm going to focus on
traditional dotnet apps by that I mean
old applications at the time plop I'm
going to use an asp.net webforms
app I'm wondering we're talking about
today is a to start with an overview of
the docker platform so for those who
aren't familiar with it very quickly run
through the major features it does a
huge amount of stuff I'm going to focus
on the main workflow for bringing your
apps into docker and what that does for
you then I'm going to take my web forms
application I'm going to get it running
in docker and show you what you need to
do to make your app running docker
without any code changes you can take it
as is run it in the new platform and I'm
going to use that as a jumping off point
to modernize my application add new
features new capabilities quickly and
safely by leveraging what we get from
docker I'm not going to talk about micro
services DevOps the cloud donate core
nano server or Linux so big list of
technologies and approaches which are
all excellent and actually docker fits
in really nicely if those things are on
your radar but the sort of apps I'm
talking about other the millions of user
facing apps that are running all around
the world that probably are not going to
fall into this into this space because
to take any of these new new trending
technologies new approaches is a Ryoka
textured job it needs a lot of
investments take your app and bring it
into this world but by using docker you
can get a lot of the advantages without
having to do a full we are Kotecha so
I'm going to start with my really simple
basic frankly rubbish demo application
which runs on my laptop to start with
it's in web forms and the sequel server
database and they run natively on the
laptop and the first thing to do is port
them so they run in docker so I've got
one container for my web application and
another container for my sequel server
database and docker cake takes care of
the plumbing to make sure all the
components can talk to each other so
there are advantages in just doing that
but that's my starting off point
so then iterate over my application and
through the rest of the session I'm
going to be adding more features until
we get to version 5 which is a fully
distributed solution it looks like a
pretty
this jumble of containers but every
single one of these components is there
for a reason
and the modernization approach that I'm
taking is feature driven so I'm adding
features to my application quickly and
easily by using docker okay so the
starting point so docker is a platform
for packaging distributing running and
managing applications any type of
application as long as don't have a UI
we're talking about server-side
applications here web websites web
services console applications that run
on a schedule or anything like that so
in the windows world it runs natively on
Server 2016 and Windows 10 and it runs
in the background as a Windows service
so if you're moving your apps to docker
what you have running on your VM or your
bare metal server is just docker you
don't install the.net framework you
don't install what is in store anything
else
this is bare minimum all you need is
docker to be running and everything else
is going to be running by docker so to
make your app running docker the first
thing you need to do is package it up
that's part of the platform the notion
of been able to take a whole bunch of
different components and package them
into one unit so for an asp.net
application the way you do that is by
building up a script that describes all
the steps you need to take to package
the app so I'm going to start with
Windows server core because I've got an
asp.net application I just didn't give
me the full Windows server features then
I install is this or now install asp.net
and that's going to give me my kind of
basic hosting environment then I copy in
my own web application with whatever
dependencies it's got and then I need to
configure it also all runs and it's all
set up and ready to go so in the docker
world you you capture all those steps in
a in a really simple script file called
a docker file which we'll be seeing
shortly and then when you're ready to
package your application you run a
command like docker build and dock it
goes through all of those steps runs any
commands you need to run copies any
files you need to copy and the output is
a single binary logically a single
binary which contains your entire
application stack the whole thing your
app all its dependencies the runtime
environment even the version of wind
is that you need to run it on is all
packaged into one unit okay
and that dockerfile is really simple so
there are a few new new instructions to
learn but actually it's a really simple
way to take an existing application and
run it in the new platform so we'll see
more of this during the session but if
you've never seen the docker file before
hands like we've never seen a docker
file before
cool so even if you've never seen it
it's fairly straightforward there's only
about half a dozen instructions you need
to learn and so we'll go through this
one by one we start with Microsoft
Windows server core so what that's
saying is this is my base this is that
this is another docker image that I'm
going to use as a starting point for my
image this happens to be Windows server
core which is a clean up-to-date
installation of Windows Server 2016
without the UI which is owned by
Microsoft and managed by Microsoft's
publicly available so when I'm going to
build this image the first thing doctor
will do is start with that image so I
get everything that's already been built
by Microsoft next command is just a bit
of boilerplate to tell doctor that from
now on in this Dokka file I'm using
PowerShell for my instructions and then
I'll start building up my application so
I run some PowerShell to add windows
features that I need so I is and the
asp.net and the.net framework and I
remove the default website gives me I
don't need the default website then I
copy in my published web application so
in a CI pipeline my build has already
run I've generated the published web app
with all the all the binaries and all
the content before I do this step and
it's the packaging step that copies in
my web app into my image and then
another bit of PowerShell to set up my
web application so I know exactly where
it is I know the physical pathways are
just copied in so I know where my
application is going to reside so I
create a new web site and I specifies
doing me on port 80 now when I run this
application is docker by default the
container is locked down
then you're not allowed to communicate
with it from the outside world you have
to explicitly enable that so the next
amount exposed it's like a really basic
firewall it says this application is
allowed traffic to come in on port 80
and the final thing is we tell docker
what to do to start my application so
when I run my application using docker
it's going to the entry point is telling
it what command to run to start
everything up which is the basic script
is going to start is and make sure that
it keeps on running so we'll spend more
time with this but this is all you need
to do to take an existing web
application and moving to docker so when
I've got my docker file I've run it
through the docker build command the
output is my image so that's a single
thing that's probably on my laptop or on
my build server I need to be able to put
it somewhere else so that I can run it
on my test environment on my production
environment so the notion of
distribution is built into the platform
so when I've got my image on my build
server on my laptop I can run docker
push and push that image up to a central
location docket calls that are registry
now that registry could be a private
registry in my data center that is only
for my company to use or it could be a
public registry and doctor have their
own public registry called docker hub
the fact that you can share software
publicly if it's open source software is
a massive enabler to start bringing
tried and trusted Enterprise break
components into your own solution which
is what are we doing today
so docker hub is one of the reasons why
docker has taken off so much in the
Linux world so at the end of last year
there are half a million images on
docker hub half a million packaged
applications you can bring down and run
most of them on Linux but the number of
Windows ones is increasing and it's
absolutely taken off so as of as of last
year there are 8 billion downloads of
these images and you think about these
images are a packaged application that
means 8 billion times
someone's downloaded those applications
and got them running in their own
environment without having to do any
other set up work we're currently
running at about a billion downloads a
month so you know it's pretty popular
stuff so when you've shared it and
you're ready to run your application
that's all built into the platform too
so you do a docker run command you give
it the name of the image that you want
to run and because it's all built into
the platform the notion of images and
registries if the image isn't available
locally docker will go off to the
registry and pull it down for you and
then it runs that application from the
image in a container and a container is
just a really lightweight boundary
around the processes that are running
inside your inside your application so
in the case of a full dotnet framework
app running on Windows Server we'll see
the processes later it's going to be the
worker process the w3 process that's
hosting my application so I've got my w3
process running on the host with all the
supporting processes it needs with a
boundary around it to isolate it from
anything else that's running on that
machine it's not like a VM where I have
to allocate CPU and allocate memory it's
just a process running on my machine and
I can run as many as I can as machine to
handle so not only is it really
lightweight it's also portable so I can
run the exact same docker run command on
my laptop and I'll get exactly the same
application the whole thing is
configured and ready to go so it'll
behave in exactly the same way no matter
where I run it if I spin up a VM in the
cloud like I said I wasn't going to
mention then it'll run in the same way -
okay so that's the basic workflow that
you get with Doc Rivers the basic
features of the platform you can build
ship and run any type of application and
it'll be the same no matter what
environment you run it in so now we're
going to cut to the demo so my demo
application it's a it's a it's like a
marketing website for a new product
launch so we're going to step into that
now and see version 1 which is running
natively on my laptop
ok so let's fire this up so I'd only get
hopes up and like it's rubbish but the
point is the code is all of them github
right now so you can check this out and
you can you can follow along with me to
see what I'm doing so this is running on
is on my laptop there's a little launch
page and there's a big button to sign up
to say I'm interested in your product
here are my details let's click that
properly and it please send me some
emails so I've got sequel server on my
laptop I've got is on my laptop I got
Nate installed if I want to run this on
the new environment if I commissioned a
new test environment I need to install
is I need to install dotnet framework
the right version I need to have access
to a sequel server and all that stuff
and that's probably gonna be in a big
complicated deployment document with
loads of screenshots that's totally out
of date and never updated so it's not a
great position to be in
so what we're going to do is move this
to dr. listening our first point of call
so what I've done because I'm trying to
do five versions of this app I'm not
going to live code I've got it all
checked in already in different get
branches so you can follow along with
this yourself and check out version two
and I'll go back to my product launch
website and show you what I've got here
so the the code for this is really basic
I've got entity framework which I'm
using for storage to talk to sequel
server I've got the main entity is this
prospect which contains all the details
that you put into the signup form it's
just an ordinary plain old object that
it contains these various details and
then my web forms application has got
the main pages this signup page there's
a whole bunch of boilerplate stuff which
not going to change throughout the
session but the interesting stuff is
when you click the Go button so when you
click the Go button it does exactly what
you'd expect it spins up a new prospect
object copies in the data from the
webform and then it uses entity
framework to make a connection the
sequel server it looks at the dependent
entities the country free select from
the drop-down and your role and then
adds it to the prospect collection and
saves the changes so as with version one
this is a synchronous connection into
sequel server actually going to make
three query to look up the details and
save my new entity and this is this is
how we're going to use it in docker the
only thing that's actually changed
between version 1 and version 2 is a
configuration setting so version one I'm
using sequel server on my machine this
is version 2 now the only change is that
the hostname for the server because I'm
going to be running sequel server in
docker now part of the doc of platform
it has a built-in built-in DNS server so
when I could talk to containers I can
talk to them by their container name so
when this applications running in one
docker container and I say I need to
sort the sequel server on a machine
called sequel server that request goes
the DNS lookup goes to dakka dakka finds
as a container running on this machine
or a different machine in the same
docker cluster and it will route the
traffic appropriately so the only
changes the configuration change to say
I'm going to run that this is this is
the name of my host which stock is being
resolved to a container for me
okay so I'm going to reset my
environment to get rid of version 1 and
start building this and we'll look at
the other pieces now okay so as well as
the full visual studio solution I've got
a whole lot some assets that are in a
docker folder which is how I'm going to
actually package this thing up and run
it in dhaka so from my website I've got
my docker file which is broadly the same
as the one that we saw earlier so I'm
starting from doing the server core so
that's going to give me my clean
installation of Windows I'm switching to
PowerShell so from now on I can run
commands in PowerShell and then I have
to do a bit of a hack to turn off the
DNS cache in windows because dog has got
its own DNS server it expects you to
always ask it for what addresses so that
you can resolve it to the correct
container running on the correct machine
and Windows is a bit too aggressive with
the way it caches DNS responses so just
a simple bit of a registry hack to turn
off the DNS cache then I install is and
asp.net and then I remove the default
website because I'm only going to have
one application in this container
I could have several websites in my in
my docker image but that's not really
what doc is about doc is about having
one container per application so I can
stand independently and releasing the
pendant lis and all that sort of stuff
so then I copy in my my built website my
published website into the image and I
run new website to set that up in is
again I know exactly where it is I'll
just put it there I'm using port 82 to
listen for traffic exposed port 80 so
traffic can get into my container and
then I need to tell doctor how to start
my application and this service monitor
app is a little XE from Microsoft that
they shipped with some of their images
which start a Windows service for me in
this case it's the is service and it
will monitor it and check that it's up
and running
so when docker starts an application it
watches the process that it started and
if it fails it will flag up to the
platform that containers gone down so
what this little this little helper app
does is it just makes that that flag
possible so if there's a problem inside
the container and is stops responding
this will bubble that up to docker
so in terms
kind of CI process this is my this is my
CI this build script here I've got an MS
I'm using MSL to publish my website so
I'm running that from my from my visual
studio solution to publish that web
application and then I run docker build
which is going to go through my docker
file and package up all those different
components okay so that build was
completed ma'am so if I run back through
my build script I've got all the output
for Emma's build packaging and moving
everything around and this is the output
from docker so when I run docker build
each of the steps in my docker file
gives me some output docker tells me
what it's doing now I said that docker
images are one binary file which isn't
true logically there one thing so I
refer to a docker image by one name and
I've got I get my one image but
physically that there's they're layered
they're split into separate layers and
those layers can be reused so what's
happened here when I built this
application docker said that I've
already got of the others that's the
same as you've asked me to build so I'm
just going to use that so it's a really
efficient way of having lots of
applications that share a lot of common
features they would share the same
layers so you don't use up a lot of
storage here because everything all the
big base layers that have got windows
and is would get reused throughout all
your applications and then at the end
I've got mine you can take that my new
image and it's going to be ready to go
but it needs a database so what I will
do is go to my cheat sheet to make sure
I'll run the write command and I will
start sequel server running on my
machine in a container okay so what this
does is it's the docker run commands
that I mentioned earlier and docker run
get rid of that because that doesn't
look good
so I'm docker running a new container
the D flag means detached so puts it in
the background
it's a long-running service this
database is going to keep running in the
background and Pia is exposing a port so
that I can get into sequel server the
image name is this so that's the image
that's also on docker hub so that's
sequel Server Express running in Windows
which is owned by Microsoft and is
publicly available on the hub sequel
service price is free
there are certain limitations but
actually they're not very severe and
it's a perfectly good and usable
relational database so I can pull that
in the hub and I can be using that I've
given it a name let's go back to that
so given my container in name sequel
server that's how the other container is
going to find it and I'm also specifying
some environment variables so I've got
an environment variable for to accept
the License Agreement and to specify a
password for the the admin user so if
you think that having that password in
plain text is bad you're absolutely
right and there are much better ways of
doing that in docker but to get through
five versions of my application I'm
doing it the quickest way possible so
what we've got a flag is doing is
passing a value for an environment
variable to the application and in this
case it's sequel server and when it
starts up it's going to pull that value
out of the environment variable and
setup the VSA user to have that as their
password okay so if I look at what's
running in docker docker PS just tells
me all the all the containers that are
running I've got one container now it's
running from sequel server it's got a
random container ID but it's also got a
name because I gave it a particular name
so now that's my effectively that's my
database setup and ready to run that and
so I know I can run my application so
docker run D the webserver it's going to
be long-running so I'll put it in the
background - P - make that port
available so pull 18 and I'll give it a
name so that I can refer to it more
easily and then the image name which is
whatever you decide to call it so in my
case 6 IDs is a bar be chosen home hub
the gears when you publish things to
docket hub you have to use your handle
and then the name of your application so
in this case is 6 I'd product actually
use a short shorter name product the
launch web v2 v2 okay so that v2 tag is
interesting because I can put versions
onto my images so I could have lots of
versions of my product launch website
image on the hub v2 v3 v4 v5 which I've
actually got and you could you can tag
them with whatever you want so out of
your build process you could target with
your your commit ID and you can track
everything back down to make sure that
you your several versions of the same
app
pation but they're all stored and you
can run any particular version at any
time okay so that's running that now
I've got my product launch websites and
my database server running in separate
containers if I want to access that that
website from on the host I can use
docker inspect and give it my container
name to find out some details about my
container and that tells me among other
things that port 80 is available and
this is the IP address of the container
so this is like a kind of virtual IP
address the docker assigns so inside the
container let's paste that in inside the
container the application thinks is
running on its own server it's got its
own version of Windows it's got hostname
it's got an IP address it looks like
it's got its own server when I first hit
this is going to be setting up it's
going to take a minute as you can spin
up because it's setting up all the stuff
this is a completely empty environment
I've got a new sequel server instance
so when entity framework finds that it's
going to create my database deploy the
sequel schema putting all my insert on
my seed data and all that sort of stuff
and then I'm also going to wait for my
worker process to spin up in the
container so that's that's up and
running now if I want to see what my
container is doing I can use more docker
commands like docker top on my web
application or web container and that's
going to tell me the processes that are
running inside the container it tells me
how hard they're working so in this case
I've got my the main thing I'm
interested in is my w3 worker process
which is using 180 Meg of RAM and there
are a few other background services
which is the bare minimum you need to
get your windows app running in a
Windows container and they're using a
few megabytes each so my whole is server
my whole web server is running in about
190 Meg of RAM to be able to serve my
content so and I'll go through and
actually use this at this time so I
click on sign up the apps exactly the
same I haven't changed the code I'm just
running it in the new platform so I can
go through and fill in some details that
hopefully are already in my cache you
know teaching them company name doctor
and roll I don't know why I'm putting to
read it like it doesn't really matter
but those are my real details okay so
clicking go runs the code that I showed
you earlier synchronously causing to my
database I don't know what's happened
here because I don't see in any output
in the web application but if I want to
find out what's happening in that in my
sequel server database I could do the
same docket in spectrum and let's run
docker inspect sequel server and that's
got an IP address - and it's got a port
exposed so I could connect this sequel
server instance running in a container
using sequel server management studio or
Visual Studio or any other sequel client
because it just looks like a sequel
database just happens to be running
inside a container but also I can make
use of docker to run commands inside the
container from the host so if I want to
find out some data in that container
without having to fire for sequel client
I can run this command which is too big
for me to remember which is why I'm
pasting it in and what that's going to
do so I've got dakka dakka exec is going
to execute a command inside the
container called sequel server and the
command is going to run is some
PowerShell to invoke a sequel query so
that sequel server docker image that
Microsoft open as well as having sequel
server installed it's got all the
powershell commandlets installed on that
image so I can use those from the
outside this machine doesn't have sequel
server installed when I said it did
earlier that was a lie I was using
docker for version one two but the the
image the container here has got invoked
sequel command so I can spin that sequel
query in and I get the output so that's
coming from my from my sequel server
running in docker okay so let's go back
here so that's version two of the
application so so far all I've done is
move my components to docker so now I've
got in version two I've got two images
that I'm using one I haven't have to do
anything for I just pulled that from the
hub because that's Microsoft's and the
other one is my own image that I
packaged up my own web application so at
this point I've already got a whole
bunch of
oh geez so one of the major advantages
I've got is consistency any time I build
a new version of my application the
process is exactly the same if I add new
dependencies or if I change to MVC or
new version of.net or whatever I'm still
going to run the build to generate my
published output and then run docker
build to package it up in an image I'm
going to store it in exactly the same
way because it's not going to go up in
my registry and I run my application in
the same way and it doesn't not only
consistent for versions of my app - in
system for different types of
application so the way I started sequel
server was exactly the same as the way I
started my web application just with
docker rot the way I manage it is
exactly the same so if I wanted to do
see what was running inside my seat will
container I use docker top will see
later on I can use docker logs to see
what's being written by my application
where application logs so I managed more
in the same way it doesn't matter what
type of app we've got and I could stop
there but we've got a bit more time so
I'm going to carry on I'm going to use
this version 2 as my foundation to
modernize my application so the fact
that docker lets me run all these
processes in containers that are really
lightweight so I can run lots and lots
of applications on my online machine
I still have isolation but docker
provides the plumbing between those
containers to talk really cheaply
enables me to add more components in
different containers and bring a lot
more features into my application so I
can deploy quickly and safely because
it's all automated and I can start
breaking up my monolith into different
components and have docker plumb
everything together for me so how do you
go about this modernization
modernization is do mean different
things for different apps and different
stakeholders and different enterprises
so you might have a you know we're not
talking about full rerp texture here so
what I like to do is to think of it from
the feature level so feature driven
modernization by by enabling more more
functionality within your application so
there's a few problems with this app
that I'm going to address for the rest
of the session the first one is
performance so I make a synchronous
connection to sequel server whenever
anyone signed up and I make three calls
to the database just when people click
on ok
so that doesn't scale
if we're doing a marketing launch and
suddenly we have tens of thousand people
hit not a website dusting that's gonna
be a bottleneck on my sequel server now
I can easily scale up my web application
I can just have more hosts and run more
containers but I've still got a
bottleneck and sequel server so the way
the application runs at the moment the
web is tightly coupled to the database
component or we need to scale them up
together so we can address that we'll
see how to address that shortly
then I've got a data insight problem so
I'm using sequel server if anyone from
the business wants to actually get some
information out they have to contact IT
so they're not going to run the sequel
command they're not gonna have access to
the database or any sequel clients if
they want to find out the the breakdown
of different types of roles from people
signing up they have to put in a support
ticket it's going to sit in the queue
for ages because no one wants to do it
when they do pick it up they're going to
have to write some sequel or put it into
an Excel spreadsheet so it's those
everybody down and the last thing is is
the UX so even though I'm running in
docker now I'm still running a monolith
in docker if it's an old application
chances are there probably aren't very
many automated tests and the release
process if is a big application it's
going to be an out of hours process with
a whole chunk of time advocated to smoke
testing and user acceptance testing
you're lots of stuff if the product
everyone wants to have a new landing
page a new homepage that's a tiny change
in terms of development but all the rest
of the cost is still there even though
running in docker because I still need
to that dock it makes it easy for me to
build that new version with the home
page and to release it but all the other
activities around there are probably
still the same so we're going to address
all these things just by leveraging what
we can do already with docker and the
first one we can look at is performance
so I've got my web application making
synchronous calls to my database so how
do I make that change that I can so I
can scale it up but it's page one of
enterprising spiration patterns or I'm
going to do is I'm going to put a
message queue in between so from my web
application when somebody signs up
instead of making a database call I'm
going to publish an event to a queue and
on the other side of the queue I'm going
to have a new component listening for
those events and that component is going
to do the database update
so that means I can scale them
independently and if I'm doing a big
marketing drive I can have lots and lots
of web containers running and the queue
will fill up and then the other
component will just pull off the
messages when it's got time to do them
and put them in the database I don't
have an SLA that says when you click
okay you must be in the database within
30 seconds so for this particular
feature it doesn't work like that it
could take a minute or five minutes
doesn't matter how big the queue gets
because we might not send out an email
for a week okay so let's see how this
looks so let's go back to my sequel
server to my server rather so you can
see there's several times
check out version three where my code is
almost v3 the V doesn't work properly
online laptop which is which is
difficult when you're trying to do this
so v3 not that I'm saying they can all
with Dell by the way so switch to v3 I'm
going to reset to get rid of the
containers from version 2 and then we'll
have a look at what's changed in the
application so visual studio knows
something's changes I have changed code
in this version of the app because I've
moved the way might be I've changed the
way my feature works so we'll have a
look at what's changed in my web
application in the sign up page it's all
the same boring boilerplate stuff until
I get to line 70 when you submit the
when you submit the form
I create my prospect object in the same
way but instead of talking to the
database and making a call I build up an
event object a prospect signed up event
which then contains the prospect details
that I've got off the forum and I
publish it to the message queue so that
message queue is in a new project and
the message queue is just a little
helper that the abstracts away dealing
with the particular message queue so in
this in this case I can publish a
message of a particular type and the
only message type that I've got defined
is this prospect signed up events so
this message is just going to contain my
prospect contains the date/time stamp
from when they signed up and it's got a
subject like a subject matter so that
people who are listening to messages
know that this is a sign up event so
it's all pretty simple stuff so on the
other side of the message queue I've got
a new component which is just a simple
dotnet console application
so this console application when it
starts is going to use the same
messaging library to create connection
to the message queue to subscribe to my
signed up event and then just sit there
and listen so this will run indefinitely
it's going to sit there and wait for
messages to come in on the queue when it
gets a message it writes some blog
entries it pulls the prospect out of the
message and then this is the exact same
code from version one to create my
prospect in the database so what I've
would have effectually done is taken
some some features some functionality
from my web application put it in a
different component I'm using a message
to connect the two together so if this
was a way more complicated function that
did all sorts of validation and stuff
the process would be the same I took it
out of my web publication I put it in a
different component and I join them
together using what I can do with Daka
okay the only other thing that's changed
is in my web config I've taken out the
connection spring so as I said you can
run any app in dakka dakka doesn't
mandate how you do configuration or
anything like that but docker integrates
better with some types of approach than
others so Dhaka doesn't know anything
about web config and config management
that's in dotnet but it does know about
environment variables and all the
frameworks support environment variables
and most of the platforms support them
so what I've done is I've taken my
content out of web config and I've put
it into environment variables so in my
model for the database for identity
framework I'm using a custom database
connection string that I pull from the
environment variable which like it's set
using docker and exactly the same for
the message queue the message queues got
a custom confit class that's expecting
to find an environment variable called
message to URL which tells it the
location of the message queue so that's
all has changed so that's going to build
this and then we'll have a look at what
we're building okay so in the docker
world now my web application docker file
is more or less the same it's the same
up to point where lines we're over I
exposed the port I'm adding a default
value for this message to URL the
default environment variable value I can
default that because I'm going to own
starting up all the containers so I can
be sure that I'll start containing a
core message queue
so I can use a default lightly for luck
nice and simple the other stuff hasn't
changed what I've also done is I've
added a health check so the health check
is a really neat part of docker because
Dhaka doesn't understand my application
doesn't know what's wrong it doesn't
doesn't really care what's happening
inside that all it can do is watch the
process it started and be alerted if
that process goes down the health check
features lets me go one step further and
tell docker how to check my application
is still healthy because I could still
have is running but it might be
returning 403 s all the time so what I
can do here is give it a really simple
little script which in this case this is
going to run inside the container it's
going to make a little Parishad call to
the localhost which is the web app
running in the container if I get a 200
response I tell docker everything's ok
and if I don't learn I tell doc there's
a problem docker will run this by
default in the container every 30
seconds so not only can I use this to
check my applications healthy I can also
use this as a smoke test that I need to
manually smoke test my app because
docker will do it for me as long as my
script is testing enough of the app for
me to be confident and it's also a warm
up because it's going to periodically
keep my app pings with it with the
request who's going to keep it alive
here I've got a cache that's got some
timed expiry then this is going to keep
my Tosh up-to-date for me I've got a new
docker file for my second component my
my message handler which is just might
dominate console app this is much
simpler because I need fewer components
to get my app up and running starts in
the same way I start from Windows server
core I switch to PowerShell turn off the
DNS cache so all doc mouse and start in
the same way I don't need to install
dotnet framework because the the Windows
server core image has already got 4.6
head up so all I do is copy in my built
project set the same default value for
my message to you and then I tell docker
when you start the application just
start my console up that's all I need to
do so let's let's start this up and
running and I'm starting in a different
way which I'll explain right now so
I've now got a distributed application
across multiple components I've got my
database I've got my web app I've got my
message queue I've got my message
handler they're all running in different
containers they need to run under the
right names to make sure they can find
each other they need to running the same
docking network they need to be started
up in the right order I could do docker
run manually and start them all up in
the right order or I could write a
PowerShell script to do that for me but
there's a another part of the docker
platform called docker compose which
lets me compose a distributed
application and specify what needs to go
into into starting that application up
so just like my docker file is
effectively replacing my deployment
document for a single component my
docker compose file is effectively the
deployment document for the whole
distributed solution so it's written in
yam also it's very easy to read and what
it does is it just takes the place of
all the docker run commands and you can
use the same sort of values that you
would pass the docker run so my database
is my product launch PB I'm starting it
from Microsoft sequel server Express
image I expose the port I set the values
for the environment that I need and I
connect it to a specific network the
message cube that's my new component I'm
using maps which is an open source
message queue it's on docker hub I'm not
building that myself and pull it down
from docker hub I've got my product
launch website which is version 3 that
I've just built from my own my own
docker image I specify the port is to
expose I've got my database connection
string in here as an environment
variable got the password in there like
I said that's not great there are better
ways to do it or we have time to show
you I can also tell docker composed of
this service my web application depends
on other services that are running so
that dependency is going to let docker
compose start them up in the right order
it won't start my web application until
my sequel server is up and running and
my message queue and my new component is
my message handler so again that's been
built from my image from my application
packaging my dock my console app I also
give it a database connection string and
it also depends on the database and the
message queue
they're all plugged into the same
docking network so all these containers
can access each other by by the host
name and then the DNS inside docker will
get them all up and running
so the way I started that was with
docker compose up which is just going to
run through my docker compose file and
start all the service in the right order
the reason I've got an error there is
because I've got a few beta bits and
pieces and sometimes they're the
networking piece in Windows isn't quite
up to scratch yet we're expecting an
update for that shortly so what it does
is it creates all my all my services and
then I can work with them in the same
way so it's a new it's a different way
of firing up my application it's a
different way of defining how the
distributed solution fits together but
when they're running they're just
running as docker containers so docker
PS I've got four containers now database
website message queue message handler
and I can work with them all in the same
way so as before I can use docker
inspect to get the IP address oh if I
give it the name of my web application
it's a new IP address because it's a new
container my sequel server database is a
new container too so it'll be empty the
previous day's that I put in there will
have been lost because I didn't start my
container in such a way that it saves
data elsewhere you can obviously do that
but I'm just showing you here that if I
take my container away for days it goes
with it and unless I've specifically run
it to have the data outside of the
container so I've got my IP address for
my web application so this is Version
three and I can spin up to that so there
was no startup time here because I was
gambling so long enough that the health
check kicked in so doc around that
health check for me it hit the home page
of my application which hit my entity
framework which set of my database for
me so that ran within 30 seconds I knew
as long as I talked long enough
there wouldn't be a delay when I started
the application so now I can sign up or
put in a different docker employee and
we'll save them we'll see how it looks
so from the user experience is exactly
the same it's the same website when I
click go I get a thank-you page really
quickly the difference is if there were
tens of thousands of people doing the
same thing I'd still get a thank-you
page quickly with the previous version
because I'm making that synchronous
connection into sequel server I'd be
sitting and waiting and I'd probably get
a timeout page so that's run now so
that's cool so we
and see what's going on in our database
by using the same command I can use that
same select command inside docker to see
what's the data that's in there I'm only
going to have one row because this is a
new container but I can also see what's
going on in the other containers so if I
do a docker PS to list them all out I
can refer to container by its name or by
its ID if I want to see what my message
home has been doing I can run docker
logs with my message x r ID and this is
the the output from console.writeline so
docker can see what's being written by
the application to the console and it
can surface those logs and it can
actually the logging part of the
platform is really extensible so I could
send those logs somewhere central and do
such cool stuff so when the application
starts it listens to this particular
event and it tells me that it's received
a message from when I click sign up it
saved a new prospect and I've got my
prospect ID in there so I can see what's
happening by looking at the logs for
this this application so this is a
dotnet console out I've also got an
asp.net app that I can treat in the same
way my message queue is a go application
so my message queue is called docker
message queue 1 let's clear the screen
if I look at what's happening in there I
can see the process is running and again
it's all behaving in this like the same
way from Dockers point of view dr.
doesn't knows as go up it doesn't know
my other apps and apps it doesn't care
it's running a process it will keep an
eye on that process and make sure it's
up and running for me the actual process
is this command D which is my which is
the DOE application that's running my
message queue it's using two and a half
megabytes of memory so that's incredibly
lightweight message to it sort of huge
amount of capability I didn't have to do
anything to get that into my application
I don't have to download an MSI or run
it or set up some configuration it's
just down on the hub ready to plug into
my solution okay so that is Version
three cool
so we fixed our performance issue which
is now I can scale independently of
decoupled my web application from my
database application my web application
for this feature is effectively
stateless now so I could run lots and
lots of my web containers and they're
all going to write to my message cube
and if it takes a long time to pull all
the messages out and save the data it
doesn't really matter for this
particular feature we're now using an
event-driven architecture because we're
publishing events to a queue that other
things can listen for but I didn't
really take the whole solution I've just
taken one feature that needed to be
fixed and I've used the docker platform
to make that change really really easily
the message amusing called Nats it's
open source cross-platform message queue
it's incredibly capable it will do tens
of thousands of messages per second
easily it cost us so I can have lots of
queues lots of queue containers running
on my on my hosts to get reliability and
scalability and it's free to use and
it's up there on the hub so you just
plug it straight in okay so that's our
first one that's a kick for our first
feature we've just added a really high
high-performing message queue that's
available on the docker hub that someone
else has put there for me put it into my
solution and then broken out one
component into another container so that
I get the stale ability from decoupling
but I've still got a problem if I want
to see any of that data so in my case
I'm using the docker command to run some
sequel for the users obviously that's
not an option they're still going to
have to put in a ticket to get some data
out so we're going to address that next
in version 4 how are we going to do that
same approach I'm going to take some
existing tried and trusted tools that
are on docker hub bring them into my
solution and because I've now got an
event-driven architecture for the event
I'm interested in I can just hook into
that so I'm going to add a new component
which listens for the same events and
publishes the information to a different
data store to a reporting database that
the users can access themselves
ok so same process as before let's clear
let's bone check out version 4 of my
application
and in Visual Studio let's reload this I
haven't changed anything to do with the
original application from the previous
version so everything is the same for my
web application for my original message
hands of the saving to sequel server
none of that changes I've added a new
component which is another dotnet
console application which looks exactly
the same as the previous one when the
app starts it connects to the message
queue and listens for the same events
the prospect signed up events and then
when it gets an event it saves it in a
different type of data store so I'm
using a document database I can be using
elasticsearch which is which is a
massively popular open source document
database the only thing that's changed
is how I build the representation for
mine from my new datastore so I'm
getting getting the same information I'm
creating as a document in elasticsearch
instead of creating it in sequel server
so this components going to be added to
my solution now so I'll go ahead and
build it and then we'll look at how it
looks in czaka so for the web
application docker file hasn't changed
for the original prospect handler that's
saving in sequel server that hasn't
changed
I've got a new component a new docker
file from my index handler which looks
exactly the same so when you start
writing these docker file to your
components actually great chunks of it
are giving the same for every type of
app that you wrote that you write
you've got the flexibility to configure
each component exactly as you want to
but some of the summe a lot of
commonality so we're in a server core
powershell turn off the DNS cache
standard copy in my new donate console
app set some environment variables again
I can be confident about this it looks
like it's a nasty hard-coded string but
because the platform is doing the make
sure that everything starts up how I
want it to I can afford to hard-code
these things so when I run this I don't
need to understand how it connects to
other other components because that's
built into the image so a default value
for the message queue so I can listen
for these events and a default value for
where I'm going to find elasticsearch
when I start I run my startup command
ok so let's clear this and get it
running so this docker compose up it's
just going to start my containers and D
is that the same D flag to put
everything in the background
okay so that's my new component the
build process well it's the same stuff
I've just added another another project
to build so I build my website build my
original hands up my new handler and
then I've got three docker built
commands one for each image so I've got
an image from my web application an
image from my save Handler and image
from my index Handler and my docker
compose file now is much bigger because
I'm adding various new components to
enable this feature this this data
insight feature I've got my database
I've got my message queue I've got
elastic search now so elastic search is
that coming from docker hub I'm not
building that myself here I don't have a
docker file for that someone else has
got the docker file for that and that's
custom from the hub and Cabana is a it's
like a self-service UI on top of elastic
search and again that's coming from the
hub and it knows about the elastic
search image so those two things are
going to work together really nicely
without me having to do anything all I
do is expose the port that it runs on
and say that this this service the
Cabana service depends on the elastic
search service there's all nice and
straightforward what web application
that hasn't changed except that I'm
using v4 so I don't actually need to
rebuild this this application to add
this new feature and whether or not you
rebuild and tag everything with the same
version is a choice that you make so I
could be running version 3 of this
application but I've chosen to keep
everything in step so i know when i need
virtual for just run version 4 of all
the components prospect times are the
same for sequel server and my new index
handler that depends on the message
queue on elasticsearch are doing to
specify where to find elastic search
because that's built into the image as
long as there's a container called
elastic search this thing just works I
don't need to specify the details about
so if we look at what Dockers compose is
done its reusing some of those some of
those components so it knows since I
last round docker compose that the
message queue and the database are up to
date they haven't changed because I'm
using new versions I'm getting a new
version of my website and my handler
I've got completely new containers for
elastic search cabana and for the new
handler so I can do it like an
incremental release with docker compose
and it will look at what's currently
running it'll compare it to the
definition of the solution in the
compose
while and it will it will basically do
the gift so it will run new things if it
needs to
it'll recreate them if it needs to and
it will start new things if they're if
they're new components but just like
before someone look at what's running so
I've got a whole bunch of containers
running now I'm still not using hardly
any of the resources that are on my
machine I could run tens or dozens of
containers and it would all work happily
and but I can work these things in the
same way so my docker website is now get
the IP address for that so and there's
my IP address and I can use that to do
some our website and person data in same
as before so it's exactly the same it's
already warmed up because of the health
check that's in there I could go and
sign up and put some more data in I'm
not gonna do that at the moment I'm
instead going to have a look at some of
these other things that are running in
docker now so I've got elasticsearch
running I can have a look and see what's
going on in my elasticsearch container
and I can see the logs that are written
by the process this is a java
application I don't have Java on my
machine
why don't I install the driver on my
machine this is running inside the
container it's with the right version of
Java it's all configured it's just
straight down from the hub it just gets
used if you run this to yourself for the
first time it will download these new
images so if I hadn't already
pre-prepared this it would take a few
moments to download that that
elasticsearch image if I look at what's
going on in Cabana which as it happens
is an odorous application and I can see
the logs of Japan are starting up and
getting started no js' governs our bank
stored in this machine it's fine it's
all configured inside the inside the
image so when I run my container it's
got everything it needs to spin up the
application now I said I wasn't going to
use I wasn't going to put in my form and
the reason I'm not going to do that is
because I've got an end-to-end test in
here which is going to pre-populate some
data for me so I've got a spec flow test
that just needs the URL or the host name
or the host brother which if I'd
actually saved it that would be useful
okay
and this sort of end-to-end test is
going to flex from the entry point is
really useful if you're doing this sort
of modernization by feature approach so
in this case I'm just using this test to
drive some data but I could have
assertions in here that makes sure my
data get served equal safety sequel
server and my data gets saved to elastic
search and if I change the setup of my
application and change where these
components live the end-to-end test
should still run so when I submit my
form if I had a check to see that the
data is in a sequel server that check is
still valid even though I've moved the
components apart so this sort of
automated testing which is just a basic
example it's really good to have in your
solution if you're looking at breaking
it apart moving moving features around
so I'm going to let that run on and
while that's going I'm going to have a
look at my Cabana so Cabana is this
again it's an open source tool if I
didn't know anything about it when I run
it I know it it's on port five 601
because it tells me here so I can go
back to my original web application and
browse the Cabana and this is straight
out of the box I'm not doing anything to
set this up it's found elasticsearch is
expected to find it on a container
called elastic search and that's up and
running the only thing I need to do is
tell it the name of my document
collection which happens to be Prospect
and it will connect to elastic search
and it finds all the data that's in my
prospect selection there's already some
data there because my tests have put it
in so I can see all the fields that that
elastic search knows about so Kabam is
really cool because i can look at the
discover tab and it's just going to show
me all the data that's in there with a
timeline showing me when these documents
got created so from my really basic
tests I can see each individual prospect
that's signed up and I can see all the
data that they put in so if I wanted to
I could get a list of email addresses
and that sort of stuff but for power
users it's really easy to pick up their
own and their own requirements and
implement it with with cabaÃ±a so if I
want to pie charts from my prospects I
can split it by the terms in the role
name field and I get my nice pie chart
that's showing me
with my test data 50% of people are
decision-makers and 30% are architects
if I want to change the look at
countries I just change the field and
run it again so this is self-service
analytics is really easy application to
use I can give this to power users I can
secure the container so I can only
access to banner from within my network
so that wouldn't be publicly available
whereas the rest of the website would
and not only can people go in and build
their own visualizations but I can build
dashboards and have lots of
visualizations and give you a central
point to find out what's going on and
users can build those themselves so it's
added a huge amount of capability here
we're doing very little work which is
you know what we all want to do I guess
so that's version 4 I've added
self-service analytics by doing exactly
the same I did with the previous
versions I've added some new containers
one of them has got some of my code in
it tiny amount of code to create my
document date my document data and I've
plugged in components that are available
on docker hub now I could do this
release during office hours while people
are using the app because they don't
need to change the existing components
in this state ID because I by toggling
with the new version but I could just
release these new components that are
hooking into my original event-driven
feature and I wouldn't have any downtime
at all for this so not only can I do a
really nice safe update and I've got
some fantastic capabilities here the two
things that I've plugged in elastic
search engine banner enterprise-grade
components they happen to be open source
but elastic search you can run it in
docker you can run clustered and
clustered elastic search instances on
different containers on different hosts
and I've had this running in docker with
terabytes of data and it just all works
you don't need to understand what it's
doing it's like on version 5 of the
product is tried as trusted people use
it all over the world we just plugged it
into our solution without having to do
hardly anything ok now we've got our
data insights of people business users
you know find out who's starting up for
art Ferrara or a great new product
launch which leaves us with one last
feature to do which is the UX so I want
to be able to change the landing page
without having to do a full release so
let's go and have a look at how we're
going to do that they're purchasing the
exactly the same I'm going to have a new
component just
my home page and I'm going to change my
original web app so that it hooks into
my new component and let's see this so
exactly the same as before I'm going to
check out version 5 so I don't have to
try and write any code and spell
everything wrong and get that building
so version 5 yes I want to reload the
only thing that's changed is the content
of my default page from my web
application so what I do now is when I
hit the default page I check to see if
there's a configured home page URL which
will be an environment variable I'm
using that like a feature switch so if
there is a home page URL configured then
instead of rendering my own content I'm
going to get the content from that URL
and render it back out so this is a way
to pass control over to another
component but I've still got the same
entry point while web application is
still where people hit and now it's kind
of evolving control out to a separate
component for my um for my home page and
in the docker world the only real change
is a new docker file for my home page
components so this is using Windows
server core to reach the PowerShell
turnoff the DNS cache it's an it's a web
application so I'm installing is but
it's not asp.net I'm using a different
framework so I'm just copying in my HTML
document and I've got a health check in
there to keep it all up and running so
let's see that's all built so I'll do
docker compose up again to start up the
new versions and in my docker compose
file I've got a new component for my
home page which is using my new version
of my home page image that I've just
built and I've changed my product launch
website to include that home page URL by
configuration and value which I'm also
using like a feature flag so that's all
up and running now so if I do a docker
PS I've now got a pretty distributed
solution I've got eight containers that
are all doing different things but my
anchor point for my app is still exactly
the same and for users the way the user
hasn't changed either so let's have a
look at this web application now so
let's inspect that Oh
so this would be this is a new container
because I've changed the image
definition switch to version 5 and when
I browse to this now we'll see my
awesome new web page Hey
so like this is like totally futuristic
design sorry I'm not available for
design consultancy but the point is this
is this is not asp.net I'm using the
marquee element for HTML but I could be
using anything in there and if I want to
change my homepage if for some reason
this wasn't popular then I could switch
just the home page component to a
totally different back and I wouldn't
have to change my running application
when this stops blinking if you can
click it I just go back into the
previous the previous page and I can put
any old data in here V 5 V 5 V 5 click
on go and that's going to go through the
rest of the stack and because it didn't
change my other components I'm still
using the same elasticsearch the same
Cabana my new data that's just gone in
there is already index finger banner so
there's my version 5 data that I've just
put in so that's all up and running so
that was version 5 that was pretty quick
and this is our full solution now so
this is a fairly elaborate
complex solution that's been built up in
stages for the feature different
approach and the final thing I did was
to swap out the home page for my my
awesome short wheels and design skills I
could be using react or attenborough or
whatever I want to use so this is good
for the I need to do for the product
owners who can change UI without having
to do a full release it's great for
architects or devs who want to try out a
new framework in a controlled way so I
can I'm only going to build the home
page we're going to test the home page
when I release it everything else will
work an interesting option is like could
use a DN a CMS in there so I could use
I'm Braco open source content management
system I think that users do their own
content ok so I've done that to our
three text arts break so let's remember
we started with the web forms up and
sequel server database and now I've got
an Avenger of an architecture
self-service analytics potentially
self-service content so that's a massive
step forward with very little
development and very little risk
when I did my new applications so your
takeaways docker is a mature platform
I've shown the key features you probably
owe a lot of questions about persistence
and security and scalability
they've all been answered because dock
is being used in production in some
significant applications significant
clients you can move wraps dock it
easily you can take your existing out
from running docker as long as there's
as long as you can install it and run it
without a UI then it'll run in docker
and then the the slogan when you've
docker eyes that you that enables you to
modernize your application so sounds a
bit like a cat poster
so there's your cat poster there are a
few more talks today about docker so
room to next Benton halls going to talk
about how Windows containers actually
work
Philips talking about monitoring later
on and then Michelle's talking about
orchestration which is how you you met
all these different components work
together a few search terms for you we
got some labs on github that you can
follow through to test this up yourself
captains are the equivalent of Microsoft
MVPs and our big conference is dr. Conn
I'm tweeting all the time about this
stuff
I'm just Ellison so minute Twitter and
that's it so I hope that was useful I'm
around all day I've got some swag if you
want it asking questions you've got and
enjoy the rest of NEC</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>