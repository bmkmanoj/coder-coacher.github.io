<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural networks by example - Natalia An &amp; Katya Mustafina | Coder Coacher - Coaching Coders</title><meta content="Neural networks by example - Natalia An &amp; Katya Mustafina - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural networks by example - Natalia An &amp; Katya Mustafina</b></h2><h5 class="post__date">2017-04-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BnnFygfVLOI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello is this working
is this ready now yeah okay
the talk neural networks by example my
name is kaity Mustafina and my name is
Natalia and we'll walk you through this
talk about neural networks fundamentals
is anyone doing machine learning in here
anyone doing deep learning hundred
percent and the percent of people do you
feel learning doing deep learning that's
awesome so this talk is about neural
network fundamentals and neural network
is an algorithm that's a computational
method and that is machine learning
paradigm but before we start talking
about machine learning I wanted to talk
a little bit about what is before you
start talking about neural networks I
wanted to talk a little bit about what
is machine learning at all and ok so
here is a traditional computer program
which is very good at detailed
instructions such as assignment
operations loops conditional statements
and what it does actually is that in
some computer language it's implement an
algorithm that is converting some input
to some output so we instruct the
computer to take our input and produce
the output with that so if we look at
the computer program as an abstraction
for example we can think of it as a
function a function is some computer
language that takes an input and
produces an output so at this point I
wanted to restrict myself by pure
numeric functions and computational
functions that could be for example
linear functions or any function that
for example the one complex one
presented on this picture or it could be
a quadratic function or something
this restriction is I'm doing for better
intuition about machine learning because
actually machine learning is dealing
with numeric calculations so now doing
intuition about machine learning so
imagine we don't have our function but
what we do have we have a set of data
some inputs and some outputs but we
don't know how it has been calculated we
know that this input produces that
output but we don't know how we don't
know the algorithm but what we do we
take pool of functions
maybe infinite variety of functions each
of those can be a candidate for that
calculation and then by doing
experimentation we are choosing the one
that fits best to our data that we know
to be correct and then we choose it for
calculation of data of similar nature
that process is called training and the
data that we use for training the data
that we know to be correct is called
training data so that is a very roughly
in tuition without machine learning and
what's actually happening so what is
neural network neural network is exactly
our function and this is an algorithm
implementing very complex nonlinear
functions and there is a feature of that
algorithm and that will go into detail
later on in the talk but before we do
that what I wanted to talk about that is
that why do we care at all whine your
networks becoming so popular with deep
learning is becoming so popular these
days what kind of problems deep learning
can solve that couldn't be solved before
and then can now can only be sold by
deep learning in neural networks powered
engines and the first interesting
application is a computer vision
computer can recognize objects the face
recognition is that something that we
are using every day on our mobile phone
if you have your Apple photo library of
Google photo library if you are doing
outer tagging in facebook that is the
face recognition handwritten text
recognition this is very classical
application that will go deeper later on
in the talk also online search Google is
using you know networks to classify its
images and then by entering the word
giraffe you can see lots of images of
the giraffe also if you don't know some
word in foreign language in Chinese for
example you can enter it and then you
can see the pictures and you can very
likely to see the pictures of that
object and you can search your image and
you can search similar images on the web
that is powered by neural networks
in security computer system can
recognize the person entering some
premises we are using our fingerprint
recognition to unlock our phones and we
are you can use for example a computer
system to survey the public places like
airports to detect people who are
actually behaving strange and they are
not they can be potentially dangerous
for someone they maybe have a bond with
them and that this kind of systems are
powered by neural networks medical image
analysis astronomy very interesting
applications for example if you have a
flooded area so disaster areas is to
rescue people for example you can
picture with the helicopter the whole
area and then computer system will
debtor mind where exactly people are in
that area and where to send actually
help and it takes hours in rescue
operations in very very cool application
with self-driving vehicles on this
picture is a Google Google car that is
using to kind of develop this used to
develop but to develop the technology
and that is Google 2014 project this car
is running around Google campus and it's
powered by neural networks Google is
planning to produce the technology ready
for public transport public roads in by
2020 and this is very big topic now in
California everyone is trying to work
with self-driving vehicles Tesla also
has now this auto pilot for most of its
for all of its models right in
production and so capitals of in cities
in Europe they trying to test and put in
production self-driving buses like
Helsinki Berlin now awesome
these projects are ongoing so playing a
couple of years will see those buses as
a public transportation systems speech
recognition automatic translation chat
BOTS text analysis in cancer research we
use neural networks to predict patterns
in protein structure
we analyzed features of genomic data to
predict for example if the approximate
that cancer would be eligible for
certain cure from the cancer in we can
use it in art we can style our pictures
with some it's her I just wanted
somewhere on the web actually with some
famous artwork so and that is also that
is a fun application but this also helps
to popularize the technology so that is
the kind of problems that we couldn't
solve at all if you wouldn't have deep
learning today so now how did it start
yeah I will tell some brief history of
neural networks don't fall asleep
I think it's actually quite interesting
we need to make a Twitter about it
so neural networks have been subject of
a great interest for many decades in
fact the era of neural networks can date
as far back as 1940 because already in
1943 when we were struggling in Second
World War know some of us at least our
parents some countries so McCulloch and
Pitt's created the first and symmetrical
model of a neuron and we will show you a
smaadahl later but for now I just want
you to keep in mind that neurons are the
very very building blocks of neural
networks and about ten years later
training procedure was invented to
actually learn these neural networks to
do some stuff and needless to say that
expectations were huge there was a lot
of hype around neural networks
comparable probably to hype we have
around networks now and people start
thinking that artificial intelligence is
just around the corner
of course it was not and then in 1969 a
famous book was published by Minsky and
Papert name it perceptrons in this book
they described a simple neural network
and analyzed it and the result of this
analysis was rather discouraging because
it showed up that such simple networks
are unable to do any sophisticated tasks
for example it cannot even classify data
which is not linearly separable so
disappointment was enormous impact of
the book was also very big because it
started an era which was which is known
as artificial intelligence winter it's
kind of a neural network decadence and
during this period
neural networks were not considered like
real science and researchers in the
field were considered like some strange
guys doing some sales of science nobody
wants to talk to them yeah so dark time
it truly was really really dark time
hard to believe now well in 1989 one of
the pioneers of artificial intelligence
research field Yuen Li kun quite a
famous guy he invented a new type of
neural network which was able to
recognize handwritten digits and that we
will show you this later in quite a
detail so the bank check recognition
system he helped to developed was widely
deployed by a national cash register and
set of other companies and were scanning
over 10% of all handwritten checks in
the United States in the late 90s and
beginning of 2000 and that could be a
breakthrough that could be the end of
dark times and start of new era but
unfortunately for neural networks
fortunately for machine learning field
support vector machines was in algorithm
was invented by about his time and it
showed about same performance as neural
networks by now by that time been
considerably cheaper to train and buy
considerably cheaper I mean it required
less examples it was faster to train so
as a result of the major interest all
the money and all the researchers went
to the field of support vector machines
leaving neural networks aside then
anyone know what support vector machine
is and that is in our it's now great
many machine learning just viewed
another algorithm not in your own arrow
yeah well then millennium come and
brought us new hope in 2002 a very
famous person in neural network field
geoff hinton invented a training
procedure which allowed for the very
first time to train and networks which
had hidden layers and more than one
hidden layer the idea of algorithm was
quite interesting because he made neural
networks to train by themselves to some
degree and then that's just fine-tune
them in the end
at about the same time GPU computation
came along and that allowed neural
networks to start slowly rival support
vector machines because it's the right
amount of data
neural networks having much better
performance or at least better
performance while support vector
machines getting to the point of not
being any better so a couple of years
later Canadian Institute of Advanced
Research Canadian Institute for Advanced
Research also known as CFR invited
Hinton to lead a small group of
scientists providing small but
sufficient money to kept kept keep the
research moving and I really I cannot
stress enough the importance of this
happening because already two years
later Geoff Hinton and his team hatched
the conspiracy to rebrand the very
unpopular term of neural networks to
something called deep learning because
as he mentioned is one of his interviews
if you mentioned neural network in in
any kind of title or description of your
research or paper it will go directly to
the trash bin nobody wants to even look
at it
so they used known marketing trick
rebranding and it totally totally worked
in the field of science well we all
humans after all no matter how clever we
are so they call the deep learning and
that's what we know now actually we
address term deep quite often what deep
basically means strictly means it means
more than one hidden layer so we all
operate with deep networks now so next
happening and next breakthrough happened
in 2012 then Geoff Hinton again and his
team achieved outstanding performance in
imagenet competition imagenet is an
annual competition with teams of
researchers submit web work and they
compete in recognizing objects and
scenes on images did you know by the way
that as for today computers are already
officially better than humans missed
tasks there is a supercomputer called
mingi at the Chinese company Baidu and
it documented better performance when a
human being so we already already behind
in that as well well back to history not
so long time ago at 2015 Eden masks and
other annonced found in open ie
organization open a is a non-profit open
organization with main goal to develop
artificial intelligence in a way which
is safe and beneficial for humanity and
just two months ago in November of 2016
giants like Microsoft Facebook Amazon
IBM and one more I don't remember is
five they announced facts actually
coming together not competing but coming
together for what they call a
partnership of ie and partnership of ie
as an organization which study and
develop artificial intelligence
technologies and also advanced public
understanding of artificial intelligence
in a way it influenced society I think
it's quite quite cool so what I want to
say is we all here we are lucky to live
in a very time of something big
happening in fact some scientists most
of scientists or some very popular
scientists agree but by 2040 we will
achieve technological singularity and
and I start thinking about is just crazy
so as a result as a wrap-up on a history
part because we already by now today I
just want to say that neural networks
success as we have it now is a result of
persistent work of few data scientists
supported by technological revolution
provided with enough computational power
and of course very important the vast
amount of data because all the files all
the information which is available on
internet is very important for neural
networks to succeed so I hope it was
interesting sounds for me like a Twitter
anyway and it so cool but I mean we
really want to know how it works so you
see people build neural networks based
on a human brain analogy and human brain
is built of about 10 billion cells
called neurons neurons have an amazing
capability of gathering processing and
transmitting electrochemical signal this
is an artist representation of how
active human brain looks like I can look
at it and definitely thinking always
happening here but it's also happening
here as well it's kind of mesmerizing it
shows me what I'm looking at how it
works so neuron this is a picture of a
neural kind of of course neuron consists
of four basic parts it's a cell body
called soma its dendrites and all the
small things on and
at the top was a connection to other
neurons when they have exome exon is
cable like element you can see it is a
long one and exome can be covered if a
small thin layer of milling the role of
the milling is to speed up a
transmission of the signal and you can
think about it as a insulation of
electrical wire and then finally there
is the synapse synapse is a gate like
element which decides if the result of
processing which came through action
should be sent on to other neurons or
not if maybe signal is not strong enough
so basic idea is very simple get a
signal process it and send it or not to
other neurons and that would be abstract
representation of a neuron get a signal
from our neurons process it send it on
each neuron can be connected to up to
10,000 other neurons so that means that
neurons are communicating to each other
there are over 100 trillion synaptic
connections and 100 trillion synaptic
connections is equivalent of computer
with exaflop processor in this inside
here inside each of your heads running
on almost zero electricity that's
amazing
so now we all know what the neurons are
and Katya will show you a mathematical
model of it so I will show the model
will be little bit of mathematics not
much of it so neural networks is a model
that is inspired by what we think
computer brain works and it's actually
what it is it's a set of notes and set
of connections between them somewhat
like graphs among nodes and edges but
unlike graph it's very structured so it
comes in layers there is input layer
there's some hidden layer one or more
and there is an output layer and we have
we are doing calculations between those
layers so our input layer is actually a
data that we want to analyze it could be
image it could be sound it could be
video or it could be whatever data that
we want to classify for example and in
image recognition understand image
output it's a set of labels of what we
know about our output and what we want
to predict for example if we are doing
in
recognition and if you're trying to
identify if there is a cat or dog on the
picture that would be cat and dog
so how calculation is done within the
neural network if you look at there no
dog neural networks that is a
computational unit that is connected to
the notes of previous layer with some
signals with the ages and they are
assigned some weights and so we do the
calculation of the node a1 in this case
based on the calculate based on the
previous layer nodes and the weights of
these connections and then the
calculated units is the calculated value
from that unit is propagated further on
and that's done with all the nodes in
the neural network that process is
called forward propagation so let's
consider just one unit what's happening
in here so a is our unit that we want to
calculate and we call it activation so
what we do we apply some function that
is based on the previous layers and
weights and we actually apply that
function to the weighted sum of this
previous layer nodes in this case its
input node X 1 X 2 X 3 X 4 and then we
we have a weighted sum of it and then we
apply some function to the function
applied to it it's called very often
sigmoid function for example it could be
other function but I'll just chosen
Sigma at forum for simplicity I would
say and here is actually the graph with
this sigmoid function does what it does
it converts the numbers into the into
the range between 0 and 1 so very large
positive number it converts to some one
asymptotically equal to one very large
negative number converse someone is a
political equal to zero that's what it
does and that is what is actually
happening in the calculation unit in in
a neural network that's quite simple so
far so if we look back on the on this
graph we see that there is some values
and then signals are being propagated
further
- the the middle layer for example in
this case in red I see the direct input
input layer nodes X 1 X 2 X 3 X 4 and
then we use it to calculate a one-unit
and then calculate y1 the output unit
and we do the same with a2 and a3 and
that process is called forward
propagation so now I introduce one
little concept that we'll just used in
an example so very often in neural
networks we use something called bias so
we add another node that is equal to 1
but it has a weight and we just do it
for a network flexibility so it's not
very important to understand the model
and just because we use it an example I
just introduce it here so what I'm going
to do now I'm going to give you some
examples that will give you intuition
how it works
so now it's the first example here is
very simple neural network we have two
input notes and then bias unit -
impecunious bias unit and output so we
have our sigmoid function that is
applied to some a weighted sum of our
input units which would be in this case
tests we have weights - 50 30 and 30 and
this minus-50 1330 is is we calculate
that and then we apply sigmoid - and
I'll remind you what the sigmoid looks
like and then I can tell you that x1 and
x2 is a binary input anyone has an idea
what function it is except for Evelina
okay we'll ask you one yeah please yes
it is an N function so let's calculate
it so first if both x1 and x2 is equal
to 0 then we get minus 15 and Sigma of
minus 50 would be would be 0 then if one
of X is 1 then we get minus 20 which is
also 0 if they apply sigmoid and here we
have 0
1 &amp;amp; 2 you have zero if both X 1 and X 2
is equal to 1 then we get minus 50 plus
30 plus 30 that would be 10 and that
would be asymptotically 1 so here we get
ant function so we calculated logical
and function using that simple neural
net so and that is as simple as it is
also I'll show you a little bit more
complicated example that is would give
you better intuition about neural
network whose the hidden layer so I'll
show you the function logical function X
nor which is equal to 1 if both X and X
1 and X 2 is equal and 0 otherwise
so that is non linear of dependency to
do calculate that I would like to write
it down as a as a bank as a combination
of simpler functions and I'll write it
down like that so X 1 X nor X 2 is the
combination of X 1 and X 2 and then not
x1 and not x2 and over function and
let's build a neural network that would
calculate it first we take a neural net
that calculate our and part we have just
seen it
then we take a neural net that
calculates our not x1 and not x2 part
and I will not go into detail
calculation of that but if we do that
you'll see that this is actually
calculating what we want and then we
combine these two to create a hidden
layer and then we apply that hidden
layer these values to the next layer
where we calculate our output so let's
calculate it if we have a 1 this is our
calculation unit computational unit so
we have calculated and as we have seen
here is Sigma and again just for remind
you how it looks then we calculate a 2
and that is exactly what we see the
values that we see on
not x1 and not x2 and then we we use
those a 1 and a 2 calculated values to
calculate the final result so we apply
it to our third layer and then we get
the result that is X nor function that
we wanted so we created a neural net
that contains a hidden layer and that'll
calculate that non linear dependency and
you can see that the neural net is
actually propagating that's knowledge
from the input layer to the output layer
so it kind of learns and hidden laying
this very simple case
so that was intuition about how it works
and Natale will give you a tour into
very exciting part yeah yeah gonna talk
about neural networks types neural
networks types are defined by topology
and that's the way neurons are connected
to each other inside layer and between
the layers and also activation function
of course so what you see on this slide
is a very simple neural network in
famous perceptron it's very simple one
input layer one output layer it's
feed-forward and by that I mean that
data is propagated propagated from input
to output and it's fully connected fully
connected means that all neurons on the
first layer are connected to all neurons
to the next layer that's it as you
already learned by now this network is
not very powerful and cannot classify
non non linear data well if we introduce
couple of hidden layers in between we
got so-called multi-layer perceptron and
both networks are much more powerful and
they can represent the sophisticated
dependencies in a data this is I think
the most used network in neural networks
fields because it's really easy to
understand easy to write and it's quite
powerful the typical usages of
multi-layer perceptron is regression
classification sometimes dimensionality
reduction as you can see the data flow
is still feed-forward but if we say we
enable data to flow also backwards
we get so-called recurrent neural
network what is interested about
recurrent neural networks it's not
feed-forward anymore it's be directional
also it's believed that having enough
units and enough depth such Network can
represent any kind of data dependency
which traditional algorithm can what is
also interested about recurrent networks
if you start thinking about each step as
a layer then each recurrent Network
becomes deep network but the depth
occurs notice between the layers but
between time steps that's what I'm
talking about
let's see we have a data and we send it
into first layer hidden layer then
ascend to the second hidden layer and if
Network wouldn't be recurrent by now at
third time step we would be out and done
but since it's recurrent the data is
sent actually back again to the second
layer at the second layer and then the
third layer and then again and again so
each recurrent network is a deep network
in itself recurrent networks are
believed to perform very well in any
kind of sequential analysis and by this
I mean speech recognition video content
analysis and so on next network I want
to address is very interesting and it's
also very simple the type of network is
out encoder and it's 1 into layer 1
output layer and one hidden layer in
between what is interesting about in out
encoder that amount of output nodes is
exactly equal to amount of input nodes
and if you ask were sketch what is it
doing the aim of our encoder is to learn
representation of the data and then
reconstruct it at the output the typical
usage of our encoder is normally
denoting and data compression in the
slide I have a four dimensional input
then I compress it to two dimensions and
then i reproduce it again to four
dimensions and for example if you have
some kind of dust on a picture when you
compress and you recreate again the dust
will be gone because it's noise and
outer encoder is clever enough to figure
out its noise and just remove it because
it's not needed to reconstruct the data
it's not containing any significant
information while out encoding itself is
quite simple
if you take several out encoder
and put them on top of each other you
have so-called deep belief Network and
the belief network want to have deep
deep means something cool deep belief
networks can perform very interesting
tasks and very sophisticated tasks like
for example object recognitions speaking
about object ignitions were coming to
the main part of our presentation and
it's convolutional neural networks
convolutional neural networks have been
very successful in recognizing objects
and it's type of network which was
invented by young Laocoon in 1989 so
those networks are networks which are
powering robotic lesions Valls networks
are recognizing traffic signs both
networks are behind self-driving cars
but originally they were inspired by a
cat visual cortex and let's speak about
cats we speak about guess all the time
so this is a picture of a cat right you
can see a cat and a laptop it's her cat
just saying okay let's have another
picture and you see a cat and the paper
and the floor pictures are different but
we immediately see a cat on both of them
what convolutional net or does it can
identify the feature of an object in our
case cats and find them on an image no
matter where an image with features are
so if I look at these two pictures and I
think what the features of a cat I would
say its eyes nose mouth whiskers I'm a
human I'm know what to look for
convolutional net doesn't know what to
look for but having enough examples it
can actually learn what to look for and
we call it extract features what it
actually does it compares all the
training examples of the pictures you
give it and by having amount about same
image patch at about the same location
it figures out that's probably a feature
so we can use it to recognize most
objects later in time each convolutional
network contains of four basic layers
and both layers are convolution
non-linearity pooling and classification
and we will walk through each of them in
a very detailed matter
so we will not look at the cats because
cats have very complicated creatures and
everybody who ever had a cat knows they
are too complicated to deal with so we
take a classical example here very
classical closer to ours
geeks numbers this is a classical
example in machine learning field
recognizing handwritten digit there is
an existing data set publicly available
containing these handwritten digits so
let's take an image of 8 and try to see
if we can tell it's an 8 or not I would
say that a distinct feature of any 8 I
can imagine myself would be a cross in
capture in the middle that's our feature
so now we have a feature and
convolutional net will walk through the
image and try to find this feature in
any possible place the search is done
with very simple math we assign all
pixels a number assigned - 1 - light
pixels and 1 - dark pixels do the same
with the feature and then to see if
feature matches or not we multiply
corresponding numbers on pixels and the
image patch and that is an example of a
full match we multiply minus 1 if minus
1 plus 1 multiply to 1 so we have 9 once
we summarize it and then divide by
number of pixels so the full feature
match ideal feature match will give us 1
and that is an example of not-so-good
feature match it gives us minus zero
point eleven trusty mathematics right
here I checked it like thousand times so
we know the mathematics behind the
convolution
let's do convolution we start from a top
left corner and they slide in imaginary
window along the image until we cover it
all what we get is an array and that
array shows us where an image our
feature matches best for intuitional
understanding with color coded with the
white being one and the best match and
the minus one the darkest and the worst
match so this is a feature map by now
you probably wonder ok this is so cool
how do we map it together to the nodes
and layers and connections we just talk
through let me show you that we already
agree with
all pixels have a number so we can
represent our input image as an input
array in our case 8x8 64 and same with
the output result it's also an array and
we call it an input node and output
nodes both inputs and outputs what we
also know that this output node is the
result of calculation of these nine
input nodes with some feature values in
terms of neural networks that means that
those nine input nodes are connected to
this one output nodes and the feature
pixels are actually weights on
connections activation function is
average isn't it cool how they apply
mathematics to kind of the convolution
process I find it fascinating honestly
well ok no mind mmm this was an example
of one feature of course usually it's
more than one so we repeat convolutional
process for every feature and what we
end up with is a set of images which you
call feature maps and of course amount
of feature Maps the same as ammount of
features as you probably understand a
number of mathematical operations is
increasing really fast with number of
pixels on a picture number of pixels on
the future and pixels and the image on
the feature and of course number of
features so what I want you to keep in
mind convolution is very computationally
expensive operation that's why you
probably need GPU for that ok
next layer is called pulling and pulling
does it actually shrink our image to
something much smaller while preserving
the most important information the most
important information for us is where is
the best feature match for pooling we
usually choose window like two by two
pixels sometimes three by three and
inside the window we take some maximum
value which is the best feature match we
don't care we're exactly on the image
width feature matches as long as it's
inside the window so let's do pooling we
slide image in the window and we do our
pooling process that's it by the end of
pooling process our image shrink by
about 1/4
next step and that is a how you do it in
layers and nodes one node output node is
connected to four input nodes activation
function is maximum and the weights are
one because we don't manipulate if any
values inside the input nodes we just
choose the maximum as is so that's
pulling next layer is rectified linear
behind a very scary name I don't know
who invent these names
really but the mathematic is really
simple take all negatives replaces zero
that's it that's what it's behind just
to place all negative values with a zero
the role of pulley of rectified
line-oriented actually to improve
mathematical performance that's it so
what we've done so far done convolution
to find all the features we've done
pooling to shrink the image or feature
maps in all case and then we done
rectified linear trust me we almost done
by now this is a nodes representation of
the same process last layer last layer
is called fully connected that's also
the layer which takes a decision to
which class our image belongs to to do
the decision we take all the results so
far we have from the convolution pooling
in rectified linear and join them into
one huge input array each valley of
misery have a vote to cast to decide if
an image is one class or to a second
class in our case first class yeah it's
an image innate and I want no no it's
not me so some values hand can have a
stronger opinions about which class or
image belongs to and walls get more
important votes or in case of neural
networks they have bigger weights and
that's actually how convolutional
Network works we are through the process
its convolution pooling rectified linear
and fully connected at the end there is
of course a catch when I started the
presentation of convolutional net I said
here is a feature and when the fully
connected layer came to take this even I
say all those vowels are more important
of course we do not set these values by
hands
convolutional net learns it during the
training process so the training process
we need a training set and we know in
advance what the results of this
training set is so initially we identify
all the needs things we need for network
with some initial weights some
randomized weights and then they sent
the image in and they observe the result
and we correct our network to get closer
to the expected result let's say we send
an image and oh wait and we got the
result it's not an 8 so we know that
result should be and you know what
result is which we get with the network
as it is so we know the error we know
the error and they know the mathematics
behind the last layer which is fully
connected in this case so we can
calculate the error of the last layer
the error of the current settings of the
last layer now we know the error of the
last layer and we know Mathematica
failed X plus next last layer so we can
calculate an IRR of the next last layer
and so on we can calculate all errors of
all layers then in on all errors and you
know the expected result we can tweak
our network and just the wall list to
get slightly closer to the expected
result and then ascend next image in and
absorb the result and this is repeated
for all images in our training set and
sometimes not even once the process is
called bad propagation and it's slightly
complicated math behind it so I'm not
going to walk you through that I just
want to give you some intuition on
understanding so that's it was a simple
convolutional net with four layers
actually I think you agree with me not a
rocket science right just a little bit
ok but it's not that rocket science that
was a simple network of course each
network can have more than one of each
type of layer and it will form a deep
convolutional network what is
interesting about deep convolutional
network as you know how it works now you
can imagine that as deeper we go in a
calculational process then features we
get are getting kind of proportionally
bigger in comparison to initial feature
and it's also becoming more complicated
in features deep inside the convolution
Network you can actually see patterns
and probably difficult to understand but
that's what I mean this is a convolution
Network joined on faces and this is a
first layer and you can see some
indistinct spots and edges and I don't
know what but the last layers are
clearly looking like human faces and I
think this is cool sorry I think that's
very cool because I mean when I when I
started to learn about neural networks
its architecture sometimes I bumped into
some videos and just wanted to I wanted
to experiment with that and I said oh ok
that is a let's create a convolutional
Network I'll explain what there's no not
your own everyone knows what controllers
not a Turkish and I was like feeling not
so good about that because I didn't know
what it is and I think now you can be of
those everyone who knows what kind of
additional entities yeah and that would
be I think that is very cool explanation
I think I personally think that so I
hope you all understand how it works now
and now the most probably the most
exciting part of our demo I don't know
our presentation we promised you a demo
so demo time it is so before we showed
the demo into into frameworks and before
that what I wanted to stress about is
that like 10 years ago maybe five years
ago maybe even before maybe even even
three years even three years ago
okay so then deep learning and neural
networks and that in working with that
was a privilege of departments of
research institutions or big companies
who could experiment with that who could
write the code from scratch in C++ and
do and do their experimentation and try
to try to train the networks and those
who had access to data and access to
very fast computers who could actually
train the network and today there are
lots of frameworks and those are open
source and those are implemented all the
sigmoid functions and other
architectures and then you it takes
really few lines of code to implement
your own network and that is for for
everyone for hobbyists for hackers for
researchers for programmers for everyone
who could who would like to experiment
with that who built their own app i
powered
or I just wanna learn what it is so that
is kind of a very good time to start
playing with that and it doesn't require
a lot of investment in training and
learning of course everybody who is
doing if data science knows that trick
is now is not to write the network trick
is always to collect the data and that's
true that's actually very true now so
we'll give it an example based on my
NIST case study every anyone heard about
them in this before yeah yeah of course
I just thought about it like half an
hour ago
come on so Emma nice as a as a database
of images that represent a handwritten
digit and our goal is to create a neural
network to predict correct digits from 0
to 9 from the image of that digit so we
see that the image of the handwritten
digit and we and we want to see what the
we want the system and the network to
tell us what digit it is and that is a
kind of example of what this digit would
like would look like for example this is
8 and then the whole bunch of eights a
whole bunch of nonsense because people
have different handwriting and they
write it differently and what we do we
kind of they have images that is 28 by
28 pixels and it looks like that and
then it's just a grayscale and intensity
of the pixel each that we see in every
and what we do actually we take each
pixel and we reshape it into big array
so we don't use it as a matrix we use it
as an array and that's become our input
layer so kind of zero pixel pixel 1
pixel 2 and then we come to the end and
then we have for each image a label that
is correct for example if there is 9
then we have a label 9 for all the nines
for all the images nice we have a label
9 so we use that data set to train the
network and this is exactly architecture
that I will use in my demo so this is a
simple perceptron and then it has
activation that's called softmax but
it's not that important at this point so
what I'm doing demo No yeah we're doing
but then we need to switch yeah
I need to switch but your computer is
asleep okay so Katya is using tensorflow
with Kara's on Titan and she's doing
perceptron not because of tensorflow can
not implement something more
sophisticated of course it can just
because it's more fun to show slightly
different stuff that's right so I just
what I do here I this is peyten API and
what I do here I just load my Emily's
data and I wrote my train set and test
set I do it so I used my train set to to
trade my network and I use completely
different images that my natural haven't
seen before just to test it and and that
what I'm doing I'm doing some reshaping
and actually this is my network this is
like exactly it's five three lines of
code to design it and then compile
underneath it so it's three lines of
code exactly and then what I do I fit a
cold feet so I actually drain my network
by using the amnesty images and then I
save it into the file so I'll just run
it now it just runs maybe 15 seconds
it's using texture for backhand and
tensorflow has also its own API I'm not
showing that presentation there's lots
of options of course everybody in
audience is very familiar with Python so
you obviously understand everything
which is on a slide right that was a
joke so so we trained our network and we
saved it on disk and what I will do now
in my demo accuracy is 92% and I must
say easier 1990 okay almost 90 free but
still for object recognition by nowadays
it's pathetic that's pretty bad and that
but it's very good for the presentation
in real life is pretty bad but yeah this
is very very more example this is very
simple architecture of the network so
what I do here I load my model so I
saved my model in JSON file and h5
binary file I load my model in here and
then it would I do I just select a
random image from my test set and then I
can visualize that
visualize that label and visualize what
data was predicted don't that look
something like that
yeah that's lifetime it digit
recognition on the fight and use
internal flow and Kiera's if we are not
done yet no no we have a real
convolutional neural net
yeah well another framework I'm not
switching back okay I do my part of the
demo with Microsoft cognitive toolkit
for deep learning and recently it was
renamed to Microsoft cognitive toolkit
before it was Mike's of convolutional
network toolkit and Microsoft rebrand
their stuff more often that I'm able to
remember so I just still address it to
SC indigo sorry for that Microsoft so
everything you need to know about Cindy
Kay is available at the same ticket I
type it in your web browser and there we
have it
you have all the tutorials all the
explanations all the basic part how to
install step-by-step very easy and the
most important there is a link to github
where it has all the code so you
actually can see how it's written and
you can participate if you feel brave
enough I don't yeah okay so it's
open-source obviously and it's cross
platform runs on Linux Windows dr. you
name it it's production-ready it's
already used in production and it scales
the multiple GPU it scales to multiple
servers and it scales to GPU in Azure
this feature was implemented just like
maybe three months ago so it's quite
cool why I like Saiyan TK is because it
allows me and you of course everybody to
compose almost any kind of networks with
very simple building blocks and those
building blocks are coming predefined if
you want to use something predefined but
it's also flexible enough if you need
something special you can write your own
building blocks and of course some
simple models already included and you
just need don't need to write anything
say I want this typical network and
there's so many inputs so many outputs
so many so this is how I want to train
it and you ready to go basically takes
some few lines of code and then you have
a script so
it's actually original it was only
script language called brain script okay
and you define your network with your
script you define your what you want to
do
usually it's train and test and you run
your script and it spits out the model I
mean it trains the model yeah so it's
training then you can when you have a
trained model we can evaluate it
that means you send a question in and
you say okay what neural network what do
you think and gives you an answer and
then of course if you're satisfied we
deploy it and you're ready to go so you
can define network as I said on brain
script but recently we published update
and lost latest version you can also use
Python for that to evaluate the model
not to train but to use already existing
model you can use from C++ C sharp
Python you name it a bunch of languages
my demo is done on c-sharp because I
used to be a c-sharp developer so I'll
show you first I just want to show you
my data here it is labels and all the
features all the 764 84 it cut already
explained it so I'm not stopping in it
that's my script I say there's a comment
the comment is train Network test
Network I need to test so I know how
good my network is and of course we have
a different data sets want to train you
on to testing then it's a dated year
blah blah blah so it says where actually
I find the data of the most interesting
part is here that's where I define my
network and that's actually the natural
definition all the known stuff
convolution max pooling again
convolution again max pooling heroes
rectified linear and then so there is
actually a fully connected one so that
is the description of a deep
convolutional network it's like
literally ten lines of code nine
actually it's even less so to run my
network when it type is to train you
know you have to train my network I need
Centocor and then I specify a config
file of course you need to install
Cintiq on your machine as a framework
but that's not complicated though I have
it installed so I will not go through it
so and the next is file config file and
then I run it in my case I already
rounded before so my net
distraint so it will skip the training
process because it understand I already
have a model so it will just validate it
oh it sounds completed test what I'm
interested here is this I didn't train
it long enough or it's not very
complicated Network so it gives me about
0.9 process percent error not so good
but good enough for demo so my network
is actually stored as a file is that one
it's 395 kilobytes so now I will build a
c-sharp application or should I have it
built I will show you which loads these
networks and call it with a Cintiq a
framework that's my web application
standard it's also from demo I didn't
build anything from scratch I just took
a tutorial silly it's like straight
forward this is whether I build my
network I loaded from the file you don't
really need to understand the only thing
I changed in example I just provided my
file where the model is and here I get
my BMP file which I upload and just make
a feature by making an array out of
pixels and that's the place where I
follow it my model and get the result so
little it's one screen code now let's
run it and see if it works because today
in the morning suddenly it's like homo
access denied I don't know why typically
so I run my super brocation see I'm very
good and user interface component Li
vanilla JavaScript no crap let's send in
picture and load and check first time
it's a bit slowly slowly no I see and
then thinks it's the one let's take
another one so you don't think it's a
hard-coded stuff hey upload and check
and CNN thinks it's an eight and that's
basically called simple it is to create
your own deep network and to run it and
train and then call it from your code
it's very simple so that was our demo
thank you and not miss it okay yeah
we have a meet-up in also it's about 850
members by now
and we run different topics deploring
included usually more like general
machine learnings we do it once a month
but sometimes more often if you have
more to talk about because it's a site
of a work so it's free time activity so
if you ever run in Oslo check maybe be
stable run some exciting Meetup
if you want to talk contact me at a
Lumiere Co that's me because I'm an
organizer so we can make a meet-up for
you yeah and the second tweet is Katya
Katya geek mhm and what we want you to
bring from this session is that neural
networks is very simple it's like the
bar now is very low go home read about
it
try build some evil or not evil
artificial intelligence in get us into
bright future thank you very much thank
you if I could to ask about questions
but we are still here just come and ask
yeah I don't think I think we run out of
time I think this still has always still</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>