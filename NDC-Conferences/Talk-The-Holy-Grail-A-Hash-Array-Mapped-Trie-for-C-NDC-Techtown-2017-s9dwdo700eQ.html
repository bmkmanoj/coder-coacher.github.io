<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talk: The Holy Grail  - A Hash Array Mapped Trie for C++ - NDC Techtown 2017 | Coder Coacher - Coaching Coders</title><meta content="Talk: The Holy Grail  - A Hash Array Mapped Trie for C++ - NDC Techtown 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talk: The Holy Grail  - A Hash Array Mapped Trie for C++ - NDC Techtown 2017</b></h2><h5 class="post__date">2017-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s9dwdo700eQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello everyone it's three o'clock
I'm gonna get started because I got a
lot to get through first few minutes is
fluff anyway if there's still people
coming in so welcome to the Holy Grail
are you persistent ha sure a map tree
the C++ a lot of words in the title I'm
actually going to spend a big chunk of
the talk just explaining what each part
of that means and how it fits together
so if there's any bit you're not clear
on now don't worry
so I'm Phil - I'm a developer advocate
for JetBrains that means that I
typically deal with with the free C++
oriented products we have C line for
cross-platform IDE
resharper C++ if you use Visual Studio a
plugin or app code for iOS and Mac
development we're not gonna be talking
about that today but if you do want to
know more about that do catch me
afterwards or tomorrow here both days
now I'm also known as C++ circles for my
test framework catch but I'm going to be
talking about that tomorrow another talk
at the same time or no catch to whom
what's new in that so if you're
interested to come along to that as well
but today we're going to be discussing
this this thing hash array metric and
I've chosen this sort of slightly
controversial title the holy grail just
to try and get more people in really my
god is this big exclamation mark at the
end let's change it actually to a
question mark the reason I chose that
term is because I'm going to present a
data structure that my claim is it has a
minimum trade-offs compared to
associated data structures that exist in
a c++ standard library we'll see by the
end of it if you feel that I've met that
claim and what that claim actually is as
we go through so the rest of the title
we're going to start drilling down all
each part of this actually means so it
can build up to where we're going with
it so I'm gonna start with the the last
word in there I've been pronouncing it
tree you may have been reading this try
before you've probably seen these before
in a different context usually a string
based tree like a string dictionary
where you go for each character and that
becomes the index into the next level
down this is very very similar but I
will explain exactly how we apply it to
hashes
and when we get their reason it's
pronounced tree is because it comes from
the middle of the word retrieve or
retrieval Factory we look at the the
Wikipedia entry we says that they were
first described in 1959 so it's not a
new idea by me stretchers so most of you
probably come across them before term
tree was Quinn two year later by edward
fredkin who pronounced it as tree after
the middle syllable of retrieval however
other authors pronounce it as try in an
attempt to distinguish it verbally from
tree because there was a bit of
ambiguity
whichever pronunciation you go with but
trees with disability like a subset of
the more general tree so it's not too
bad so you can carry on pronouncing it
try if you like I'm going to stick to
tree partly because that's like
historical accuracy but mostly because
I've already become well known for catch
I'd only become known to try as well so
there's that all right let's uh let's
mix it up a bit let's talk about sex
in fact more generally I want to talk
about associative data structures in C++
which there are currently eight here's
all of them nicely grouped together and
these obvious pairings we got the the
single and the multi valued versions I'm
sure don't need me to explain what the
differences are but in terms of
underlying data structure there's really
no difference this is really more of a
constraint on insertion there may be
implications but usually that's the same
underlying their implementation so I'm
going to cancel these out and we're just
going to look at the slightly simpler
single key versions so you've got set an
ordered set of map an ordered map so now
we have a like an obvious division on
the the horizontal axis there we've got
access to the swap maps on the bottom
again terms of the underlying data
structure there's very little difference
you can implement a map in terms of a
set just by having a set of pairs where
you really do the identity on the first
value the key and that's often how
they're implemented so
terms of interesting differences in data
structure I think we can just
concentrate on sets of my opening a
surgeon there let's talk about sex this
now becomes a
indivisible division of two different
types of data structure before we get on
to what the difference is actually are
just an honorary mention to the good old
salted vector it's not strictly a
standard library associative container
but it can be used as one I do many
applications it's still a really good
choice particularly compared to a
standard set gives it the binary search
but we better cache locality and less
fragmentation so we're not a Raleigh
tail but for our purposes today I'm not
really going to consider this although
we may touch on it a little bit more
later so back to the sense a few
properties I want to draw your attention
to again it should be nothing new but
those underlying data structures sets
are not required to be implemented those
red-black trees they almost always are
haven't actually seen one that isn't
there are at least balanced binary trees
we'll come into that a little bit later
an ordered set is obviously based on a
hash table so this leads to various
other properties so looking at the
constraints and the types you put in
them we've set you have a strict weak
ordering usually as a result of having a
lesson operator you could also provide a
custom predicate unordered set the types
must be hashable usually with an
specialization of standard hash and
equatable usually with an is equal
operator again this can both be provided
with custom predicates if you prefer but
a sort of orthogonal constraints in the
types and sometimes that will actually
dictate which structure we can use
usually we're more interested in the
complexity guarantees which tend to lead
to performance so set house login or
lookup we're mostly going to be
conserving lookup today whereas an order
set has this really good constant time
complexity on average but we have to
forget this it can actually tend towards
linear in the worst case and we don't
often actually hit that but but you can
actually get into it we will see an
example of that a little bit later but
usually that that constant time lookup
on average is what gives us the
performance actually translates into the
performance
so usually where we can performance is a
goal we will reach for an ordered set
over set and that's an important point
we'll come back to a bit later a little
bit of background we're going to dig
into some of the technical details of if
in a moment I wanted to move on so one
of the other words in the title to clear
that up and this is the persistence
persistence and actually want to mention
first of all what are we not talking
about so I put this picture up here so
this is a near the end of the London
Marathon last year it's definitely not
the type of assistance we're talking
about here but we're also not talking
about anything to do with databases file
system networking you anything like that
that's not the type of persistence we
mean that's common confusion when people
hear about persistent data structures
for the first time we are talking about
is really best explained for an example
so start with something really simple a
list they don't tend to use lists so
much in C++ these days you could say we
hate lists but they're really useful
example and still very common with
functional programming languages because
of this property so let's just have a
quick recap on how a singly linked list
is implemented okay we're gonna whiz
through this because I'm sure you're
familiar with it but linked lists will
have a number of nodes each with a value
and also a pointer to the next or
previous note depending on how you look
at it and the first node is usually
called the head and the rest for the
first node of the rest is called the
title now what's interesting is if you
if you want to add a new node to the
head the only thing you have to do is
just write pointer to the original head
into the new node the original nodes
don't actually need to change at all
they don't usually deal with the nodes
raw we usually have some sort of wrapper
type there which we used to call the
list and they would have the pointers to
the heads and maybe a size as well so
they're pretty lightweight and shape all
the other storage is in the nodes and
that means that if you perform all your
mutations by actually producing a new
list instead of mutating in place
you can keep the original values around
that's what we mean by persistence the
old values persist and this is a really
interesting property that usually
naively we might think this will evolve
a lot of copying and yes sometimes it
can tend towards that but in the best
cases like this one there's no
additional copying involved to those
notes the long to working at the head
and we can we can do removal operations
as well from the head we can stack these
things up we can get whole generations
list with a very low cost because
they're all sharing their common State
and of course that breaks down a bit if
you start to do mutations further down
the list that will have that be more
copying and we don't tend to use linked
lists in C++ anyway very much it's very
occasionally the right
they destruction but all the point of
hopping lack of cache locality and that
sort of thing will I mean that back to
the usually a better choice but very
illustrative for this purpose and we can
we can take what we just have been
talking about there and start to move it
up by just adding a single extra pointer
so we have two pointers and each node we
have a tree simple binary tree like this
one here is our strict weak ordering
ordered from from left to right now if
we want to add a new item to this
structure say it is for up here well we
just reverse down the tree to find where
we need to add it in we do have a couple
of choices we could hang off the free or
off the five but we hang off the five
then we don't need to rewrite any other
notes and that's what we always do we
hang up an existing leaf that's nice and
simple and if this was a mutable data
structure we could stop there but if we
want persistence we need to keep the
original structure around so we can't
actually mutate our five node there what
we're gonna have to do is take a copy of
it and then write the new pointer to the
new leaf into the copy and then by doing
that we also have to write a new copy
into its parent all the way up to the
root so this is a common pattern you'll
see in persistent de disruptions this
sort of copying a path up to the root
but that's all the copying that's
involved and in a larger tree this will
actually still be sharing a lot of
and state she end up with something like
this so we've got the entire original
tree untouched over here and then just a
new path with links into the old tree so
it's very a lot of common state usually
a lot more than is shown here so that I
was actually fairly easy to do but this
was a simple binary tree and they can
tend to get very unbalanced imagine
loading out we've already sorted data
you'd end up with effective like a
linked list on one side or the other so
we need some strategy for rebalancing
the trees that's why say with standard
set there's got to be some sort of
balanced binary tree and different
strategies for doing that the most
popular one is the red black tree at
least in the standard library
implementations I'm not going to go too
much detail on this because that's not
the subject of this talk but he's all
the rules if you're interested what I do
want to do is just quickly look at how
that would play out with our example
just now and how it relates to
persistence so here's the same example
again we've now covered all of the nodes
red or black according to those rules we
can easily do that with with no storage
overhead by the way we've added a new
four node down here and that breaks one
of the invariants which was that no red
chili red node should have a red child
so in order to to fix that we usually
consider the parent grandparent or uncle
nodes so we can do it locally with local
reasoning and in this case we just have
to recolor those nodes
according to those rules and now we've
exciting they're in but no we broken it
again between the three and a seven so
we just recursively go up towards the
root that violates the black node black
root invariants so we'll you recall
about to fix that so we're just been
recursively applying those I couldn't
seem to come up with a small enough
example to fit on a slide that actually
involve moving the nodes around
so in this case we just recolor them but
they will often involve moving them
around as well
the reason I wanted to illustrate this
is because now if you look at all the
nodes we touched as well as that path up
to route we've got a little bit of
collateral damage as well better touch
some of the other nodes around them so
in terms of persistence as a slightly
higher cost
we've red black trees but they said
persistent red black tree
a really useful data structure fact
consulting Wikipedia again interesting
paragraph on the page for red black
trees says that red black trees are also
particularly valuable in functional
programming where they're one of the
most common persistent data structures
used to construct associative arrays and
sets which can retain previous versions
after mutations what we were talking
about then it talks about the the log in
space and time so there are useful I've
implemented these in the past that being
very useful for us but think back to
where we started talking about sets
versus an ordered sets usually the
reason we go for the own order sets for
performance so just because we want to
make these persistent do we want to have
to get up on that performance and that's
where we're going to lead to all gel now
hash free so you could actually approach
this from the the hash table side you
can't make a hash table in itself
persistent I mean you can generally
evolve copying the whole hash table so
it becomes counterproductive really so
you might start to think well what about
if we break this table down into chunks
that we arranged in a hierarchy and that
sort of line of reasoning may take you
to where we're going I would approach it
from the other side though start we've
red-black trees which why I've dwelled
on that for a little bit too long the
problem was we've only got the two
pointers at each node and that puts a
limit on you know how shallow we can
make the tree it's pretty much fixed
I've got that log in complexity so if
that's the problem
oh and by the way about the implication
of this certain examples you've got a
set of about 15 million entries in it
which is not that unusual this could be
about twenty four nodes deep it's a lot
of pointer hops a lot of cache misses
and that's where the hash tables really
start to shine but if the problem is not
enough pointers surely the solution is
just to add more pointers and in fact
that is it's exactly what we want to do
so don't have too many because then you
end up with something like a hash table
where defeats the ability to make a
persistent so we've got to have a
all enough number that we get the
benefits without so much of the downside
and in fact that's exactly what we gonna
do but it raises now a couple more
questions in terms of how we implement
that and in answering those questions
that's going to then lead us to our hash
tree so what are the questions by the
first one is how do we know which
pointed to follow with the binary tree
was easy we just do a less than
comparison go left or right what do we
do here first have a look at a slightly
more concrete example so we've got
branch know is you've got leaf nodes
each branch node is going to have an
array of pointers in I've chosen 32 here
that's not an accident will come in two
why in a moment and you know some of
those have pointers to branches some tea
leaves so what we do then is we take the
hash of the key or the value exactly as
we would do with a hash table but this
time we just peel off the first five
bits and that becomes the index into B
right this is why why it's a tree crie
so in this case that 27th bit is a
pointer to another branch node so we
follow that down we then take the next
five bits that gives us a new index six
in this case the six bit takes us to a
leaf if it been another branch we would
go down again and we'll keep going until
we are exhausted half or we hit a leaf
so exactly as we've hash tables we will
have hash collisions so the leaf nodes
must contain an array of values and we
have to have a strategy for resolving
that as well so that could be a linear
search if you would do with a standard
set now you could do other things as
well
we'll come back to that point little bit
later as well so that's where the tree
comes into it and that's how we managed
to to have a large number of pointers
but that leads on to the next question
which is how do we deal with this large
number of pointers forgot in this case
32 at each node and on 64-bit
architecture that's quite a lot of space
that most of the time as you can see
it's going to be null pointers we want
to pay that space
for all those no point more pointed so
sort of smells like we want some sort of
sparse array to hold these and that's
exactly what we're going to do so what
we do is we have a compact array which
has just enough slots for the number of
pointers that we need free in this case
and then we have an additional integer
which acts as a bitmap into our compact
array so if you imagine a virtual sparse
array of 32 slots each on set bit just
corresponds to a null pointer and a set
bits to a pointer in the compact right
pretty straightforward so we just use
that bitmap to map by finding the bit
that corresponds to the index you want
and then counting the number of set bits
to the right of it so you mask out the
top bits let's count the set bits and
that gives you the index so if you take
the the leftmost bit up there there's
two bits to the right to the right
that'll give you an index for two it
gives you the right entry sounds simple
enough then it raises another question
of how do you efficiently compute the
number of set bits you could do that
naively quite easily but gonna do it
efficiently we'll come back to that
point a bit later but that's the
approach that's where the array Maps
part comes into it so we now pretty much
have the complete definition of a
persistent hash array mapped tree so
we've arrived so what we get something
like this again trying to keep it simple
for the slide so I haven't shown a lot
of branching but it's a few branches
there with more than more than two
pointers to give you an idea and you can
see the hash codes that we've we've been
peeling back as we go down it's a simple
path up to the root so he's a much
shallower fact let's have a look at the
some of the properties of this structure
so for a 32-bit hash because you could
have 64-bit hash of course we've got a
max depth of 6 or 7 say 6 or 7 because
if you peeling back five bits at a time
you'll have two at the end it's probably
not worth considering that part the hash
let's just leave it to hash collisions
so they'll give you six usually for that
15 million baddies example we've got an
average depth of
five significantly better than 24 in the
red black tree case not as good as a
hash table but close enough
as we said the leaseholder raiser values
because we need to deal with hash
collisions but it's more space efficient
than a hash table because we're doing
that sparse all right technically in
this case the complexity is log 32 n
which sounds really awkward see where to
imagine in practice therefore most
real-world purposes it does track much
more closely the what constants I'm on
average complexity of the hash table but
it's not quite as good and here's a
bonus we don't need to do any
rebalancing like we did with the the
binary trees the reason is because that
distribution of our tree is going to be
based on the distribution of the hash
flipside is we do need a good hashing
algorithm if the the hash is not very
well distributed then our tree won't be
over and we'll see an example of where
that goes wrong in a while as well so
that's the theory let's actually look at
some of the the outcomes of all this if
you looked at the abstract on the
website for this talk there's a bit that
I hope I made this claim I like that
talk about hash trees say they're close
but not quite as fast as pure hash
tables that's the claim now I've written
a reference implementation of this it's
a it's not complete for usability
purposes but it's completely enough to
run these run some tests on it to see
how this plays out so I'm gonna have a
look at some graphs now control your own
conclusions so we look at the the Palmas
so my methodology is well I used a micro
benchmarking framework called non est Oh
since I did this a few months ago I've
actually started to add micro
benchmarking support to to catch and
I've actually converted the tests over
to that but it's not quite as rich
enough to produce the graphs I need to
get so I've kept the original graphs for
now and what no-nos does it will run
your your tests long enough to to be
meaningful versus the clock accuracy
but then it will run a number of samples
about default or hundred some of them
have done a thousand to make sure they
are statistically relevant either never
do statistical analysis on that and give
you the the mean in the variation and so
on discarding outliers so it's pretty
good for that now be used clang with
lits lip C++ that's relevant because
we're going to be comparing against
stone to stone I don't always set its
bat libraries implementations so there
may be quality of implementation there
concerns
that's all I've done so far though would
be interesting to see how others fare
I've started off by testing loading up
one of these containers all of these
containers with a hundred thousand
integers the monotonically increasing
series of integers so let's have a look
how that plays out here's the first
graph she's for insert and the the blue
box here is a standard set large green
box is hash table or hampt for short and
in this orange bar down here almost
hardly see is an ordered set but the y
axis is time
which means I'm really not doing very
well on this one it's a bit of a
disappointment I'm about four or five
times slower than standard set and about
twenty times slower than on August says
so what's going on here there's a couple
of things the most important one is that
this is insert and mostly optimization
so far has gone into lookup but we're
actually playing the full cost of
persistence here every time you add a
new node you'll be copying whole whole
paths so it's quite a big overhead there
are some optimizations you can apply
which I've done very successfully in my
red black tree implementation before now
which mostly involves a form of
copy-on-write if nodes are not shared
you can actually take them in place
because no one outside has seen them yet
I haven't done that here yet I'm fairly
confident that will bring it down to at
least the level of first standard set
and hopefully closer to an ordered set
but I can't quite prove that yet so
there's some asterisk around this one
and that's insert so I said look at
lookup then I'll shoot before we do that
another thing to note about these graphs
is let's figure it back and if you can
see the the black lines which is the
variance is actually quite high variance
in places if you look at the the
scatterplot diagram instead you can see
that the orange in the blue lines are
not too bad but the the green one for
the hash tree gets very jittery towards
Ian and the right at the end there's a
big spike there's obviously something
going on on my machine when I ran this I
try to shut down as much as possible but
you can't get everything so no Gnaeus
will explode those outliers but it's
it's averaged included in that second
half of the graph there as I think it
would be ferret when you considered the
first half doesn't make a big difference
in this case but I wanted to point that
out because for now on I'm just going to
look at maybe these graphs or my
aggravation aggregated views all right
since they could look up so the same
hundred thousand integers but now we're
doing a find so now the blue of the
bottom is still unordered sat still the
winner but the orange on top is standard
set so green is now beating now so we're
getting closer starting to get there but
it's still closer to stand on our old
set
oh sorry standard set rather than
unordered set so I'm not quite meeting
that claim yet so slightly disappointing
still boss what's the problem here
well and where I said that our the
balance of the trees dictated by the
quality of the hash algorithm so we're
using a sequence of monotonically
increasing integers and we're using lip
C++ and it's a default implementation
for standard hash for an integer is just
the identity function which means our
hashes are also a monotonically
increasing series of integers so we've
got a really unbalanced tree here and
even with that were beating sets that's
not too bad but as an opportunity for
improvement that there's a few ways we
can we can tackle this which I've not
had a chance to do yet I did experiment
with first and hashey mixing algorithms
so he made things worse so I've got that
for now them I'm confident we can get
that down as well so let's try another
example where we have better hash
distribution so what I did next was I
took 100,000 integers and then I by
sixth base 64 encoded them
and use the strings resulted from that
so we've got 100 hundred fellow before
we get on to that actually I did plot
this for all the orders of magnitude
between 100 and a million well we'll
come back to those so the next one is
strings with a base64 encoded versions
and here you'll say for the orange line
at the top is set again but the blue
line in the middle is unordered set at
the bottom the winner on this particular
test is hashed rate and that was a
little bit surprising
we're actually beating hash free in this
example that's a specific case and we're
comparing against a specific
implementation and say there can be
quality of implementation issues so I
fully expect another implementation of
unordered set 2 to beat hash tree but I
think this is good enough to show that
at least in this case we can be in the
ballpark for my claim few asterisks good
name but this is really promising I'm
still a little bit worried about the the
Instituto so I then took those strings
I took their hash codes and I ran the
test again with integers based on those
hashes so you've got the same
distribution but with integers and this
is interesting I wasn't expecting such a
change in the profile but we start off
with hash tree being the winner again
setting on order set a much closer but
they're all much closer but somewhere
between 10,000 and 100,000 they will
cross over until at the top this one
goes up to ten million by the way
standard set becomes the winner
especially beating unordered set and an
hash tree and I suspect the reason for
this is though that this level the the
hash collision start to become
significant so I said before that that
linear search at the end can become
significant I think that's what's
happening here
and we've hash hash tree is also
currently doing a linear search for its
hash collisions but what we could do is
we could detect whether to type our
supports strictly recording and then use
that to do a binary search at the end I
haven't tried that yet but
I have a feeling that that will bring us
down to probably beating out standard
set at the top there as well but in any
case I'm reasonably happy with that
terms of trade offs yeah I think we're
doing okay so that's the performance of
the safe I'll leave it to you to decide
would be meeting that claim because
there are a couple of asterisks but I
want to move on to is looking a little
bit of the code now the first version I
did of this talk I actually had 90
minutes I went into the code in quite
some depth of standard time for that
here but there's a couple of bits I want
to call out the first ways to go back to
this slide where we talked about the the
array mapping and now I said at the time
that the trick here is how we did
account set bits that efficiently
there's one implementation got this from
a stack overflow of course in fact stack
overflow led me to this this site on
Stanford Hagee it's a whole page or bit
shifting algorithms to do just this task
counting set bits whole load of them
this one seemed to be about the best on
average the more specialized cases are
some better ones
there's no branching in here so it's
it's actually pretty performant it's all
bit shifting but notice that common in
the middle like this comments from the
code by the way could be substituted for
an assembler instruction if available
and in fact if you're using GCC or clang
you can use this intrinsic built-in pop
count but with a certain compiler switch
will give you that assembly instruction
and that's what I used in these tests so
if you if you use the previous bit
shifting version instead everything just
sort of shifts up a little bit it
doesn't really change much except I
think but the one where hash tree was
winning its pushes it slightly above an
ordered set but everything basically
hold so it's actually pretty efficient
this is obviously better and most
architectures will support it so it's I
think it's fair so also going back to to
this slide where we showed the hash
table like we said with lists we don't
usually deal with the nodes raw we use
wrapper type which will also include the
sign so we didn't have to visit
everything to compute that and in fact
what we can do in code is just capture
those in a single data structure it's
going to be a pointer in a size which
for most architectures we can deal with
atomically and that has some
implications so I use this hash tree
data structure to to fit into a more
general hash tree wrapper and we're
going to the whole this in detail but it
basically just has more sort of standard
container like a wrapper for that and
all of the logic for insertion is in
there as well greggers interesting is I
have another type called shared hash
tree and this will hold that hattery
data atomically so we can do an atomic
compare and exchange on that but that
means this can be a place that you can
share between threads and you just ask
it for its current copy the hash tree
data because it's a copy that we can get
atomically we could then work on that
off to the side and then commit it back
later like put things in there to make
that a bit easier so looking at a test
case for this
I'm actually dealing with concurrency
here but you can see get the shared hash
tree start a transaction gives you back
an object that you can get a hash tree
from you can then insert it your ledger
and commit it back now commit won't
necessarily succeed because if someone
else in the meantime has changed it then
they all know it'll say that failed and
you have to do it again in a loop so to
make that a little bit simpler I've got
this update with method which will you
pass in a lambda which gives you back
the hash tree you can start modifying
and at the end it will commit it back
and we'll run this in a loop that's
necessary so did I yes that's the
implementation in it it's actually
pretty straightforward the cause in to
commit their commits what does the
atomic exchange but it's pretty
straightforward but otherwise it will
rebase based on the current value of -
tree
so I managed to get through that in time
which I was quite surprised about come
to our summary so hopefully I've
convinced you that we can approach the
performance of hashmaps even though
there's a little work to be done as we
mentioned the more space efficient the
hash-table so that can be an advantage
when you're dealing with particularly
large data structures cop is a cheap and
memory efficient particularly if they're
non-modified copies there are basically
just copying two integers and you can
choose whether the old value persists or
not you can hold multiple variations in
memory at once we didn't really delve
into this but this is a really good
practical use case I originally got into
this when I was dealing with financial
data and we would have large object
graphs of market data where we're going
to run scenarios on them so we actually
take multiple copies and multiple
dimensions of the whole graph and make
small modifications and then you could
have mostly shared common state between
them we couldn't do that all in memory
before we have these sort of
implementations and there's many other
use cases like that unless we touched on
at the end can be made to work nicely
with concurrency by default if you use
the Atomics if you haven't got a lot of
contention on the writes then that can
work really really nicely if you have
then you might need to use locks around
that but you get the choice so there the
the benefits and that's me there's a
list of references on my website a level
of indirection com if you can't remember
that I've also got extra level of
indirection comm and that redirects
there slash storage slash hampt rats to
HTML so it includes my reference
implementation another one that's come
up since the previous talks on the
subject and some papers so a lot there
to to pick out or you can cache me on on
Twitter so thanks very much for
listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>