<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Networks are like onions: Practical Deep Learning with TensorFlow - Barbara Fusinska | Coder Coacher - Coaching Coders</title><meta content="Networks are like onions: Practical Deep Learning with TensorFlow - Barbara Fusinska - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Networks are like onions: Practical Deep Learning with TensorFlow - Barbara Fusinska</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tIOZasGMrP8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone can can you hear
me
yeah right
so thank you for joining me for this
lovely afternoon very warm in London but
nothing is actually very warm in London
maybe for two weeks
so welcome representation about layers
in tensorflow out of curiosity who has
ever here worked with tensorflow okay
more and more people every time I give a
talk on transfer flow who has ever
worked with deep learning okay who has
came here because there's this big hype
and you want to keep up to date for
machine learning and yes yes of course
of course so you're in a good place but
this is not like a very introduction
airy talk and I will tell you how this
is actually not a bad thing for you so
Who am I to talk to you about this I am
a program I have been a programmer for
over 13 years maybe 14 this time because
we have a new year so I can count it for
a new year and my background is in
machine learning also because I wrote my
master's degree the certain machine
learning but it was so long time ago it
wasn't called machine learning it was
called data mining and then I was doing
my PhD dissertation also on that topic
at the moment I meant data science
freelancer I'm X Microsoft I'm joining
Google in like three weeks so I will
stop being freelancer anymore and I will
be a machine learning engineer so
because it's not actually the
introduction area talk I have a good
news for you all of my examples instead
of you going to my github account and
trying to download the examples which
would never do I will point you out to
this amazing platform that's called
cutter code and you have fully installed
setup environment you can run the code
there it's like a shell in the browser
and you don't have to install anything
so all of the demos I'll be showing you
today are there if there is anything
unclear if there is something I couldn't
because of the lack of time explained to
you it will be there
so what we are going to talk today but
what I might going to talk today my
favorite topic which is machine learning
deep learning in particular because we
are talking about tensorflow and then I
will tell you a little bit about how
neural networks are built and I've put a
layered cake here so this is an
indication that we'll be talking about
layers so I won't be focusing on the
training phase I won't be focusing on
the tensorflow syntax I will be showing
you how to use tensor file to build
layers in our neural networks and I will
be showing that to you too to solve the
task of classification and we'll be
using nest data set if you don't know
what it is I will introduce it in a
second
and then I'll go straight to cut a coda
into the code and we'll be showing you
how to build your network deeper and
deeper so we'll go from one layer to
five maybe six depending how you define
the layer so what's deep learning who is
brave enough to tell me what's deep
learning come on don't be shy
yeah
so deep learning is is some kind of like
machine learning that is human neural
networks with more than one layer
depending on the book article or website
the number of layers differ of course
yes yessir supervisor so if you're
saying it's unsupervised learning as
opposed to this supervised learning
actually it can be used for both
supervised and unsupervised learning but
the cool stuff that is happening that
you can read in the headlines are
actually in the area of unsupervised
learning so all of the generation calls
all of those cool tweaks that you put
your picture there and put some because
so paintings there and say change my
picture in this style this is all
unsupervised learning but they have a
supervised elements too inside but yes
yes of course
anyone else okay so my favorite answer I
I agree with you those answer are very
good but my favorite answer for deep
learning was its matrix multiplication
and if you know a little bit about
linear algebra and deep learning you
you'll appreciate the joke if you don't
yet I will try to explain it I know it's
not a good thing to explain a joke once
you told it but that's kind of a point
of this presentation so deep learning
for me is something it's a tool I'll
give you a second to read both text and
both on the t-shirt it's also very nice
so for me deep learning is a tool that
gives us now the opportunity to feel
like magicians not magicians wizards
wizards or witches in my case but we
have gender not so-and-so Wizards we are
now actually living in the future the
stuff that were in sci-fi movies that
were impossible few years back deep
learning neural networks are giving us
the chance to to actually do them and
don't get me wrong I know it's its own
mouths sometimes very complex and
but it's very down-to-earth when you
when you look at those deep networks but
the result the the stuff that you can
see what those headlines are like
hitting those are incredible I
personally feel like Hermione's
sometimes when I when I know the right
comment when I know the right algorithm
to use and solve the problem so I think
we are in a state that we can say our
technology is indistinguishable from
magic
so what are those neural networks where
do they come from
what they are so the idea like decades
ago was to mimic our brain so our brain
works amazingly and we can we can
achieve amazing stuff so someone thought
not just one person a few people thought
why don't we just yeah
figure out how does it work put it in
the machine machines are better in
calculating their cluster so we'll
create something that will be an
artificial intelligence and there was
one problem with that nobody knows how
human brain works so knew a few things
we knew that our brain has neurons right
we have cells that are called neurons we
know those neurons are connected and we
know they are like passing the impulses
to achieve something from there's
neurons intelligence but that's
basically what we know if you know more
on this in this area if you're like
neurologists or a person that is
interested of course we know much more
about how human brain works but when you
think of deep learning and neural
networks as the technology that is in
mimicking our brain that's basically it
we took a concept of a neuron we took a
concept of neurons being being connected
and we took the concept of passing
signals that's all what it is in
artificial neural networks nothing else
from human brains basically taken there
is a concept of layers of course but
from what I read recently there are
layers in our brain but they're not as
straightforward as distinguished as they
are in the artificial neural networks so
well yeah it has to be obligatory some
of you are probably too young to
remember this this movie but this is how
my title came and so basically neural
networks and deep learning is based on
layers the deeper network gets the
better results you're getting the
downside is it really increases that
time that your network is being trained
but the result is worth it
so layers and artificial neural networks
this is a very simple artificial neural
network it only contains fully connected
layers so what does it mean we have
those layers are put here we have four
of them although some people are saying
that input nodes are not actual layer
depending where you read depending on
the tool you you're using and what makes
the network deep are actually hidden
layers so let's start from the beginning
we have some inputs let's say the image
we have some output this is what our
network is trying to figure out and we
have hidden layers this is a very bad
name by the way
I mean in my opinion because those
hidden layers are not really hidden you
can touch them you can see what's there
and the whole magic is actually
happening in those hidden layers so you
can increase the number of cells in your
layers but the real complexity is
introduced and your network will perform
better if you increase the numbers of
layers and this is where the whole fun
starts so let's look closely what is
happening in a single cell let's look at
this cell this cell is connected to all
of the cells from the previous layer and
this cell is starring wait for all those
inputs so it's taking on those inputs
multiplying them by weights and then
aggregating the results usually by
Meishan and then it goes through
something that's called activation
function this is not a very complicated
Maps right it's multiplying adding and
applying a function a function can be
complicated but us as programmers we
know all those libraries have them
already implemented so you just wrap it
up with the function and you're done if
you know anything about linear algebra
this is the explanation of my joke about
matrix multiplication this is matrix
multiplication multiplying adding and
putting in the right places in an output
so for every layer we have a matrix that
is storing the weights for every cell so
we have a matrix input layer contains a
vector and then we are multiplying it
and this is basically what those
calculations are I know if you're not a
mathematician in your hearts not very
funny anymore but yeah I find it funny
so this is fully connected layer this is
like the basics the foundation of how
the maths work and deep learning and
artificial neural networks but the story
of artificial brain starts in the
forties so you can see they were first
like gates electrical/electronic or
gates and they were trying to to figure
out the weights how to teach how to make
them learn to to adjust and then when we
go for 50s 70s and then we have this
first time perception 57 multilayered
with backpropagation the algorithm that
is used for training till now still 86
then there was this type of SVM machine
so everyone that was working in machine
learning and data science till very
recently like five years ago who is
doing as VMs and then there was deep
neural network and basically if you if
you think of principles if you think of
the Foundation's nothing really changed
and now there is this big height now we
can do miracles now we live in the
future now our technology is in
thing possible from magic what happened
what changed before I will tell you what
or you will tell me what I'll just tell
you like somewhere here in 2005 for when
I was picking up my PhD dissertation and
I had this talk with my mentor and we
were thinking what to do next what to do
next with this data mining thing and I
said well this is amazing what those
neural networks can do is we're just
adding multiplying stuff and it can do
miracles and he said well nobody's using
it it's it's not going anywhere it's
very niche projects purely academic so
we decided I will deal with evolutionary
algorithm which is now nobody is using
and it's a very very niche project and
very Academical so what changed why now
why we have all those neural networks
now actually doing things it's not like
it's purely academic it's actually in
production we are all in like
interacting with it one way or the other
or the others who what changed we have
hardware right we have technology yeah
yes so we have hardware but we don't
have to buy hardware right we have cloud
and the cloud has better hardware we
have harder that can do matrix
multiplication better right we have GT
use TP use what else changed we have
data with actual big data it's not like
data that doesn't fit into your memory
is an actual data every company is
storing the data what else changed yeah
I know
but it's very vague I did it on purpose
we have tools right there is this whole
concept now of democratizing AI and it's
not just you know it is also for
headlines of course and forgetting
grunts but if you want you can do AI
like everyone can probably the first try
it won't be great but you will be doing
it so so it's out there for you to read
right and in my opinion because people
actually proved it works so they prove
that you can solved the most complex
task in computer vision you know what's
the most complex task in a computer
vision distinguishing between cats and
dogs nobody could do it before now we
have deplaning now we can do it now it's
a soft problem and a few years ago it
wasn't people are trying to put
algorithmic this cut has pointy ears our
dogs also have pointy ears right and you
try to describe it and the deterministic
rules and algorithms and hit left and
now we just feel your network with
examples you label it boom you have
cotton duck sometimes dogs like
Chihuahua dogs are mixed with cupcakes
that has notes but yeah it's also I
think it's now the unsolved computer
vision problems though so there was
there was this great TED talk and
encourage you to watch it and about the
research facility that was doing this
amazing new technology innovative thing
was trying to teach computer to
recognize images like we teach kids or
like kids are learning from observing
the world and like one kid goes to the
she's telling the story when kids go to
the park and sometimes parents are
saying this is a cup this is the tree
but most of the times they are not and
so they were trying to to teach the
machine teach the algorithm model how to
understand images and they succeeded now
normal now we know that this is the way
to go but back then it was very
innovative for a revolutionary idea and
they succeeded and another thing is we
have no tools it also like tensor flow
tensor flow is not the only one by the
way but it's currently the most popular
at the moment so if you're doing deep
learning at least when you like started
recently you probably got in touch with
thanksfor flow tensor flow works in a
way that you're building a computational
graph and then you're feeding your
daytime to it once you want to do the
computations I will show you that in a
second principle gives you a lot of
api's libraries classes on different
levels of abstraction so if you want to
just say build me a network that has
three or five fully connected layers you
can do just this or you can go deep down
and create your own algorithms from
scratch
let's go to our classification task if
you haven't encountered this a manÃ­s
dataset is a very well-known machine
learning data set and before there was
deep learning computer vision algorithms
were trying to solve it because it's
very simple but understandable and quite
production kind of wise data set so
there is a data set that has handwritten
digits and the task is to recognize the
digit very straightforward don't be
fooled those are not numbers those are
digits so this is a classification task
those are labels those are categories
not numbers we are not calculating this
number based on those pixels we are
recognizing it it's a different task so
every image is fed well every image the
the every image from the training data
set is fed through our inept neural
network the networks go goes through
training and then it's ready to
recognize new examples every digit in
this particular data set there are few
of them but in this one is in grayscale
and it's
twenty eight by eight twenty eight
pixels and what you usually do as your
first attempt or what people usually did
in their first attempts at least when
there weren't deep learning they were
flattening it so in our first attempt
who flatten this file so we will get 784
607 880 phone numbers by significant so
this will be number between zero and one
thing which which will indicate how
close it is to white or plug in another
concept which is very introduced in many
tutorials very early is something that
is called what something is called its
batches cone and concept so in classical
machine learning when you're training
your algorithm you're usually using all
your training data so take all your data
you train your model with all of them
and and you're like repeat the process
most of the times with deep learning
it's rarely the case mostly because we
use deep learning for actual big data
like we have tons of examples so one
thing could be we cannot even load the
whole dataset into our memory that could
be one thing another thing is it would
slow down the process massively
so if you're thinking like Alumnus data
that I think it has 60,000 examples in
the training train data set every time
reloading everything and putting it
through our model this will slow down
stuff but that's that's only the reason
why people started to look for new
solutions and batches were actually
proven to work almost as good as the
whole thing or even better so one
iteration using yours in one batch
another iteration using another brush
and it spits down the process like the
speed is going very well and that cracy
stays the same so there are a few
algorithms like gradient descent
stochastic gradient descent
and and batches approach that will give
you the opportunity to fit in this hyper
parameter than the size of the batch so
this is this is a decision you have to
do when you're creating your algorithm
so once we have our process trained
during this process we are just in the
parameters the parameters are the
weights the things that we are
multiplying our inputs with so those are
things that has to be adjusted that's
that's the whole point of the training
process once we have them we can then
use it to classify new examples so then
you can draw a digit with into grayscale
it can go through your neural network
and boom it should work first attempt
what we'll do I mean I would do and you
can do it and cut recorder later and I
will build a neural network that has two
layers or just one layer depending on
the definition so input layer and output
layer that's it no hidden layers so we
have 784 cells in my in the input and 10
cells 10 neurons in the output layer so
basically we're taking our image
flattening it putting it through the
output once the weights are trained and
adjusted it's using a soft max algorithm
to figure out which cell activated the
best and we have our answer so I will
show you a demo not
so if you go to cut a coda and to my
profile and it's called tensorflow
layers and our layers in tensorflow
make it a little bit bigger most of the
stuff I was saying they will be here in
this tutorial but what I will show you
is is the code basically so what I will
show you is this thing so first of all
before we will build the network I will
show very quickly how the training
process looks like and I won't go into
details just for you to know what is
happening underneath and if you have a
better idea for the training process and
hence there is a better idea I just made
it very simple so not to go into details
you can change it if you want so it's
all fully like you go to this code you
change it you run it and it will be and
it will be there for you so first of all
I wrote the code it's in Python I wrote
a function that is just reading this
dataset tensorflow gives you the
capabilities of reading the MM this
dataset out of the box so you don't have
to do it yourself you can there is no
need and then I'm building the training
if I is actually building something that
is called a computational graph which
I've mentioned a little bit and
computational graph is like a recipe of
this is how I want my computation to be
performed and once you build it then you
run it but first I will build it so what
I'm doing is I'm defining a loss
function for those of you who are new to
machine learning this is something that
will be minimizing or maximizing because
machine learning process is basically
optimization process and you cannot
actually mix it up with accuracy because
we've lost function we want to minimize
it with accuracy
we hope minimizing the loss function
will increase our accuracy so those are
not the same function that they are
trying to get
go so our loss function not to go into
many details there are links in cata
coda if you're interested it's using
softmax cross-entropy algorithm so
basically is trying to minimize the
difference between what our neural
network is giving us as the output and
what are the right answers it's trying
to like see the difference and minimize
the the difference between them and my
training step is very simple this is
actually like one of the my favorite
things in tensor flow I'm just saying
you know I'm an optimizer take my loss
function and minimize it and that's it I
don't have to think how to implement my
gradient descent stochastic gradient
descent or the atom optimizer I can just
say optimize it minimize it and another
thing I want to do is I want to
calculate their currency so like every
few steps I want to see how my how my
network is performing at the end I want
to check it on the train on the test
data the data that wasn't visible there
during the training phase so this is
accuracy calculations and this is when I
run all of it so I start an interactive
session to run your calculations to run
your computation graph you have to start
the session and then I'm using number of
steps the default is 1000 so I'm running
thousands of steps and I'm getting
batches I'm getting batches using the
function that is called Nell's but also
provided by the tensor flow and then I'm
saying take this bar and take the labels
of this but and use it every hundred
steps I'm calculating the accuracy and
at the end of the iteration I'm running
the Train step so I'm saying train stop
this optimization process here run it
run it using the budget data you can see
that I filled it in with the batch data
so in every step I'm using a separate
batch and that's basically it and then
I'm using it for validation and sorry
for validation and test data set
so let's let's run it iPhone oh no I
won't run it because I didn't show my
network so let's see how do I build my
network so the one layer of network
input and output so I read Munez data
set and I said my image size is 28 label
size is 10 because we have 10 digits 10
labels in the output and I'm defining
two placeholders plate holders in
tensorflow are the places that you're
feeding your data when you're running
your computational graph so in our case
this will be the batch stuff
the blood inputs and the batch labels so
at the moment when I'm defining my graph
we don't know what are the values we're
just saying those values will be of this
shape of this type and this is how we
create it so I'm just saying there will
be a batch we don't know what size of
the batch would be it will have 748
shape and the output labels will be of
the 10 will be of the size 10 because
we're using 100 actor so for every label
there will be one in a place and the
label is this is the boring stuff the
nice stuff is that I just defined now
here my whole network my one layer in
one line and this is why I love the TF
layers package or module I'm just saying
build me a dense network which is which
is the name of a fully connected layer
and use input as an X input as input so
this is the placeholder I defined
previously and use label size so I will
have only 10 cells in my output network
and then I'm using what I've shown you
in the help file so build training and
training so I'm building the training
computational graph and then I'm
building and then running it so let's
run it dance
5
so it's downloading than this data set
and you can see in the file system that
there is a folder that it downloaded it
and then it run pretty quickly but the
accuracy is 83 percent not great it's a
horrible but not great so how can we
make it better we'll just put it up now
how can we make it better how can we
make it better more layers yes that's
the point of this talk because I was
talking too much about deep learning now
let's go for layers so we'll add some
hidden layers this will rule at 1 you
can play and add more if you want but
you'll get the gist and with the hidden
layers especially for kind of like
computer vision stuff you have to think
of the activation functions so it's not
just matrix multiplication I like to you
so it's matrix multiplication we're
taking weights multiplying by inputs and
then we're adding bias and then we're
wrapping it into activation function and
there is it's another hyper parameter
which kind of an activation function you
will use for for your layer so there is
a softmax function there is a sigmoid
function this one hyperbolic tangent and
they have the green and the blue they
have problem the problem is called
vanishing gradient descent so basically
the back propagation the way how the
network is trained is based on
derivatives and if you know a little bit
about derivatives you can you can guess
that the relative here will be print
like zero not kind of exactly zero very
close to zero and our computers are
great and magical but they still are
running up to zero small numbers right
so derivative will be 0 here and here so
the network will stop passing ups
passing their signals and yeah there is
a problem so sometimes it's better to
use
other activation functions so very
popular one recently and like I still
cannot get my mind for the maps because
it's very simple
the reloj function it gets zero for
everything that is smaller than zero and
it's a linear function for everything
that is more than zero and for me it's
basically a linear function so I don't
know how does it work exactly but it
does work so activation functions are
necessary in necessity for deep neural
networks and this is what we are going
to use will use the reloj function so we
have our input layer didn't change we
have our output layer just didn't change
but now we're putting one in the middle
so what we have to change is this one is
no longer connected to the other one
it's connected here and the output is
fed by the hidden layer what size of
hidden layer should we use nobody knows
this is another hyper parameter that you
have to think of try it out experiment
see what what works oh just you know
look at other people's work and see what
are their conclusion and of course then
you can use it so I'll show you how to
build your hidden layer now it will be
fully connected layer so the code should
be enough for you to already know that
so the file is called Trello so
everything stays the same at the
beginning I just added hidden layer size
which I've put for a very round number
1024 and
palace holders are the same and the
first layer is now hidden so I mean it's
not hidden I just called it hid and it's
still dense layer and it gets an input
as an input the placeholder and I'm
putting hidden size as a unit and there
is something more so if you compare it
to the code of the output layer we have
this activation function now right
if you don't specify activation in your
function
it will just take the linear function as
an activation function which means
basically not applying any function
that's all because it's just passing the
output of your layers so this is how is
it is one thing I'm just connecting
input with the hidden and then hidden is
connected to the output pretty
straightforward if you want to do matrix
multiplication then circle allows you to
do it you can always do that of course
it's just once you're doing it for like
tenth time maybe fifth you get bored and
you're thinking like this is all right I
already know that I got this
give me some API or some function to do
it for me because I get the concept but
I encourage you to do it if you haven't
and the rest stays exactly the same
so remember how quickly the first one
run let's see how how quick this one
will go dance
it's so quick but it's not as quick and
why is that because we have now 1,000
424 cells that stir every of the cells
tours 784 wait for the previous layer so
we had 1024 x 768
and adjustment yes so in this I think
it's pretty fine because if you think of
activation function you could think of
it as another layer of course so if it's
here it's straightforward just point her
to this function whatever it works you
probably know better
it's probably like a pointer to the
function that has just been applied to
every single cell in the layer but yeah
I cannot tell like 100% so yeah we have
now a hidden layer there and it worked
oh did we look at the kursi it is better
right 94 that's that's kind of like okay
we could could live with that but we can
do better I tell you we can do better
and what we can do better with that is
actually adding something that is called
convolutional layer and this is where
the whole fun with the whole magic
actually started working with with
computer vision tasks convolutional
layers were known I think in the 60s
it's still nothing new to the tech world
it's just somebody took it applied it
and we actually learn how to train the
convolutional neural network so
convolution as a mathematical operator
is something that gets the part of the
input doesn't have to be the image but
it's
part of the input and applies the same
let's say calculation the same the
matrix multiplication so we are applying
weight we're adding it adding bias and
putting it as as the output so somebody
took this idea and applied it to neural
networks and we gained few things from
that first of all fully connected layer
1,024 times 748 a lot of weight for
fully connected layer if you're hidden
males grow grow in size you have a lot
of parameters a lot of weights and
biases to Train and with convolutional
neural networks you're sharing weights
so this operation that is applied to the
part of the image is sharing those
weights those ways are adjusted in all
those steps but they they stay the same
and second of all those convolutional
neural networks make your network
smaller make your layer smaller but
deeper and this death that was
introduced actually shown that it can it
can recognize some interesting things so
like with image recognition and it can
recognize the borders it can recognize
the shapes and Folwell your in your
network it can recognize the eye the
nose and then the whole face so how it
usually works is you're getting your
convolutional neural network and we
stack those conversional neural layer
and stack it on top of each other then
you follow up with few fully connected
layer and this is how the basic
convolutional networks work it's not
just it there is usually something that
it's called polling and polling is an
operation that gets part again works on
part of your image and gets for example
a maximum value or an average value so
you can see like the green rectangle
it's taking 21 the next one goes and
it's 12 then it's 18 and 10 and what has
been proven as it decreases size the
size of your network so you'll save a
lot of space well it still keeps all the
all the values you're not losing
anything
are you losing not a significant amount
of information so this is an operation
that's usually stuck next to
convolutional layer so you usually have
and computer vision neural networks like
convolutional layer polling layer
convolution area polling layer etc and
then few of the fully connected layers I
will show you how to build it there is
there's one trick actually two tricks so
convolutional layer works on the 2d 3d
images I mean it can work on one deep
convolution can just go through your
vector and take part take parts of the
vectors but what it actually has proven
to work for computer vision stuff is it
takes your part of your image and now
your image matters I mean the place
where your pixel is matters previously
when we flattened it the 10th or 700
pixel doesn't matter we could we could
basically mix them up as long as we keep
the order we we don't care about where
your pixel was now we take advantage of
what what are the neighbors of of my
pixel so we actually reading the image
right not just treating us as numbers so
that's a very very nice way to think
about convolution so our our input that
has been flattened we know to have to
put it back into 2d and after our
convolutional and polling layer we'll
have to flatten it back because then
there is this hidden layer that is fully
connected and only takes one day thanks
let's go to the demo and I will show you
how to build a convolutional layer I'll
actually run it now because it takes
some time
so you can see now that it's actually
taking some time if we not only added
another layer at the convolutional light
so image size
besides hidden size all stays the same
placeholders and now I'm reshaping the
input so tensorflow reshape function is
very very useful one you'd use it all
the time basically
so first I'm using creating
convolutional neural network
convolutional layer and again I'm using
a package TF layers I'm using
convolution comes to D and I'm saying
get my input the one that Irish shapes
the to D use thirty-two filters the
depth size use the window 5x5 my
convolutional window and you don't
actually define the strides here you're
saying use the padding that the same
which means the output of this
convolution will be of the same size as
the input so 28 by 28 and then use
activation function that Israel and you
could add some bias here I didn't do it
for the sake of clarity but you could
yeah and then I'm using a polling layer
and my opinion polling layer is not a
layer it doesn't store and evaluates and
doesn't store any biases it does not
adjust any parameters it's not the layer
that is being trained it's just an
operator that goes through some input
and through some calculations but it can
be applied to anything you want and
that's probably why the authors of the
package put it as a separate layer which
kind of makes sense so I'm using the
polling I'm using the convolutional as
convulsion layer as an input I'm saying
use two by two and just write - that's a
common practice that's the first thing
you should try if it doesn't work then
you can play around but it's usually how
it's done and then I'm flooding this
polling output and again using the
tensorflow reshape function
and this is a very good calculating
example why do I have 14 by 14 by 32
we'll leave you this as an homework and
I actually encourage you to build your
own convolutional layers with all the
weights and biases and like build it
from scratch I just wanted to show you
how once you understand how does it work
once you understand how to use it how
you can take advantage of some API some
abstractions that libraries like
tensorflow offer you and then I have my
hidden layer that I get my flattened
stuff nothing changes after this so
you've noticed that it's slowed down the
process like dramatically but we have
better accuracy right 97 basically we
cannot go that further if we just use
convolutional and fully connected layer
we probably could go to like 99 and and
we'll try to do it and so you could try
to do it let's just add more layers and
it should speed it up but like in
general when we talk about accrue see
100 is the forest it can go right it's
usually takes you the same time to go
from 40% to 70% or like 80 or even 90%
and then like every percent is a win
after that right so what what I will
show you just very quickly going to
slides yes I just came back is how to
add another convolutional layer and you
can use this if want to just add another
another another another all you have to
do is just put another in another and
another put it as 2d and flatten the
last layer to feed it into the fully
connected layers
yes yes so in my examples they have
thinking have to like actually like you
probably have seen so many headlines
about those cool things that machines
can dream and generate the images so
they're using the convolutional layers
and polling layers like also for this
because it's playing with the sizes it's
sometimes mixing it up so it's it's a
machine type of an innovation and
creativity out save and and and there
are some there's some articles that say
that actually polling layer which is
very popular with computer vision tasks
is not doing a good job with generating
images so generating images it it it
leaves you with like rectangles there
because it's getting like average or max
poll and passing it as the output so as
the result sometimes you if you like
zoom it you can see all those small
rectangles so it's not that great so
there are new techniques now you could
replace the polling with but they are
the classical convolutional networks are
usually the pairs of convolution polling
convolution polling convolution polling
and at the end they're fully connected
layers so again I will I will run it
because now it will be very slow Python
to spy and we will look at the code
nothing changed nothing changed
reshaping the image and first
convolutional the same 32 filters 5x5
window padding the same activation
function realm polling layer two by two
strides and then I just use the polling
layer as the input to my second
convolution
I don't have to reshape it now because
now it's 2d actually it's a it's free D
because we have the same size and we
have depth so we have no 3d it takes
some time for people like with so
imagination like mine to go through the
fact that this input is freedom and we
have to deconvolution like how does it
work it's actually applying to
deconvolution to every piece of your
depth but I'm not here to teach you
about convolution this is quite a
complex topic I recommend to like take
some course if you really want to
understand it it's worth understanding
and there are plenty of resources out
there so just google it and so my second
convolutional layer I said I want 64
filters so now I want to I want to do 64
deaf and my kernel size size is still 5
by 5 and I have padding the same then
I'm using polling layer and then I have
7 by 7 by 64 seriously it took me like
two hours of a headache to come up why
why is it I knew it's working because I
checked it but yeah and by the way most
of the problems that you will have with
tensorflow
and basically deep learning doesn't have
to be tensorflow any deep learning
library would be that mixed up the
dimensions of your tensors so if you if
you mixed it up you can always use
something like reshape or stuff that
will squeeze or spread yours your things
and probably would work but this is the
thing that you have to think of at first
when something is going wrong so we have
two convolutional layers you get the
gist how to add another and another and
another
now you just have to think off of this
size when you're flattening it at the
end but there are also equations out
there will help you
figure this out then I'm flattening it
then I'm fitting it to the hidden layer
rest stays the same and I've been
blabbing for a few minutes now and it's
only on sec
step 600 so yeah let's let's have a cool
have a wait maybe not for a moment I
will be telling you other stuff in a
second and we'll go back and see how
does it perform so let's leave it like
this
so yeah convolutional net neural
networks pretty basic and starting to be
complex topics the topic so I found like
fully connected layers every in every
introduction Airy course training
tutorial it's pretty simple too grass
especially when it's so easy to to
implement convolutional neural networks
even though they have been there for 46
50 years they start to like maybe not
you you're smarter than me but they
started giving me a headache once I
started learning about them so the idea
I got like today we're taking advantage
of the pixels that are near to each
other
I get the depth I get I've seen it I've
seen I look at the hidden layers and
I've seen all those edges recognitions
all those noses eyes etc you can see
that and it and you can even feel it's
working it's just like no no it's not to
be complex now it's all this reaching
out there democratizing of a I actually
demands you from something from you you
have to learn you have to understand
something you have to code something you
have to know when to use this kind of
the network for those kind of problems
and don't get me wrong
learn deep learning and and go out there
and see what's out there but this is
pretty basic stuff and if you haven't no
this topic
unless I'm a great teacher that I just
taught you all about this you probably
don't get it
fully at the moment right so all right
all as I can see in the audience so this
is when it starts to be a train learning
process this is when when it's like well
I thought this fanciful think we'll just
like I'll just like say recognize the
digit and it will write it will just
figure out which network I need what do
I have to train and I thought this this
will be this easy and it is easy and
it's easier it just doesn't mean you
don't have to do learn experiment and
because you do it's it's called data
science for a reason okay the last
concept the last layer I would like to
add to our network is something that's
called dropout so I've read in many
books that reports should be only added
to fully connected layers which is not
true basically but I will explain it to
you on fully connected layer so there is
this thing called overfitting
overfitting is glorified named for your
model not just neural network to learn
examples by heart and well it's
overfitting it has a hard time to
recognize examples it hasn't seen during
the training phase so it's learned so
well on those those examples that and it
limited itself so we have a lot of
techniques to avoid it and one of them
is dropout so dropout is I think I have
this theory I have to check it and that
some people were like sitting in a bar
having a few drinks and like saying our
network is learning by heart those
examples it's too smart for those
examples let's make it dumber so that's
the concept of dropout randomly choose a
cell put the probability under to
include the suspect
your neuron in the training sighs so if
it does not pass the threshold we're
just not looking at this narrow neuron
so you could think like of applying
weights zero in this neuron it's just it
does not forget what it has been taught
so far it's just not being taken into
account in this particular iteration
next time it could be in and those ways
that we're so far trained they will
still be there and that's the key part
it's only happening in the training
phase once you have your model that is
built once you want to check how well it
performs on some examples you can then
randomly drop out your neurons you have
to have a deterministic answer so only
in the training phase only when you're
adjusting your parameters your weights
and biases then you're applying dropout
when you're testing it even when you're
like like I'm checking the Icarus II
every for every hundred steps even then
I cannot use this drop up because it
will be completely random the result
would not mean anything so that's the
concept and my question is I didn't put
it as a separate layer
although TF layers package is putting as
a separate layer is this a separate
layer so in my opinion not because again
it does not store any weights does not
have parameters to be adjusted it just
randoms randomly
apply some operations into my layer so
if we go back to cut a coda let's see
how did it go so it was now 97 and it
was 97 28 97 66 it is better but you
know every every percentage now is again
so I'll just run it because it will take
some time on dropout yeah
it's the trading batch ITRC was ninety
million percent and then the test
accuracy was late 70s but also you need
to think that we are actually training
on batches and we are evaluating only
one batch so we could be yeah we could
be lucky or could be unlucky on this
particular but so it gives you some
indication but you shouldn't really rely
on that but if I was evaluating the
whole training data set even every 100
steps in a serious real-world problem
this would like increase the time
massively but yeah it's sometimes it's
sometimes actually how you recognize it
like you're saying it's it's so good at
recognizing my examples right during the
training phase but it's so rubbish when
I put it in production so just don't put
it in production before you test it and
put it into validation phase that's like
101 of machine learning so yeah trip out
let's let's look at dropout so
everything else stays the same I will
just show you
oh just show the dropout so this is the
dropout I have my hidden layer and I
have dropout and I'm saying rate is 0.5
so we have 1/2 and 1/2 chance that our
neuron will be dropped out and I'm
saying this is a training phase or not
so should the drop is actually another
placeholder that I'm saying that I will
put the value when I'm evaluating it so
if you're if you go to help this is this
is the thing when I'm doing the training
I'm saying it shouldn't drop if I'm
going to test it I'm saying it shouldn't
drop if I'm training it is it should
drop so depending on why you're when
you're using your neural network
you need to remember that if you're
applying dropout you to tell your model
which phase it is there because
again it's not magic times tougher won't
think for you you have to think about
your algorithm so yeah
did we apply it I will come back to this
later because we have to kind of finish
yeah yeah good point good point so drop
out we'll put some kind of a probability
there right so it's not deterministic
anymore there are a few points in like
atom or stochastic gradient descent or
great innocent that sometimes takes some
probability too but yeah pretty much it
is kind of deterministic and drop out
now is is adding some randomness
specifically for this overfitting reason
that's why it's it's good once you have
a model that works to save it you have a
model you save it you don't train it
that's one of the things
another thing is reward examples we
reward datasets it will take you as
hours sometimes days to train so don't
just want to do it every time you need
to use it right so it's time for summary
talk to you a lot about deep learning I
could talk more because it's my favorite
topic and then we went through the
classification task form this dataset we
build very simple convolutional neural
network convolutional neural networks
are usually much more complex and have
other spikes like resonates like
Inception they actually have an
inception mem that is an official member
inception that comes from the film
Inception very cool and we've built one
two three four five six seven layers
depending how you define it so keep in
touch if there's
anything I could help you with don't
hesitate to contact me and I wish you a
great rest of the day and if you want to
see what Sakura see I'll put my slide in
the second 97 78 yay so it did work
robot actually works
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>