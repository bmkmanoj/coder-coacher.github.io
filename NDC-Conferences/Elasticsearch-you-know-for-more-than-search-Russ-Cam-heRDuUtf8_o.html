<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Elasticsearch: you know, for more than search - Russ Cam | Coder Coacher - Coaching Coders</title><meta content="Elasticsearch: you know, for more than search - Russ Cam - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Elasticsearch: you know, for more than search - Russ Cam</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/heRDuUtf8_o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thank you thank you all very much for
returning up coming to my talk
it's much appreciated there's some other
really good talks going on right now as
well so thank you for coming on coming
along and listen to me
rubble on for an hour or so hopefully
some things would be interesting for you
things that you don't know about
elasticsearch so yeah title of my talk
here is you know for more than search so
elasticsearch is typically known for
search the clues kind of in the name
right but it can actually do a lot more
than just search and I'm gonna spend a
bit of time just going over a few
different examples and a few different
features within elasticsearch that you
may not know about that can probably
help you in certain scenarios how many
of you are familiar with the elastic
stack or elastic search or using it
already cool okay so about half of you
awesome
I'm gonna give the the elevator pitch
for what the stack is for those that
aren't aware so the elastic stack so
elastic I should say is the company
behind the elastic stack and elastic
search and the these are a suite of
open-source tools that enable you to
explore search analyze data there's a
few components here so at the top here
we have Cabana which is like the user
interface that's the window into the
stack that's where you can analyze the
data you can build graphs you can do
ad-hoc queries control user access and
then in the middle we have elastic
search which is the storage the the
search and analytics engine that kind of
powers all of this stuff you interact
with it
via REST API you send JSON documents
into into elastic search you can then
search on those aggregate on those and
then down the bottom here we have a
couple of ingestion options so ways to
get your data into elastic into elastic
search logstash is kind of like a
centralized ETL pipelining tool so you
would go and run it in one particular
place you would send data to log stash
it can receive data from various
different sources you get for Twitter
for example elastic search sqs queues
that come to calm
kind of sources that you're probably
familiar with can then transform that
data so you can filter that data remove
fields that aren't important you can
split down fields into separate fields
build up a document enrich it with other
data as well so do things like take take
an IP address and get you the geo IP
data for the IP address of the country
code maybe the country name as well and
then finally we have beats here as well
as a beats are very very lightweight
data shippers these are the things that
are intended to run on the individual
nodes within your server farm so they do
sort of one thing and they do it well
there's a collection of them they do
different things but for example far
beat is able to look at a particular
directory on the server and tale the log
files in there and send those to
elasticsearch or send those to logs - a
couple of other things here this X pack
here which are a set of commercial
extensions that extend this open source
offering here and elastic cloud down the
bottom here is basically elasticsearch
in Cabana as a service hosted within the
cloud there so if you are not keen on
running it yourself internally or
locally you can instead run it on the
cloud instead and we can we can manage
that for you that's the very very brief
overview if you would like to know any
more
come come to the booth and come and have
a chat and I will chew your ear off so
many problems can actually be viewed as
search problems so elasticsearch yep its
origins are largely in search it's based
on top of leucine which is a search
library which has been going since 98 99
so it's you know it's a very very mature
product but there are lots of problems
that can actually be viewed from a
search angle so some of these are for
example recommendations so here's a clip
here from a well-known online music
streaming service and given the Black
Keys as an artist
show me related artists here so
recommendations here might be powered
off of those users that listen to the
Black Keys what other artists are they
also listening to
frequently what are they also listening
to often also the etiology of disease
outbreaks our understanding and finding
what the origin what the patient zero is
of a particular outbreak so in this
particular case it could be something
like Ebola so you have all the data for
all of the people that have been
infected and you've you have entity
centric data for that with documents
that represent all of the users that
have been infected in the time line of
how that has happened and locations of
where they've been you might be able to
take all of that data and analyze it in
a particular way with an elastic search
which I will show you later and
understand where the where that spread
from who was the original person there
that was infected and how did this
infection spread from there we can also
look at look at that in terms of viruses
computer viruses within networks and
other malicious activity that's it -
Troy hunt here we needed a hacker
picture so a chuck one in there security
analytics are kind of related to the
last slide there understanding how
viruses have spread throughout your
system also understanding anomalous
activity that may be happening who's
trying to log into this particular
server X number of times within the last
five minutes you know and fit all these
failed attempts are they successful
attempts what's happening over here in
on this particular server within the
infrastructure so if you're collecting
that data centrally in some in some
location you can analyze and find and
search over that data to uncover these
this nefarious activity and then
anomalous behavior so understanding how
your systems behave normally then
understanding how understanding of
finding situations where they have
behaved outside of the norms and we all
go we'll go through some of that stuff a
little bit later as well
fraud detection
very sort of similar kind of use case to
understanding the etiology of disease
outbreaks understanding this suspicious
activity so if we have a profile of what
a normal transaction looks like or a
normal set of transaction look like for
a particular user banking transactions
we can build a profile of what normal
looks like and then we can look at other
activity and determine whether it falls
within our bounds of what we believe is
normal and therefore uncover particular
Mellisa
malicious activity there as well so
first one I'm going to talk about our
recommendations and inferring
relationships from co-occurrence of
terms that sounds mystical right now but
hopefully in the next few slides it will
become more apparent of what's happening
there so this is a fairly kind of
familiar scenario on a lot of services
here so I've just taken this from IMDB
you see similar things on Netflix on
Amazon its users who like the movie PI
also like these other movies over here
so requiem for a dream the fine tune
Black Swan etc etc the co-occurrence
part here is the fact that these films
here are closely related to users that
also like this movie PI and we can
actually use that co-occurrence to to
build out a recommendation system for a
for a particular set of data
so within elasticsearch as i talked
about before we would have a collection
of documents where our documents are
JSON based documents so we have four
users here so this is a user centric
data for users and we have the likes of
the movies that each user prefers there
so a common thread here looks as though
these four users like pi and then we
have some one over here like stark city
requiem for a dream Mulholland Drive pop
fiction Shawshank Redemption Fight Club
we can actually view this though more as
a as a graph as a relationship rather
than just
looking at the documents in isolation
inferring the relationships between the
liked movies within the individual
documents and if we were then to take
that as a graph we can start to and take
each individual movie and plot that as a
vertex within the graph we can then
start to build connections between the
liked movies there so focus in on pie
here we can see requiem for a dream dark
city Mulholland Drive etc etcetera this
is also referred to as the wisdom of
crowds so using collective crowd
behavior to infer what things someone
would like and what things someone would
not like and I just want to show you a
very very quick example of that in
elastic because I think it'll become
more apparent so we're just going to
look at the movie lens data so movie
lens produces a massive collection
massive corpus of data for users that
have liked particular movies and they
rate them between 1 &amp;amp; 5 so all we've
done is we've taken that particular
piece of data any movie that's been
rated four or more we've indexed that
into a liked field within the documents
anything two or less is gone into a
disliked field so if we're looking at
the movie lens user's index here which
contains all of those documents within
the graph within the graph plug-in here
within cabaÃ±a
what we can do is we can look at the
liked field here so those that contain
all the movies that we liked and we can
select this particular one and what
we're going to do now is just go and
search for the movie pie within that
liked movies field
so what we see come out here is our
movie PI users who like the movie PI
also there seems to be a very strong
relationship here with requiem for a
dream so the strong relationship we can
denote here from the thickness of the
connection between those two vertices
and if we look over here on the right
hand side we can see a Venn diagram for
the overlap between those two between
those two vertices so we see requiem for
a dream here's our very strong
co-occurrence with PI within the user
documents Donnie Darko as well matrix
more Holland Drive Life of Pi as well
could be coincidence maybe an inception
over here I can actually do the similar
kind of thing for movies that they they
might dislike as well so if we were just
to select the disliked fields and give
it an appropriate icon probably the
lightning bolt and if we just select the
the the PI node here if we just expand
that selection out so users that like
the movie PI tend to dislike Godzilla
wild wild west Star Wars Episode one
good job Batman and Robin
yeah it's probably not the strongest in
this series right and I'm a-gettin well
who knows what could happen this year so
we can kind of see you know we can build
a recommendation engine based on this
because everything that we see here is
just exposed as an API within
elasticsearch and in fact we can just go
to settings here we can see what the
last request was so every request is
JSON based so the query DSL within
elastic search searches JSON based and
we're just going to get back a JSON
result there and from that response we
are able then to build this this graph
visualization here so we can see for
example field disliked Star Wars Episode
one and this was the weight here of the
significance between those two between
those two vertices so between pi and
Star Wars Episode one and that can then
be used then
to build that waiting of that line so
this was using something called
significant terms which I will go on to
tell you about in a second but we can
also start to explore graph as well
without using significance so what do I
mean so if we just take everything out
here at the moment and remove all of the
all of the nodes come to settings here
and we just tick this box here to
uncheck significant links and then we
come back in here again we deselect this
like Timon you're going to look look it
liked for pi and we're just going to run
the same same query again well what we
see here actually is quite a different
set of results so what we see here are
Fight Club
The Matrix American Beauty Shawshank
Redemption these kind of to me these
don't really look like the movie pie or
they don't really seem to be in the same
kind of genre of pie so what's kind of
what's kind of going on here so within
graph theory there's this concept known
as super connected entities so super
connected entities are these things that
everyone likes so if we look at bands
they're things like the Beatles they're
things like Coldplay things like
Radiohead in movies they're things like
Shawshank Redemption Forrest Gump
they're just the movies that just
everyone likes so Shawshank Redemption
Fight Club Pulp Fiction Mulholland Drive
when we're looking for interesting
recommendations for users that like the
movie pie do we really want to be
showing them these Shawshank Redemption
these fight clubs because they're not
really kind of relevant I don't think
related necessarily to pie so we kind of
want to remove these super connected and
entity relationships here and as I said
they exist in various different datasets
so if you're looking at Twitter
in tweets and your vertices your nodes
or hashtags then yellow and make America
great again are gonna be very popular
hashtags in their super connected entity
similarly for movies Shawshank
Redemption Fight Club and if we look at
phone statements over here in Australia
there's probably a high propensity for
calls to center link as well and if we
look at bank statements yeah Amazon
PayPal and probably being fleeced for
all of that coal we need to burn in
order to power our homes and if we
actually plot those super connected
entities or we we plot the frequency of
how often these things occur within an
entire set of documents or an entire set
of entities we actually see a frequency
distribution that adheres to something
called zips law and what zips law is is
that essentially the the first ranked
the the highest ranked entity there it's
it's frequency is inversely related to
its ranking so the most most frequent is
very highly represented and the next
frequent then is kind of half
represented and then a third and a
quarter and we see this kind of
distribution actually in a lot of
different scenarios a lot of different
data sets it's a very very common one so
zips law came from linguistics came from
looking at terms within corpus corpuses
of of words and corpses of documents but
it equally applies also to other data
sets that I just showed you there as
well and typically the way the graph
databases kind of account for this for
these super connected entities is that
they try and limit the the influence of
them by taking a few kind of shortcuts
so typically in a graph database what
you do is you you have all of your
individual nodes you understand the
relationship between them when you index
them into the graph database you say
this node is connected to this thing and
this is the waiting but of
nice to it and based on that what the
graph database is going to do if you
were to go and say tell me movies that
look like the movie PI its then going to
build a threshold for connections that
it determines to be interesting and then
it's going to use that threshold then to
determine which which movies it thinks
are going to be interesting for you the
problem with taking this kind of
approach though is that some things that
are interesting might be down here some
might be down here where do you kind of
draw this line where do you kind of
build that threshold I mean you do have
some control over that in with many
databases many graph databases but
essentially you're kind of building a
golden ratio to say this thing you know
I think this thing is important I think
this thing is not that important the
difference with how the graph API works
in elasticsearch is because it's a date
because it's a search engine whenever we
index documents into elasticsearch we
actually calculate a bunch of statistics
about every individual field within
every individual document those
statistics are just calculated because
they're used for determining relevancy
and search but they can also be used
here to infer significant interesting
relationships so we calculate the term
frequencies when we index documents so
I'm a user I like PI I like requiem for
a dream these are the terms we're going
to calculate how often those terms
appear within all of the user all of the
user documents then we're going to
calculate the co-occurrence of say other
movies that appear within PI and then
we're going to calculate whether that's
statistically significant within the set
of documents that we're looking at and
then we're only going to show you those
that we believe to be statistically
significant so when I tick that
significant links box or when I
unchecked it what we were actually doing
was just searching via which movies are
the most popular which ones just have
the highest counts within the
corpus of movie users and when we had it
checked we were using statistical
significance there to infer the
relationships to kind of explain that a
little bit more cuz this is still kind
of relatively abstract if we were to
look at every single users favorite
movies then requiem for a dream is going
to be very very small set within the
entire list of users favorite movies so
it's actually in this background set of
all users movies the foreground set of
requiem for a dream users that like that
movie is very very small sis you would
probably say it's not very significant
if we're just looking at the users whose
favorite movies include pie however the
representation of those that also like
requiem for a dream is pretty massive in
comparison the foreground set is huge
compared to the background set now what
we can do then within elasticsearch is
we can take the relative difference
between those percentages and we can
take the absolute difference between
those percentages we can multiply them
together and we can apply some other
smarts as well and we can come out with
a number to determine whether requiem
for a dream within users that include
the movie pi is statistically
significant that's fundamentally how how
graph works any questions on that
no cool so it's good for recommendations
there are some very very fun and
interesting things that you can do with
recommendations come and have a chat to
me at the booth afterwards that have
time to go into it but often you have
people that like various different
things they have a taste for manga
movies say and they also have a taste
for horror movies and if you're looking
at horror movies you probably don't want
to see recommendations for manga movies
so you want to understand and segregate
the different likes that or their
different tastes that someone have
within an individual set of documents
there but we can so we can also use
graph here
or exploration and to determine the root
cause analysis or do a root cause
analysis to find exactly how a
particular problem has occurred so
similar similarly again we can still
represent the data as a graph and if
we're looking at one particular field
here for a virus and there's a couple of
users involved or related to this virus
that the Hat that has occurred within
our logs and there's also a couple of
host machines here as well and a couple
of ports and we have some weighting
between these so user B seems to seems
to be strongly infected with this virus
and seems to have a close connection
with this machine over here 1000 4 but
how do we actually use grafter to
understand and explore this data I'm
going to go through an example of doing
that a couple of things that are very
very important we talked about
statistical significance to work with
recommendations when we're doing this
kind of ad hoc forensic analysis every
single connection to us is important
every single connection could be a lead
here we don't want to just only look for
those that are statistically significant
we actually want to go and explore every
single connection here we want absolute
certainty in the relationships between
between fields within the documents here
and then we want to be able to traverse
those relationships and understand the
linkages between them so I'm just going
to go and show you an example of doing
that so if we just look at a different
index here so this this particular index
has been taken from a server farm this
is a real set of data and this in this
particular set we have intrusion
detection logs we have Windows Firewall
logs we have firewall logs so we pair
firewall logs we have
some other types of logs as well we have
wind it out antivirus logs that's the
one I was looking for so if we want to
go and explore this data the first thing
we want to do is remove significant
links and we want absolute certainty
here as well we want to turn this into
forensic mode so the minimum number of
documents that are required as evidence
before producing a related term we want
to set that to one we want to absolutely
be sure so we're going to look at this
data there's a number of different
fields that we're going to be interested
in here so the first the first one that
we're going to be interested in is
device custom string one now that
doesn't mean anything to you it means
something to me what what is actually in
that field is the name of the virus that
has been extracted from the logs or has
been determined from the logs so if we
if we add that particular field and the
next one that we're interested in is the
category device type so this is where
it's actually being detected
so is it from an anti antivirus software
is it from a Windows Firewall and then
the next field that we're interested in
is the destination username
so who was the user that was actually
logged on at this point in time it's
gonna change the icon for this one and
we're also interested in what port did
they actually log onto what was the host
machine
and we actually we also want to know
what time this actually happened as well
so we can determine which which event
actually occurred verse so if we
deselect all of these and we only look
at the device custom string we're going
to have a look for the configure virus
so the configure virus has been detected
within our logs re and and Mel got woken
up late at night last night and they're
very very unhappy with us
because we put the system in and they
want to know exactly what has happened
and why they were alerted so we need to
understand why how is configure actually
got into our system so the first thing
we were going to do is look for
Conficker now once we have configure we
just want to see where this was detected
within the system so if we select the
category device type so we're going to
look was it on the firewalls was it on
the intrusion intrusion detection
software let's have a look so configure
was seen on these four different pieces
of software probably a collection of
software because we like security and in
in droves so there seems to be a very
high connection here with network-based
intrusion detection antivirus you would
expect that so the next thing we want to
do is see well which users are actually
involved in this config of virus which
which ones are closely related here so
we just take a step back we are only
interested in the config a virus we're
not interested in any of the others so
if we just select this one there's a
couple of users here so Ann Warren seems
to have a seems to be heavily infected
with configure here based on this line
someone's logging in is administrator as
well which is a little bit worrying
maybe we ought to have a word with them
maybe I ought to have a go and have
look and see which machines they've been
logging into so we can see there's a
couple of users here involved here so
let's just have a look at administrator
for a second
what kind of ports are they typically
typically looking at so if we just
select administrator and expand that one
okay so they're normally logging in on
port 1 3 8 480 port 53 ok and what host
what hosts are typically involved in
this what computers are typically
involved here that are also connected to
configure so if we just select the host
names here ok there's a few there's a
few machines involved but if we look at
the ones that are connected here to the
users we've got this wks 6 3 2 4 5 that
seems to be heavily connected with n
Warren we also have this machine over
here this Ferrar on not sure who named
these machines but sort of Ferrar on
there which is connected with the
administrator and these are all
connected here with conficker right now
so we've got two we've got 2 users we've
got 2 machines so where where is this
actually come from where we're actually
spread from so what we want to do is we
want to have a look at this machine and
let's have a look at the end dates which
dates are associated with this we've got
some numbers there
unfortunately I index this data with
numbers rather than dates so excuse me
but bear with me we'll all get through
this so we've got four dates involved
here with flora on and let's do the same
thing for this machine down here which
is also which is connected to n Wharram
ok one individual date here as well so
if we were to take these take these
numbers here which are actually the
epoch milliseconds so there is some
sense to that number if we were to take
one of these numbers here which I know
in advance is the one I'm looking for it
happens to be this one here we can
obviously do the same for all of these
numbers but if we look at this number
here we can see that this was in fact
the first date that happened it was the
administrator logging on to this machine
for our on on the 8th of August 2017 at
1250 and this is how the configure virus
ended up spreading through the network
then Ann Warren at some point in time
has so the administrator then has also
logged on to over port 80 on to wks at
some point and n warren then is somehow
poor and warren is somehow badly got
infected there with Conficker as we can
see from the way of this line that's
just a demo of the kind of forensic
analysis you can do as well it's very
interesting to run this kind of run this
over data such as the Panama papers and
understand the connections between the
entities involved within the Panama
papers even though some of those
relationships have been denied the
evidence is very very clear within the
documents there and I did encourage you
to go and have a look at that as well
but yeah we found exactly where and how
that has happened so just a very quick
graph recap for you so there's a couple
of different examples there of ways in
which you can use the graph API to go
and explore your data so to power
recommendations there and also to go and
perform ad hoc forensic analysis by
turning it into forensic mode disabling
significant links and exploring the
various connections between the
different fields within that log data
the nice thing about the graph API in
comparison to other graph databases is
that this is real time and and scalable
it's real time in the sense that as new
users come and like new movies the
recommendations are going to reflect
that we don't need to do anything
special to make
those two have those recommendations be
updated the graph API is simply going to
show you the most significant ones at
that point in time it uses your existing
data models so if you're using
elasticsearch already you're already
indexing JSON documents into
elasticsearch there is nothing required
in order to explore that with the graph
API and use relevance based reverse
traversal to do so so the next thing I
wanted to talk about was reactive
monitoring so taking action when
particular events happen in the system
so you're taking all of your event logs
and you're putting them all into
elasticsearch you're building some nice
visuals you've got some reports coming
out but you've also I bought some
particular things that are happening in
this system that look quite weird you
want to set up some kind of automation
to alert you or notify you when those
things happen so for a long time you've
been able to do reactive monitoring
within a within elasticsearch with a
particular piece of piece of technology
that I'm going to talk about next but
the way that you typically perform
search queries is you have a query
represented as JSON you send it into the
elastic search cluster you get a set of
matching JSON documents out of it pretty
straightforward query in document sight
what you've been able to do for a long
long time in elastic search is do
something called percolation or
percolate queries also referred to as a
reversed search so it's a bit of a mind
Bend but because queries are essentially
JSON ub can store that query as a
document and we can send in a document
that we are going to index and then tell
me which queries that i have indexed
match that document i send in so it's
kind of the reverse of what we saw here
we're going to send in the document
we're going to index in elasticsearch
we're going to ask it to percolate that
and then give us back any queries that
are saved that match for that particular
document
and there are a number of use cases that
this is really really useful for it's
doing things such as very simple
alerting so price monitoring so your
indexing prior documents that have
prices in them if the price goes up
after over a certain level you may have
a saved query for that when you run a
percolation you will get that saved
query back and you can then take some
action on that same thing for news
alerts stock price alerts as well some
all system are there interesting
scenarios that you can do in terms of
advertisements as well so if you run
something like in marketplace where
people can advertise things for sale I
might be interested in a particular item
I can say yes I'm looking for this thing
and I want you to save my search and
tell me when that thing comes up or what
comes you know it becomes available in
the in the area that I'm looking in and
what we can do then is when someone is
actually typing in an advert yeah I've
got a I've got this thing for sale we
can give them some kind of feedback to
say actually the thing that you're
selling there are five people looking
for that or we can also then make some
recommendations to them we can say there
are five people looking but if you also
include this other thing there will be
15 people looking for it instead so we
can give that kind of feedback at the
point at which they are entering some
data and we can also use it for
classification as well so automatic
tagging language detection so given a
set of queries that have a corpus of the
words for a particular language in there
when a document comes in we can use that
that query that contains those corpus of
those lanco words from that language we
can use that to classify that document
and say that looks like polish or that
looks like Chinese that looks like
English and we can do a similar thing
with geo tagging there as well
percolation is great but a collation is
really interesting useful but the
problem is sometimes context is key if
you're looking at documents in isolation
definitely has its uses however you
often
want to understand and notice trends
within the data so if you have a
collection of documents coming through
you want to understand is the trend is
it going up is it going down what is
happening there and this is where
alerting comes in
so with alerting which is built into
elasticsearch you can take some action
when some input meets some condition and
then we can have this alert happen every
trigger time and what we can do then is
we can take action to say email the
sysadmin x' to say that server
you know server 510 is playing up again
and we need to go and look at that or we
can hit them up on slack or we can send
a couple of messages to their favorite
pager duty platform or we can just write
it to a log or send it to a web hook all
of these things that we can do with
alerting and if the demo gods both the
old and the new are playing playing well
with me then I'd like to show you an
example of using that alerting so what
I've got over here is a simple asp net
core application so i've jumped on the
microservices bandwagon I'm thinking
yeah I'm gonna I'm gonna build a
platform so yeah I've heard monoliths
are bad so I'm jumping straight in four
separate in this stuff out and what I've
got here is just a couple of controllers
I know two controllers in one project
one controller here that just receives a
tweet some tweet information from
somewhere so an ID and the text of the
tweet is going to send that off to Azure
cognitive services it's going to send
off to their sentiment analysis API is
going to determine whether or what the
sentiment of that tweet is whether it's
good whether it's bad whether it's
neutral whether it's slightly good or
slightly bad and then what's what it's
also then going to do is index that
sentiment score back into elasticsearch
also then have another controller action
here that is going to automatically
retweet from my account so rather
dangerous for a demo but it's going to
retweet from my account any tweets that
have positive sentiment really positive
sentiment so this thing is running
already I'm running it already so you're
thinking where does the elasticsearch
side come in here so if we jump into
management here within cabana and we go
to watcher I have this watch here
running already which is a I'll show you
what it looks like bear with me this is
getting prettier this is kind of the
lair of a watch so trigger every single
minute it's going to take this as input
so it's going to perform a search
request it's going to look at the
twitter index it's going to run a query
over that twitter index it's only
looking but anything that's happening
between now and the last day so anything
in the last day and anything that
doesn't already have a sentiment score
attached to it so any tweet that doesn't
have a sentiment attached to it if we
happen to get some results from that so
that's our condition if we happen to get
some results then I want you to take
some action within elasticsearch and the
action here that I'm gonna take is call
a web hook so I'm just going to call my
local service that's running my local
rock micro service the sentiment
endpoint and I'm just going to pass it
the payload and the payload there is
just the ID of the document the tweet ID
of the tweet and just the text from that
tweet as well so if I could have a
member of the the audience anyone who
would like to say something really
really nice about my talk and include
NDC Sydney and elasticsearch in it
hopefully we'll get we'll get an auto
retweet there so I know my colleague is
a feverishly typing away there thank you
thank you my time fingers crossed this
is going to work
so if we just keep keep an eye on what's
happening here in the meantime because
this is only running on a 1 minute
interval sorry yes so that's that's the
doing the sentiment analysis if we just
jump back a second I also have another
watch that runs every minute that is
again just going to look in the tweet
index it's going to look for anything
that's strongly positive there which
hasn't been tweeted by me and it's then
just going to send that to my retweet
micro-service my retweet endpoint so if
we just want to visualize first and just
see what's happening so I've done a
little bit of prep work here or no
results found in the last 15 minutes
that's not a good sign but if we just
have a look see what's happened since
this morning and zoom in a little bit
here we go we're starting to get there
ok so we've got some neutral slightly
positive slightly negative what's been
happening here oh there's a strongly
positive one at 12:22 awesome it's
worked the question is so have i
retweeted it let's have a look are they
are we gonna are we gonna get any dice
so let's have a look here so that's my
time thank you Martine let's jump back
here
it's fantastic if I have i retweeted
doesn't look like it
we could be waiting a while then it does
work I promise you if we just have a
look down my timeline here there's a few
that I woke up nice and early this
morning there's a few here richard banks
thank you very much
i was just looking at NDC sydney tax
from twitter earlier and richard just
happened to tweet it just the right time
as i hooked all of this up and i ended
up retweeting him so thank you richard
for taking part in my talk here we go
that's a nice little gem for him this is
working so i was just this is a very
very contrived example of course i'm not
suggesting that you all go and dump your
message bus architectures you know going
give the guys hell in on those boos i'm
not suggesting any of that i'm not i'm
not suggesting you do anything of the
sort all i want to show you is that the
data that you have within elasticsearch
that you may be searching that you may
be aggregating on you can also do things
automatically with that data within
elasticsearch you don't need to build
something separate to do this you can do
it from all inside of elasticsearch the
final thing i wanted to talk about
it's sad that didn't work the final
thing i wanted to talk about is anomaly
detection so we've looked at
recommendations ad hoc forensic analysis
alerting or reactive monitoring the next
thing we want to do is anomaly detection
so to kind of follow the the story arc
here so we looked at percolation single
documents great but we need some context
around them alerting brilliant we've got
some context but the problem with alerts
is that sometimes building rules for
particular rules based alerts are
insufficient it
we're looking at this particular data
here I mean do we set the threshold here
do we set this threshold here do we say
yeah this between these times in a day
if it hits this threshold if it goes
over that or under that then send some
notification or similar sort of thing
here it's very very hard to actually do
that and sysadmin sind dev ops guys are
probably going to give us a hard time
because they're they're suffering from
alert fatigue there so how do we go
about analyzing this data how do we go
about understanding this data better
with anomaly detection the first thing
that we want to do is we want to
understand what does normal look like at
any given scenario and therefore what
does abnormal look like and to give you
an example if we were trying to learn
what normal looks like for a given
scenario what we might try to do is
collect some data on it so if I was
looking at my emails that I get every
day I could say yeah on a Monday I got
10 emails on a Tuesday got 12 etc etc
it's probably more than that right now
because I'm talking but what we can
start to do is we can start to build up
a distribution graph for how many emails
I typically get over a day this is
probably what mine looks like you know
my distribution for the emails I get
every day is probably looks something
like this I get obviously I get less on
the weekend but in the week of mine
probably looks like this my mum's emails
probably looks like this she probably
gets a few on a Monday and that's
probably about it my Gran's probably
gets one every now and again
so her distribution of emails probably
looks like this here what we can do with
this and what we've actually built here
is is a probability distribution
function so we can say what is the
likelihood that on a given day on a
given time based on the data that we
have
seen before and we have built a
probability distribution for what is the
likelihood that this thing is normal we
can build the bangs we can built you can
we can use the distribution to
understand the bounds and if that thing
falls outside the bounds of our
distribution we know that it's anomalous
or we would beam it to be anomalous one
of the problems here though is how do we
kind of determine which kind of model
best fits our distribution for a set of
data now obviously this is really
contrived here these distributions here
look very very normal they look very
regular but some distributions for some
data may look very strange and very
jagged how do we determine what what
function fits that distribution well
that's a hard problem and that's one of
the things that machine learning can do
for you you don't need to understand you
don't need to build that distribution
function yourself machine learning is
able to understand and learn from the
data and build that distribution
function itself so the types of
anomalies that machine learning is able
to handle is univariate time series data
so if we're just looking at for example
the number of requests to our website we
can then look at particular times where
the number of requests seems lower than
normal and where the number of seems
higher than normal we might also then
want to split out that univariate time
series data data and we might have
multivariate so we may be looking at the
overall unusual traffic on the website
there may not be any anomalies in that
but what if we were to split that out by
another field if we were to partition it
by another field we might actually see
that there are some anomalies within
those individual within those individual
countries in this case and the anomalies
may happen in very very different places
other types of anomalies that we can do
here we can also use it for population
analysis so we can build up a
essentially a prototype of what normal
looks like and then we can use what
normal looks like to understand whether
something falls outside the bounds of
what we determined to be normal for that
given entity so for example if we were
looking at website logs we were looking
at the number of requests from a
particular IP address you know let's say
a user comes to a website you know they
go and hit this page then that page and
typical user hits a three four pages in
a couple of minutes we can build a
profile of what a typical normal user
looks like and we can then use that to
determine which users look like BOTS
which users look abnormal they go to a
thousand I mean a thousand would
probably be very obvious in the graphs
but let's say they go and hit ten sites
here they wait five minutes they go and
hit another ten pages here and they seem
to do this at particular frequencies
over the day we would be able to
understand and build a profile and then
use that profile to determine that that
behavior is anomalous a couple of other
ways that we can do this we can use
anomaly detection here as well is to
categorize in our logs unusual or rare
events so there's a couple of examples
here we have a little spike on a
relatively flat time series set of data
that's probably an anomaly and here we
have regular spikes a regular interval
there seems to be one missing here
that's probably an anomaly as well so I
just want to give you some very quick
demos of the machine learning
functionality
so if we just jump back into cabana we
go to machine learning I'm just gonna
take this one off delete that one
thank you what we can do here is we can
set up a job so I already have my data
within an index and elasticsearch I now
have some ideas of how I might want to
analyze that data so I'm gonna come and
create a new job I'm only gonna look at
single metric here and I'm going to look
at my server data so this is just a
collection of data that has come from a
web server it's just the total number of
bytes coming out of the web server at
any particular time so this is actually
looking in a number of indices and what
we want to have a look at here it's just
the sum of total bytes over time so
we're looking at the sum of the total
it's badly named but yeah sum of total
bytes over time now what we want to do
in order to understand what's normal at
any given point in time it would be
unfeasible to do that for every single
event that comes in every single let's
say they were coming in every
millisecond it would be unfeasible to
determine and build a distribution
function for every single millisecond
where buckets bang comes in is what
we're essentially saying is we want to
take that time series data and we want
to chunk it down into buckets of time
and we want to then build that
distribution function or build that
predictive model over buckets of that
particular that particular interval so
we want to use all the data we're
looking at 30 minutes here so we get a
quick overview of what our data looks
like so yeah I mean it looks fairly
regular to us this looks a little bit
strange this looks a bit a little bit
strange but it's you can see it's very
very difficult maybe not so much this
one but if we were just eyeballing this
hello looks alright you know kinda looks
normal but if we're gonna carry on and
set up a job for this
so Total Request let's just give it a
name and let's create that job now what
we can see here with in machine learning
is we can see that predictive model
being built over that data as it's
running through the data it's building
up that model of so I chose the sum here
so it's going to be looking at anything
that goes over a threshold a high
threshold for the Sun and anything that
goes below a low threshold for the Sun
but what we can see here at the
beginning is this very faint blue is the
predictive model so as it's gone through
this particular set of data it doesn't
really understand the shape of the data
as it's going going through here but at
this point here it's kind of starting to
understand well this looks like data
that has a regular kind of frequency a
regular periodicity to it and it started
to refine the model at this particular
point if we go and have a look at the
results because we could see there that
there are a few yellow bands and a few
red bands there that had flagged
particular pieces of data if we have a
look in the single metric viewer and
have a look over the entire set of data
so we can see that predictive model here
we have a couple of points here that
have been highlighted as being anomalous
and they've been given a score here if
we just zoom into these ones here so
anything in red is much more anomalous
than anything in yellow or orange if we
just zoom into these ones here this
particular range here we can see we've
got some anomalies here that fall
outside of our model and if we have a
look down here down the bottom they've
been given a severity level between 0
and 100 and
we can also drill down into that
information as well so this this
severity and the coloring is a category
that is actionable by you right it's
something that you can take some action
on a max severity of anything over 90 is
something I really need to to be
concerned about but if we drill down
into the actual details of that anomaly
we can see what the typical value is at
that point in time what the actual value
was observed here and it's 1.2 times
lower than the model expected at that
point in time and we can drill into
these other ones here as well but we
have an overview here and we can have a
look to say we're only interested in
those that are marked at critical if we
just jump back a second I have a set of
data here that looks at a multi various
multivariate analyses so we have a
collection of logs here that are coming
from different servers they all conform
to a unified log structure so we're
taking that data from all those servers
with indexing it into elastic search
using time-based indices and we want to
now have a look and understand if there
are any anomalies on individual services
or individual servers there and what
they're what the influencers between an
anomaly on one server might be compared
to those other servers so this is
something very very useful for micro
services where you have a micro surface
talking to maybe one to three other
micro services if there's a problem in
one of those micro services you probably
are going to see it in other areas as
well you're going to see some kind of
knock-on effect for those issues if we
just go and have a look in the anomaly
Explorer at these applications or
they're all running on different these
services are all running on different
servers what we get here is an overall
anomaly timeline here as a swimlane but
we have it broken down here by service
so we've got seven different services
here and what we can see is that anomaly
detection is flagged a few of these as
being very very anomalous if we just
have a look at this particular
one here we can see here that service up
five these these particular points
looked highly anomalous we can see that
the host here for this particular
application was server three and it was
1.1 times higher than what we would
normally see at that point in time we
can have a look at the overall time line
here as well
which is going to give us a view across
all of the services so there was some
anomalous behavior on app 6 app 5 the
rest of them look fairly regular at that
point in time fairly normal and we can
drill down then into that as well and
understand that further we can actually
go even further as well we can take the
results of that machine learning data
and feed it into alerting as well so the
results of anomalies detected by machine
learning can then be used for alerting
so in summary hopefully that's given you
some ideas for some of the different
things that elasticsearch can do it does
search very very well does aggregations
very very well but there's also some
other really really cool tech there that
can help you with some other scenarios
thank you very much
come and grab me at the booth if you
want to know more or you want to have a
look at some other demos happy to go
through those with you oh one more thing
we also have an elastic meetup coming up
the end of this month on meetup so on
meetup calm so if you're interested in
understanding what's coming up in the
next major version please come along to
that as well thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>