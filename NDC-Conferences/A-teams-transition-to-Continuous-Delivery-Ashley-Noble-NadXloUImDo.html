<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A teams transition to Continuous Delivery - Ashley Noble | Coder Coacher - Coaching Coders</title><meta content="A teams transition to Continuous Delivery - Ashley Noble - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A teams transition to Continuous Delivery - Ashley Noble</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NadXloUImDo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning my name is Ashley Noble
today we'll be talking to you about our
groups transition to continuous delivery
as a small group in a large organization
I'll talk to you about the impetus for
the change the choices we made and the
challenges we faced I work for Honeywell
Honeywell is a fortune 100 company has a
hundred and thirty thousand employees
twenty thousand engineers and eleven
thousand software engineers so as a
company goes it's on the large size
Honeywell makes a lot of exceptional
products but it also suffers from the
normal big company challenges such as
inertia to change first a definition
continuous delivery is the ability to
get changes into production or into the
hands of users safely quickly in a
sustainable way for us continuous
delivery meant that we wanted to be able
to commit our code and get it into
production to get value from it to get
feedback as quickly as possible our team
for the last three years have been
working on an IOT platform the IOT
platform like a lot of big companies is
our backbone for getting some of our
products connected to the cloud so that
we can add new and better services for
our customers and provide better value
to them we've been working on this
platform for roughly three years we're
going to start our journey two years ago
which was a year after we started and
when we realized that we needed to make
a step change so two years ago we had
six developers one very large solution a
semi-automated build with hundreds of
automated unit tests we had a very
manual deployment which had which was
PowerShell scripts executing PowerShell
scripts we would at best deploy to a QA
environment after every after every
sprint sometimes we'd miss a sprint or
two because we didn't have anything to
actually deploy it would take us hours
to deploy it would be manual steps we
would follow multiple wikis we'd have
tens of steps and every one of those
steps had to be followed in order and
exactly we would often find that we
would find ourselves debugging a problem
that we could trace back to the fact
that we hadn't quite ticked one of the
boxes or made one of the
plummet steps that we were meant to we
cut a corner we don't need to do that
this time even if we got ups everything
absolutely right in our deployment it
would take us days to test and validate
we're sitting there manually making sure
that the new features worked but also
that we didn't have introduced any
regressions into the product from a
build infrastructure perspective we're
pretty light on we're using PowerShell
make which is pronounced sake using
Visual Studio online using its get and
build system this is 2 or 3 years ago
it's not the Visual Studio online that
we know and love today
it was clunky is probably the best way
of describing it and we're also using
PowerShell to do our deployments what we
were deploying was a combination of as
Europe has instances we decided early on
we didn't want to be standing up our own
VMs we wanted I wanted to go to the
cloud and try and be cloud native so we
used as your Active Directory we used
quite a few cloud services which are
worker and web roles basically hosted
iis we used a lot of service bus
namespaces topics and queues as you'd
imagine from an IOT platform and we also
used Bob and Bob and table storage sort
of rounded out our has infrastructure
that we were using so that's where we
were starting from where we are now is
we have roughly 15 developers spread
across four four main time zones we have
multiple solutions which are totally
automated builds with thousands of
automated unit tests we now have a fully
automated deployment where we do on
average 22 deployments a day of which
three of those are production
environments so this isn't exactly
Google or Yahoo or AWS territory in
terms of thousands of deploys today but
it's a good step along the way we do so
many deployments now that we don't even
notice there's no pain to do a
deployment we also have hundreds of
acceptance tests that execute every time
we commit some into our release pipeline
from our infrastructure build build
tools we now have moved to the atlast
until stuck really nice integrated set
of tools use JIRA for our issue tracking
bid bucket for git
bamboo for our build system we use
octopus deploy for our deployments
launch Darkly for feature toggling
artifactory for our package management
and sake and PowerShell still from our
services what we're actually deploying
we've extended some what we now as well
as the previous services we also now do
native as your websites use cosmos DB
document DB use a lot of stream
analytics from azure we use event hubs
and we're replacing our have replaced
our service bus names in queues many of
them with event hubs for the scale and
we also use as your IOT hub so that's
where we started a couple of years ago
where we are now what was the reason for
us to make the change what sort of we
knew we weren't working as well as we
could be we knew there was a better way
the catalyst for us was a book called
the Phoenix project the Phoenix project
is a very non-traditional IT book it's a
piece of fiction it's written as a novel
it's a story that you read and the story
aims to tell aims to illustrate that
lean practices don't just apply to
manufacturing they can also apply to IT
and does a really nice job of that from
this we recognized we could sort of
start to name some of the issues that we
had we needed to reduce our work in
progress
we would often find that we had so many
things going at the same time that we
hadn't finished them by the end of a
sprint or even the following sprint in
order to do that we needed to do smaller
work packets so we could turn them
around faster so we could deploy and get
value to our customers quickly and we
also needed to automate everything sort
of we know that we need to automate
stuff but how do we make it a priority
so it's really pointed out to us that we
had to make it priority the Phoenix
project also had a really nice reference
to the next reference that helped us
which was the book continuous delivery
this is an exceptional book it's really
the Bible of continuous delivery it
describes all of the practices and the
processes that you should or need to
follow in order to do continuous
delivery safely and sustainably
from this we learn a few things like we
learned what we needed to do but then we
needed to figure out how did we
transition from where we were which is
very manual and quite clunky to getting
to continuous delivery and we identified
three main points that we could change
first one of those was to move to
continuous integration and trunk based
development the second one of those was
to build out a set of automated
acceptance tests not just unit tests
that ran as part of our build but
acceptance tests that ran as part of our
deployments we also needed to build out
a more formal deployment pipeline rather
than just to deploying something to dev
something to QA and something to
production and they're not necessarily
always the same things so the first
thing we tackled was continuous
continuous integration and continuous
integration for us was around also doing
trunk based development continuous
integration we thought we knew what it
was we had a CI server we've been doing
that for years but we didn't really
understand until we started digging into
it about it's the fact that you're meant
to integrate your code daily or even
more frequently as possible
everybody work gets their branches and
merges their answers down to a common
central repository master every day the
idea of this is that if you have to if
you introduce a bug introduce an error
that your tests are gonna catch then you
can catch them very quickly you've got a
very small amount of code a half a day
or a day's worth of work you can catch
them that's a very small amount of code
to trawl through to figure out where the
bug is and it's all really fresh in your
mind
if you wait for an entire Sprint's worth
of work and then you try and find a bug
it's a lot more code to work through and
can take a lot longer to find continuous
or integration also reduces the risk and
the stress of a merge of merging all of
your code typically at the end of a
sprint we would find that we were
merging all of our code we'd have
several people trying to merge this
entire Sprint's worth of work at the end
of a sprint that's creates a lot of risk
usually a lot of conflicts and it would
be a fair amount of pain the first
person that merged in
sitting pretty really their code was the
only new code the second person that
went to merge their code they if there
was any errors if there was any
conflicts they had to go through two
peoples sprints worth of code if they
need to find errors they had to trawl
through theirs and someone else's code
the third Purdy third person there
pretty much stuffed they had to go
through three lots of people's code and
so on whereas if we merge daily then
those daily merges there really any
conflicts they're going to be small
they're going to be easily resolved and
you can just talk to the person it's
just done it we can communicate we can
find them early we can find out and fix
issues very early on and adjust our plan
so that we don't end up stepping on each
other's toes we also find that every
process we took is a little while but
every process is leaner as an example
our code review process after a little
while we realize that code reviews were
actually going a lot smoother than they
used to be they taking a lot less time
and we worked out that that's because we
were used to doing our code reviews at
the end of a sprint we'd have an a
sprint a week and a half or so as worth
of work that someone needed to review if
I was doing the review I'd have I'd get
presented with a very large pull request
often I'd have to get that branch out
encoded and actually dig through it
because it's really hard to do a lot of
files in a pull request and understand
the design I might have been involved in
the whiteboarding of the design but then
I needed to sit down and understand how
it was actually implemented when it was
from that implementation and it needed
to check that it there was no bugs the
algorithms were correct there wasn't
gonna be any performance issues and it
made our normal standards and criterias
and that can take that would take
roughly a day a week and a half or two
weeks worth of work could take me a day
to review it's a sort of well published
fact now that the larger a code review
is the lower the quality of the review
so if we can change that cycle and do
our commits daily all of a sudden our
reviews are also happening daily all of
the review the design issues that we
might find then
found at the start of the sprint and as
the sprint goes along the code reviews
actually become shorter so we found that
maybe it takes me 20 minutes 30 minutes
to do a review near the start of the
sprint towards the back end of the
sprint maybe it's only taking 10 or 15
minutes to do each day's worth of a
review that's because they're mainly
fleshing out code adding tests and error
handling that sort of thing it actually
becomes a lot quicker so then when we
get to the end of the sprint
there's no stress it's no no two or
three day cycle where you iterating over
big reviews and big change sets so
there's a lot less stress and the last
sort of reviewers really just almost
check the box it's much easier so we've
dramatically reduced the amount of time
that it takes us to do a review and
reduced the stress sure
yeah so the question is how does merging
daily affect our pull requests and our
stories so as part of continuous
integration the idea is that you do
multiple pull requests per story so you
will have the same juror or whichever
tracking system you're doing the same
request will be committed to multiple
times and you'll have multiple pull
requests for that so you're not closing
out an issue until we have made the
final sort of commit so you end up with
multiple multiple pull requests and
they're much shorter the other area that
this addresses is that right at the end
of the Sprint someone's been working for
a week and a half and you look at the
code you say this design this wasn't the
design that I intended you've made a
mistake sorry you can't commit this like
we're going to have you're gonna have to
start again that's a that's really
demotivating and you've lost an entire
persons with work for the Sprint and
maybe it'll be quicker to do again but
really you want to find that stuff out
earlier so if you can watch if they're
committing code you're doing reviews
earlier you can catch those sort of
design question and design issues much
earlier and make sure that you actually
capture the value at the end of the
Sprint so that's how our code views just
an example of the processes become a lot
leaner and this happens across the board
for many of the processes so this is
what continuous integration is and some
of the things we learned we were
starting from get flow get flow is
pretty much the opposite of continuous
integration and trunk based development
it's a very formal and strict process
not going to try and describe everything
that goes on here but there is a process
for every step creating when you start
the feature how you merge that down into
a dev branch how you create releases
those releases then get merged down into
something that gets deployed to
production we were doing when we were
doing manual steps that was sort of okay
because we could do this manually but we
wanted to move to something which we
must much faster we want to do
continuous integration and we didn't
really see at the time how we could use
continuous delivery
with get flow now after doing just
delivery for a while could squint our
eyes a bit and say yeah I can understand
it's may be possible but I still think
this was going to create a lot of
headaches for us so what we moved to was
continued integration continuous
integration ish okay it's a journey and
we're not there yet it's both a process
and a cultural change for the team and
it's taking us a while to work through
this for to do continuous integration
and trunk based development which is
means merging always merging into master
taking a feature branch and merging into
master it's a it really pushes the done
is done is done philosophy for the team
it raises the quality bar because the
next time you merge you're merging to
master and that is going to end up in
production so if the if you haven't
written enough tests if your automated
tests don't catch a bug it's going to
end up in production and that really
that raise the bar and across across the
team we don't have any metrics around
this but we noticed as a team that we
were injecting far fewer defects
downstream we didn't have a test we no
longer needed a test team so we didn't
we weren't catching them we were
capturing those defects but we were
injecting far fewer the developers and
the code reviewers were doing a more
comprehensive job we pushed quality to
the left so it's a change but changing
to continuous integration we did trunk
based development doing daily commits is
a bit of a challenge it's a process and
a cultural change if if it was a
continuum between feature branch
development where you merge at the end
of the sprint and continuous integration
where you're merging months or twice a
day that seems at the moment fall
somewhere in the middle there and
developers fall somewhere along that
line there's Kent Beck put a tweet out
recently which I think really clarified
this for me you can't effectively
address cultural problems with process
just as you can't affect address process
problems with culture we were trying to
change the process and the culture and
we really only thought about the process
we change the process
we didn't do enough to help the teams
work around to improve the culture as an
example of a culture that needs to the
culture that needs to change is that
everybody has to be happy to work in
faster cycles you have to be able to
commit your code every day we need to be
able to do code reviews every day you
can't just say I'm going to work for a
week and a half and then I'll do reviews
for someone else I can't just leave the
code review so then you can't beaver
away and ignore code reviews because
you're blocking someone else so we need
to change the culture of the team the
team also had to learn see the team also
had to learn new skills so they had to
learn how to create work packets that
could be merged in a day that made sense
you don't want half half tested code
committed every day you need to work on
a logical piece that made sense that
could be committed so that when so when
you are merging with everyone else it
made logical sense that's a skill that
people need to learn and various people
have that skill and very various people
needed to learn that for example some
people needed to learn how to build
effective thin slices so to build a thin
slice through their code the people
needed to learn how to build individual
modules and then connect all the
services together so their skills and
and techniques that the developers had
to learn the developers also had to
another thing that got in the way was
developers had to make sure that their
build and test processes were running
fast so they're faster you can get a
feedback that you haven't broken
anything the faster you can commit to
master and get into production safely so
if your build and test process takes ten
minutes then you could perhaps merge to
master every hour or two because there's
not a lot of long amount of time to wait
for feedback however if you if it takes
a day to get a boot build and for all of
your tests to run then there's no sense
merging
every hour or two because you have to
wait a day to find out the results of
that so you push your times out to
multiple days before that sort of you
get the reward and it makes it worth
committing the code we fall somewhere
between those two at the moment it takes
us an hour an hour and a half for some
of our components to give us results and
feedback so for us that works out to be
about a day's worth of effort is about
right it takes about a day's worth of
effort an hour and hour and a half later
we can get some verification that what
we've added is successful or not so
we've simplified our development flow we
have reduced our merge conflicts and
risk now we have to automate our build
and our deployments the first thing the
traditional software problem is we need
to break up our monolith that's what we
had a one very large solution that came
from a year of beavering away adding
features showing that we could build
this product and it was going to add
value then we had to we knew that we had
to make a step change and this is the
start of the journey so we had to split
a pattern split apart our monolith
microservices are all the rage now
they're all the rage a couple of years
ago when we looked at what we needed to
do in order to make to get to
microservices it was going to take us a
few months our code if we squinted we
could see like it was reasonably well
structured we could see how we could
extract it out into a full micro service
based architecture but it was going to
take us a few months to get there and
from a product owner perspective it was
going to be a hard sell we're gonna
promise we're going to go faster we're
just going to take two or three months
do nothing add no extra value in that
time and then and then we'll be able to
deliver much faster that's a really hard
sell so we sort of went somewhere in the
middle we broke our code up into six or
seven groups components that
encapsulated several services that were
logically deployed together we did just
enough and to some extent we hid the
cost of doing this from our product
owners where the we would say every
every sprint every developer was
spending two or three days doing a
deployment and validation and testing
so of all the manual steps that adds up
to twelve or eighteen days per sprint of
manual work we figured over two sprints
we could probably use that time to fully
automate it our build and that's why
we've sort of we cut it down to two
Sprint's that we said okay this is how
much we can split apart our code so we
made some practical decisions there
around how much we wanted to break up
our code so we broke it up into six or
seven components mixture of library and
application code and the solution where
we had project references we used you
get packages to share the code that we
needed to reference downstream we also
had to do a bit of tweaking to make sure
we didn't have any site circular
dependencies but we used new gate in
order to break the dependency cycle of
course now we needed a way to store and
retrieve and you get packages at the
time the company didn't have a general
solution for storing you get packages
there was a few people had a few build
machines under their desk which hosted a
local instance of and you get anyone
who's used that style knows that it
doesn't scale very well we found when we
found that out and also it doesn't take
very long to find out that it is
incredibly slow after a few hundred
packages so initially we started with my
get my get with private repositories we
couldn't use new get dog because we
really didn't want to be publishing our
private company code add into into the
intern you get so we used my gate with
some private repositories Mike it's
really nice really recommended as a way
as a as a service which has some great
great UI really nice package management
features and good user management
through our transition we moved to the
adolescent tool stack as part of that
one of the infrastructure teams stood up
artifactory manufactory not as nice not
as good not as many features but it made
sense for our organization with the
number of different types of containers
number of different packages and package
management systems that could support
and also was
within the corporate network which meant
that I didn't have to field so many
questions around our code is in the
cloud and that was being out of bring it
back and not have so many audits the
security audits around where our code
was that was a nice okay so now where we
know we're building packages we're
putting when we've got need to get our
packages into a repository what on earth
are we going to how do we version them
I've never been a fan of the build
machine having write access to a git
repository and I don't like the approach
of tagging every time we do a build
putting a tag on the get on a git commit
so if we used a tool called get version
it was designed originally for git flow
to support the git flow mechanism to
help version that but kit version also
can be applied to trunk based and
feature based development for us kid
version calculates a build number so
rather than taking it from the from a
tag it calculates it from an either a
stage where you've you've merged in some
code or from a tag further down and so
if you check the code out a year later
it's exactly the same version as it was
at the time so it's nicely calculated
which means we didn't need to give write
access to our repositories for a real
machine so we've split our build up
we've made multiple components we've
figured out how we're getting them into
new gate and we're versioning them the
next step was we needed to be able to
deploy them and for each of those
components we need to deploy them
we chose octopus deploy for us this
became has actually became an easy
choice we had a cloud architect who
joined the company about the time that
we're making this choice and a lot of
experience with various types of
deployments we said what should we use
we use chef or ansible puppet he's
somewhat surprised to some came back and
said using new using net using Azure
just use octopus and at the time we were
trying to figure out how we're going to
break and break up all of our builds
break up our deployment subsystems we're
happy to take some ones straight-up
answer on that so we didn't we didn't
need to go through and
analyze all the rest of them octopus was
great solved a lot of problems for us
it's it's really it's great value for
money has very nice support every time
we've asked a question they give us very
quick and useful feedback it has some
very nice dashboarding features and best
of all and allowed us to build in single
click deployment what we did for each of
the components we turned them into a
deployment in octopus and we had the
process steps within octopus execute
pieces of powershell that we've been
executing manually before and the steps
that we had been executing manually we
figured out how to automate those and we
also ran those steps inside octopus so
then all of a sudden this one changed
where we could turn all of those steps
into an automated process I did two
things they took away hours of time
every time we need to do a deployment of
watching it and sitting there and
running through steps and also reduced
the number of manual mistakes that we
were making the mistakes we made the
amount of time that we spent chasing
down phantom issues because of a
deployment mistake all of those went
away this saved us an enormous amount of
time all we have to do is say deploy
this release to this environment and
we're off
octopus has another number of other
really nice features and it has it
implements something akin to the
configuration repository which the
continuous delivery book talks about a
lot and recommends and says that it's an
essential part of continuous delivery a
configuration repository is something
where every time you do a deployment it
pulls the configuration from the
repository and applies that to the
deployment for that environment that
you're deploying to you should have a
number of features such as very good
security it should have auditing and it
should have versioning so you can go
back at any point in time and deploy the
same thing octopus solves this using its
libraries of variables variable sets and
environment scoped scoping on variables
octopus also allowed us to do immutable
releases so an immutable release is
where a set of binary x' and a set of
configuration come together that's a
release that we deploy
if you change the configuration that's a
new release if that change in
configuration breaks everything you just
simply go back to the previously
released deploy that and it's exactly
what it was before
so the auditing the sorry the versioning
of configuration is a really important
step to be allow you to allow you to
roll forwards and roll backwards
we used octopus as well to help build
our release notes every time we did a
build we'd include we'd create a release
in octopus and that release would
include the git commit hash so that we
could trace back to exactly what where
that deployment came from we also
included the git commit version git
commit comment and some metadata such as
the JIRA ID or the tracking ID so that
we could go and look at what was the
reason why we were making those changes
so those sort of release notes helped us
to solve some of the issues that we had
with our release management team honey
was a big company has a lot of processes
and release management is high up on its
list of important processes sometime
afterward a little while after we
started using octopus and been doing our
deployments a release management team
came along and said hello we're your
release management team I then we're
here to make your releases better ok
great thank you how are you going to
help us as I said what we're going to do
is we're going to set up all the all of
these processes for you we're going to
have monthly change Control Board
meetings where we go through and approve
every change you're going to deploy in
the next month which means you need to
know what you're going to deploy in the
next month you want to know your release
dates we also want to know when you're
going to have downtime and any and we
also want handovers for all of your
scripts of your deployment scripts so
that we can run them for you we scratch
that chin a bit and tried to explain
that we were doing we want to try
something new we're trying something
called continuous delivery we changed to
deploy to production multiple times a
day so they came back and said that's
great we're going to have monthly change
control board meetings we're going to
need to know all of your scripts in
advance you can see how that went
so after
ten meetings we sort of managed to turn
the ship a little bit when we managed to
influence and come to some agreements
where the release management teams
requirements could be met the intent of
their requirements and not necessarily
the processes that they had and ours
where we wanted to be able to deploy as
as frequently as daily or even more
often to production we did this by
defining two sets of changes and this is
some typical ITIL terminology we were
using standard and non-standard changes
but we defined non-standard as anything
where we had to stop a stop or tape some
downtime to in order to deploy so if we
were upgrading or changing our
infrastructure then that would require
us to stop some services to do a
deployment or if we were doing a
database change which was one way where
there was we're going to need to do a
database migration even if we were doing
it on the fly we'd still we still needed
to do that would still be considered a
non-standard change now we only do two
or three of those a year so that wasn't
such a big deal
everything else was considered to
standard and we got it to the point
where anything that was in a sprint plan
on our scrum board that wasn't one of
those non-standard changes was
considered automatically approved so we
managed to sort of weave our way through
the release management process to meet
their goals and ours some of the some of
the things that the release management
team really liked about octopus is they
really liked the auditing capability
they liked being able to see who
deployed and why they'd like to be able
to see what was in every release so the
tooling really helped us there they
really loved but they really loved the
single click deployment so when we
showed them this you can often we're on
a telecon you could sort of hear their
jaws hitting the table as they like what
it's a single click we don't have to run
through scripts we don't have to do all
these manual steps pretty much like we
were doing before they were often thrown
those things to do deployment so they
were really pleased about having all of
this behind octopus deploy one thing
that they were concerned about was how
was our test team going to keep up
there's a test I'm going to keep up with
the deployments every day how are they
going to be able to say yes this is a
good build all right
for that was that we had created
automated acceptance tests we used
executable specifications using a tool
called speck flow very similar Fitness
and other similar tools where we would
write pros we would describe what we
were doing in text and then there was
code behind which took those sort of
this domain-specific language and turned
it into steps which were executed
against a deployment so this wasn't
executed against a build time it was
executed against something that we
deployed we'd used but spec flow before
we were familiar with it and it's a
really nice way of describing
particularly for a platform which is
basically just api's and messages the
intent of each feature so if we wanted
to know what a feature did we could just
have a look at the acceptance test that
the developer was writing executable
specifications do have a learning curve
there is sort of an impedance mismatch
between the nice text that you write and
the code that's running behind the
scenes
it did take it does take constant
vigilance to make sure that we don't end
up with sort of spaghetti code or
duplicated code behind the scenes the
challenge we faced once we started
writing acceptance tests was around our
execution times how long was it taking
to get feedback around our execution
around our acceptance test we started
off we were doing very we had a few
tests proof of concept we added a few
more tests yeah this is great but it's
running a little slow let's do some
local improvements get it back down to a
reasonable amount of time we said this
is working really well let's add tests
for everything in our platform so
together as a team we got we had a whole
sprint where all we did was write tests
now we didn't try to go for 100%
coverage we went we had automated unit
tests as well well we did go for was a
hundred percent happy path the coverage
so every single API and message needed
to have at least one test through it to
make sure that everything was integrated
and we let the automated unit tests
handle the rest of them we also did some
of the more important sad case scenarios
so every happy poet empath and some
sad paths of course the predictable
happened we had it took an enormous
amount of time we all of a sudden we had
hours and hours of acceptance tests
actually nearly half a day worth of
acceptance tests it took to run the
tests which is all of a sudden slowed
down our feedback loop and sometimes it
took if there was failures it would take
even longer so our feedback loop has
just stretched out from where it was it
was when we started about 15 minutes and
now we're at about an hour but when it
was half a day it's just simply too long
to be able to get effective feedback so
we bit the bullet the the choice and we
said okay let's run everything in
parallel spec flow has is a really nice
tool that allows you to run things in
parallel allows you to specify some
parameters around it how many things
should execute it once it collects the
feedback in a really nice way so it's
bethe flow really supports running in
parallel and so we managed to drag our
time down back to a reasonable level and
actually came down to about 20 minutes
unfortunately we were running so fast
that we started running into transient
problems so we needed to slow our tests
down and that's why we're back up at
around an hour to execute our tests we
had to slow them down we were running
into transient failures where a test
would fail in one spot you'd retry it
would go away you'd run another set of
tests and it had fail in a completely
different spot in unpredictable places
it's really crept up on us we were as
our code and our tests became more
complex we started off we would see a
transient failure maybe one out of ten
times we were running a test and
developers just okay let's retry see if
this was transient after a while it was
almost one in two times it would fail
every second time we tried to run a test
we ran our tests it would fail with the
transient failure developers just got
used to just clicking retry and we
realized that we were actually even
though the tests might be executing in a
certain amount of time it was more than
twice or three times that because they
were just hitting retry when we
recognized that problem like we really
need to look into this now because we're
trying to add new features but this was
starting to slow us down
so we looked into it and unfortunately
turned out to be one of our pair's
components as your Active Directory
which is causing us the problems here
there's two ways that have caused us
problems we'd written really nice
isolated acceptance tests every test
when it started up it created objects in
Azure Active Directory and set up the
permissions in order to be able to use
those objects and then our test would
run and use those permissions to execute
all of the tests now what would happen
was all of those tests are running at
once and so we effectively load testing
the right path to Azure Active Directory
it's really optimized for reading and we
were putting so much load against the
writing and the updating of permissions
that they started throttling us and
particularly as we got more complex the
throttling got more severe and even with
exponential retried back offs we would
still lose enough of the messages that
at some of our tests would fail so to
address that we had to take away some of
our nice isolation we had to change it
so that all of our tests at the start of
the test run before it did anything else
he created a common set of permissions
and then we would share out those
permissions to each of the tests as we
stood them up and that felt a little bit
dirty but we really didn't have a see a
way around it at the time the other
problem that we faced was around a
directive directories eventual
consistency it's a very it's a very
largely scaleable service it can scale
to millions of objects with the
permissions and references in between
them however in order to do that it has
eventual consistency what does is when
you make an update it makes that it then
replicates that to all of the other
services in the same data center and
other data centers unfortunately it's a
it's really not a trivial amount of time
after talking to Microsoft about the
issues we were seeing they came back
with some stats along the lines of it
takes roughly two minutes for most
changes to replicate around the globe
and around even into the next into the
same data center and four minutes for
almost all 99.999% of changes to be
replicated that's a non-trivial amount
of time and it meant that for
to be confident that our acceptance
tests were working correctly we had to
add a four-minute delay at the start of
all of our tests and anytime within our
test that we did something which changed
permissions and there were quite a
number of tests which change permissions
one to three times throughout the tests
and so we would end up with a twelve
minute delay where we just sat there
twiddling our thumbs waiting for that
for minutes because there was no way of
knowing if the permissions had been
replicated everywhere or not we're still
working on solutions for this but this
is the reason why we had to slow down
our acceptance tests and it's sort of
one of those pragmatic decisions about
well do we keep creating retry or do we
just slow it down and be more confident
so the the release management team they
were happy with the where we were going
and they were particularly happy when we
said that we've got coverage across all
of our functionality the happy path
across all of our functionality they
still are interested in what our QA team
how's our QA team was going to let the
release management know when it was okay
to to deploy to production for us that
was a relatively straightforward answer
we didn't have a QA team we no longer
needed one when we were doing our manual
steps and doing our manual testing we
actually started onboarding some test
teams a test team and we had them
starting to learn how to execute the
tests but that was right at the start of
when we started this journey as we were
making this journey it turned out that
we actually didn't need them to be
running tests anymore every test that
they were running was easily done with
the acceptance tests so what we did was
they became some of our DevOps people
maybe came good at doing automation and
helping us actually improve our
processes rather than sitting there and
writing tests anymore running the tests
for us so this is good for us we've been
happy that we don't and haven't needed a
QA team up until now even still so we've
got our tests in place with we've got
our builds we've got our deployments now
we need to set up a pipeline that was
the next step along our journey the
continuous delivery book has a lot to
say about pipelines it talks about what
they are the need for them and gives
some samples
we pretty much adapted ours from one of
their more simple samples and a pipeline
walks along the lines of each column is
a stage and you progress your release
across from left to right going from one
stage to the other and if it breaks at
any stage then that release is dead it's
not allowed to be promoted to further on
to the next stage typically if we're
going to anything is going to break it's
going to break in the acceptance tests
and that means we need to change code or
change configuration in order to get a
new release that we then get through the
through the pipeline the description of
our pipeline is we used we were we used
to have a lot of deployment failures
particularly when we were trying to
guess at the rest interfaces that we
should be using to deploy our code so we
used a deployment canary to a very quick
deployment to find out if there's going
to be any deployment failures that we
probably don't need this anymore because
we very rarely have a deployment failure
however we execute this if that passes
if it deploys that's all it does it then
moves on to the next stage which is the
acceptance tests we execute our
acceptance tests if there's any failures
in there then it can't go any further to
the pipeline we don't run our acceptance
tests in every environment that we go to
because it takes a long time it takes
roughly an hour to run and also if
there's failures we haven't successfully
managed to make it so that there's no
sort of pieces left over we don't manage
to successfully clean up everything if
there's failures so we sort of leave
that murkiness in that acceptance test
environments and occasionally you go
through and clean those up assuming that
our acceptance tests pass we then move
on to the QA stage which is where we do
our load testing security testing and
where we have our sort of traditional QA
we don't do automated load testing at
this point that's something that we're
getting to on our plans and to be able
to execute a set of load tests a
traditional QA well we actually don't do
anything there all of our tests are
running over an acceptance tests and QA
is where other applications that are
using our platform play it sort of the
Wild West where they're learning to use
our platform they try and stress it they
try
break it and so that's a really nice
place for us to learn and build in some
resiliency and robustness we find out
about the things that we need to take
into account so that they don't end up
there in production from QA we then
promote to production and this is the
one manual step in our process we have
one manual control and it's a bit that
says that we're continuous delivery not
continuous deployment continuous
deployment is where you have a
completely automated pipeline from code
to production no manual steps in between
continuous delivery is when you have at
least one step in the one manual control
in the way our manual control is we
actually wait 24 hours so we we have a
release come in to QA 24 hours we
manually promote that to production and
what we look for in that period is we
look for any alerts or integration
issues that have happened on any of our
QA environments if there's anything that
happens that we can that would prevent
us from deploying to production then we
don't deploy this doesn't happen very
often and so now we're in the process of
changing our changing these steps to be
an automated deployment so what we're
going to do is we're setting it up so
that we're going to wait for hours and
automatically look at those environments
and see if there was any alerts or
incidents and if there wasn't then we'll
deploy to production so we're just about
there - getting it to being a continuous
deployment rather than continuous
delivery so how do we know that the code
that we're running in our acceptance
tests will work correctly on our
productions and for us we have
production like environments everywhere
the infrastructure for every environment
from testing all the way through to
production is exactly the same the only
difference is scale but the we use the
power services and everything is exactly
the same all the way through we do this
so that there's no sort of works in this
environment and it doesn't work in this
other environment where they're slightly
different one of the rules that we have
though because we're deploying we deploy
overtop we deploy to the same in
over and over again as you or does a
really nice job when you've got a lot of
instances running of making sure that
everything stays up by stopping one
upgrading it started starting it
stopping the next one is you it does a
great job for us there we very rarely
see downtime but what we do have is
long-lived environments long lived
infrastructure so something that we need
to take care of there is that we don't
end up where someone manually modifies
something through the portal and all of
a sudden we're out of sync so we have
some very strong rules around what we
can and can't do and from the portal
it's pretty much a rule is you cannot
make any changes and we restrict we take
it as far as for our operational teams
they have no ability to make any changes
to production they're the only teams
that can see production apart from a
couple of the DevOps members and they
have no ability to make changes apart
from occasionally restarting a server if
that's what the run books call for our
developers have no access to QA or
production in order to make any changes
the idea behind that is that the if the
developers are not going to have access
to production so they may as well
practice on QA not have access there so
that they can learn what instrumentation
what tools they need to build in to be
able to support the platform so
developers have no access to QA or
production only the dev ops team if a
change needs to happen to infrastructure
it has to happen through the deployment
scripts has to happen with through the
deployment scripts or through octopus
configuration variables that's the only
way we allow a change and it has to be
through a deployment so far this is
working out pretty well for us so this
is our release pipeline just work
through our release pipeline what about
developers see they have a much smaller
pipeline and the developer pipeline is
along the lines of they make some
changes they create a branch make some
changes push those changes to the
central repository our build server sees
those changes creates the builds the
artifacts creates the deployment
artifacts and pushes them at octopus and
creates a release in octopus at that
point it stops because we're not really
sure we're never sure that the developer
actually wants to
once to deploy their code at that point
they may be just asking for an informal
code review so that we stop at that
point and we allow the developer to then
go and click on that release click the
button and deploy to one of our
environments when they deploy to that
environment it executes all of the
acceptance tests so not only the
acceptance test run as part of the
release pipeline they run across all of
whenever the developers deploying for
their testing so we put all that
together our flow for a development
process looks something like the
following a developer creates a feature
branch as I just described
they push their changes that create
kicks off the build machine which kicks
off octopus to create their releases and
octopus they can then the developer then
tells octopus to deploy that to deploy
that to Azure that executes the
acceptance tests and the developer looks
at the results of those acceptance tests
and our deployment fails if the
acceptance tests fail but they look at
the results make sure that they're it's
all the features that they expected that
their tests ran and then say yep that's
great so they go and create a pull
request the code reviewer looks make
sure the build worked they make sure
that the acceptance tests succeeded they
review the code and say look this is a
great feature
we'll take I think this is going to be a
great ad this is approved so prove it
and then the developer merges to the
master pipeline that becomes that hits
our release pipeline where we follow the
same build and deployment but this time
when the release is created in octopus
that automatically kicks off the
pipeline so that it goes to our
deployment canary if it passes there it
goes to the QA pipeline it goes to
acceptance s and in the QA stages so
it's totally automated from that point
on all the way through to QA so this is
a description of the process that we
follow and most of the tooling that we
used there's one tool that we found out
about last year at this conference which
is launch Darkly launch Darkly is
feature toggling as a service when doing
continuous integration you want to get
the
of integrating frequently but you don't
necessarily want the code that you're
integrating that you may only have a
partial solution for running in
production feature toggling is a
solution to that where you can make sure
that it's turned off in production and
to you're ready for it
launch darkly as I said it's a
cloud-based service it's exceptional has
a really nice UI has great auditing
which is important for our release
management team it's very secure and has
clients in a lot of different languages
and it has additional support for things
like dynamic toggling when you change a
toggle it'll strain that down to the
clients that are listening and they'll
make those changes at the time
it also has user based and a be star
toggling as well so you can say this
user should be allowed to have it so in
production or in QA you can turn on a
feature for a particular user the a/b
testing you can say I'd like to I'd like
sort of 2% of my calls to go to this
toggle and 2% with the toggle off it's a
nice way of getting back the feedback
from your customers so we had a basic
form of toggling before we came across
launch likely but it was deployment
based we'd make a change in our
configuration in octopus and then deploy
it so is either on or off it was very
slow and we couldn't we couldn't
effectively manage it when with launch
Darkly we could do the we had the
ability to create now two different
types of toggles we could create dev
toggles and operational toggles we could
create the developer toggles were around
a feature that we were implementing and
when we got to the end of the feature
turn it on in production we then strip
out that toggle and take it away from
the codebase operational toggles allowed
us to turn on and off entire features
that were that maybe the customer
doesn't need those features something
that we decided in advance that a
customer may or may not need in the
environment that we're deploying to
launch Darkly also has a really nice set
of api's so what we do in the api's is
when we do a deployment we go to the
launch darkly and so
have you seen this environment before if
it hasn't seen that environment before
we create it we also have a list of all
of the feature toggles
in the that are in the codebase at that
time and so it goes and updates launch
duckling with those feature toggles so
that we don't need to go the developer
never needs to go to launch darkly and
type in a feature toggle you know all
the manual entry issues that could occur
there so we use the api's in order to in
order to set up and automatically
configure our environments in launch
darkly so when we put all of our tools
together over time we've gone from very
like a very small tool chain into a much
larger tool chain and a much more mature
set of processes when we look at all of
these tools as they join together our CI
CD pipeline actually when you look at a
high enough level most people see ICD
pipelines look pretty much the same we
could replace bitbucket with get bamboo
with jenkins octopus deploy with chef or
puppet
there's your with AWS or Google they all
look pretty much the same the only thing
that I'm pretty confident at the moment
you can't replace is something is
launched darkly like it provides such a
nice set of features that I don't know
of another replacement for it so this is
how our tool chain hangs together we've
got our we're starting we've got our
continuous delivery we're not as fast as
we like we know there's some things that
we want to improve but what's next for
us well fortunately someone wrote
another book in fact the authors of the
of to of the of the other two books got
together and wrote something called the
DevOps handbook this is a really nice
reference that includes a lot of the
information from the continuous delivery
book in the first half but then it
extends it and moves towards operations
so it talks about how you should have
how you should manage your product in
the entire lifecycle continuous delivery
was around how do you get it deploying
this is about how do you manage it and
there's some things in here that we
thought about that this makes a lot of
sense we'd really like to do this
we really want to be able to add some
more metrics and dashboard into our
application so we can track it an IOT
platform we have a lot of stuff going on
a lot of load going through it how do we
see how it's performing at each point so
we really want to we want to improve our
metrics so that they're a little bit
more fine-grained we ought to add more
instrumentation so we can do some
predictive log analytics so the DevOps
handbook talks about how you use
predictive log analytics to figure out
when you may have a problem coming you
can do some analytics on it and that
sounds it sounds really interesting from
a computer science you sort of
perspective but it'll also be useful
from our operations if we can get in
front of problems that are starting to
occur along with improving all of our
existing processes where we can we
desperately want to speed up our
acceptance test to reduce that feedback
loop we're going to improve our build
speeds our builds running at somewhere
like 20 minutes as well we'd like to get
both of those things down to around 10
minutes and we're thinking about how
ways that we can do that for our
development environments we also want to
stand up dynamic development
environments so at the moment we have a
very fixed set of development
environments and that works well from a
making the developer think about
migration at every step so if the first
thing if they know they're making a
migrating change the first thing they
have to do before they can deploy
anything
is address that migration that keeps it
in the front of their mind but it also
means that we have sort of these
long-lived environments and very
occasionally we find that someone's made
a change which doesn't work in when you
stand up when we stand up a completely
new environment that those changes we
met it may not work so we want to be
able to stand up sort of dynamic
environments and test them from scratch
there's a lot more to do for us so for
us building a CI CD pipeline was a
journey we're only part of the way along
that journey we made incremental steps
towards that destination and we don't
think we're there yet
we know we've got a long way to go we
know that there's a lot of things that
we want to improve and I'm not sure that
will ever be finished this journey
so that's it that's all I've got time
for today well I had prepared we've got
a little bit of time if people want to
if we want to take some questions
otherwise I'm happy to I'll take some
questions outside as well and also a
morale around for the rest of the
conference if anyone wants to chat to me
because I love talking about this sort
of thing okay so the question was how do
we make sure that we clean up our
feature toggles is that manual or
automated and it is a manual process but
actually launched actually helps us with
that as well
because what we can do is we go through
launch darkly every now and again and
look and it tells us when the last time
a toggle was used so we can see well
this this toggle wasn't even queried the
code didn't query this toggle in the
last month so that makes it easy to
clean up and we go okay now who will
write some code we do want to adjust our
code so that it automatically deletes
them but we haven't got there yet it's
on our list of things to do so at the
moment that's a little bit of a manual
step but luckily helps us to identify
them
okay so the question was do we carve out
time to make the improvements here I
talked about the fact where at one point
we sort of heed some of those
improvements it's sort of a combination
we use it as trying to some developers
will sort of think of an idea or we'll
talk about an idea and some developers
sort of do it on the side but things
which are a bit bigger like making
turning out acceptance tests into
parallel tests that we try and carve out
time for so it really depends on it's
sort of a mix of priority for us
priority for the product owners if we
can really show value to the product
owners then we really include it if it's
something it just makes our lives a
little bit easier but product owners may
be a little less excited about it then
we may do it as sort of a background or
a sort of a sort of a hidden sort of
thing where we do it outside ours or as
a background task okay
so the question is how do the developers
they don't have access to production or
QA how do they know what's going on how
do they know what whether it's working
the features are working the
environments working so we use a couple
of other tools we use app dynamics and
we use spunk app dynamics provides the
application monitoring force and Splunk
provides the logging and our developers
have access to those tools so they can
they need to add instrumentation so they
can effectively track their features
through the code and app dynamics is
sort of an application monitoring tool
that tells us when we're throwing an
unusual number of exceptions or any
exceptions or all of a sudden seeing a
spate of unauthorized calls that sort of
thing
so app dynamics helps us there
yep okay so that's a different question
so if our application throws and starts
throwing errors at 1 a.m. who wakes up
we actually we're a big company we have
a team that's allocated who is watching
for those sorts of things when I
mentioned that we have a team that's
running across four time zones we
actually have teams in Argentina
Australia Argentina Australia Beijing
and India and North America and so
there's usually someone awake as well so
there's usually someone who's available
to answer that okay we're out of time
but if there's any more questions I'll
just be outside I hope this has been
interesting thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>