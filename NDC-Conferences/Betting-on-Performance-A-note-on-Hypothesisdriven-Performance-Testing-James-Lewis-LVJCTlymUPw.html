<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Betting on Performance: A note on Hypothesis-driven Performance Testing - James Lewis | Coder Coacher - Coaching Coders</title><meta content="Betting on Performance: A note on Hypothesis-driven Performance Testing - James Lewis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Betting on Performance: A note on Hypothesis-driven Performance Testing - James Lewis</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LVJCTlymUPw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I feel like Chancellor Palpatine
addressing the Senate in Star Wars it's
not intimidating at all ah those are
them first of the volume so welcome off
coming thank you very much I'm really
talking about how we can use performance
testing as a lightweight probe to test
hypotheses and to further our goal of
using evolutionary architecture to build
systems so I'm not going to be talking
about so testing stress testing all the
myriad of types of performance testing
I'm going to be concentrating
specifically on this idea of lightweight
performance testing and how we can
enable us to make different decisions
about building software so that's what
I'll be talking about today
first of all I should introduce myself
so my name is James I work for thought
works that's kind of a confession it's
like the first step on the path to
recovery I've been at full legs for
about 12 years now it's bollocks if you
haven't come across us we're a bespoke
software development consultancy we're
sort of all over the world is about
4,000 people these days maybe more four
and a half something like 40 offices
although known in scandia we're in
Germany or in Spain in Europe or in
Turkey we're in London we're in
Manchester and I thought these are these
these publications up because one of the
things we're really passionate in
thought works about is sharing with the
wider community so whether that's like
me here
sharing my experiences talking
conferences or whether that's writing
books or writing articles we're pretty
passionate about doing that or releasing
open source software so you can see
there's a ton of open source software
there we were behind the first
continuous integration server cruise
control our naming didn't get any better
we've now got one called go at the same
time as Google releases golang brilliant
nice one impossible to Google for either
which is quite entertaining we've also
got a bunch of other tools and things
that we sell and many of those sort of
practices that we I associate with
disciplined software delivery I learned
about one I'd run for weeks and I
I started using when I joined bulwarks
and actually a lot of these practices
have been written about my foot workers
so continuous delivery that's probably
one of the most important books about
how we deliver software was written by
three sorry by two-foot Works employees
both acts now more recently I could call
out building micro services so Sam
Newman a good friend of mine he wrote
building micro services well it's all
work so that's that you might also know
me because of this and that may if
you've read this but I had the dubious
honour of defining what micro services
are with Martin Fowler a few years ago
this is pretty much the only mention of
micro services in this talk so I should
probably get the opportunity to say
Dakka Dakka Dakka Micro fabrics service
fabrics etc out the way at this point so
that's me I'm James one of the things we
call out in this article is this idea of
evolutionary design so one of the cool
ideas about how you go about building
systems of systems composed of micro
citizens is that we don't decide
everything upfront right we don't decide
everything when we know the least and
that's what I'm going to be talking
about during this talk so a lot of ask a
question I guess which is how do we
normally make decisions about design
architecture and performance in my
experience over the years I've been in
the industry I mean what I think the
phrase is if you've you've been in the
industry over 20 years then you're a
gray beard officially I actually have a
gray beard as well so that kind of makes
me a double braid is and in those years
what I've sort of noticed over time is
generally people tend to make decisions
based on experience which is a good
thing right the more experience you have
maybe the better decisions you make
depending on the types of experience
often will people often people will make
decisions based on on gut feel so how
this feels kind of right this is kind of
how we should go and oftentimes you make
them upfront so we decide everything at
the start of a project it's going to
look like this even with micro service
projects we're going to build all these
micro services or
go go go my developers build all our
microservices and they need to look like
this I don't think I don't think that's
the way we should be going I think we've
now got a toolkit that allow us to build
to make better decisions about how we go
about building micro-services or any
other form of software come to that and
I'm not the only one because I was sort
of doing some research for this tool
this is sort of you know this is I'm
opening up the inside of my head to you
at the moment right you're going to get
basically what's going on in my head at
the moment about building software and I
was looking online to see if we could I
could find any other examples of what
I'm sort of talking about and funnily
enough I found a book online that
exactly describes the story I wanted to
tell you today I tend to find that the
most I guess powerful way of
communicating ideas is through narrative
through stories whether that's myths and
legends or whether that's you have a
fiction other types of fiction and I
found this book which describes
accurately the story I want to tell you
so this is what it's called it's called
the endlessly bifurcating trousers of
reality and it's about evolutionary
architecture evolutionary design the
endlessly bifurcating trousers of
reality for reasons that will become
clear so this is a very particular type
of book it's a choose your own adventure
book and if people have come across
choose your own adventure books but the
sort of the style of a choose your own
adventure book is the story is set up at
the start and then you roll your
character it's a bit like Dungeons &amp;amp;
Dragons or role-playing but for the
smaller children you roll your character
and your character goes off and follows
an adventure through the book and this
this is what this book is so it's a
choose your own adventure book and the
adventure that our hero is going to go
on is the story of building replacing a
pretty pretty substantial content
publishing platform for a scientific
publisher in based in London so this
platform was built about 17 years ago
based on the service-oriented
architecture
C++ and you was making about three
billion dollars a year annually which is
not bad going I think for in
architecture over 15 years you know
successful by anyone's standard but
unfortunately a couple years ago these
are these content publishing people came
to the realization that they needed to
replace this platform for pretty
standard reasons ones that I guess will
it will might ring what might be
familiar to you so things like we can't
get work done quickly enough it's too
difficult to change the people who built
it have left the company we need to
improve our availability and performance
so let's take a look at the story so we
start with the history the little good
product owners of the publishing house
that long lives in all in fear of their
publishing system in all four they have
made a tremendous amount of gold but in
fear of the time taken to change them
their slowness and their fragility a
messenger was sent to fetch help from a
distant land famed for its mighty
wizards you have taken up the challenge
and so the way is choose your own
adventures work is you get this
introduction I've set that sets the
scene we're going to replace this
publishing platform and the rest of the
adventure is about how we did that or
some of the decisions we made so page
one you must save the product owners by
rebuilding their website you start off
the projects in the course of
discussions you discover that you that
your goals are threefold you must
improve availability improve performance
and reduce the cost of delay an
Enterprise Architect approaches and
addresses you you may use is obviously
you've got spells and stuff that you
carry in your backpack you may summon a
walking skeleton in which case you turn
to page four or you may cast analysis
paralysis at the enterprise architects
hoping he'll go away for six months and
then you can give in a functional
aspects of specification some nice
architect's diagrams and then the
project will probably be canceled so
it's unpack this a bit improve
availability so publishing
interesting especially paid for
publishing science publishing
essentially what these sites are are
there to do is to get people in academic
institutions or government or people at
home to click on links and download
articles right that is how they make
their money they charge MIT they'll
charge University here huge sums based
on the number of people who click links
and click the articles to download them
so availability is a massive deal right
if the site isn't there you can't click
on it all right so you're losing money
and actually such a big deal that they
report this quarterly in their results
to the markets the CEO stands up and
says this is how many impressions
essentially we got and so let's stop
price is directly correlated with site
availability and this has been getting a
lot worse over time
the second one improved performance
right so they had some very I guess
ambitious goals with performance mainly
because of the rise of the China market
and the fact that you the fastest way to
get data to Australia is to stick it on
it on it on a cruise ship and send it
and bring it back basically by post so
they had terrible pages of page load
times in China and it had terrible page
load times of the order of 16 seconds in
Australia all right so that's another
goal of the project let's the group
availability group performance and then
this third thing reduce the cost of
delay so the cost of delay is the amount
of money you could have made if a
feature was in production earlier
alright so if I could have been making
$10,000 every day with this feature
being live and it's a month late then
you've effectively lost a month times
through our ten thousand dollars right
so improve the cost of delay reduce the
cost of delay so we're going to make a
choice at this point because I'm an
old-school software architect so I'm
going to initially choose to cast
analysis paralysis at the Enterprise
Architect is approached let's see where
happens you cast analysis paralysis at
the enterprise architects foolish young
adventurer so the architects we follow
the evolutionary school of architecture
here and we shall have none of the ways
the lawful evil ways of waterfall the
last thing you see before everything
goes down
is the architect in camping in a strange
voice you have died to enter phase one
so that didn't work out very well so
we'll cast something else we will summon
a walking skeleton so the idea behind a
walking skeleton which is a real thing
as I'm sure you're aware is that you
build a thin slice of your software all
the way from user through to any
integration points or whatever it is a
really thin slice and get it all the way
through into production as early as you
possibly can so you don't start building
out kind of layers or frameworks hey
we'll do the view then we'll do our
business logic in some service down here
we'll build this service first and then
that service and the website you build a
thin slice all the way through I think
the phrase was coined by net price and
Steve Freeman and there's a great book
that they've got called growing
object-oriented software guided by tests
which is about how you use test-driven
development to actually drive the design
of your software so we can have some
inner walking skeleton and others that
look like your walking skeleton
coalesces in a cloud of noxious gases
and solidifies as a java drop wizard or
I mentioned the j-word application you
reach into your backpack and deploy a
content store your walking skeleton
reaches out its skeletal arms and grabs
handfuls of raw XML would you like to
transform the XML inside the skeleton or
use a magic box so I should point out
what's going on here so essentially the
content that we've got right content
that's presented on a website in this
case science publishing website this is
stored in a giant s3 bucket somewhere
off in the cloud on someone else's
computers right and so we've got a ton
of content stored off in the cloud and
what we need to do is fetch that content
use a functional programming language to
apply some transformations to it and
then present it as HTML to our users
that's fundamentally what we want to do
and the choice that we've got is whether
to do that transformation inside inside
the walking skeleton itself or isolate
that transformation externally and
separate process a separate service like
a contents that there's an article
service or something so those are the
choices that we've got at this point I'm
going to kind of pause I should point
out that the functional programming
language is awesome and it's access at
XSLT for for those of you wondering at
this point I'm going to pause because
what we're seeing in this book is a
series of choices being laid before us
about how to proceed when we're going to
build this application act and this list
this is not a toy application this is
going to replace a publishing platform
with three billion dollars a year all
right so we've got this path laid out
before us if you remember the name of
the book the bifurcating treasures of
time or reality so the terry pratchet
quotes I see building software these
days as a series as a tree like
structure a series of decisions that I
make now which open up or restrict
diverge or converge around decisions I
can make in the future and this is
essentially what evolutionary
architectures are background it's about
this idea that we have a number of
options in the future we don't
necessarily know well maybe on based on
experience or gut feel we can narrow
down some of the options for the ones
that we think are correct but we don't
necessarily know for sure which options
are going to be right until we try try
them out so I'll just illustrate this on
this tree so one example here is we have
a choice right we can go down one branch
of this one branch of this tree this
graph and we could transform content
within our walking skeleton we can add
the xs/s LT transforms maybe in a module
inside our inside our web application
right but we've got another choice
another choice would be to create this
service outside and isolates encapsulate
that logic for transformation separately
as a process and as soon as you make
this choice we branch we get and
completely different branches in terms
of the way the shape that's upon the
took the topology of our software
architecture for the future I'm just you
know reductio ad absurdum if you follow
the one you end up with a distributed
system and if you follow the other
branch
you end up with like a monolithic MVC
app and all logic transformations
everything in one place in one process
right if you follow these for their
logical conclusions so here's the
question how do we decide which path
define
should we continue to use gutfeld should
we continue to use what I've been doing
this for 20 years I'm pretty sure I know
what to do with it in this particular
situation we can use patterns we can do
all these things but I would suggest
that these days we've got a set of tools
in our toolbox that allow us to take
better data-driven decisions based on
things like lightweight performance
testing so I called this talk betting on
performance I'm going to unpack betting
on performance when I say betting this
is the dictionary definition of betting
the act of gambling money on the outcome
of a race game or other unpredictable
event and this is essentially what we're
talking about right we don't know
upfront which is the right thing to do
given our particular circumstances
especially since building software
systems software systems and the
organizations which produce them are
complex adaptive systems there is not
there is not there is not necessarily in
a distributed system a direct
correlation between cause and effect you
can't directly say this happens and then
this happens there are so many variables
to consider in a distributed system or
in an organization that this becomes it
becomes very hard to reason about
without data what's happening
so betting on performance can we use
this idea of a bet a small bet which I'm
going to define as an amount of money I
can afford to lose all right with some
tooling in order to make better
decisions about software and the
software architects we're going to build
this sort of comes back to something
called option theory so know if anyone
here works in the financial services
industry I spent quite a few years in
investment banks and options are a thing
you can make money on them black Scholes
is the proof that you can make money on
options options themselves have a value
so when I talk about option theory I'm
in the rest of this talk I'll be talking
about it in the sense of a financial
product and that using that financial
product to model actually the outcomes
from small bets that we can place
amounts of money that we can afford to
lose one of those bets look like they
look like people doing work alright
people building software people doing
work so my hypothesis this is recursive
talk my hypothesis is that evolutionary
architecture allows us to place small
bets and then reevaluate our decisions
based on outcomes so when we're in a
place where we can imagine a number of
possible future states right in stock
market terms there are a number of
options that we could put all right we
can imagine you know maybe I'll unpack
options so in option theory essentially
what you have is you buy financial
product at a certain price which gives
you the ability to buy another financial
product at a certain price in the future
that's essentially what an option is so
I'm going to make a small bet now that
Apple's going to go up Apple stocks are
going to go up and if in two months
Apple stocks have gone up above the
price I thought they were going to be
I'll have made some money and if they
haven't I've just lost what's called I'd
be the strike price which is the price I
paid for the option so what we're
talking about here is applying option
theory to decision making in software
architecture I've got a number of
possible futures all right and I'm going
to invest a little bit of money I'm
going to buy those options by getting a
couple of people to do some stuff do
some things maybe all the things to
allow me to make data-driven decisions
so evolutionary architecture allows us
to place these small bets of going up
like conversely if you think about it
upfront design is the opposite all right
we have that tree of possibilities
the infinite number of possibilities
potentially what we're doing when we're
doing upfront design is we're saying no
I know best when it collapse all of that
to a single path this is what we're
going to do which means we're not able
to take advantage of circumstances
changing we're not able to take
advantage of advances that we've made
recently anyway I would suggest it's
more expensive to do it that way back to
the story you throw the magic box in
between the walking skeleton and the
content store a villager approaches and
exclaims this beautiful content I see in
front of me it seems to take an awful
long time to get here you must somehow
make the content arrive faster if you
have a HTTP cache in your inventory you
may use it now so and we've got options
where we're going to put this cache I'll
unpack the architecture a bit more now
but we could cache in between s3 and Ana
content service or we could cache in
between our web app or essentially our
templating service and and our content
service so in the end you have some time
into building this product what we ended
up with is this templating engine at the
front which did Ed site includes
essentially across a number of different
services so the other services here are
things like citation services
recommendation services you like this
author maybe you'd like these other
papers by this author that sort of thing
but fundamentally the core of this the
things that makes the money is the
content the thing that a researcher is
written and needs to be displayed so
that people can read and we've got this
big big chunk 49 50 Meg 60 Meg trunks of
xml sitting in s3 in our case they were
sitting in an s3 over in the US and we
were building this thing sitting in
London so we had a computationally
expensive content transformation service
sitting in Amazon AWS which was using
this functional programming language to
transform the XML into HTML which could
be included by our edge site include
service and what we found initially when
we sort of put this into a production
like environment
was that our performance was frankly
awful awful so these these were our
goals for the program for the project it
was this is global right so this is
Australia this is China there's a noir
point eight seconds behind the first
bike and then one-and-a-half seconds for
page-load and there's a team of about 30
people working on it such a pointer and
when we started running when we sort of
started running some tests on our
production like environments this is
what we found it's really about three
weeks in we ended up with a page load
around about a thirty five seconds mark
oops I think this is what you saved the
back oops
so these are our initial observations
about 35 seconds so I we think we've
detected some performance problems and
everyone else in the room said what you
think you've detected some performance
problems okay what's the simplest thing
we can do what's the smallest bet we can
place to work out where these
performance problems are and to address
them going back to my tree again at this
point we progress down this busy route
this evolutionary pathway and we've
detected some performance problems and
someone on the team said as you do let's
just add this cache right it's in the
story have already explained so let's
just add a cache and well we're going to
put the cache we've got these options
maybe we'll just put this cache in front
of our computationally expensive service
now this seems like a good bet right
because we know most of the time from
our the tooling we've built up around
this is spent in this computationally
expensive service we also know that
fetching stuff from s3 across the
Atlantic is going to put at least 200
milliseconds on on every single call
that kind of sucks so maybe we can kill
two birds with one stone we'll stick
this a nice varnish cache in front of
our computationally expensive service
then we'll save the transformation cost
and the fetch that's kind of nice we
using HTTP we producing HTML one of the
nice things about restful architectures
of course is you can put caches
lots of places use them as reverse
proxies but the problem comes only when
we considers the actual types and access
patterns of the content that we had in
s3 and the funny thing you I'm sure
you've heard this phrase the long tail
long tail of content access patterns for
content which has a long tail doesn't
necessarily fit with with caching
actually the problem is even worse for
this sort of content scientific papers
there is there are millions of these
things going back decades and decades in
some cases 100 plus years sitting in
this in this context or in s3 all right
and in essence when a new article is
published it might might get some hits
initially but then essentially the whole
thing is a long tail so in order to
catch this stuff with our submissions
now essentially if you plot this sort of
access patterns or hits versus misses
and things as a histogram of page load
time what you end up seeing is we're not
on the on your when cache hits
predominates as in when you when if it's
content is accessed a lot then you can
get a big decrease in latency alright so
you can get a big decrease in page loads
but if you've got this entirely long
tail which we do with the content
publishing content publishing data set
with content data sets essentially we
end up with not much benefits unless we
pre populate the cache so that point
we're kind of scratching our heads move
when restitutions cache pre-population
right ok well that leads us onto things
like what happens when the content
changes how are we going to make sure
that the cache is up-to-date okay well
there's a famous quote this is the
origin I think this is from the C 3 C 2
wiki brother there are only two hard
things in computer science cache
invalidation and naming things to which
some some bright spark added and
off-by-one errors but suddenly we're in
this position where we're making a
decision that's going to have quality
architectural consequences just where
are we going to put this caches tuned
into where where are we going to put
this cache how are we going to build a
system that's going
invalidate content when content changes
how are we going to refresh the cache do
we need to pre-populate the cache
because everything's a longtail mmm
head-scratching
so this you know let's just add a cache
that's just that a cache I would say
said no one ever
right it's not the first thing you reach
for so coming back to the title of my
talk I want to unpack the second line
betting on performance notes on
hypothesis driven performance testing
I'm not sure of people I fault sees I
would ask you to Pat handle to make any
sort of make any sense of it so I would
ask you if people in the room knew who
this this guy was so this is dick
Fineman who's in the Nobel winning
physicist and sort of card that you can
say Bongo player safecracker got some
great stories about about finding like
subcritical uranium dumps in Los Alamos
and things and breaking into the highly
restricted highly restricted safes
containing sensitive documentation and
things but also these are genius right
he's also famous for a series of
lectures he gave called I think it was
on the characteristics of natural law on
the characteristics on the character of
law of natural rule and in that he gives
the best description I've ever heard of
what other scientific methods and rather
than me explain what I think the
scientific method is all about I think
we should probably just deferred
directly to him so he can tell us all
about it so strapping
Kazu that's my stop I'm going to discuss
how we would look for a new law in
general we look for new law by the
following purpose first we get it then
we control air that's the beauty truth
then we compute the consequences of the
guest to see what which this is right if
this war that we guessed is right we see
what it would imply and then we compare
those computation results to nature or
we say compare to experiment or
experience compare it directly with
observation to see if it works
if it agrees with experiment it's law in
that simple statement is the key to fine
it's something make it different how
beautiful your guess is it doesn't make
it look like smart you are who made it
get or what his name is
if it disagrees with experiment wrong a
call is to it
the great richard fineman explaining
what the scientific method is you make a
guess that's not what most people think
about the scientific method right first
of all you make a guess I really like
what he says at the end there and it's
again recursive with me standing on
stage doesn't matter who you are what
your name is how beautiful your ideas
are if it disagrees with the real world
it's just wrong all I'm trying to do is
tell you a story about my experiences so
this is a belief but like my academic
friends would say this is but a low-end
number I believe so table everything I
say with a hint of with with a touch of
skepticism and test the stuff out
yourself but going through what dick
Klieman said right so first of all we
observe nature we're doing this all the
time the second thing we do is we make a
guess about about why things are
behaving the way they are the next thing
we do is compute the implications of
that guess well if that guess is true
what does that mean and then we design
an experiment that we can use to work
out whether our guess is correct or not
we compare it with nature as he says we
draw our conclusions or rinse and repeat
but some different terms next to this
first of all we need to be able to
inspect in terms of software we need to
be able to make the guess we need to be
able to make small changes to our
software and actually more and more to
our hardware to our networking set up to
our deployment topologies and it's built
to measure the results and then we need
to say was I right or not right and if
the results don't are the results of the
experiment don't agree then I was wrong
so what do we need to do to actually use
this approach now when we're things like
our cloud native we've got Software
Defined Networking we can do continuous
delivery infrastructure has carried
lightweight performance testing and so
on so I think we need good monitoring to
start with I'm going to unpack most of
this over the rest of this half-hour we
need a brain I mean most of us have got
one of those so that's kind of okay I'll
just leave that there we need the
ability to deploy small changes into
production quickly
we need lightweight probes this is where
the betting on performance comes in this
is where the hypothesis-driven
performance testing comes in and need to
be able to draw conclusions so first of
all talk through some practices I think
that have become commonplace the last
few years and in some cases only the
last few years that enable us to use the
scientific method to drive our
architecture using an evolutionary
approach much more effectively than we
have done previously so the first thing
I pointed out is good monitoring we need
to be able to understand what's going on
we need to be able to understand observe
our systems but what do we mean when we
say monitoring monitoring can mean lots
of different things in lots of different
places so I'm going to define what I
mean is monitoring I'm going to say
monitoring is five things who are four
and then a bonus one if you're if you're
lucky in terms of super-awesome so the
first thing is we need instrumentation
right by instrumentation I mean our code
and hardware are operating systems they
have to be able to describe what they're
doing right instrumentation the second
thing is telemetry those systems need to
be able to tell us what they're doing
there's no good just having logs sitting
on a log on a server somewhere on
someone else's datacenter sorry the
cloud right those logs have to be
aggregated so that we can observe them
right it's no good having plek be
running if everything is dumped locally
we need to be able to get that Slama
tree and make use of it so we have a way
of gathering the data from our
applications or services then you've got
monitoring right this is our ability to
meaningly visualize what's going on and
there's awesome tools in this space
these days and finally of the sort of
main four you've got alerting so this is
our ability to take action when we've
discovered that something is happening
that's out of the ordinary and the bonus
one at the end is predictive alerting so
yeah this is we can tell before
something goes wrong that something is
about to go wrong
we can tell a hard drive is going to
fail before the hard drive fails become
because on average in our data centers
they fail every X number of days
so we can predict in advance when
something goes wrong so I'm talking
about here really is the first three
instrumentation telemetry and monitoring
these are really useful for running
essentially experiments right for
understanding which of the options that
we can think about or we can imagine
which of those options are the best
quote or to buy rather so what do I mean
by by monitoring what I mean what sorts
of things would I imagine monitoring in
a distributed system so you've got two
services they're running on two boxes
this is my patented way of trolling
Simon Brown if you know Simon Brown I
utterly receivin though I uses his c4
model on my teams I refuse to use it in
conferences just a troll Simon so that's
the thing what I mean by by what should
we be monitoring what should be what
should be instrumented so we should be
thinking about request latency all right
we need to know for every request going
through our system the request latency
and it needs to be sent to us so that we
can observe it and we need to know that
request latency throughout the entire
system it's very good
just knowing that goes into one system
and goes back we need to be able to
trace that throughout the entire
threatening try an entire call stack we
need to know about service health so are
my dependencies okay all right what are
the queue depth if I'm using queues how
many messages have I got backed up what
what sort of thread pools am i using how
full they are how busy they are etc we
need to understand if our downstream
dependencies are healthy we need to be
able to know just by looking at
something a dashboard whether a service
can talk to other services it's
dependent on we need to understand what
operating system metrics and we need to
collect operating system metrics we need
to understand about whether our CPUs are
busy whether were blocking on i/o a lot
and finally again I said we need to do
request tracing so there's a great group
of companies that come together to
create open tracing the i/o so this is
Zipkin was the the tool from Twitter to
allow you to do distributed tracing
across a distributed system for requests
open tracing our air is then plus others
getting together to produce a an open
open-source version of that well
actually observe the standard
so I have a look at that but these are
the things I think we should be
monitoring as a minimum in our systems
especially for building micro services
in the projects we've been talking about
the content publishing site we were
using a ton of tools these are these are
more prevalent in the Java world some of
them are specific to the Java world but
the analogs exist in all languages so
things like hysteretic for circuit
breaking we were using graphite to the
visualizations dashboards
music movies we're using coda hell
metrics and we were using a lightweight
application server sorry we were using
jetty so and drop wizard which I guess
Nancy is an equivalent and we had a ton
of dashboards so even when we were in
development we had a ton of dashboards
this is one of our graphite dashboards
which shows us how long requests are
taking for various types of requests so
you use a request coming in from real
people also requests between services
and so on this is a look at what our
circuit breaker dashboard to look like
so this is the history dashboard history
its view and the history historic sees
will circuit breaking is kind of
interesting you get a lot of very dense
information a lot of information sorry
rewind that the information density for
these dashboards is very very high
season
you not only get things like requests
per second but how many servers are in a
pool
you get weather circuits are open and
closed so that's the first thing
monitoring and specifically unpacking it
to those first three things right
instrumentation telemetry and
visualizing what's going on think we
need that the next thing is continuous
delivery all right this is totally
revolutionized the way we think about
building software these days I know I'm
speaking to the converted you've opted
into this conference so I imagine rule
all over this I'll just go into some a
little bit of detail about it these are
the this is my three monkeys of
continuous delivery so this was at a
conference over in Aarhus we were a
friend's house having dinners it's some
damn North's and Newman who's around and
wood guy with his hands over his eyes is
Jess humble there's the three monkeys
of Sidi so what I mean by city well I
mean I want the ability to safely and
sustainably push single line of code
changes all the way through from my
machine into production right so I'm
sure you've seen build pipelines before
it's my legatees hey what's up my
machine I want to push that into a
repository and that code is going to be
automatically pushed all the way through
into production right and I don't just
mean code for functional code for the
functional software writing I mean all
code so I include infrastructure code so
in this project we were using ansible to
manage all of our environments and to
manage our up our production deployments
so that should also be we should do
continuous delivery on our on our
infrastructure codes too and obviously
we get more product one way but we want
things to fail early and fail fast we
get faster feedback the other way which
we've got these things what does that
mean then especially with things like
continuous delivery well with continuous
delivery we can actually go through this
loop really quickly now right rather if
we're not waiting for three weeks a
month six months the kind of old style
water water fall from release process
all right we can we can create
experiments of run experiments much much
more quickly than we could do previously
that's kind of nice hours minutes on
demand hopefully so the next thing I'm
going to talk about is lightweight
performance tests these are really
useful I think as a probe onto our
system it's a probe onto our system and
I'll unpack that a bit so when I'm when
I'm on teams I kind of sometimes I'm
pairing on on a I'm pairing on a team
sometimes I'm attacking teams sometimes
I'm a kind of principal across many
teams and so on but in all those cases I
try and punch it really hard to get
performance testing lightweight
performers test into my build really
really early so I mean like within two
weeks I should be running continuous
performance tests or performance tests
on every check-in I want to say
performance tests I mean you know let's
run let's run something for a minute and
and store the results somewhere so we
can graph them over time because the
nice thing with that is not that you
know
even if it's not run on a representative
production life environment we get this
sort of ratchet effect where we can
observe whether our performance is
increasing or decreasing over time so as
a developer when I check in some code my
little performance test runs
maybe it's branched off in a separate
pipeline I think don't have a look
whether I've made something a lot worse
or not so lightweight performance
testing we call this out on the
technology radar but with technology
radar some years again there and what
sort of tooling is out there to support
it well there are really simple tools
like command-line tools like just a
single command tools that will help with
us so some examples ABC Apache bench is
used to benchmark the Apache server that
losing the name is another tuple sees
another kind of great really lightweight
command line tool and vegeto which is
golang are quite like Vegeta because it
gives you the ability to specify a
constant rate of requests so the others
you generally say run for a minute and
it'll just the tool will hammer as many
requests as possible after the path of
thing but with Vegeta you can say run
for a minute and do two requests per
second and that way you get you can use
it as a probe the next thing I think is
really useful is infrastructure as code
and this is the book is only just well
probably last year in the book came out
by another colleague of mine Keith
Morris and this is this is again cool
cool to this technique I'm talking about
today all right so on this project we
have the ability in our answerable
scripts to say things like ok increase
the number of servers I'm running on
from 3 to 5 go it does it in all your
environments after being tested in a
build pipeline you know 10 minutes later
you've got more servers running decrease
it we can bring them back write deploy
more in terms of caching for example you
know if we wanted to apply varnish
cluster somewhere that is as simple as
adding a few lines of code in a script
and and hitting go so this is this is
really I think crucial the idea of
infrastructure as code so this is
actually a real picture from by tacked
together some Python to be honest and
created a little visualization in one of
our environments this is our so this is
one of our environments for
project after about the time when we
were thinking about having caches and
this is it doing a Bluegreen deploys so
and this is all just described
encouraged - or describes encodes
checked in with with with the rest of
the code
so talked about having good monitoring I
think we sort of got that we've all got
brains that's kind of cool we need the
ability to deploy small changes well
continuous delivery gives us the ability
to do that both for functional code but
also for our infrastructure code and
we've got the idea of lightweight probes
and tools that will support us probing
our system to understand whether our
hypotheses are correct our guesses
alright let's see what happens when we
put it all together you might remember
the initial observation we had was that
we've got these requirements and 0.8
seconds of first bytes one and a half
seconds of page loading and we had a
initial page load of about 35 seconds it
was actually way more on the high
percentiles that I'll stick with 35 -
that's I'll do so we can't kind of have
a faint cram we thought okay well we've
got a number of options for what we can
do next
right and each of them is going to cost
some money because we're going to have
to spend some money to fix this thing
all right but what's what's the smallest
best we can place all right what's the
smallest bet we can place number of
different options we can add some CPUs
because we've got this thing doing lots
of XSLT transforms and that's pretty CPU
intensive we can you know put a couch in
front of s3 we can put a cache in front
of our content service we can and then
there are other options as well so based
on this approach of bit thick fireman's
not this idea of observing nature so the
first thing we did is look at what was
going on right using our monitoring all
right so we had a look using our
graphite tooling the next thing we did
is make a guess that we said XSLT we
know from experiences can be expensive
maybe we are see maybe we are CPU bound
what the implications of that well maybe
if we increase the number of CPUs on the
boxes that lack compute units
it's on AWS if we increase the number of
complete units per box that will have an
impact that will help us with our
performance so we compared compared it
with measure we ran an experiment right
so we used in this case siege and it's a
single command line we gathered some
data about whether whether our change
that we've made add in CPUs have made a
difference and then we observe the
results I should say this change right
is a single line of code in a ansible
sprint to their playbook increasing the
number of CPUs robots is like changing
one line of code and hitting go and it's
happening in all your environments ten
minutes later or sometimes less than
that and after we went through this
process were like okay something look at
the results
ah turns out it was CPU bad all right
nice so we got massive increase in
performance just by adding some more
calls for more computing units into
those resources so the next guess we
made ones based on the fact that we'd
observes
we've observed that the length of time
it was taking to fetch content from s3
was was pretty long so I don't if we
know this but there's like a secondary
industry now in monitoring s3 regions
around the world and showing you their
latency and it's pretty inconsistent
actually it says spikes you get lots of
different spikes on average we were
going from the UK to the US it was
between two and four hundred
milliseconds latency so alright well
that kind of sucks we can probably
remove some of that by moving moving our
computer to other data is I'm not
transferring these 50 megabit megabyte
files around slowly across the Atlantic
we don't want to where sense if we don't
know where the electrons that do it you
know passing all that data back so we
tore down our infrastructure and we
built it in in the US u.s. East and that
again was pretty much three lines of
code comes down one place stands up
somewhere else so what was the result of
that experiment now we gained a couple
of seconds actually on that nice the
third guess we made was that
transformations are slow and could be
optimized so let's actually look at the
code that's running these
transformations can we optimize those
transformations and yes we could and we
gained a half a second from that and
then the final guess we think we
probably need to exercise the option of
putting a cache in somewhere now because
we've exhausted the other options we've
played all those bets and we've got a
lot out of them but we're still off our
offer our goal which was the one and
half seconds and nor five seconds
so it's ticket caching so that's what we
did we started cashing in front of our
computationally expensive service and we
were down to null point two seconds for
payload suspected this is for the
smaller files this does not feel like
you're 50/50 Meg but you know 200
milliseconds actually pretty good and so
as you can imagine when we got the
results and we talked about them in our
standard the next morning like a
spongebob so getting back to our sort of
decision tree right the choices that
we're making this divergent graph of
choices
you know the beginning we decided that
we were going to create another service
that we were going to that we were going
to transform our content in which opened
up a number of other options for us at
this point later on after we exhausted
options in terms of tuning performance
of the XS XSLT transforms moving our
computer's data and adding CPUs for CPU
CPU band box we reach another decision
point do we need to add a cache we
decided we did need to add a cache the
interesting thing here is because early
on we chose to create a separate service
alright for our content transformation
we notice that increase the number of
options available to us if we put
everything in the same box right we
would have limited our options because
we could only have put our stuff in
front of s3 but because we kept their
options open
we've got a choice of whether put cache
or website implement caching so this is
our decision do we put a cache in front
of s3 do we pass in front of our content
service these are the options that we've
got and we can place bets by these
options if we put it in front of s3 well
essentially what it does is it just
saves the fetch from s3 which we know is
back 200 milliseconds if we put the cash
in front of the content service we saved
both the fetch and transform costs so
let's see what happens
back to our story content trickles into
the store you keep up by listening for
the new content and casting will get on
the cache to keep it refreshed new types
of content appears content the villagers
have never seen before
content the walking skeleton is unable
to combat every time the structure of
the content changes the cache must be
refreshed the cache grows and grows
until it is the size of the great wahoo
you've the raster ababil something it is
no longer possible to refresh it your
latency increases again you have died to
enter page 1 so by putting the cache arm
packets in front of our content service
yes we saves the time it was taking to
transform to transform the confer
content and we were saving the fetch
time but we actually limited our ability
to deploy changes as every time we
wanted
could change to an aid to that API we
had to refresh terabytes of cash and the
whole thing came crashing to a halt
turns out that option wasn't
particularly a great one to start here
in the end it was a sort of dead end if
you like an evolutionary dead end but
that's fine because we have other
options we can place and we've got the
tools in them in place all right to make
other bets so we made a little with a
little change of our ansible playbooks
and put move varnish from in front of s3
to aside from selling the content so
there's two in front of s3 moved in
movie one down at that point even though
we didn't save the XLT XSLT transforms
we got more or less the same benefits
yeah so I think at this point it's
probably time to close this book so what
I've talked about in over the last sort
of 50 minutes or so it really comes down
to this idea of option theory we're
options and our ability to place bets
and the ability to use tooling that we
now have available and practices that we
now have available to decide different
things and to build decide about to make
different decisions about our
architecture than we have done in the
past cheaply all right we no longer have
to think really hard upfront because
changing things other things afterwards
is expensive especially hardware
all right especially big design
decisions that's gone that's like in the
past we have the ability now with
infrastructures code CD and lightweight
tooling to make small bets plays small
bets many of them and go where our path
takes us so I often come back to the art
of UNIX programming and in the art of
UNIX programming all right Eric Raymond
describes various conversations with
various other UNIX luminaries one of
which he sort of boils down to a quote
from Kent Beck operate on make it run
make it right then make it fast
also you've got these 17 rules of UNIX
programming right this is 40 plus years
old now and there's a few that apply
here rule is simplicity
designed for simplicity and complexity
only when you must I could add to that
actually now and say we've got the
ability to to design for simplicity and
as complexity or by the ability to add
complexity later if you need to the root
of economy programmer time is expensive
conserve it in preference to machine
time a little optimization prototypes
before polishing get it working before
optimizing it so I'll leave you with
these thoughts so I think evolutionary
architecture it keeps our options open
it doesn't collapse the things we can do
in the future doesn't collapse all those
options down which upfront design does
right we've got continuous delivery
we've got lean product engineering you
might want to say lower case agile are
in mind I talked about leading product
engineering these days and we've got
infrastructure has carried and they
reduce the amount of money we need to
spend placing bets we don't need to
place big bets anymore we can place
really small ones and learn more of them
and then we've got this idea of
lightweight performance testing doing
this early and having good monitoring
and things in place they allow us to
make guesses and test our hypotheses so
we can actually approach this like bit
fireman says using the scientific method
that's all I got
thank you I think I've got five minutes
for questions if there's someone fit
enough to keep running at the devil who
stands for four minutes now
I've in Jersey lights so that's one in
front it's always the people in the
front row who have the difficult
questions this is especially the guys
already done a talk about performance in
this okay so my question is you
mentioned Richard Fineman and
calculating the effect of those those
things I have seen lots of calculations
there now maybe you've just skipped over
which is okay so we how would you know
that adding CPUs can make a difference
so off are you saying we should be
calculating that looking and going what
so I mean am i cpu-bound here let's do
that or are you saying I'll actually
dammit why don't we just try would you
prefer a sort of kind of we think it's
that and then just explore or would you
want to say actually looks a bit more
calculation or do you think that that's
just on a whichever you prefer at the
time basis that's a great question so so
the question was do you just in it is it
a case of I'm gonna something is slow
let's put more CPU in here right just
that's the guess because we could have
said something too slow let's put more
CPUs in one of our other services or in
the way in the web front-end instead or
do you say look at what the data you've
got and make guesses based on the data
that you've got and you I did skip over
that because that's kind of the approach
the second thing I just mentioned was
the kind of approach we took is we have
that that visualizations we were able to
look at look at the length of plan
request for taking between each of our
individual services and the length of
time actually processing was taken in
each of our individual services and then
make a guess essentially based on that
data so um we could see that method
calls that were double running XSLT
transformations we're taking a really
long time and we could correlate that
with the fact that yeah our CPUs a
redlining front we could say our guess
is because but we're going to increasing
if we increase the number of complete
units we'll get we'll get some
performance improvements so yes
does that answer the question yeah cool
yeah sure
I should have should've clarified thank
you
cool
well thank you very much all for coming
I'm actually back here in 20 minutes -
let's open architecture and
organizational design so maybe see some
of you then Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>