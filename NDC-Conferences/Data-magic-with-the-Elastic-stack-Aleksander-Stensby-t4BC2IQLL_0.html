<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data magic with the Elastic stack! - Aleksander Stensby | Coder Coacher - Coaching Coders</title><meta content="Data magic with the Elastic stack! - Aleksander Stensby - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data magic with the Elastic stack! - Aleksander Stensby</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/t4BC2IQLL_0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I think we'll start given that I've got
a lot to talk about my name is Alexander
stem spear and I'm going to talk about
the elastic stack focusing on elastic
search and Cabana
how many people have heard about elastic
search good how many people use elastic
search or I've tried it not too many
okay okay so I'm not going to spend a
lot of time talking about what elastic
surges in this talk because I've done
that way too many times but I think
everyone can follow along I'm going to
show you a lot of really cool things um
but first the mandatory slide so I work
in a company called Monaco and I also
work in a company called cloud nests
which is not related to the IT industry
at all it's a 70 year old shipping
company and the examples I'm going to
use today is actually based on the
shipping industry it's a really
old-fashioned industry and we're trying
to see if we can really improve the
nature of shipping so I'll show you some
some hands-on examples I've been working
with Serge since 2004 search and data
analytics and that is my passion and and
that's what I'm talking about the
elastic search as well I'm trying to
show you how we can use what's
fundamentally is a search technology to
actually do data science and analytics
not just text search which is what most
people associate with the search engine
so hopefully you'll leave the room later
today with a lots of ideas of how you
can take the technology back to your
work and and apply it so I use this
slide every time and I I just love the
story behind this so everyone knows that
this is the Mars rover right so this
little fella here has been driving
around on Mars for quite a while this is
the Curiosity and in 2015 a few years
back I did a talk on elasticsearch here
at NBC and
was really inspired by this sweet this
tweet came from one of the engineers at
NASA stating that now elasticsearch is
powering the analytical engine that they
use for all the sensor data that they
collect every day from the rover so
that's a huge amount of sensor readings
every single day and they do all of
these surface monitoring temperatures
atmosphere compositions and all of these
things and everything is powered by
elasticsearch and I thought wow that's
so cool because that's the like I can
talk about how we use elasticsearch for
analytics but you know the power is
really in the big brands the big brands
that everyone know and so I don't think
we can have a much better name than NASA
up there so traditionally most people
know that the elasticsearch is powering
some of the most important tools that we
have we wouldn't survive without them
namely Stack Overflow and Wikipedia but
in addition to that that's obviously for
document retrieval so I'm looking for
this find that document in the haystack
that's essentially what the search
engine does what's cool is that over the
last few years the search engines have
taken a step in the direction of data
analytics and that's where I'm going to
focus on today so I'm not going to talk
about traditional search I want to talk
about how big brands are now using this
technology to do really really
sophisticated data analytics so for
instance nas I've already mentioned
Walmart is one of the biggest grocery
stores in the or ya grocery stores in
the US and they have so many stores
around the US and all of the information
about everything that they sell they run
through elasticsearch so they have a
real time analytical platform on what's
being sold in every single store
throughout the u.s. it's a huge amount
of data now I thought that I was working
with pretty big data but like this is
like on a different planet
tinder interestingly enough
their entire technology stack is based
on elastic search all the matchmaking
everything that they've done is
utilizing elastic search geographical
location proximity searches finding
people near you everything is done by
elastic search Netflix they use a whole
range of different technologies
obviously but they also use elastic
search eibar is another example okay now
you might think that I work for elastic
search but I do not I just love the
technology and I love search so I've
started working with Lucien back in 2004
then I moved on to solar and I spent
probably six or seven years working with
solar and then over the past five years
I've been four or five years I've been
using elastic search and I think this is
this is the future and I think it's my
obligation and duty to tell you about
all the possibilities that is in the
technology because you know more people
should use it next time I want to see
more hands using having tried it because
it's really simple to get started okay
so this was the the agenda when the NDC
guys called me and said can you talk
about elasticsearch and as a actor I can
do that elastic search v was launched
earlier this year that's that's a good
topic to do but then two weeks ago they
launched version six alpha in technical
preview and I thought well it's no fun
talking about all technology so I had to
redo my whole presentation so excuse my
French screw that I'm going to talk
about the last feature six alpha to
today so I'm going to do a lot of live
demoing and everything will probably not
work perfectly so I'm excused by using
the alpha version right okay I'm going
to focus on Qabbani
because that's really a great tool last
time I spoke about this two years ago I
said there's there's a lot of potential
in Cabana but there's also too many
limitations so we had to build our own
dashboards we had to
their own tools now I think Cabana has
come to a point where we can start using
it
there are still limitations but we can
overcome them and then if we have time I
might give you a sneak preview of
machine learning which is something they
launched in beta less than a month ago
okay
so the elastic stack so I can get this -
it consists of elasticsearch obviously
the heart of everything
kibana which is the tool that we're
going to use which sits on top of
elasticsearch which allows us to explore
the data navigate through the data and
do a lot of the data analytics that
we're going to do today and then below
those components we have two projects
one called log stash which has been
around for a long time that's
traditionally been the entry point for
most software developers into the
elastic stack that's a something that
has to additionally be used to collect
logs and structure logs and make them
searchable ultimately so that you can do
analysis on your logs and then over time
new use cases for log stash has popped
up it's a quite robust system to
actually parse different files and
different data formats so it's not
limited to logs although log is
obviously central to the name of the
technology over the past year or so
we've seen the emergence of something
called beats which is a really exciting
project that the elastic team with the
community has developed any nice since
this is a platform for collecting data
from remote nodes sending it back into
one central storage elasticsearch so
that you can analyze your data so tiny
tiny containers that actually just sits
there and sends a beat at whatever
frequency that you specified and in
addition to this the stack
obviously contains x-pac as well which
is the
the licensed part of elasticsearch the
commercial side of elasticsearch I'm not
going to talk too much about that
because I love the open-source aspect of
this but now over the last year x-pac
has really grown to become one package
that it's really worthwhile so if you
are in production and if you care about
security and monitoring it's really a
no-brainer and if you're using elastic
cloud which is previously the Norwegian
company found which is the elastic
search as a service then you get this
x-pac solution as part of the cloud
solution which is which is really good
they also introduced something called
alerting I'm not going to show that
today but I just want to mention it
because the remember that the guys at
elastic and the community around it is
developers and they tend to focus on
things that developers tend to focus on
I try to focus more on the business side
of things because ultimately that's who
pays us to do our job as developers so
more alerting was built and logstash was
built and beats was built with the
aspect of a developer monitoring your
server's monitoring your infrastructure
alerting on events that happens your
infrastructure so that you can make sure
that your services are always up and
running but the same logic the same
fundamentals can be applied to data data
quality not just server and maintenance
so in terms of alerting we can use that
now to set up business rules on data
that we can actually send out to our
business users which i think is really
fascinating we can use it to have
thresholds on data quality not just that
our servers are running but actually if
there's an anomaly in the data if
there's something wrong if there's an
emerging trend in the data and this is a
really quick way for us to have logic
reach our business users the last two
components of the xpac is
grass module which I'm not going to talk
about and then the new machine learning
module which is I would say a promising
module but obviously far from a complete
machine learning library so last few
slides and now I'll jump into the fun
stuff if I get a lot of questions about
data ingestion how do I get started how
do I get data into elasticsearch where
do I start
now before logstash was probably the
only way where you could sort of read in
quite easily with a little bit of hassle
sometimes but easily now if you can read
in data and get started but ultimately
you want to collect data from your
databases and shove them into
elasticsearch and make them available
for search for analytics etc and that
has been more difficult and then I have
a lot of use cases where I've got
business users saying I've got these
Excel spreadsheets I'd really love to
get them into the engine and start using
that data in elasticsearch or in Cabana
and that has been problematic as well
but with the introduction of beads and
pile beat as one of the elements here we
can read any type of file and we can
build their own beads if we want to we
can also use log stash to read in from
databases as well so there's a lot of
opportunities now and it's no longer a
hassle to get data into elasticsearch
but the cool thing that beats has
actually triggered is the introduction
of something called and ingest node and
in this node is essentially a part of
elastic search the core system of
elastic search that allows you to define
a pipeline of processors essentially
allowing you to do the transformation
steps that we haven't done before so
essentially what you do is you read in
data and then you can apply rules easily
before you actually store the data in
your index that means that you can
transform a row
data set you can enrich it but it comes
at a cost obviously but basic things you
can do variously and it comes with
something like 20 out-of-the-box
processors that you can start using one
of them is a grok processor which means
that you can essentially use any growth
pattern to transform your data you can
add fields you can expand on your data
set by combining fields by by splitting
fields there's a lot of opportunities
there and it's very easy to get started
with alongside of the ingest node is
something called the re-index API that's
another question a lot of elasticsearch
users have been asking how do we is li
reindex our data and the answer has
previously been well you have to go back
to the source and reindex from source if
you've made a mistake and you need to
change your mapping you have to re index
the data that's how elasticsearch works
but with the re-index api in combination
with ingest node you can actually do a
lot of these things without going back
to the source and the reindex I'll just
show you what the payload looks like I'm
just going to escape this see if I can
yeah thank you very much I can do that
make it a bit bigger so in this case
what you actually do is you can specify
even a remote host which I've done in
this case just put in some dummy data
here I can actually read an entire index
or limited to whatever query that I
specify and say I want to take all of
this data and I want to index it locally
where I'm running this query from so
here I say that I'm going to take this
test index and put it into my local test
index and then I'm also going to apply
this ingest pipeline saying that you're
going to take the data and then apply
all of these rules that I've specified
which is a great way to actually
transform your index when you've made a
mistake it's also a great way to give
your developers a
sample of the production data for
instance to use locally when you're
actually doing logical testing on the
data which was really difficult before
and you actually had to do your local
indexing process so I just wanted to
mention that because that's hidden quite
well in the documentation it's a hidden
gem in my opinion okay so I'm just going
to jump back here given that not all of
you have ever used elasticsearch I think
that I have to just give you a very
quick introduction to the syntax so
we're using Jason lasting search is
written in Java it's a rest-based jason
api and the syntax looks like this which
can be a bit scary to begin with but
it's actually quite straightforward
so they've done a lot of changes over -
over the last few years now there's a
lot of breaking changes in 6.0 so a lot
of your data that you have in 2 or 5
probably not going to work straight out
of the box but I'm not going to go
through all of those things the most
important thing is to note for those who
have used elasticsearch is that the type
is disappearing so previously everyone's
that tried to explain lastik search in
this way it's similar to a database
where the index is the database and the
type is your table so in this case
they've realized that that was a flawed
logic to that and so now they're
actually removing it so in the future
now you can run 6.0 with an index
containing multiple types so now an
index can only contain one type and in
seven which is coming probably next year
they're going to remove it completely
so there's some job for us developers
there in the future when we migrate when
we write queries we normally use the
boolean query as our starting points and
then we build out from that the boolean
query allows us to do combinations of am
being an orange of different query
classes and then allows you to specify a
filter the key difference between a
query and a filter
that the filter is either yes or no that
means that it's a binary question do you
match or do not match whereas with a
query we can actually say how well does
it match and we can introduce something
called scoring not going to go into the
details there but the key thing is that
filters is highly efficient and you
should always strive to use filters if
you don't care about relevancy or
scoring and the reason why it's very
efficient is that we can cache the
results with bit sets so my lesson
number one is always use filters if
possible so the examples that I'm going
to show you later on will be using
filters here's one example where I'm
taking geolocation so let's say I'm
dropping a pin on my phone and this is
my location it is where I'm standing
show me all the coffee shops within well
200 kilometers that's not that
interesting but let's say 3 kilometers
and then this will give me all the
results that matches so essentially this
just draws a circle around my point very
simple query very powerful query now the
keeping before we go into Kabana
is aggregations that's really the power
of elastic search when it comes to data
analytics aggregations you can somewhat
compared to sequel with functions so
mathematical functions like averages and
min and Max and some and those kind of
things and then another thing of sequel
which is the group by function
essentially what they call buckets in
elastic search can we separate our data
into buckets so we have aggregations is
essentially defined as a combination of
buckets and matrix and it's really
powerful because we can build and nest
on these aggregations so in this case
I'm saying give me a count of all
documents and
route by the speaker in this case then
you can build on that and say I'm
interested in beer who isn't I want to
group by the type of beer and then I
want to for each of those calculate the
average ibu value and the result would
then give me all the different beer
types like Brown Ale IPA etc and then a
number for each of them so if I had 10
IPAs
I'd get the average number of IBEW
values from those 10 IPs now you're not
impressed yet okay so in 2.0
elasticsearch introduced something
called pipeline aggregations and this is
where it starts to become interesting
before the developers was required to
actually solve this so you could do your
query you could get the buckets and the
metrics etc but then if you wanted to
actually calculate something with the
results of those aggregations you
actually had to do that in code or in
Excel afterwards if you took it out so
in this case pipeline aggregations
allows us to compute on the results of
the aggregations that we specified now
this is really useful there's a lot of
different types here I'm not going to go
through all of them but I'll show you an
example so one one example is if we've
got sales data so of you know a volume
of sales or individual invoices that
we've we've sold and we group them by
month so we're using what we call a date
histogram essentially what we use to do
time series analysis and then what we're
going to do is we're going to do an
aggregation called sum which is a metric
aggregation so for every month we're
going to calculate the total sales so
then we can display that as a bar chart
or a line chart or whatever where you
see how much you sell every month right
but then it would be really convenient
for me to also be able to display a line
on my chart my front-end developer is
nagging me and say can you also give me
out-of-the-box
an average line calculate
based on this data not very difficult to
do in code obviously but convenient to
get that so here I'm actually doing an
average bucket and I'm saying I'm going
to use these sibling aggregations
because this is this point on the same
level as the route aggregation and then
I get back to the data this could be
used for instance when you're doing like
temperatures and you want to do rather
than average bucket you do a max bucket
or a min bucket and then immediately you
get those values out the highest number
so the the warmest month or the coldest
month in this case I'm going to skip
past this but you can also do moving
averages you can also do something
called cumulative sum which means that
it can stack up over time so you can do
something like this total sales by type
of car for instance I'm going to show
this in Kabana okay so I'm just going to
jump over to kibana
and I'm going to do two different data
sets and the first one is it quite tough
sorry about that
see if I can get
there we go okay so let's do like that
two different data sets so the first one
we're going to look at a data set that
it's being used quite often it's an open
data set that anyone can go download and
it's a database of accidents traffic
accidents in New York City I've done
extensive work for the fire department
in Norway working with a lot of data on
all fires and accidents and everything
that the fire departments do across
Norway somewhat similar but much more
complex data and obviously sensitive
data so I can't show that so I figured
that I'd show you an example of how we
can can use this data instead which is
something you can download as well so
the data structure is not too
complicated and I'll skip down here and
see if we can zoom in a little bit it's
got quite a lot of data it's got geo
locations which I'll come back to it's
got some different tags so it's
categorized by the borough etc the type
of car involved in this case you know
see noisy day now which is not uncommon
when you work with data analytics and
then you also have some calculated
numbers so number of people killed
number of people injured number of
bicycles bicyclists killed injured etc
and obviously this is a good starting
point to do some statistics so what I
want to do is I want to show you how
easily we can do this in Kabana
how many of you have ever tried the
Kabana before good ok have anyone tried
alpha 2 of version 6 okay good there's a
lot of really cool stuff in here so
you'll recognize some of this obviously
but I'll start the sauce play and I'll
do like a very simple thing first so I'm
going to do a histogram so I'm going to
do a bar chart over time and I'm going
to utilize what we call the date
histogram so I go into kibana
I select the visualization that I want
to use click on this and I select the
date histogram that's the starting point
automatically detects the date field if
there are more you can switch so I'm
going to use this the time stamp now in
the upper right hand corner I can choose
what type that's a bit difficult to see
but you can probably see it it says last
five years so I can change that easily
but for the purpose of this
demonstration let's use five years now
I've got a time series it just gives me
the total number of accidents in this
case by day or month probably let's see
it's automatic so I can change that and
let's say we want to do monthly so I
just do that and that was monthly okay I
can also then do yearly if I wanted to
do that then I get a total number of
accidents per year and my data might be
flawed because this actually started in
July 2014 I think so we're not going to
get a full data set and kibana cleverly
indicates that there's probably
something wrong with this data so you
can see there's like a gray bar here
saying that well this actually doesn't
look like it's a complete data set and
this one isn't a complete data set and
it also shows you that when you mouse
over so they've had a lot of neat little
features there now I said when we do
aggregations we can do nesting and we
can also do other things I'm just
counting so let's do a function if we
were to code this with adjacent requests
we would have to do a date histogram and
then we would have to nest a average or
some calculation inside of it now we can
do that with just point and clicking
kibana
which is quite convenient so I'm just
going to click here y-axis by the fold
says count I'm going to change it to sum
and then I'm going to select the field
and I'm going to do number of persons
killed very dramatic okay so let's just
have a look at that so that that's the
number of people killed in a car
accident in New York per year so it's a
positive trend in that sense it's
dropping down and then I can also do
something cool I can split that by the
type so recall
that we have a tag in the data set that
said type of vehicle so I can actually
say how many were killed
divided by the different type of
vehicles involved so I'm going to just
do that right now I'm going to add a sub
bucket or a sub aviation and then I'm
going to say I'm going to split each of
my bars and I'm going to do a terms
aggregation that's essentially the group
by function from sequel which you will
recognize so this is something you
normally would use on a field in your
data set that it is somewhat limited in
the number of potential values because
obviously if this is a free text field
it's a bit more difficult to run that
calculation so normally you do it on
tags on on properties like the Burroughs
etc so I can start with that and say
burrow and then I'm going to do play and
then I see zoom out a little bit I can
see where most people get killed in New
York in car accidents or in traffic
accidents is what it call it so here you
see Brooklyn is the biggest sinner of
this and then I can also do like we said
we can say I have a look at the type of
vehicle so I'm going to do vehicle type
down here from the drop-down and I'm
going to just do play and then I get
passenger vehicle oh not too surprising
then I got bicycle up here so no people
killed in 2015 and a bicycle accident as
it appears there okay
now this isn't too exciting because
you've seen this in previous versions of
kibana as well and just want to quickly
show you that you can obviously do a lot
of different visualizations for instance
a pie chart and people think well what's
the point of that we can do that in
power bi we can we can shove data into
power bi the key difference is obviously
volley
of data that you can use another very
important aspect is the licensing
agreement that you get with power bi so
in a corporation having used power bi
also in parallel with Cubana is not
really feasible to use cable use power
bi for a large number of users in an
organization that that's obviously not
the most important part but I love to
talk about open source so in this case
the the most important thing is actually
the abilities we have to do really
advanced combinations of queries which
you can't really do in power bi so you
can do simple filtering on and off it
should match this value or that value
etc it could be this time span etc but
once you actually start introducing
things like geo data once you start
actually doing aggregations that we have
seen now starting to nest that power bi
falls short so and obviously the other
aspect which makes me love this is that
we can take whatever problem that we
can't solve here and easily do it by
talking with the developers and say
let's do this there's a JSON API we're
just going to write a query and we're
actually just going to solve that
problem so the cool thing is in its own
looking at a pie chart it's not really
that interesting where we're going to do
it anyway so I'm going to do a terms
aggregation and I'm going to do it on
this vehicle type I'm going to do top 5
I can change that and say I'm going to
do top 10 so now I have a pie chart but
I could do that in Excel of course the
thing is this is an interactive pie
chart I can use that essentially I can
use that as a filter in my dashboard so
I can use that to toggle on and off
values I'll show you how this comes
together once we build our dashboard I
want to do one really important thing
and that's what's completely new in
elasticsearch 6 and that's called the
visual builder I think this has been
triggered by a development called
timeline which was introduced
one and a half year ago and which
focuses on time series data there's a
lot of limitations on look and feel and
other forms charting in Qabbani this is
the first and the major step towards
making it much more customizable so I'm
going to show you some cool cases using
the visual builder now I'm going to
we're still looking at this accident
data so I'm going to just start doing
the same thing I did earlier I'm going
to do the sum of number of people killed
so then I'm going to actually just say
that I'm going to limit this to my index
called NYC traffic data so that I don't
have to choose all this and then number
of persons killed okay so this updates
automatically once I do anything
essentially it'll it'll change and I can
then do another time series here can
obviously say number of people killed
and then I'm going to do another one
which actually adds up so you remember I
mentioned this cumulative the pipeline
aggregation which allows us to use the
data from a previous aggregation to do a
new calculation now I'm going to do that
so I'm going to actually say down here I
have to start by specifying the sum one
more time here so I'm going to do number
of persons killed then I'm going to add
another metric now you'll notice on the
side there that the little eye symbol is
now only showing on the bottom line so
the previous one is not going to be
displayed so it's the last one here this
one here that is going to be displayed
so this one okay so what I'm doing now
is I'm actually going to use the value
from the previous one and I'm going to
calculate the cumulative sum so I'm
going to use that sum to calculate and
I'm going to change the color so it's a
bit easier to actually see it all right
now obviously that's on a completely
different scale that my previous Green
Line
so that's completely disappeared now we
can also introduce a secondary axis so I
can do that here and say yes do that on
a separate axis so I mean it's not magic
yet but at least we're getting data or
calculating on the data and we can use
this to to do pretty powerful
visualizations now the other thing that
we can do is actually add annotations to
our data so I've cheated conscious of
the time that I've got left so I'm
actually going to move over here and I'm
going to have a look at one that I've
built earlier so I'm going to show you
this one now this essentially shows the
cumulative sum that I did just now as a
line rather than this filled area you
can edit that you can change the type of
charts that set or just by going to the
options tab here so I can do that here
and I can just change this to say fill 0
so that means that's not going to be
anything below the line now the
interesting part here is that I can
actually introduce what are called
annotations now
annotations is information that can help
the user make sense of the data now
these aren't just random lines they're
actually specified by a rule that I've
written so what I've done is I've said
on this index I'm going to query
something and I'm saying I'm saying give
me only the results where there has been
more than three people three or more
people killed so that means essentially
I'm trying to identify the major
accidents where there's been a really
significant car crash with a lot of
people involved so I'm just saying that
number of persons killed should be from
three to star which means anything
including three and above and then I say
I want to show this as an asterisk or an
exclamation triangle on the chart which
you can see here and then I'm also
saying that I would like to have some
text if I mouse over so I'm just saying
the number of business person
people killed in single accident so if a
mouse over this has three people killed
in single accident
June 25th so I mean these things yes we
could obviously do that by programming
but this would take quite a bit of time
for us to actually put that together now
jumping forward if I just put all of
this stuff into a single chart I'm going
to do one more which I think is neat I'm
going to do this from on accident so
what I've done here it's actually a bit
interesting so since we can do more
advanced calculations rather than just
some averages we can also do like the
first order derivation the second order
derivative of a chart we can calculate
how much change there is in our data we
can also then do that again and compare
it against one of the other data series
that we have so let's say we're doing an
average we're doing a moving average and
then we're doing the actual and then
we're going to compare that and see how
far off is it from the moving average
and this is like a very very basic way
of doing a normally detection you can
use this to do data quality checks and
you can use this to essentially see if
if you're doing the performance
monitoring of your servers you can see
if your over-consuming memory if you're
above a threshold below a threshold etc
and you can easily indicate that on the
chart like I've done here so I'm here
I'm just plopping red circles if it's
above a certain rule and the rule I've
specified down here so here I say if my
count is bigger than the moving average
the count - the moving average is bigger
than 200 then it's a significant anomaly
in my data then I want to indicate that
if it's much lower meaning that there
this day there has been really you know
a lack of accidents I would expect more
accidents then that's obviously a green
circle
I can combine these things and make
powerful visualizations now the cool
thing is then if you put that all in
together
come on and then we're going to see that
on a dashboard so in this case I'm just
going to remove some of the filters so
now I can use matrix with just those
averages or some sort whatever you
specify put them on on the dashboard I
can change my time series if I want to I
can obviously zoom in on a portion of
the data everything is obviously updated
and changed accordingly and this you
don't have to be a developer to use this
right this you know you can give to your
business users they can start using it
they can make sense of the data that can
explore the data and I think that's the
key it's our duty as developers to make
data available to our users because
we're the bottleneck in solving problems
in the day-to-day business we're the
bottleneck because we have limited
capacity and we've got our roadmaps and
we've got our sprint priorities and the
business users they don't want to sit
around and wait you know until we have
capacity to prioritize them they want to
work on solving the problems that they
have in the day to day job and I think
this is a great step for us to make the
data available the capability available
and once they have solved some what's
the problem they can come back and have
us automate it so in this case I can
drill down I can say I only want to look
at accidents in Brooklyn see how that
changes I can only look at for instance
bicycles so any sorry those this one so
I'm going to zoom in on that one and
then I'm actually only looking at
accidents where there has been an a
bicycle involved now the interesting
part of that is that you saw the
previous chart if I took away this one
and I took away this one it doesn't
really look like there's any trends here
I can even say let's do the five years
again and then it yes there are some
peaks and frauds here but it doesn't
really make much sense but if I look at
for instance bicycles just bicycle
Wow there's a really really clear trend
obviously this has to do with the
weather and that they don't bike in the
wintertime but you know it is cool that
we can we can we can easily spot these
trends so if you just did a very basic
comparison to a threshold this would be
an anomaly because it's really below the
average of the time series but this
isn't an anomaly
this is a natural seasonality and that's
where this machine learning module comes
into play that they've now introduced
into Kabana caveat is that you have to
have x-pac so you have to pay for the
license but this allows you to actually
do machine learning on things like this
saying that where's the anomaly when
this is actually normal this is the
trend but it's the trend is there data
deviating from the trend so the basic
use of the machine learning has been
tailored towards anomaly detection
they've said that they want to do more
models more advanced machine learning so
it is somewhat limited for those of you
who have worked with more advanced
algorithms but I think it's a very good
starting point so this takes a bit of
time to run so I'm not going to do that
but I'm just going to show you the
results so if I look at this one showing
a number of accidents I'm just going to
show you over the entire time span this
is after I've run the algorithm and it
actually tries to then detect anomalies
and it shows me this is a bit of a
strange one but I can show you them the
biggest or the most significant
anomalies according to the seasonality
according to the data set not just very
simple this - that is wrong or correct
the cool thing is you know I don't know
much about the traffic scene in New York
so I you know well what does this mean
so I did just to first to Google them
just the date and New York and the first
thing that pops up on the first one
there was an eighth of November 2012
snow after Hurricane sandy massive
accidents all over New York the second
one there which has like something like
61 accident in a single day that was
black ice related accident caused 911
not 911 but 911 in New York so these are
major events that you know got in the
New York Times because of their impact
on the city and they're spotted
immediately by the algorithm
traditionally you'd have to go through
all of this data do the digging to find
these things and it wouldn't be enough
to just look at the peak or the trough
so that's I think there's a lot of
promise in this one okay so last part of
my talk I'm going to switch over to
another data set so I'm just going to
exit this and I'm just going to do this
it's the beauty of doing live demos
right I'm just going to start this one
instead and while that boots up I'm
going to move back here okay so over the
past two years I've been working with a
shipping company meaning we have really
really really big boats that move goods
from one part of the world to another
part of the world I don't know much
about shipping I've learned a bit Tim
last year what caught my attention is
that five or six years ago it became
mandatory for big vessels to start
transforming their position via
satellite so every vessel over a certain
size has to transmit their position at
all times the interesting part is that
this data is not private data which I
thought wow that's cool because that
means we can not only look at the
hundred and forty vessels that we
command we can actually start monitoring
the entire world fleet all vessels in
the entire world of any size we can see
where they are where they have been and
then we can use that information to
predict where they're going and as you
can probably imagine even without
knowing anything about shipping there's
obviously a lot of money and power in
knowing what's going to happen in the
future for any industry really now the
cool thing is I've got this data set of
it up on my computer with a snapshot
over the past month or so I'm going to
do the live demo with that data I've
limited the data so that it's really
clean down to the bare bone it's got the
position it's got the speed of the
vessel it's got an identification of the
vessel it does not have the information
that the captain has reported so we
don't know where his set is going or
anything like that we've got to the
heading and we've got the status of the
vessel meaning if it's import if it's
moving etc so it's a exclusively a
numeric data set we're going to see what
we can use that data for so one of the
things that we've been using it for in
the past is to essentially track data
over time so collecting data two and a
half years of an entire world fleet is a
lot of data and then we're trying to see
based on that information can we say
something about where vessels are moving
what route they will take and keep this
in mind today all routing on vessels is
done via a predefined route so they say
this is where you're supposed to go but
then whether an unexpected things happen
and vessel seldomly go where they were
supposed to go they deviate so we're
tracking where they actually went not
where they were supposed to go but then
we can compare so we can use that to
also give better information on when
should we expect the vessel to arrive
and this is critical information to all
of those people that owns the cargo that
is onboard the vessel when is this
vessel going to arrive based on its
current position its current speed when
is it going to arrive so we can use
elastic search to calculate that and the
beauty of elastic search is that we have
all of these filters we have all of this
data about the vessel now the examples
I'm going to show you doesn't have the
enriched version where we can say that
this vessel is of this size it's of this
name etc it's just anonymous data but in
our data set we've combined that so we
know how big the vessel is we know the
carrying capacity of the vessel we know
a lot of data about the vessels and we
can use that in addition to saying
my vessel is in position p5 give me all
other vessels who's ever been in this
position give me all of those vessels
that have come from the same place where
I came from
so take one step back in time and look
at p4 now we've filtered out quite a lot
of vessels then we can add other filters
they I'll only want to look at those
vessels that have this speed I only want
to look at vessels of this type of this
size etc to narrow down our data set and
then calculate the distance the time
etcetera it is really fascinating and
and then you can zoom down obviously
because this was like at a very very
high level counting the number of
vessels in different areas we can also
drill down really really down to
information about the ports because
there's a lot of costs associated with
what's happening in port there's waiting
times and that cost money so last two
months ago we had two vessels and this
is probably boring for you but anyways I
thought it was interesting two months
ago we had two vessels in Brazil they
were going up in the Amazonas River
crazy how far they can actually go with
these big vessels but they ended up with
a waiting time so they couldn't do what
they were supposed to do and then the
cargo owner or the receiver has to pay
for the waiting time for a vessel of
that size they pay something like twenty
thousand dollars per day we had waiting
time of 45 days 45 days now being able
to optimize and know and predict that
this is going to happen you might as
well slow down spend half the amount of
fuel which is going to cost you much
less you know these kind of things
optimizing this just based on a is data
and using elasticsearch which I think
it's really really cool and you can
drill even further down and look inside
the harbor look at what birth which is
it's called is this vessel versus that
vessel where's the waiting time what's
the loading time on this berth
the with the crew that works on that
berth versus this one a lot of
information that we can actually extract
from from there
so I'm just going to show you now the
last minutes that I have left I do have
a few minutes left I think even though
they're applauding in the other host so
I'm just going to jump in here and I'm
going to have a look here yeah okay
so let's do have a look at this okay so
here I've got my location data I've also
got another index called port so this is
just a port database meaning that it's a
database consisting of all the different
ports in the world that we care about
and then within those we've got the
position we've got a polygon which is a
shape saying this is actually the port
and within that we can actually have
polygons for each of these birds so that
we can actually say I want to look at
berth number five in this port now I can
use that and I can easily visualize that
on the map so I'm just going to show you
quickly how we do that so we can do a
coordinate map which is something we
have in Cabana and I'm going to do the
port index and I'm going to do geo
coordinates and you can obviously write
a JSON query for this as well and I'm
going to select the geolocation field
going to click play now I got you know a
grouping of ports around the world you
see some of them are bigger meaning that
there are more ports in that area I can
zoom in and look at the specific area
and let's do one of these where there's
more than one so let's do down here then
you see that I've got several of these
ports around here problem is can't
really tell what basis this is just like
yeah here's there there are two and
here's one so if I do another
visualization called
a data table which we have here I can do
the same thing I do port index and then
I'm going going to do a terms
aggregation on the CEO
port name now essentially this is just
going to give me a list of ports now
five in this case but the cool thing is
if I put them together in a dashboard so
I do pork dashboard like this I've
pre-built this and just switched off
these ones I'm going to delete this now
I've got one map here which shows me the
positions I've got another map which is
new in 6.0 which allows me to actually
do the country codes this is like a
convenience map really give I've stored
the country codes of the map of the
ports saying what countries does this
port along to and then it will plot that
on a map so saying that okay I've got
154 ports in China
I've got 147 ports in the US I've got 74
in Australia I don't care about the rest
of the world only one look at Australia
I can do like that and then I limit my
view can click here and it will jump to
Australia so it's quite easy to actually
navigate the data now this wasn't really
easy before in Cabana but now I can
combine these different visualizations
to navigate my data I can then zoom in
and I can say well I only care about
this part of Australia so I'm going to
zoom in on that and then I see the list
now I really want to go down and look at
this one and see what is this port here
oh that's Kwinana it's one of the ports
there now this is just like a database
so this yes you could do this in power
bi or whatever but the beauty here is
that we have two indexes in
elasticsearch we can have as many as we
want obviously and in Cubana we can
actually query across the different
indexes so what I want to do is just
show you how I've actually combined my
location data my a is data and I've put
it into one dashboard where I look at
both the port data and the location data
so here I'm actually just going to
remove this query for a while and then
have a look at that
now here I'm actually looking at all the
different vessel positions so that's a
pretty noisy map here I'm still looking
at the same port map that I was looking
at earlier if I want to one common use
case is to say show me all the vessels
that are in this particular area so I
can do that by just drawing up a polygon
so I can say I want to give me all the
vessels that are somewhere in Brazil I'm
doing obviously this very rough like
that so then that limits it down to all
vessels who's in my in my data set one
month has ever been in Brazil now
vessels move obviously so this gives me
the vessels that has ever been there I
don't know when I don't know you know
the timing unless I go into the data but
we can use that for it and I'm just
going to make it a bit more explicit so
I'm going to jump down into one of these
ports let's do one of these in Australia
so I'm just going to assume down here
and I'm going to zoom in here and I'm
going to do yeah I'll do this Kwinana
one and then I'm going to draw I'm going
to crash the whole computer okay let's
do like that okay so I've limited that I
have to remove my other one now I'm
getting those vessels that's ten vessels
have not too many vessels now the beauty
is I've got a data table here showing me
the ID of the vessels so I can actually
scroll down to my table here and I'm
going to just export that table now I'm
cheating a little bit but I'm just doing
that in Excel okay so I'm just going to
since I have so little time I'm just
going to blast through this bit tedious
but okay let's do like that I've got a
list of 10 I knows I could do like the
193 that I saw earlier just going to do
it for these ones I'm going to list just
copying it from excel going to go back
up here and I'm going to say IMO and
then I'm going to do brackets and I'm
just going to paste it now what happens
now is that are going to remove the
polygon that just constrained me to that
port that's going to show me all vessels
who's been in that port and where did
they go for the rest of the data set
which is quite strategic information
that we can use so now I actually see
where they went which is really really
cool now the port map disappeared
because obviously I've limited them I'm
oh now there's a quick and easy fix for
that because I can cheat and I can say
there's two different indexes I'm just
going to say or port name star then I'm
going to get that it's not going to
affect my location but I can still see
the port names so that that way I can
sort of go down here zoom in and look at
okay where did this vessel go well
actually it went down here
well what port is in this area well it's
Rich's Bay
unfortunately I've spent my time I have
time for some questions
yes yes so you can share now with x-pac
you have user access as well so you can
actually limit you can say you have a
read user just to this index to this
type etc so that would require that you
login if you have x-pac if you are not
concerned about sharing it publicly
which I would never do to be honest with
you if at least if it's a production
system never do that because it's easy
to crash the whole index if you're
misusing gabbana but yes you can share
it you can embed it as well and you can
share just a module so I could go into
just this one and if I click on edit
here I can actually go on this one and
share just this visualization so I can
do that one and I can share it here
either as an iframe as a link I can
share snapshot which means that you're
just giving them the data dump and not
the live data so you have it they've
come a long way in this aspect because
you have much more granular user access
with x-pac which you don't have in
standard Cabana
yeah yeah sure but yeah absolutely
I mean if you want to share the data and
you don't have any concerns about
sharing it then it's fine the only
concern that I would take into account
is that if I do a bit of an advanced
query here I can crash the whole system
so if I can do that then an external
user can do that as well and that's not
just crashing it for my session it's
actually crashing elasticsearch so I can
do that right now if if we wanted to
what I do is actually say no limitations
no filters and then just scroll down and
then do the calculation down here on
this level now it's going to try to plot
and actually do the aggregation at a
level that is so granular that my
computer is most likely going to run out
of memory if I can do that then anyone
can do that right so that's my only
concern by essentially giving like a
full access to anyone but you know
publicly so actually it survived but
yeah okay yeah sure any other questions
no one come on must be some questions
well okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>