<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Cleansing With SQL And R - Kevin Feasel | Coder Coacher - Coaching Coders</title><meta content="Data Cleansing With SQL And R - Kevin Feasel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Cleansing With SQL And R - Kevin Feasel</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qGqAhlFzkSU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's get started we are on day one
right after lunch carb load has hit us
and my general rule is no more than half
the audience is allowed to be asleep at
one point in time so if you feel
yourself nodding off just kind of nudge
the person on the right again so we're
gonna talk about data cleansing using
both sequel server and our my name is
Kevin feasel I'm a manager of a
predictive analytics team in Durham
North Carolina also I'm a data platform
in VP and I have a blog curated sequel
the idea behind curated sequel is that I
want to try to find and link to five to
ten interesting blog posts per day so
interesting here is database development
data based Administration could be
security power bi our Hadoop
this broad data platform space curated
SQL comm the general concept that I want
to hit today is what is dirty data how
can we go about trying to fix some of it
there are a lot of different definitions
of what dirty data is different concepts
in this space and I want to take a
couple of minutes and talk about them in
some procession before I get into that
detail let's talk about my general
philosophy clean the data as soon as
possible I mean this sounds kind of
anodyne this sounds like well obviously
this is the case but we have data come
in and it goes into a transactional
system maybe or comes into some source
of record and then if it's bad in there
it's probably gonna be bad downstream as
well so maybe we can try to fix it in a
warehouse it's like the movie equivalent
of we'll fix it in post production
sometimes that works sometimes though
you don't exactly know what the correct
answer is the correct answer might have
been known at the beginning but we lost
that ability when we accepted the bad
data occasionally we're able to clean
things up down at the end when we're
generating reports when we're doing
analytics that's pretty interesting and
I'll show you some tools to help you out
in this regard but as a general
principle this is not the best place to
do it because my analysis will be
different from your analysis because the
way that I cleanse the data will be a
little different from the way that you
do unless we follow exactly the same
rules we may get different results when
to do the same thing so try to fix it as
soon as possible and as mentioned we're
going to be looking at different
techniques across sequel server and are
there are plenty of tools and other
techniques that I wish I could get into
but we're not going to have the time for
that and that includes things like data
provenance tools data quality services
is a mediocre to bad attempt at a
solution but it exists there are much
better solutions available as well so
we'll talk first about high-level
concepts and then hit some sequel server
stuff item number three I'm going to
leave as a bonus for you because bonus
means I won't have time to cover it
because I want to cover are in some
detail so let's start talking high-level
at the basic concept of dirty data think
about you have a title and that could be
a mr. or mrs. MS miss dr. Reverend
captain whatever your titles may be now
suppose that is a free-form text field
so anybody can type something in so you
may have Mr dot you may have Mis ter you
may have mr without the dot you may have
mer furrow which is my attempt at trying
to type mr on my cell phone and all of
those things are supposed to represent
the same concept but when we pull it in
and try to group by that title we're
going to get a whole bunch of different
answers so that's one example of dirty
data this is a data consistency problem
we have other problems like is your data
physically or logically impossible if
you have a six-year-old with a driver's
license then that's a very interesting
place to live in probably that's just
bad data do you have data that you
expect to be complete but isn't maybe
you have a lot of nulls one example of
this might be that I have a sales a suit
a group of sales associates they have
you fill out a form and well you as a
consumer say I just want to consume this
thing I don't want to spend the rest of
my life filling out a fifteen page
document telling you everything
from my blood type twos and the name and
numbers of my dogs so I'm gonna leave a
lot of this stuff blank so then the
sales associate dutifully enters nothing
into the form and then you try to
analyze and you have a bunch of empty
results so the question would be do we
have some sort of reasonable coding that
we can do for a blank value or do we
have to throw these rows away can we do
something with this and are these fields
even vital for analysis if they're not
why are you asking me what my blood type
is if you're not gonna do anything with
it the answer to sell it to somebody
else so data accuracy I wish that I
could make five hundred million dollars
an hour spoilers I don't I don't know of
anybody who does that if I see it on a
form is an absurd answer it is patently
wrong what is the correct answer I don't
know maybe it's $500 an hour maybe it's
$50 an hour maybe it's 500 million
Zimbabwe dollars that doesn't sound
nearly as nice as 500 million Australian
or US dollars but yeah this comes down
to what do we do when something is just
completely wrong or a little bit less
absurd of example we have two source
systems and those two source systems
should agree but sometimes source system
one says the value is 14 source system 2
says that it's 18 how do we determine
which of those two values if either is
actually correct how do I figure out if
I have duplicate results let's say we
have a form somebody hit the submit
button and got really impatient and hit
it five more times so now we have six
rows in there or alternatively maybe we
had six people enter the exact same data
and hit the submit button and we have no
way of differentiating so is this truly
duplicate data is there some absolute
uniqueness and if so you know are we
taking advantage of that this leads to
some rules of thumb if you've got
impossible measures throw them away
this is obviously bad data if you have
data that's missing you know
all values I keep it because it's
missing but that doesn't mean it's bad
and also may be interesting to see only
2% of people are filling out their blood
type what's up with those 2% who
actually are if you have data that you
know is fixable like those various forms
of mr. probably should fix those because
you know what the answer was
you know what the intended answer was
and just because somebody didn't get
quite to that intended answer doesn't
mean that our analyses down the stream
should fail as a result but that five
hundred million dollars an hour that's a
tougher call I don't think you can just
say well let's change it to some other
value because you don't know what that
other value is if you absolutely have to
change it typically a safe thing to do
during a data analysis is I'll set it to
the median of all values in the set that
way I don't affect the aggregates but if
you have a bunch of values you're doing
this to this can lead to distortions of
its own so you may end up dropping those
records from the analysis so those are
principles I'd like you to keep in mind
through through this talk and we'll look
at ways to try to clean up some of these
problems using both sequel server and
are the biggest most interesting way to
do it in sequel server is something that
probably you're already doing and if so
bully for you
keys constraints ways of limiting the
data that can get into a system
normalization is the first constraint
it's a logical constraints not a
physical one and there are several ways
to normalize a set of tables to get to
an answer but this is probably the first
gate in order to protect bad data from
getting through
then we talk about data types and the
actual physical constraints that exist
in a relational database system so with
normalization when in doubt go up with
Boyce Codd boyce codd is a general form
of third normal form there are higher
forms than third and I love me some
fourth and
normal form but that's not always
important for most applications it pains
me I'm having to force myself to say
these words because I am such a
normalization aficionado I love the I
love the concept so much and yet I could
say it's not always that important to go
up to fifth normal form that was
actually painful for me but I have a
link in case you're interested in how to
find a really cool way of deriving Boyce
Codd normal form from a gigantic table
so you have all of your attributes
together and then this will get you out
into the set of a set of tables that
will work fine for Boyce Codd it's a
really interesting solution so I'll show
you the link at the end and also give
you a quick example of it when we get
into the demo as far as data types go
smallest data type you possibly can use
tightest data type you possibly can use
so in other words this is numeric values
must be greater than zero and let's say
between 0 and 100 is our domain well I
probably don't want to use of our char
max because that's gonna let anybody
type in whatever they want and there
will be some troll who will come in and
type in the word 70 instead of the
number 7-0
I'm usually that troll something to
think about
Titus domain you don't necessarily need
an 8-byte big int for a 0 to 100 value
you could use a one-bite tinyint that
has some benefits downstream for the
database administrator who won't be
chewing you out quite as much about why
are you using up all of my disk space I
can't just don't grow on trees you know
I have to go buy these things thankfully
I'm no longer that crotchety database
administrator I'm still crotchety I'm
just no longer a database administrator
so smallest data type best data type
when it comes to constraints primary key
constraints tell you this is a unique
row this is the thing that tells me that
this is a new row a neat key constraint
is another way of saying this is a
unique row there are some minor
differences in how
relational systems implement primary vs.
unique key constraints let's gloss over
that stuff basically every one of the
sets of columns that define a unique row
should get their own key constraint pick
one for the primary all the rest will
get uniques we use foreign keys to tie
together different tables that way if
you take over a data model that has 500
tables and has 0 foreign keys you go
insane because there's a very difficult
way of trying to reconcile how these
things actually relate foreign key
constraints let us get that easy
reconciliation but they also provide
data cleansing in the sense that if I
have a lookup value mr. mrs. MS miss I
can choose that lookup value and I can
ensure that this value exists in my
table so there's no way that I can get
an invalid value in my table the foreign
key constraint will actively prohibit it
check in default constraints are ways of
tightening down our domain even further
so a check constraint might say well you
can only go from 0 to 100 a tinyint has
a domain of 0 to 255 but I can put in a
check constraint that says that if
you're over 100 then this is not valid
this is an invalid element default
constraints say I don't need to accept a
null I can accept some value that will
default to maybe a true or false maybe
it could default to some specific number
or some specific text all right let's
talk demo so I have a database in here
that is called Raleigh crime this is a
set of crime data for Raleigh Durham or
excuse me for the City of Raleigh in
North Carolina from the Year 2005
through 2014 and I have this database
already created and I've got a table
loaded called police so this police
table came from a CSV data set that had
columns that were really helpful II
named like LCR and Inc know so we're
gonna work on that a little bit
we're going to normalize these tables
we're going to create some constraints
we're going to give slightly better
names because I like naming things and
we're going to try to make this overall
just a better data set so let's top 100
from DB up police and we'll see a quick
idea of what this data set looks like
LCR and LCR desk it turns out this is
the incident code and this is a
description of that incident code now in
this data set every time you see this
particular incident code you are always
going to see this particular incident
description this is what is known as a
functional dependency so my description
is functionally dependent upon a code
these functional dependencies define how
we normalize tables using Boyce Codd
normal form because what it says is if I
have a functional dependency like I have
on LC are pointing to I'll see our desk
or see an other way around then I need
to make sure that LCR is a primary or a
unique key on this table
well LCR cannot be a primary or unique
key on the table because we have
duplicates so I have to break that out
into its own table this does not belong
in the big table police so I'm going to
give it a new table that will have my
key column the LCR and it will have my
functionally dependent columns the
description here I'll break that out it
will become a new lookup table and I
repeat this for each functional
dependency that's in this data set and
that in about 50 words is how you get to
Boyce Codd normal form again the link
that I have will show you a video where
you walk through it in a little bit more
detail it's kind of an interesting
concept aside from that we have a few
other data types incident date/time this
is a string that represents a date/time
value so probably should make that a
date in a time the beat
this is where and a set of officers
patrols in a specific area during a
specifics at a time we don't have any
additional information on this beat but
I could see this being a lookup table of
its own we can pretend that the Raleigh
the Wake County Police Department is
going to send us this data in a couple
of weeks but we don't have it today
incident number is a unique identifier
for a particular incident
now this follows the first rule of data
type naming which is that if you have
number in the name it is never numeric
so all right we got to deal with its
text that's okay I'll live with it
and location is geo located notice that
it has a precision of some ridiculous
number after the decimal in reality this
event did not happen exactly at this
location this would be approximately a
molecule given how far out we're at the
police know exactly which molecule the
bullet hit in reality with what they're
doing is more of let's say the incident
is here they're going to build a an area
and they're going to pick some arbitrary
point in that area that way your
analysis is still accurate on net but it
protects the privacy of everybody
involved
so all right that's a lot of words let's
let's start doing this stuff I have a
table incident code and this is what I
was talking about this was the LCR so
these are really incident codes and I've
given them type so incident code has a
varchar' v and incident description of
our char 55 that's the maximum length of
an incident description in my data set I
want to create a primary key constraint
so that I can tie it later to a foreign
key and I'll insert all of my
descriptions from the police table and
so I've already created these two tables
so I don't have to run them here save me
about a minute I also create an incident
table here I've created a table with a
surrogate key there's a whole debate
about whether surrogate keys are
necessary important wise or foolishly
terrible ideas I'm not going to get in
that discussion
today I'm okay with them I'm also okay
without them but we've created one so
this is my surrogate key and just years
and years of this drilling being drilled
into my head has me create that as the
primary key and ends up being a
clustered index I also have my incident
code but I took off incident description
that lives in its own table now so it no
longer exists in my BigTable I renamed
incident date beat ID changed incident
number to a varchar' and made incident
location of geography I have a unique
key on this incident number whoa I have
any key on this incident number and I've
defined that down here I have a foreign
key on incident code and then I've put a
check constraint on beat ID I know that
the beat must be a positive value so
beat must be greater than or equal to
zero I also know that my incident date
must be within a certain range because I
pulled this data in the year 2014 I know
that there shouldn't be any values that
are later than 2014 because if there
were then we've got some side of a
Minority Report precognition police
thing going on and as far as I know
Raleigh doesn't hire any of those police
officers so I insert data into the
incident table from dbo police let's
start polling I'm using tryparse
which was introduced in sequel Server
2012 this will allow me to specify a
particular date format and say you know
what you figure this out I'm gonna tell
you that the people who put this in
we're using us English as their as their
locale and I want you to give me a date
time that matches up with it so going
back to this table might notice that
these are day month or excuse me month
day year instead of day month year so if
I change this to UK or Australian
English then I'm going to get some
different results but because I know
that this was US English then I specify
and I get back the appropriate values a
date time my geography I'm using a CLR
function that's been around since
sequel's
2008 to convert a string into a point a
geographic point so latitude and
longitude pair I run that and I now have
a couple of tables so there's this
incident table and this incident code a
table the incident code table shouldn't
shock you is a couple hundred records
and just shows you different incident
codes ignore this for now we're going to
talk about it in a moment so we have the
different incident codes if I come back
to the incident table
I have incident IDs and the Associated
code we've got the date the beat the
incident number and location notice that
location is now binary it's because
geography types are saved as binary and
then I can use other functions to
convert them back into point values this
means is I can also say show me all the
values within a certain circle or show
me all the values that are relatively
near to this point so I've opened up the
things that I can do with this data so
we skip this and start going into
mapping tables which is what I was
hiding just a moment ago see ya the
problem sometimes with a data set is I
want to analyze data at a higher a
coarser grain than may exist a
transactional system in the police
example I may want to take all of my
types of a particular incident and
convert them into a more generic form so
we'll see that there are cases where
there are a bunch of different types of
forgery I don't necessarily care about
that for my analysis I just want to know
is this forged is this forgery or not if
it is forgery let's assign it to a
forgery type so let's do that now I have
a note in my text that you know
sometimes I'm not able to modify the
table itself sometimes I may have to
create another table and create a minute
of any relationship with that table
where I'll create an incident type table
and an incident code table
an incident code incident type even
though I know that there's only one type
her code so sometimes you have to do
that because the business side says no
we can't make changes to the original
data set or you know it's a third party
vendor tool where you're not allowed to
make any changes to it in this case I
talked to the product owner who happened
to be me and I explained to the product
owner that this is a really important
thing and the product owner said okay go
ahead and do it so I was able to add an
incident type ID so my incident codes
are going to have a type and I've got a
type table notice here that yeah I did
use a surrogate in this case I didn't
need to I could have just made an
incident type my primary key is on the
ID my unique key is on the type so every
type is unique and then I'm going to
create a foreign key constraint going
back to incident code that says every
record and incident code that has a
value better show up in this incident
type table if it doesn't show up in the
incident type table then it's invalid
now I have a whole bunch of rules for
mapping and don't even worry about
reading this I'm gonna show you
something even better spoilers so let's
create a table incident type and I'm
going to switch over to use the Raleigh
crime data set I'm going to insert into
the incident type table this let's let's
walk through this because I love a cross
apply but it's kind of unfamiliar to a
lot of people from the incident code
table I'm going to use the cross apply
function to chain a bunch of replace
statements think of the cross apply
operation as for each record go do
something so for a developer this makes
a lot of sense intuitively and for a
database administrator or a database
developer this performs well because
it's a set based operation it's one of
those cases where that procedural step
by step
and the set-based do everything at once
turn out to work quite nicely together
so my first case I'm going to replace
the incident description and replace
every statement that has a space and a W
/ with a minus W / let's go take a quick
look at what that means as I mentioned
there are a bunch of cases where I have
forgery or there are a lot of different
types of fraud
there's flim-flam and then there's
identity flecked flim-flam I to this day
I still don't know exactly what
flim-flam is and I don't want to look it
up because that would ruin my vision of
what flim-flam is but I don't care that
this is flim-flam with identity theft I
just it's fraud so I want to try to
basically take the first
interesting-looking word in the
description and slice it out so forgery
all of these things are forgery all of
these things are fraud and so on to do
this let's replace any space W with a -
W murder
embezzlement assault larceny and rape
tend to have are the only cases that
have space and then another word so I
replace that space with a slash then I
now have a an incident description and
by the way you'll see that I'm I'm
chaining this so that like ID W is what
I create here and so I look at ID w
sense in description and call that IDM
and I look at ID Em's incident
description I call that IDE and so on
when I'm done I've got a final incident
description and I'm going to find the
first case of a slash of a - of a and
open parens and I'm going to assign
those as values first / first - first
print then here I have a case where I
say
all right I know that char index work
will return 0 if that value does not
exist so if my incident description does
not have an open parenthesis it will
return 0 I don't want it to return 0
because I want to get the minimum value
so I want to find the first of these
characters so let's just put that as a
sentinel value way out they at the end I
know I'm never gonna hit 999 because my
incident description length is 55
characters so I'm never gonna get there
until eventually it gets 1,024
characters and I get there but those are
problems for another day so I've got 999
if it doesn't exist and I've got the
actual character value if it does exist
so I've got that that point in the array
so to speak then quick check what's the
first of these deciding characters I'm
gonna take that first deciding character
and I'm going to get the substring from
incident description starting at Point 1
up to but not including that deciding
character and il trim in our trim or if
you're really fancy and you're using
sequel server 2017 I trim that right
there just save me a bunch of characters
316 rows affected but notice that a
intellisense doesn't work for it in the
version of management studio that I have
which is 2016 so if you want to tell us
since to work with trim function you
have to download 2017 s management
studio ok so now we have incident types
and I want to get a dense ranking of
them what that does is it will give me a
unique number for each one of these
incident types and by making it a dense
rank we go from 1 to 2 and then the next
value will be a 3 and so on otherwise it
would have gone 1 to 9 on and on dense
rank is not necessary for this it just
it makes it look a little bit nicer I
can easily see when the next value comes
up so 316 rows
I can then insert all these values into
the incident type table so I've got all
of these incident types 1 2 3 and so on
and through this I update incident code
set the incident type ID based off of
that incident type I've been able to
take this data set winnow down to the
specific rules that I want create a new
table based off of that and then update
the existing table to point to that
higher-level type I'm gonna use that
type later on in analysis so that I can
see all the cases of fraud all the cases
of murder all the cases of embezzlement
so let's go into the are section and
we've looked at handling some of this
work with sequel server now let's go
pick out another fun language are this
section flows from a vignette that had
Lee Wickham put together Wickham is one
of the key players in the our space and
he his work structuring data sets to
facilitate analysis despite having a
very academic title is a really good
read
basically he starts from the premise
that look you've got data sets and data
data sets are made up of variables and
observations a variable in a data set is
a column an observation in a data set is
a row in relational database terminology
variables are attributes and
observations are entities all the same
thing variables he goes a little bit
further and defines down variables a
little bit more and says that these
actually should measure the same
underlying thing so let's say you have a
variable called height every observation
that thing should be a height it should
not be a height for one person and then
a favorite color for another person and
then a tail length if it's a dog in this
data set instead of a person it should
all be the same thing
observations should contain the same set
of variables for each element in the set
so in other words let's say we have a
set of hospital visits every one of
these things is a hospital visit and it
has the same set of variables we have
the same structure the reason that we do
this is because it's a lot easier to
describe relationships between variables
in other words to build those functional
dependencies that I just glanced over
age is a function of when you were born
and what today's date is we can tease
that out a lot more easily if if today's
date and date of birth are two variables
instead of two observations by contrast
it's a lot easier to group data to
aggregate data across observations than
it is across variables our tooling is
built for grouping observations so how
many people are using this phone number
how many people came in on the 19th with
a case of they're presenting for
tuberculosis we can get these numbers
pretty easily in either a sequel
language or in our and what Wickham ends
up saying is look tidy data is basically
third normal form you take your data you
normalize it I prefer Boyce Codd but you
normalize your data and then he says as
a practical matter you probably are
going to denormalize the data back in
for that art analysis but at least at
one point you know this data was
normalized so he has a series of
libraries that are designed to solve
these problems around data cleansing the
first one that I'm going to look at is
tied er this is just some simple
functions to tidy up data and let's take
a look I've got some Jupiter notebooks
notebooks are also fully available the
thing that I like about these notebooks
is that you can take them and if you
have Jupiter installed you can run them
yourself so I'm going to start
actually let me quickly talk about what
I did up here I am saying if you don't
have this package tidy verse then go
install it and load it if you do have it
already in memory cool I don't I don't
need you to do anything and I do the
same thing with the ggplot let's bump
this up a little bit more so tidy verse
is a whole bunch of different libraries
that are all designed to work together
to cleanse data tidy our is just one of
them and we're gonna look at a couple
more as well
ggplot2 is a way of mapping data it's a
way of displaying graphs it's a way of
visualizing data I run them and I get a
bunch of warning messages that say
nothing of importance to us and then we
start looking at our first data set
alright so this is a data set that was a
Pew survey of people and this is what
they presented to people we have one
variable here it's very clearly a
variable and it's a religion we have
another variable this spread out a bunch
of among a bunch of columns and that is
income bracket and the numbers in
between these are the numbers of people
who decided to answer a particular way
on religion and income bracket the
problem is that if we want to do any
type of analysis as opposed to just
looking at the chart if we want to
analyze this at all what we really want
is for this to be its own variable so
that we would have a religion variable
and an income bracket variable and then
we would have a measure that would tell
us how many people answered this
religion this income bracket and we can
use tidy R to do that in just a few
lines of code so I'm going to read the
data from this CSV that I have
everything eventually comes down to CSV
s and I am taking the data and I'm using
a pipe operator so this is the pipe
operator and our percent angle bracket
percent what that is saying is take this
data frame
this data set and send it to the next
function the next function here is
called gather gather act as an unpaid
operation so what I'm doing is I'm
saying I want you to gather and create a
new column called income and that income
is going to have all of the names of the
income brackets I want a new column
that's called frequency and that's going
to be the the inner guts of the cross
product of this religion and an income
bracket all of the columns that are
needed for income are then defined
afterwards and I have a - before
religion basically says every column
except for religion is actually part of
income so I can use - I can also list
the columns specifically and I do that
and now I have the data that is unfitted
so I have frequencies that I have
religions and I have incomes and now I
can use a tool like ggplot and in a
half-dozen lines of code what I can do
is create a really ugly looking graph in
about four more lines of code I could
make it a much less ugly looking graph
but all I'm doing here is I'm saying
okay take this data set that you have
now and then apply it basically apply
that data set and say the x-axis is
religion the y-axis is the number of
participants who accept who said this
and the coloration is income bracket I
could also lay it out as a heat map I
could also lay it out as some other
visualization format the rest of this is
commentary the rest of this is just
adding things like labels and titles so
that's unfitting data it's a pretty
simple example I want to get to a
slightly more complex example and here's
a data set this is tuberculosis data I
have a couple of variables here and that
I have these things
this is males between the age of 5 and
14 males between the age of 35 and 44
females between the age of 0 and 14 what
I have here is either two or three
separate variables for today I'm gonna
treat it as two variables male or female
and age range and then I have some
number of cases notice that a lot of
these have n/a we don't have any samples
for this particular sex and age range
whereas here we do have a sample and
there were zero values so those are
important to consider all of these n A's
I don't have those samples we can throw
them out there only
they were only needed to pad out this
grid but we're not going to keep the
grid we're going to unfit the data break
it out and then get our measures
appropriately so that's what we do
starting here I take my tuberculosis
data and I pipe it together so again we
unfit the data we take all of those
values we say I want to create a new
variable called demo demographic data I
want to create a new measure called in
number of participants then I take the
set of columns that will make up
demographic and that is everything
except for ISO 2 and year and by the way
let's remove all the n/a values let's
remove all of those null values because
they don't really exist
they were just padding after that I used
the separate function to crack out the
particular demo value into two separate
columns so I do that and I end up by the
way I should probably note that the one
parameter here separate after the first
character so I know that there is one
character and I can break it out into 0
to 4 and as we keep going down we have
females who are
undefined and that's at the end of the
data set so overall this was a little
bit over a hundred and fifteen thousand
observations and I can take those
observations and I can plug them into a
model
maybe I say I think that the number of
cases of tuberculosis is dependent upon
it's a linear function related to year
and whether you're male or female and
your age range and I can run this and I
can scroll down and see all these
asterisks which say that yes these this
is a really tiny p-value and I'm going
out and I'm gonna publish this paper and
I'm gonna get rich because that's what
Pat that's how you do it in academia I
think until somebody goes and looks and
says you know that that explains about
2% of your variability right and I say
yeah but I got p-values prolonged sigh
and they're on purpose so let's let's
move on from my horrible horrible
academic career and talk about eav by
horrible horrible professional career
sometimes datasets come in as entity
attribute value combinations so what we
have is we have the entity itself we
have an attribute and we have a value
associated with it in this case we have
kind of a real kicker because you know
first of all you'll never guess how many
number of these there are because this
is this is de values this is like day 1
day 2 day 3 we're looking at weather
station data and these are the minimum
and maximum temperatures for weather
stations in some locations in Mexico
over a multi-month period so we have 31
of these that are all really supposed to
be the same thing it's the day of the
month we have min and Max and notice
that for day two the men and the max are
in the same column the problem here is
that if I want to do some sort of
aggregate
let's say I want to get the average max
in this data set I cannot simply take
the average of day two I have to take
the average of day two where element is
T Max and that's a lot more effort than
it should be so let's fix that up I am
going to take my data set and in just a
few lines we're gonna crack this thing
we start by taking the original data and
piping it to gather so we run pivoting
now previously I'd showed you unfitting
by taking every value except for the
ones that we named here I'm naming the
values specifically I'm saying every
value between D 1 and D 31 in my data
set I want you to take all of those
things and unfit them turn them into a
column that column will be called day
and the numeric measure that was there
will be called value then I use another
element of the tidy verse read R to
parse the number out from that day
basically get rid of the D so that's
gonna be called day and I mutate so I'm
modifying the data set I'm modifying
that data frame and changing a d1 to 1
and making it numeric then I want to
select only the relevant fields so I
used the Select function then I want to
sort them then I want to spread which is
pivoting so I have a min value and a max
value and I can spread them and turn
them into two separate columns so let's
do that here I take the head of that
weather set and now what I have is four
explanatory variables which weather
station what year what month what day
and then to aggregate able measures max
temperature and min temperature and now
I can plot the max temperatures and I
can see the range and this is by day of
month so the maximum
temperature the fourth day of the month
was just over 24 degrees Celsius and
then we go up to the top where I was
about 37 somewhere on the 26th or so of
the month so that was tidy our let's go
talk about the next set of the tidy
verse which is deep liar see that tidy R
was just one little piece I also showed
you a little bit of deep liar which
we'll see I also showed you just a
little bit of radar which we got to see
as parsing numbers we also we won't be
able to see some of the other elements
of the tidy verse like lubra date which
is a date function so this will allow
you to convert strings to dates really
easily there's also other libraries like
purr which works really well with api's
but all of these things are in that tidy
verse library so let's talk about deep
lyre
once again I'm loading my tidy verse
data and I also have this data set
called Gapminder Gapminder is a data set
that looks at data from the year 1958
I believe 1952 to 2007 and it will
include the population of a country and
also the GDP of that country in each of
the five year periods so we sample every
five years once I load the data so I've
loaded that package already oh I hope I
didn't mess myself up good okay I'm not
on the internet right now so it's
probably it would have choked if it had
to go look anything up anyhow I can use
the filter function tact as sort of like
a where clause filter Gapminder where
the life expectancy is less than 30 and
we can see that in two cases Afghanistan
in 1952 and Rwanda in 1992 the life
expectancy was less than 30 years let's
go the opposite way filter where life
expectancy was greater than eighty one
and a half and here we have a couple of
Asian countries and a couple of European
countries all in the most recent data
sets
and Japan was in 2002 as well so filter
acts like a where clause now there is no
and or or but you can get that same
logic by saying comma is an and a pipe
is an or so let's ask for countries
where the population is at least twenty
million and the GDP per capita is at
least thirty six thousand USD we at
Canada in the United States let's look
at where population is 1.2 billion or
GDP per capita is over a hundred
thousand USD this is all purchasing
power parity normalized for I think two
thousand seven dollars but that's beyond
the scope of this discussion anyhow 1.2
billion we have one country in the list
GDP per capita over a hundred thousand
we have one country in the list but
we're able to pipe that together and get
two separate data sets combined together
the Select function let me pick certain
columns from my data set so from
Gapminder give me your life expectancy
and it only shows you those two columns
so select works that way I showed you
the pipe operator already and I can use
the pipe operator to say filter out any
country that has a population under 1.2
billion so throw those away and then
give me the country your life expectancy
of China basically and we see the life
expectancy during that point in time i
I've used mutate already to good effect
but as a quick reminder mutate allows
you to modify a data frame add to add
columns or to change columns here I'm
adding a new column called GDP it is
equal to population multiplied by GDP
per capita and when I do that I now have
a GDP of this as a quick note
Afghanistan in 1952 that's about six
billion dollars that means that Bill
Gates and Jeff Bezos could have bought
and sold Afghanistan between them about
what thirty six times
more or less a strange game they have
but whatever I can also arrange datasets
sorting so I've arranged by GDP the
lowest GDP turns out to be sell Thome
and grab the top ten using the head
function if I want a sort in descending
order
I use the de SC function descending
function and I can see that the top 10
per capita China and Japan show up here
on this set so continuing along new gap
I want to group it by a continent just
like in T sequel I have a group by
function there I have a group by
function here then I have a summarize
function that gives me a measure so in
is just a count so let give me the count
of cases by continent and Africa has 624
this is not surprising because there are
more African countries than there are in
other places of the world and Oceania is
down here in 24 that is Australia in New
Zealand at least in this data set again
I'm not going to get into details on how
we label data I want to group by
continent and I want to summarize by the
average life expectancy across the
entire data set so I use the mean
function to get that I can also get the
median life expectancy or any other
aggregation function that I that I could
think of Australia New Zealand combined
to have the highest average life
expectancy in the overall data set I can
then say you know I don't think it's
that fair to talk about life
expectancies in 1952 versus 2007 maybe
we want to filter them out and look at
slices in time because that's more
meaningful
during the slice of time we see average
life expectancies per continent and then
in 2007 you can see that Africa had
jumped about 15 years Oceania had jumped
about 11 years so there was some
catching up in terms
of average life expectancy but I'll cut
all continents across the board
increased here's a little bit more
detailed data so I have selecting
certain values and I'm grouping it by
continent and country then I'm building
this new column life expectancy Delta
change and life expectancy
so it's this life expectancy minus the
lag life expectancy in other words the
previous in my group and then I want to
summarize that by finding the worst life
expectancy change and getting the the
first value in that in that regard for
each continent so Rwanda had a 20-year
drop in life expectancy during their
genocide Cambodia had a nine-year drop
during there's al salvador montenegro
strain interestingly I guess it's not
strange it's it's a positive Australian
New Zealand have never had a five-year
drop in life expectancy the worst case
was a very slight increase so if you
don't want to be depressed you can go
look on the opposite side we could say
how about we get the largest life
expectancy change Cambodia and Rwanda
had big jumps after their genocides
ended New Zealand was the biggest jump
in Oceania with two years so everywhere
else there were big increases big
decreases but this has been a very
consistent data set for Oceania and
single biggest gains in GDP by continent
so again I take GDP per capita and I lag
it and I get the max so this is all the
same pattern Asia Kuwait had a huge jump
when they found out that they have a lot
of oil Libya had a big jump Trinidad and
Tobago had a big jump I don't I don't
get that one I looked into it and I said
I still don't get it
not surprisingly there's been a slow but
consistent growth in Australia New
Zealand never any decreases always
consist
stantly slow increases or normal
increases so in that regard it's doing
pretty well for Australia in New Zealand
so let's spend the last few minutes and
look back at the data set the Raleigh
crime data set
while I do this I am going to start
running this query so that it'll be done
by the time I'm done talking about this
stuff basically we have looked at using
ty DRD plier on some academic examples
and now I want to tie it back to the
crime data set that I had before and
perform some fairly simplistic
analysis to try to check data
cleanliness nope I did not
vamp long enough but that's okay we'll
talk about this this is my query from
incident oh hey I've finished the way
that I could tell it finished is if this
dot is dark color that means it's doing
something
it is currently uh white so didn't do
anything all right from incident joined
incident code joined incident type I
have my beat code description type
incident date and incident number have
all of that data in this set and that
gives me four hundred and thirteen
thousand observations of those six
variables I have some basic data cleanup
here where I'm changing data types
making certain values character and
others I'm making numeric specifically
I'm adding some new variables incident
year month and day and those are integer
values I have a function here called
complete cases this is an R and what it
does is it will tell you if you have any
nulls in your in your record so if I
just run complete cases like this rally
2014 it will give me back a true or
false value for each observation what
I'm doing here is I'm saying
filter out any of the records where
complete cases returns false so in other
words if I have any nulls throw out that
row turns out I don't have any Naz we
took care of all of that in the original
data cleansing in sequel server so I
Group
the data in a couple different ways
using the group by and summarize
functions I want to get by year and by
incident type that way I can plot
incidents by year and what I see is
fairly consistent year by year a little
bit over 40,000 incidents each year
until 2014 when there's a big drop
that's not because people stop
committing crimes in 2014 it's because I
took the data in the middle of the year
so 2014 is probably not a great year for
our analysis I can skip 2014 when it
comes to looking at year by year
comparisons next up do a histogram of
incidents by incident type and what we
see is this is how many times an
incident occurs in our data set there's
a couple that occur very frequently
there are a couple that occur pretty
frequently and there are a bunch that
are less frequent so we can see what are
the most frequent incidents larceny and
miscellaneous we can see what's between
20 and 80 thousand incidents assault
drugs burglary and everything else
taking these I can focus in on these
particular incident types and say I
would like to put up a box and whisker
plots I want to see how these change on
a month-to-month basis so filtering
where incident years less than 2014
grouping it by type year and incident
month giving me the count
and also tell me about these popular
incidents where I have at least 20,000
of this incident
I have these data sets and I want to
merge them together this is the
equivalent of joining so merge using
Raleigh instance by type and month
joined two popular incidents on this
column incident type I take those two
together and I can build them in a box
plot and what this shows is a few very
interesting measures the median 75th and
25th percentiles and
the data point that is at least the
first data point that is least one point
five times the interquartile range away
from the median if I scroll up and down
a little bit I also see these little
dots here are outliers there aren't many
outliers in this data set we're in a
fairly consistent band in other words so
different incident types happen pretty
consistently from month to month but if
I group it by beat I get to see a
slightly different result here's our cut
off everything cuts off at zero we can't
have less than zero incidents but notice
that we start to see some actual
outliers including these two up here
that are way higher than everybody else
so something is going on in this
miscellaneous category in two particular
locations and we can dig into that we
can figure out what's going on here I'm
gonna spoil it for you turns out that
those are near a mental health facility
and those were all related to mental
health incidents where a police officer
was involved in taking somebody to or
from a mental health facility I can also
filter by other categories so in this
case we have prostitution and again we
have a couple of major outliers and
should you be inclined to investigate
you could figure out well is it that
this is where all the prostitution is or
is it that this is where the police are
looking for it serious question to which
I don't have an answer
wrapping up with QQ plots basically a QQ
plot in this case I'm asking the
question is this data normal in the
sense that it follows a normal
distribution and I'm building up a
normal distribution and applying a QQ
norm and Q cube line the idea is that if
your data set follows this line
then it will be it'll show you that yeah
your data is fairly normally distributed
our data set falls off of the line so
there's this big fall off at the end and
that tells us that no it is not normally
distributed and in fact I could go and
look at the box plot up above and tell
you this is not going to be normally
distributed because you have a hard stop
at zero so if you can't have less than
zero you can't go out infinitely long on
both sides then your data is not going
to be normally distributed and here's a
QQ plot somebody who knows a lot more
about QQ plots than I do could look at
that and explain the same thing that I
did oh the distribution here yeah that
you're falling off here means that
you're going to end up being capped on
the left-hand side so over the course of
the last hour and we've looked at a
variety of data cleansing techniques
from here you could look at data quality
services or look at something that's
better than that
you could integrate with external api's
so for example if you're looking up a
postal address or if you're looking up a
UPC or hitting Amazon and pulling back a
manufacturer's number and continuing
along into other analyses Benford's law
is one of my favorites it is around the
distribution of the first digit of of
data sets I have a link to that in the
slides here as well if you want to get
more details so slides if you want the
code if you'd like additional examples
it's all at CS more info slash on slash
cleansing if you have any questions at
all please feel free to reach out to me
email address Twitter handle
I'm also here all week so thanks
everybody
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>