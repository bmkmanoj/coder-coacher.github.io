<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Streams, lakes and oceans – working with on big data with Azure ML - Barbara Fusinska | Coder Coacher - Coaching Coders</title><meta content="Streams, lakes and oceans – working with on big data with Azure ML - Barbara Fusinska - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Streams, lakes and oceans – working with on big data with Azure ML - Barbara Fusinska</b></h2><h5 class="post__date">2017-01-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-cU27p4G9So" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">mobile microphone won't work with me so
forgive me I'll be understanding here
and there but there is no no much room
to wonderful anyway and yeah it's a good
time just before the ones i hope you
want once keep it at the end to to just
grab something to it there will be
plenty left i can assure you and my name
is Barbara fishing sky will be talking
to you about big data and other ml with
my politically phrase title streams
lakes and oceans so basically it is a
talk about Big Data Who am I to talk to
you about Big Data another a mouth I
would programmer for 12 years before I
joined Microsoft quite recently a couple
of months ago and now I'm a data
solution architect my background is in
machine learning my master's degree was
in machine learning or trend to
producing new type of a classifier and I
was always a muff enthusiasm into the
cell this job is perfect for me so what
do we do is we try to help companies
move their workflow to the cloud as
you're obviously because of Microsoft
and my particular job is to help them
moving their work whole connected with
data all the data services analytics
machine learning etc this is what we
will be talking today about so part of
the big data we'll talk about what what
they meant by ocean streams and lakes a
little bit about machine learning itself
and how it actually connects to big data
and how they are not the same thing and
of course how you can do big data on
other platforms will jump straight into
their specific use case because it won't
be just a theory it will be use case
your band presentation and I chose for
you especially for you aircraft
predictive maintenance it will go
through their machine learning model
will go through the whole architecture
how can you make predictions and how can
you visualize it where you will discover
my stunning
design skills you're building we're
building dashboard this is our youth
case aerospace predictive maintenance it
is a big data project if you imagine
that you're in charge of an aircraft and
you need to make sure your airplanes are
working correctly when I was flying here
to Sydney we had like half an hour delay
i was flying in from seattle and they
said well we have we have some leakage
and you're thinking yeah well that's
that's not good well we will get get a
maintenance team and and they will they
will investigate what happens and i was
thinking here i'm going there with the
presentation what if you could foreseen
it what if you could like predict when
there will be a leakage or any kind of a
black a break of it so i chose the
example for you that deals with engines
so the problem is actually not just
about you know passage passengers not
feeling comfortable just before the
slide it's about reducing costs if you
can predict when your machines are
likely to break you can have a plan you
have a roadmap of maintenance work and
you can save a lot of money this is a
big data project so what would actually
big data is a massive buzzword of course
everyone is using big data for
everything connects it with data so I
heard big data is the same as data fine
big data is the same as machine learning
machine learning is big data no it will
connect it or it can be connected but
it's not the same thing if you if you
want to think about data science as such
I think this is the broadest umbrella
that underneath has all of the stuff
that you can hear under any buzz word
you can you can
same connected with data that's why it's
called data science but big data this is
smaller umbrella that you can think of
any stuff that that cannot be done that
are broken with traditional relations
relational approach it doesn't mean that
when you have a relational database you
cannot have big data of course you can
and actually think relational databases
are the structures or the knowledge that
we have and really really understand so
big data doesn't mean no sequel big data
doesn't mean a very often the volume
doesn't mean mean to have a lot of data
it can mean variety of data we have
different data at the month we have
sensors we have we have I OT we have
social media so this is also big data
doesn't mean that it needs to be a lot
of them I get questions all the time
about the volume of big data so people
are asking what what's good the size
right puts the size of big data because
with today's computers we can just put
stuff into our memory it is do
calculations there why do I need all
those funky technologies so my advice is
if you can do it do it do it in memory
do it in your console if you have like
grab stuff and you can search through it
you don't need all those stuff if you
can do it on your laptop for you it's
not a big data but it can be even with
the same size of the volume for someone
else with different circumstances so why
is the traditional approach a little bit
broken well it doesn't fit anymore this
is a very nice slide I think it's very
nice flight from Martin taller web page
here has a gift of explaining some
concepts very very clearly when we think
of data warehouses usually how you work
with
as you prepare data beforehand and then
you fit your warehouse with them you
clean them you structure them you
arrange them in a certain way you do
aggregations you do joint and then data
warehouse is serving those data in a
nice usually sequel kind of a form to
people that want to get information so
you have a lot of preparations
beforehand and they feel like it's
something that has the desertion serving
different purpose the purpose is we have
so many data and we don't want to spend
all this time preparing them why because
for example might not need all of those
data but we always think we'll so we'll
keep them just in case and then you can
keep them you can store them in
different forms structures and
structured and structured blocks files
media however you want and then if
systems want to do something with them
they can search for them they can do
different stuff and use different
technologies to deal with them in my
title there are also ocean what is the
data ocean if you think like beta
science-wise there is no clear concept
yet at least I think about what data
ocean is but if you think of a data lake
it's something people can just jump in
and pick up the data from different
forms from different sources oceans
would be something more global they
kallik if a is a structure that you
built for your company that you built
for your organization data ocean for me
would be a concept that is more
worldwide and if you think of it the
internet is a data ocean all the API is
that are there all the data that you can
just jump in and take and you can drown
in it as this cute little guys doing you
can think of
ways of data you can sink any analogy
that that is connected with with the
flow and end with oceans but also you
cannot think of data ocean as a big
database because big database is static
is something that you take as a whole
when you do select you select for
everything of course you have indexes
and you can you can optimize it but it's
a different ways of approaching data
everything of the ocean you take just
one area and it's right to swim there
you don't swim through the whole ocean
but if you need the data from the other
side you probably need to plan it and
enter take your journey there to do it
but it's not not a small piece of data
for machine learning and big data for me
much more important is the second aspect
of big data which is velocity and when
you think about velocity we need to
understand how data are served so when
we go back to dreams oceans and lakes
you usually think of strength of that go
to the river river goes to the sea and
this is a great analogy but sometimes
you have a data leg and you can create
streams from the lake you can create
streams from the ocean from the sea this
this is where analogy is broken but not
where big data is broken you can do any
kind of speed and kind of velocity and
kind of direction where your data are
searched ingested passed through analyte
change so let's talk about bi and
analytics when we think of big data when
you think only about volume and you try
to build a dashboard
usually end up with something like data
warehouse so you have some data you
store them somewhere and you present a
report this is what our I don't know
revenue for the last quarter was right
so Devon matter if you have like really
really fresh data because standard
classic bi and analytics are just
reading the data and trying to
understand what happened in the past
when you're thinking more real-time then
you could do stuff like you know f5 and
refreshing or you can think about
something smarter about web sockets
about pulling something that is changing
in front of your eyes so even though
it's still be I even though it's still
an elliptic it still dashboards it's
still just visualization of data you can
approach the problem different way and
develop the velocity is is a key thing
here because depends on what what are
your needs you need to take care in to
take into consideration how how the data
will change to the person that is
looking at them so we have streams
streams are very interesting very not
understood type of velocity so you can
imagine on the left side of this diagram
you have like sensors IOT devices we
have small computers I have joined my
purse mine that are sending data some
more and something needs to be done to
ingest it and it's not an easy thing
because it's really hard to predict the
pace of those data the type of those
data and something people building stuff
like this that can ingest anything right
so you have even haps we have stuff like
Kafka and simple ques simple like
message buses that can take those
signals can take those data and do
something with it so what can we do the
you think we can do after we ingest the
data is to just prevent it the kind of
play like it's like a data lake but it's
not data like because all those data
will disappear after some time because
that's how usually this part is built
something needs to read it very often
this is implemented as pops up kind of
architecture so the data is served if
you want it register and and get the
data and then if we want to do some
Emily's analytics we need to have
another piece in this diagram that is
ingesting it Kafka is doing it so stream
analytics and other art is doing it it's
not an easy thing because you again you
don't have a piece of data that you can
go and select stuff and aggregate you
have stream so you have a time frame to
work on and this is why Apache Hadoop
was built to work with huge data volumes
and it works in a way of divide and
conquer it can deal with any data very
variety because there are different ways
how to how to for example query the data
they can be structured or unstructured
and of course stuff like spark like
storm that that can deal with with the
flow of the data of constant flow the
talk is about machine learning and I
keep getting this the misunderstanding
machine learning and big data are the
same thing they are actually completely
different and if you work with machine
learning you probably know this but if
you don't you might get this
misconception so actually when you think
of machine learning how it works and
look at this diagram we have some
algorithms and we have some training
data the algorithm is trained and once
we decide okay my our girl is good we
publish it and then when it's published
we can use it to do stuff in our system
but this side of training of learning
the model it done beforehand behind the
scenes and it's not really usually
plugged in in our system this is what is
plugged in and our system we have
published model and we can ask for the
opinion of our model in case of our
predictive maintenance for example when
will my machine break so where is the
big data part in this the data part is
here and data part is here here we have
training data and believe me you usually
are not big data because learning an
algorithm is a time consuming job and if
you have bigger data set of course you
can think your model will be more
accurate but if you if you do it smart
you don't need to have a big data set
actually if you think of algo machine
learning studio they can only allow you
to ingest 10 gigs of data it's hardly a
big data right so this is not the part
when you when you think of a big data
but this can be when you have your
publish model and you put it in your
system and you have incoming staff in
case of predictive maintenance we can
have our sensors read read we can query
can ask for the opinion of our published
photo or we could do it even smarter we
could gather the batteries with two
batches of our sensors and ask for the
opinion calling on calling our published
model only ones right but this is like
an architectural choice but but in this
case which usually come to mind no big
data unless you do something called
online learning so your training stream
is coming in and constantly updating
your model unfortunately you cannot do
it in a dremel at least yet another
mound you have this offline learning you
have the training model and and the
classic way of doing it when you're
thinking of big data and Azure and all
of the stuff I just told you this is
like a classic a classic slide that we
people of Microsoft can have it but it
perform every page this is all the
technologies and all the protests almost
that you can think of when you're
thinking of the big data lately there is
something called IOT hub so instead of
even have and if you want more
management with your IOT devices you can
also use il t-hub so you have data
factory data catalog even have those are
and the technologies that you can use to
move your data to some place that that
you can do something useful with them
and you can see you have data warehouse
and you have data lake because both of
those approaches are not excluded you
can actually use your data lake and do
some stuff like cleaning the data and
fit them into your data warehouse
because data warehouse it's not
something bad because it's a classic
approach is just something that cannot
deal with many problems that we have
today but can deal with many problems
that we are used to having but the most
interesting staff are happening here we
have machine learning we have data like
analytics which are using approach like
lambda I only call it functionally only
pay for this analytics ceramic once it's
done enough pain we have aged the
insight which is a Hearton works
distribution of Hadoop and screen
analytics this is the think that
cognitive services both Freeman and
Cortana who has ever used Cortana
yes so this all is called Cortana
intelligence wit but kurt imel's just
one one of the pieces here because of
the Cortana intelligence you have all of
those stuff together there would be a
different pricing model if you make a
deal with Microsoft that's one thing
another think it's just very easily
pluggable together if you take those
pieces we might get easy for you to
connect the dots but for me the most
important thing is that there are so
many ready to use solutions and I will
show that you in affecting so let's talk
about our problem description we have
sensors that are that are readings from
our engines and we want to have
something useful from them we can ask a
question well well this machine break if
we know we can plan in advance right or
we can a different question because this
would be a regression question you know
who you get like months or days or weeks
so can plan different question would be
well my device fell in two weeks because
every two weeks I could run this
algorithm or every week or every day
even and I can plan in advance where to
send the maintenance people or there is
I had a hard time with understanding
this third question which is like will
it fail in two weeks are will try in
between two and four weeks or will it
not fail in four weeks but what I found
out this is a very valid question when
you're planning maintenance so you have
those it doesn't have to be two weeks or
four weeks you can you can put any unit
of course there but this is something
when people are planning the maintenance
job maintenance work are actually
thinking and categories like this with
our example we have 21
sensors read and stuff like three
settings which are the settings of the
device so we need to know how the device
is it started with and there is
something called cycle because machine
is engine is sending the signals for
cycle per cycle and the cycle can be a
one day 1 minute 1 second one
millisecond whatever you want so if you
if you think of this as a specific
example it is because it's real aircraft
sensors architecture that i will show
you but if you're planning to do
something similar for any device this is
a very abstract way of of putting a
problem you made you might have just
five rate and your devices could be all
the same so we don't even need those
settings but if you think of a cycle you
can adjust your system to your needs so
how does machine learning work in this
case someone created a testing data and
how did they do it they've noticed that
machine is going fine and then breaks in
some at some point right and then it's
broken so an example here it's broken in
135th cycle and someone said well that
probably not just connected with the
current reading the last one art or the
one before it's somehow connected with
the history of those sensors rates right
so they probably try to draw something
like this but if you think of like
milliseconds the device is sending the
sensors and of dimensions when we have
21 sensors it's not something a human
being can analyze unless your genius and
I appreciate that but most of us can't
and so the idea was how do we capture
the past or like the year
passed before the machine breaks and the
idea is we get the sensors read and we
get the average of those sensors read
from I don't know five life cycles to
last minutes to days whatever it is and
standard deviation so average if you
take the rig and divide it by the number
of reads standard deviation is you don't
know it's something that tells you how
the reef differ from the average so
let's see it in machine learning studio
how do we actually decide remember
question is will the machine break in
two weeks this is what I love Cortana
not for the of course for the technology
and those are very cool stuff but most
of the jobs you google and Cortana right
thing getting Cortana and you find the
answer you find the solution sometimes
not just this is actually a just a
machine learning piece but sometimes
when you google it sorry search for it
you have the whole solution with whole
architecture you click deploy and you
have the whole infrastructure there's an
amazing and sounds brilliant but be
careful because it it sounds easy it is
is it will deploy it will create all of
those stuff but if you really want to
plug it in for your own problems you
really need to understand your problem
you really need to understand how those
people did this so it's like an open
source community but not always you can
find all the documentation for example
because this is what people are sharing
and what they decide they are sharing
and sometimes they are so deep dive into
into what they're doing you might not
share all the details but it is a
community you can reach to them you can
ask them there are comments below so
that's my favorite part when
it comes to Cortana to speak into so i
did because any solution i would figure
out wouldn't be as good as what those
people actually did what all the data
source is when you think of a dremel
you're plenty of them you can you can
get sequel can get block you can run
high square e from hadoop unfortunately
you cannot use document DB which is a
shame because the community be size is
like 10 gigs and you can only in jest
and geeks to azure ml brilliant but not
yet i think they're working on it but
i'm not sure it would make sense but
it's not yet there with other male once
you create your machine learning model
you can publish it as a REST API or
maybe after seeing the run stock
yesterday I should say you can publish
it as an API so you'll have an API and
you can ingest it and can use it in any
system you want you don't even have to
use other anymore you can just use any
technology because it's just HTTP right
so this is my architecture that I will
present to you we have sensors I'll read
them in even half our pipeline through
stream analytics and stream analytics
will call machine learning the
algorithm's I would just I will just
show you in in a second it will call the
algorithm which was previously I'll show
that in a second which was previously
trained but romantic has another job
remember those are average stuff and
standard deviation swim analytic needs
to actually figure out those this
history and calculate it and put it
through the machine learn then it goes
to document eb and i built a stunning
beautiful pink up that will show you how
those sensors events are red and if they
need maintenance or not another thing is
when we change our model we can force
the model
we trained and there is a way in a
dremel to just do it behind the scenes
once it retained you can plug you don't
have to do it it plugs it in the new
version and and you have a new newly
learned model okay maybe I'll just go to
the browser let's go here you have you
have to sit yeah i really love machine
learning but the only way you can talk
to it and you can sell stuff is through
the web browser at least at the moment
and you can see that if you have a don't
worry about what what's there you really
i really encourage you to go online and
look for for the details we won't go
into details it would be too too long
for for this for this talk but i just
went went to the Cortana page i clicked
it i copied it my own workspace now like
music it's that simple if you want to
understand if you have everything there
the thing is they use a different
classification problem and also benefit
of the cloud you can just use different
algorithms and check which one is the
best and if one proof proves to be the
best then you say oh there you go
publish this one so this is the outrage
standard machine learning experiment and
if you go to like tutorials and how to
do it you can build very simple ones but
real words are girls look more more like
this they don't fit usually and wine in
one page I rearranged it to fit in one
page because I wanted to take a
screenshot in cases it's Wi-Fi doesn't
work properly but to understand it
better you really have a complicated
thing here and believe me that it will
be easy to understand is a little bit
maze but it is easy to get a
ready-to-use solution like this once you
have this you can publish it as a web
service
and funny thing on that webpage on
kirtana you can actually download this
bit only this bit you have a trained
model and you have a published white web
service you just need to run you need to
click here deploy a web service and it's
they're running you don't even go for
this machine learning part with those
four different classification models
because the old did it for you together
I encourage you to do it again because
it's good to understand the problem when
you're trying to put an in production
system of course but if you just want to
play if you just want to understand how
other people are doing it or just want
to use something that it is proven for
example you want to put the sentiment
analyzes on Twitter you really don't
care that much how how it's done just
take it how it is so we have a web
service and we have some URI and of
course from access key that will be
using later so going back to my maybe i
will show you the solution
architecture and this is just the base
of machine learning let's look at even
help how is it done so to create an even
hope you still need to go to the not to
the last one portal if you know what I
mean if you are dealing with with the
newest boarder so this is the new portal
and this is the new new portal service
bus and even have we are still not moved
to the new newport or you can see them
as resources which well you can barely
do anything with that so if you go here
you you see your service bus and if you
go to your event hub i was trying to
feed the system with some data you can
see the statistics right so as you can
see how your system is behaving because
i'm talking for ya half an hour now not
even longer for half an hour there were
no data i wasn't sending any data and
this is this easy
the only thing you need to configure
when you're creating even hub let's see
it's the method retention so how long
did I message ingested message will stay
in the queue and the partitions data is
very interesting if you're if you're
into stuff like this from it's brilliant
it will send stuff and even half is
figuring out where to put it to deal
with it and for later for the readers to
get the data in a quick form so that
everything doesn't broke right doesn't
break it's like message queue but but
better because it is partitioning and
it's all done for you just brilliant you
need to create something called as a
traxxas policies policies and units of
course to have an access key so that
then you can plug and the output of your
even have somewhere else and of course
if you want to send the data we are even
happy need to know how to access this
they think that people usually forget
and that's why there is a default for it
or consumer groups if you have different
up different clients reading from your
event hub you should create different
consumer groups but if you just want to
test if you just want to see how it's
working there is no need you can just
use the default but the default will be
proudly if your target and in a
production system the next thing after
we after we read our our event from the
sensors is how to maybe member will go
to the end because trim analytics is the
most important but we need to know how
to put it at the end so document BB I
chose it but I warn you if you have big
data in terms of volume and you want to
store them somewhere document DB is not
the data document DB is big data in
terms of unstructured data this is this
part of big data one of four V's but
it's not a date
a basement to be or any document
database i'm not talking about this one
in particular i'm talking about anything
oh c'mon would be big how to be anything
its database where you don't care that
much if you lose some data which could
be the case here we might not care if
some sensor data loss but just you need
to decide if that's the case and you
need to realize that it only takes like
gigabytes of data mined in terms of
volume not big data database so i just
created a one collection and if i go to
the collection i have one document if
you speak in terms of no sequel you'll
probably know what i mean if you don't
it feels the relational person this is
like a database and this is like a table
which a no sequel word doesn't mean much
because you can spare an instructor in
your table but it's for your draft of
the problem life but it gives you a lot
of power and comes with a lot of
responsibility you can see all the
statistics here and this is the part
where a lab other portal is still
working because i was trying to hand the
sensors and put it into the database for
the last hour before i start at the top
and I still could see no data no
requests zero it is refreshing stuff
when it wants to do and decides to be so
don't be alarmed if you're putting the
data and you don't see if you're
definitely miss actually with even top
when I went to the dashboard and yeah I
was sending the data ya like like here
so it's arrested but if I will send in
the data it's my London time by the way
that's right seven yeah seven ten you
wouldn't see it for the next half an
hour
because this is the place it refreshes
stuff ok we have even have we have our
database this is a cool think you can
actually I wanted to do this you can run
a simple query if you want so this is
the way of checking if you have data in
your database rather than the statistics
and because they are not refreshed very
well and then you need to have something
that processes it let's let's look at
the architecture again so whatever
stream analytics is doing is taking our
sensors data it applied the average and
standard deviation to it and then called
machine learning there's a lot of going
on in this small to two arrows I wrote
three our arrows so we have a lot of
stuff and with their with the standard
approach i will show you that i took
screenshot because i didn't believe in
this Wi-Fi and kind of us right in a
standard approach if we want to do
average from our sensor this is what we
would do right we want to see what's
what was the average for this kind of
that device is this understandable for
everyone oh and but in criminal ethics
we only have pieces of base alright so
so we want to we need to win to think of
something smarter and actually with this
particular time frame let me have this
so what we need to tell the stream
analytics to do is apply tumbling window
and then the timing can be widened you
can do smarter things right but if you
don't if you want to do it it wants the
query we want path that the job won't
start so you won't do anything stupid of
course but you might not understand if
if this is like the first contact you
have with processing streaming
you can use your application time so if
you have some feels that that is a date
the date time you can use it then you
need to timestamp your stream properly
so we need to have average and standard
deviation and then we need to join it to
our sensors that are coming in right
we've done is we have similar problem
this is how we would do drawing lines we
have our stream and we have our
aggregated data and we want to join them
by ID again we have a time frame and
only pieces of data we need to apply
something that is called tape date
difference so you think that the date
difference between CMA which is our
stream and the aggregated data can
differ differ only by a minute and then
German Alex can adjust so I'll show you
now I will stop my maybe now because
we'll see i will show you how our query
query look like and it is blurry if i
stop it it will just not stop for a
couple of minutes they will walk you
through so you create something that is
basically calling average and standard
deviation so you have one table with our
statistics you have another think that
is just taking the data taking the
settings taking the cycle taking the
sensor data and this is the thing when
we call machine learning so you're
taking our sensors is taking our average
our standard deviation stuff and you're
plugging it in and calling machine
learning I will show you that maybe in a
slight maybe this will be much more
visible I we'll unhide it
since this is our like first table which
I called it aggregate this is our table
with prediction and business where we
call machine learning and this is where
we join it with aggregate so at the end
we have all the settings how I call all
the reads all they are average beta all
the standard deviations and then we have
prediction the record has been learning
and we have our prediction for that what
we need to do later is to put it all
together and send it to document DV
don't know if I can nag
then we have we have our ID we have all
of our scores labels and everything goes
to document it it is a hard query mostly
because we have 21 sensors so you have
21 averages you have twenty one standard
deviation things and then this is
something that you take from the machine
learning which was called result before
if you remember here okay prediction was
called result and you get on these court
labels what are scored labels if you
look inside of the machine learning it's
the label if it will need a maintenance
or will it not need my maintenance one
would be s0 would be no so maybe we have
like 15 minutes maybe i will show you my
beautiful you i finally what i need to
do is to run much visual studio because
i didn't publish it no IIF
so what we what we have is we have our
data in a database I will try to
generate now more I would start my
generation another good example of how
you I works is actually something I
download it again from Cortana something
that generates the data because I don't
have actual like thousands of planes
that will generate my engine risk in a
backyard and so it will put the data
into even half our team our magnetic
stop is still running so it will profess
it it will put it into the database and
now i will try to show you how how it
will work on the UI hopefully
now we need to into it for 10 seconds
yes see the green ones mean that it's
working Lee doesn't need maintenance
that the red ones means you need to look
at this device and plunder maintenance
in the next two weeks if you want to do
something interesting with this a few
can but i will put the the most of the
code is already on github but it needs
to tweak that I did the last minute so I
need to publish them and push them on
the github so this is basically what
your dashboard in a more useful form
would look like so those are like
real-time data I of course put my own
pace because I'm doing silly polling so
I put paste that every second it will
just pop in your rate if you have a
different pace if you have more data
basically you need to do it smarter you
need to in to your socket for example
right and do something something more
fancy
okay we went through the stuff the
interesting part you can do once you
have your reach and you see your
dashboard and your tank yeah it says
it's okay but I know it last time when
it had those wreaths it broke so if you
have someone that is monitoring
constantly the output of your machine
learning or any algorithm basically that
is deciding and predicting you need to
apply some kind of an auditing and then
you can try to update your training data
because maybe in the face of training
your algorithm didn't get all the
information so we we think it works
correctly because it tested it on a test
data but with the real real life we need
to adjust and we need to adjust our our
model this is why retraining concept was
introduced and we're still talking about
this offline updating so we update our
training data and you run our learning
process again we're not adding new data
to our existing running model so what
else we need to do is to just no trigger
the learning phase and in any model that
you in any platform there's probably
like a button or you need to rerun your
Python or our script so it has new data
with machine learning and on adder you
can actually set up a web service to do
it so what you need to do if this is
this is the simple experiment that fits
and one half of the page it's quite
valid is it it predicts your particular
when you want to take along a risk if
you will be risky for a bank so it looks
the income and and it can make a
decision if the bank should give you
alone or not it actually I heard it
actually works like on production system
and when you have an algorithm like this
so if you have your algorithm if you
have your model you of course need to
train your experiment and then you need
to have something that it's called
scoring experiment so you need to have
this in your experiment what's calling
does is it takes your data and apply
those labels our numbers depending on
what your experiment is doing is it a
regression ares at the classification we
need to have this bit another thing you
need to do you need to publish your web
service in the classic way I just shown
you so you publish your predictive model
and you have it and you can use it and
then you need to have something you need
to add it manually and put it into the
train model why because this will be the
web service this will be the API that
you will be calling while retraining the
model so I mean to create something
additional to this what you've just
created and apply it to your train model
so create a web service and you connect
at the output to your train model and
another thing you need to do is to add a
new identifying if you go to machine
learning studio and to go to and go to
experiment and there is a know if you
can see the web service is top you have
a list of web service you just need to
add a new endpoint it's not done
automatically as it is for normal
publishing the endpoint because the
default just get your scoring model just
your predictions if you want to add the
retraining you need to do it manually
but it is simple it's just the thing you
need to do so how do we do the retaining
scenario we'll go to my up
so my idea was someone comes in and just
do a random chatting so click on one of
the devices and look at the data i
shorten it only five rates and only five
other oh sorry she didn't tell me I'm in
the presenters mode so when I go about
just do it again someone goes to the up
and decides I will see about this rate
they click on it yesterday and I shorten
the list only 25 rates to only 25
averages and five standardization but
you get the point and they look for the
numbers and think that's not wrong leave
nothing will break in two weeks we can
have it like human coming in we can have
it more automatic ultimate price and we
can have many ways that the idea is
someone says this prediction is not good
right and we need to change it and we
don't have to just change it like here
so someone can see this motion is
breaking we want our algorithm to know
for the future that this reading is that
so what we could do is we say now this
is not good this is actually read and it
can say submit to our submit to our list
of stuff that we change we are autistic
and then you can click retrain button
which would update your training data on
a cloud and trigger retraining API and
the retraining model will now do the
proper think it could also take time
depending on how quickly your your
experiment is trained right so so this
is the basic idea yeah i think we
different time i only have a couple
slides left
you could have some questions efferent
the last Hank I want to talk to you is
about scaling web services because we
said machine learning studio publishes
web services and as default it's both
from 20 to 200 concurrent requests so
not natural question is what if I want
to do more then you need to go to the
azure portal and just treat this
endpoint click this those web services
like any other try to randomly
distributed just another web service
nothing to do with machine learning per
se is just how you would scale another
endpoint this is it we basically talked
about mostly a different velocities of
data which is the mind think when you
think of of machine learning and a
little bit about machine learning and
how it differs from from online learning
and and how the classic one look like
which is basically what you can do in a
dilemma we went through there for the
scenario of airplane maintenance
objections and this is my architecture I
wanted to share with you and this all
will be available on github but I still
have it locally so i probably will tweet
about this then you can just take it
spin it up on either if you want i'll
just look how i would configure it if
you don't want to spend your other
credits i encourage you to go through
the links of this presentation and go to
the court i'ma solution which is much
more complex and used with other
different technologies this is more for
me to show you that you can easily plug
in the stuff but in real life for
example you could try to actually save
those readings and then in a tan theta
lake or even data warehouse whatever to
stir it because there can be thousands
of millions of data terabytes of data
and
the community they wouldn't be a good
choice you can plug in different stuff
you can you want to do all stream
analytics production maintenance
prediction as is just one of them but in
a huge system and have other concerns
and use the same for example even have
to get the data in just so if you have
any questions we have a couple of
minutes otherwise i'm i'm here until the
end so don't hesitate to it to talk to
me and thank you for coming again and so
you probably need to return it anyway
even if you download the relative so you
need to retrain it because the training
phase is like on your account it's done
on your account but and you probably
need to adjust the training data
structure because this particular
experiment or any experiment you take
takes a specific data structure if your
structure is different then you need to
retrain it but you probably very often
you just need to change this part the
data ingestion cook the rest of the
other of the experiment will just it's
just about the you know the input
structure
okay the other why would create two
separate as stream analytics jobs but
have just one model which you can query
right and just different frequency yes
but it would be with any API is just
something that it's served in the form
of API if you're calling an API you
still need to maintain it right if you
have a lot a lot of traffic nothing no
difference and in this way
there is no if statement and in
experiments in a dremel so far at least
and so I think this would be a solution
for your problem but no there isn't
unless you write something in our in our
you can have it statements or to pick up
any algorithm but there is no no tools
in those drag and drop stuff that you
can control the flow exactly thank you
yes
there is some model format but it's not
accessible and I think this is something
people really complain about and I would
think this this is something people are
working on but i have no knowledge of it
but this is also something that bothers
me i would really really like or even
upload and model like this like our Ram
template and have it they're not storage
some somewhere there in the cloud which
it's a cloud for you know at least for
now of course under the hood there is is
just not publicly accessible for you
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>