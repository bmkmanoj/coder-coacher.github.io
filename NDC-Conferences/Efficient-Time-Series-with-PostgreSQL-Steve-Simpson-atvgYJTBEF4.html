<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Efficient Time Series with PostgreSQL - Steve Simpson | Coder Coacher - Coaching Coders</title><meta content="Efficient Time Series with PostgreSQL - Steve Simpson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Efficient Time Series with PostgreSQL - Steve Simpson</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/atvgYJTBEF4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Steve Simpson I've given
this talk a couple of times before but
only two I need PostgreSQL conferences
so this is the first time I've given
anything like this outside a postcodes
conference so the usual response is yes
that's great we love postcards that's
fine so it'd be interesting to see how
it goes down at the mall with a more
broad audience with that in mind quick
warning this talk does contain a lot of
sequel so if that offends you you this
might not be the right talk for you to
watch
so I don't actually have any affiliation
with post CREZ QL so I'm not going to
try and convince you to use it in this
talk that's your decision to make so a
quick overview and talk split into about
seven sections it did a bit of
background a bit of background on the
use case and then we'll go into
technical details and there's also a
small section at the end which after
running through this tour couple of
times realized there really wasn't
enough time to do it but it needs to be
mentioned so that's wise in small
letters so my background so I'm
primarily a software developer and I've
done a lot of this in the past and is to
work on embedded hardware so this is an
Ethernet switch and 10 Gigabit Ethernet
switch with a chip inside that can
transfer a terabyte of data per second
which was quite fun to work on and the
last five years or so have been working
on databases and software company called
just won their little startup in the UK
and they based their product off post
grades 12 hence my kind of interest in
Christmas
so either mentioned on from a
little-known city perhaps from the UK
called Bristol I wouldn't really get a
lot of attention in the news but it is
according to the BBC the best place to
live in Britain so if you ever go there
and you want to move there move to
Bristol because it's great yeah now for
that so I work for a company called
stack HPC so we do consultancy on HPC
for OpenStack and I probably doesn't
mean anything it doesn't matter the
purpose of this talk we work on systems
that looks a bit like this so they're
kind of they fill rooms rather than
filling under your desk and something
interesting about these systems is the
amount of work they do is kind of
staggering and a lot of moving parts it
used to be that this was sort of they
were quite unique in this regard but in
the sort of advent of cloud you know big
data center that looked like this aren't
really that unique anymore but the
problems they tell their face are quite
interesting so we work in partnership
with University of Cambridge on one of
these not this entire system maybe one
rack of that system building a HPC
cluster which is currently used for
medical research processing brain images
so that leads us on to kind of the use
case for this talk and that's monitoring
so it's been too long on this but I
think it bit of background is kind of
useful so I'm monitoring what we were
interested in taking load of information
from those racks and racks of servers
we've got for the software the hardware
and presenting it in some way so people
can see when things go wrong and draw
pretty graphs and it's kind of important
for a number of reasons
so fault finding and alerting so I know
one thing goes things go wrong and we
need to fix things and for post-mortem
and preemption
so in the keynote this morning we were
told that when we have a failure the
most important thing we can do with a
failure is learn as much as we possibly
can and the way we can learn as much as
we possibly can but having as much
historical information as possible
before whatever went wrong went wrong
utilization analysis efficiency analysis
so how well are we using the hardware
that we spend you know tens of millions
of dollars by performance monitoring
profiling auditing and so making sure
that people using the system that are
allowed to use a system and and
decision-making so future planning my
current system is this big how big need
to be in a year site so the common
components of these kind of systems you
often get these sort of systems that do
checking and alerting their ping things
check that there's no disk errors that
kind of thing so log collections big one
these days and certainly gets a lot of
press so all of the logs from all of
your hundred or thousand servers put in
one place so you can index them or
search them and then metric collection
so historical CPU usage or disk boosted
for example so why was my disk full
answering that kind of question so the
kind of incumbents in this space I Tinga
or Nagios some of you might have used
this kind of thing does things like ping
services check that HTTP endpoints are
available gives you a little dashboard
and I'll nag you at 2:00 in the morning
telling you something's gone down and
you need to reach the coffee cabana it's
quite popular these days coming with
your search engine full of your logs
ganglia probably less well-known it's
very big in HPC space and it's been
around quite a long time as well in this
space I don't think it used to look this
pretty bow
this is actually Wikipedia's monitoring
system so if you go to a quick Wikimedia
gangee adult wikimedia org you can go
and look at what all their cpus up to
fascinating most hours i've been a hotel
room doing this very late at night
so lobby probably seen this graph on a--
quite popular tool similar kind of thing
plots graphs for you very pretty and
always wins points with kind of
executive level people and but the
profile doesn't actually store any data
for use especially collect any dates
just a front end so what you need is you
need a database to store all of the data
that you wanna graph so you've got a few
choices in this regard and so this list
I wrote beginning of this year I'm sure
there's more and I'm sure there's more
kind of obscure open source projects as
well but haven't listed here and there's
definitely a big list of proprietary
ones that haven't listed here but these
aren't small projects they've actually
got backing from some quite big
companies they were like SoundCloud and
Rackspace and Netflix and Spotify
they've all decided to write their own
time-series metrics database for this
particular problem
so as I said before ganglia kind of came
into existence around 2000 particularly
in HPC space but some as well and
graphite is kind of just a database and
that kind of emerged about 2010 and then
about 2013 just get this huge kind of
space of development of all of these new
time series databases as follows you've
probably heard of in there Prometheus
perhaps in flux DB open psdb
so a bit more background so this system
might kind of look familiar to you it's
something that we're working on or
something that we've sort of taken on
from some other people so we've got some
stuff that's producing some metrics for
us and some log files we've kind of got
this middleware in the middle that gives
us this HTTP API and does some alerting
force ever it's trying to build a sort
of an all-encompassing monitoring system
so we've got the checking the logging
and the metrics all in one system so we
can do some information sort of analysis
with that
so this middleware has a my sequel
database so if you use I tingle or
Nagios you might have a database
collecting history for you the metrics
at the moment we're taking putting them
in influx DB and could replace that with
any one of the other open source
products we saw me again and that's got
good Farnum on top so you can graph
things the father has a database which
stores dashboards and state and now I
think it can store alarm state as well
as of the most recent version so that's
got sequel Lite in it the logs go
through thing called log stats which
probably heard of go into elastic I've
got cabana as the front-end some people
then wanted to put a message queue in
the middle so we've got Casca there it's
just for fun and it's all the logs and
gopher and yeah all this is done for
good reason I'm not knocking any of it
people need to be able to handle huge
rates of logs and bursty kind of data so
that's a good thing Africa is great with
technology for all of so if a couple of
these things we need a zookeeper
database as well it's actually still
much data in there but it's needed for
coordinating the others and then in
particular system and we've also got a
patchy storm which is like a stream
processing framework and that takes data
from Kafka and creates alerts and
creates events in my sequel for you so
this
anything to do with what our systems
actually doing this is just the
monitoring for the systems right I mean
some of you should be thinking hold on
then we need a monitoring system for
this monitoring system well you probably
do and and the thing that worries me the
most about these kind of systems is that
there's six places in it which store
data six persistent storage areas my
frequent flux Kefka elastic zookeeper as
you could like and if you lose any of
that data the whole system kind of gets
in a bit of a mess so as I said this is
commendable kind of right job right tool
for the job attitude not knocking at all
and works very well lots people use it
but could we at least unify the
persistence of the system
fewer failure modes which have fewer
backup strategies we need to worry about
if you're monitoring a business critical
system and you're monitoring goes down
how do you know your business critical
system isn't down your monitoring system
is as critical as any other system you
run so needs backup
fewer replication protocols if anyone's
ever dealt with sequel databases or any
databases and try to replicate them
things go wrong and they're all
different so one set of consistent data
semantics yeah so a lot of these new
sequel know sequel databases some of
them have acid some of them are
eventually consistent you've got to go
and learn in each one of those what it
does and the most important one for me
is you can reuse existing operational
knowledge now you probably already got
somebody in your business that knows
about my sequel or post careers or
sequel server why don't you just use
that knowledge for something else so
this is our idea okay could we at least
sort of unify the persistence
and could all this problem is is just
data storage and NASA's problem and we
happen to know a bit about postcode so
we thought let's use that we like
Postgres people use papers and you know
we don't have to do it like this you
know it can be microwave services so
it's early have to be one big instance
you know because that's that's bad we
don't do that anymore we don't have one
big database we have lots of little ones
and that's fine you know maybe we have
one to make sure to hunt for logs on for
alerting so we still have that
separation but what we're using is a
common technology for all of this
storage so postcodes can do a lot of
good things and it can act like a lot of
databases little these kind of no sequel
database it can add you can have JSON in
it you can do text searching in it you
can do searching within JSON and you can
of course have your normal kind of crud
style data in there and we can't do time
series we've just seen a list of 20
databases that were kind of produced for
this purpose and so time series bit of a
background about time series bit of
information so periodic time series the
awesome collector maybe it's collecting
CPU usage maybe it's collecting
temperature maybe it's collecting
rainfall maybe it's the next IOT gadget
collecting something as data's gonna
have a time as can have a value in CPU
usage maybe we've got some percentages
like that it comes in quite regularly so
every minute every second every 100
milliseconds some fixed resolution
what happened sorry okay so what you
might have and in fact what you more
commonly have is multiple collectors
selecting different metrics so every
second or every minute you'll get clean
attrex three values so to distinguish
from these we have some meta data so we
say I actually Stu or a CPU usage and
these to a temperature and this is I
wouldn't say it's kind of the standard
but it's definitely a popular way but a
lot of these databases store this data
and we have this kind of dimensions or
tags you sometimes see it called and you
can tag it with one or more bits of
extra information so you can say well
actually this is a separate metric so
it's on a different host or in a
different data center something like
that and then every time period you get
one of these readings and the time the
period might not be the same but you can
get the idea you then have this kind of
sporadic version of time series where
data sort of comes in whenever it feels
like and in this case you might actually
want to store some extra metadata so
maybe you've got some maybe you want to
send some event every time you get a big
chunk of logs and you want to record the
message alongside it or maybe you've got
some alarms going off and you want to
store the reason that the alarm we're
not going to focus too much on this
today because I kind of think even
though a lot of people are thinking that
this is a good application for time
series database I don't think it's the
most interesting one
okay what sort of data gets into this
system so at least in the system we're
working on JSON is the is the King in
the moment luckily we kind of kicked the
XML habit which I'm absolutely thrilled
about and with JSON is king so
everything's JSON got time stamp
suddenly matter what format you know we
happen to send them around in unix time
but it could be anything else the value
say 42 is good number as we learnt
earlier they've got the information that
identifies the metric so you got the
name and dimension or tags as they're
called so a CP percentage for host dev
and there's that extra metadata field
which is kind of optional and we're not
gonna worry too much about that but it's
there it's kind of payload so what sort
of queries do you want to do on this
data well we want to take some of this
data maybe want to take one of the
series maybe want the temperature for
this rack on your graph maybe you want
to take CPU usage for those two hosts
draw them on a graph something which is
important about these queries we're
doing is that we don't always want our
entire data set so we could be storing
weeks months of this data we only want a
few minutes of it maybe over the last
five minutes
so we want all the queries are bound by
some time range so this clear is kind of
important so we want to think about how
we're going to how it's going to perform
as we scale all various things in the
system so we want to grow the volume in
the system maybe today we want to store
days of day of data
maybe tomorrow one two days or a week or
a month and the number of metrics might
change new hosts might come online new
data centers might come online we want
to monitor them
and quickly complexity is quite an
important thing to consider in
particular the time range are actually
clearing so if you want to clear you the
complexity of a query to query the
second of data is a lot different
including a day of data so how does this
map in the relational world quite easily
just map it like that just put it in
table need two rows a measurement might
not be a good idea some of you might be
crying right now but it's a start so in
Postgres
just use time temps tzd the time points
if you want to represent a point in time
just use time time stampeded its UTC and
when daylight savings happens it won't
hurt you value um
floating point is quite popular for this
if you're doing anything in this regard
we have anything that needs really
strict accuracy using numeric for that
in Postgres it's a decimal type so if
you're doing things with money like
adding nought point one a naught point
one will give you not point two so
important things like that
or in this case it's just temperatures
and CPUs it doesn't matter too much for
Strings just use Viator our fixed length
datatypes don't really matter in
Postgres and even though they think in
any database anymore the way postcodes
handles updates means that you don't
really get any benefit from having a
fixed size field such a postcode has a
json b type which is pretty cool and so
I can store arbitrary JSON but it can
also let you clearly inside it and
search for things inside that JSON
really efficiently so it's a binary
encoding so it's a bit more compact but
it also has just a JSON encoding as well
and really all this is just text field
which is validated and this is fine
if we don't want to actually do any
processing on it it's just payload and
that applies to our value matter so the
other sort of query will do and if we
want to get a listing of the series
potentially an ugly dwell on this too
much because it's really not very
interesting you select four distinct
things in the table we just showed it's
going to be horrific ly slow but we'll
fix that bit later and we can actually
filter it so we can say actually give me
all the metrics for this name so give me
all the series I can look at for the CPU
we could say give me all the metrics I
can look out for this host kind of
interesting things to know so this is
the interesting one the actual query
that we're going to kind of focus on for
the rest of the talk so we're going to
find some measurements from our table
and we want to specify a time range and
we want to say what series name we're
looking at anyone to say what tags we
want so we want the CPU percentage for
that particular host all very good now
we're going to aggregate the data points
to some periods because we're going to
just assume we're visualizing this data
we want to point every 10 seconds or
every 60 seconds so we can take the
average over that particular time period
and we just got a little help function
called time round which does that for us
and this is an actual native in
postcards but you can google it it's not
very interesting
and then we group by that interval and
take it up at rally this is kind of like
the most basic time series query you
ever want to do
so what we concerned about was this
query but we can tell about how long it
takes and in particular but concerns
about how long it takes when our data
grows so the data volume increases
calendar the query take and how long
does it take
as we expand the time range so we want
to clearly one day of data where we also
want a view of the last month of data
and we want those to be just as quick as
each other and really our target for
this is 100 milliseconds now I'm not
much for UI developer but 100
millisecond just kind of the magic
number that you can trick humans into
thinking something is instantaneous so
if you can get something to happen when
they click a button what happens in 100
milliseconds we kind of think oh that
happened straightaway fantastic and it's
a nice round number as well so that's
what we're aiming for so this kind of
red area on all the graphs this is the
danger zone that we don't want to get
get into so for our relational model we
don't actually start off very good so we
got so what we're looking at here is as
we expand the time range that we're
querying so these are in seconds so
we're creating a thousand seconds and
two thousand three thousand example and
I've got three different series here
four different volumes of the database
what we can actually see is the query
doesn't matter what range we're clearing
from the data still takes the same time
but if we increase the amount of data in
the database us when it starts to slow
down so if we turn this on its head and
instead graph this so data volume now
you can see that it goes up query gets a
lot slower as we put more data in what's
interesting is we can actually be okay
if we were sort of under a million rows
and so we could probably scan over a
hundred million rows display a million
rows not hundred million in less than
100 milliseconds
so I guess a good thing about this is
the query times fixed regardless of time
range but and we're kind of on target
for less than a million rows but time
scales linearly with the volume of data
this isn't very good okay we want to
store increasing amounts of data in our
database and this is because every query
we do needs to read the entire table so
anyone with any kind of knowledge of
databases know where this is going so we
need to do some indexing couple of
points so time stamps to essentially
integers so I've got like fantasy about
time stamps unix time since 1980 I think
postcode stores it a little bit
differently but Postgres has many
different types of index and be three
hash bringe in gist's there's also one
called run I think which is going to be
available soon
I think gin and rum were made by a
certain group of Russian post players
developers as I've met a couple of them
at the conferences and they're very
interesting people very clever people as
well
so each was excellent for the quality in
between operations so that's worth
bearing in mind so a bit of b-tree
revision who's familiar with b-trees a
couple people okay well you can all
check your email while I do this I'll
just give it a quick overview for beach
views for everyone else so you got some
table there's some data in the table
it's all jumbled up still bit of a mess
post pro split split data into pages
they not to be too concerned at that but
it does the index is a separate
structure and at the root of the
structure there's page of data so impose
this the number of things in this page
vary and but in this example just shown
it to and that contains some of the
midpoints of the data and then just a
classic tree structure so all the values
less than three go to a separate page
which is pointed to all those between
three and six or in their page and all
those between
over six different page and all those
point to the actual data so when we
query we can say seven we're looking for
number seven is seven greater than three
yesterday's of the greater than six
yesterday's da da
go down that road oh look we found seven
there's our data the more interesting
thing it can do is if you're looking for
all the values between 6 &amp;amp; 8 so I think
timestamps you can say well 6 is greater
than 3
okay we found 6 great I'm going to get
the data for 6 it just have to go back
and do another look up because the
b-trees ordered so if it needs if it's
looking for all the values before 8 just
walks the bee tree and just finds seven
and goes on finds 8 really efficient so
that one bee tree look up now imagine
this is two or three pages so two or
three disk i/os after that to get the
entire time range you just walking
through this index really really
efficient so with that in mind let's
look at our query and we've got a
between predicate good and this
eliminates huge amounts of the table so
if you've got six months of data we want
five minutes this is a really selected
predator so it's an excellent Camden
candidate for index so we do this great
index on table using B tree imposed rows
B trees the default C then have to say
that kind of using it here explicitly
and you say we want to index on
timestamp
so this is a lot better so graft here
there was all we had before without an
index and graph here and yellow with the
index its staggeringly better as you
might imagine
so we zoom in a little bit on that we
can see we're well into our gold and
it's kind of Felicity's volume so as the
volume increases there is still a bit of
an increase as we increase the time
range but not very much so if we put
time range back at the bottom we can see
it's creeping up so that's something to
be aware of but we're definitely a lot
better off and so now I'm going to
introduce we've had one metric up until
now now let's say we've got ten
different metrics in the system so every
tenth row and kind of a different metric
unfortunately if we go up to 100 metrics
this problem kind of exacerbates itself
even more and we definitely want more
than 100 metrics maybe one a thousand
maybe 10,000 so we kind of back in the
danger zones that's not good so we find
that fine up to 10 million and we find
up to 9,000 seconds with 10 metrics and
the clue times stable is the data volume
increases this is a big win over where
we were this might you know it's might
seem straightforward it's worth
understanding especially if you want to
build more complicated systems the time
range kind of falls over with 100
metrics and it's now apparent that the
query duration actually increases the
time range grows so it's no longer
increasing as the data volume grows its
increasing as the time range grows yeah
so the reason why these metrics causing
such a problem is because there's now
more data to filter out before we can
find the interesting stuff so what could
we do Wow
we could do more indexing so there's two
other clauses in here and if you kind of
really optimizing databases 101 it will
say go and look at your where clause and
add indexes for the things in your where
clause so we can do that and we can
create a couple more indexes on the
measurements table so on name we're just
doing a quality so you can use be tree
for the JSON field we can actually use
this gin index I'm not going to go into
any detail about this but this is a
really cool index and this is the thing
that lets you find any key value in your
JSON using an index in adjacent column
it's really cool could do a whole talk
amount so we're not going to go into
that the problem is now we've added
these indexes we've actually made it
worse so that yellow line there is with
the extra indexes but you can see the
lying
I actually just dips under so we're
going in the right direction here and if
we kind of extrapolated this that red
line would go up linearly and that
yellow line would sort of start to
flatten out a bit so as the volume at
the time rain grows in the query we're
actually having a positive effect but
too much indexing can be kind of harmful
so next step in databases 101
normalization to those people that were
crying earlier about my table design can
now hopefully take a little relief so we
take our measurements and we split into
two tables one table just stores our
values so in that table what we're going
to do is going to take a timestamp the
value in that metadata and put that in
there and for the metrics we're going to
take the information that identifies the
metrics name and dimensions put them in
there
the way we referenced the metric is with
an idea my expect and that reference is
a metric in that table so these are you
stored once and what we've done is we've
removed a huge amount of data from that
values tables that tables are not
smaller and because we've only got that
one thing identifying metrics we can
eliminate a lot of data in that table
and this makes it a lot more efficient
to read so the metric table defines the
identifier and you can use a serial in
place code to do that as I'm sure you
can in most databases we then have this
unique car constraint because we only
want to have one ID for each set of name
and dimensions and this handily creates
the correct index force so when we're
not deep and we're normalizing our data
the right index is in place for us so we
can make this data make these two tables
mimic the measurements table we had
start with a view and these are defined
by just a query so we can say when you
query this actually do this and give
that data back instead and what this
does is it just joins the two tables
together on our ID and our query doesn't
have to worry about any of that
splitting out we just did and in fact if
you look at the query plans if you
manually write the join and if you write
the query against the view you'll see
they're exactly the same so you lose
nothing by clearing the view and having
this layer in front other than that your
queries are simpler to read so it's
really good
so we can do a bit more trickery as well
just to make a life bit easier so we can
say the problem with the view is we
can't insert into it because it's a view
it's sort of a read-only representation
of this query that we've specified but
what we can do is we can say on this
view when somebody doesn't insert to it
do something else and in this case what
we'll do is we'll insert into our values
table and the only difference here is
we've got this little helper function
which allocates are symmetric ID so this
is essentially doing the normalization
for us but it's really transparent to
the user Long's all he wants to do is
insert which is all we really care about
and so I'm going to too much detail this
is a lot of code and it's a bit small
although this projector is fantastic so
it does actually
it's almost readable I won't bore you
with it though so it's a still procedure
it takes in dimensions and it returns
the ID to get the ID you do that you
select it if it's not found then you
insert it allocate a new one good leave
some indexing as well so timestamp index
same as before and that occurs on the
values table and the we're also going to
do is we're going to sort of add the
indexing on the metrics as well but
instead of on the individual fields we
can do it on the ID so this sort of
serves a similar purpose to what we were
trying to achieve before so now when we
look at this graph again so as we grow
the time range of our query the other
lines obviously would be the indexes we
had before and the red one is what we've
just done and unfortunately we're just
back to where we started so we started
on blue or just the time indexes we
added some extra indexes and we just got
back to the start again and so
something's not quite right here so
we've eliminated that additional
overhead we still don't actually have
the right effect
so I'm going to explain this thing
called a bitmap index scan which is what
post goes calls it so we have our two
indexes and we laid the time index give
me the values between two and three and
from the electric index we ask give me
the values which we call to the time
index dutifully says well this one is
fine this one matches this one right so
this one matches good so far
make sure if index says matter to have
to do that's two that's two as well and
then it combines these together and it
says the rows you're actually interested
in because we're handing these two is D
and F and this works quite well some of
the time it worked really badly a lot of
people a lot of other times in in
particular our use case what would be
better is if we just had one index that
was for time and metric and we said to
it I'll give me all of the rows and
match our clauses and it just tells you
that those are the ones so luckily you
can do this coincidentally so our index
that we had before got a timestamp
non-metric ID and what we can actually
do is we could create an index on both
of these columns and you have it with
one index structure they represent both
these khans at the same time so if you
haven't have a query that's querying
both of these columns it's indexed
absolutely perfect for it we have rid of
a problem though which order do we put
them in well we can either put timestamp
in then metric or expert metric and then
timestamp and if you look in the
documentation which I'm sure everybody
reads then it will tell you that if
you've got a range Creek range predicate
then you're better off putting it on the
right so are the qualities on a metric
if we put metric first and then times
down and they so we can take advantage
of that brilliant behavior of that be
tree again so once we're doing our range
we navigate this beach we want to find
the metric and then to find the start of
our time stamp but then once we found
the start we just iterate down the index
it's really efficient
this is well I've sort of discovered
this I was quite happy because I was
thinking this is one of those cases
where such a generic structure happens
to be the perfect thing what you're
trying to represent so the line in green
is good and that's what we've just done
and this is staggering
you had better it is so we've taken
those two separate indexes and we turn
them into one it's really quite dramatic
as you see is the time range goes all
the way up we're still well in our
threshold so let's make this a bit
harder let's increase the volume a bit
so we were and we were looking at a day
to age of 10 million rows in so now
let's up it to 100 million so with 100
next tricks this is equivalent about
going from one day of data to 11 days of
data at one measurement per second on
Hertz and let's increase the maximum
time ranges as well so before we were
topping out a 2.5 hour range and now
we're going to be trying to get ten one
day range so again we just flat with
data volume which is good but we are
going up as the time range of our query
goes up and of course it goes up a
little bit too high so as we get to
about sixty thousand seconds or so
they're kind of creeping over that
danger zone again so this we haven't got
rid of this problem as we clearly more
data it takes longer as we increase the
number of metrics it goes really off the
charts literally off the charts but it's
actually indicative of something very
odd going on the database and this is
that we're craving a lot of data and
we're actually hitting some memory
limits in the database so we live a
conflict week this does flatten off a
bit but it does raise an issue
so we can go up to 100 million and up to
some quite good time ranges for 10
metrics where we do hit some limits so
we need a better strategy for handling
this so I call this summarizing you can
call it roll-ups or Agra create
aggregation or number of things
materialization all kind of mean the
same thing so we've seen that for 100
metrics we kind of miss our target so
for over eight eleven days of data this
query might be returning up to 40,000
data points to do it this is actually
necessary if we're visualizing this data
if we're drawing a graph why do we need
40,000 data points to draw this graph
all the graphing software is going to do
is average them down and average
monitors only got 2,000 pixels so you
can't see any of it so it's a real waste
so let's just say that say that 4,000
are enough or even 400 no you're not
going to use a whole screen for one
graph so what's the concept here so the
concept is we bring up a second table
and what we're going to do is as the
data comes in to this table
we're going to store it in our
summarized table until we get a time but
it falls into the same bucket as a time
we've already got so we're rounding this
time so I call this values 2 so every 2
minutes we're going to have 1 summarized
data point so when we get this value and
we're actually going to do is update an
aggregated value in this table so in
this play what we're actually doing is
we're recording the sum of the two
values that have gone in for the same
time period and then for this one we go
in there oh and update up and now we're
in a different time period so create a
new one
and if it goes on like this we update
this one and update it all so we've
ended up with the table that's actually
a fraction of the size of our source
data which is gonna be a lot cheaper to
query and yeah this is an example with
two but could be any period that you
like so bit of sequel to create this
create our table because it values 10 so
what we're going to do is we're going to
roll up every 10 seconds into one second
and we have one entry per timestamp and
per metric and this is unique so we have
one this comes in handy in a bit later
on and then we actually record multiple
different aggregates so in that example
I just gave it was just some but
actually we're going to do is we're on a
record
account and the men in the max and
anything else that you might want to
record so when you come to query this
data you can say give me the sum and for
that entire period it's just all you
there computed for you and we can even
make this look a bit nicer so we can do
drawing that we did before with our view
so we can play our summary and because
we're only storing the metric ID we need
to get the other data from the metrics
table and this just simplifies queries
and it'll be joining for us
so how am I going to implement this or
someone in the last talk so triggers are
awful so we're gonna use triggers sorry
about that and there are other ways to
do this but triggers are the simplest
way to describe this problem and once
you kind of understand the fundamentals
you can think of umpteen other ways to
do it with message queues in the middle
and eventual consistency database over
here and over there the triggers are
really easy way to do it so this is all
just boilerplate they're worried too
much about this just defining a trigger
and then attaching the trigger so we
insert a row there it puts over there
this is the interesting one so this is
what we run when we insert a row
and what we're actually going to do is
we're going to insert into our summary
table instead and in this example new is
the row which is entering the database
so we round the time down so we get the
time period up to say 10 seconds and
then these are just the initial values
for the aggregate so for the sum the
initial value is just the value for the
count the count is now 1 causes the
first row and the min and max the
initial value is the value if however
there's already an entry for that
timestamp and that metric which we can
say using common conflicts then we
update the row that's already there and
what we're essentially doing here is
we're kind of aggregating a little bit
we're doing a bit of an aggregation
which we've been doing a query time so
for the Sun we just add the thumb to the
time that's already there for the count
we just add the count that's already
there and what we're just adding one
because we defined one earlier on it min
and Max you a little bit more
interesting so the value that's already
in the table we take the min of that
with the new value that's coming in and
likewise for the max okay if we look at
the query actually mostly unchanged from
before except we're now clearing this
summary table instead which remember is
now a tenth of the size so that's really
good and the only bit of fiddling we
have to do is see we sort of we're now
aggregating aggregates so for sums we
have to sum sum of sums for counts we
have to sum the count and for average we
kind of have to do that ourselves so a
little bit fiddly but not too
complicated you can also do standard
deviation if you also record sum of
value squared
so now as we scale the time range of our
query we write back down where we should
be and if we compare that to where we
were before we're doing a lot better and
this makes perfect sense because we're
only clearing a table that's a tenth of
the size but we've lost nothing in as
far as the user is concerned so this is
this really is the key to kind of what
all of these time series databases are
doing they're doing the pre a gregarious
Oh have a 100 second roll-up period for
example so you can query months or years
of data really fast
this isn't kind of a new concept in
database world either so just for fun
let's increase the volume to a billion
rows anyone run a database with a
billion rows in a table yeah bad idea
isn't it Oh interesting
we should talk after see I mean this is
this is kind of it's a bit of a cheat
because we're not actually clearing a
billion rows we're clearing a tenth of a
billion rows but we can do it and you
get some quite interesting results so
now we could store this is now
equivalent to 16 weeks of data and we're
clearing up to ten days so as a data
volume in Crystal even all the way up to
a billion that index lookup isn't
getting any more costly okay this is a
really well well engineered bit of code
but of course the amount of time it
takes goes up and if we look at the time
range requiring so recurring a billion
rows a billion rows and we're querying
up to 900,000 seconds of data which will
as I say was earlier 11 days or
something and we get all the way up here
before actually gets to being too slow
so
we're clearing all this data but the
user just sees it coming back instantly
literally know what's going on
underneath that's fantastic
and if you want to scale it further
maybe do one hundred to one and this
makes sense as well if you want a
summary over a year you really don't
want every one second data point in that
so I'm completely out of time as I
thought I would be and this is the
section that was in small letters at the
front of the talk but it is so important
that kind of has to be mentioned so I'm
going to be very quick about it
so partitioning splitting your table by
time interval into lots of little tables
and what this does is this makes
deleting old data really fast you just
drop tables so you've got a six-month
retention period any day too old in six
months you want to drop really fast this
is going to do it it also makes it much
more efficient to maintain the tables
you've already got and you can do some
really cool things like reindex and
cluster index and this can make your
crews even more efficient cluster index
especially is really good older data can
be put on slower disks so maybe half
second response time is okay for data
with a year old and you can avoid some
performance issues which you do get when
you have very very large be trees and
once you get to tens of billions of rows
in one index some things do start to
happen especially with regards to
ingesting data you can also partitioned
by metrics so putting different chunks
of metric into different tables so we're
splitting by metric so that would be on
the name or the dimension hash for
example and this will allow you to scale
the number of metrics in your system
almost forever because you just have a
different system for a different set of
metrics and if what you can do is in the
monitoring case if you can put all the
metrics from a group of hosts say a data
center or rack into different databases
and you can actually build a view on top
of that and make it look like one really
easily
this needs a talk all of its own but
unfortunately so this is where we finish
a couple minutes over so come and talk
to me afterwards if you have any
questions or think it's a horrible idea
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>