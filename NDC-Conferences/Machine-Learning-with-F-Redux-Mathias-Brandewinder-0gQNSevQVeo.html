<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with F#, Redux - Mathias Brandewinder | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with F#, Redux - Mathias Brandewinder - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with F#, Redux - Mathias Brandewinder</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0gQNSevQVeo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone so this talk will
be about machine learning with f-sharp I
want to make me
vacation is like there was with f-sharp
comma Redux so this talk is not about
Redux I do not do any JavaScript I do
not do any web stuff like part of it was
me revisiting all the ideas I had about
machine running and see if they still
held so if you're looking for anything
related to JavaScript or web stuff this
is probably not the place you want to be
at right now so I realized a bit late
that this was a potential ambiguity so
so this is on the other hand about
machine learning and F shops if you are
there for this hopefully we get what you
want so briefly so I said already good
morning so my name is Mattias
vandevander you can find me on Twitter
as a this little creature here this is
also how Luke and github and my two so
I'm not a software engineer by training
like my background is really in applied
math economics like that type of stuff
and at some point I moved from France to
California people who moved to
California to write code so I started
writing code I became a developer and I
fell in love first I started working
with c-sharp and like a couple of years
back somebody told me every year you
should learn a new language it was 2010
I looked at F I completely fell in love
with the language and so at that point
most of what I do is either F sharp or
machine running typically both of them
at the same time hence the talk so where
is this talk coming from so the I'm
going to start with the two statements
which are probably not polemical that
one of them is like machine learning is
a pretty hot topic it has been hot for a
couple of years and is getting only
hotter so it's a topic you should
probably care about as a software
engineer and now like if you come from
mostly a dead background like I do and
like you might be a bit concerned too
because if you look at machine learning
talk so much don't need discussions you
will hear a lot about Python a lot about
are a lot about maybe Scala many
languages and what you will not hear
typically is dotnet and so you might be
concerned and say like does this mean
that if I'm a developer coming from
dotnet and I want to do the machine
running should I move away and start to
embrace Python and are and the short
answer for me that you should probably
run Python anyways because you should
learn a new language every year but
otherwise I have been doing machine
learning with F sharp on dotnet for like
four years straight and it has been
working great for me so
I'm hoping to convince you today that
not all things are desperate and if you
want to do machine learning on dotnet
there is a great solution right there
for you for the picking which is called
a shop that's kind of the gist for why I
wanted to do this book so I really have
two goals here one of them is I see that
for a lot of developers when you come
into the field of machine learning it
looks pretty magical like people do
numbers and things happen like you saw
the the keynote earlier like you or you
get flying cubes all this stuff and
exactly it looks a bit like magic and
it's not quite clear what is happening
and so my goal here would be to give you
a domestication of machine learning try
to give you a sense for what is it that
machine knowing people are doing how is
it close to what you do as a developer
and maybe get a sense for in real world
what is it that machine learning people
do and on the other side is like if
there are machine learning practitioners
in the room when you're familiar with
the shop I'm going to try to show you
why you might want to consider also
using F shop as a tool in that space so
this is what I'm aiming to do what I'm
not aiming to is a deep dive into
machine learning or into F shop that
would be too much for one hour it will
be too much period so I will not try to
do that in that frame my plan is going
to be roughly like two big parts one of
them is like a demystification of
machine learning so i'm going to try to
show you like if you are a machine
learning person what is it that you do
and so I'm going to go through it and
like try to show you like very
practically if I had a machine learning
problem how would I approach it what
tools would I use and what does this
mean well so in that frame we will
looking first at for machine learning
you need data so I will show you a bit
like what you can do with data what did
i looks like then what does this mean to
create a model and then one thing which
is I think a very important compared to
software engineering traditionally which
is like validation like how is it that
you know that your model is working so
this is the first spot will be like what
is machine learning doing and the second
part would be focused a bit more on
specifically F sharp and focus on
programming and that will be about why
some specific reasons why I think it's
great one of them would be using a
statically typed language like F sharp
is actually pretty interesting I'll try
to show you why I think so and the other
one is if you use a functional
programming language it's also pretty
convenient for all sorts of things like
parallelization and scaling algorithms
so I'm also going to talk a bit about
that
so in that frame let's start with the
first part which will be what do machine
learning people do so to do this I'm not
going to do my focus here we're not
going to be spectacular demos I will
take just a dataset which happens to be
a classic it's the Titanic data set so
I'm assuming everybody's familiar with
the Titanic like was a great boat and as
so I found this on history.com and like
the caption for the picture was at the
time of the completion many claimed that
the Titanic was indestructible I'm
assuming that you are aware of the fact
that it was not indeed indestructible
and just for this like this would
already be like my first advice in
machine learning is that can always take
with a grain of salt
any form of prediction somebody or some
Allah will give you like it might be
true but like it's not because I tell
you that it will happen that it is
correct so always be cautious now the
reason I want to talk about the Titanic
so this is a classic data set and which
so this is why I'm going to be using it
because Titanic has been extremely well
documented you even have a website
called the encyclopedia titanica
where you will find many things among
others you will find a good full list of
the passengers on the boat with a
boatload of information about them
things like you had like 13,000
passengers and for them you will know
whether that person survived or not how
that person was called how all they were
how much they paid whether the where
first class so that's a third class so
this is like a nice data set it's it's
one I like too because you have a bit of
everything you have numbers you have
text you have missing data like you will
pretty much hit every problem you could
have with the data set using the Titanic
data set so I like this one it's also I
don't know maybe there is a bit of a
morbid sense of humor around machine
learning people but for some reason it's
also kind of fun to try to predict who
dies and who doesn't if you feel
uncomfortable with that idea imagine
that instead of looking at people dying
of surviving the Titanic and you're
really looking at people who clicked on
a button on your website and you try to
predict not if they die but if they
clicked on a button to buy your product
that's pretty much what you would get
also in a more neutral less gloomy data
set so so we have a data set and the
first step you will do is like there is
machine learning with the data so the
first thing you would want to do is that
let's dive in the data and see what we
have there before even trying to do any
form of modeling or machine learning and
so to do that I will actually be using a
tool which is called
ipython notebooks so I don't know if
you're familiar with it but the this is
something which is as the name suggests
which is coming from the Python
community so the Python notebooks came
from the idea that if I'm doing a bit of
research work on data on my machine
I might want to be able to share it with
you I might want to work with other
colleagues and they might want to see
how I got my results and all these
things so the idea is something like a
development environment where I can work
freely in something like a scripting
environment but I can share it with
others and I'll get comments or like
yeah share it so that started with
Python and then people actually realized
that maybe if it worked with Python it
would be a good idea for other languages
as well and so it supports kernels which
support a variety of languages that are
like all these things and among others
like there is an f-sharp kernel which is
what I'm going to be using now and I'm
going to show you how it looks so the
way you would run an iPad 2 notebook you
have really a bunch of ways to do that
one of them would be you could run it on
a docker instance like it could run on
your machine I'm running it now locally
completely on my machine as it happens
it's like it exists also on Azure now
under the name as your notebook so it's
in preview but if you want to play with
it like it's a it's a pretty nice tool I
would recommend to take a look like
regardless of whether you do data
science or not like very interesting if
you want to if you do any form of data
exploration and you want to share
findings with colleagues it's awesome so
let me show you how it looks so here
what I did is I created a notebook for
the Titanic and what I'm going to do is
I'm going to start looking at the data
using that tool and showing you maybe
like what is in the data and what we
could potentially do with it to make
predictions which is what we are after
so before doing this I will start with a
I'm assuming of course I would like to
assume that everybody in the room knows
F sharp and does it day in day out it
might be the case so I'm going to try to
tell you like the couple of things you
might want to know about the kernel
about F sharp and about notebooks and
then I will dive into the data itself so
if you have not seen F
job before F shop is a functional first
statically typed language so what this
means here is like so here I'm in my
notebook and I can start to write code
and that code will be a statically typed
and all of this only I'm getting things
like system dot IO so I'm getting like
intelligence and all these things this
is pretty nice I'm getting an online
scripting environment with all the
things I would expect from a full IDE
the point here is like I'm writing F
sharp um I can use system I can use all
the libraries from the net so I've sharp
is fully compatible with dotnet so as a
result I could do things like ask what
day is today so here wrote a bit of code
and I can run it and if I look at what's
today as a day of the week I can take
that bit and I can run it and I'm going
to see that today is apparently
Wednesday which I believe is correct so
the point here being like I'm running
code life and seeing like what the
results are and if I share that notebook
with you you will be able to run my code
and check if yes or no
like it's great if you trust me but as a
data scientist you should not trust
people so now you can take my code run
it and check if yes or no you agree with
my results so I can use dotnet otherwise
like in a way similar to Python f sharp
is a user significant whitespace but
this means that no curly brace is none
of this thing so if I write a function
is like every time you have space it
means like you have it's the scope is
delimited by the spacing third thing is
a like PowerShell or like UNIX f sharp
has this feature called the pipe or a
pipe forward and what this the idea
behind the pipe forward is like it would
be nice if I could compose operations
and so the pipe that's exactly this here
I have like add X&amp;amp;Y and what I can do
now is I can say take one pipe it to add
one so it will take it push it to the
next function pipette to add to which
will push it and so on and so forth so
this give me a mechanism for I can take
something push it to a function push it
to a function push it to a function
which gives me workflow of operations
happening so that says I think we all
you need to know all most of what you
need to know but F sharp to be able to
follow this talk the other thing I want
you to show before diving in our data is
a if you're coming from a c-sharp
background is like a you're probably
with link and so a lot of the operations
you will do an f-sharp or manipulation
of collections or datasets look a lot
like link with slightly different names
and the two operations you you probably
want to know about is like map and
filter so map is the equivalent to
select take a collection apply something
transform it and give me another
collection which has been transformed
and filter is the equivalent of where
like take a collection and whenever it's
true keep it and whenever it's false
drop it so this would give me something
like this I can say here I have a list
of 1 to 10 and I can say take this pipe
it to a filter so that I will say like
out of all the X's here keep only the
ones greater than 4 and then map it so
that take all the XS x 2 so what I
should get here if I run this is
something like so I should have a number
only about 5 6 7 9 and 10 x 2 I should
see 1012 bla bla bla this is what is
happening and this is what we'll be
using throughout the talk so so that was
the introduction on the F drop side one
thing which is nice about notebooks is
like a notebooks is really full-fledged
dotnet development environment so I got
intelligence I got all this stuff what I
got as well it's like it would be a bit
sad if I could not use NuGet packages
right it's like I don't want to write
everything from scratch you have plenty
of libraries and as it turns out is that
you can use them by using packet so
packet is like a nougat client and I can
do things like use packet and I want to
use the new get package of shop data and
export pluckley
I can do that and now I can use it in my
script so at that point I'm pretty happy
because what this really means is I
pretty much have everything I would have
in Visual Studio but I will be able to
share my script with you now let's get
into the data set we want to explore so
I mentioned the Titanic data set so let
me first show you how this looks so now
I'm going to go for a second in in vs
code and so this is a CSV file because
like because it's always we file in the
end and so the CSV file contains a bunch
of data so I have thirteen thirteen
hundred passengers and for each of them
I have some information I know the class
of the passenger was it like your first
class second class or third class
I know whether the person survived or
didn't survive like that's denoted as 1
or 0 how they were called whether they
were male or female how
all whether they had like family on
board essentially how much they paid for
the ticket like all sorts of information
so this is what I want to use and I'm
going to be using the data set to try to
make various types of predictions using
machine learning techniques so now with
a quote I really want is like I have
this file I want to get it in memory so
that I can actually work with it and so
we'll do this in the in my notebook so
let's me go back to the Python notebook
here and the one mechanism which is
pretty awesome about F sharp is you have
this thing called typewriters and so
type of order you can think of it as a
mechanism where you can point it to a
certain type of data it will look at it
and it will immediately do what you
would have to do pin fully by hand if I
had to pass the CSV file I will look at
it see I have headers I have things
which are numbers boolean's and all of
this I would create a class with all
these fields I would read it and I would
have wasted half a day to do that with
the type provider what I can do is I can
say hey here you have a file which is
called Titanic that CSV so that's the
location on my file here I'll create a
type called Titanic based on it and so
and if I run this now is that I can
start consuming my my data set and
working with it so for instance I can
read it in memory and I can ask like how
many items do I have and I should have
run this before ok I will not run it
here that's fine
the but the what I should have run at
the beginning of the script before so we
don't do that here but what this will
get me is like essentially it will
create for me on-the-fly classes which
allow me now to access the data in a
statically typed manner for instance I
could grab the first passenger for my
sample and I could start to do things
like first passenger dot and what this
will tell me is like based on the file
you just gave me I had nothing to do I
will give you like properties which you
can start consuming for instance I can
say I have the class of the passenger
which is an integer did the person
survive as it happens it like it was
smart enough to see that I had 0 and
once if I have only the 0 and once in
all like you this is a boolean so it was
smart enough to say like this is
probably a bool
similarly I have an age which is a float
and all these things so the the gist of
the story here is that with the type of
error is like all the painful work you
have to get data to your environment
it's kind of done for you and now all I
have to do
can work with the data settles what you
see do I have anything interesting here
so let's look a bit at what we could ask
so this is where pipes and maps and all
of this is going to come in handy I
could for instance ask things like how
many male and female were in the Titanic
and so I would write something as simple
as this like take the sample and now is
like I want to count by the sex and so
if I do this and I run this it's like
nicely I will have something which will
tell me I had like 486 females on the
Titanic and 8:43 male on the Titanic so
I can start exploring very freely my
data set one way you could think about
notebooks perhaps is like it's something
similar to link pad like where you get
all these nice tools which allow you to
see what is happening in your data
except that you will be able to persist
it and save it and give it to a
colleague to look at so I have males or
females I can see how many people
survived and died so this is all good
and this is great and I can I could ask
you a question of like look at how much
people paid for the ticket and count it
by the price they paid and so if you do
this like you will get two things which
might be interesting to note like the
first one is like the first thing which
will come off is like any n and so any N
is like not a number and so the lesson
here or like my experience with machine
learning in data science is your biggest
problem will be like any D test that you
have will be worse than what you expect
it to be you will have missing data you
will have it will be a mess and you will
waste a lot of time on it will never be
asked in as you hope so that's the first
part and the second part is like what I
did here was accounted by the price
people paid for the ticket and this is
pretty useless now I can see that how
many like I have very close prices so
the the point here is like the operation
I would want to do on how much you paid
for the ticket is not a count it makes
sense to ask like how many male and how
many female where on the boat in the
case of how much you paid like the
question is a bit different because this
is real number and if I have a number
here I would ask things that what is the
average which is the maximum what if the
price increased by 20 percent and so the
broad point here is like even though
everything will be encoded as numbers in
your dataset you have to broadly two
widely different shapes for the data one
of them is you have numbers like how
much you paid how old you where
questions like a way to mix sense to
compare people increase the number
decrease the number and you have data
which is called categorical and
categorical you're really after is this
thing A or B
what's the person a man a man or a woman
was the person first class second class
or third class and the question like
where you like 20% more man then that
passage it doesn't really make sense so
this is important because based on the
shape of the data you will use the data
in a very different way and you will ask
very different questions about it so so
that was the first point here that we
have very different shapes of data in a
data set and so that's something we want
to keep in mind when we are going to try
to make predictions or use the data to
do whatever we want to do with it so I
mentioned the fact that the notebooks
give you the ability to run a shop code
and like start to analyze data looking
at numbers is nice but like sometimes
it's also pleasant to look at shots that
could shot can tell you a very good
story in a in a way which is much more
say human friendly than just a series of
numbers so wouldn't it be nice if we
could actually see shots as it happens
like you have this in notebooks whoops
and so here I'm going to use a library
called plotly which is like a nougat
package again so I load it and now I can
do things like before what we did was
the count bisects so what I had was like
how many passengers were male/female and
I can simply say pipe it to chart that
column so give me a chart for this and
sure enough what I will get is that in
line in my notebook I will be able to
immediately see this is how it looks in
graphical form so this is pretty
convenient similarly I can do things
like I can ask you have an information
and we are the passenger in box like so
you have like normally three places
I think it's sharp work and the two
others I don't remember somewhere in
Britain and again like we're going to
hit the fact that data is never in a
good shape and so here again we are
supposed to have three things but there
is a big block here which is we have
actually people where we don't know
where they came from so again we are
going to hit missing data in the data
set good and another thing you could do
so I can produce like more charts for
instance like if I look now at the how
much the passenger paid I could produce
a shot which is a bit different which is
a histogram like the density so I can
get a view of like how much how much
people paid for the ticket and so you
can
see not surprisingly that boost people
didn't pay for our merchants like as the
price of the ticket goes up you have
less and less people not a surprise so
the broader point is that I can start
using my data and start to produce
charts to see a bit like how does this
look what do I have here so so far this
is not very useful
I mean it's useful because it gives me a
sense for what do I have but wait I'm
after with machine learning is that I
really want typically to do something
like do predictions and so if I want to
do predictions the underlying assumption
is like if I look at this data the rest
probably information I can use which I
can leverage to actually give you a
prediction which is better than a random
choice so the underlying assumption
behind this again is the fact that there
is relationship between the data you
want to predict and the other data you
can observe so in this case that let's
look a bit at what type of relationships
we have between the data so for instance
if I was morbidly interested in knowing
if people survived or died on the
Titanic
what I'm looking for is that out of all
the other features or numbers I have can
I use any of these to make predictions
about whether the person died or not so
for instance I'm going to take my sample
and I'm going to say let's explore the
idea perhaps whether you were a man or a
woman mattered in whether you survived
that might be a reasonable assumption so
if I do this I'm going to take my sample
I'm going to group it by sex so now I
have two groups like male and female and
I'm going to take each of these groups
and for each group I'm going to compute
the average survival rate for each of
them so I'm computing like what's the
average percentage of female which
survive and what's the average
percentage of male which survive and if
I plot this chart I will see first that
I don't know if chivalry is dead but
like it looked like back in the days
when the Titanic sank is like there was
a time with chivalry was here like you
can see here on this chart that if I
take the group of females
I see like they had a 72 or 70 plus
survival rate conversely if you are a
male is that you had a measly 19 percent
survival rate so the the short version
is like if you were in the Titanic it
was much better to be a woman than a man
and the second part is like this shows
me like there is clearly information
which I could use if I wanted to predict
if somebody died or survived like now
one model I could do would be asking the
question are you a male or a female
and essentially here I'm going to say if
you're a female I'm going to predict
that you survived and if you're male I'm
going to predict that you died it's a
bit brutal but this is kind of what the
data tells me now and so this is this is
really essentially what machine learning
model does so right now the machine is
not really learning I'm doing the
machine learning by hand and so what I'm
after here is something like this I'm
going to create a predictor which is a
function which is going to predict what
I'm trying to predict is what happens to
a passenger and we simply say if you're
male then false like you die otherwise
are true you survive and now I can use
it take pieces of my data and produce
predictions it's not a very smart model
but like it looks simple but like this
is essentially the simplest possible
machine learning model it could build
with the data now and that's that's
really what machine learning does good
so now I'm using a one piece of data the
thing I have is that I have much more
than this I have also how much you paid
what your age was what your class was
all these things so perhaps I could look
at other pieces of information which may
be useful so if I do this like I could
look at things like the class so I'm
going to do the same exercise looking at
first class second class third class and
here again I will see a bit of a pattern
if you were a first class passenger you
had a 60 Plus percent chance of
surviving if you were second class
passenger forty percent and third class
twenty something so again like so this
one is not about travel really like and
I don't think it changed that much is
like it was much better to be a rich
person of the Titanic than to be a poor
guy in third class and I could build
again a model using that piece of
information I would essentially say if
you're in first class I would predict
that you survive I will be right 60
percent of the time if your second or
third class is like too bad you die and
I will be right for a free 30 percent of
the time so this would be another model
which is possible and now the the
problem is like this is a very manual
process and I see that I really have
like two pieces of information which I
could use the the issue is like can I
use them together and so this is where
like the manual process is going to be
better knowing right because now we'd
have to combine them how do I do this
how do I put together male-female first
class second class third class what
would I start to do if I put like age
fair and all these things and so what
the machine learning tools will do for
you
is that they will do what we did by hand
but they will allow you to put more and
more and more information in your model
and and combine it together so that you
will get the best possible prediction
given the data you give it that's really
what the machine learning machine
learning algorithm does another quick
point here is that we looked at two
pieces of information which are actually
useful I could look at another one which
is how many people how many family
members you had on board and here I'm
going to see that yes I have a bit of a
pattern but like really none of them is
a significantly above 50% so this will
not really help me much do predictions
so the other point is like this is not
magic some pieces of information some
some features or some of the data will
actually help you make predictions some
of it will just not so you will also
have to maybe discard some of it do the
type of stuff and that's it so so we now
go back to slides and make a couple of
comments on what I did so far so the
highlights of this was like first the
first thing is that if you want to work
with data the way a data scientist or
machine learning person will like it's
really important to have an interactive
scripting environment whether that's a
rap or whether that's something like
notebooks because what you will be doing
is like what you really want is loaded
it at once and you will be asking you a
lot of questions all the time and you
don't want to be spending your time
rebuilding reloading the data doing all
these things so the first step is that
you absolutely need a scripting
environment and as it happens with
notebooks and with the shop in general
we have a phenomenal scripting
environment here so that's great the
second point I was making here is like
there is no magic in a machine learning
machine learning is going to take a set
of data and it's going to look at can I
see patterns in the data and can i
potentially use it to make predictions
and so we saw first that we had two
types of data categorical data so which
one is that male or female and we have
numerical data how much is it like did I
pay more or less than you and yeah the
third lesson is all the last lesson is
like data will always be worse than what
you expect and that's a recursively true
story every time you think you have
solved that you will have more problems
with data good so at that point we
looked at the data and it's a she what
we saw was there is potential in the
data to actually produce predictions
about who survives who dies and all of
this how would we go about
doing that so we're going to search for
patterns and essentially what the
algorithms will do is that you will pick
an algorithm and the algorithm will take
the input you have like all the features
all the input you have and it will try
to fit a function so that if you give it
the input the output would be as close
as possible to the true answer and what
you hope then is like that what you
found it that particular dataset will
actually be usable beyond what you have
in the data you use there so elaborating
a bit on the point that you have
different shapes in your data you have a
bit of a choose-your-own-adventure set
of questions to answer when you start
doing a machine learning the first one
is like do you have data and it might be
an e if it might be a silly question but
I've seen people believing that machine
learning is so magical that it will just
happen and the point here is like if you
do not have data you will not have
machine learning so first step if you
have no data go back and find me data
and the variant of this is like maybe
you don't have enough data so get more
data so I'm assuming that we have data
so now is that way in the yes box next
step in the adventure is like what is
the question you're trying to answer and
there are two broad cases here one of
them is like I do have a question for
instance I do want to know like who dies
on the Titanic I do want to know how
many how much people will pay for my
product like you have a very specific
question you know the output you're
trying to predict you have a second
question which is possible which is I
really don't know I have data and I
would like you to tell me something
interesting about it so which is the
book that surprised me and that's also
possible for instance like you could
have your boss coming and say here here
is the sales data for last year can you
tell me something interesting about this
so these are like two different problems
you could tackle and these are two
different categories of machine learning
problems and once you are in if you're
in the category I have a very specific
question you have also an important
distinction which is what is the type of
answer you expect from the model and the
first answer would be how much like how
much did you pay and the second one
would be which one like what are you
dead or alive
are you a man or a woman or something
like that so these are also like two
important questions and so the reason
put this as a tree or as a as an
adventure here is like these all have
like names in machine learning and so
it's a it's useful to know what they're
called because now you know where to
search for them so the surprise me
category where I don't know really what
I'm searching for is called unsupervised
learning
the reason it's unsupervised is like I
don't I can't even tell you what it is I
want to learn I will give you data and I
want you to try something by yourself
and tell me something interesting
so that's unsupervised learning the
category I do have a question is called
supervised learning with two variants
which one will happen like what category
will happen is called classification so
for instance did the person die or
survive and the technique is a
classification problem and by contrast
how much would it be is what what people
call a regression problem like how much
did you pay for your ticket is a
regression problem that could be a
number which could take a wide range of
values which is a continuous variable so
with that in mind now let's look at the
second step which is now I looked at the
data I have an idea that yes it's
reasonable to think that I can make
predictions how would I go about that
the way we'd go about that is that at
that point I'm not really interested in
exploration I'm interested in producing
a model which will make predictions so I
will now move into a vs code because
what I want is to reuse code which I can
run so that I can actually produce
answers so I'm going to go into vs code
I mean the in the vs code scripting
environment and so as a as a reminder
like I'm going to start with the Dom
best possible model I could do and so
what we started with was we observed
that if we look at female passengers 70%
of them survive roughly 72 and if we
look at male passengers like a 19%
survival so what I'm going to do now is
like I'm going to use a machine learning
algorithm to learn something similar
like using that feature can I can I
produce can I predict if somebody
survives or not so I'm going to use here
a library which is a coming from so the
libraries called Accord which is
probably my which is probably the
library I would recommend to go to if
you want to do mesh start with machine
learning on dotnet like has a very
complete set of algorithms you can
use and so I'm going to learn now a
model called a logistic progression and
so if I want to do this I will need to
take the data and put it in a shape
which the algorithm can understand like
the one thing algorithms understand well
is numbers so I will transform all the
input from the CSV file into something
which is rows of numbers so I want to
learn from input and output so first
step is like I want to transform my
input so here I'm going to take my
sample which was like passengers from
the Titanic and for each passenger I'm
simply going to extract whether the
person was a male if that person is a
male I'm going to mark it as a 1 and
otherwise I'm going to mark it as a 0 so
pretty exciting but now I get like a big
big list of ones and zeros not very
complicated what I want to learn is the
output so here I'm going to take my
passengers again and for each passenger
I'm going to ask did the person survive
or die so it's going to be a big array
of like true or false and now the next
step is I will pick an algorithm which
happens to be here the logistic
regression it's not usually important
which one I picked you could pick other
ones you would get to roughly the same
structure so I picked the algorithm and
I'm going to tell it hey now I have data
for you here is the input you can use
here is the output you can use learn so
now on and out of this what I'm going to
get is now a function or code which I
can run which will give me answers and
so I can for instance do something like
give me a decision so here I'm going to
say logistic to decide and if I give you
one this was included a person like a
man so if I run this like I would expect
the answer to be false you did not
survive which looks at correct under
just to see if that what happens if I
put a female let's see if I would expect
this to be true and sure enough it's
true so it looks like so far the model
is doing something reasonable
furthermore I can also ask something
like this like now if I give you a man
what probability do you assign to that
man to survive and the answer here would
be 19 percent and that should be
familiar because that's exactly what we
got from the chart initially so what we
did here is like and if I do this for a
female let's do this quickly
let's run this and see and I'm getting
72 percent which should also be familiar
because that's what you get from the
chart so what we did here is like
took the very painful expensive road to
do what we did by hand just by looking
at the chart
but the point is that this was no magic
and like it did exactly what you would
have done by hand and it just did it
automatically by taking the numbers now
the the point I was making earlier is
that we have much more data than a male
or female what we would want is like to
put all of this together and use all the
information at once to make a prediction
and so this is where like this is where
like using a library or like using
machine learning would be useful because
I don't want to do this by hand this
would be tedious it should be an odd fun
instead of this what I can do is I can
take this and instead of taking just one
input I'm going to start to do things
like this now I'm going to say like if
you take a passenger I want you to
extract that piece of information where
your male a female 1 or 0 how much did
you pay how many people how many friends
did you have on board where your first
class second class third class did you
embark in Cherbourg in this person in
that place so instead of like one rubber
one or zero
now I'm getting like a big flat array of
numbers which is representing what I
think is useful to make predictions
about whether you died or not so this is
called like the numbers web extracting
is called like features so what I'm
doing here is a feature extraction I
start with the data set and I want to
extract out a vector of numbers which I
think will be useful to make predictions
the output doesn't change so this is
still like I still want to know whether
you died or not and the process is
absolutely identical I will pass it in
to logistic regression I will tell it
now learn from that input which is not
much bigger under the output and now I
can ask more complex question like if
you were a female and if you paid 150
dollars for your ticket and if you're in
second class and impact this what would
you predict would happen to me and so in
this case it's going to tell me that I
would expect that you survived and it's
telling me also that in that case I
expect you to survive with the 90%
chance probability so that was a woman
here so let's put a man and see that
what will happen if I put a man and now
is like what I would get is something
like I would expect it to go down
because it was not good to be a man on
the definite and it went down to 41%
if I had like somebody who was in
for in like not in a certain class but
in first class I could ask that what
happens how does this compare so same
person
pass it here and instead of 41% I see
now that it goes up to 47% so you can
the the overall point here is like what
the algorithm did for me is like
cranking it together like much more
information but it did the same thing I
did by hand before I just did
automatically for me and it's giving me
a much richer answer using much more
information that what I had before but
otherwise like no difference yes so like
what we can do here is the case energy
we are reducing using the old math trick
like if you have a problem you know how
to solve a problem you don't know how to
solve reduce it the probably know how to
solve so anything you have in machine
learning you will start with the data
set which is whatever you have you will
reduce it to vectors and once you have
vectors you can use all the algorithms
so that's that's what that's what
machine learning people do one question
I will leave aside for a second is like
is our model any good like I did
features I did monomers my assumption is
that it should be better than what I had
before I didn't really check that it's
not guaranteed so I will revisit that
question in a couple of minutes the
other thing I wanted to do before is
that I mentioned that you have read
three broad classes of problems one of
them was classification the other was
regression and unsupervised learning so
I'll show you briefly essentially that
this is exactly the same except that you
use different algorithms if I wanted to
do a regression on that same data set I
could ask a different question which is
if you give me a passenger can you
predict how much that passenger paid and
here the only difference I would still
use the same data set I would use input
so in that case I would not use how much
you paid as an input because that's the
output I'm try to predict but I do the
same type of transformation but now the
only difference is like my output will
be the fare how much I paid and now I
will pass it to an algorithm which
actually works for regression that
should be like your regression model I
will still learn from the input the
output and I will still use it pretty
much exactly the same way like now I can
say like for that procedure I would
expect that passenger to pay like $30
and so the broader point is like it's
really not different like the biggest
difference is like the output is a float
or double otherwise I trust from things
into vectors I put it into learning and
I'm getting a model which gives me
prediction so No
difference on the methodology I'm going
to skip that part and the other part was
about unsupervised learning so the one
of the techniques in unsupervised would
be like I want to know if there is any
pattern in my data for instance if I
didn't know anything about the Titanic I
would say that give me a tick that data
set and can you find me that
representative passengers or do you see
groups of passengers which are different
so the big difference you will see here
is a so this is called clustering so I'm
going to run a clustering algorithm I'm
still going to do the same thing so you
should see a bit of a pattern here I'm
going to take some features like male
females survived or not all that stuff
I'm getting again a big fat array of
numbers and now the big difference with
before is like now I don't have an
output because I don't have a question
so here I don't have an output to pass I
will simply tell it like take that data
set and crank it into a model called
k-means and try to find me two groups of
passengers which are like similar to
each other
so I'm going to run this now the
algorithm again is learning I'm learning
just from input and I have no output
because I'm not trying to predict
anything and what I can ask the model is
so now I'm going to wait I want to see
what I got out of the model let's see
what I got out of the model is like two
prototypical passengers and so what I
get here is they give first passenger
type this nice because the algorithm
knows nothing about the technique or all
these things and what it's telling me is
like I see a first group where like the
sex variable is high so this is probably
males survival is low class is a high so
meaning like it's two or higher so it
extracted one group here which is
essentially telling me like if I didn't
know anything about the data and I
looked at two groups one group you
should probably look at is like meals
which were like in second or third class
and these guys probably didn't survive
and conversely if I asked like about the
second group it found so I'm going to do
that right now
and I displayed this what I would see is
a different group and so the second
group it sees like hey this group is
mostly females these mostly survived
they were mostly in first class and so
the next thing here's like I didn't tell
the algorithm anything and it found some
information which is pretty relevant to
the data set the method was the same but
like the the type of question you're
asking is different so this is like what
you would get from unsupervised learning
so that's in practice like
how the day of a machine running like
this is what it means to create a model
from a machine running machine running
person perspective so highlights here is
at first even though dotnet is not the
place it's not the first place people
think of machine learning algorithms
there is actually plenty of libraries
around a court is a good one
ad lib is a good one so you have like
plenty of tools around the second one is
that we have like we so like three types
of a models or three types of questions
you can ask classification regression
and clustering and the Third Point was
like even though the problems are widely
different in the end we did pretty much
the same thing which was taken data set
transform it to vectors give it to an
algorithm let it learn and show me the
output and every time so the under
similar and the process what we do is
that you start with the sample with
input output may be no output if you're
doing unsupervised pick the algorithm
extract the features and yeah you want
to be careful about numerical and
categorical data so that's that's fine
so so so this is what you do when you
try to learn from the data like the
hopefully these the mystifies things a
bit first like this is not really
magical it's just like take vectors and
try to fit a function at that point you
might say but this is a this is a really
not all that fancy what you describe to
me was like fine data if you put more
numbers into the vectors you will
probably get better answers so and then
is like try every algorithm you have and
then ship it so in that frame it would
look like all you are doing when you do
machine learning is put more and more
and more inputs and at some point you
get bored or you have no more data to
put in and so you move on with your life
and so that would sound like a bit
simple and it's not quite as simple and
so the the first thing we left out so
far is that we haven't looked at we
haven't looked at what makes a good and
a bad model like we just assume that
maybe the model was better but we
haven't done that and so the the way you
know a model is good is the proof of the
pudding is in the eating I think the
proverb goes and the proof of the model
is like if the model is good at making
predictions it is a good model and so
what the machine learning algorithm does
is that you give it input you tell it
what the output is and it's going to try
to find the best match so now the first
thing we need is that
we need some form of metric to be able
to compare two laws and say is this
better or worse than the the the other
model we had before and so the way we
quantify this would be take the input
take the output compute the error look
at how far you are and the further you
are from the correct answer the worst it
is so you can start to compare two
models together and so what the model
does is like it's trying to take all the
input you gave it and find the best
possible fit to the data you give it and
so now the the problem here goes back to
an earlier slide I had is the cue hope
here is like I'm learning something on
the training set which is the data I
give you to learn and my hope is that
what I learned on that particular data
set will work well in other data I will
give you in the future in the end I
don't care how good you are but
predicting data which where I know
already the answer what I want is to
make predictions about things where I do
not know the answer and so and so this
is really a this is really one of the
problems where people spend time on and
I'm going to show you a bit why
so the way you would validate a model
and we - first we set this a bit and I'm
going to go to the part about a model
quality I'm going to so beginning of the
script is absolutely the same as before
I'm just going to load my data do all
these things and so the question here is
like is is it as simple the two
questions are how would you go about
comparing two models and is it as simple
as just adding more and more and more
features and just moving on with your
life so one model that could build is
like by hand I could say like I'm going
to predict if your meal you die if
you're female you survive and one way I
could measure whether this is any good
is like I really want to check is the
answer any card so one way I could
validate this pretty quickly say take
the model I just created take all the
sample for each passenger in the sample
compute what the model predicts check
the correct answer you know and if it's
the same if it's yeah either it's the
same oh it's not like what I want is
like as many matching a value so it's
giving me a big area of true/false this
is not quite what I want like what I
really want is a number which quantifies
like how close your and so what I really
want here is like how many what's the
proportion of incorrect answer I get so
here I'm going to do take all the
passengers and
whenever the answer of the model is
different from the correct answer I'm
going to knock it as a one this is not
correct otherwise I'm going to mark it
as a zero this is correct and if I take
the average of this with this will give
me it tells me like on average your
model makes 22 percent incorrect
predictions so that's now a model which
I can use as a benchmark to compare to
another model for instance I could take
a much stronger model which is like I'm
going to predict that everybody dies on
the Titanic so for you me and I could
say like how good is this model and if I
run this one through the exact same
metric is telling me that this is 38
percent of the time incorrect so now I
can very precisely quantify like this
model is better than the other one
therefore I should use it because
because this is good and this is what
I'm trying to do so so now we have a way
to compare two models together so now
the the question I'm going to be after
is like what so what happens if you just
add more data to the model is this just
going to get mechanically better and so
I'm going to simply run something here
so I'm going to predict the output did
you survive or did you die and here I'm
going to start so you can and I'm going
to take like only two features like are
you a male or your female and are you
first class or you're second class or
you neither
which means you're third class so just
going to take two features and I'm going
to pass them into a model I'm going to
learn and I'm going to compute quality
metrics so that's fine
input and code this will get like
included my data and I'm going to use
the here tree the reason I'm using a
tree is like it's a model where it's a
great model but like the problem we want
to show happens to show up pretty easily
with the type of model so now again I'm
learning the output from the input and I
can ask then it's like how good is this
model at predicting things and that
model makes that model is a mix error
only 21 percent of the time so it is
actually slightly better than what you
had before we added one feature this is
great now I'm going to do something
which is a bit weird and I'm going to
add here noise so I'm going to add
features and these features are going to
be random numbers either 0 1 and reason
you would expect that this should not
help my model right I'm just adding
noise to it so why would this make any
difference so I'm going to run this and
like you can probably guess that this
will actually make a difference
otherwise I would not be the demo so I'm
going to encode again so now is that I
have like 10 columns a black random
numbers and I'm going to again run my
tree and I'm going to evaluate it and I
had like 21% of errors and now is
excellently by adding like complete
random noise it's like this model
dropped to 3% of errors and so when you
see this it's like you should see huh
like this this can't be possibly right
right is like all I did was adding
random numbers and so the point here the
first point is you should be careful
it's not because you add more features
to your model that your model is going
to get better or to be more precise is
like the more data you give to a des
learning algorithm the more it has to
actually fit the data that you give and
it's going to get better and better and
better at modeling the data you give it
but it's a that doesn't mean that it's
going to work for data it has not seen
before so now it's running way too much
it's kind of learning details which are
completely irrelevant about the data
will give it so the real proof would be
like what I would really want to see is
like how is that model performing on
data which I didn't use for training so
this is what I'm going to use down there
first thing this is a common problem if
that problem is called overfitting and
this is one of the reasons why machine
learning is not quite as easy as just
dumping more features to the model and
more data into the model so here I'm
going to do the same exact thing I'm
going to add like these 10 fake features
I'm going to take the input but now I'm
going to use the same algorithm but now
instead of using like 13 13 hundred
passengers I'm going to learn on only
1,000 of them like the 1,000 first ones
and then I'm going to compute the
quality metric on two different parts
I'm going to compute the quality metric
on the data I used for training and I'm
going to compute it under there I didn't
use for training and so this should give
me now a good estimate for is my model
any good so if I do this like it's
showing me that on the training set that
model gives me like roughly two percent
error so I still get the same spurious
result but
if I run it on the rest of the data
which has not been used for training my
error is a jumps back up at like close
to 40% so this is a the method I'm
showing here is called cross-validation
and that's at the core of how people
that he did the first thing you need is
the key metric to know what is a good
model and what is a bad model and a lot
of time will be spent at trying in
features putting new features and see if
the model is better on the validation
set so that method which is called
cross-validation and this serves the
same purpose as a test harness for a
business application except that it's
just a bit different like you will not
get something we shall do it works it
doesn't work it will do it works a bit
better and so that's so that's one of
the parts that's why it's not that easy
to do machine learning you just don't do
add more numbers to the model like you
have to pay lots of things can go
terribly wrong is that what I'm trying
to say so let's look at this you have
yeah so let me go back to the slides for
a second so bigger point here is that
the fact that you have good results on
your training set means nothing it
really means like the algorithm was very
good at learning that particular data
set so if you have results which are too
good to be true they are probably too
good to be true like the if your model
is really good at predicting that you
should really want or if you are cheaper
editing right the second point is like
the fact that you have more data is
paradoxically not always going to help
you it's like at the same time you have
more data to learn stuff but you also
have more noise which your model could
pick up as well so you have to pay
attention to that therefore you use
something called validation or cross
validation where you're going to split
your data in training and validation and
you're never really done and you're
going to want to iterate like maybe
other features if it works if it works
on the validation set remove it
transform it do that type of thing this
is where I'm going to move to the last
part of the talk which was about data
types and pipes so what I showed you so
far is like pretty much every script and
every polymer is looking at is real
looking exactly the same way I have data
somewhere CSV it's equal it's whatever
you want first thing is I want to read
it from somewhere in memory then out of
this I want to extract features which
are the numbers I care about then I want
to pass it to an algorithm and then I
want to validate it that's all there was
all the things we did is that and out of
this the pot where you're going to spend
the most time is Rhys step to extract
the features like this is where you're
going to think a lot like what is it
that could be useful making predictions
the rest is pretty mechanical and so now
the problem I have with the script I had
was it's going to be painful because
like I don't really have a domain
visible like I have an array I'm going
to add things remove things like this is
not really a good representation of the
domain I'm trying to represent here and
so as a result I will have a lot of
friction in my script I will not be able
to iterate I'm going to be a bit stuck
like so this is not a good place to be
where I would really like to do is
something along these lines what I would
like to do is like to say I'm going to
use features which I don't see in my
script so far a feature takes a
passenger and gives me floats features
could be categorical in America and I
want to say this particular model will
use these features and I want to use a
logistic regression and learn so and
right now my script is doing that but
it's carefully hidden in a lot of craft
a lot of array manipulation and so
that's not a good place to be so this is
where I find a language like F sharp is
actually helpful because it statically
typed I can do things like I use type
for a greater good so I'm still going to
do the same thing in the beginning I'm
still going to load data that's fine
yeah my point here was that that type of
thing like when I start to have big
areas like this like you can as a soft
angio you probably see this and like
this is a ripe for bugs like if I start
to change this extraction I can't see
anything about what I'm doing here I
don't see that I'm working with where
the passenger embarked I can't see that
it's a categorical that is a numerical
value and all this stuff so how could I
use like something like a functional
language to make that situation a bit
better I could do this by remarking a
couple of things like here what I did
was I'm using a map and the first map
I'm doing is I'm taking passengers and
for each passenger I'm giving you back
an array now the second way I could look
at this is that can redoing a map in a
map so the first map is that I'm taking
a passenger extracting numbers and is no
worse than RI so what I want is I want
to really pass features and for that
passenger I want to map the features to
it so that it gives me a flat away so
the way it would look is something along
these lines
what I could say is like a now a feature
is going to be a function which takes a
passenger and gives me back a float if I
do that then I can say for it so sex is
a feature that's a function fare is a
feature that's a function age is the
feature that's a function as well so all
these things are functions which take a
passenger and give me back a float and
if I do this now I can clean up things
quite nicely because I can say my model
is not going to use like this ugly array
in line I can say my model is a compose
the features which are sex fare and age
if I look at this in the script I know
exactly what I'm doing so I can do that
and now I can actually use that map and
say like you know what I can do with it
I can say take a passenger take a
collection features and encode it by
applying with a map all the features
extracted to give me a flat vector so I
do this and so for instance I can do
things now like this take the input
every passenger map it by applying the
end code which is a map which applies
the features and if I do this now in one
line I have this whole thing where I'm
getting my flat vector so this is
already a pretty nice clean up because I
went from a mess
to something where I can clearly see I'm
using three features I can add one I can
remove one and all the alignment of the
data is going to be done for me
so I'm a happy camper already but
because because they kept shot people
tend to be a bit obsessive about types
is like there is still something which
is a bugging me here and the thing which
is bugging me here is like
it's still related to domain is the
quote I said in the beginning was I
really have like numerical features and
categorical features and so that's not
visible anywhere in my in my model so I
would want to be able to model that so
I'm going to go a bit so this is the
type of public this is ugly because I
don't know if I forgot one I can't
really see if I have like three of them
seven of them so what I would want is
that we want to have a clear visible
notion of like categorical numerical and
what would I do because I'm a natural
person I will create a type because like
if you have a problem at the type and so
here I will say really a feature could
be two things it could be continuous oh
it could be categorical continuous will
give me a number and category call will
give me like say three cases and which
case is actually active so now if I do
this I will skip I will skip a bit on
the details but I will show you the
final result like what I end up with
this is something along these lines I
can write a nice clean DSL where I'm
going to say like purple-topped this is
here so still I can now use my data I
can use my sample so this is all good
but now is like using the DSL wrote I
can write something which looks like
that I can say I will use a passenger
for a passenger I'm going to take age
and this is a continuous feature for
embarked this is a categorical feature
and I have actually three cases it
should match with the C and s oq f it's
class it's something which could be one
two or three and if it's sex
it could be male and female so now that
if I look at this not only do I have a
very clear view and what features did I
use I see also equity suppose too much
I have cases which are like three three
either a number or like three categories
and now if I run this I can also pass it
to the logistic I wrote a couple of
adapters I also also didn't load the
dependency needed so like this is
feeling miserable so we'll ask you to
trust me here but like the the next
thing I got with types is like instead
of this ungodly mass of arrays where
pull things from everywhere I can now
state very clearly this is my model this
is the features I want to use and this
is the type of features I have so this
is going to help me avoid a lot of
problems I had I had another I had
another piece I wanted to show but I
think I'm running slightly short on time
so I'm not going to be able to do the
full demo here
but like yeah so the the the point here
I was trying to make is like compared to
a dynamic language like our like Python
like all these things is like one thing
which is nice with a chopstick first it
makes things easy like loading data was
pretty easy and then I can use types to
a clarify intent like instead of having
numbers which are in RS I see this is a
categorical feature this is this type of
feature it will help me prevent mistakes
because I have types so the compiler is
suddenly playing with me and not against
me I don't have runtime exceptions like
either it builds an environment of good
shape or it doesn't and then it's like I
know it's not even worth running it and
it makes just very because I have typed
now I can start to eat rate much much
faster on changing my model modifying
them combining things together so I
think types are awesome this is not
surprising from another person in
particular I love text I'm going to skip
on the part about pipes I'm just going
to mention briefly like the point I was
going to meet with that I had to demo
bit and we're going to do the demo so
the other question you could ask at that
point is like putt putt putt like you
are working from a scripting environment
scripts are great what happens if you
have a lot of data and if you have a lot
of data of course you might have a
problem like I'm not going to be able to
open petabytes of CSV files on this tiny
machine I will have only two CPUs that's
not awesome so what do you do in this
case and what you do in this case is
like you so what you do in this case
first like you probably don't have big
data like people tend to obsess about
big data I have seen it once in my life
on a project but in the possible case
you have big data using a statically
typed functional language is actually a
pretty nice because that most of the
work we do is using things like filters
and maps and if you consider an
operation like a map and a filter this
is an operation which is trivial to
paralyze the reason for this is like in
this type of representation I have
absolutely no shared state what I did
here is that I take a passenger that
passenger can apply function in code and
I give you back your vector passenger
and code vector and each of them
operating completely separately right so
I can do this in that single machine or
in the single core but I could also
split it in two datasets or two cores
and that could work just the same or 27
and all of this and so if you have this
type of structure using a using map
filter and all this stuff this is the
type of stuff which is a bond
be parallelized and as it happens is
like the parallelism as so this is where
I'm not going to show it I'm going to
but it's very easy to take the type of
problems I shoot so far and first that
you could run it on two cores and four
cores and 16 cores with a literally in
no effort and with using a library like
embrace you could also take that and
scale it so that I can take it from a
scripting environment and ship it an
azure cluster and run it on like 50
machines if I want so the good news is
like by default walk on your script
environment locally and this would be
great and if it's not enough then it's
like converting it into a way which you
can scale it across multiple machines
will not be difficult and so you're
still in the good shape conclusion first
like what I hope I conveyed was like
yeah there is not much magic in machine
learning like machine learning is way
less complicated than what people make
it sound the other point is that it is
about coding and so that's the point
which is important to me personally
because like machine learning is a
really really fun place to be and people
tend to have this impression that this
is for statisticians this is for
mathematicians the reason it's called
machinery and not statistics is because
half of it is about using mathematics
and half of it is about running code in
production and so if you take
statisticians they're good at the first
spot and not this is you the second one
and so these people need your help so if
you like distributed computing if you
like algorithms and all these things and
you don't hate math I really really
strongly encourage you to look at this
because it's packed with really really
fun problems and it's the right time to
look at it too people are looking for
these people and otherwise and the f
shop side is a people tend to describe F
sharp as a functional language it's like
one thing which I think is understated
is F sharp is an absolutely phenomenal
scripting language I hope I give you a
bit of sense for this is like a it's an
amazing language because most scripting
languages are untyped
this is a statically typed language
which has all the flexibility of an
typed language so I love F sharp as a
scripting language I love it too because
it's one of the few languages where I
can write code in a script and then I
can take that script and I can pretty
much compile it and dump it as a DLL
into your running net code and so that's
one of the big reasons if if you have a
code base which is in the 13 C shop
considering a language like F drop is
great because that you can now write
your script
do machine learning and then convert it
to code which you can actually run in
production and which is a typically
pinpoint in lots of areas and otherwise
a greater determination machine learning
works very well with the functional
stuff
another type system is your friend so so
this is what I had so I hope you got
something out of the talk like thank you
very much feel free to ask me questions
afterwards I usually cannot shut up
about a chop and machine learning so if
you want to talk about these things
I'll be around that's what I had so I
can find me on Twitter and all this
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>