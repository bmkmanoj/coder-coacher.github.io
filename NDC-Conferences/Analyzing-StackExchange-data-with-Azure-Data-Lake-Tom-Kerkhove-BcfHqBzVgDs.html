<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Analyzing StackExchange data with Azure Data Lake - Tom Kerkhove | Coder Coacher - Coaching Coders</title><meta content="Analyzing StackExchange data with Azure Data Lake - Tom Kerkhove - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Analyzing StackExchange data with Azure Data Lake - Tom Kerkhove</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BcfHqBzVgDs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today this session I will talk about
analyzing Stack Exchange with Azure data
Lake
so first I'm Tom Kharkov I'm from
Belgium I'm an azure consultant at codit
which is a company in Europe I'm based
in in Belgium so I'm also an agile MVP
and an advisor and I'm part of the agile
user group in Belgium I wrote a little
white paper on the Internet of Things if
you're interested you can find it over
there so what's on the agenda first a
quick introduction to Azure data Lake
and then we'll dive into what is Azure
data Lake store and analytics and then
I'll use the stack exchange to show you
how you can combine them and how easy it
is for me as a developer another data
scientists to write queries that will
run on larger data sets so first big
data it's a big buzzword and if you
start looking into it you have Hadoop
with all these other kind of
technologies which are open source and
it's a rapidly evolving ecosystem
so first the no-brain that will be let's
go open source and use these components
but as a c-sharp and sequel developer
working with the Microsoft stack that
brings a couple of challenges so the
first is most of them support sequel
like languages but not the sequel
language we're used to and also the
custom code well there's no c-sharp
support so I need to learn a new
language need to maintain that language
as well so it's another challenge it's
rapidly evolving and also I need to spin
a plus this operators clusters need to
know how to set them up it's also an
extra burden that I as a developer don't
want last but not least it's typically
with Linux which is also not something a
lot of Microsoft developers use as of
today it's evolving more towards Linux
as well but not everyone
is used to it so it's yet another hurdle
to jump so Microsoft has an alternative
so they have what's called Azure data
Lake which is a kind of umbrella of
three services on the left you have data
Lake store which is in web HDFS data
store where basically there is no size
limitations you can put whatever data in
it that you want then you have data like
analytics which is which is an analytic
job service where I can just write
scripts give it to data like analytics
and they'll run it for me I don't need
to provision anything and then last but
not least you have hdinsight where you
can provision those open source
technologies if you want on a cluster so
this is what was available before where
you still have to maintain the cluster
shut it up operate it and you use the
open source but for me it's it's a big
jump to go to Big Data so let's dive
into the details so data like store like
I said is built on web HDFS compatible
and it can store any sizes of the files
and it has no limitation on the total
size so it's basically an infinite
storage if you will but you know
infinite doesn't exist you can also
store any formats
it's a schema less data store so you
don't need to define the schema out
front just store the files and it's a
write once and read many principle so
it's optimized for running analytics
chops on top of it it also has
enterprise-grade security which we'll
see later it's built on Azure ad so not
on fancy authentication keys just using
the security we already have so it's
basically the big data store and a
if you don't want to run your own
cluster so it's fairly easy and as you
can see it's the center of your big data
ecosystem if you are working with the
cloud where on the left
you have all these other services which
you can use to ingest data on top you
can have multiple services to process
the data so we'll use Azure data like
analytics then you can visualize it with
other services where we will use power
bi and then on the right you can also
download it or move it to another data
store if you need so basically it's all
built around data like store a small
caveat what is the data like and how
does it compare with the data warehouse
because it's not the same a data
warehouse is typically a warehouse which
is also a central place to store your
data but it's a central place with
structured data so this means that you
need to define the schema of your data
up front you need to create those
extract transform and load pipelines to
make sure that the data is in the format
that is required so that's it's been
around for a while a lot of companies
use this and then on the other side you
have data like where you just basically
dump all the data in the raw format and
the data lake itself
this can be unstructured structured and
semi-structured and you don't need to
define any schema okay and then the data
scientists just go in the data they can
select the data they are interested in
check what the schema and then process
it this has the advantage that you don't
get rid of the of certain data
attributes when you're storing it
because with the data warehouse you need
to define the schema and it can happen
that as of today you don't need a
certain data field within two years she
might but since you had to transform the
data set you already lost all that
information so you can't use it anymore
but the downside of the data like is
since it doesn't have any structure by
nature it can be a data swamp which is
not nice so if we see this picture of
Martin Fowler which wrote a very nice
article about this you can see on the
Left data warehouse where you basically
clean the data
before you store it and then the
analysis is known based on that schema
so basically you prepare the data for
the analysts where on the right you have
the data lake where you just have
everything in one big data store and
they just transform the data they need
for their report visualizations but
whatsoever you can basically compare it
with fishing where on the Left if you
want to eat a trout you go to the data
warehouse and you buy the trout it's
already prepared and everything well on
the right you can still go fishing in
the data Lake where you can select the
fish you want and cook it how you want
and prepare it how you want but if if
the lake is full and you create a data
swamp your fish will die and the data
will be useless it's a small comparison
you can make so a security for data Lake
story can use role based access control
for the store itself and you can grant
users and group access to certain
folders and files so you can really lock
down who has access to what and really
control this it also has a firewall
which is off by default unfortunately
but this allows you to really protect
your data set from the inside and only
allow the certain IP addresses to get
access to it and you can also add
encryption at rest where either
Microsoft controls the keys or you
provide your own with Azure key vault ok
and pricing here's a big overview but
basically you use a pay-as-you-go model
or you use a monthly commitment package
which makes it up to 33% cheaper but
then you need to pay those packages you
basically paper to reside per
transaction for read and write and as
you can see read is cheaper why because
it's built for analytic
and they want to motivate you to less
frequently you write to the data store
okay and then a small comparison when I
should blob storage because you can use
that as well you can see that while data
like store doesn't have any limitations
as if storage has them they're very big
so I think you can have up to 200 now
100 accounts per subscription up to
hundreds of terabytes so it also very
big but you need to separate them across
blob storages then security it has the
ad support like I mentioned while in
blob storage II need to use SAS tokens
which is less secure but the pricing for
data like stored is more expensive I
think the only storage option that is
more expensive is a globally redundant
storage with read access so that's the
most expensive one redundancy is a
different topic because data like story
don't have access you don't control how
redundant it is I think it's only
locally redundant so if one region goes
down you currently lose your data which
is not a not really nice well with blob
storage you can basically control it
locally zone geo read access scalability
well data like store is built for
analytics so it's really optimized for
reads less for writes while blob storage
basically is built for the two and they
both have integration with data factory
data catalog and hdinsight so from that
perspective there's the same okay we're
almost through the introduction data
like analytics is the most interesting
one to me because you basically have a
serverless kind of analytic service
where you write the jobs you write the
scripts you give it to the service the
service interpret
them optimizes them shares them across
multiple notes and they run it all for
you
you basically just give them the script
and to do that you can use you sequel
which is a mix of T sequel language and
c-sharp language so basically basically
I can just write the sequel scripts if I
need to have some extensibility I can
just use my c-sharp scripts you
provision a certain amount of analytics
units once you submit the job which
basically defines how many nodes you
want and then you pay per processing
time per analytics unit so the more
units you use the faster the job will be
completed but you will pay a lot more
which makes sense
but we'll see that later on so basically
I have these kinds of data stores that
are supported so data like store
obviously which is also a required one
because they store metadata for the jobs
and then you can also use blob storage
for query and write and then any kind of
sequel datastore and Azure for reads and
data like analytics also has built-in
partition tables if you need those which
can come in very handy so basically you
just use the correct data store you want
if you for example have a lot of
telemetry data and data Lake but you
need to correlate them with a different
data set which is in another database
you can just do that as well you're not
limited to data Lake store and then this
is an example of a UC cool script where
at the top you basically extract the
data from a data set where you also
define the schema of that data set so
this is where you need to do it not when
you ingest the data between once you
want to extract it and you could say I
want to use CSV TSV or you can write
your own which we will do then you can
just transform and analyze the data how
you
using see a sequel syntax in line for
example over here or over here or you
can also use method calls however you
want and then at the bottom you
basically output the result of your
scripts to another location so here it
is a data Lake store but you can also do
it to to blob storage and then you can
order by or whatever and then you can
choose how you want to output it here
it's with the CSV but again you can use
TSV or create around output so this is a
very basic example script but you can
really go as crazy as you want but this
is the way you need to do it basically
extract transform analyze and output it
so extensibility like I said you can use
n9c sharp oregon creatives custom
defined functions operators and
aggregators depending on your scenario
and those operators are also a couple of
them so we will create our own
extractive to extract it from the data
store then you can have processes
reduces combiner suppliers and output
this all with their own news case but
they all come in handy this is a big
overview of the metadata model i won't
go in detail because of the time but you
can basically see that you have an as a
data like analytics catalog which is
your instance in Azure under which you
can have multiple databases basically
like you have with the database server
you can also have multiple databases and
then limit who has access to which
database and then under there you have
also data sources that are linked schema
stables etc and also c-sharp assemblies
that can be used in the scripts for that
extensibility okay
if you provision you seek to see you
sequel script it's basically compiled
and then they define how the processing
should look like and then they optimize
the script for
you by creating the optimized plan they
use the job scheduler
to keep the job for you then the job
manager will divide the work across
multiple vortexes so that's basically
your data set which they divide in
smaller pieces and then they will just
distribute it across all the nodes once
they are all completed they will come
back to the job manager and then your
job is finished and the output is in
data like stored if you want to have a
more detailed overview of this Michael
Royce did a very good presentation on
tuning and optimizing new sequel scripts
where he goes more into depth on this
topic so if you have more advanced
scripts this is really useful to have ok
and this is an example of how you can
write a simple script and how they
optimize it into all these pieces for
you and you don't need to know how it
all works just submit it and they do the
magic for you then you can see the all
the states of a job basically you create
a new one where they compile the script
they cue it and they schedule it and it
starts processing the transient and it
ends so we can all follow this along in
the tooling so almost there security we
also have our box support so you can
support the instance in Azure where you
can select who can manage the instance
itself it also has a firewall and you
can also have access control on the
catalog or the database like I mentioned
so we can lock down one database for
user a what a user B only has access to
the other one and you can also use
resource management because it's not
that cheap you can also make sure that
if somebody new starts they don't
provision 100 analytics units while they
only need 10 so you can use these groups
where you can say ok these are new
developers can only provision 5
analytics units well if it's for
production we want to have more
analytics units and a higher priority
and the job service
and then on an account level you can
also assign the maximum analytics units
maximum concurrent jobs and how long the
metadata of the job should be available
and for pricing you basically buy pay 28
pennies per minute of an analytics unit
which is not that much but like I said
the more you sign the more expensive it
gets and it's not because you assign
more units that script will be faster it
all depends on on how the script works
the tooling also provides an optimizer
where you can see
okay I've provisioned 20 Analytics units
but basically the service only use 10 so
if I submit it again we maybe should
optimize to 10 units because we overpaid
for 10 units and here we also have the
monthly commitment packages okay a lot
of talk so let's show it in action
so what we will do is Stack Exchange has
over twenty two hundred and eighty
websites including the meta websites of
which they have 150 gigabytes of data
they publish so we'll use that data set
as an example where we will get the data
by downloading it I actually already did
that because we also need to extract the
zip files upload those and data like so
I did that already but I'll show you the
script I used it's fairly easy and then
once it's there we will start merging
all the datasets for each website and to
an aggregated file and then based on
that aggregated file we will try to
determine how many people filled in the
location on their profile which actually
matches to a country and then we will
visualize that in power bi so this is a
small setup of what we used
so we have archive.org which is the
website where they publish this so I
provision the VM and North Europe where
I download all the the zip files I
extract them on an SSD of one terabyte
so I had a lot of XML files and then I
use the PowerShell script to upload it
to data Lake store then I used two
scripts to aggregate them and run the
business logic on top of it and then
power bi uses uses the data of data like
store ok so let's not even do it first
this is the website for those are
interested its archive.org but somehow
this thing is in front of it where you
basically have every I think it's six
months a new data set of all the data
and if you have a look they basically
have one zip file app or website
except for Stack Exchange because it's
way too big so if you have a look over
here you can see for example this guy
all the posts on Stack Overflow is 18
gigabytes gigabytes and a compressed
version so it's not that small so what
did I do
I wrote a small PowerShell script which
basically authenticates with Azure ad
big with as your research manager sorry
yes how do you do any Visual Studio
codes
no I don't work on code ah yes it does
sorry okay
so first we will authenticate and assign
the subscription we want which basically
just authenticates what as a resource
manager and then it will just use the
azure data like store commandlets to say
okay sorry wrong script that was to
download because I actually had to move
one from a data Lake to another data
like so I first had to download it
okay the correct script so basically I
just authenticate and then I want to
upload them all so what I did was I
specified the data like store account
name the local folder on my VM and then
the destination folder on my data Lake
which was basically the routes so if we
have a look I basically just clean up
the parameters first and then for each
file
I just loop over all the folders then
remove slashes because data Lake doesn't
like them and then we start uploading so
if it's a folder I just check okay does
it already already exist if it doesn't I
just create a new a Jairam data like
store item where I say okay it's a
folder and this is the destination if it
isn't and then I just do an import of an
item where I say okay this is the
account again this is the path to my
file this is where I where it should end
up and then I force it and that's all I
had to do so basically just iterate over
all the files if it's a folder create a
folder and data like if it's not just
import a file that's all I had to do
then you can see I imported three data
set which result in 620 gigabytes and if
I go
over here you can see I have a three
data Lake store accounts where the last
one contains all my stock exchange data
actually if I have a look at that one
you can see that there is a data
Explorer where you can basically just
browse the whole data store and download
all the files get previews etc so here
you can see all the websites etc I can
if somebody needs to gain access I can
do it over here okay so this is the data
set we will be working on so for this we
need to create a new data like store
accounts to publish our result for power
bi because I don't want to use the other
one so you just give it a name for
example NDC demo use the resource group
that's better
so any research group the region I want
to do and then you can select the
encryption settings so either I can
choose data like store or key vault and
because I like keyboard volt more I just
then select the keyboard I want to use i
select the encryption key so let's use
this one
I select encryption key I want to use
and then I hit OK but because it takes a
while I'm going to skip this and use
this one for example
and once you provision it it gives me
this warning
why because I should keyboard is using
Azure ad as an authentication mechanism
and basically over here I first need to
say okay this data like store account I
grant this access to my date to my a
jerky vault so over here you can see my
vault
my subscription and the key I want to
use and then you basically grant the
permissions
okay but I'm not going to do that
because I already have one over here and
as I mentioned it has a day it has a
firewall which is off by default so this
is something I would typically turn on
but for the sake of the demo we don't
care for the rest there's nothing really
fancy in here we have a data Explorer
that I showed we have some metrics which
we which we found on the dashboards and
that's basically it so no alerts that's
it so if you need to operate this the
only thing you have is an overview of
the amount of data
okay so aggregating the data our data is
in data lake we can now start creating a
you sequel script that will run on top
of all these websites and that will
output the result set in this data Lake
store for that we need a data like
analytics instance which I'll I already
created oh sorry
which are already created as this one
and if I go over here I can see that
there's a default data source why is
this every data like analytics unit
needs a data like store account because
it needs to store sure metadata about
the service and this is c-sharp
assemblies or also the history about
jobs files basically just the data store
for your data like analytics but we can
also add another data source which
allows us to use it in in the you sequel
scripts so if you want to use one and
you reduce equal scripts you need to add
it here first
because otherwise it will fail so we
will add one by account name and I just
select the data like the stock exchange
data like store the one we saw so I
click add and now I can add this to my
use sequel scripts
so this guy is ready to go now we go to
visual studio sorry I forgot to show you
the data set so as you can see this is
an XML for data a big data processing
xml's are not really nice why because
they have a certain structure if I want
to tear this file apart into multiple
pieces to separate it across multiple
notes this is actually a problem why
because if I separate it into two pieces
my route notes will be not in the same
file as the the ending root note so
basically once we the first step and the
aggregation is also transformation to a
CSV file so what we will do is we will
write a custom extractor
because this is not supported by default
and then in that custom extracted we
will also say each file should be
processed in the same format so that we
can keep the whole structure of the XML
so here it's just a root node with all
rows in it but imagine if you have
multiple sub structures that would also
be broken so basically what you need to
do is you just need to just pull in a
nougat package which is this guy and you
basically just derive implement I
extracted and then you need to provide
implement this extract methods and also
notice over here this is the flag
telling data like analytics not to
separate file into multiple pieces
because of the structure this is
actually a very funny one to find so
don't ask me how I know so basically
what you get in is just an input and an
output where we will just use an XM
read from donut where we load the stream
of the input and then we just do the XML
parsing like we do and that's where we
check okay if it's a row then we know
it's an entry for our outputs and then
we just loop over all the over the
schema and then we interpret the data
okay and then if it's null then we just
set a default value because we need to
have a default for each row because
otherwise our CSV is messed up and then
we check if it's a string then we
simplify it why do we do this because we
read it what is it
okay we remove these because they don't
like it and certainly if it's a CSV that
uses a comma I don't want to use a comma
in the data fields I just change it to a
you know the English word a comma with a
dot on top of it
semicolon thank you okay so basically we
just clean the data a bit and there's a
byte boundary so if it's exceeds that we
just adapt that and then we just say
okay output this is in the name of the
column and this is the value if it's not
a string for example an int so here this
column basically refers to the script
where we will define the scheme of the
fowl and then we just convert it to the
value okay and then we just return every
row okay if you want to play with this
later on it will be available so I'm
going pretty quick but you will be able
to do this afterwards so for example
this guy if we want to aggregate all the
users which we will need for the demo we
start first with referencing to the
assembly which contains our custom
extractor which will be deployed in data
lake and a letter
and there we say okay go to this data
lake store account where I can use these
kinds of place holders to loop over all
the folders instead of having to add all
of them I just say okay let's call this
dump name and I can use this mi output
as well which is actually very nice and
then the source is the name of the
website and my folder structure which
will also be included in in the result
set and then I just referred to my
custom extractor and captured this in a
variable and then I just output this one
to a CSV file on my default data like
store so that's why it's a shorthand
notation well over here I need to prefix
the whole name that's the difference
okay for the sake of the demo we will
not use the three data sets why because
we will have a lot of duplicate users
because it's the same data but just a
slice of six months added to this so I
have another one where I don't use the
dump aim but basically fix the June data
set but before we want to do this we
need to deploy our custom code first so
there's one option where you can run au
sequel script where you say okay in
database master drop the old version
first because this is the new version
where we do create assembly and give it
a name and then we can point to a data
like store folder where we uploaded the
DLL an alternative is we go to visual
studio and we just say okay register
this DLL and then I don't need to upload
it come on wake up there.this so I
choose my data like store a yeah and the
italic analytics account sorry and I
used my user and it will ask me which
database so for the sake of
the demo I'll just use the master
database but if you want to use another
one you select another one here and then
I give it a name if there was already
another one I could select this this
checkbox to override it and then I
choose the amount of data lakes units I
want to use for this but because it's
just registering a DLL I could provision
five but it will only use one so then I
hit submit and let's hope the Internet
is very nice okay
so now what you can see on the left is
actually the flow of the of the job so
first it's preparing it what is it doing
now so it's compiling the script
checking theories in custom codes well
is that custom code deployed on on the
data like analytics account you are
referring to a certain folder do I know
that datasource etc so once everything
is compiled it will queued the job for
processing and then it will be run and
then it's finalized so by now you see
it's finalized but it's only registering
the DLL so I don't have a job graph what
you can also see over here is the
duration of the compilation of how long
it queued basically not and then how
long it took to run it's only three
seconds but also over here you can see
again the parallelism which is the
analytics units and then the priority so
the priority is how higher the amount
the less Heights and priority actually
so priority one is the most important
one
okay so now my custom code is there now
I can run the aggregate script so I can
just select NDC Sydney I can submit it
so now it's compiling that script
actually let me go to the data explorer
to see the results so now and come on
and the data Explorer we see that we
have a reference data folder which
contains a foul we'll use during the the
business logic let's say then we have
the catalog and the system folder which
is the metadata for the service itself
this is only because this is the default
data store and then once the job is done
see now it's running I use parallelism 5
which was not a good idea I should have
used more ok ok so now you see we have
328 stages and 328 vertices so like I
mentioned four dishes are smaller pieces
of data sets but because in my custom
extractor I said I want to use a Tomic
processing it just uses one for each
file so basically each stage as one file
and now if I do expand all expand oh
yeah you can see this why because we
have all of these files but if i zoom in
yeah you can see that there's just one
aggregate shape to aggregate all these
files and then it's outputting it to my
user CSV okay this will take a while
let's cancel it and provision more
analytics units
yeah okay so it's canceled
let's try again okay in the meantime
let's go through the next script so we
don't need this one anymore no this one
not okay so after that we will create a
small script - so what will we do we
will take all the users in the
aggregated file check their profile
there's a location in there and then we
will check doesn't match the name of the
country so it needs to exactly match it
for the sake of the demo but in reality
you would check does it match the
country and sent three or if it has more
information in the field for example I
live in Belgium it should also detect
Belgium and link it about but for the
sake of the demo we won't do this but
that means we need to have some
reference data that contains all the
files and all the countries in the world
and for that there's a data set based on
the ISO 3166 that contains all the
countries in the world where I also
added a field friendly name why because
in that standard the official name for
the USA and the UK are hold on a second
United States of America where I added
United States and then for the UK United
Kingdom of Great Britain and Northern
Ireland so there I just added a United
Kingdom as well and basically I should
also add Ireland but okay
because I don't take a lot of people
using that naming when they need to
provide a location so we will use these
friendly names map it to the user
location if we have a match we will add
it to that bucket if we can't resolve
the country we will add it to the not
found bucket
if they don't specify it we added to the
not specified bucket okay now we can
create a certain of visualization and
then we also do averages and certain
things so for that we first need to
again extract all the aggregated users
from our dataset so it's basically the
output of the previous job and then we
also load all the countries from our
reference data and as you can see it's
also on data like store in this folder
with his name etc and that's a CSV so
then we have two variables containing
our datasets and we can perform some
logic on top of it now first have a look
moving on so because it's case sensitive
too to match the fields I also created a
cleansing method let's say where I just
add the locations and I just lowercase
everything so I can match the friendly
name with the location in a lower case I
could also use that existing c-sharp
assembly but each script also has a code
behind so if I go to the code-behind of
my script you can see I have a class
with a small method in here which just
takes a string if it's null or
whitespace I just remove it I just give
it back because I'm not interested if it
isn't then I move all the the trailing
and just remove all the spaces at the
beginning or the end and then I do just
to a to logger so I'm sure that the data
is the same before we match it I do this
for all the users and I do it only for
the location because I only need it
there and then for the countries I do it
for the name and the friendly name the
region and the Cyprian well actually I
just need a friendly name
whatever okay so my data set is as clean
note that I cleaned the data sets which
I already extracted so I'm not cleaning
it on the persistent data I'm doing it
in the let's call it in memory data okay
so I'm not adapting the original data
and then I can just use sequel like
language with c-sharp well over here I'm
using inline c-sharp if there's no
region sorry if the region is null then
I add not applicable and otherwise I use
the region
I do account I use an average while
average is not the best fit but okay
then I just do a join on both the users
and the countries where the location and
the friendly name are the same and I
drew by region sup region and name so
here I'm basically creating an overview
of all the countries per region per
Cyprian to determine how many users live
in Australia for example and what the
average reputation is okay and then the
second one is where I divide them in the
buckets so this one will include all the
users then this one so I don't filter
over here and then the ones that are not
found I do a left join where I also
check if the friendly name is no and the
location is different from an empty
string and to make sure that I don't
match the not specified ones as well
which I have over here which I also
Union by the way and here I also have an
inline c-sharp expression and then I
Union with all the found users where
they are actually the same
so no rocket science but also shows you
that I can just write a small script on
top of the big data big data set and
then I just output it to two CS fees
problem is I need is this guy to finish
okay so really there
so then I just mmm
that was not smart if you just click
Submit it uses the previous yes 20
should be fine so now it's running that
one and over here so now this this one
is done but you can see that you can
just I need to load the profile first
and then I can come and then I can just
replay the whole processing so now it's
just yeah all of these boxes better than
the next up I'll show it as well where
you can see okay this step was taking a
lot of processing and then you can see
okay this is because we're aggregating
after blah blah blah and then you can
see where the bottlenecks are in your
script also I mentioned resources I can
see how many of the allocated analytics
units that were used so we can see the
most used ones were 20 so actually I
offered proficient 5 analytics units if
I schedule this kind of job in the
future I can only use 20 because we can
save money with that which is actually
important and then you have this modeler
so let's say I had 10 units
it shows you a prediction it's not not
certain that you will get this result
but that's what they think will be the
outcome so this is actually very
interesting if you need to model your
script to optimize with the cost ok so
let's see what this one is it's
refreshing
ok we're already at half and you can see
that it's already a more fancy graph
well I don't need to know how to model
this I just need to write the script
they do logic for me don't need to
profession any clusters just write the
business logic and run it okay
okay it's finished and now you can see
that on the left side how many bytes
it's Brett and how many were written and
also that we separated the one big
aggregated user file across 37 vertices
okay and if I use this one I need to
load profile' first but instead of
progress you also have these others - to
view the processing so if I use
computation time for example you can now
see what the bottlenecks are so
extracting really helpful if you need to
troubleshoot certain things and then if
we go back to the resource usage
you see I provision 20 well actually I
could have used 10 or less see 12 would
have been perfect but that means if we
go back to our datastore we have the
aggregated data folder where we have
this aggregated users so it's only three
0.66 gigabytes but if you if I would
have run on all the data sets or used
other files it would be bigger because
the users is actually pretty small but
here I can also have a small overview so
on the left was the source so the name
of the of the website and you can see
that they have these test users across
the same all the websites which all live
on the on the server farm so you can get
it
I won't download the file but that's how
you can also check the result of your
data set and then
output of the reports is now over here
and I can just use power bi so I'll just
quickly first I need you where I and
then I can just go over here select data
like store I actually need to speed up a
bit and then you need to authenticate
why because it's again using Azure ad
for the security to know if I have
permissions to view this
okay and then I can connect to the data
like store account so now it gives me
this so basically it's just a folder
structure which is not the data set I
want I can edit it and it goes to this
modeler come on
Australian Internet okay I'll let it
load there Wow I'll proceed with the
slides first
okay so operations which is important
data like stored like I mentioned we
have the graphs for storage read/write
ingress/egress
but we don't have metrics and we don't
have alerts which can be a problem and
then we also have all the time request
logs which can be interesting but
another crucial for certain scenarios
and then we have data like analytics
where we also have the graphs where you
can see the amount of failed jobs
succeeded jobs Council jobs etc and the
evolution of the used analytics unit
time and we also have metrics of those
but we don't have a lurch on it which is
not perfect in my opinion because I want
to have an alert if a job fails for
example if I schedule certain jobs on
the daily basis I don't want to log in
and check it every morning
so tooling I showed Visual Studio where
you have also a store explorer where you
can just connect to your store and
browse it over there so you don't need
to log in the portal and I also showed
you the job visualizer where you can
just troubleshoot your jobs and to a
certain extent you can do the same in
the portal but the visual studio tooling
as provides a more richer capabilities
if you like visual studio codes you can
also use it
to run the unit tests on the
extensibility for example which I didn't
show you or just use the local local
execution because yeah if you want to
save money you'd want to test your
scripts first locally which you can
actually also do in plain Visual Studio
an integration with other services the
most important one is data factory where
you can create these pipelines and
schedule them as well
so one scenario would be that you copy
data over into data Lake and then after
that it triggers a script to run on top
of data Lake store does something with
the output etc you can also use Azure
data catalog to annotate your data sets
to kind of document all the data sets
you have so that you are certain that
it's not becoming a data swamp and then
let's see how power bi is doing not good
okay so I want to have my report so I go
to this table and let's use the location
and then I can just use all this
information so that's not supposed to
happen okay I am NOT interested for all
the for all the results because we know
that but let's use this data set
normally you would give them nice names
but for the sake of the time I'll skip
that I'll also include an example and in
the github repo so you can see how you
can just use them but let's say I want
to use a map which is over here
and then I know by heart that this is
the location I should have used them the
names okay
they should be the location and then no
I'm using the wrong dataset no back yeah
this is the one I want skip the names
again because I like living on the edge
okay so location here we go here we go
and then I know this is the continent
and then this is the subcontinent
actually this is the amount of total
users so I can adapt the size or use the
reputation and then I can add some
tooltips okay so if I go over over the
u.s. really bad names where maximum
reputation is 612 and the maximum views
is 35 you can see that in in India
there's a lot more people using Stack
Exchange for example okay I'll just
leave the power bi part now but if
you're interested I can show you after
the session if you want to know more
about Azure data Lake they are preparing
best practices for now it's not public
yet but if you want contact me then you
can get a preview of this this is a book
I can really recommend by showing at
each other about all the things about
data analytics and asset he gives you a
whole overview of the data stores you
can use the kinds of analytics you can
use how you can visualize it prepare it
etc we
nice one then they also have a Microsoft
Virtual Academy introducing Azure data
Lake they have a github repo with all
the release notes and the announcements
and the you sequel documentation okay
and then a small summary yeah big data
is not a hype anymore but that's obvious
because data is the new currency if your
data like store you can use to analyze
today and tomorrow because it's data
Lake not the data warehouse so you're
not throwing away your valuable data and
beware of the data swamps you don't want
to have that and then data Lake
analytics you don't have the cluster
management which you also don't have for
store but you can just write your
scripts and give it so in a certain
sense it's also server less to include
the buzzwords I can reuse my existing
skills to write these scripts I don't
need to learn high4 whatsoever
don't need to learn Java because I don't
want to do that and I just pay for what
I use so I don't need to pay for all the
VMS of the cluster as well I just pay
for the computes so my advice would be
if you want to use big data and a
and you're a c-sharp or sequel developer
use data like store and analytics thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>