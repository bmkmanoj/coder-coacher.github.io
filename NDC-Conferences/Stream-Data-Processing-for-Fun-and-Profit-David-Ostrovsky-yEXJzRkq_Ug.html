<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stream Data Processing for Fun and Profit - David Ostrovsky | Coder Coacher - Coaching Coders</title><meta content="Stream Data Processing for Fun and Profit - David Ostrovsky - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stream Data Processing for Fun and Profit - David Ostrovsky</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yEXJzRkq_Ug" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right I think we can start thank you
thank you very much for coming I'm David
I'm a Solutions Architect at Couchbase
cadres develop say no sequel database
kind of like MongoDB which probably
you've heard of but not really much
better obviously the site what I'm
actually going to talk about today has
nothing to do with databases we're going
to be talking about processing streaming
data a lot of it I'd like to point out
there's only one person brave enough to
sit in the front row and I salute you
for that and the rest of you obviously
should feel shame alright so we've
before we start right I like to do a
little a little quiz to see who we have
in the room so how many people here
write code for a living everybody who
doesn't who does something else like
managing or architecture stuff nobody is
willing to admit to managing all right
good so who writes code in dotnet it's
like again everybody all right how about
Java it's like five people all right oh
this is going to beat our crowd because
I do have a lot of job of examples and I
don't say I didn't warn you
how about JavaScript it's like half the
room I don't know what you guys are
doing here honestly but all right okay
anything we're done this is Eric like
PHP Python Erlang alright cool so anyway
we're going to be talking about
streaming data and processing it in real
time and getting interesting stuff out
of it but obviously the first thing is
why do we even need this stuff right now
we can always write an application which
you know listens to a queue does
something and puts it in a database and
this is basically like 95% of
applications in the world that's
basically what they do they listen for
incoming messages do something with them
and put them in a database so when I say
streaming data and processing data first
of all what I mean is at large scale and
when I say large scale let's say who
knows how many twists happen every
second approximately let's look hundreds
who thinks it's a hundred nobody thinks
it's hundred a thousand about ten
thousands per seconds mind you first
thing all right twenty thousand a
hundred thousand tweets per second
what no it's more like 6000 per second
an average which is like half a billion
a day which is a lot of data and if you
consider that every tweet comes with
it's not just 140 characters right it's
actually a lot more that's metadata and
all that it's a lot of data to process
in a single day and even in a second
what which aren't actually a lot of data
how many Google searches happen every
second 10,000 100,000 a million Google
searches every second like no one's
willing to write so yeah I confuse
everybody through this it's more like
50,000 per second Google searches a lot
let's go higher YouTube video views
having a youtube video views 10,000 no
one's willing to actually play this game
anymore all right I'll just tell you so
it's around a hundred thousand YouTube
views per second and when I say views
you have to think about what actually
happens when a user goes and does a
Google search or does a YouTube video
view or goes to a website that you guys
built and assuming your do some kind of
metric collection or user tracking and a
bunch of stuff every visit actually is a
lot of operations in the background
right it's going to generate a bunch of
user interactions with the website which
you probably need to record somewhere
and eventually probably process any of
show ads to the user because that's what
the internet is basically about and
every one of those causes a whole chain
reaction of different things that you
have to process so a single tweet is one
event on the front end but it's hundreds
of events on the back end let's change
the process you need to find all the
users who follow that person and updates
their feeds and you need to do some
processing to discover trends and you
know separate out to the hashtags and
there's a bunch of stuff and by the way
I'm going to keep going back to twist
because it's just very convenient to
talk about tweets in a streaming context
but you can just mentally map this to
pretty much any domain you work in
whether it's you know doing for analysis
or online shopping or whatever you can
pretty much think of it in terms of
events coming in from some kind of edge
whether it's a device or a website going
into your back-end server system doing
some kind of processing with them and
the reason I'm talking about streaming
data is because not all data is created
equal right data loses value very
quickly right if you're if you're going
to be
monitoring again for example tweets if
you have a startup which does social
network monitoring this like 100
starters to do this right and they
create alerts and possibly you know
change bids on ads or do kind of some
kind of stuff in response to social
media events the difference between
doing is a second later or a now later
it is huge right so data actually can i
tape result and loses value very quickly
which is why we actually want to process
data in real time for a lot of
applications and again doing this at
scale is very difficult and there's a
lot of different tools which I'm going
to be talking about throughout this talk
so in general stream processing can be
divided into a couple of things and it's
not new right stream processing actually
has been around for a couple of decades
at least it originated as complex event
processing mostly in the stock market
where there was a lot of need for
analyzing trends and aggregating
different data merging different stock
feeds and trying to predict you know
what to do with stocks and that kind of
complex events processing was very
expensive and very processor and time
consuming but modern applications don't
really need that kind of complexity so
we have a whole separate domain which is
simple stream processing where you have
events coming in you have to do
something very simple with them it may
be update a database profile or maybe
run some kind of process in the
background as the response to that event
and then you forget about it or you
write it to some permanent storage so
these two worlds they're kind of close
together but they're actually separate
I'm mostly going to be talking about
stream simple stream processing and
we'll talk a little bit about complex
event processing a bit later because the
tools used there are different and
they're kind of outdated
so it's not very interesting for this
talk so let's talk about the types of
data processing you can do right and
when we talk about data processing we
can easily talk about the throughput of
how many things you can actually process
per second versus how quickly you can
process them right and if you think
about something like a database right
databases we expect them to process
hundreds maybe thousands of events per
second square ease and transactions and
we expect latencies in the second or ten
second range if you want to do something
much more high scale you know you go to
no sequel databases or in-memory
databases where you can processing on
hundreds of thousands of events at
millisecond latencies and the higher are
going to memory you can process things
quicker
and it becomes much more expensive and
much more volatile because keeping
things in memory is not as durable as
keeping them on disk if you go in the
other direction you talked about things
like Hadoop and spark how many people
have actually heard of a Dupin spark
pretty much everyone how many people
have use that in their company does not
necessarily yourselves like know you've
heard of it but no one except one person
is using it you have no Hadoop or spark
or any kind of data processing in your
organization at all
excellent you'll learn a lot of new
stuff today all right it's actually good
so the kind of large-scale batch
processing which we expect to take
minutes or hours and but then we expect
a very large volume of processing right
if you have to process a petabyte of
events or a petabyte of data it will
take a long time but that's what we have
batch processing for and real-time
processing is on the absolute scale of
that right we want to process events as
quickly as possible and obviously at the
high scale as possible which kind of
conflicts with each other right so
higher throughput means we will take
longer to process every event so let's
talk about what we can use for that
right and there are a lot of open source
frameworks which can do all of this
hopefully you've heard at least the
names of some of them spark has anyone
heard of sparked our precious Park
District has the room look we're so good
and there's a bunch of others there's a
patchy a storm all of these projects
have the names starting with Apache
something right there are all
open-source projects all of them JVM
based we'll get to Microsoft stuff in
just a moment
there's flink and Kafka Casca has I'm
sure everyone's heard of Kafka right
it's a message queue okay don't have to
raise your hands it's fine all right so
a storm and each one of those actually
does something somewhat different and so
it's interesting to think about not just
about frameworks because the frameworks
themselves you know there's new
frameworks coming out every year and all
frameworks die off but the concepts each
one implements are very interesting even
if you're going to go and roll your own
which I don't recommend but if you're
going to roll your own framework it's
important to understand what it is
you're going to be processing and what
the trade-offs are when you're trying to
do real-time data processing on streams
of data right so storm is one of the
oldest ones it's been developed at
Dinn sorry a Twitter actually summers
been developed at Twitter like five or
six years ago and it's been the
underlying framework for all of their
real-time processing so every time you
go and you do something with a tweet and
all the processing in the background
happens on a storm topology which does a
lot of processing and they recently came
out with storm 2.0 which they're which
is a new project called Heron which is
basically almost the same thing but just
better implemented now spark spark is
the process that evolves to replace
Hadoop because a doufu is really very
slow in disk bound and spark does a lot
of the stuff in memory and in our case
we're actually going to be more focused
on spark streaming which is an extension
to spark to the distributed processing
engine which can do this in memory in a
streaming fashion so it will
continuously process new data it comes
in and spark is very popular it's a huge
project lots of contributors and it's
one of the really mainstream very basic
solid tools to use and there's the two
on the bottom are relatively new both
flink and Casca Cafe is a message queue
it's very solid and everyone a lot of
companies use Casca but not everyone
uses it for processing messages which is
autumn what I'm going to be talking
about today so if we go over to
Microsoft there is a very cool tool from
axis research called Microsoft Orleans
as anyone heard of Microsoft Orleans
almost everyone that there was actually
a very good talk by Sergey Bick of last
year here about Orleans
if anyone caught it I was really good so
Orleans is actually an actor system
right it's a similar to a cow or a cadet
net it's not actually a stream
processing system to begin with it's a
lets you create actors which is just
pieces of user code which you can deploy
and they run within the system and can
pass messages to each other and it's
very good for creating distributed
systems willing to be robust and
scalable but on top of Orleans there is
an extension called Orleans streams
which does stream processing because if
you look at message passing between
actors in a system it's very similar to
how streaming and data processing works
because the data processing model for
streaming data is basically to build a
graph a directed graph of operations and
then you stream data into it from one
source you each one of the operators
does something with the data and
passes it on to one or more downstream
operators each one of those does
something else isn't eventually they all
go to some kind of sink where it's
either write it to disk or it doesn't do
anything let's talk about a practical
example write in this is an example
actually I'm going to show a bit later
we want to do some we were building a
startup and we're going to do something
no one's ever done before we're going to
do sentiment analysis on tweets because
it's the hot new thing right it combines
machine learning with IOT and the ten
different other buzz words which will
definitely get it funded so we're doing
that right so what do we need to
actually do in order to get tweets in
real time and remember there's you know
half a billion of those happening every
day we need to bring it into our system
we need to store them and remember
Twitter the Twitter feed is a unwind
double screen which means if we miss a
tweet or if we can't process it in time
it's gone forever we can't go back and
restream it because meet with scheme
coming in and the API doesn't support
rolling back so it's very important to
actually per system very durably as fast
as possible and then do all the other
processing stuff we want to do with them
so want to put your tweet somewhere
presumably in some kind of cue a
distributed cue or maybe a database that
can actually absorb the six thousand
writes per second so it's going to be
scaled out to a lot of machines and then
we're going to take all this stuff take
it out of our store do some kind of
processing on it maybe enrich it with
you know geolocation data maybe in
Richard obviously do semantic analysis
on the tweet add that to the basic data
then enrich it with maybe data about the
user if we do some kind of user tracking
or profiling put it all back into the
data store and then expose all of this
stuff for querying but then keep in mind
we have 15 billion tweets every month
and that's you know maybe 15 terabytes
of data or so so we tend to keep all of
this stuff in our database forever right
we need to take that offloaded somewhere
else
and then we're going to go back to last
year's Twitter and reprocess them is
going to be in a different system which
can actually process now petabytes of
data whereas we'll keep near let's say a
month of tweets back and process them in
real time in a more real-time fashion so
for this we need some other set some
kind of
was actually collect tweets and if you
want to mentally map this to whatever
you want like a website which has events
and interactions with ads you can do
that right instead of connecting to a
Twitter - Twitter and streaming tweets
from there imagine that your users come
to your website they click something it
causes a server event suppose back to
the server and then your web application
or your micro service or whatever it is
has an event it's a real-time event you
can't go back because the user isn't
going to go back and redo the event
again you have to store it somewhere if
you want to record it forever
then you store it quickly and you know
hopefully your website is very
successful so there's millions of users
doing it's called concurrently so
there's a lot of events happening right
and we don't put all of them into a
durable storage and then process them so
there are basically two types of
processing we can do in real time one is
actually really real time which means as
soon as the tweet comes into our
processing framework we do something
with it and we can't rely on any kind of
history or any kind of other data
because every class or every user code
in our system just has this one event to
work with so the most we can do is
either account how many of those there
are and keep local track locally or
maybe we can talk to an external store
maybe you know put it in a database or
data field in a database that's pretty
much all we can do with a single event
the other kind of processing we can do
is micro batching there's obviously also
batching but batching is by definition
not real-time right you can go back and
look at all the tweets and calculate
stuff but in as close to me as you get
to the real-time is micro batching which
is taking a collection of events all
together let's say 10 seconds of tweets
or a minute of tweets doing some kind of
processing on them and then extracting a
bit more knowledge for example I want to
know what hashtags are trending for the
past minute right if I know what's
trending obviously I need more than a
single tweet to each to work with so I'm
going to collect let's say a minutes of
tweets which is going to be you know
quite substantial amount it's going to
be several hundred thousands and then I
can do some kind of real-time processing
it's still close enough to real-time
right it's a minute minute is you know
it's sort of on the edge of real-time
then I can extract something that's
trending and then I can send this
downstream from my streaming topology
and something will happen with the tuner
maybe will adjust bids on ads or maybe
will what
you know sends you a large to users
saying that you know this thing is
trending so does it you want the two
models we can do and this frameworks I'm
going to be talking about they all
implement one or the other right because
it's a it's a very different kind of
processing engine and you can freely
combine them to very efficiently so
storm attaches to a patchy storm does it
implements a very continuous model you
basically define a lot of classes user
classes which I can think of them as
actors or just pieces of user code and
each one of those classes represents a
single operation and it can receive
inputs from a different one or maybe
from some kind of cue or source you can
do something with that input internet
can program send the input on to the
next in in the chain alright and this
happens in real time in which one of
them process is exactly one event at a
time all right obviously you can store
local store kits user class right you
can define any kind of variables in it
but if the application dies anything
that's stored in memory is going to
disappear so it's not actually a robust
way to store local data you do have to
have some kind of external store which
can store your state and the reason it
scales so well is because storm will
create a lot of copies of your
processing class right so if you have a
class which let's say calculates
sentiment on the tweet which we will
have in just a moment we can't just run
one of them right because there's no way
it can process to it fast enough we need
a hundreds of those maybe a thousand of
those to actually do the processing in
parallel right and so we have some kind
of is the framework itself takes care of
distributing the twist to the different
instances of our processing class and
then collecting the messages from them
and pushing them downstream to the next
class in the next class right and spark
streaming is built on spark itself so
it's as its roots in Hadoop and my
produce so it does micro batches it
actually fools us and you think in its
real time by taking very small batches
of data and then doing normal spark
processing on them right doing normal
MapReduce stuff which takes all the
batch crunches it together and then gets
a bunch of a batch of results and pushes
a downstream Kefka is the message queue
originally it's only did messages so if
you think of you know how many people
know what as your cues are your cues you
use them right
simple cues in Amazon right so it was
kind of like that right where you can
actually put a rain read data out you
had a bunch of different topics you can
read in the end consume and eventually
the guys at Lincoln who built Casca left
Lincoln and founded a new company called
constant and they're starting and they
created a actual processing platform
based on Kafka and they have enterprise
support and they build a bunch of other
stuff around it which does more than
just push messages around it actually
processes messages you can create
basically the same kind of topology for
processing messages as you can with
other frameworks and we'll show that in
just a bit and flink is actually very
similar conceptually to storm where we
define a bunch of actors which will
process your topologies and then we can
talk about how you do that like let's
get to the actual code right let's look
at some code so there's two ways to do
this right there's two ways to actually
declare her how our processing framework
is going to work we can have a
declarative style which looks very much
like linking dotnet everybody knows how
link works in dotnet pretty much yeah so
you actually link by the way is an
excellent example for stream processing
right you can have an infinitely large
enumerable and you can do some kind of
processing and anyone know how P link
works killing yes parallel link it's
like one kind saying yeah I use it every
day so you can actually do the same kind
of declarative processing in parallel
and if you take that and you can think
of it as doing the same thing but on
multiple machines
that's basically what stream processing
is when you do it declaratively you take
some kind of streaming source and you
transform it in different ways for
example this very simple example which
is like the hello world of the big data
world which is word count you can take
it take text you had count how many of
each word there are this is basically
the equivalent of writing hello world
when you learn a new programming
language so we take every string we
split it into into words and then we map
every word to account which is 1 and
then we reduce them we count how many of
each words we've seen right and this is
actually very concise syntax and for
simple things it works very well for
more complex things it doesn't work very
well because for example maybe instead
of just counting words
what we want to do is go and update a
database right so in one of the steps we
take an event in and we want to go into
beta database it's not really something
you can put in a single line in a
processor right and then so you can use
a compositional API style which lets you
actually define user created classes
which you know just all share some kind
of common ancestor in this case the
example on the left is from storm and
storm uses of a metaphor of spouting
bolts they have a thing with water going
on so a spout is the source of your data
it's a class which takes data from
somewhere and sends it downstream and
every bolt in the chain does some kind
of transformation so we can have a bolt
which just prints out the data it
receives for example so it doesn't
produce any data it only consumes data
or you can have a boat which takes in a
sentence it splits it into words and it
sends each out each word out so it takes
a small stream and it expands it into a
longer stream of multiple words right
and then what we can do is declare a
topology a graph of these components so
we can take you know three bolts you can
say this is one sends data to the second
one the second one sends data to the
third bolt and then the spouse will
start sending messages into the bolts
each one will do their thing and the
whole topology will work and obviously
we will have multiple copies of which
bolt which is how the whole thing scaled
right and most of these tools are
unfortunately written in JVM
unfortunately because even I'm in a
mostly dotnet conference in a Java
conference I'd say luckily they're all
written in Java in Java or Scala but
yeah this is the unfortunate truth of
the big data world most of the tools are
written in Java and there are very few
Microsoft issue or JVM related tools so
they're all they're all so for JVM some
of them actually support Python if you
want to declare your code in Python and
Orleans is basically the only example we
can point you and say Mike Stetz have
done something in this and we'll talk
about Orleans I'll show you an example
of that in just a minute let's look at
some examples right let's look at actual
code because it's early in the morning
everyone's still fresh we can look at
code right so let's look at code and
we'll start with storm because storm is
a has the simplest API and most
expressiveness so we can actually
demonstrate it is the easiest way
excellent all right so let's go to our
storms follows you there's going to be
some Java if you can just mentally
pretend it's broken c-sharp everyone's
going to be happy right it's kind of
this is kind of like Russian speakers
feel about Ukrainian and vice versa they
can kind of understand each other but
they don't don't admit it all right so
what we're going to do is as I said the
simplest thing we're going to count
words it's not exciting we're going to
do something more exciting in just a
couple of minutes but for starters we'll
do something very basic so we're going
to have a spout a storm spout which is a
class that sends out a bunch of text and
we're going to count how many times
we've seen every word it's a contrived
example but it demonstrates what we're
talking about very easily all right so
the storm topology is connected is know
yeah that everyone can see it especially
in the back it's a very weirdly shaped
room all right good so actually defining
our processing topology in storm is very
straightforward what we say is in our
topology we're going to have three
components three bolts right we're going
to have n1 spouts the spouts just
generate random text random sentences we
call it a spout and then we start
connecting processing classes to it
whether class called splits and sentence
and I'll go into this and show you the
code in just a sec but the name pretty
much says it all it takes a sentence and
splits it and bye-bye spaces right and
we connected to our spouts
so anything that gunk comes out of the
spout will go into our split sentence
class and notice we're actually going to
be running this is the parallelism we're
going to be running eight instances of
this class so we have you know a million
messages coming in every second is going
to be split between eight different
classes and those will almost certainly
run on different machines right so we
need to spin up a whole cluster of storm
oisin and I'm going to bore you with how
it's actually done in the engineering
sense but it's not easy I'm so I'm sorry
to say but it's much easier than writing
it all the way all on your own which is
the point so we're going to take the
split sentences which means are a small
string of sentences as three most
sentences is going to actually expand
every sentence is going to turn into
a bunch of different events in the
stream I'm going to split those and send
them to the word-count
both which actually does the actual
counting and stores the state of how
many times it's seen each bolt and the
important here and a thing here is we're
going to be shuffling it not just
randomly we're going to be consistently
sending any particular word to a
particular instance of this bolt right
so only so if the same word comes up it
will go to the same instance of a bolt
so it will actually be able to count it
right because each bolt we'll see in a
second internally thread safe but if you
same sent the same word different bolts
each one will have a partial count and
that's not what we want and from there
we just send it back send the actual
count to our printer bolt which all it
does is print it doesn't actually emit
anything it only collects data just look
inside the split sentence both just so
we can actually see how it works the
book itself is a very simple class right
it's a class which extends some kind of
basic storm class that storm knows how
to run it has exactly one method that
you can run it's called execute it gets
an input which is a tuple it's just a
class with a key in the value right and
it has a collector where you can write
output and it can write as much or as
little output as you want it doesn't
doesn't have to be one to one so we can
get a tuple which is a sentence in as
the input we split it and we get an
array of words from that sentence and
then we omit each one of those as a new
tuple right so it's very simple that a
sentence in we get a bunch of words out
done this goes into our actual word
count boat which again doesn't do
anything super complex all it takes is
every tuple and again in this case it's
not a sentence anymore the word count
both actually gets single words and it
just has a hash map which is a
dictionary for you C sharp speakers
where it just updates the count of every
how many times it C in the word and as
you can see the boat itself is entirely
thread-safe right we're using a local
variable we're guaranteeing that this
thing is going to always execute on the
same thread
there's no way to quote this exact code
concurrently so storm actually guarantee
is a very simple programming environment
for us all the stuff you know about how
you need to do logging
and thread safety and all that we don't
need to worry about any of this because
the framework takes care of the actual
execution environment and we just need
to provide the user code which does the
actual operations it's extremely
convenient especially when you start
thinking about how you're going to
synchronize concurrent code across a
cluster of 100 machines right so the
fact that all of these frameworks
actually take away the concern for
threading away from the developer is
extremely convenient right so we didn't
do here is we do all collect all the
counts right and then we omit the actual
count itself so instead of just omitting
the word we made a word and a number of
how many times we've seen that word
right now this is obviously very unsafe
who can think of why this thing is
actually a very bad implementation it's
very unsafe anyone not at your question
let me give an example what happens if
we shut down one of the machines in our
hundred machine storm topology again now
two questions
what happens we shut down the machine so
any instances of this bolt that we're
running in the memory of that machine
we're going to lose right so we're going
to lose all the states which means in
the machine will actually even a
shutdown storm will redistribute the
bolts to different machines storm takes
care of all that stuff but every one of
those will get a fresh new instance of
our bolt which has no state storage it
will just start counting from zero right
so we'll have some words with an
existing account in some words with a
new count so this is why in this case
we'll actually have to manually take
care of State and usually the answer to
that is put it somewhere else right
where do you put state in database
anyone said the debate is exactly
correct database is the place to store
external state obviously a fast database
because you know sequel server probably
isn't going to run at you know a hundred
thousand events per second or anything
all right so the printer bolt actually
that's exactly what it says in the cover
it does exactly one line so we got a
tuple and we print it that would be the
equivalent console console.writeline in
c-sharp I know I didn't have a
translator and I just like just like it
all right so let's run this and see if
we get some word counts right so we can
run the topology itself now I will run
it locally but in a real production
environment there's actually a cluster
of storm machines running and they're
doing nothing at the moment they only
start doing stuff when you
create a topology which is just a
library you built you compile and build
and you submit your library with your
user code to the storm cluster and then
it starts executing all the stuff you
defined alright because normally when
you spin up a storm cluster or spark
cluster or any of these things we're
talking about it's not actually doing
anything because it has no user code to
execute so you have to submit the user
code and it will get executed so let's
run this and unsurprisingly it will spin
out some text and we'll see that it
actually produces counts for different
words so the example itself is not very
interesting it spins up a bunch of
different stuff in the background
because again the clustering even if you
do it locally is very complex but then
we start seeing a bunch of word counts
appear if we pause this for a second we
can see that yes indeed it's actually
counting words so let's see see we had
at at 42 and then a second later we had
at 43 as well just continue accumulating
counts for every word this is obviously
not useful for anything but if you just
mentally map this to something useful
like let's say you're doing an anomaly
detection and you're listening to your
company Network you're attached to
active directory this is a real example
by the way from a company I worked with
they're attached to the active directory
event they're scanning all the login
events all the network access anyone who
opens the socket or calls the URL on the
front end and they're counting how many
times any IP has tried to log in or
access the computer and they're looking
for anomalies in their behavior right so
they're counting how many logins for
every machine for a very P happens
they're storing it in a separate
database and any time logging happens
outside of normal parameter for example
an IP which only connects during the
week days connected tries to plan during
the weekend that will cause an alert and
it's all implemented as a storm topology
with a bunch of rules and just counters
which counts how many times everything
happened and it runs on about ten
different machines because they're
purchasing a lot of events for a large
customer and this works very similarly
to this just a little more complex in
the actual execution logic all right so
let's switch over to spark which is an
entirely different concept spark
actually as I said came from was born
out of Hadoop and the needs to run very
large batch
processing operations in parallel and
then what happened was they took that
concept and Maps it on 2 micro batches
so it became much more real-time and
spark actually let's run this spark
works in a entirely different way from
storm first of all you don't declare
workers you actually declare your logic
as a logical expression kind of like
link kind of like the way Java streams
work so in this case we're creating a
stream of data coming in from my socket
just slice it up I'm going to type in
text it will listen to sockets on port
11 1811 and it will count the words
coming in from that in real time and the
logic itself is exactly the same as we
saw in storm right we're doing word
count
so we have our stream called lines and
we flat map it the same thing as we
would do with you know dotnet or Java or
anything any framework framework which
supports logical expressions with flat
tappet
by splitting the string itself by space
so we turn the small stream into a
larger stream of words then we map every
word to account and every word has a
count of 1 and then we're used to stream
right we aggregate the stream by word
and then we get the count of how many
times you seen every word and the
important thing is you can't do this
infinitely right because then it would
never finish right you can't just
because the stream will actually wait
for this for the data to finish before
it in aggregate and reduce it so we have
to define some kind of window of how
long to wait before you actually run
this calculation and so continuously run
the calculation in intervals all right
so we creates the stream with a 5 second
window
all right so every 5 seconds it will
take whatever it's accumulated until
then it will execute the count logic and
split and all that stuff and spit out an
answer in this answer can go somewhere
you can go into the database or some
kind of other storage or it can go and
be the basis of a different expression
which does something so let's run this
and see it actually works well
yeah there we go
so it'll actually just listen for
connections and sockets 1111 and there
we go let's go a a a b and c and we
should start seeing counts every 5
seconds right and as you can see in the
background or you can't see anything in
the left corner so let's move this let's
move it over here and this over here
there you go much better so you can see
that it's pretty out aggregations every
so often and if I actually give it some
text to collect in the next 5 seconds it
will count the letter B five and three
times right if we do this and continue
doing stuff it will spit out
intermediate counts all right every five
seconds this is actually the main
difference between the two approaches is
twofold one they have different latency
and throughput right storm is has much
lower latency because as soon as an
event comes in we run it for the entire
topology we do all the processing and we
get a result immediately usually within
milliseconds and there's monitoring that
can actually tell you how long the
processing takes for a single event
which spark the minimum latency
obviously is five seconds right we can't
get a result in less than five seconds
or an average of that would say even if
it even comes in on average will it will
wait two and a half seconds right
but because spark can batch results it
can reuse a lot of internal mechanism
for threading and memory allocation it
can process data a lot faster at scale
so if you need to process a million
events it will be a much more expensive
to process them one at a time in storm
even though every result will come back
quickly
then in spark because it will take the
whole million as a batch do processing
in the background and spit out a result
so spark will actually have spark
streaming rather will have much higher
throughput per second in the storm and
this trade-off is what you have to play
with right and companies which come
which come at this from fresh right they
have no streaming topologies they have
no data processing and they're building
something which needs to handle all the
traffic that's coming from their event
source whether it's a website or a
mobile application that's sending back
telemetry or whatever IOT is very hot
right now and obviously you have
something which produces events on the
edge device sends it back to the server
and then
you have to do something with it right
you have to do some kind of cool machine
learning stuff to serve ads to the user
obviously because again that's what the
internet is all about and one other
thing which I'm not allowed to say but
the difference is again if you need
real-time processing you go with
something conceptual conceptually built
for processing single events at a time
if you need something just
high-throughput and you don't care about
latency as much you go with storm and
you have the two approaches and all the
other frameworks they fall somewhere on
this continuum right we'll look at Kafka
which is also like at one events the
time we look at flink which does
batching and they're going to fall
somewhere in the middle all right so
let's look at the weather we have next
let's look at one more thing with spark
which is very cool and that's sparks
equal on top of the normal spark
execution engine we can actually run
queries spark has a language called
spark sequel which lets you actually run
queries on top of your stream in real
time right because you don't have to
have just one stream you can have
multiple streams for example let's say I
have a stream of user events coming in
from my website and I have a stream of
geoip GIPS or some kind of something
that provides extra context for every
user I can take both themes in spark and
I can join them right I can take for
every user or for every IP in the user
stream join with the matching IP and the
location from the Geo hash stream all
right and the stream can actually be
entirely cached in memory and that's
what spark will do and you can actually
go and just use a regular sequel
expression to join the streams in memory
we can do the same thing which would
storm for example but then we'd have to
manually write the join logic which is a
bit more complicated so in this case we
have exactly the same thing with
basically the same localhost to connect
to the socket who listens for forwards
and then instead of batching them what
we do is we store them in memory every
five seconds and then we run a query on
them where we do where we define a
schema on our data and the scheme is
very simple right it's a quiet table
with two columns we have a column cord
called word and a column called count
right and then we just select from the
Select
sorry : cool words an account is always
one because it's a single word and then
we just run select word and count of
everything as total right as we would in
sequel query and this will actually run
every five seconds and take the
aggregate data and run the query on the
aggregated data every five seconds so if
you run this we'll see that it does
basically the same thing I said let's
open our socket and listen to it and
take the while screen up because all of
these frameworks actually have a bunch
of stuff happening in the background
and I'm going to run a bunch of data
we'll see it now we have a one and in
the next five seconds we'll see all the
other letters I types again and now we
can actually see them as a cycle table
right and you can do sequel operations
on those which is very convenient
especially when you are trying to do
complicated analysis on them all right
and if link works small is the same it
works like a storm we're defined both to
call them differently but it's the same
idea
we're defined operators you connect them
in a graph one pointing downstream to
the next and you run your data through
them the main difference is the link
comes with a bunch of different
libraries which lets you do complicated
stuff on top of the streaming and
streaming stuff for example link comes
with library for CD complex event
processing where complex event
processing deals more with finding
patterns or doing more complicated
queries on top of your streaming data as
an example let's say we we have our word
stream coming in letters ABC and so on
I'm going to look for patterns I want to
find every pattern where the letters a B
and C repeat within a certain time frame
and if you think about this and look at
actual real world examples where this
originated think of for example stock
trades you have a stream of stocks
updates coming in all the time
and I want to look for patterns where a
certain stock behaves in a way it goes
up down up and down and then you expect
it maybe with some probability to go up
right and if it behaves this way within
a certain time frame maybe you go and
you adjust some positions and buy the
stock before it goes up again and again
we're talking about microseconds here
right because we want to adjust
positions within microseconds buy the
stock and sell it on microseconds later
if it goes up by you know I'm micro
percent or you can think of other things
like user behavior again the same thing
with the tracking user threats within
the network if we want to look for a
certain pattern of events and I didn't
identify them as a threat right we want
to search for patterns where someone
tries to login and it fails three times
and then immediately the same ip tries
to login with a different user name
right so any time a pattern happens
where one username fails three logs in
and then the same IP logs in with a
different username we push out an alert
and an alert right and you can also do
this manually you can implement this
logic manually but it's very convenient
if you can just define the pattern and
listen for it all right and I think
that's the same thing right we can see
we have exactly the same thing we had
earlier with the word tokenizer we take
in text with tokenize it by lines same
thing we had the storm but in this case
we define a pattern and listen to that
pattern in the stream so we're we want
as pattern which starts where the text
is equal the letter A it has to continue
and be equal to a letter B and there can
be other letters between those right
because we're listening for a whole
stream which one's a and then B and then
we want C to appear within one second
right and if it's more than one second
it's not interesting but it appears
within one second that's part of our
pattern and we wants to listen to it and
then we do something very interesting
because flink lets us lets us actually
connect to its internal state externally
it has a our PC mechanism we can connect
out form of tidal topology and ask it
questions right which is a very
interesting because then you can look at
your streaming process three processes
in topology it's kind of a micro service
which can serve requests right I can
always turn to it and say what's the
latest pattern or what's your current
internal state how many users have you
counted right and then instead of just
having a very closed processing system
we have something that reserved
intermediate data externally will
actually see an example of that in just
a moment right so let's run this and
again same thing we had earlier not you
I meant you open sockets run this thing
and now it will actually listen for
events coming in on port whatever
there we go yes and if I just put in it
all does its print out the letter but if
I happen to print in the exact pattern
it will find the pattern right and if I
do this in any sort of order it will
still detect the pattern within the
stream right and if I change if I wait
more than a second between B and C then
that's not part of the pattern obviously
right so if we do several of those
please act Q of them right so now we
actually have the ability to not just
take our events but actually do some
very complex processing that's declared
in a very straightforward matter right
you need to go and write some kind of
logic which aggregates and keeps State
and memory because state as we saw is
vulnerable vulnerable to failures to
network failures and machine failures so
it's very hard to store state and it's
very convenient when the framework does
this for us right so let's move on let's
move on to our last contender which is
Kefka and Kafka is interesting for two
things one it started out as I said as a
messaging queue and then they tacked on
the ability to actually execute code on
the events in in real time later and two
they've they've completely separated out
the runtime environment from the actual
processing environment one of the
problems with all of the mechanisms I
talked about right support our storm
spark link all of these is they
implement their clustering solution on
their own right so they have to do a lot
of work in a build a complex system
which will do all the scaling and
processing and failover and load
balancing between different cluster
classes in the topology and essentially
it breaks the single responsibility
principle by having a system which both
runs user code and manages resources and
it tries to figure out where to run
every piece of code where it's efficient
where it has resources to spare and you
know do all the self-healing katka
either by laziness or by lack of
resources or by design I have decided
not to deal with any of that
and they've separated out the execution
part from
clustering part a catechist reams
application just uses a library which
handles all the logic of connecting to a
Kefka topic after q reading messages
processing them in the application and
putting them in a different queue it
does nothing to say to say where the
code will run it's just a java
application or you can actually
implement it and go there's a catgirl
library for go and for dotnet as it
happens and it's your responsibility to
create copies of this application and to
run them which lets us do very
interesting things with scaling the
application because it's up to the
actual organization who uses them to
scale the application so for example
anyone who's using currently Kafka
streams most likely using some kind of
operations whether it's docker or you
know kubernetes or one of the
orchestrating frameworks which let it
let you run a bunch of copies of
application and every application
internally uses Kafka streams to
coordinate and read messages from Kafka
process them put them in different Kefka
queue right so separating out the actual
execution from resource management was
actually a very wise decision for Kefka
because it both let them write less code
and simplify the execution and also give
the control of where to execute code to
the actual user which which lets you
avoid trying to set up a very complex
system right spinning up a cluster of
storm or spark it's just a daunting task
on the engineering level to connect and
configure everything correctly so they
talk to each other alright so let's look
at cough cough before we move on and
it's exactly the same exact example only
we're going to improve something this
time instead of printing out the counts
we want to expose it as some kind of API
that you can query and get the current
count externally right and you can start
thinking of stream processing as kind of
a small use case of micro services
everyone knows about micro services
right it's the new thing everyone's
doing micro services even if they don't
have to so you can think of it as a
micro service right we're going to do
what we're going to do is the code
itself that does the processing is a
micro service except now instead of
getting web request is getting events as
requests does some kind of processing
and push the response
synchronously somewhere else and on top
of that it keeps state which we will
expose as an actual micro service with
the REST API and everything so you can
query the external state of our
processing right so we're going to be
counting words because again stupid
example but it's interesting from an
engineering point of view and then we
want to expose the count as a rest
service to anyone who wants to query it
instead of just printing it out all
right so let's take a look at that this
one all right so our cafe application is
exactly the thing we had earlier right
it takes messages don't have to read the
codes now I'm not there yet but what it
does is it takes messages from a stream
from a catechol queue in fact and any
anyone can put messages into a queue
right they can come in from anywhere at
all it takes messages from one queue
counts how many words there are puts
them in a different queue but you can
look at a queue or other is at a stream
as a very large change log of a table
right there's actually a duality between
a change log or a stream and a regular
table a key value table right because
what's a database table it's just the
current state of all the changes
accumulated so far so let's say we're
counting words and we all have two words
in our dictionary a and B at a certain
point we have a 1 then you have B 1 so
you have a 2 and a 3 and so on if you
look at this from a stream point of view
we'll see a bunch of A's and we can
count all of them if we coalesce this
into a table we'll just have a table
with 2 or 2 items a and count 4 and B
and the count 1 or something right so
you can always expose the current state
of a stream as a table and then you can
query this table to get a real-time look
into what's going on in the stream all
right so this is what Kefka actually
does and less you both process the
stream and expose it as kind of a
virtual table on top of the stream which
just duplicate all the values and gives
you the latest snapshot of every value
so as I said all it does is exactly as
we had earlier let's look at this
it takes a bunch of text lines from a
queue it splits them and as a explicit
them by spaces it flat flat maps the
values into words in a longer stream and
then it just groups them by the key and
the word right and then it prints out
the counts into a topic called something
else right so this part is not
interesting we've seen it like five
times so far but the instant part is we
can actually now spin up a rest service
and I'm going to show you how to spin up
a restaurant in Java because you know if
you have to do it you already know and
if you don't well you're in luck and
you're going to live much better lives
and happier not to know how to do rest
services in Java but basically we spin
up a rest service which can respond to
requests and it has a very simple API
you can go to slash counts and the word
itself will tell you how many times it
says seen the word right so if we run
this we need to produce some words let's
run the example we'll produce some words
the ABA ABC yes enough I think if we
read the topic we can see that will
actually have a bunch of words right so
now we read it and it's all from the
command line because that's the way
Kappa operate it's going to have a bunch
of words
but now the interesting part is we have
a rest service on top of it listening to
how many events we had so if you go into
our web app I we can call it and get a
response and say that we have a 25 times
writing to go and look for the count for
B there's 10 B's right so now we can
actually take a stream topology and
process a lot of data in a very scalable
manner processing our thousands or
millions potential events per second and
if we store States we can now query this
state as an application and this is the
direction a lot of streaming frameworks
go towards where they can let you
actually build applications on top of
town on top of your stream processing
topology and expose them as services
that someone else can consume and this
first of all in this case we've just
replaced the database right if we can
query our streaming topology directly we
don't need to put the data in database
which actually eliminates one very
complicated component from a complicated
system because you can actually now talk
to our data directly and get a snapshot
right away and also we actually have a
much more real-time view of what the
data is doing right because it updates
in real time so now let's look at a much
more interesting example where we can
actually do what I said earlier and
that's we're going to take tweets and
we're going to analyze them and do some
sentiment analysis I'm going to store
them and do a dashboard that shows you
tweets in real time and I have exactly
ten minutes to do this so hopefully
we'll make it just in time and yeah I'll
tell you it took a bit more than ten
minutes just to actually set up the demo
but not by a lot right because you have
you can connect all these things like
Legos Seth looks at our example and I
purposefully mixed and matched different
frameworks here you know obviously you
would never do this in production you'd
use just a single framework but what I'm
doing is I'm going to be using storm to
read tweets sorry sparked retweet from
the Twitter firehouse and Twitter by the
way has been just a massive boon to
anyone who's doing demos because it's a
this the best source for streaming demo
demos you can have right you should get
data and it's free its own you have an
internet connection you can get it the
problem with it is you have to provide
some keywords to filter tweet and I've
shared a lot of keywords and most of
them don't produce any tweets but I
finally figure out the keyword which
produces the best tweets and it's Trump
and I have no personal stake in the
election in the US but they've been
great for my demos so what you're going
to find a bunch of tweets with obviously
politically loaded keywords because it
gets you a lot of tweets and most
importantly they're beautiful for
sentiment analysis because there's a lot
of emotion and sentiment in political
tweets so all we do is we actually use
the built in plugin for course for spark
streaming which knows how to connect
Twitter and actually stream tweets to
you and we're going to be doing two
things we're going to aggregate them by
window by about 10 second window and
count hashtags and find the most
trending hashtags every 10 seconds and
then we're going to take all of the
tweets throughout most of the data save
about 6 different fields as a JSON
object and put it in a database wine and
database well I work with a Jewish I
have to plug casually somewhere in my
demo at least once I think it's Mike
it's in my contract somewhere and also
because we want to query them right and
I've seen I've shown you how you don't
have to use a database with Casca and I
now I want to show the other way where
you can actually put us in a datastore
and query it as a regular database and
then we'll take all the data do some
sentiment analysis on it and put it in a
hopefully a real-time updating graph so
that's from this and by the way really
really shouldn't is going to show all
the Switch on the screen please don't
read them they're terrible tweets they
don't I've made the mistake of reading
them because I was trying to find the
ones with the most positive and negative
sentiment and it ruined my life
basically all right so we have tweets
coming in if we go into our database
will see that we are indeed getting
tweets let's look over here and yeah we
get tweets like this
something-something what we're
interested in is text right so now what
you want to do is actually take these
tweets do some sentiment analysis enrich
them with data and then put them back in
a database so we can query them
somewhere right now why don't we do this
first we foo before we put them in
database because the processing might be
very complex it might take a lot of time
and as we know tweets aren't rewind able
if you lose the tweet if the processing
framework fails and before you store the
tweet you're going to lose it forever so
the first thing we want to do is store
it durably and then we want to do all
the other processing stuff so you can
either put them in a queue like Kafka or
Amazon Canisius or something like that
or put them in a database and then
stream them from that place and do all
the processing and stream them back so
trim them back we're going to use a
storm because why not
but you can do either any one of those
frameworks in this case we have a very
simple topology we have a spout which
the streams data from in this case
calibration the database but it could be
Casca or any kind of cue it filters out
anything that's not a tweet just so we
don't you know put garbage in a database
and then it calls the sentiment bolt
which takes it sweet calculates the
sentiment score it attaches it and then
source it back in a database so the
sentiment bolt itself looks very simple
like this there is all right it takes a
tweet it uses the stanford natural
language processing library which is
free and open source and you should
definitely play it if you're into that
which calculates a sentiment score from
0 to 4 0 is very negative 4 is very
positive and 2 is neutral and it just
gives us an integer right and then we
attach this integer back to the JSON
object and store it back in a database
so now some of the if some of the
objects will start getting sentiment
scores and we now have actually two
streams running in parallel one is
storing tweets and the other is trying
to catch up in attach scores to every
tweet right and because calculating
scores is much more complex the stream
is actually going to run at different
speeds because tweets and storing twitch
is very easy and likely in some
sentiments takes a lot longer so this is
the second stream the session second
processing is what we need to scale out
to much more much many more machines if
we want to actually catch up to the
first stream which is just saving tweets
so let's run this there we go and we'll
start actually reading all the old
tweets right because we've started
saving tweets like five minutes ago we
have a backlog now we can choose for the
backlog and then we'll get to real-time
and start getting the real line tweets
and it's going to spin up and do a bunch
of stuff and now we can go into our
database and actually do some queries on
the tweets right so we can do something
like select from tweets
come all right so let's say the app what
the average sentiment score is for
regular twitch which don't mention Trump
and if we look here we can see that it
is in fact let's expand this a bit I
think like Cougars
is about to explode I'm doing way too
many things all two ones here all right
that's about 1.4 1.3 which is on the
negative side but it's it's somewhere
view negative and neutral and if we just
change this to or have to stop this
because things are going to explode less
as I said natural language processing
takes a lot of CPU power and that's the
part we need to actually scale so stop
this and then go back to our queering
okay it's going to respond eventually
vary there we go that's better
all right so let's run this and we can
see that if we do include those then the
tweets are obviously much more negative
which really shouldn't come as a
surprise to anyone but it's interesting
to be proven right all right so as it
happens and we don't have a lot of time
so I'm going to show you how but I'm
also replicating tweets in the
background for my datastore to
elasticsearch who's heard of
elasticsearch hopefully everyone like
half the room ok good lesson church is
an engine for doing full text analysis
and real-time queries on data and it has
a plug-in called Cubana which does
real-time dashboards really nicely and
very simple to setup so in the
background I have a job running and
replicating data from our data store to
my elasticsearch and I've set up a
dashboard in elasticsearch to actually
show our tweets on the map if they have
a location and show the number of tweets
on a nice histogram and the graph of
sentiments and notice the top most
prolific users and this is just binding
by way of sharing an example that
streaming frameworks are actually
excellent ETL tools right you can stream
data from one place do some kind of
transformation in real-time and stream
it to another place every framework that
I know of has at least three common
plugins there's a plugin for reading and
writing data to HDFS
Amazon s2 s3 and elastic search let's
search is always in top three things
where you want to put your data all
right that's the first thing most
framework builders actually implement as
you can see tweets are still coming in
right we have shown to it for the last
15 minutes and refreshes every five
seconds and I can see the some of the
tweets happening and then you have bars
coming in and the sentiment graph has
stopped it's not actually moving on
because I stopped the topology which
calculates sentiment but the apology
that brings into it is still going right
and I cannot go back and rerun the
sentiment analysis and this is one of
the benefits of persistence queues and
persistent stores that I can go back and
actually change the way I calculate
things in parallel so let's say we have
the sentiment apology running all the
time and I change the algorithm
I now calculate sentiments differently
but I don't want to stop showing my
graph right now I wanted to keep running
and I can spin up a second copy of the
topology with the new algorithm and run
from the beginning of my log over all
the tweets stream all of them from the
beginning recalculate and when the
second apology catches up to the
real-time I can turn the first one off
and then I will have seamlessly actually
transitions from one type of processing
to another without any downtime and
without losing actual real-time data all
right so in this case as you can see the
end of this graph is actually moving
further in time because we stopped
processing it all right I'm done exactly
on time which is a new thing for me
because I like demos and I have a lot of
demos hopefully you've learned at least
something interesting and thank you very
much for coming and enjoy the rest of
the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>